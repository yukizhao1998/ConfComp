Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Container),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Reference),Outward issue link (Reference),Inward issue link (Supercedes),Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Quickstart's assembly can possibly filter out user's code,FLINK-1342,12763837,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,mbalassi,mbalassi,26/Dec/14 17:27,14/Apr/15 08:56,14/Jul/23 05:56,19/Mar/15 19:31,0.8.1,0.9,,0.9,,,,,,,0,,,"I've added a quick solution for [1] for the time being. The assembly still filters out everything from the org.apache.flink namespace, so any user code placed there will be missing from the fat jar.

If we do not use filtering at all the size of the jar goes up to almost 100 MB.

[1] https://issues.apache.org/jira/browse/FLINK-1225 ",,fhueske,githubbot,hsaputra,mbalassi,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 08:56:45 UTC 2015,,,,,,,,,,"0|i23se7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/15 20:30;fhueske;This is also true for all dependencies which are from the o.a.f namespace.
For example flink-hadoop-compatibility is not added to ./lib and not in Flink's class path by default.
Programs which depend on the hadoop-compat module and which are built with the quick start Maven config won't run unless the hadoop-compat jar is manually added to ./lib.

I'd suggest to only filter those dependencies which are present in ./lib even though this might end up in a rather bulky blacklist. ;;;","24/Jan/15 21:49;sewen;Can we use the maven shade plugin when building the fat jar, for more control? We can explicitly exclude the flink dependencies (with their transitive dependencies).;;;","16/Mar/15 13:52;githubbot;GitHub user rmetzger opened a pull request:

    https://github.com/apache/flink/pull/486

    [FLINK-1342] Fix filtering of usercode from quickstart

    With this change, we'll not just exclude everything from the ""org.apache.flink"" groupid.
    
    This has caused many troubles to users because it filtered out stuff from the maven modules which aren't in ""flink-dist"" (hbase, streaming-connectors, hadoop compat, gelly).
    
    Also, if users use the ""org.apache.flink"" groupid in their quickstart, we were filtering them out with the previous approach.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rmetzger/flink flink1342

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/486.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #486
    
----
commit 123e0c2cc6da062c76f7dc3be76d32080458bbc0
Author: Robert Metzger <rmetzger@apache.org>
Date:   2015-02-24T09:45:37Z

    [FLINK-1414] Move quickstarts to website

commit 57967ee761b6b49fb9a776c67c379f87864d20be
Author: Robert Metzger <rmetzger@apache.org>
Date:   2015-02-24T13:02:18Z

    [FLINK-1342] Use maven-shade-plugin in quickstarts to build fat-jar

----
;;;","16/Mar/15 14:00;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/486#issuecomment-81682932
  
    Looks good to me.
;;;","17/Mar/15 20:04;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/flink/pull/486
;;;","17/Mar/15 20:04;sewen;Merged into 0.9 in 04a738836f30d8967c427208ab04b1aef34f370a;;;","13/Apr/15 18:21;rmetzger;I'll backport the fix to release-0.8;;;","13/Apr/15 18:40;githubbot;GitHub user rmetzger opened a pull request:

    https://github.com/apache/flink/pull/597

    [FLINK-1342] Backport quickstart fixes to 0.8

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rmetzger/flink release-0.8

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/597.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #597
    
----
commit f2d0c70fa8640a3ca9e53105fbded47a25531e1f
Author: Robert Metzger <rmetzger@apache.org>
Date:   2015-04-13T18:37:54Z

    Backport quickstart fixes to 0.8

----
;;;","14/Apr/15 08:54;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/flink/pull/597#issuecomment-92703319
  
    I'm merging this
;;;","14/Apr/15 08:56;githubbot;Github user rmetzger closed the pull request at:

    https://github.com/apache/flink/pull/597
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in StringValue binary copy method,FLINK-1336,12762580,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,sewen,sewen,18/Dec/14 14:27,16/Jun/16 17:11,14/Jul/23 05:57,18/Dec/14 16:28,0.8.0,,,0.8.0,,,,,,,0,,,,,mbalassi,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 18 17:12:46 UTC 2014,,,,,,,,,,"0|i23krb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/14 14:31;rmetzger;Is this a release blocker?;;;","18/Dec/14 14:37;sewen;I would say so, yes;;;","18/Dec/14 16:28;sewen;Fixed via 6e9b2848d5fabace5c6ef491c87c562eed9b5f43;;;","18/Dec/14 17:12;mbalassi;Ok, then I'm also adding it to the release branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Getter/Setter recognition for POJO fields with generics is not working,FLINK-1333,12762116,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,16/Dec/14 21:06,18/Dec/14 11:12,14/Jul/23 05:57,18/Dec/14 11:12,,,,0.8.0,,,,,,,0,,,"Fields like
{code}
private List<Contributors> contributors;
{code}

Are not recognized correctly, even if they have getters and setters.
Workaround: make them public.",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 18 11:12:23 UTC 2014,,,,,,,,,,"0|i23hx3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/14 11:12;rmetzger;http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/63ef8e86;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible deadlock with two broadcast sets,FLINK-1332,12762022,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,ssc,ssc,16/Dec/14 14:52,19/Mar/15 10:39,14/Jul/23 05:57,19/Mar/15 10:39,0.7.0-incubating,,,0.9,,,Runtime / Coordination,,,,0,,,"We are writing some code for a student homework that reads two broadcast sets in the open method of a RichMapFunction.

We are seeing inconsistent behavior, sometimes the code works, sometimes it deadlocks, sometimes it crashes with an OutOfMemoryError...",,sewen,ssc,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 10:39:46 UTC 2015,,,,,,,,,,"0|i23hcn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/14 13:12;sewen;Can you share the program, or the program plan? Does it involve iterations or DAGs?

Broadcast variables can OOM the TaskManager, if they become too large. They are essentially user-code variables. OOM errors are a bit non-deterministic in Java, and depend on heap fragmentation, ...

We are pushing the new network code into 0.9-SNAPSHOT soon, which should solve the deadlocks in principled way completely...;;;","18/Dec/14 13:26;ssc;Here is the code, it doesn't do anything fancy:

https://gist.github.com/sscdotopen/70d77ea1532a6ced8f91;;;","19/Mar/15 10:39;uce;Fixed in 9d7acf3, 9c77f07.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MemorySegment ByteBuffer wrapping may fail with IndexOutOfBoundsException,FLINK-1327,12761765,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sewen,sewen,15/Dec/14 15:36,15/Dec/14 16:58,14/Jul/23 05:57,15/Dec/14 16:58,0.8.0,,,0.8.0,,,Runtime / Task,,,,0,,,The calls to {{ByteBuffer#limit(int)}} and {{ByteBuffer#position(int)}} are in the wrong order. The positioning fails if the old limit is smaller than the new position.,,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 15 16:58:12 UTC 2014,,,,,,,,,,"0|i23fsn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/14 16:58;sewen;Fixed via 283c398e43239bd62afec90875894eb2a4e5b110;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The output serializers may consume excessive memory,FLINK-1326,12761729,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,15/Dec/14 11:43,03/Feb/15 13:48,14/Jul/23 05:57,03/Feb/15 13:48,0.8.0,,,0.9,,,Runtime / Coordination,,,,0,,,"The SpanningRecordSerializer creates large byte arrays to buffer records during serialization. Since they never release the records, they may occupy a large amount of memory. Currently, we use one per output partition, which is due to the original architecture.

There are two ways to fix this:

 1. Use only one serializer per output, rather than one per output partition. This is the preferable way in the long run anyways.

 2. Release the arrays in the serializer (quick fix).",,aitozi,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 03 13:48:17 UTC 2015,,,,,,,,,,"0|i23fkv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/14 11:54;sewen;I have implemented the quickfix variant, for now.;;;","03/Feb/15 13:48;sewen;Fixed in bc69b0bb530a6daeea2c18b1ff9edbb4d1faa48c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Iterations may fail when a cached data set based on a sort is not fully consumed,FLINK-1324,12761622,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,14/Dec/14 14:58,14/Dec/14 15:40,14/Jul/23 05:57,14/Dec/14 15:40,0.8.0,,,0.8.0,,,Runtime / Task,,,,0,,,"This happens for example when a sort feeds into a merge join and the merge join exits early (zig zag merge determined that it can stop).

You can reproduce this bug by running the transitive closure in Scala (or Java with an enforced merge join) using the following toy data set.

{code}
Path(2, 1),
Path(4, 1),
Path(6, 3),
Path(8, 3),
Path(10, 1),
Path(12, 1),
Path(14, 3),
Path(16, 3),
Path(18, 1),
Path(20, 1) );
{code}",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 14 15:40:43 UTC 2014,,,,,,,,,,"0|i23exb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/14 15:40;sewen;Fixed via 4cc6bb1db8390d8339b585171e1cac63a1903c8f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala API does not respect WriteMode set by configuration,FLINK-1322,12761361,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,aljoscha,trohrmann,trohrmann,12/Dec/14 14:33,14/Dec/14 22:28,14/Jul/23 05:57,14/Dec/14 22:28,,,,0.8.0,,,,,,,0,,,"The Scala API does not have output methods which do not take a WriteMode parameter. As default value the NO_OVERWRITE is set. Consequently, a possible global WriteMode set in the configuration is always overwritten. The Java API behaves differently, if no WriteMode is provided. We should sync both APIs to guarantee consistent behaviour.",,githubbot,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 14 22:28:16 UTC 2014,,,,,,,,,,"0|i23den:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/14 14:50;githubbot;GitHub user aljoscha opened a pull request:

    https://github.com/apache/incubator-flink/pull/266

    [FLINK-1322] Make Scala API respect WriteMode set in Config

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/aljoscha/incubator-flink scala-write-overwrite

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/266.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #266
    
----
commit 844d8ed94608b236518b23d7085df63a8c92c578
Author: Aljoscha Krettek <aljoscha.krettek@gmail.com>
Date:   2014-12-12T14:48:19Z

    [FLINK-1322] Make Scala API respect WriteMode set in Config

----
;;;","14/Dec/14 15:00;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/266#issuecomment-66915741
  
    Looks good, will merge this...
;;;","14/Dec/14 22:26;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/266
;;;","14/Dec/14 22:28;sewen;Fixed via 0028238b31e36bb13f0642672d3a493f428f90eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Webclient fails to display plans where nodes are references from multiple iteration closures,FLINK-1316,12760754,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,10/Dec/14 14:24,10/Dec/14 14:51,14/Jul/23 05:57,10/Dec/14 14:51,0.8.0,,,0.8.0,,,Runtime / Web Frontend,,,,0,,,,,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 10 14:51:30 UTC 2014,,,,,,,,,,"0|i23a2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/14 14:27;aalexandrov;I have a student who is working on merging the Web and the submission clients as part of his bachelor thesis. 
I suggest to let him look into that.;;;","10/Dec/14 14:51;sewen;Fixed via a79ea784e6c5ee491cecf5c93aad2248029e3c4b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spurious failure in compiler due to corrupt branch tracking logic,FLINK-1315,12760738,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,10/Dec/14 13:52,28/Feb/19 14:30,14/Jul/23 05:57,10/Dec/14 14:52,0.8.0,,,0.8.0,,,API / DataSet,,,,0,,,"The optimizer fails in that case with the following stack trace:

{code}
Exception in thread ""main"" org.apache.flink.compiler.CompilerException: Bug: Tracing dams for deadlock detection is broken.
	at org.apache.flink.compiler.dag.TwoInputNode.placePipelineBreakersIfNecessary(TwoInputNode.java:618)
	at org.apache.flink.compiler.dag.TwoInputNode.instantiate(TwoInputNode.java:553)
	at org.apache.flink.compiler.dag.TwoInputNode.addLocalCandidates(TwoInputNode.java:504)
	at org.apache.flink.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:436)
	at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258)
	at org.apache.flink.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:305)
	at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258)
	at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258)
	at org.apache.flink.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:305)
	at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258)
	at org.apache.flink.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:305)
	at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258)
	at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:268)
	at org.apache.flink.compiler.dag.BinaryUnionNode.getAlternativePlans(BinaryUnionNode.java:105)
	at org.apache.flink.compiler.dag.BinaryUnionNode.getAlternativePlans(BinaryUnionNode.java:104)
	at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258)
	at org.apache.flink.compiler.dag.BulkIterationNode.instantiateCandidate(BulkIterationNode.java:296)
	at org.apache.flink.compiler.dag.SingleInputNode.addLocalCandidates(SingleInputNode.java:367)
	at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:315)
	at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258)
	at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258)
	at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258)
	at org.apache.flink.compiler.dag.BinaryUnionNode.getAlternativePlans(BinaryUnionNode.java:105)
	at org.apache.flink.compiler.dag.BinaryUnionNode.getAlternativePlans(BinaryUnionNode.java:104)
	at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258)
	at org.apache.flink.compiler.dag.DataSinkNode.getAlternativePlans(DataSinkNode.java:194)
	at org.apache.flink.compiler.PactCompiler.compile(PactCompiler.java:561)
	at org.apache.flink.compiler.PactCompiler.compile(PactCompiler.java:466)
	at org.apache.flink.client.LocalExecutor.executePlan(LocalExecutor.java:233)
	at org.apache.flink.api.java.LocalEnvironment.execute(LocalEnvironment.java:51)
	at org.apache.flink.api.scala.ExecutionEnvironment.execute(ExecutionEnvironment.scala:391)
{code}

The program that fails in the optimizer is from a user. I have not been able to reduce it significantly into a standalone test case that reproduces the bug.",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 10 14:52:05 UTC 2014,,,,,,,,,,"0|i239zb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/14 14:52;sewen;Fixed via ac80458fc089b79f1e793f8760d331a988c407c1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Auxiliary nodes in iterations are not correctly identified as ""dynamic"" or ""static""",FLINK-1311,12760418,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,09/Dec/14 11:01,28/Feb/19 14:30,14/Jul/23 05:57,09/Dec/14 14:46,0.8.0,,,0.8.0,,,API / DataSet,,,,0,,,"The static/dynamic path tagger starts on the original roots of the step functions, ignoring possible auxiliary nodes that we need to attach to the root (such as NoOps, when the root is a union)",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 09 14:46:39 UTC 2014,,,,,,,,,,"0|i2381j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/14 14:46;sewen;Fixed via 94c8e3fa9086d847aac0cd75fddbc3b5a797b474;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala Closure Cleaner logs very aggressive,FLINK-1310,12760412,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,aljoscha,sewen,sewen,09/Dec/14 10:30,12/Dec/14 00:11,14/Jul/23 05:57,12/Dec/14 00:11,0.8.0,,,0.8.0,,,API / Scala,,,,0,,,"The Scala Closure Cleaner puts out a lot of messages on INFO level. I vote to reduce this to DEBUG level, as it is not very informative and floods the log.",,aljoscha,githubbot,sewen,uce,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 12 00:11:30 UTC 2014,,,,,,,,,,"0|i23807:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/14 11:17;aljoscha;+1, change it;;;","10/Dec/14 14:52;sewen;[~aljoscha] Will you make a pull request for this issue?;;;","11/Dec/14 11:08;aljoscha;https://github.com/apache/incubator-flink/pull/262;;;","11/Dec/14 16:47;githubbot;Github user hsaputra commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/262#discussion_r21689347
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/ClosureCleaner.scala ---
    @@ -4,7 +4,7 @@
      * distributed with this work for additional information
      * regarding copyright ownership.  The ASF licenses this file
      * to you under the Apache License, Version 2.0 (the
    - * ""License""); you may not use this file except in compliance
    + * ""License"") you may not use this file except in compliance
    --- End diff --
    
    Please do not remove the semicolon (;) from the license header.
;;;","11/Dec/14 16:52;githubbot;Github user aljoscha commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/262#discussion_r21689770
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/ClosureCleaner.scala ---
    @@ -4,7 +4,7 @@
      * distributed with this work for additional information
      * regarding copyright ownership.  The ASF licenses this file
      * to you under the Apache License, Version 2.0 (the
    - * ""License""); you may not use this file except in compliance
    + * ""License"") you may not use this file except in compliance
    --- End diff --
    
    Yes, correct. I thought I fixed it, because the scalastyle plugin complained. Forgot to create the commit and push the update.
;;;","11/Dec/14 16:53;githubbot;Github user hsaputra commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/262#discussion_r21689877
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/ClosureCleaner.scala ---
    @@ -4,7 +4,7 @@
      * distributed with this work for additional information
      * regarding copyright ownership.  The ASF licenses this file
      * to you under the Apache License, Version 2.0 (the
    - * ""License""); you may not use this file except in compliance
    + * ""License"") you may not use this file except in compliance
    --- End diff --
    
    Wow scalastyle plugin complain about the typo? Impressive =)
;;;","11/Dec/14 16:56;githubbot;Github user aljoscha commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/262#discussion_r21690089
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/ClosureCleaner.scala ---
    @@ -4,7 +4,7 @@
      * distributed with this work for additional information
      * regarding copyright ownership.  The ASF licenses this file
      * to you under the Apache License, Version 2.0 (the
    - * ""License""); you may not use this file except in compliance
    + * ""License"") you may not use this file except in compliance
    --- End diff --
    
    Yeah, we have the header that we want hardcoded. No deviation allowed. :D
;;;","11/Dec/14 18:30;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/262
;;;","12/Dec/14 00:11;uce;Fixed in 880801caf61cc5aa1a4fe619cec2907ea0e3a4b5.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink logo in documentation does not link to website home,FLINK-1308,12759960,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ktzoumas,twalthr,twalthr,06/Dec/14 10:59,23/Feb/15 08:51,14/Jul/23 05:57,23/Feb/15 08:51,,,,,,,Project Website,,,,0,,,Clicking the Flink logo in the top left corner in documentation does not link to the websites home but to the documentation home. This should be corrected.,,ktzoumas,rmetzger,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 23 08:51:03 UTC 2015,,,,,,,,,,"0|i235an:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/14 12:48;rmetzger;I also noticed this. I think the old behavior was better.;;;","07/Dec/14 12:37;ktzoumas;Thanks, I will fix this.;;;","23/Feb/15 08:51;rmetzger;The issue has been resolved in https://github.com/apache/flink/commit/74bc7dd3c62cca347596ebf979ecdbc280b0bb3e#diff-7b2e6639a313e1c1692961ca7ec5566c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink's hadoop compatibility layer cannot handle NullWritables,FLINK-1305,12759717,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,ssc,ssc,05/Dec/14 16:30,16/Jun/16 17:07,14/Jul/23 05:57,14/Dec/14 22:27,0.7.0-incubating,,,0.8.0,,,API / DataSet,,,,0,,,"NullWritable is a special object that is commonly used in Hadoop applications. NullWritable does not provide a public constructor, but only a singleton factory method. Therefore Flink fails when users to try to read NullWritables from Hadoop sequencefiles.",,githubbot,sewen,ssc,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 14 22:27:40 UTC 2014,,,,,,,,,,"0|i233t3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/14 16:55;sewen;How does Hadoop handle this? In many cases, they create a new instance as well (public null-ary constructor) and use that to read the data...;;;","05/Dec/14 19:01;githubbot;GitHub user rmetzger opened a pull request:

    https://github.com/apache/incubator-flink/pull/252

    [FLINK-1305] [FLINK-1304] Test for HadoopInputWrapper and NullWritable support

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rmetzger/incubator-flink flink1304

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/252.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #252
    
----
commit 48778e83d2c04e2f65649b7c96f1aa92d4e1d350
Author: Robert Metzger <rmetzger@apache.org>
Date:   2014-12-05T18:19:29Z

    [FLINK-1305] [FLINK-1304] Test for HadoopInputWrapper and NullWritable support

----
;;;","07/Dec/14 22:54;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/252#issuecomment-65959720
  
    This change adds hadoop as a hard dependency to the `flink-java` project. Per the discussion on the mailing list, concerning support for Hadoop Writables, we voted to not do that and instead add an ""mimick interface"" to the Java API.
    
    For a big change like reversing that, it would be good to have some reasons...
;;;","08/Dec/14 12:56;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/252#issuecomment-66112318
  
    The reason why I added the dependency to `hadoop-common` is that I need it to be able to instantiate the `NullWritable`.
    
    Before that, we only needed the `Writable` interface which didn't require any other classes from Hadoop. However the `NullWritable` is depending on a lot of classes. I first tried it by copy-pasting classes from Hadoop, but after the 5th file or so I gave up. I don't know how many files it will be in the end. 
    But it quickly becomes dangerous shipping Hadoop code due to incompatible versions.
    
    Also, at the end of the day, people are going to have the Hadoop jars in their classpath anyways, because flink-runtime is depending on it.
    
    The only argument left is probably the collection based execution. I think that one only requires flink-core and flink-java. But if somebody has an issue with the hadoop dependency, they can exclude it.
;;;","14/Dec/14 15:36;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/252#issuecomment-66916880
  
    Okay, let's add it as a dependency library. Might also make the ""Hadoop compatibility"" implementation easier, if we can assume it as a dependency.
    
    For the collection execution, it should be fine. One can strip the Hadoop libraries if no Hadoop-related feature is used.
    
    +1
    
    Will merge this...
;;;","14/Dec/14 22:26;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/252
;;;","14/Dec/14 22:27;sewen;Fixed via 13968cd4de446b4f565a094554380eb8559b6cf9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBCInputFormat does not implement NonParallelInput interface,FLINK-1302,12759649,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,rmetzger,rmetzger,rmetzger,05/Dec/14 10:59,13/Apr/21 20:39,14/Jul/23 05:57,12/Dec/14 10:39,,,,,,,,,,,0,starter,,"The JDBCInputFormat for the Java API is not implementing the NonParallelInput interface.
There are no methods required for implementing the interface. Its just a maker for the optimizer.",,githubbot,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 12 10:40:17 UTC 2014,,,,,,,,,,"0|i233dz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/14 19:10;githubbot;GitHub user rmetzger opened a pull request:

    https://github.com/apache/incubator-flink/pull/253

    [FLINK-1302] Make JDBCInputFormat implement the NonParallelInput interface

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rmetzger/incubator-flink flink1302

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/253.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #253
    
----
commit 8dddd9b698abe83aab73a037fa7750f6f5550981
Author: Robert Metzger <rmetzger@apache.org>
Date:   2014-12-05T19:06:36Z

    [FLINK-1302] Make JDBCInputFormat implement the NonParallelInput interface

----
;;;","07/Dec/14 22:51;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/253#issuecomment-65959597
  
    Makes sense, good fix.
    
    +1 to merge.
;;;","12/Dec/14 10:38;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/253#issuecomment-66756897
  
    Okay, merging it.
;;;","12/Dec/14 10:39;rmetzger;Resolved in http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/66915995;;;","12/Dec/14 10:39;githubbot;Github user rmetzger closed the pull request at:

    https://github.com/apache/incubator-flink/pull/253
;;;","12/Dec/14 10:40;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/253#issuecomment-66757108
  
    Merged in http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/66915995, but I forgot to close the PR (closed it manually)
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several files without proper license headers,FLINK-1301,12759389,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fhueske,fhueske,fhueske,04/Dec/14 09:14,05/Dec/14 21:35,14/Jul/23 05:57,05/Dec/14 21:35,0.7.0-incubating,,,0.7.1-incubating,,,Build System,,,,0,,,"There are several files in our source distribution without proper AL2 license headers, including .md, .html, .css, .svg files in the documentation, and .scala files in the scala-quickstart.

These files are not found due to rather coarse-grained excludes configuration of the Maven Rat plugin.",,fhueske,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 05 21:35:53 UTC 2014,,,,,,,,,,"0|i231t3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/14 21:33;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/250
;;;","05/Dec/14 21:35;fhueske;Fixed with 770ce23152068807468ab7f5a16b2f408cdefef8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate class DataInputViewStream,FLINK-1300,12759224,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,03/Dec/14 17:24,28/Feb/19 14:29,14/Jul/23 05:57,06/Jan/15 15:52,0.8.0,,,0.9,,,,,,,0,,,"There is currently
{{flink-core:org.apache.flink.core.memory.DataInputViewStream}}
and
{{flink-java:org.apache.flink.api.java.typeutils.runtime.DataInputViewStream}}

CAVEAT: They implement a different variant of reading a byte: one reads unsigned, the other reads signed. Might be that one of the variants is wrong.",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 06 15:52:28 UTC 2015,,,,,,,,,,"0|i230sn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/15 15:52;sewen;Fixed via 875c6aacc455583e2bbf71b88c2d4c9aa6c22236;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for very large record for sorting,FLINK-1296,12758681,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,01/Dec/14 15:51,04/Feb/15 22:15,14/Jul/23 05:57,04/Feb/15 22:15,0.8.0,,,0.9,,,Runtime / Task,,,,0,,,"Currently, very large records (multiple hundreds of megabytes) can break the sorter if the overflow the sort buffer.

Furthermore, if a merge is attempted of those records, pulling multiple of them concurrently into memory can break the machine memory.",,fhueske,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 04 22:15:13 UTC 2015,,,,,,,,,,"0|i22xfz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/14 23:11;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/249#issuecomment-65960391
  
    This failes with the java 6 compiler, because it has a generics bug. I'll try and work around it...
;;;","04/Feb/15 22:15;fhueske;Fixed in 482565608414fea8df7ed86d5d9a8d90fdfcd4f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimizer prunes all candidates when unable to reuse sort properties,FLINK-1290,12758205,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,27/Nov/14 12:21,28/Feb/19 14:30,14/Jul/23 05:57,27/Nov/14 17:59,0.8.0,,,0.8.0,,,API / DataSet,,,,0,,,"Programs fail with an exception that no plan could be created.
The bug can be reproduced by the following code:

{code}
val data : DataSet[(Long, Long)] = ...

data.distinct(0, 1).groupBy(0).reduceGroup(...)
{code}",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 27 17:59:10 UTC 2014,,,,,,,,,,"0|i22ujr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/14 17:56;sewen;Turns out, the bug was actually more complicated, and required custom partitioners and binary operations to reproduce.

I implemented a fix already...;;;","27/Nov/14 17:59;sewen;Fixed via 45fb6d82386260fef5222cff498583036db20855;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken links to k-means examples in website,FLINK-1289,12758033,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ktzoumas,vkalavri,vkalavri,26/Nov/14 18:33,13/Apr/21 20:40,14/Jul/23 05:57,23/Feb/15 08:55,0.6-incubating,0.7.0-incubating,,0.8.1,,,Project Website,,,,0,documentation,easyfix,"The links to the k-means example in the Java programming guide (Iteration Operators and Broadcast Variables section) are broken, both in 0.7 and 0.6 documentation.",,chesnay,rmetzger,vkalavri,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 23 08:55:22 UTC 2015,,,,,,,,,,"0|i22tjb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/14 22:04;chesnay;iirc this is already resolved, but the change is not yet reflected on the website.;;;","23/Feb/15 08:55;rmetzger;The links are working in the current documentation. I'm closing the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN ApplicationMaster sometimes fails to allocate the specified number of workers,FLINK-1288,12758012,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,26/Nov/14 17:06,29/Jan/17 21:38,14/Jul/23 05:57,29/Jan/17 21:38,,,,,,,Deployment / YARN,,,,0,,,"There seems to be a bug somewhere that Flink fails to allocate a full YARN cluster.

For example, my cluster has 42 nodes with 46.09 GB each (according to the NodeManager)
If my Flink client is requesting 41 + 1 (workers + master) containers (with 46GB each), I'm only getting 41 containers (instead of 42).

I've contacted the yarn-dev list already for help and I'm trying to figure out the cause of the issue.

As a simple workaround, users can just request one more container than they actually need.",,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/15 12:33;sewen;Yarn error log.txt;https://issues.apache.org/jira/secure/attachment/12696751/Yarn+error+log.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 29 21:38:07 UTC 2017,,,,,,,,,,"0|i22ten:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/15 12:33;sewen;I have another case of the error, log attached...;;;","29/Jan/17 21:38;rmetzger;This issue doesn't appear anymore in the recent YARN implementations.

Most likely it was caused by YARNs scheduler;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failures and bad error message in CollectionInputFormat,FLINK-1286,12757978,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,26/Nov/14 14:43,28/Feb/19 14:29,14/Jul/23 05:57,26/Nov/14 17:51,0.8.0,,,0.8.0,,,API / Scala,,,,0,,,"Serialization failures occur in some cases in the CollectionInputFormat. The error message is also very non-helpful:

{code}
java.lang.IllegalStateException: unread block data
at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2421)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1382)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:274)
at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:236)
at org.apache.flink.runtime.operators.util.TaskConfig.getStubWrapper(TaskConfig.java:281)
at org.apache.flink.runtime.jobgraph.InputFormatVertex.initializeOnMaster(InputFormatVertex.java:46)
at org.apache.flink.runtime.jobmanager.JobManager.submitJob(JobManager.java:380)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.flink.runtime.ipc.RPC$Server.call(RPC.java:420)
at org.apache.flink.runtime.ipc.Server$Handler.run(Server.java:947)
{code}",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 17:51:57 UTC 2014,,,,,,,,,,"0|i22t7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/14 17:51;sewen;Fixed through d85893036b9a3122900010ce975feba43b27531d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update website layout,FLINK-1281,12757945,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ktzoumas,ktzoumas,ktzoumas,26/Nov/14 10:52,07/Dec/14 12:35,14/Jul/23 05:57,07/Dec/14 12:35,,,,,,,,,,,0,,,"The current front page of the Flink website does not clearly convey the salient points of the system from a user's perspective. The current layout was also originally created as a placeholder.

This issue tracks changes to the layout of the website and documentation and the content of the front page only.",,ktzoumas,,,,,,,,,,,,,,,,,,,,FLINK-1089,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-11-26 10:52:22.0,,,,,,,,,,"0|i22t07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the Record special code paths,FLINK-1278,12757708,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,25/Nov/14 14:44,07/Dec/15 17:25,14/Jul/23 05:57,07/Dec/15 17:25,0.8.0,,,1.0.0,,,Runtime / Task,,,,0,,,"There are some legacy Record code paths in the runtime, which are often forgotten to be kept in sync and cause errors if people actually use records.",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 17:25:35 UTC 2015,,,,,,,,,,"0|i22rl3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/15 17:25;sewen;Done in 0cae3eae27b4bdb48eee061be141c20240327798;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misspelled class name SlotAvalablbilityListener.java,FLINK-1276,12757313,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,smarthi,smarthi,smarthi,23/Nov/14 22:55,13/Apr/21 20:40,14/Jul/23 05:57,25/Nov/14 19:24,0.8.0,,,0.8.0,,,Runtime / Task,,,,0,,,"1. Misspelled Class name - 'SlotAvailablblityListener'.
2. All methods in MathUtils.java are declared as static final.
3. Many other minor fixes",,githubbot,hsaputra,smarthi,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 25 18:57:21 UTC 2014,,,,,,,,,,"0|i22p73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/14 23:46;githubbot;GitHub user smarthi opened a pull request:

    https://github.com/apache/incubator-flink/pull/227

    FLINK-1276

    Fixes FLINK-1276

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/smarthi/incubator-flink suneel

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/227.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #227
    
----
commit 1753024d3bd002ec65f4ca449a0f6fd48553a6d8
Author: Suneel Marthi <suneel.marthi@gmail.com>
Date:   2014-11-23T23:43:37Z

    Fixes FLINK-1276

----
;;;","24/Nov/14 13:15;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/227#discussion_r20788430
  
    --- Diff: flink-java/src/main/java/org/apache/flink/api/java/CollectionEnvironment.java ---
    @@ -24,13 +24,11 @@
     
     public class CollectionEnvironment extends ExecutionEnvironment {
     
    -	private boolean mutableObjectSafeMode = true;
    --- End diff --
    
    We actually want to make the execution mode configurable with a setter method, so this would actually be god to stay.
    
    We are thinking about how to do the configuration:
      - A set of setters
      - A configuration/parameter object
    
    I am leaning towards the second option, because we will have many more things to configure (closure cleaning, custom serializers, file system modes, mutable object modes)
;;;","24/Nov/14 13:17;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/227#discussion_r20788520
  
    --- Diff: flink-java/src/main/java/org/apache/flink/api/java/typeutils/ValueTypeInfo.java ---
    @@ -131,7 +127,7 @@ public String toString() {
     	
     	// --------------------------------------------------------------------------------------------
     	
    -	static final <X extends Value> TypeInformation<X> getValueTypeInfo(Class<X> typeClass) {
    --- End diff --
    
    The `final` modifiers in are technically redundant on such methods (as they are on private methods). To me, they still sort of express an intent that the logic is the function is a hard assumption that should not be easily swapped for something else. This, the modifier is a soft documentation token...
;;;","24/Nov/14 13:22;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/227#discussion_r20788734
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java ---
    @@ -49,10 +49,7 @@ public Instance getLocation() {
     	}
     	
     	public void setSharedSlot(SharedSlot sharedSlot) {
    -		if (this.sharedSlot == sharedSlot) {
    --- End diff --
    
    Is this correct? Does the second condition subsume the first?
;;;","24/Nov/14 13:26;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/227#discussion_r20788933
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/operators/hash/CompactingHashTable.java ---
    @@ -718,8 +719,7 @@ private void createPartitions(int numPartitions) {
     	}
     	
     	private void clearPartitions() {
    -		for (int i = 0; i < this.partitions.size(); i++) {
    -			InMemoryPartition<T> p = this.partitions.get(i);
    +		for (InMemoryPartition<T> p : this.partitions) {
    --- End diff --
    
    The for-each syntax creates an iterator, while the for loop with indexes does not. 
    I think this is not critical here, but in general, I have used the indexed access in some parts on purpose to avoid iterator creation and circumvent the danger of a concurrent modification exception.
;;;","24/Nov/14 13:45;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/227#discussion_r20789890
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/operators/hash/HashPartition.java ---
    @@ -44,8 +46,8 @@
     
     /**
      * 
    - * @param BT The type of the build side records.
    - * @param PT The type of the probe side records.
    + * BT - The type of the build side records.
    + * PT - The type of the probe side records.
    --- End diff --
    
    Should this be `@param <PT>` ?
;;;","24/Nov/14 13:54;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/227#discussion_r20790380
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/operators/hash/HashPartition.java ---
    @@ -297,9 +299,7 @@ public int spillPartition(List<MemorySegment> target, IOManager ioAccess, FileIO
     		}
     		
     		// return the memory from the overflow segments
    -		for (int i = 0; i < this.numOverflowSegments; i++) {
    -			target.add(this.overflowSegments[i]);
    -		}
    +		target.addAll(Arrays.asList(this.overflowSegments).subList(0, this.numOverflowSegments));
    --- End diff --
    
    The statement is again more efficient than the one below (which is somewhat better readable).
    
    I think it is good in this case, as the spillPartition() method is not part of a an inner loop. Inside inner loops, I try to be very careful with such operations, to keep object allocations and indirections to a minimum.
;;;","24/Nov/14 13:56;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/227#discussion_r20790487
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/util/EnvironmentInformation.java ---
    @@ -98,7 +98,7 @@ public static String getUserRunning() {
     			return UserGroupInformation.getCurrentUser().getShortUserName();
     		}
     		catch (Throwable t) {
    -			if (LOG.isDebugEnabled() && !(t instanceof ClassNotFoundException)) {
    --- End diff --
    
    I actually added this check on purpose to avoid a confusing error message in local execution (out of the IDE), where Hadoop is frequently not in the classpath.
;;;","24/Nov/14 13:58;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/227#issuecomment-64196769
  
    @smarthi I have some inline comments / questions at some points. What is your opinion on those issues?
    
    Most of the changes are good fixes.
;;;","24/Nov/14 23:32;githubbot;Github user hsaputra commented on the pull request:

    https://github.com/apache/incubator-flink/pull/227#issuecomment-64285034
  
    @smarthi, this PR contains multiple fixes for different issues and improvement on code styles.
    
    Could you reduce the fix to do what the original PR want to do or the other way is to expand the description of the PR to explain things included in the PR.
    

;;;","24/Nov/14 23:52;smarthi;[~hsaputra] Yes i was gonna do that. But I can't get to it immediately, maybe first thing tomorrow.;;;","25/Nov/14 00:15;hsaputra;Thanks! The PR was hard to review due to the scope of the fixes included. It also making hard to do cherry pick for patches if needed.;;;","25/Nov/14 01:14;githubbot;Github user smarthi closed the pull request at:

    https://github.com/apache/incubator-flink/pull/227
;;;","25/Nov/14 01:19;githubbot;GitHub user smarthi opened a pull request:

    https://github.com/apache/incubator-flink/pull/229

    Fixes FLINK-1276

    [FLINK-1276] Misspelled class name SlotAvalablbilityListener.java

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/smarthi/incubator-flink suneel

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/229.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #229
    
----
commit 8750bf32c77b4bd49b5375d98e5d9cb7746174b4
Author: Suneel Marthi <suneel.marthi@gmail.com>
Date:   2014-11-25T01:17:51Z

    Fixes FLINK-1276

----
;;;","25/Nov/14 09:19;githubbot;Github user aljoscha commented on the pull request:

    https://github.com/apache/incubator-flink/pull/229#issuecomment-64330026
  
    This looks good now. Any objections to merging this right away?
;;;","25/Nov/14 10:01;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/229#issuecomment-64349550
  
    Looks good, +1
;;;","25/Nov/14 12:17;githubbot;Github user hsaputra commented on the pull request:

    https://github.com/apache/incubator-flink/pull/229#issuecomment-64391429
  
    +1
;;;","25/Nov/14 13:31;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/229#issuecomment-64399399
  
    Thanks @aljoscha for merging this. I vote to use GitHub for synchronization *before* merging, because the sync between Apache and GitHub is rather unpredictable. ;-) It would also be nice to adjust the commit message to conform to our general format when necessary.
;;;","25/Nov/14 14:10;githubbot;Github user aljoscha commented on the pull request:

    https://github.com/apache/incubator-flink/pull/229#issuecomment-64404219
  
    I merged it, but the sync is rather slow...
;;;","25/Nov/14 18:57;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/229
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
configure() not called with usercode classloader as context classloader,FLINK-1265,12756731,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,fhueske,fhueske,20/Nov/14 16:25,20/Nov/14 16:54,14/Jul/23 05:57,20/Nov/14 16:54,0.7.0-incubating,,,0.7.1-incubating,,,Runtime / Task,,,,0,,,"The configure() method of user functions is called with the system classloader as context classloader.
Since, configure() is usercode, context classloader must be the usercode classloader to have acccess usercode classes. ",,fhueske,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 20 16:54:29 UTC 2014,,,,,,,,,,"0|i22lzz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/14 16:54;fhueske;Fixed with 4a74f3281bd04ed76b911d2038ea8e3d9c0b7963;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom partitioners are not properly forwarded to the runtime,FLINK-1264,12756709,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,20/Nov/14 14:59,20/Nov/14 19:55,14/Jul/23 05:57,20/Nov/14 19:55,0.8.0,,,0.8.0,,,Runtime / Task,,,,0,,,,,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 20 19:55:50 UTC 2014,,,,,,,,,,"0|i22lv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/14 19:55;sewen;Fixed via d0f2db06e17d1a5d20adcf4f9ca555f4885774b9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Manual Partition operations are considered to keep data types constant,FLINK-1263,12756707,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,20/Nov/14 14:50,28/Feb/19 14:30,14/Jul/23 05:57,21/Nov/14 15:58,0.8.0,,,0.8.0,,,API / DataSet,API / Scala,,,0,,,This leads to a big loss in optimization potential and fully voids some cases.,,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 22 12:48:48 UTC 2014,,,,,,,,,,"0|i22lun:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/14 15:06;sewen;Actually, the problem is different. Binary operations on custom partitions fail to consider the partitionings as compatible.;;;","20/Nov/14 23:52;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/223

    [FLINK-1263] [optimizer] Implement compatibility checks for binary operators and custom partitioning

    This enhances the optimizer to reuse custom partitionings (created via groupinga and partition operations) in joins, if the same custom partitioning is used on both input data sets.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink binarycustom

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/223.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #223
    
----
commit 2ffd90bce5b1de9d6f5b8038ddcbd7c543f10f7e
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-20T14:25:21Z

    [APIs] Enhance test coverage for CollectionInputFormat and add tests for failed serializations of user code objects

commit 69a54ebb759e44cca52b0065bdb2aab621cbc30e
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-20T15:09:40Z

    [FLINK-1263] [optimizer] Implement compatibility checks for binary operators and custom partitioning

----
;;;","21/Nov/14 15:58;sewen;Fixed via cf54a1c2af1e228f83609903ba288476c5f05fee;;;","22/Nov/14 12:48;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/223#issuecomment-64079206
  
    Manually merged in cf54a1c2af1e228f83609903ba288476c5f05fee
;;;","22/Nov/14 12:48;githubbot;Github user StephanEwen closed the pull request at:

    https://github.com/apache/incubator-flink/pull/223
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inconsistency between CsvReader.java and TupleGenerator.java,FLINK-1262,12756696,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,Chen Xu,Chen Xu,20/Nov/14 13:24,20/Nov/14 17:38,14/Jul/23 05:57,20/Nov/14 17:38,,,,,,,,,,,0,,,"Party of codes in CsvReader.java are generated by modifyCsvReader() in TupleGenerator.java. However, it seems the codes in CsvReader.java are manually changed, so that there is an inconsistency between CsvReader.java and TupleGenerator.java in current code base.

CsvReader.java.
return new DataSource<Tuple1<T0>>(executionContext, inputFormat, types, Utils.getCallLocationName());

TupleGenerator.java
sb.append("">>(executionContext, inputFormat, types, DataSet.getCallLocationName());\n"");",,Chen Xu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 20 17:38:26 UTC 2014,,,,,,,,,,"0|i22ls7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/14 15:47;rmetzger;Upps. I think I caused this in FLINK-1221.;;;","20/Nov/14 15:50;Chen Xu; I have sent a pull request to fix it.;;;","20/Nov/14 17:38;rmetzger;Thank you for fixing the issue.

I merged your pull request in http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/b3e5ed0b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add custom partitioning to documentation,FLINK-1260,12756544,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,rmetzger,fhueske,fhueske,19/Nov/14 21:20,28/Feb/19 14:29,14/Jul/23 05:57,17/May/15 09:50,0.7.0-incubating,,,0.9,,,API / Scala,Documentation,,,0,,,"The APIs allow to define a custom partitioner to manually fix data skew.
This feature is not documented.",,fhueske,rmetzger,,,,,,,,,,,,,,,,,FLINK-1961,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 17 09:50:32 UTC 2015,,,,,,,,,,"0|i22kxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/15 09:50;rmetzger;Resolved as part of http://git-wip-us.apache.org/repos/asf/flink/commit/b335f587;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FilterFunction can modify data,FLINK-1259,12756542,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,fhueske,fhueske,19/Nov/14 21:16,28/Feb/19 14:30,14/Jul/23 05:57,06/Jan/15 15:51,0.7.0-incubating,,,0.9,,,API / DataSet,API / Scala,,,0,,,"The FilterFunction returns a boolean for an input record which determines whether the record is filtered or not. 
However, the function can also modify the input record which has effects if the record is not filtered.

The optimizer assumes that the data is not changed by a FilterFunction, i.e., it assumes that a Filter preserves physical data properties (orders, partitionings, etc.) and might also be pushed down in the future. These assumptions can result in semantically incorrect programs, if the function actually changes its incoming records.

Possible solutions are:
- document the requirements (and hope that users read it and behave nicely)
- hand a copy to the function which can be modified but is not passed on. This has major performance implications and might confuse users as changes are invalidated. However, this could also be integrated with the mutable/immutable runtime switch (FLINK-1005)
",,fhueske,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 06 15:51:54 UTC 2015,,,,,,,,,,"0|i22kwv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/14 16:22;sewen;I vote to document that we strongly assume filter functions to not modify the data.
It seems like a reasonable assumption that should be easy to grasp.;;;","15/Dec/14 10:50;sewen;Any updates or other opinions on that? Otherwise, let's add to the Java / Scala Docs that we assume that the data is not modified...;;;","15/Dec/14 11:01;fhueske;Yes, let's add it to the documentation for now.
If we find that many users run into this problem, we can integrate it into the object non-reuse mode.;;;","06/Jan/15 15:51;sewen;Added description to documentation.

Fixed in e71ee0b7953f7b061f3541c63650651a471cb6b7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problems with generic types in Scala API,FLINK-1255,12756412,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,ssc,ssc,19/Nov/14 14:24,23/Feb/15 15:37,14/Jul/23 05:57,23/Feb/15 15:37,0.7.0-incubating,,,0.9,,,API / Scala,,,,0,,,"the code below produces the following exception:

{noformat}
Error:(47, 18) could not find implicit value for evidence parameter of type org.apache.flink.api.common.typeinfo.TypeInformation[K]
    data.groupBy { extractKey }
{noformat}

Fixing K to Long made the code run though

{noformat}
  def groupCount[T, K](data: DataSet[T], extractKey: (T) => K): DataSet[(K, Long)] = {

    data.groupBy { extractKey }
        .reduceGroup { group => countBy(extractKey, group) }
  }

  private[this] def countBy[T, K](extractKey: T => K,
                                  group: Iterator[T]): (K, Long) = {
    val key = extractKey(group.next())

    var count = 1L
    while (group.hasNext) {
      group.next()
      count += 1
    }

    key -> count
  }
{noformat}",,aljoscha,sewen,ssc,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 23 15:37:11 UTC 2015,,,,,,,,,,"0|i22k4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/14 14:44;sewen;I talked with [~aljoscha] about that and he mentioned, that the trick is to ask Scala to pass the implicit parameter into the utility function, rather that have it generated in the utility function.

Try this:
{code}
def groupCount[T : TypeInformation, K : TypeInformation](data: DataSet[T], extractKey: (T) => K): DataSet[(K, Long)] = {

    data.groupBy { extractKey }
        .reduceGroup { group => countBy(extractKey, group) }
  }
{code}
;;;","26/Nov/14 18:02;sewen;[~ssc] Do you think this is a fair workaround?

I think we should have an FAQ entry on this error in any case.;;;","23/Feb/15 15:37;aljoscha;Fixed by adding a FAQ entry explaining the situation in: https://github.com/apache/flink/commit/2bdfd88eb1fabd0e93bd6293213e7157ed9b1437;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimizer bug during pipeline breaker placement,FLINK-1254,12756373,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,19/Nov/14 11:26,28/Feb/19 14:30,14/Jul/23 05:57,19/Nov/14 14:49,0.8.0,,,0.8.0,,,API / DataSet,,,,0,,,"The compiler fails on certain programs when trying to place pipeline breakers.

This code reproduces the error:

{code}
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(8);

// the workset (input two of the delta iteration) is the same as what is consumed be the successive join
DataSet<Tuple2<Long, Long>> initialWorkset = env.readCsvFile(""/some/file/path"").types(Long.class).map(new DuplicateValue());

DataSet<Tuple2<Long, Long>> initialSolutionSet = env.readCsvFile(""/some/file/path"").types(Long.class).map(new DuplicateValue());

// trivial iteration, since we are interested in the inputs to the iteration
DeltaIteration<Tuple2<Long, Long>, Tuple2<Long, Long>> iteration = initialSolutionSet.iterateDelta(initialWorkset, 100, 0);

DataSet<Tuple2<Long, Long>> next = iteration.getWorkset().map(new IdentityMapper<Tuple2<Long,Long>>());

DataSet<Tuple2<Long, Long>> result = iteration.closeWith(next, next);

initialWorkset
	.join(result, JoinHint.REPARTITION_HASH_FIRST)
	.where(0).equalTo(0)
	.print();

Plan p = env.createProgramPlan();
compileNoStats(p);
{code}",,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 19 16:32:12 UTC 2014,,,,,,,,,,"0|i22jvr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/14 13:02;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/216

    [FLINK-1254] Fix compiler bug for pipeline breaker placement

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink compiler_fix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/216.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #216
    
----
commit 8ac89d741a949933dc3c93090c642e3d5cd4f21c
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-19T11:28:06Z

    [FLINK-1254] Fix compiler bug for pipeline breaker placement

----
;;;","19/Nov/14 14:49;sewen;Fixed via ce822bf7f5ec80df5d5a749b1439320af3fb8b18;;;","19/Nov/14 16:32;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/216
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests occasionally die with GarbageCollectionOverhead exceeded,FLINK-1253,12756361,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,19/Nov/14 10:41,19/Nov/14 16:32,14/Jul/23 05:57,19/Nov/14 14:48,0.8.0,,,0.8.0,,,Build System,,,,0,,,"I have seen tests occasionally dying from GC Overhead Limit exceeded exception. I assume it happens because we reuse the JVMs across tests in the unit tests, possibly some test artifacts linger.

I suggest to add the {{-XX:-UseGCOverheadLimit}} option to the tests, so that they do not break on the build server.",,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 19 16:32:12 UTC 2014,,,,,,,,,,"0|i22jt3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/14 13:06;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/217

    [FLINK-1253] Make sure tests do not die with garbage collection overhead exceeded

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink gc_fix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/217.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #217
    
----
commit 6784064f5537383d6298f6913651414ee757c6e5
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-19T13:04:35Z

    [FLINK-1253] Make sure tests do not die with garbage collection overhead exceeded

----
;;;","19/Nov/14 13:22;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/217#issuecomment-63638378
  
    I think this is good to merge. See http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html#cms.oom for details about the flag.
;;;","19/Nov/14 14:48;sewen;Fixed in ae505adb86d4f9b48c1473f9be15ee6d9b35acb1;;;","19/Nov/14 16:32;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/217
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CollectionInputFormat is not able to handle Enums,FLINK-1251,12756256,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,18/Nov/14 23:08,28/Feb/19 14:29,14/Jul/23 05:57,20/Nov/14 14:32,,,,0.8.0,,,,,,,0,,,"It seems that the CollectionInputFormat is not able to handle POJOs containing enums properly.

{code}
	public enum Category {
		CAT_A, CAT_B
	}

	public static class PojoWithDateAndEnum {
		public String group;
		public Date date;
		public Category cat;
	}
{code}

leads to
{code}
java.lang.IllegalStateException: unread block data
	at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2424)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1383)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:245)
	at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:231)
	at org.apache.flink.runtime.operators.util.TaskConfig.getStubWrapper(TaskConfig.java:281)
	at org.apache.flink.runtime.jobgraph.InputFormatVertex.initializeOnMaster(InputFormatVertex.java:46)
	at org.apache.flink.runtime.jobmanager.JobManager.submitJob(JobManager.java:380)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.flink.runtime.ipc.RPC$Server.call(RPC.java:420)
	at org.apache.flink.runtime.ipc.Server$Handler.run(Server.java:947)

org.apache.flink.runtime.client.JobExecutionException: java.lang.IllegalStateException: unread block data
	at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2424)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1383)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:245)
	at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:231)
	at org.apache.flink.runtime.operators.util.TaskConfig.getStubWrapper(TaskConfig.java:281)
	at org.apache.flink.runtime.jobgraph.InputFormatVertex.initializeOnMaster(InputFormatVertex.java:46)
	at org.apache.flink.runtime.jobmanager.JobManager.submitJob(JobManager.java:380)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.flink.runtime.ipc.RPC$Server.call(RPC.java:420)
	at org.apache.flink.runtime.ipc.Server$Handler.run(Server.java:947)

	at org.apache.flink.runtime.client.JobClient.submitJobAndWait(JobClient.java:256)
	at org.apache.flink.test.util.JavaProgramTestBase$TestEnvironment.execute(JavaProgramTestBase.java:216)
	at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:578)
	at org.apache.flink.test.javaApiOperators.ReduceITCase$ReduceProgs.runProgram(ReduceITCase.java:364)
	at org.apache.flink.test.javaApiOperators.ReduceITCase.testProgram(ReduceITCase.java:66)
	at org.apache.flink.test.util.JavaProgramTestBase.testJob(JavaProgramTestBase.java:121)
{code}

I don't know if this is related: https://bugs.openjdk.java.net/browse/JDK-7132873",,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 20 14:32:49 UTC 2014,,,,,,,,,,"0|i22j5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/14 10:42;sewen;I think that happens if our own type serializer (which we use inside the writeObject() method) fails.;;;","19/Nov/14 13:04;rmetzger;Yep. It is working now with a Serializer for Enums.;;;","20/Nov/14 14:32;rmetzger;Resolved in http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/8081ddc5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong config key for taskmanager heartbeat timeout,FLINK-1250,12756094,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,18/Nov/14 13:11,28/Feb/19 14:01,14/Jul/23 05:57,18/Nov/14 16:29,0.8.0,,,0.8.0,,,Runtime / Coordination,,,,0,,,"The current config key is called {{jobmanager.max-heartbeat-delay-before-failure.sec}} even though the value is specified in milliseconds.

I propose to change the key to 
{{jobmanager.max-heartbeat-delay-before-failure.msecs}}",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 16:29:29 UTC 2014,,,,,,,,,,"0|i22i73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/14 16:29;sewen;Fixed via 9f6a0b8fa314d0593ac802e65adf6072c39743ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add custom partitioner for CoGroup,FLINK-1249,12756077,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,18/Nov/14 11:25,28/Feb/19 14:30,14/Jul/23 05:57,26/Nov/14 13:57,0.8.0,,,0.8.0,,,API / DataSet,API / Scala,,,0,,,"Currently, custom partitioners exist only for Join, Partition, and the different groupings.",,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 13:57:34 UTC 2014,,,,,,,,,,"0|i22i3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/14 17:18;sewen;I will fix this issue...;;;","24/Nov/14 23:44;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/228

    [FLINK-1249] [APIs] [compiler] Add custom partitioner for CoGroup

    Java AP support done, Scala API support pending

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink cg_partition

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/228.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #228
    
----
commit ffb3d62f31d29762c80b27e734dfa6acad615b6f
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-24T19:01:21Z

    [FLINK-1249] [APIs] [compiler] Add custom partitioner for CoGroup

----
;;;","25/Nov/14 14:28;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/incubator-flink/pull/228#issuecomment-64406831
  
    Looks good. I'll merge it.
;;;","25/Nov/14 18:57;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/228
;;;","26/Nov/14 13:57;sewen;Fixed via bcdd167f4671976a92ad9300256874804598dd3e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Manually built docu doesn't apply CSS/images,FLINK-1248,12755963,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,chesnay,chesnay,17/Nov/14 23:49,26/Nov/14 18:18,14/Jul/23 05:57,26/Nov/14 18:18,0.7.0-incubating,,,,,,Documentation,,,,0,,,"When i opened the .html files created by the build_docs.sh script no CSS or images were applied. a quick investigation found leading '/' in a lot of relative paths to be the cause, ex.:
(taken from config.html)

{code}
  <head>
    ...
    <link rel=""stylesheet"" href=""/css/bootstrap.css"">
    <link rel=""stylesheet"" href=""/css/bootstrap-lumen-custom.css"">
    <link rel=""stylesheet"" href=""/css/syntax.css"">
{code}

removing these causes the website to be shown properly.","Ubuntu 14.04 LTS
Firefox 33",chesnay,sewen,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 18:18:19 UTC 2014,,,,,,,,,,"0|i22hen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/14 18:15;sewen;[~uce] Can you comment on this issue?;;;","26/Nov/14 18:18;uce;Fixed in 82f5154a9581678e18abd9af8c3663b2181fdc38.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Links in documentation broken,FLINK-1247,12755942,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,trohrmann,trohrmann,17/Nov/14 23:04,19/Nov/14 16:32,14/Jul/23 05:57,19/Nov/14 09:38,,,,,,,,,,,0,,,"There seem to be some links in the 0.7-incubating documentation broken. For example, the Aggregator link in the Accumulators & Counters section does not work.",,chesnay,githubbot,trohrmann,uce,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 19 16:32:12 UTC 2014,,,,,,,,,,"0|i22h9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/14 00:21;chesnay;i looked for this this particular link in the latest master and noticed i fixed it already, but is just not shown on the website. :>

but i found a few other broken links on the way which made me suspicious, so I'm gonna check the whole documentation now.;;;","18/Nov/14 20:50;githubbot;GitHub user zentol opened a pull request:

    https://github.com/apache/incubator-flink/pull/213

    [FLINK-1247] [docs] Fix more broken links in documentation

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/zentol/incubator-flink docu_more_links

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/213.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #213
    
----
commit 3fe0161369c810971b28a54d6fc6432c5885dec7
Author: zentol <s.motsu@web.de>
Date:   2014-11-18T18:53:39Z

    [FLINK-1247] [docs] Fix more broken links in documentation

----
;;;","19/Nov/14 09:26;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/213#issuecomment-63612421
  
    Thanks. Will merge this now.
;;;","19/Nov/14 09:38;uce;Fixed in 1ecacf0e70b8e17a6d5d03cc40d84ce4096d62b7.;;;","19/Nov/14 10:28;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/213#issuecomment-63619839
  
    Very good.
    
    +1 to merge
;;;","19/Nov/14 16:32;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/213
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test RecoveryITCase,FLINK-1246,12755866,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,17/Nov/14 18:12,06/Jan/15 14:03,14/Jul/23 05:57,06/Jan/15 14:03,0.8.0,,,0.9,,,Runtime / Coordination,,,,0,,,"The job execution sometimes finished with ""FAILED"", despite recovery.",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 06 14:03:26 UTC 2015,,,,,,,,,,"0|i22gtr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/14 11:24;sewen;I added additional debug output, so far the error could not be reproduced.;;;","06/Jan/15 14:03;sewen;Fixed with the introduction of through the Akka for distributed coordination, as of 88e64fc7837a3fb081e15b23a07d7764c91f3b6f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove JVM MaxPermSize parameter when we use Java 8,FLINK-1243,12755133,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,sarutak,sarutak,13/Nov/14 19:03,18/Nov/14 13:53,14/Jul/23 05:57,18/Nov/14 11:37,0.8.0,,,0.8.0,,,Runtime / Coordination,,,,0,,,JVM for Java 8 doesn't accept MaxPermSize parameter so let's remove it when we use Java 8.,Java 8,githubbot,sarutak,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 13:53:13 UTC 2014,,,,,,,,,,"0|i22cef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/14 19:04;githubbot;GitHub user sarutak opened a pull request:

    https://github.com/apache/incubator-flink/pull/200

    [FLINK-1243] Remove JVM MaxPermSize parameter when we use Java 8

    JVM for Java 8 doesn't accept MaxPermSize parameter so let's remove it when we use Java 8.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sarutak/incubator-flink remove-maxpermsize

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/200.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #200
    
----
commit 5d9dec32575fd8e9d58bf5ffde99d5aef86289b2
Author: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
Date:   2014-11-13T19:00:24Z

    Removed MaxPermSize parameter for JVM 1.8

----
;;;","14/Nov/14 19:50;githubbot;Github user hsaputra commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/200#discussion_r20381770
  
    --- Diff: flink-dist/src/main/flink-bin/bin/jobmanager.sh ---
    @@ -30,7 +30,12 @@ if [ ""$EXECUTIONMODE"" = ""local"" ]; then
         FLINK_JM_HEAP=`expr $FLINK_JM_HEAP + $FLINK_TM_HEAP`
     fi
     
    -JVM_ARGS=""$JVM_ARGS -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256m""
    +JAVA_VERSION=$($RUNNER -version 2>&1 | sed 's/java version ""\(.*\)\.\(.*\)\..*""/\1\2/; 1q')
    --- End diff --
    
    What is the $RUNNER env variable here?
;;;","14/Nov/14 19:54;githubbot;Github user sarutak commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/200#discussion_r20381999
  
    --- Diff: flink-dist/src/main/flink-bin/bin/jobmanager.sh ---
    @@ -30,7 +30,12 @@ if [ ""$EXECUTIONMODE"" = ""local"" ]; then
         FLINK_JM_HEAP=`expr $FLINK_JM_HEAP + $FLINK_TM_HEAP`
     fi
     
    -JVM_ARGS=""$JVM_ARGS -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256m""
    +JAVA_VERSION=$($RUNNER -version 2>&1 | sed 's/java version ""\(.*\)\.\(.*\)\..*""/\1\2/; 1q')
    --- End diff --
    
    Thanks for pointing out it.
    It's typo for JAVA_RUN and now I've fixed it.
;;;","14/Nov/14 20:51;githubbot;Github user hsaputra commented on the pull request:

    https://github.com/apache/incubator-flink/pull/200#issuecomment-63127028
  
    To clean up the patch, could you rebase them into one?
;;;","17/Nov/14 12:04;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/200#issuecomment-63295530
  
    I am a bit confused by the statement `if [ ""$JAVA_VERSION"" -ge 18 ]; then` - it looks as if it adds the flag only if the java version is greater or equal than Java (= 1.8 = 18).
    
    Is that not the exact opposite of what we are trying to do? Or am I parsing bash wrong here...
;;;","18/Nov/14 10:57;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/200#issuecomment-63453354
  
    Regarding this issue, I've found this: http://openjdk.java.net/jeps/122. If the argument is set anyways, you get a warning. So +1 to add this. @StephanEwen is right, though. It should be lower than (`-lt`). I've tested the version extraction and it works for me.
    
    Will merge this later today with the respective change.
    
    

;;;","18/Nov/14 11:37;uce;Fixed in 6989cec4c237cfd76c574e052955c678e503f257.;;;","18/Nov/14 13:53;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/200
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming examples project causes build error in Eclipse,FLINK-1242,12755115,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,13/Nov/14 17:19,17/Nov/14 12:31,14/Jul/23 05:57,17/Nov/14 12:31,0.8.0,,,0.8.0,,,Build System,,,,0,,,"I get the following error: {{maven-dependency-plugin (goals ""copy-dependencies"", ""unpack"") is not supported by m2e}}

I think it should be possible to define an entry in the m2e lifecycle plugin to ignore this plugin command. We have done this various times for other plugins not supported by m2e. {{flink-tests/pom.xml}} does that for example (very bottom)",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 17 12:31:57 UTC 2014,,,,,,,,,,"0|i22can:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/14 12:31;sewen;Fixed via e23874cd886711d4a0c2812562e20719cc35ebb4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compiler rejectes non-nested iterations in constant path of an Iteration.,FLINK-1235,12754774,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,12/Nov/14 15:12,28/Feb/19 14:30,14/Jul/23 05:57,12/Nov/14 22:39,0.8.0,,,0.8.0,,,API / DataSet,,,,0,,,,,albermax,githubbot,hsaputra,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 12 22:39:23 UTC 2014,,,,,,,,,,"0|i22a8n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Nov/14 18:01;hsaputra;HI Stephan, could you add more information on how to reproduce this issue?;;;","12/Nov/14 18:21;sewen;The observation is based on a job posted by Maximilian Alber on the users mailing list.

I have a condensed version that produces the error as a test case. I can add that (with an option to ignore the test, as it obviously does not yet pass).;;;","12/Nov/14 20:14;sewen;This branch has a test that reproduces the error (https://github.com/StephanEwen/incubator-flink/blob/FLINK1235/flink-compiler/src/test/java/org/apache/flink/compiler/NestedIterationsTest.java)

The optimizer correctly rejects nested iterations and accepts iterations in constant paths. The JobGraphGenerator, which takes the optimized program and creates the data flow schedule does not properly handle those cases, yet.;;;","12/Nov/14 20:46;sewen;Update: The runtime does not support iteration nesting right now.

Inside iterations, you can join / cogroup / cross with data that is ""loop invariant"". We call that the ""static path"" of that operator. When that an iteration occurs in the static path of another iteration, those two are not really nested: the on in the static path has to finish before the other one can execute.

The recursive nature of the translation incorrectly identifies them as nested, though.;;;","12/Nov/14 20:50;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/198

    [FLINK-1235] Compiler accepts iterations referenced from the static path of other iterations

     - Fix NepheleJobGraphGenerator to support iterations referenced on the static path of another iteration
     - Catch nested iterations on dynamic path properly in optimizer (and give a good error message)

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink FLINK1235

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/198.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #198
    
----
commit 21b1b975ccb50e1831172894bde96c6d3269dc57
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-12T15:21:59Z

    [FLINK-1235] Compiler accepts iterations referenced from the static path of other iterations
     - Fix NepheleJobGraphGenerator to support iterations referenced on the static path of another iteration
     - Catch nested iterations on dynamic path properly in optimizer (and give a good error message)

----
;;;","12/Nov/14 22:38;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/198#issuecomment-62808541
  
    Manually merged in 21b1b975ccb50e1831172894bde96c6d3269dc57
;;;","12/Nov/14 22:38;githubbot;Github user StephanEwen closed the pull request at:

    https://github.com/apache/incubator-flink/pull/198
;;;","12/Nov/14 22:39;sewen;Fixed via 21b1b975ccb50e1831172894bde96c6d3269dc57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test AggregateITCase,FLINK-1233,12754206,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,10/Nov/14 17:43,17/Nov/14 00:37,14/Jul/23 05:57,17/Nov/14 00:37,0.8.0,,,0.8.0,,,Runtime / Coordination,,,,0,,,"The test seems to trigger a race condition in the scheduler.

My first guess is that it is the structure of the job that frees slots from the slot sharing group (when the final aggregation is running with parallelism 1) and then tries to add slots to that sharing group again from unoccupied slots.

{code}
java.lang.Exception: Cannot schedule the receivers, not enough resources
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.lookupConnectionInfoAndDeployReceivers(ExecutionGraph.java:591)
	at org.apache.flink.runtime.jobmanager.JobManager.lookupConnectionInfo(JobManager.java:558)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.flink.runtime.ipc.RPC$Server.call(RPC.java:420)
	at org.apache.flink.runtime.ipc.Server$Handler.run(Server.java:947)
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Not enough free slots available to run the job. You can decrease the operator parallelism or increase the number of slots per TaskManager in the configuration. Resources available to scheduler: Number of instances=1, total number of slots=4
	at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleTask(Scheduler.java:220)
	at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleImmediately(Scheduler.java:135)
	at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:203)
	at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:342)
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.lookupConnectionInfoAndDeployReceivers(ExecutionGraph.java:585)
{code}",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 17 00:37:35 UTC 2014,,,,,,,,,,"0|i226xj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/14 00:37;sewen;Fixed via 8a955e51959fbf3a3028496f809f89b60c4e7945;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quickstart does not work,FLINK-1225,12753719,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mbalassi,twalthr,twalthr,07/Nov/14 15:41,26/Dec/14 17:41,14/Jul/23 05:57,26/Dec/14 17:41,,,,,,,,,,,0,,,"We tried the quickstart on two PCs with different Flink versions (0.8-SNAPSHOT and 0.7), but if we submit the generated Jar-File in the web interface the error always is:

{code}
Neither a 'Main-Class', nor a 'program-class' entry was found in the jar file.
org.apache.flink.client.program.ProgramInvocationException: Neither a 'Main-Class', nor a 'program-class' entry was found in the jar file.
at org.apache.flink.client.program.PackagedProgram.getEntryPointClassNameFromJar(PackagedProgram.java:453)
	at org.apache.flink.client.program.PackagedProgram.(PackagedProgram.java:133)
{code}

Something is wrong with the pom.xml's, but I was unable to figure it out.",,githubbot,hsaputra,mbalassi,rmetzger,sewen,twalthr,,,,,,,,,,,,,,,,,,,"10/Nov/14 09:48;twalthr;quickstart-0.1-flink-fat-jar.jar;https://issues.apache.org/jira/secure/attachment/12680557/quickstart-0.1-flink-fat-jar.jar","10/Nov/14 09:48;twalthr;quickstart-0.1.jar;https://issues.apache.org/jira/secure/attachment/12680556/quickstart-0.1.jar",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 26 17:41:45 UTC 2014,,,,,,,,,,"0|i223zz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/14 16:12;sewen;I had this issue as well but thought I fixed it in https://github.com/apache/incubator-flink/commit/3f3327865ec1ca30e589e71fce40de4c1f8456db?diff=unified

Can you validate that the jar indeed has no main-class entry? (open it in an archive viewer or so...);;;","10/Nov/14 09:48;twalthr;I have tried it with the newest 0.8-SNAPSHOT but without success. I'll upload the generated JAR file to this issue. I think the problem is that the MANIFEST.MF has no {code}program-class: org.apache.flink.XYZ{code} entry.;;;","10/Nov/14 11:23;sewen;You don't need a {{program-class}} entry. A {{Main-Class}} attribute should work as well. The program looks for both attributes when it searches for the entry point.;;;","10/Nov/14 11:29;rmetzger;I tried to reproduce the issue but I couldn't. Both jar files contained a {code}Main-Class{code} entry in the MANIFEST.MF file.

However, there is another issue: If the job is in the ""org.apache.flink"" namespace, the fat jar will be empty.;;;","10/Nov/14 15:15;twalthr;Sorry, it seems that it was my fault. I forgot the ""-DarchetypeCatalog=local"" property. 

Anyway, yes the fat jar is empty and I'm unable to run the contained WordCountJob in the WebClient. I tried {code}-c org.apache.flink.WordCountJob{code}, {code}run -c org.apache.flink.WordCountJob{code} and {code}org.apache.flink.WordCountJob{code} but it always tries to execute {code}org.apache.flink.Job{code}.;;;","10/Nov/14 15:26;sewen;In the webclient, the {{-c}} option is not supported. The weblient takes the command in the form
{code}
assembler org.apache.flink.WordCountJob
{code}
Still very legacy...;;;","10/Nov/14 15:29;twalthr;Pretty, legacy. I never heard of this command :D

Thanks!;;;","10/Nov/14 15:31;twalthr;I'll open an issue for that.;;;","18/Nov/14 13:27;sewen;What is the status of this issue? Is it really a bug? Or is it that the filters in the assembly plugin excluded the user code from being packaged?;;;","19/Dec/14 15:03;rmetzger;A user reported the issue on the mailing list.
We should fix the assembly plugin filtering.;;;","19/Dec/14 19:42;rmetzger;I made this issue a blocker.

We should not release 0.8 without a fix for this.;;;","26/Dec/14 10:42;githubbot;GitHub user mbalassi opened a pull request:

    https://github.com/apache/incubator-flink/pull/279

    [FLINK-1225] Fix for quickstart packaging

    Quick fix for the two issues I've found:
      1. The user code was filtered out, so I moved it into a namespace outside org.apache.flink.
      1. The build of our 0.8 and 0.7 versions do not really work together due to the hadoop profile changes, so I have decided to bump that version. This means that the scripts depending on the 0.8.0 version will only work after the release. The first rc is hopefully coming today.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/mbalassi/incubator-flink release-0.8

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/279.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #279
    
----
commit 9fa4f37cc93687ff09d9937afe1b7b5f0c502d30
Author: mbalassi <mbalassi@apache.org>
Date:   2014-12-26T10:36:41Z

    [FLINK-1225] Fix for quickstart packaging

----
;;;","26/Dec/14 15:08;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/279#issuecomment-68144307
  
    Looks good.
    Can you open another issue once this is merged to properly fix it (users can still acidentially use the wrong namespace when creating archetypes interactively)
;;;","26/Dec/14 17:39;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/279
;;;","26/Dec/14 17:41;mbalassi;Temporal fix added to no longer block release of 0.8.0.
For a possible nicer solution added the following:
https://issues.apache.org/jira/browse/FLINK-1342;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix .getExecutionEnvironment of the StreamingExecutionEnvironment to return proper context,FLINK-1224,12753705,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,07/Nov/14 14:24,28/Feb/19 11:16,14/Jul/23 05:57,09/Nov/14 19:11,,,,0.8.0,,,,,,,0,,,"The .getExecutionEnvironment() method for the StreamingexecutionEnvironment does not work properly because it always returns LocalEnvironment for running on the minicluster. 

A fix needed to make it possible to return the proper context when executing from command line client.",,gyfora,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 09 19:11:29 UTC 2014,,,,,,,,,,"0|i223wv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/14 19:11;mbalassi;Fixed via https://git-wip-us.apache.org/repos/asf?p=incubator-flink.git;a=commit;h=34c13e978c37f0be21393002eab7e24b81081a9f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow value escaping in CSV files,FLINK-1223,12753407,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jkirsch,jkirsch,jkirsch,06/Nov/14 14:25,28/Feb/19 14:29,14/Jul/23 05:57,11/Nov/14 11:05,0.8.0,,,,,,API / Scala,,,,0,,,"The CSV Parser currently does not interpret escaped values

The example from here
http://en.wikipedia.org/wiki/Comma-separated_values#Example

{code}
Year,Make,Model,Description,Price
1997,Ford,E350,""ac, abs, moon"",3000.00
1999,Chevy,""Venture """"Extended Edition"""""","""",4900.00
{code}

Does not work currently.

Here escaping inside the string field generates an error.

For reference 

An interesting post about the fallacies that could be encountered when parsing CSV files.

[http://tburette.github.io/blog/2014/05/25/so-you-want-to-write-your-own-CSV-code/]",,githubbot,jkirsch,rmetzger,uce,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 11 11:11:05 UTC 2014,,,,,,,,,,"0|i2225b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/14 20:53;githubbot;GitHub user jkirsch opened a pull request:

    https://github.com/apache/incubator-flink/pull/187

    [FLINK-1223] Allow value escaping in CSV files.

    Strings can now contain "" quoted characters
    Skip trailing whitespace after quote

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jkirsch/incubator-flink csv

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/187.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #187
    
----
commit 1c03d901b47c8a7147fc5ed17ca36f5b1791af99
Author: Johannes <jkirschnick@gmail.com>
Date:   2014-11-06T19:19:56Z

    [FLINK-1223] Allow value escaping in CSV files.
    
    Strings can now contain "" quoted characters
    Skip trailing whitespace after quote

----
;;;","07/Nov/14 10:32;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/187#issuecomment-62124858
  
    Looks good. I have one inline comment/question, though
;;;","07/Nov/14 10:34;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/187#discussion_r20003612
  
    --- Diff: flink-core/src/main/java/org/apache/flink/types/parser/StringParser.java ---
    @@ -30,7 +34,14 @@
     	private static final byte WHITESPACE_TAB = (byte) '\t';
     	
     	private static final byte QUOTE_DOUBLE = (byte) '""';
    -	
    +
    +	private static final Set<Byte> trailingCheckSet = Sets.newHashSet(
    --- End diff --
    
    The parsers are often stressed badly when reading large CSV files, so we think a lot about performance here.
    
    I am wondering if a hash set is the best choice to check for these three elements. Computing hash, table lookup, entry lookup, comparison, ...
    
    Might be cheaper to just hardwire the check for those types...
;;;","07/Nov/14 12:07;githubbot;Github user jkirsch commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/187#discussion_r20007010
  
    --- Diff: flink-core/src/main/java/org/apache/flink/types/parser/StringParser.java ---
    @@ -30,7 +34,14 @@
     	private static final byte WHITESPACE_TAB = (byte) '\t';
     	
     	private static final byte QUOTE_DOUBLE = (byte) '""';
    -	
    +
    +	private static final Set<Byte> trailingCheckSet = Sets.newHashSet(
    --- End diff --
    
    You are right
    https://microbenchmarks.appspot.com/runs/b3ab8918-7226-4527-b019-c622a728d144
;;;","07/Nov/14 12:07;githubbot;Github user jkirsch commented on the pull request:

    https://github.com/apache/incubator-flink/pull/187#issuecomment-62133790
  
    Thanks for having a look at the code
    
    Indeed the Set approach is nicer to read but there is a performance penalty incurred
    
    I just ran a simple caliper benchmark and on my machine the set approach is about 10x slower, so I fixed that.
    
    https://microbenchmarks.appspot.com/runs/b3ab8918-7226-4527-b019-c622a728d144
    

;;;","10/Nov/14 09:45;rmetzger;Hey [~jkirsch], I gave you ""Contributor"" permissions on JIRA so that you can assign issues to yourself.;;;","10/Nov/14 09:46;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/187#issuecomment-62361647
  
    Looks good.
;;;","11/Nov/14 10:45;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/187#issuecomment-62530397
  
    OK. I'm merging this in the current batch.
;;;","11/Nov/14 11:05;uce;Fixed in e855ef4712c6d065d0580b2f58e9485b0909dbf8.;;;","11/Nov/14 11:11;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/187
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Don't use ""new Integer"". Use ""Integer.valueOf"" instead.",FLINK-1218,12753177,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sarutak,sarutak,sarutak,05/Nov/14 20:47,14/Nov/14 10:01,14/Jul/23 05:57,14/Nov/14 10:01,0.8.0,,,,,,,,,,0,,,"There are lots of ""new Integer"" or something line it in the code.
We should use valueOf to avoid wasted instance creation.",,githubbot,rmetzger,sarutak,uce,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 14 10:01:58 UTC 2014,,,,,,,,,,"0|i220r3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/14 20:48;githubbot;GitHub user sarutak opened a pull request:

    https://github.com/apache/incubator-flink/pull/183

    [FLINK-1218] Don't use ""new Integer"". Use ""Integer.valueOf"" instead.

    There are lots of ""new Integer"" or something line it in the code.
    We should use valueOf to avoid wasted instance creation.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sarutak/incubator-flink FLINK-1218

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/183.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #183
    
----
commit eb5d4f8b77662a00ac17670d5218fcc7444ace44
Author: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
Date:   2014-11-05T20:46:11Z

    Fixed wasted instance creation

----
;;;","06/Nov/14 19:24;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/183#discussion_r19967614
  
    --- Diff: flink-examples/flink-java-examples/src/main/java/org/apache/flink/examples/java/graph/TransitiveClosureNaive.java ---
    @@ -63,8 +63,8 @@ public static void main (String... args) throws Exception{
     					 */
     					public Tuple2<Long, Long> join(Tuple2<Long, Long> left, Tuple2<Long, Long> right) throws Exception {
     						return new Tuple2<Long, Long>(
    -								new Long(left.f0),
    -								new Long(right.f1));
    +								Long.valueOf(left.f0),
    +								Long.valueOf(right.f1));
    --- End diff --
    
    It would be better to do this via
    `return new Tuple2<Long, Long>(left.f0, right.f1);`
    
    That avoids creating new boxed Longs entirely, just reusing the previous ones.
;;;","06/Nov/14 19:26;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/183#issuecomment-62036117
  
    Virtually all cases are tests, where it makes no matter. One case can be entirely avoided.
    
    I think skipping the calls entirely and just letting the autoboxing do its magic (it uses Integer.valueOf() as well) would be even nicer.
    
    All in all the fix is fine, though.
    

;;;","10/Nov/14 09:50;rmetzger;Hey [~sarutak], I gave you ""Contributor"" perms in JIRA so that you can assign yourself to issues.;;;","10/Nov/14 09:51;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/183#issuecomment-62362208
  
    Looks good to merge
;;;","13/Nov/14 14:55;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/183#issuecomment-62901438
  
    I agree with Stephan, but since the fix is fine, I will merge it later today.
;;;","13/Nov/14 18:39;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/183
;;;","14/Nov/14 10:01;uce;Fixed in c339c266f3c40987b1251a70b1c4a1d4a62001c1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSystem.initOutPathLocalFS can lead to race condition,FLINK-1215,12753029,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,aljoscha,aljoscha,05/Nov/14 12:51,07/Nov/14 10:17,14/Jul/23 05:57,07/Nov/14 10:16,,,,0.8.0,,,Runtime / Task,,,,0,,,"This can fail when concurrent tasks notice about deleting/creating the output directory.

Seen in a travis log:

Failed tests: 
  LocalExecutorITCase.testLocalExecutorWithWordCount:61 java.io.IOException: Output directory 'file:/tmp/wctext7874076083387578604.out' could not be created. Canceling task...
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:232)
	at org.apache.flink.api.java.record.io.CsvOutputFormat.open(CsvOutputFormat.java:261)
	at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:172)
	at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:245)
	at java.lang.Thread.run(Thread.java:745)",,aljoscha,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 07 10:17:04 UTC 2014,,,,,,,,,,"0|i21zv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/14 19:39;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/186

    [FLINK-1215] Fix spurious failures when creating output directories due to I/O races

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink FLINK1215

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/186.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #186
    
----
commit ef406916dbeabaef79b4ffd38fe5916cdd34bd2f
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-06T14:14:12Z

    [FLINK-1222] Tasks send close acknowledgements early.

commit ef9a37390dfbd325b3bf2422334f99d22fca2a1a
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-06T18:14:13Z

    [runtime] In local mode, make sure taskmanagers have completed registration before starting a job.

commit e58049711e4275d86197223b7efcb47d2f801244
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-06T19:33:38Z

    [FLINK-1215] Fix spurious failures when creating output directories due to I/O races

----
;;;","07/Nov/14 10:16;sewen;Fixed in e58049711e4275d86197223b7efcb47d2f801244;;;","07/Nov/14 10:17;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/186#issuecomment-62123220
  
    Manually merged.
;;;","07/Nov/14 10:17;githubbot;Github user StephanEwen closed the pull request at:

    https://github.com/apache/incubator-flink/pull/186
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent partitioning pushdown unless partitions fields match exactly,FLINK-1214,12753020,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,05/Nov/14 11:44,28/Feb/19 14:30,14/Jul/23 05:57,06/Nov/14 13:02,0.8.0,,,0.8.0,,,API / DataSet,,,,0,,,"Consider an operation grouped on fields (A, B), followed by an operation grouped on field (A).

Right now, the optimizer can push down the partitioning on (A), which serves both operations (the first step locally still groups by A and B). This may however by a bad idea for the cases where the field A has a low cardinality, or the value distribution is skewed.

Since we cannot determine that robustly yet, I suggest to disable this optimization for now.",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 06 13:02:24 UTC 2014,,,,,,,,,,"0|i21zt3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/14 13:00;sewen;I am preparing a fix for this;;;","06/Nov/14 13:02;sewen;Fixed in 6ecd0f8264ab5cfb6101046a415c996993b682e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Interpreter directive in shell scripts should be ""/usr/bin/env bash"" instead of ""/bin/bash""",FLINK-1212,12753010,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fhueske,sarutak,sarutak,05/Nov/14 10:15,10/Nov/14 16:11,14/Jul/23 05:57,10/Nov/14 15:33,0.8.0,,,0.8.0,,,,,,,0,,,"There are some scripts which interpreter directive are {code}#!/bin/bash{code}. We should use {code}#!/usr/bin/env bash""{code}instead.",,githubbot,sarutak,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 10 16:11:53 UTC 2014,,,,,,,,,,"0|i21zqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/14 10:16;githubbot;GitHub user sarutak opened a pull request:

    https://github.com/apache/incubator-flink/pull/180

    [FLINK-1212] Interpreter directive in shell scripts should be ""/usr/bin/env bash"" instead of ""/bin/bash""

    There are some scripts which interpreter directive are ""#!/bin/bash"". We should use ""#!/usr/bin/env bash"" instead.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sarutak/incubator-flink sh-directive-modification

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/180.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #180
    
----
commit 5a80b025f85bf3e9a71a50fe94eaa14a77a1b92a
Author: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
Date:   2014-11-05T10:13:03Z

    Fixed shell script interpreter directive

----
;;;","05/Nov/14 10:23;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/180#issuecomment-61786792
  
    Looks good.
;;;","07/Nov/14 10:22;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/180#issuecomment-62123798
  
    Will merge this one...
;;;","10/Nov/14 10:11;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/180#issuecomment-62364307
  
    Can you close the pull request manually? It seems like GitHub is unable to detect the close comments.
;;;","10/Nov/14 10:12;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/180#issuecomment-62364389
  
    Ah. The issue hasn't been merged to Flink. @StephanEwen referenced to it from his ""incubator-flink"" branch.
;;;","10/Nov/14 15:33;sewen;Merged and fixed in 90730fe31ee3528e3258ef5d47c68161ed6cee90;;;","10/Nov/14 16:11;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/180
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOL character of Windows Batch files should be CRLF,FLINK-1211,12753008,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sarutak,sarutak,05/Nov/14 10:04,10/Nov/14 16:11,14/Jul/23 05:57,10/Nov/14 15:32,0.8.0,,,0.8.0,,,,,,,0,,,"Flink has some Windows Batch files. Windows expects the EOL character of Batch file is CRLF, not LF.",,githubbot,sarutak,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 10 16:11:53 UTC 2014,,,,,,,,,,"0|i21zqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/14 10:04;githubbot;GitHub user sarutak opened a pull request:

    https://github.com/apache/incubator-flink/pull/179

    [FLINK-1211] EOL character of Windows Batch files should be CRLF

    Flink has some Windows Batch files. Windows expects the EOL character of Batch file is CRLF, not LF.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sarutak/incubator-flink bat-eol-modification

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/179.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #179
    
----
commit f69e710e0b1acb62053eeed6c38115b067c70ccb
Author: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
Date:   2014-11-05T09:59:38Z

    Fixed EOL character of Windows Batch files

----
;;;","05/Nov/14 10:12;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/179#issuecomment-61785588
  
    Makes sense and looks good to merge...
;;;","05/Nov/14 19:52;githubbot;Github user sarutak commented on the pull request:

    https://github.com/apache/incubator-flink/pull/179#issuecomment-61870113
  
    I don't think, the failure is not related to my changes. How can I retest?
;;;","05/Nov/14 20:05;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/179#issuecomment-61872206
  
    This looks like a spurious failure in the file creation.
    
    I would consider this build as successful, given that the other builds all went well.
;;;","07/Nov/14 10:27;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/179#issuecomment-62124359
  
    Will merge this one.
    
    Thank you for the contribution!
;;;","07/Nov/14 13:58;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/179#issuecomment-62147923
  
    This pull request seems impossible to merge in combination with #181 
    
    One introduces the line end normalization, the other one does it manually. I am getting weird conflicts that cancel this commit out when resolved
;;;","07/Nov/14 18:23;githubbot;Github user sarutak commented on the pull request:

    https://github.com/apache/incubator-flink/pull/179#issuecomment-62189105
  
    @StephanEwen I merged #181 into this PR. If this changes look good, I'll close #181. Thanks.
;;;","10/Nov/14 15:32;sewen;Fixed in 4b75d83ebc5f1217c5280c4a73127f88e9acdae8;;;","10/Nov/14 16:11;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/179
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Error Message in Delta Iteratione when Next Workset does not Depend on Workset.,FLINK-1210,12752722,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,04/Nov/14 14:15,28/Feb/19 14:30,14/Jul/23 05:57,04/Nov/14 15:44,0.7.0-incubating,0.8.0,,0.8.0,,,API / DataSet,,,,0,,,"Currently, the job fails with a NullPointerException in the NepheleJobGraphGenerator",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 04 15:44:24 UTC 2014,,,,,,,,,,"0|i21xzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/14 15:44;sewen;Fixed via 233161b25eecebb94301f4a1ff06de94940b805b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forgetting to close an iteration leads to a confusing error message,FLINK-1209,12752688,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,04/Nov/14 12:20,28/Feb/19 14:29,14/Jul/23 05:57,18/Nov/14 16:29,0.7.0-incubating,0.8.0,,0.7.1-incubating,0.8.0,,,,,,0,,,"The rror message you get is ""Unknown operator - SolutionSetPlaceholder / WorksetPlaceholder / PartialSolutionPlaceholder""",,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 16:35:22 UTC 2014,,,,,,,,,,"0|i21xsf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/14 13:33;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/212

    [FLINK-1209] & [FLINK-1250] Fixes for error messages and config keys

    [FLINK-1209] Improve error messages when forgetting to close an iteration
    [FLINK-1250] Correct and document config keys for heartbeat intervals and timeouts

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink checks

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/212.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #212
    
----
commit 83d02563ea4a1c7d05540849bf3bf033d968b021
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-17T18:04:13Z

    [FLINK-1207] Context environments are realized through factories
      - local execution blocking is reset after each run

commit 2000b45ce3e71ed6eddecbb3f8658ebecec58230
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-13T15:26:07Z

    [FLINK-1237] Add support for custom partitioners
      - Functions: GroupReduce, Reduce, Aggregate on UnsortedGrouping, SortedGrouping,
                   Join (Java API & Scala API)
      - Manual partition on DataSet (Java API & Scala API)
      - Distinct operations provide semantic properties for preservation of distinctified fields
      - Tests for pushown (or not pushdown) of custom partitionings and forced rebalancing
      - Tests for GlobalProperties matching of partitionings
      - Caching of generated requested data properties for unary operators
    
    This closes #207

commit 838b2c80785ecb9803852f253e9ff82933bb6c07
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-18T13:08:33Z

    [FLINK-1209] Improve error messages when forgetting to close an iteration

commit c6206d3d845f241ac8c6d757f9bf419b6feb6720
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-18T13:24:57Z

    [FLINK-1250] Correct and document config keys for heartbeat intervals and timeouts

----
;;;","18/Nov/14 13:34;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/212#issuecomment-63471105
  
    Since github is out of sync with the apache git, this PR appears to have four commits. Only the later two are relevant.
;;;","18/Nov/14 16:29;sewen;Fixed via b264221b32ef2b1a4abe3d40c3174fccea21974f;;;","18/Nov/14 16:35;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/212
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switch ContextEnvironment to Environment factory,FLINK-1207,12752545,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,03/Nov/14 22:23,28/Feb/19 14:29,14/Jul/23 05:57,18/Nov/14 11:05,0.8.0,,,0.8.0,,,,,,,0,,,"We implement the ""context dependent switching"" of the execution environments (cluster / local / test) with static variables in the ExecutionEnvironment.

That means that these environments are potentially shared between multiple threads that run programs (also in case where they run one after the other).

This may lead to exceptions, as we sometimes see in the tests, when using forked test execution: The later test in the same JVM may access the same environment object as the prior ones. In particular, we see that half finished programs may still be associated with the execution environment, such that mixes between programs occur, producing hard to understand cast exceptions (see trace below)

This is so far only relevant to tests with forked execution, but may become relevant to users that build different programs at the same time.

I propose to change the static members from environments to environment factories. That way, we can switch type of environment depending on the context as before, and we guarantee that each call to ""ExecutionEnvironment.getEnvironment()"" returns a dedicated and fresh environment.


Running org.apache.flink.api.scala.operators.translation.DistinctTranslationTest
java.lang.ClassCastException: org.apache.flink.api.common.operators.base.DeltaIterationBase cannot be cast to org.apache.flink.api.common.operators.base.GroupReduceOperatorBase
	at org.apache.flink.api.scala.operators.translation.DistinctTranslationTest.testCombinable(DistinctTranslationTest.scala:39)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.027 sec <<< FAILURE! - in org.apache.flink.api.scala.operators.translation.DistinctTranslationTest
testCombinable(org.apache.flink.api.scala.operators.translation.DistinctTranslationTest)  Time elapsed: 0.024 sec  <<< FAILURE!
java.lang.AssertionError: org.apache.flink.api.common.operators.base.DeltaIterationBase cannot be cast to org.apache.flink.api.common.operators.base.GroupReduceOperatorBase
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.api.scala.operators.translation.DistinctTranslationTest.testCombinable(DistinctTranslationTest.scala:46)",,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 13:53:12 UTC 2014,,,,,,,,,,"0|i21wx3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/14 21:17;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/211

    [FLINK-1207] Context environments are realized through factories

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink factories

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/211.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #211
    
----
commit 06c259f641ffcdb4e524370f85c4c9b12dcdfc28
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-17T20:55:51Z

    [tests] Various stability fixes to tests

commit ae07abe3781eb583774eab917953f31cf4925cd9
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-17T21:07:09Z

    [build] Manage version of joda-time to prevent conflicts between dependencies.

commit 83d02563ea4a1c7d05540849bf3bf033d968b021
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-17T18:04:13Z

    [FLINK-1207] Context environments are realized through factories
      - local execution blocking is reset after each run

----
;;;","18/Nov/14 10:42;githubbot;Github user uce commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/211#discussion_r20497561
  
    --- Diff: flink-runtime/src/test/java/org/apache/flink/runtime/execution/librarycache/BlobLibraryCacheManagerTest.java ---
    @@ -73,7 +73,19 @@ public void testLibraryCacheManagerCleanup(){
     
     			libraryCacheManager.unregisterJob(jid);
     
    -			Thread.sleep(1500);
    +			// because we cannot guarantee that there are not thread races in the build system, we
    +			// loop for a certain while until the references disappear
    +			{
    +				long deadline = System.currentTimeMillis() + 30000;
    +				do {
    +					Thread.sleep(500);
    +				}
    +				while (libraryCacheManager.getNumberOfCachedLibraries() > 0 && 
    +						System.currentTimeMillis() < deadline);
    +			}
    +			
    +			// this fails if we exited via a timeout
    +			assertEquals(0, libraryCacheManager.getNumberOfCachedLibraries());
    --- End diff --
    
    minor: I would add an error message, so we don't panic when this test fails and have an idea of what happened ;)
;;;","18/Nov/14 10:48;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/211#issuecomment-63452309
  
    Looks good to me :) +1
;;;","18/Nov/14 11:05;sewen;Merged in 83d02563ea4a1c7d05540849bf3bf033d968b021;;;","18/Nov/14 11:28;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/211#issuecomment-63456556
  
    Merging this.
;;;","18/Nov/14 13:53;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/211
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add option to the web client to specify default parallelism for a program,FLINK-1206,12752517,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,qmlmoon,sewen,sewen,03/Nov/14 20:31,18/Jan/15 19:04,14/Jul/23 05:57,18/Jan/15 19:04,0.8.0,,,0.9,,,Runtime / Web Frontend,,,,0,starter,,,,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 18 19:04:08 UTC 2015,,,,,,,,,,"0|i21wr3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/15 19:04;sewen;Fixed via d3072ba57877a95001d6c210ec0f156b478619fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LibraryCacheManager should track execution IDs of library registrations,FLINK-1205,12752484,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,03/Nov/14 18:40,04/Nov/14 15:45,14/Jul/23 05:57,04/Nov/14 15:45,0.8.0,,,0.8.0,,,Runtime / Coordination,,,,0,,,"Currently, it is hard to guarantee that on a failed execution attempt, the libraries are de-registered exactly once. Only references per JobId are tracked. If we track the registered ExecutionAttempts, we can make sure we de-register only if we have not yet de-registered that execution attempt.",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 04 15:45:22 UTC 2014,,,,,,,,,,"0|i21wkn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/14 15:45;sewen;Fixed via a6152c372b86d3a62745b3703975d5d4eb243053;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix streaming example builds to provide self-contained jars,FLINK-1204,12752449,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,mbalassi,mbalassi,03/Nov/14 15:54,28/Feb/19 11:16,14/Jul/23 05:57,13/Nov/14 15:08,0.7.1-incubating,,,,,,,,,,0,,,The examples depend on the example data provided by the batch examples however it is not packaged with them. Individual example jars should be included in the examples directory of the distribution.,,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 13 15:08:42 UTC 2014,,,,,,,,,,"0|i21wcv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/14 15:08;mbalassi;Fixed via https://git-wip-us.apache.org/repos/asf?p=incubator-flink.git;a=commit;h=c6dd9b104dad2fdf35d3bbca27779060792dc877;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCast exceptions in parallel tests (Surefire Bug?),FLINK-1203,12752401,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,03/Nov/14 11:24,03/Nov/14 16:07,14/Jul/23 05:57,03/Nov/14 16:07,0.8.0,,,0.8.0,,,Build System,,,,0,,,"I am frequently seeing weird (non-deterministic) class cast exception in the tests, where apparently casts fail in serial parts of the program. The casting attempts have nothing to do with the program context.

{code}
java.lang.ClassCastException: org.apache.flink.api.common.operators.base.DeltaIterationBase cannot be cast to org.apache.flink.api.common.operators.base.GroupReduceOperatorBase
	at org.apache.flink.api.scala.operators.translation.DistinctTranslationTest.testCombinable(DistinctTranslationTest.scala:39)
{code}

I am wondering whether that might be a strange bug in the forked execution, a possible bug in maven surefire.

I propose to deactivate the ""reuseFork"" option in surefire, to create a clean JVM (and class loaders) for each test.",,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 03 16:07:43 UTC 2014,,,,,,,,,,"0|i21w27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/14 11:33;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/174

    [FLINK-1203] Deactivate fork reuse in tests ()

    Works around a potential surefire bug where we see confused class cast exceptions
    
    ```
    java.lang.ClassCastException: org.apache.flink.api.common.operators.base.DeltaIterationBase cannot be cast to org.apache.flink.api.common.operators.base.GroupReduceOperatorBase
    	at org.apache.flink.api.scala.operators.translation.DistinctTranslationTest.testCombinable(DistinctTranslationTest.scala:39)
    ```
    


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink fork_false

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/174.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #174
    
----
commit 21d3bdbc8d61e1d2712fe396d3fed4258f5c3b8b
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-03T11:27:03Z

    [FLINK-1203] Deactivate fork reuse in tests (workaround for potential surefire bug)

----
;;;","03/Nov/14 16:06;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/174
;;;","03/Nov/14 16:07;sewen;Fixed via f42dcc3573dd595c4ee11f089f7ea5d68905ebca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed file outputs should remove partially complete files,FLINK-1202,12752396,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,03/Nov/14 10:47,03/Nov/14 16:08,14/Jul/23 05:57,03/Nov/14 16:08,0.8.0,,,0.8.0,,,Runtime / Task,,,,0,,,"Without this, fault tolerance retries may fail because files already exist.",,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 03 16:08:14 UTC 2014,,,,,,,,,,"0|i21w13:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/14 11:35;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/175

    [FLINK-1202] Remove partial files when file outputs fail (or are canceled)

    Builds upon #174 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink file_cleanup

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/175.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #175
    
----
commit 21d3bdbc8d61e1d2712fe396d3fed4258f5c3b8b
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-03T11:27:03Z

    [FLINK-1203] Deactivate fork reuse in tests (workaround for potential surefire bug)

commit 25e27daf66b029a686db5a7ad52d9842848129d9
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-11-03T10:47:51Z

    [FLINK-1202] Remove incomplete file outputs on failure

----
;;;","03/Nov/14 16:06;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/175
;;;","03/Nov/14 16:08;sewen;Fixed via a747b6146ed5d5766b42e6bed3c2e7a811e8d00e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct  references to dependencies in docs,FLINK-1199,12751502,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,mbalassi,mbalassi,29/Oct/14 20:45,13/Apr/21 20:39,14/Jul/23 05:57,30/Oct/14 01:17,0.8.0,,,0.8.0,,,Documentation,,,,0,,,The documentation contains obsolete 0.x-incubating style dependency references as reported on the mailing list. Should update the environment variable responsible for that and double check for possible occurrences.,,mbalassi,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 30 01:17:54 UTC 2014,,,,,,,,,,"0|i21qmn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/14 23:15;sewen;fixed on the website.

The markdown files / config files are still invalid;;;","30/Oct/14 01:15;sewen;Remainder is fixed and website is updated;;;","30/Oct/14 01:17;sewen;Fixed in 36a8fdad106c0851a084a2446ed813d424b66b91;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong manifest entries and compiler configuration in Java Quickstarts,FLINK-1194,12750752,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,27/Oct/14 13:53,28/Oct/14 14:02,14/Jul/23 05:57,28/Oct/14 14:02,0.7.0-incubating,,,0.8.0,,,Build System,,,,0,,,"The quickstart archetype for java:

  - sets wrong main class attribute in jar
  - sets wrong main class attribute in fat jar
  - is unnecessarily compilcated to enable for java 8
  - unnecessarily requires an extra m2e connector in eclipse

Pull request is pending",,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 14:02:59 UTC 2014,,,,,,,,,,"0|i21m1z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/14 13:58;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/165

    [FLINK-1194] Fix Java quickstart archetype POM file

    Fixes the quickstart archetype for java:
      - sets correct main class attribute in jar
      - sets correct main class attribute in fat jar
      - no special steps required any more to enable it for eclipse


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink quickstart

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/165.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #165
    
----
commit 6b7be22cfc51893dc5e6d45ea52eb5e70f39403f
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-10-27T13:53:49Z

    [FLINK-1194] Fix Java quickstart archetype POM file

----
;;;","28/Oct/14 13:50;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/165
;;;","28/Oct/14 14:02;sewen;Fixed via 3f3327865ec1ca30e589e71fce40de4c1f8456db;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"InvalidTypesException for Array[Array[Tuple2[Int, Double]]]",FLINK-1193,12750749,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,trohrmann,trohrmann,27/Oct/14 13:38,03/Feb/15 12:26,14/Jul/23 05:57,03/Feb/15 12:25,,,,0.9,,,,,,,0,,,"The TypeExtractor cannot handle nested arrays with non-trivial element types. For example, the following code cannot be run
{code}
val input = env.fromCollection(List(1,2,3,4))
val mapped = input.map{
  id => {
    val pair = (1,1.0)
    val a = Array(pair)
    val b = Array(a)
    b
  }
}
{code}

The resulting type Array[Array[Tuple2[Int, Double]]] cannot be extracted. It would be nice, if Flink supported these types.",,githubbot,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 03 12:26:00 UTC 2015,,,,,,,,,,"0|i21m1b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/15 12:57;githubbot;GitHub user twalthr opened a pull request:

    https://github.com/apache/flink/pull/348

    [FLINK-1193][java-api][scala-api] Complete support for multidimensional arrays

    This PR introduces official support for multidimensional arrays. 
    
    It contains bug fixes for TypeExtractor and Serializer (see [FLINK-1193]) as well as adds tests for Java and Scala API.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/twalthr/flink MultidimensionalArrays

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/348.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #348
    
----
commit 9b66b6a56ceeb948f1cb6b6045dd9063ad33016f
Author: twalthr <twalthr@apache.org>
Date:   2015-01-27T13:07:12Z

    [FLINK-1193][java-api][scala-api] Complete support for multidimensional arrays

----
;;;","29/Jan/15 15:05;githubbot;Github user aljoscha commented on a diff in the pull request:

    https://github.com/apache/flink/pull/348#discussion_r23773125
  
    --- Diff: flink-tests/src/test/scala/org/apache/flink/api/scala/types/TypeInformationGenTest.scala ---
    @@ -255,6 +255,53 @@ class TypeInformationGenTest {
         Assert.assertEquals(BasicTypeInfo.STRING_TYPE_INFO, tti.getTypeAt(0))
         Assert.assertEquals(BasicTypeInfo.STRING_TYPE_INFO, tti.getTypeAt(1))
       }
    +  
    +  @Test
    +  def testMultidimensionalArrays(): Unit = {
    +    // Tuple
    +    {
    +      val ti = createTypeInformation[Array[Array[(String, String)]]]
    +    
    +      Assert.assertTrue(ti.isInstanceOf[ObjectArrayTypeInfo[_, _]])
    +      val oati = ti.asInstanceOf[ObjectArrayTypeInfo[_, _]]
    +      Assert.assertTrue(oati.getComponentInfo.isInstanceOf[ObjectArrayTypeInfo[_, _]])
    +      val oati2 = oati.getComponentInfo.asInstanceOf[ObjectArrayTypeInfo[_, _]]
    +      Assert.assertTrue(oati2.getComponentInfo.isTupleType)
    +      val tti = oati2.getComponentInfo.asInstanceOf[TupleTypeInfoBase[_]]
    +      Assert.assertEquals(BasicTypeInfo.STRING_TYPE_INFO, tti.getTypeAt(0))
    +      Assert.assertEquals(BasicTypeInfo.STRING_TYPE_INFO, tti.getTypeAt(1))
    +    }
    +    
    +    // primitives
    +    {
    +      val ti = createTypeInformation[Array[Array[Int]]]
    +    
    +      Assert.assertTrue(ti.isInstanceOf[ObjectArrayTypeInfo[_, _]])
    +      val oati = ti.asInstanceOf[ObjectArrayTypeInfo[_, _]]
    +      Assert.assertEquals(oati.getComponentInfo, PrimitiveArrayTypeInfo.INT_PRIMITIVE_ARRAY_TYPE_INFO)
    --- End diff --
    
    This line is too long. That's why mvn verify fails.
;;;","03/Feb/15 10:41;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/flink/pull/348#discussion_r23995382
  
    --- Diff: flink-core/src/main/java/org/apache/flink/api/common/typeutils/base/GenericArraySerializer.java ---
    @@ -72,7 +72,9 @@ public boolean isStateful() {
     		C[] copy = create(from.length);
     
     		for (int i = 0; i < copy.length; i++) {
    -			copy[i] = this.componentSerializer.copy(from[i], this.componentSerializer.createInstance());
    +			if (from[i] != null) {
    +				copy[i] = this.componentSerializer.copy(from[i], this.componentSerializer.createInstance());
    --- End diff --
    
    This should not use the method with re-usable instance.
;;;","03/Feb/15 10:46;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/348#issuecomment-72630115
  
    I will merge this, fixing the comment...
;;;","03/Feb/15 12:23;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/flink/pull/348
;;;","03/Feb/15 12:26;sewen;Fixed via 57b3e9f9e50705033200dcca0762a5a65d1aba60;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TypeAnalyzer MatchError when analyzing BitSet type with Scala API,FLINK-1191,12750728,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,trohrmann,trohrmann,27/Oct/14 11:10,05/Nov/14 13:19,14/Jul/23 05:57,05/Nov/14 13:19,,,,,,,,,,,0,,,"The type analyzer of the ScalaAPI treats BitSet as a ListType. When the unapply method of the ListType is called with a BitSet type then it will be considered a TraversableType. However, within the TraversableType unapply method there is an non-exhaustive pattern matching which fails with BitSet. There is also a comment which says ""TODO: make sure this works as it should"". As a result, the user only sees that some implicit evidence parameters of type TypeInformation are missing, because they weren't generated. From the user perspective, it would be nice to receive a more meaningful error message.",,aljoscha,githubbot,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 05 13:19:39 UTC 2014,,,,,,,,,,"0|i21lwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/14 15:59;aljoscha;Yes, I was aware of the problem. Some special Scala types, such as Option and Either are also not usable.

I have a branch where I work on addressing these issues: https://github.com/aljoscha/incubator-flink/tree/scala-collections-support.;;;","04/Nov/14 14:11;githubbot;GitHub user aljoscha opened a pull request:

    https://github.com/apache/incubator-flink/pull/177

    [FLINK-1191] Add support for Scala Collections and Special Types

    ""The special types"" are Option and Either. This should work for all
    Scala collections except SortedSet and SortedMap, for which the type
    checker prints an error message.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/aljoscha/incubator-flink scala-collections-support

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/177.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #177
    
----
commit c123937642983d6d9f2a049ae634f7cb9bb04779
Author: Aljoscha Krettek <aljoscha.krettek@gmail.com>
Date:   2014-10-22T09:35:15Z

    [scala] Add macroparadise to enable Quasiquotes

commit d5803112bd95a62b1764e1afb4e0adfdda4d0ee3
Author: Aljoscha Krettek <aljoscha.krettek@gmail.com>
Date:   2014-11-04T13:59:57Z

    [FLINK-1191] Add support for Scala Collections and Special Types
    
    ""The special types"" are Option and Either. This should work for all
    Scala collections except SortedSet and SortedMap, for which the type
    checker prints an error message.

----
;;;","05/Nov/14 10:02;githubbot;Github user aljoscha commented on the pull request:

    https://github.com/apache/incubator-flink/pull/177#issuecomment-61784331
  
    Any comments on this? I would really like to get this in soon.
;;;","05/Nov/14 10:20;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/177#issuecomment-61786464
  
    Looks like a very nice piece of work.
    Tests looks good.
    
    Missing is only an entry of the new dependencies to the LICENSE, NOTICE, and DEPENDENCIES file(s).
;;;","05/Nov/14 13:19;aljoscha;Resolved in https://github.com/aljoscha/incubator-flink/commit/0c9a1488f2aeb97f895038e314758bce16869043;;;","05/Nov/14 13:19;githubbot;Github user aljoscha commented on the pull request:

    https://github.com/apache/incubator-flink/pull/177#issuecomment-61805253
  
    Manually merged.
;;;","05/Nov/14 13:19;githubbot;Github user aljoscha closed the pull request at:

    https://github.com/apache/incubator-flink/pull/177
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join key selection for nested Tuples is broken,FLINK-1186,12750715,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,qmlmoon,rwaury,rwaury,27/Oct/14 10:01,28/Feb/19 14:29,14/Jul/23 05:57,11/Nov/14 09:06,0.7.0-incubating,,,0.7.1-incubating,0.8.0,,,,,,0,,,"When using strings to select join keys of nested tuples (e.g. ""f0.f1"") an incorrect join key is chosen and the operator produces an erroneous result.

The join works fine when using KeySelectors.

I will attach a program to reproduce the bug.",LocalExecutionEnvironment,qmlmoon,rmetzger,rwaury,,,,,,,,,,,,,,,,,,,,,,"27/Oct/14 10:02;rwaury;KeySelectionBug.java;https://issues.apache.org/jira/secure/attachment/12677280/KeySelectionBug.java",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 11 09:06:17 UTC 2014,,,,,,,,,,"0|i21ltr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/14 16:45;rmetzger;I'm trying to look into the issue in the next few days. (But I'm not assigning it to myself since I can not guarantee that I have enough time);;;","28/Oct/14 14:52;qmlmoon;I would like to take over it if you have not started. ;;;","28/Oct/14 14:55;rmetzger;Yes, sure. I have not found time yet.
;;;","11/Nov/14 09:06;rmetzger;Fix pushed in http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/8e4c772a

Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad Error Message when trying to use expression keys on non-POJO types,FLINK-1182,12750610,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,26/Oct/14 16:24,28/Feb/19 14:29,14/Jul/23 05:57,28/Oct/14 16:57,0.7.0-incubating,,,0.8.0,,,API / Scala,,,,0,,,"It says ""type is not composite (but is) and goes on talking about POJO types."" I think that should be fixed.

{code}
Type GenericType<flink.tutorial.pagerank.PageRank.Page> is not a composite type. Key expressions are only supported on POJO types and Tuples. A type is considered a POJO if all its fields are public, or have both getters and setters defined
	at org.apache.flink.api.java.operators.Keys$ExpressionKeys.<init>(Keys.java:249)

{code}",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 16:57:28 UTC 2014,,,,,,,,,,"0|i21l6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/14 16:57;sewen;Fixed in b0c1af59bf139a369aac5a0680c81a6fc08acccc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI actions list -r/-s don't work,FLINK-1181,12750363,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,vkalavri,vkalavri,24/Oct/14 14:45,28/Oct/14 17:00,14/Jul/23 05:57,28/Oct/14 17:00,,,,0.8.0,,,,,,,0,,,"The actions {{list -r}} and {{list -s}} of the CLI Frontend hang. 
The following exception is found in the JobManager's logs:
ERROR org.apache.flink.runtime.ipc.Server
java.io.IOException: java.io.IOException: The return type of method getRecentJobs is not a primitive type (or boxed primitive) and not of type IOReadableWriteable
        at org.apache.flink.runtime.ipc.RPC$Server.call(RPC.java:428)
        at org.apache.flink.runtime.ipc.Server$Handler.run(Server.java:947)",,githubbot,sewen,vkalavri,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 17:00:14 UTC 2014,,,,,,,,,,"0|i21jov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/14 13:39;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/167

    Fixes for [FLINK-1181] and [FLINK-1182]

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink fixes

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/167.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #167
    
----
commit 05d821e43aa74730a8de9f9e384aa8d6f49cb3eb
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-10-27T23:59:53Z

    [FLINK-1182] Improve error messages for POJO type mismatches

commit 1db0a987c6becca16a5035342d81017cea09bd3b
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-10-28T00:47:08Z

    [FLINK-1181] Fix IOReadableWritable checks in RPC service

----
;;;","28/Oct/14 16:49;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/167
;;;","28/Oct/14 17:00;sewen;Fixed via 0ee60d3242fd3c56e79a2e0173f20365d7004882;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Links to APIs inside Iterations guide broken,FLINK-1172,12749087,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,rmetzger,rmetzger,19/Oct/14 08:47,17/Nov/14 16:37,14/Jul/23 05:57,17/Nov/14 16:09,,,,0.8.0,,,Documentation,,,,0,,,"Here: http://flink.incubator.apache.org/docs/0.7-incubating/iterations.html
the links to the Java and Scala APIs are broken.",,githubbot,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 17 16:37:09 UTC 2014,,,,,,,,,,"0|i21bvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/14 16:51;githubbot;GitHub user zentol opened a pull request:

    https://github.com/apache/incubator-flink/pull/206

    [FLINK-1172] Fix broken links in documentation

    i need someone to check the actual .html files produced since i can't get the build_docs.sh script to run properly, one cryptic error message after the next...

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/zentol/incubator-flink guide_api_links

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/206.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #206
    
----
commit a501560e4670efcfb51893548e518a019b45e073
Author: zentol <s.motsu@web.de>
Date:   2014-11-16T15:57:02Z

    [FLINK-1172] Fix broken links in documentation

----
;;;","16/Nov/14 21:29;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/206#issuecomment-63240383
  
    Good catch, thanks! Looks good to merge...
;;;","17/Nov/14 14:50;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/206#issuecomment-63314943
  
    I verified the links (all good), and am merging this...
;;;","17/Nov/14 16:09;sewen;Merged in 9c1585eefd4bc621e1fe9fbd9cb3055404b0dff1;;;","17/Nov/14 16:37;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/206
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move scala tests to flink-tests project,FLINK-1171,12748899,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,17/Oct/14 14:46,28/Oct/14 17:02,14/Jul/23 05:57,28/Oct/14 17:02,,,,0.7.0-incubating,,,,,,,0,,,"Eclipse does not manage to make the macros defined in {{src/main/scala}} available to {{src/test/scala}} - making it impossible to develop the scala project in Eclipse.

Moving the tests to a different project (here: {{flink-tests/src/test/scala}}) solves the issue.

See mailing list archive for discussion: http://mail-archives.apache.org/mod_mbox/flink-dev/201410.mbox/browser",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 17:02:10 UTC 2014,,,,,,,,,,"0|i21aqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/14 17:02;sewen;Fixed via a0ad90314f456f51e7889a3d0a77c279edf39f2d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Localization of InputSplits is not working properly,FLINK-1170,12748855,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,17/Oct/14 09:25,18/Oct/14 15:14,14/Jul/23 05:57,18/Oct/14 15:14,,,,0.7.0-incubating,,,Runtime / Coordination,,,,0,,,"While running some benchmarks, I found that Flink is not properly assigning the InputSplits.

On my testing cluster, ALL splits were assigned to remote HDFS DataNodes, which causes a lot of network I/O.",,githubbot,hsaputra,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 18 15:14:39 UTC 2014,,,,,,,,,,"0|i21agn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/14 13:52;githubbot;GitHub user rmetzger opened a pull request:

    https://github.com/apache/incubator-flink/pull/158

    [FLINK-1170] Fix faulty input split localization

    Pass hostname to split assigner
    Avoid clashes by only using the first component of the fully qualified hostname

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rmetzger/incubator-flink localizableInputSplitAssignment

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/158.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #158
    
----
commit 8896b091c2c707f4df331f7f40a89375e2f6f02d
Author: Robert Metzger <rmetzger@apache.org>
Date:   2014-10-17T08:58:15Z

    [FLINK-1170] Fix faulty input split localization
    Pass hostname to split assigner
    Avoid clashes by only using the first component of the fully qualified hostname

----
;;;","17/Oct/14 17:25;hsaputra;HI [~rmetzger], could you share details on how to reproduce this issue?;;;","17/Oct/14 17:38;rmetzger;I found the issue while running a very simple ""distributed grep"" job that is just reading a lot of data, filtering it for a certain string.
I had 1 TB of input data on a 24 nodes cluster. 
The runtime was very bad with the issue (~1 hour), after the fix, I've got it down to less than 4 minutes.
Flink and HDFS seem to use different hostname-representations. While hdfs was just using ""worker1"", Flink was using the full hostname (""worker1.hdcluster.company.com""). This caused the input splits to be assigned randomly, not local to the actual data.

After the fix, the data has been read locally most of the time (without costy network IO).;;;","17/Oct/14 17:48;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/158#discussion_r19032254
  
    --- Diff: flink-core/src/main/java/org/apache/flink/api/common/io/LocatableInputSplitAssigner.java ---
    @@ -173,13 +175,13 @@ public LocatableInputSplit getNextInputSplit(String host) {
     		}
     	}
     	
    -	private static final boolean isLocal(String host, String[] hosts) {
    -		if (host == null || hosts == null) {
    +	private static final boolean isLocal(String flinkHost, String[] hosts) {
    +		if (flinkHost == null || hosts == null) {
     			return false;
     		}
    -		
     		for (String h : hosts) {
    --- End diff --
    
    I would add some null checks here. Input Splits are user generated an can contained arbitrary nonsense in general
;;;","17/Oct/14 17:49;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/158#discussion_r19032266
  
    --- Diff: flink-core/src/main/java/org/apache/flink/util/NetUtils.java ---
    @@ -0,0 +1,36 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.util;
    +
    +
    +public class NetUtils {
    +	/**
    +	 * Turn a fully qualified domain name (fqdn) into a hostname.
    +	 * 
    +	 * @param fqdn
    +	 * @return
    +	 */
    +	public static String getHostnameFromFQDN(String fqdn) {
    +		int dotPos = fqdn.indexOf('.');
    --- End diff --
    
    Would add a null check here.
;;;","17/Oct/14 17:50;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/158#discussion_r19032342
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/instance/InstanceConnectionInfo.java ---
    @@ -154,7 +176,7 @@ public void read(DataInputView in) throws IOException {
     		this.ipcPort = in.readInt();
     		this.dataPort = in.readInt();
     		
    -		this.hostName = StringUtils.readNullableString(in);
    --- End diff --
    
    Would either serialize/deserialize the hostname as well, or at least set it to null.
;;;","17/Oct/14 17:59;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/158#discussion_r19032889
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/instance/InstanceConnectionInfo.java ---
    @@ -25,7 +25,10 @@
     import org.apache.flink.core.io.IOReadableWritable;
    --- End diff --
    
    Eager computation of the FQDN (in the constructor) on the taskmanager would be a good idea. Otherwise, in corner cases, reverse name lookup falls to the jobmanager (which might get overloaded doing too many reverse lookups).
;;;","17/Oct/14 18:06;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/158#issuecomment-59552098
  
    Other than the comments, I think this is good.
;;;","18/Oct/14 15:14;rmetzger;Resolved in http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/23e30f09;;;","18/Oct/14 15:14;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/158
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join Hint Specification in .join() Function is not Documented,FLINK-1169,12748634,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,aljoscha,aljoscha,16/Oct/14 16:05,28/Feb/19 14:29,14/Jul/23 05:57,06/Jan/15 15:51,0.7.0-incubating,,,0.9,,,API / Scala,,,,0,starter,,,,aljoscha,chesnay,chiwanpark,fhueske,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 06 15:51:09 UTC 2015,,,,,,,,,,"0|i2196n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/14 20:51;chesnay;it is not documented in the programming guide, but in the DataSet Transformations page:

http://flink.incubator.apache.org/docs/0.7-incubating/dataset_transformations.html#join-with-dataset-size-hint;;;","18/Nov/14 11:08;rmetzger;I would vote to close the issue as invalid.;;;","23/Nov/14 17:12;chiwanpark;I also think this issue can be closed.;;;","24/Nov/14 08:34;fhueske;I think the issue refers to specific execution strategy hints, not to the size hint, i.e.,:

{{input1.join(input2, JoinHint.REPARTITION_HASH_SECOND).where(0).equalTo(1);}}

This feature is not documented, AFAIK.
I reopen this issue.;;;","06/Jan/15 15:51;sewen;Fixed in c52f4cab709eed2c8b355b4c24f87ace987d8fa9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompilerException caused by NullPointerException,FLINK-1167,12748278,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,albermax,albermax,15/Oct/14 13:35,18/Oct/14 17:47,14/Jul/23 05:57,18/Oct/14 17:47,0.7.0-incubating,,,0.7.0-incubating,,,,,,,0,,,"Run into it during working on my code. Seems not caused by my plan, or anyway the compiler should have a NullPointer isssue:

org.apache.flink.compiler.CompilerException: An error occurred while translating the optimized plan to a nephele JobGraph: Error translating node 'Union ""Union"" : UNION [[ GlobalProperties [partitioning=HASH_PARTITIONED, on fields [0]] ]] [[ LocalProperties [ordering=null, grouped=null, unique=null] ]]': null
	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:543)
	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:95)
	at org.apache.flink.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:170)
	at org.apache.flink.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:196)
	at org.apache.flink.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:196)
	at org.apache.flink.compiler.plan.OptimizedPlan.accept(OptimizedPlan.java:165)
	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.compileJobGraph(NepheleJobGraphGenerator.java:163)
	at org.apache.flink.client.program.Client.getJobGraph(Client.java:218)
	at org.apache.flink.client.program.Client.run(Client.java:290)
	at org.apache.flink.client.program.Client.run(Client.java:285)
	at org.apache.flink.client.program.Client.run(Client.java:230)
	at org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:347)
	at org.apache.flink.client.CliFrontend.run(CliFrontend.java:334)
	at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1001)
	at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1025)
Caused by: org.apache.flink.compiler.CompilerException: Error translating node 'Union ""Union"" : UNION [[ GlobalProperties [partitioning=HASH_PARTITIONED, on fields [0]] ]] [[ LocalProperties [ordering=null, grouped=null, unique=null] ]]': null
	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:338)
	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:95)
	at org.apache.flink.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:162)
	at org.apache.flink.compiler.plan.WorksetIterationPlanNode.acceptForStepFunction(WorksetIterationPlanNode.java:196)
	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:398)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.runtime.operators.util.TaskConfig.setDriver(TaskConfig.java:307)
	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.createDualInputVertex(NepheleJobGraphGenerator.java:793)
	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:286)
	... 18 more
",,albermax,rmetzger,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/14 13:38;albermax;X;https://issues.apache.org/jira/secure/attachment/12674994/X","15/Oct/14 13:38;albermax;Y;https://issues.apache.org/jira/secure/attachment/12674995/Y","15/Oct/14 13:38;albermax;bb.tar.gz;https://issues.apache.org/jira/secure/attachment/12674993/bb.tar.gz","15/Oct/14 13:38;albermax;random_file;https://issues.apache.org/jira/secure/attachment/12674996/random_file",,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 18 17:47:46 UTC 2014,,,,,,,,,,"0|i2171r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/14 13:38;albermax;flink run -v bump_boost-0.1.jar -c bumpboost.Job x_file=X y_file=Y out_file=/tmp/tmpnWYamw random_file=random_file dimensions=1 N=100 width_candidates_file=/tmp/tmpTJ4LDh iterations=30 multi_bump_boost=0 gradient_descent_iterations=30 cache=False min_width=-4 max_width=6 min_width_update=1e-08 max_width_update=10

width_candidates_file is not needed ;;;","18/Oct/14 17:47;rmetzger;Fixed in http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/259f10c0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uninformative error message when having an identity iteration with the Scala API,FLINK-1164,12747866,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,trohrmann,trohrmann,13/Oct/14 22:31,18/Oct/14 17:48,14/Jul/23 05:57,18/Oct/14 17:48,,,,0.7.0-incubating,,,,,,,0,,,"If one implements an identity mapper with the Scala API, then one gets a strange error message which might be very confusing for the user: 

Exception in thread ""main"" org.apache.flink.runtime.client.JobExecutionException: java.lang.Exception: Failed to deploy the task PartialSolution (BulkIteration (Bulk Iteration)) (1/8) - execution #0 to slot SubSlot 1 (0bbe935796159bfe18abca6f1b32769a (0) - ALLOCATED/ALIVE): java.lang.Exception: The number of writers created in 'registerInputOutput()' is different than the number of connected outgoing edges in the job graph.",,aljoscha,githubbot,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 18 17:48:41 UTC 2014,,,,,,,,,,"0|i214l3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/14 08:28;aljoscha;This also happens in the Java API. You wrote ""identity"" mapper, but you meant ""Identity Iteration"", right? Because when I do

{code}
val result = input.iterate{ in =>
  in.map { x => x}
}
{code}

Which is an identity mapper, it works.;;;","18/Oct/14 15:42;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/160

    Bug fixes for iteration cornercases [FLINK-1164] and [FLINK-1167]

    Fixes issues [FLINK-1164] and [FLINK-1167]

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink empty_iter

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/160.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #160
    
----
commit 1a493919dfa63702ae682656865f14266b1fde69
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-10-17T23:55:08Z

    [FLINK-1164] Gracefully handle empty (identity) iterations

commit da61d8bb97cd6287787b4107e7fda8bc14208e1a
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-10-18T01:08:39Z

    [FLINK-1167] Handle unions at the root of the iteration

----
;;;","18/Oct/14 17:31;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/160#issuecomment-59622694
  
    The pull request is affecting the JSON plan generator. 
    Have you tested the plan visualization after the change?
;;;","18/Oct/14 17:43;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/160#issuecomment-59623095
  
    Okay, I've tested the plan viz, all good. I'm going to merge this and include it into the 0.7 release.
;;;","18/Oct/14 17:46;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/160
;;;","18/Oct/14 17:48;rmetzger;Resolved in http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/867e3a57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala DataSet cannot be used within iterations because it is not serializable,FLINK-1163,12747863,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,trohrmann,trohrmann,13/Oct/14 22:17,27/Nov/14 16:53,14/Jul/23 05:57,27/Nov/14 16:53,,,,0.8.0,,,API / Scala,,,,0,,,The Scala iterations cannot deal with a Scala DataSet in its closure because it is not serializable.,,aljoscha,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 27 16:53:18 UTC 2014,,,,,,,,,,"0|i214kf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/14 22:52;trohrmann;Well, actually it seems to be a problem of proper closure cleanup. The problem occurs for me in the following case:

{code}
object Job {
  case class Pagerank(node: Int, rank: Double)
  case class AdjacencyRow(node: Int, neighbours: Array[Int])

  val dampingFactor = 0.85
  val maxIterations = 2

  def main(args: Array[String]) {
    // set up the execution environment
    val env = ExecutionEnvironment.getExecutionEnvironment

    val numVertices = 10
    val adjacencyMatrix = getInitialAdjacencyMatrix(numVertices, env)
    val initialPagerank = getInitialPagerank(numVertices, env)


    val solution = initialPagerank.iterate(maxIterations) {
        _.join(adjacencyMatrix).where(_.node).equalTo(_.node).flatMap {
          _ match{
            case (Pagerank(node, rank), AdjacencyRow(_, neighbours)) =>{
              val length = neighbours.length
              (neighbours map {
                Pagerank(_, dampingFactor*rank/length)
              }) :+ Pagerank(node, (1-dampingFactor)/numVertices)
            }
          }
        }
    }

    solution.print()

    env.execute(""Flink Scala API Skeleton"")
  }
}
{code}

If I put numVertices as object member, then everything works properly.;;;","14/Oct/14 08:22;aljoscha;Yes, the problem is that the closure of the main method gets included when a local variable from there is used inside a user-code function.;;;","27/Nov/14 16:53;aljoscha;Fixed with merge of ClosureCleaner.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot serialize Scala classes with Avro serializer,FLINK-1162,12747855,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,trohrmann,trohrmann,13/Oct/14 21:49,04/Mar/15 13:47,14/Jul/23 05:57,04/Mar/15 13:47,,,,0.9,,,API / Scala,Runtime / Task,,,0,,,The problem occurs for class names containing a '$' dollar sign in its name how it is sometimes the case for Scala classes.,,rmetzger,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 13:47:13 UTC 2015,,,,,,,,,,"0|i214iv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/15 08:34;rmetzger;I guess we can close this issue because we use Kryo now?;;;","23/Feb/15 09:50;sewen;Yep, this one should be resolved.;;;","04/Mar/15 13:47;rmetzger;Fixed with Kryo.
I suspect there is no test for it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Case style anonymous functions not supported by Scala API,FLINK-1159,12747763,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,stefanobaghino,trohrmann,trohrmann,13/Oct/14 16:43,02/Oct/19 17:42,14/Jul/23 05:57,04/Apr/16 19:33,,,,1.1.0,,,API / Scala,,,,0,,,"In Scala it is very common to define anonymous functions of the following form
{code}
{
case foo: Bar => foobar(foo)
case _ => throw new RuntimeException()
}
{code}

These case style anonymous functions are not supported yet by the Scala API. Thus, one has to write redundant code to name the function parameter.

What works is the following pattern, but it is not intuitive for someone coming from Scala:
{code}
dataset.map{
  _ match{
    case foo:Bar => ...
  }
}
{code}",,aljoscha,githubbot,rmetzger,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 04 19:33:22 UTC 2016,,,,,,,,,,"0|i21407:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/14 08:36;aljoscha;Yes, I'm aware of this problem. The type checker cannot deal with it when the map function that takes a MapFunction is present. When you comment that one out you can use the ""case-style"" functions.;;;","14/Oct/14 08:48;rmetzger;Mh. How about adding an additional method with a different name for that case?
Will Scala be able to support this at some point or is it a fundamental limitation?;;;","18/Nov/14 11:36;sewen;Can we ever resolve this issue? Unless the Scala compiler improves the type checker?;;;","20/Nov/14 09:24;aljoscha;I think we could only resolve it if we renamed the functions that take for example a MapFunction. So we would have mapRich() and map(). But that's a bit ugly.;;;","17/Feb/16 09:27;stefanobaghino;If that's ok with [~aljoscha] I would assign this issue to me as I've started working on a solution ([here|https://github.com/radicalbit/flink/commits/1159-implicit]) that resulted from a mailing list discussion with [~till.rohrmann] and [~sewen] ([here|http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Case-style-anonymous-functions-not-supported-by-Scala-API-td10052.html]).;;;","17/Feb/16 09:46;sewen;Sure, I'll assign the issue you. As far as I know, Aljoscha is not currently working on this...;;;","17/Feb/16 09:54;aljoscha;Assigned it to you.;;;","24/Feb/16 12:08;githubbot;GitHub user stefanobaghino opened a pull request:

    https://github.com/apache/flink/pull/1704

    [FLINK-1159] Case style anonymous functions not supported by Scala API

    The proposed API extension methods would allow developers to pass a pattern matching anonymous function so that they are applied on a `DataSet` or `DataStream`; many methods defined on the `DataSet` and `DataStream` APIs don't support those functions due to the overloading[[1]][[2]]; pattern matching anonymous functions allow a very idiomatic approach in Scala to *decompose tuples, case classes and collections*.
    
    The PR does not pollute the original `DataSet` and `DataStream` APIs but is provided as an optional set of extensions methods, implemented via implicit conversions and made available to the developer by explicitly importing the required package, e.g.:
    
    ```scala
    import org.apache.flink.api.scala.extensions.acceptPartialFunctions
    env.fromElements('a -> 1, 'b -> 2).mapWith {
      case (key, value) =>
        ... // key and value are now available and have a sensible name
    }
    ```
    
    [1]: https://groups.google.com/d/msg/scala-user/3oHnDEl1UsM/dDNir9BsiG4J
    [2]: http://www.scala-lang.org/files/archive/spec/2.11/06-expressions.html#overloading-resolution

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/radicalbit/flink 1159

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/1704.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1704
    
----
commit 61d184075cce9fa4f3a8e477634adef083d3a070
Author: Stefano Baghino <stefano@baghino.me>
Date:   2016-02-24T12:05:16Z

    [FLINK-1159] Case style anonymous functions not supported by Scala API

----
;;;","24/Feb/16 14:35;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r53945878
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/package.scala ---
    @@ -0,0 +1,174 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala._
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * acceptPartialFunctions extends the original DataSet with methods with unique names
    +  * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +  * the fact that overloaded methods taking functions as parameters can't accept partial
    +  * functions as well. This enables the possibility to directly apply pattern matching
    +  * to decompose inputs such as tuples, case classes and collections.
    +  *
    +  * e.g.
    +  * {{{
    +  *   object Main {
    +  *     import org.apache.flink.api.scala.extensions._
    +  *     case class Point(x: Double, y: Double)
    +  *     def main(args: Array[String]): Unit = {
    +  *       val env = ExecutionEnvironment.getExecutionEnvironment
    +  *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +  *       ds.filterWith {
    +  *         case Point(x, _) => x > 1
    +  *       }.reduceWith {
    +  *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +  *       }.mapWith {
    +  *         case Point(x, y) => (x, y)
    +  *       }.flatMapWith {
    +  *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +  *       }.groupingBy {
    --- End diff --
    
    Isn't it called `keyingBy`?
;;;","24/Feb/16 14:35;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r53945929
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/package.scala ---
    @@ -0,0 +1,174 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala._
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * acceptPartialFunctions extends the original DataSet with methods with unique names
    +  * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +  * the fact that overloaded methods taking functions as parameters can't accept partial
    +  * functions as well. This enables the possibility to directly apply pattern matching
    +  * to decompose inputs such as tuples, case classes and collections.
    +  *
    +  * e.g.
    +  * {{{
    +  *   object Main {
    +  *     import org.apache.flink.api.scala.extensions._
    --- End diff --
    
    This won't work. You have to import `org.apache.flink.api.scala.extensions.acceptPartialFunctions._`
;;;","24/Feb/16 14:36;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r53946030
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/package.scala ---
    @@ -0,0 +1,174 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala._
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * acceptPartialFunctions extends the original DataSet with methods with unique names
    +  * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +  * the fact that overloaded methods taking functions as parameters can't accept partial
    +  * functions as well. This enables the possibility to directly apply pattern matching
    +  * to decompose inputs such as tuples, case classes and collections.
    +  *
    +  * e.g.
    +  * {{{
    +  *   object Main {
    +  *     import org.apache.flink.api.scala.extensions._
    +  *     case class Point(x: Double, y: Double)
    +  *     def main(args: Array[String]): Unit = {
    +  *       val env = ExecutionEnvironment.getExecutionEnvironment
    +  *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +  *       ds.filterWith {
    +  *         case Point(x, _) => x > 1
    +  *       }.reduceWith {
    +  *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +  *       }.mapWith {
    +  *         case Point(x, y) => (x, y)
    +  *       }.flatMapWith {
    +  *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +  *       }.groupingBy {
    --- End diff --
    
    Sorry forget my comment.
;;;","24/Feb/16 14:36;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r53946101
  
    --- Diff: flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/extensions/acceptPartialFunctions/package.scala ---
    @@ -0,0 +1,133 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.streaming.api.scala.extensions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.streaming.api.scala.{JoinedStreams, CoGroupedStreams, KeyedStream, DataStream}
    +import org.apache.flink.streaming.api.windowing.windows.Window
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * acceptPartialFunctions extends the original DataStream with methods with unique names
    +  * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +  * the fact that overloaded methods taking functions as parameters can't accept partial
    +  * functions as well. This enables the possibility to directly apply pattern matching
    +  * to decompose inputs such as tuples, case classes and collections.
    +  *
    +  * e.g.
    +  * {{{
    +  *   object Main {
    +  *     import org.apache.flink.api.scala.extensions._
    +  *     case class Point(x: Double, y: Double)
    +  *     def main(args: Array[String]): Unit = {
    +  *       val env = StreamExecutionEnvironment.getExecutionEnvironment
    +  *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +  *       ds.filterWith {
    +  *         case Point(x, _) => x > 1
    +  *       }.reduceWith {
    +  *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +  *       }.mapWith {
    +  *         case Point(x, y) => (x, y)
    +  *       }.flatMapWith {
    +  *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +  *       }.groupingBy {
    --- End diff --
    
    Shouldn't this be called `keyingBy`?
;;;","24/Feb/16 14:37;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r53946205
  
    --- Diff: flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/extensions/acceptPartialFunctions/package.scala ---
    @@ -0,0 +1,133 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.streaming.api.scala.extensions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.streaming.api.scala.{JoinedStreams, CoGroupedStreams, KeyedStream, DataStream}
    +import org.apache.flink.streaming.api.windowing.windows.Window
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * acceptPartialFunctions extends the original DataStream with methods with unique names
    +  * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +  * the fact that overloaded methods taking functions as parameters can't accept partial
    +  * functions as well. This enables the possibility to directly apply pattern matching
    +  * to decompose inputs such as tuples, case classes and collections.
    +  *
    +  * e.g.
    +  * {{{
    +  *   object Main {
    +  *     import org.apache.flink.api.scala.extensions._
    --- End diff --
    
    import shouldn't work if I'm not mistaken
;;;","24/Feb/16 14:43;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r53947144
  
    --- Diff: flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/extensions/acceptPartialFunctions/package.scala ---
    @@ -0,0 +1,133 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.streaming.api.scala.extensions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.streaming.api.scala.{JoinedStreams, CoGroupedStreams, KeyedStream, DataStream}
    +import org.apache.flink.streaming.api.windowing.windows.Window
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * acceptPartialFunctions extends the original DataStream with methods with unique names
    +  * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +  * the fact that overloaded methods taking functions as parameters can't accept partial
    +  * functions as well. This enables the possibility to directly apply pattern matching
    +  * to decompose inputs such as tuples, case classes and collections.
    +  *
    +  * e.g.
    +  * {{{
    +  *   object Main {
    +  *     import org.apache.flink.api.scala.extensions._
    +  *     case class Point(x: Double, y: Double)
    +  *     def main(args: Array[String]): Unit = {
    +  *       val env = StreamExecutionEnvironment.getExecutionEnvironment
    +  *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +  *       ds.filterWith {
    +  *         case Point(x, _) => x > 1
    +  *       }.reduceWith {
    +  *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +  *       }.mapWith {
    +  *         case Point(x, y) => (x, y)
    +  *       }.flatMapWith {
    +  *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +  *       }.groupingBy {
    +  *         case (id, value) => id
    +  *       }
    +  *     }
    +  *   }
    +  * }}}
    +  *
    +  */
    +package object acceptPartialFunctions {
    +
    +  implicit class OnDataStream[T: TypeInformation](stream: DataStream[T]) {
    +
    +    /**
    +      * Applies a function `fun` to each item of the stream
    +      *
    +      * @param fun The function to be applied to each item
    +      * @tparam R The type of the items in the returned stream
    +      * @return A dataset of R
    +      */
    +    def mapWith[R: TypeInformation: ClassTag](fun: T => R): DataStream[R] =
    +      stream.map(fun)
    +
    +    /**
    +      * Applies a function `fun` to each item of the stream, producing a collection of items
    +      * that will be flattened in the resulting stream
    +      *
    +      * @param fun The function to be applied to each item
    +      * @tparam R The type of the items in the returned stream
    +      * @return A dataset of R
    +      */
    +    def flatMapWith[R: TypeInformation: ClassTag](fun: T => TraversableOnce[R]): DataStream[R] =
    +      stream.flatMap(fun)
    +
    +    /**
    +      * Applies a predicate `fun` to each item of the stream, keeping only those for which
    +      * the predicate holds
    +      *
    +      * @param fun The predicate to be tested on each item
    +      * @return A dataset of R
    +      */
    +    def filterWith(fun: T => Boolean): DataStream[T] =
    +      stream.filter(fun)
    +
    +    /**
    +      * Keys the items according to a keying function `fun`
    +      *
    +      * @param fun The keying function
    +      * @tparam K The type of the key, for which type information must be known
    +      * @return A stream of Ts keyed by Ks
    +      */
    +    def keyingBy[K: TypeInformation: ClassTag](fun: T => K): KeyedStream[T, K] =
    +      stream.keyBy(fun)
    +
    +  }
    +
    +  implicit class OnKeyedStream[T: TypeInformation, K](stream: KeyedStream[T, K]) {
    --- End diff --
    
    What is with the `fold` operation?
;;;","24/Feb/16 14:44;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r53947242
  
    --- Diff: flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/extensions/acceptPartialFunctions/package.scala ---
    @@ -0,0 +1,133 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.streaming.api.scala.extensions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.streaming.api.scala.{JoinedStreams, CoGroupedStreams, KeyedStream, DataStream}
    +import org.apache.flink.streaming.api.windowing.windows.Window
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * acceptPartialFunctions extends the original DataStream with methods with unique names
    +  * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +  * the fact that overloaded methods taking functions as parameters can't accept partial
    +  * functions as well. This enables the possibility to directly apply pattern matching
    +  * to decompose inputs such as tuples, case classes and collections.
    +  *
    +  * e.g.
    +  * {{{
    +  *   object Main {
    +  *     import org.apache.flink.api.scala.extensions._
    +  *     case class Point(x: Double, y: Double)
    +  *     def main(args: Array[String]): Unit = {
    +  *       val env = StreamExecutionEnvironment.getExecutionEnvironment
    +  *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +  *       ds.filterWith {
    +  *         case Point(x, _) => x > 1
    +  *       }.reduceWith {
    +  *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +  *       }.mapWith {
    +  *         case Point(x, y) => (x, y)
    +  *       }.flatMapWith {
    +  *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +  *       }.groupingBy {
    +  *         case (id, value) => id
    +  *       }
    +  *     }
    +  *   }
    +  * }}}
    +  *
    +  */
    +package object acceptPartialFunctions {
    --- End diff --
    
    I think the `ConnectedStreams` and `WindowedStreams` are missing.
;;;","24/Feb/16 14:48;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r53948074
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/package.scala ---
    @@ -0,0 +1,174 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala._
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * acceptPartialFunctions extends the original DataSet with methods with unique names
    +  * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +  * the fact that overloaded methods taking functions as parameters can't accept partial
    +  * functions as well. This enables the possibility to directly apply pattern matching
    +  * to decompose inputs such as tuples, case classes and collections.
    +  *
    +  * e.g.
    +  * {{{
    +  *   object Main {
    +  *     import org.apache.flink.api.scala.extensions._
    +  *     case class Point(x: Double, y: Double)
    +  *     def main(args: Array[String]): Unit = {
    +  *       val env = ExecutionEnvironment.getExecutionEnvironment
    +  *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +  *       ds.filterWith {
    +  *         case Point(x, _) => x > 1
    +  *       }.reduceWith {
    +  *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +  *       }.mapWith {
    +  *         case Point(x, y) => (x, y)
    +  *       }.flatMapWith {
    +  *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +  *       }.groupingBy {
    +  *         case (id, value) => id
    +  *       }
    +  *     }
    +  *   }
    +  * }}}
    +  *
    +  */
    +package object acceptPartialFunctions {
    +
    +  implicit class OnDataSet[T: TypeInformation](ds: DataSet[T]) {
    +
    +    /**
    +      * Applies a function `fun` to each item of the data set
    +      *
    +      * @param fun The function to be applied to each item
    +      * @tparam R The type of the items in the returned data set
    +      * @return A dataset of R
    +      */
    +    def mapWith[R: TypeInformation: ClassTag](fun: T => R): DataSet[R] =
    +      ds.map(fun)
    +
    +    /**
    +      * Applies a function `fun` to a partition as a whole
    +      *
    +      * @param fun The function to be applied on the whole partition
    +      * @tparam R The type of the items in the returned data set
    +      * @return A dataset of R
    +      */
    +    def mapPartitionWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +      ds.mapPartition {
    +        (it, out) =>
    +          out.collect(fun(it.to[Seq]))
    +      }
    +
    +    /**
    +      * Applies a function `fun` to each item of the dataset, producing a collection of items
    +      * that will be flattened in the resulting data set
    +      *
    +      * @param fun The function to be applied to each item
    +      * @tparam R The type of the items in the returned data set
    +      * @return A dataset of R
    +      */
    +    def flatMapWith[R: TypeInformation: ClassTag](fun: T => TraversableOnce[R]): DataSet[R] =
    +      ds.flatMap(fun)
    +
    +    /**
    +      * Applies a predicate `fun` to each item of the data set, keeping only those for which
    +      * the predicate holds
    +      *
    +      * @param fun The predicate to be tested on each item
    +      * @return A dataset of R
    +      */
    +    def filterWith(fun: T => Boolean): DataSet[T] =
    +      ds.filter(fun)
    +
    +    /**
    +      * Applies a reducer `fun` to the data set
    +      *
    +      * @param fun The reducing function to be applied on the whole data set
    +      * @tparam R The type of the items in the returned collection
    +      * @return A data set of Rs
    +      */
    +    def reduceWith[R: TypeInformation: ClassTag](fun: (T, T) => T): DataSet[T] =
    +      ds.reduce(fun)
    +
    +    /**
    +      * Applies a reducer `fun` to a grouped data set
    +      *
    +      * @param fun The function to be applied to the whole grouping
    +      * @tparam R The type of the items in the returned data set
    +      * @return A dataset of Rs
    +      */
    +    def reduceGroupWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +      ds.reduceGroup {
    +        (it, out) =>
    +          out.collect(fun(it.to[Seq]))
    +      }
    +
    +    /**
    +      * Groups the items according to a grouping function `fun`
    +      *
    +      * @param fun The grouping function
    +      * @tparam K The return type of the grouping function, for which type information must be known
    +      * @return A grouped data set of Ts
    +      */
    +    def groupingBy[K: TypeInformation: ClassTag](fun: T => K): GroupedDataSet[T] =
    +      ds.groupBy(fun)
    +
    +  }
    +
    +  implicit class OnJoinDataSet[L: TypeInformation, R: TypeInformation](
    +      dataset: JoinDataSet[L, R]) {
    +
    +    /**
    +      * Joins the data sets using the function `fun` to project elements from both in the
    +      * resulting data set
    +      *
    +      * @param fun The function that defines the projection of the join
    +      * @tparam O The return type of the projection, for which type information must be known
    +      * @return A fully joined data set of Os
    +      */
    +    def projecting[O: TypeInformation: ClassTag](fun: (L, R) => O): DataSet[O] =
    +      dataset(fun)
    +
    +  }
    +
    +  implicit class OnCoGroupDataSet[L: TypeInformation, R: TypeInformation](
    +      dataset: CoGroupDataSet[L, R]) {
    +
    +    /**
    +      * Co-groups the data sets using the function `fun` to project elements from both in
    +      * the resulting data set
    +      *
    +      * @param fun The function that defines the projection of the co-group operation
    +      * @tparam O The return type of the projection, for which type information must be known
    +      * @return A fully co-grouped data set of Os
    +      */
    +    def projecting[O: TypeInformation: ClassTag](fun: (Seq[L], Seq[R]) => O): DataSet[O] =
    +      dataset {
    +        (left, right) =>
    +          fun(left.to[Seq], right.to[Seq])
    +      }
    +
    +  }
    --- End diff --
    
    `GroupedDataSet` is missing
;;;","24/Feb/16 14:49;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r53948137
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/package.scala ---
    @@ -0,0 +1,174 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala._
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * acceptPartialFunctions extends the original DataSet with methods with unique names
    +  * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +  * the fact that overloaded methods taking functions as parameters can't accept partial
    +  * functions as well. This enables the possibility to directly apply pattern matching
    +  * to decompose inputs such as tuples, case classes and collections.
    +  *
    +  * e.g.
    +  * {{{
    +  *   object Main {
    +  *     import org.apache.flink.api.scala.extensions._
    +  *     case class Point(x: Double, y: Double)
    +  *     def main(args: Array[String]): Unit = {
    +  *       val env = ExecutionEnvironment.getExecutionEnvironment
    +  *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +  *       ds.filterWith {
    +  *         case Point(x, _) => x > 1
    +  *       }.reduceWith {
    +  *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +  *       }.mapWith {
    +  *         case Point(x, y) => (x, y)
    +  *       }.flatMapWith {
    +  *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +  *       }.groupingBy {
    +  *         case (id, value) => id
    +  *       }
    +  *     }
    +  *   }
    +  * }}}
    +  *
    +  */
    +package object acceptPartialFunctions {
    +
    +  implicit class OnDataSet[T: TypeInformation](ds: DataSet[T]) {
    +
    +    /**
    +      * Applies a function `fun` to each item of the data set
    +      *
    +      * @param fun The function to be applied to each item
    +      * @tparam R The type of the items in the returned data set
    +      * @return A dataset of R
    +      */
    +    def mapWith[R: TypeInformation: ClassTag](fun: T => R): DataSet[R] =
    +      ds.map(fun)
    +
    +    /**
    +      * Applies a function `fun` to a partition as a whole
    +      *
    +      * @param fun The function to be applied on the whole partition
    +      * @tparam R The type of the items in the returned data set
    +      * @return A dataset of R
    +      */
    +    def mapPartitionWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +      ds.mapPartition {
    +        (it, out) =>
    +          out.collect(fun(it.to[Seq]))
    +      }
    +
    +    /**
    +      * Applies a function `fun` to each item of the dataset, producing a collection of items
    +      * that will be flattened in the resulting data set
    +      *
    +      * @param fun The function to be applied to each item
    +      * @tparam R The type of the items in the returned data set
    +      * @return A dataset of R
    +      */
    +    def flatMapWith[R: TypeInformation: ClassTag](fun: T => TraversableOnce[R]): DataSet[R] =
    +      ds.flatMap(fun)
    +
    +    /**
    +      * Applies a predicate `fun` to each item of the data set, keeping only those for which
    +      * the predicate holds
    +      *
    +      * @param fun The predicate to be tested on each item
    +      * @return A dataset of R
    +      */
    +    def filterWith(fun: T => Boolean): DataSet[T] =
    +      ds.filter(fun)
    +
    +    /**
    +      * Applies a reducer `fun` to the data set
    +      *
    +      * @param fun The reducing function to be applied on the whole data set
    +      * @tparam R The type of the items in the returned collection
    +      * @return A data set of Rs
    +      */
    +    def reduceWith[R: TypeInformation: ClassTag](fun: (T, T) => T): DataSet[T] =
    +      ds.reduce(fun)
    +
    +    /**
    +      * Applies a reducer `fun` to a grouped data set
    +      *
    +      * @param fun The function to be applied to the whole grouping
    +      * @tparam R The type of the items in the returned data set
    +      * @return A dataset of Rs
    +      */
    +    def reduceGroupWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +      ds.reduceGroup {
    +        (it, out) =>
    +          out.collect(fun(it.to[Seq]))
    +      }
    +
    +    /**
    +      * Groups the items according to a grouping function `fun`
    +      *
    +      * @param fun The grouping function
    +      * @tparam K The return type of the grouping function, for which type information must be known
    +      * @return A grouped data set of Ts
    +      */
    +    def groupingBy[K: TypeInformation: ClassTag](fun: T => K): GroupedDataSet[T] =
    +      ds.groupBy(fun)
    +
    +  }
    +
    +  implicit class OnJoinDataSet[L: TypeInformation, R: TypeInformation](
    +      dataset: JoinDataSet[L, R]) {
    +
    +    /**
    +      * Joins the data sets using the function `fun` to project elements from both in the
    +      * resulting data set
    +      *
    +      * @param fun The function that defines the projection of the join
    +      * @tparam O The return type of the projection, for which type information must be known
    +      * @return A fully joined data set of Os
    +      */
    +    def projecting[O: TypeInformation: ClassTag](fun: (L, R) => O): DataSet[O] =
    +      dataset(fun)
    +
    +  }
    +
    +  implicit class OnCoGroupDataSet[L: TypeInformation, R: TypeInformation](
    +      dataset: CoGroupDataSet[L, R]) {
    +
    +    /**
    +      * Co-groups the data sets using the function `fun` to project elements from both in
    +      * the resulting data set
    +      *
    +      * @param fun The function that defines the projection of the co-group operation
    +      * @tparam O The return type of the projection, for which type information must be known
    +      * @return A fully co-grouped data set of Os
    +      */
    +    def projecting[O: TypeInformation: ClassTag](fun: (Seq[L], Seq[R]) => O): DataSet[O] =
    +      dataset {
    +        (left, right) =>
    +          fun(left.to[Seq], right.to[Seq])
    +      }
    +
    +  }
    --- End diff --
    
    `CrossDataSet` is missing
;;;","24/Feb/16 14:50;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r53948314
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/package.scala ---
    @@ -0,0 +1,174 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala._
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * acceptPartialFunctions extends the original DataSet with methods with unique names
    +  * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +  * the fact that overloaded methods taking functions as parameters can't accept partial
    +  * functions as well. This enables the possibility to directly apply pattern matching
    +  * to decompose inputs such as tuples, case classes and collections.
    +  *
    +  * e.g.
    +  * {{{
    +  *   object Main {
    +  *     import org.apache.flink.api.scala.extensions._
    +  *     case class Point(x: Double, y: Double)
    +  *     def main(args: Array[String]): Unit = {
    +  *       val env = ExecutionEnvironment.getExecutionEnvironment
    +  *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +  *       ds.filterWith {
    +  *         case Point(x, _) => x > 1
    +  *       }.reduceWith {
    +  *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +  *       }.mapWith {
    +  *         case Point(x, y) => (x, y)
    +  *       }.flatMapWith {
    +  *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +  *       }.groupingBy {
    +  *         case (id, value) => id
    +  *       }
    +  *     }
    +  *   }
    +  * }}}
    +  *
    +  */
    +package object acceptPartialFunctions {
    +
    +  implicit class OnDataSet[T: TypeInformation](ds: DataSet[T]) {
    +
    +    /**
    +      * Applies a function `fun` to each item of the data set
    +      *
    +      * @param fun The function to be applied to each item
    +      * @tparam R The type of the items in the returned data set
    +      * @return A dataset of R
    +      */
    +    def mapWith[R: TypeInformation: ClassTag](fun: T => R): DataSet[R] =
    +      ds.map(fun)
    +
    +    /**
    +      * Applies a function `fun` to a partition as a whole
    +      *
    +      * @param fun The function to be applied on the whole partition
    +      * @tparam R The type of the items in the returned data set
    +      * @return A dataset of R
    +      */
    +    def mapPartitionWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +      ds.mapPartition {
    +        (it, out) =>
    +          out.collect(fun(it.to[Seq]))
    +      }
    +
    +    /**
    +      * Applies a function `fun` to each item of the dataset, producing a collection of items
    +      * that will be flattened in the resulting data set
    +      *
    +      * @param fun The function to be applied to each item
    +      * @tparam R The type of the items in the returned data set
    +      * @return A dataset of R
    +      */
    +    def flatMapWith[R: TypeInformation: ClassTag](fun: T => TraversableOnce[R]): DataSet[R] =
    +      ds.flatMap(fun)
    +
    +    /**
    +      * Applies a predicate `fun` to each item of the data set, keeping only those for which
    +      * the predicate holds
    +      *
    +      * @param fun The predicate to be tested on each item
    +      * @return A dataset of R
    +      */
    +    def filterWith(fun: T => Boolean): DataSet[T] =
    +      ds.filter(fun)
    +
    +    /**
    +      * Applies a reducer `fun` to the data set
    +      *
    +      * @param fun The reducing function to be applied on the whole data set
    +      * @tparam R The type of the items in the returned collection
    +      * @return A data set of Rs
    +      */
    +    def reduceWith[R: TypeInformation: ClassTag](fun: (T, T) => T): DataSet[T] =
    +      ds.reduce(fun)
    +
    +    /**
    +      * Applies a reducer `fun` to a grouped data set
    +      *
    +      * @param fun The function to be applied to the whole grouping
    +      * @tparam R The type of the items in the returned data set
    +      * @return A dataset of Rs
    +      */
    +    def reduceGroupWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +      ds.reduceGroup {
    +        (it, out) =>
    +          out.collect(fun(it.to[Seq]))
    +      }
    +
    +    /**
    +      * Groups the items according to a grouping function `fun`
    +      *
    +      * @param fun The grouping function
    +      * @tparam K The return type of the grouping function, for which type information must be known
    +      * @return A grouped data set of Ts
    +      */
    +    def groupingBy[K: TypeInformation: ClassTag](fun: T => K): GroupedDataSet[T] =
    +      ds.groupBy(fun)
    +
    --- End diff --
    
    `combineGroupWith` is not supported.
;;;","24/Feb/16 14:55;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-188289230
  
    Thanks for your contribution @stefanobaghino. I really like this feature a lot :-)
    
    Currently, the implementation is not complete, because the supported set of API calls is not complete. 
    
    With the current packaging structure one would always have to import `org.apache.flink.api.scala.extensions.acceptPartialFunctions._`. I would rather like to import the following to get partial function support `org.apache.flink.api.scala.extensions.acceptPartialFunctions` or if I want to import all extensions: `org.apache.flink.api.scala.extensions._`. We could achieve this by introducing an `extensions` package object which does something like:
    
    ```
    package object extensions {
      implicit def acceptPartialFunctions[T: TypeInformation](ds: DataStream[T]):
        DataStreamWithPartialFunctionSupport[T] = {
        new DataStreamWithPartialFunctionSupport[T](ds)
      }
    ```
    
    What do you think?
    

;;;","24/Feb/16 14:56;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-188289888
  
    It would also be great to add some tests to make sure that the import is working. Furthermore, it would be great to add documentation for the extension feature.
;;;","24/Feb/16 15:05;githubbot;Github user stefanobaghino commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r53950747
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/package.scala ---
    @@ -0,0 +1,174 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala._
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * acceptPartialFunctions extends the original DataSet with methods with unique names
    +  * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +  * the fact that overloaded methods taking functions as parameters can't accept partial
    +  * functions as well. This enables the possibility to directly apply pattern matching
    +  * to decompose inputs such as tuples, case classes and collections.
    +  *
    +  * e.g.
    +  * {{{
    +  *   object Main {
    +  *     import org.apache.flink.api.scala.extensions._
    --- End diff --
    
    Yup, leftover from the previous implementation, thanks for pointing it out, I'll fix this.
;;;","24/Feb/16 15:06;githubbot;Github user stefanobaghino commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r53950863
  
    --- Diff: flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/extensions/acceptPartialFunctions/package.scala ---
    @@ -0,0 +1,133 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.streaming.api.scala.extensions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.streaming.api.scala.{JoinedStreams, CoGroupedStreams, KeyedStream, DataStream}
    +import org.apache.flink.streaming.api.windowing.windows.Window
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * acceptPartialFunctions extends the original DataStream with methods with unique names
    +  * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +  * the fact that overloaded methods taking functions as parameters can't accept partial
    +  * functions as well. This enables the possibility to directly apply pattern matching
    +  * to decompose inputs such as tuples, case classes and collections.
    +  *
    +  * e.g.
    +  * {{{
    +  *   object Main {
    +  *     import org.apache.flink.api.scala.extensions._
    +  *     case class Point(x: Double, y: Double)
    +  *     def main(args: Array[String]): Unit = {
    +  *       val env = StreamExecutionEnvironment.getExecutionEnvironment
    +  *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +  *       ds.filterWith {
    +  *         case Point(x, _) => x > 1
    +  *       }.reduceWith {
    +  *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +  *       }.mapWith {
    +  *         case Point(x, y) => (x, y)
    +  *       }.flatMapWith {
    +  *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +  *       }.groupingBy {
    --- End diff --
    
    Copy/paste error, good catch! I'll fix this.
;;;","24/Feb/16 15:15;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-188301093
  
    Thanks @tillrohrmann, I'll fix the errors in the comments and add the missing methods and extensions.
    
    Regarding the import mode, I agree with you. I started off with just one `DataSet` but then had to support many, I'll try to go back to the original design while retaining multiple ´DataSet´ subtype extensions.
    
    The only thing I'm not very convinced of from your snippet is about returning the extended `DataStreamWithPartialFunctionSupport` class. Implicit conversions have no runtime cost, so maybe it would be better to just return the `DataSet` to make the code more compact and readable. What do you think?
    
    Regarding the tests: I actually have some tests in place locally but basically they're just copies of the original tests on the operators. Should I commit them as well?
    
    Regarding the docs: absolutely, I wasn't sure about this. Where do you think it would be better to put them? Should I add a new chapter under the programming guides?
;;;","24/Feb/16 16:01;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-188320230
  
    How do you want to do the implicit conversion if you return a `DataSet[T]`. If it makes the code more readable, then I think it's a good idea :-)
    
    We should have a test, which makes sure that the implicit conversion works when you import the corresponding package. That should be enough.
    
    I think we could add the documentation to the streaming guide. We could add a new page with the methods and link it from the scala tab of [DataStream Transformations](https://ci.apache.org/projects/flink/flink-docs-master/apis/streaming/index.html#datastream-transformations) in the streaming guide.
;;;","24/Feb/16 16:08;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-188325067
  
    Just the streaming guide? Not on both the streaming and batch?
;;;","24/Feb/16 16:18;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-188329671
  
    Of course, you're right. Also in the batch guide :-)
;;;","24/Feb/16 16:25;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-188335368
  
    Looks very nice in my opinion.
    
    Could you remove the `ClassTag` context bounds? We recently removed them from `DataStream`, because they are not needed.
;;;","24/Feb/16 16:39;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-188343795
  
    :+1: @StephanEwen I started working on the extension before the removal, will fix this as well, thanks for the feedback.
;;;","24/Feb/16 16:58;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-188352497
  
    @tillrohrmann I got mixed up reading your proposal and just got what you meant with the `extensions.acceptPartialFunctions` implicit conversion, thanks for the tip, I'll add it to the PR as well.
;;;","24/Feb/16 19:08;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-188409068
  
    This should cover the missing implementations. I also included the tests I used to test the functionality, let me know if you prefer a wider coverage. I'll provide the docs ASAP.
;;;","25/Feb/16 13:50;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-188790808
  
    @StephanEwen I had to restore some of the context bounds on `ClassTag` to make it compile, apparently the delegated methods use them; I've rebased with the latest changes on the master before putting them back in place.
;;;","26/Feb/16 11:31;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-189232475
  
    @stefanobaghino Ah, yes, for `DataSet` you need the `ClassTag`, for `DataStream` they should not be needed...
;;;","03/Mar/16 14:55;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-191799798
  
    There are scalastyle violations in the code:
    
    ```
    error file=/home/travis/build/apache/flink/flink-tests/src/test/scala/org/apache/flink/api/scala/extensions/AcceptPFFilterITCase.scala message=File must end with newline character
    error file=/home/travis/build/apache/flink/flink-tests/src/test/scala/org/apache/flink/api/scala/extensions/AcceptPFFlatMapITCase.scala message=File must end with newline character
    error file=/home/travis/build/apache/flink/flink-tests/src/test/scala/org/apache/flink/api/scala/extensions/AcceptPFMapITCase.scala message=File must end with newline character
    ```
;;;","04/Mar/16 12:21;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-192259307
  
    @tillrohrmann Yes, I've already fixed them locally, thanks for the notice and for pinging me. I'm working on the documentation, sorry if it's taking me a little bit longer than expected but I had little time this week. I want to complete the PR by Sunday evening.
;;;","04/Mar/16 23:32;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-192522625
  
    I've squashed the commits, I don't know what I was thinking about. :disappointed: 
    This should more or less be it, I'd just like to add more tests to cover all the operators.
;;;","11/Mar/16 09:39;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-195290267
  
    @tillrohrmann The tests failure seem to be flaky, I've re-run them all on our fork and they're all green now (after a couple of retries).
;;;","11/Mar/16 18:36;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-195491606
  
    This pull request adds a lot of tests (which is actually good), but all tests fire up a cluster to execute many programs. But need to somehow get this down, as these ""fire up mini cluster"" tests have made our build times explode.
    
    Is actual program execution needed here? Or is it sufficient to see that the partial function for a certain transformation creates such a transformation?
;;;","14/Mar/16 12:36;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55993580
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/OnDataSet.scala ---
    @@ -0,0 +1,104 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions.acceptPartialFunctions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.{GroupedDataSet, DataSet}
    +
    +import scala.reflect.ClassTag
    +
    +class OnDataSet[T: TypeInformation](ds: DataSet[T]) {
    +
    +  /**
    +    * Applies a function `fun` to each item of the data set
    +    *
    +    * @param fun The function to be applied to each item
    +    * @tparam R The type of the items in the returned data set
    +    * @return A dataset of R
    +    */
    +  def mapWith[R: TypeInformation: ClassTag](fun: T => R): DataSet[R] =
    +    ds.map(fun)
    +
    +  /**
    +    * Applies a function `fun` to a partition as a whole
    +    *
    +    * @param fun The function to be applied on the whole partition
    +    * @tparam R The type of the items in the returned data set
    +    * @return A dataset of R
    +    */
    +  def mapPartitionWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +    ds.mapPartition {
    +      (it, out) =>
    +        out.collect(fun(it.to[Seq]))
    --- End diff --
    
    Does `it.to[Seq]` materializes the `iterator`? If so, then this is not so good because you can run out of memory.
;;;","14/Mar/16 12:37;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55993625
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/OnDataSet.scala ---
    @@ -0,0 +1,104 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions.acceptPartialFunctions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.{GroupedDataSet, DataSet}
    +
    +import scala.reflect.ClassTag
    +
    +class OnDataSet[T: TypeInformation](ds: DataSet[T]) {
    +
    +  /**
    +    * Applies a function `fun` to each item of the data set
    +    *
    +    * @param fun The function to be applied to each item
    +    * @tparam R The type of the items in the returned data set
    +    * @return A dataset of R
    +    */
    +  def mapWith[R: TypeInformation: ClassTag](fun: T => R): DataSet[R] =
    +    ds.map(fun)
    +
    +  /**
    +    * Applies a function `fun` to a partition as a whole
    +    *
    +    * @param fun The function to be applied on the whole partition
    +    * @tparam R The type of the items in the returned data set
    +    * @return A dataset of R
    +    */
    +  def mapPartitionWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +    ds.mapPartition {
    +      (it, out) =>
    +        out.collect(fun(it.to[Seq]))
    +    }
    +
    +  /**
    +    * Applies a function `fun` to each item of the dataset, producing a collection of items
    +    * that will be flattened in the resulting data set
    +    *
    +    * @param fun The function to be applied to each item
    +    * @tparam R The type of the items in the returned data set
    +    * @return A dataset of R
    +    */
    +  def flatMapWith[R: TypeInformation: ClassTag](fun: T => TraversableOnce[R]): DataSet[R] =
    +    ds.flatMap(fun)
    +
    +  /**
    +    * Applies a predicate `fun` to each item of the data set, keeping only those for which
    +    * the predicate holds
    +    *
    +    * @param fun The predicate to be tested on each item
    +    * @return A dataset of R
    +    */
    +  def filterWith(fun: T => Boolean): DataSet[T] =
    +    ds.filter(fun)
    +
    +  /**
    +    * Applies a reducer `fun` to the data set
    +    *
    +    * @param fun The reducing function to be applied on the whole data set
    +    * @tparam R The type of the items in the returned collection
    +    * @return A data set of Rs
    +    */
    +  def reduceWith[R: TypeInformation](fun: (T, T) => T): DataSet[T] =
    +    ds.reduce(fun)
    +
    +  /**
    +    * Applies a reducer `fun` to a grouped data set
    +    *
    +    * @param fun The function to be applied to the whole grouping
    +    * @tparam R The type of the items in the returned data set
    +    * @return A dataset of Rs
    +    */
    +  def reduceGroupWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +    ds.reduceGroup {
    +      (it, out) =>
    +        out.collect(fun(it.to[Seq]))
    --- End diff --
    
    Same question here with the materialization of the iterator.
;;;","14/Mar/16 12:37;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55993657
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/OnGroupedDataSet.scala ---
    @@ -0,0 +1,75 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions.acceptPartialFunctions
    +
    +import org.apache.flink.api.common.operators.Order
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.{DataSet, GroupedDataSet}
    +
    +import scala.reflect.ClassTag
    +
    +class OnGroupedDataSet[T: ClassTag](ds: GroupedDataSet[T]) {
    +
    +  /**
    +    * Sorts a group using a sorting function `fun` and an `Order`
    +    *
    +    * @param fun The sorting function, defining the sorting key
    +    * @param order The ordering strategy (ascending, descending, etc.)
    +    * @tparam K The key type
    +    * @return A data set sorted group-wise
    +    */
    +  def sortGroupWith[K: TypeInformation](order: Order)(fun: T => K): GroupedDataSet[T] =
    +    ds.sortGroup(fun, order)
    +
    +  /**
    +    * Reduces the whole data set with a reducer `fun`
    +    *
    +    * @param fun The reducing function
    +    * @return A reduced data set of Ts
    +    */
    +  def reduceWith(fun: (T, T) => T): DataSet[T] =
    +    ds.reduce(fun)
    +
    +  /**
    +    * Reduces the data set group-wise with a reducer `fun`
    +    *
    +    * @param fun The reducing function
    +    * @tparam R The type of the items in the resulting data set
    +    * @return A data set of Rs reduced group-wise
    +    */
    +  def reduceGroupWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +    ds.reduceGroup {
    +      (it, out) =>
    +        out.collect(fun(it.to[Seq]))
    --- End diff --
    
    Materialization?
;;;","14/Mar/16 12:37;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55993670
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/OnGroupedDataSet.scala ---
    @@ -0,0 +1,75 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions.acceptPartialFunctions
    +
    +import org.apache.flink.api.common.operators.Order
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.{DataSet, GroupedDataSet}
    +
    +import scala.reflect.ClassTag
    +
    +class OnGroupedDataSet[T: ClassTag](ds: GroupedDataSet[T]) {
    +
    +  /**
    +    * Sorts a group using a sorting function `fun` and an `Order`
    +    *
    +    * @param fun The sorting function, defining the sorting key
    +    * @param order The ordering strategy (ascending, descending, etc.)
    +    * @tparam K The key type
    +    * @return A data set sorted group-wise
    +    */
    +  def sortGroupWith[K: TypeInformation](order: Order)(fun: T => K): GroupedDataSet[T] =
    +    ds.sortGroup(fun, order)
    +
    +  /**
    +    * Reduces the whole data set with a reducer `fun`
    +    *
    +    * @param fun The reducing function
    +    * @return A reduced data set of Ts
    +    */
    +  def reduceWith(fun: (T, T) => T): DataSet[T] =
    +    ds.reduce(fun)
    +
    +  /**
    +    * Reduces the data set group-wise with a reducer `fun`
    +    *
    +    * @param fun The reducing function
    +    * @tparam R The type of the items in the resulting data set
    +    * @return A data set of Rs reduced group-wise
    +    */
    +  def reduceGroupWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +    ds.reduceGroup {
    +      (it, out) =>
    +        out.collect(fun(it.to[Seq]))
    +    }
    +
    +  /**
    +    * Same as a reducing operation but only acts locally,
    +    * ideal to perform pre-aggregation before a reduction.
    +    *
    +    * @param fun The reducing function
    +    * @tparam R The type of the items in the resulting data set
    +    * @return A data set of Rs reduced group-wise
    +    */
    +  def combineGroupWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +    ds.combineGroup {
    +      (it, out) =>
    +        out.collect(fun(it.to[Seq]))
    --- End diff --
    
    Materialization?
;;;","14/Mar/16 12:41;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55993999
  
    --- Diff: docs/apis/scala_api_extensions.md ---
    @@ -0,0 +1,392 @@
    +---
    +title: ""Scala API Extensions""
    +# Top-level navigation
    +top-nav-group: apis
    +top-nav-pos: 11
    +---
    +<!--
    +Licensed to the Apache Software Foundation (ASF) under one
    +or more contributor license agreements.  See the NOTICE file
    +distributed with this work for additional information
    +regarding copyright ownership.  The ASF licenses this file
    +to you under the Apache License, Version 2.0 (the
    +""License""); you may not use this file except in compliance
    +with the License.  You may obtain a copy of the License at
    +
    +  http://www.apache.org/licenses/LICENSE-2.0
    +
    +Unless required by applicable law or agreed to in writing,
    +software distributed under the License is distributed on an
    +""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    +KIND, either express or implied.  See the License for the
    +specific language governing permissions and limitations
    +under the License.
    +-->
    +
    +In order to keep a fair amount of consistency between the Scala and Java APIs, some 
    +of the features that allow a high-level of expressiveness in Scala have been left
    +out from the standard APIs for both batch and streaming.
    +
    +If you want to _enjoy the full Scala experience_ you can choose to opt-in to 
    +extensions that enhance the Scala API via implicit conversions.
    +
    +To use all the available extensions, you can just add a simple `import` for the
    +DataSet API
    +
    +{% highlight scala %}
    +import org.apache.flink.api.scala.extensions._
    +{% endhighlight %}
    +
    +or the DataStream API
    +
    +{% highlight scala %}
    +import org.apache.flink.streaming.api.scala.extensions._
    +{% endhighlight %}
    +
    +Alternatively, you can import individual extensions _a-là-carte_ to only use those
    +you prefer.
    +
    +## Accept partial functions
    +
    +Normally, both the DataSet and DataStream APIs don't accept anonymous pattern
    +matching functions to deconstruct tuples, case classes or collections, like the
    +following:
    +
    +{% highlight scala %}
    +val data: DataSet[(Int, String, Double)] = // [...]
    +data.map {
    +  case (id, name, temperature) => // [...]
    +  // The previous line causes the following compilation error:
    +  // ""The argument types of an anonymous function must be fully known. (SLS 8.5)""
    +}
    +{% endhighlight %}
    +
    +This extension introduces new methods in both the DataSet and DataStream Scala API
    +that have a one-to-one correspondance in the extended API. These delegating methods 
    +do support anonymous pattern matching functions.
    +
    +#### DataSet API
    +
    +<table class=""table table-bordered"">
    +  <thead>
    +    <tr>
    +      <th class=""text-left"" style=""width: 20%"">Method</th>
    +      <th class=""text-left"" style=""width: 20%"">Original</th>
    +      <th class=""text-center"">Example</th>
    +    </tr>
    +  </thead>
    +
    +  <tbody>
    +    <tr>
    +      <td><strong>mapWith</strong></td>
    +      <td><strong>map (DataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.mapWith {
    +  case (_, value) => value.toString
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>mapPartitionWith</strong></td>
    +      <td><strong>mapPartition (DataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.mapPartitionWith {
    +  case head +: _ => head
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>flatMapWith</strong></td>
    +      <td><strong>flatMap (DataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.flatMapWith {
    +  case (_, name, visitTimes) => visitTimes.map(name -> _)
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>filterWith</strong></td>
    +      <td><strong>filter (DataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.filterWith {
    +  case Train(_, isOnTime) => isOnTime
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>reduceWith</strong></td>
    +      <td><strong>reduce (DataSet, GroupedDataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.reduceWith {
    +  case ((_, amount1), (_, amount2)) => amount1 + amount2
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>reduceGroupWith</strong></td>
    +      <td><strong>reduceGroup (GroupedDataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.reduceGroupWith {
    +  case id +: value +: _ => id -> value
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>groupingBy</strong></td>
    +      <td><strong>groupBy (DataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.groupingBy {
    +  case (id, _, _) => id
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>sortGroupWith</strong></td>
    +      <td><strong>sortGroup (GroupedDataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +grouped.sortGroupWith(Order.ASCENDING) {
    +  case House(_, value) => value
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>combineGroupWith</strong></td>
    +      <td><strong>combineGroup (GroupedDataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +grouped.combineGroupWith {
    +  case header +: amounts => amounts.sum
    +}
    +{% endhighlight %}
    +      </td>
    +    <tr>
    +      <td><strong>projecting</strong></td>
    +      <td><strong>apply (JoinDataSet, CrossDataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data1.join(data2).where(0).equalTo(1).projecting {
    +  case ((pk, tx), (products, fk)) => tx -> products
    +}
    +
    +data1.cross(data2).projecting {
    +  case ((a, _), (_, b) => a -> b
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>projecting</strong></td>
    +      <td><strong>apply (CoGroupDataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data1.coGroup(data2).where(0).equalTo(1).projecting {
    +  case (head1 +: _, head2 +: _) => head1 -> head2
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    </tr>
    +  </tbody>
    +</table>
    +
    +#### DataStream API
    +
    +<table class=""table table-bordered"">
    +  <thead>
    +    <tr>
    +      <th class=""text-left"" style=""width: 20%"">Method</th>
    +      <th class=""text-left"" style=""width: 20%"">Original</th>
    +      <th class=""text-center"">Example</th>
    +    </tr>
    +  </thead>
    +
    +  <tbody>
    +    <tr>
    +      <td><strong>mapWith</strong></td>
    +      <td><strong>map (DataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.mapWith {
    +  case (_, value) => value.toString
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>mapPartitionWith</strong></td>
    +      <td><strong>mapPartition (DataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.mapPartitionWith {
    +  case head +: _ => head
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>flatMapWith</strong></td>
    +      <td><strong>flatMap (DataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.flatMapWith {
    +  case (_, name, visits) => visits.map(name -> _)
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>filterWith</strong></td>
    +      <td><strong>filter (DataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.filterWith {
    +  case Train(_, isOnTime) => isOnTime
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>keyingBy</strong></td>
    +      <td><strong>keyBy (DataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.keyingBy {
    +  case (id, _, _) => id
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>mapWith</strong></td>
    +      <td><strong>map (ConnectedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.mapWith(
    +  map1 = case (_, value) => value.toString,
    +  map2 = case (_, _, value, _) => value + 1
    +)
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>flatMapWith</strong></td>
    +      <td><strong>flatMap (ConnectedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.flatMapWith(
    +  flatMap1 = case (_, json) => parse(json),
    +  flatMap2 = case (_, _, json, _) => parse(json)
    +)
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>keyingBy</strong></td>
    +      <td><strong>keyBy (ConnectedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.keyingBy(
    +  key1 = case (_, timestamp) => timestamp,
    +  key2 = case (id, _, _) => id
    +)
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>reduceWith</strong></td>
    +      <td><strong>reduce (KeyedDataStream, WindowedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.reduceWith {
    +  case ((_, sum1), (_, sum2) => sum1 + sum2
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>foldWith</strong></td>
    +      <td><strong>fold (KeyedDataStream, WindowedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.foldWith(User(bought = 0)) {
    +  case (User(b), (_, items)) => User(b + items.size)
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>applyWith</strong></td>
    +      <td><strong>apply (WindowedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.applyWith(0)(
    +  foldFunction = case (sum, amount) => sum + amount
    +  windowFunction = case (k, w, sum) => // [...]
    +)
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>projecting</strong></td>
    +      <td><strong>apply (JoinedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data1.join(data2).where(0).equalTo(1).projecting {
    +  case ((pk, tx), (products, fk)) => tx -> products
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +  </tbody>
    +</table>
    +
    +
    +
    +For more information on the semantics of each method, please refer to the 
    +[DataStream](batch/index.html) and [DataSet](streaming/index.html) API documentation.
    +
    +To use this extension exclusively, you can add the following `import`:
    +
    +{% highlight scala %}
    +import org.apache.flink.api.scala.extensions.acceptPartialFunctions
    --- End diff --
    
    Does this really work? Don't you have to import `o.a.f.api.scala.extensions.acceptPartialFunctionsOnDataSet` etc.?
;;;","14/Mar/16 12:43;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55994195
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/package.scala ---
    @@ -0,0 +1,201 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.extensions.acceptPartialFunctions._
    +
    +import scala.reflect.ClassTag
    +
    +package object extensions {
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataSet with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = ExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.groupingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnDataSet[T: TypeInformation](ds: DataSet[T]): OnDataSet[T] =
    --- End diff --
    
    I'm wondering whether we shouldn't overload the method `acceptPartialFunctions` with the different data set types instead of having a different method name for the different data sets (grouped, normal, etc.). I don't think that the user will want to enable partial function support on such a fine grained scale. Either he wants partial function support or not.
;;;","14/Mar/16 12:44;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55994329
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/package.scala ---
    @@ -0,0 +1,201 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.extensions.acceptPartialFunctions._
    +
    +import scala.reflect.ClassTag
    +
    +package object extensions {
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataSet with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = ExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.groupingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnDataSet[T: TypeInformation](ds: DataSet[T]): OnDataSet[T] =
    +    new OnDataSet[T](ds)
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataSet with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = ExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.groupingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    --- End diff --
    
    JavaDocs does not fit to method since it only works on `JoinDataSet`.
;;;","14/Mar/16 12:45;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55994370
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/package.scala ---
    @@ -0,0 +1,201 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.extensions.acceptPartialFunctions._
    +
    +import scala.reflect.ClassTag
    +
    +package object extensions {
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataSet with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = ExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.groupingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnDataSet[T: TypeInformation](ds: DataSet[T]): OnDataSet[T] =
    +    new OnDataSet[T](ds)
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataSet with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = ExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.groupingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnJoinDataSet[L: TypeInformation, R: TypeInformation](
    +      ds: JoinDataSet[L, R]): OnJoinDataSet[L, R] =
    +    new OnJoinDataSet[L, R](ds)
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataSet with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = ExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.groupingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    --- End diff --
    
    Same here with the JavaDocs.
;;;","14/Mar/16 12:47;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55994602
  
    --- Diff: flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/extensions/acceptPartialFunctions/OnWindowedStream.scala ---
    @@ -0,0 +1,78 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.streaming.api.scala.extensions.acceptPartialFunctions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.streaming.api.scala.{DataStream, WindowedStream}
    +import org.apache.flink.streaming.api.windowing.windows.Window
    +
    +class OnWindowedStream[T, K, W <: Window](ds: WindowedStream[T, K, W]) {
    +
    +  /**
    +    * Applies a reduce function to the window. The window function is called for each evaluation
    +    * of the window for each key individually. The output of the reduce function is interpreted
    +    * as a regular non-windowed stream.
    +    *
    +    * This window will try and pre-aggregate data as much as the window policies permit.
    +    * For example,tumbling time windows can perfectly pre-aggregate the data, meaning that only one
    +    * element per key is stored. Sliding time windows will pre-aggregate on the granularity of the
    +    * slide interval, so a few elements are stored per key (one per slide interval).
    +    * Custom windows may not be able to pre-aggregate, or may need to store extra values in an
    +    * aggregation tree.
    +    *
    +    * @param function The reduce function.
    +    * @return The data stream that is the result of applying the reduce function to the window.
    +    */
    +  def reduceWith(function: (T, T) => T) =
    +    ds.reduce(function)
    +
    +  /**
    +    * Applies the given fold function to each window. The window function is called for each
    +    * evaluation of the window for each key individually. The output of the reduce function is
    +    * interpreted as a regular non-windowed stream.
    +    *
    +    * @param function The fold function.
    +    * @return The data stream that is the result of applying the fold function to the window.
    +    */
    +  def foldWith[R: TypeInformation](initialValue: R)(function: (R, T) => R) =
    +    ds.fold(initialValue)(function)
    +
    +  /**
    +    * Applies the given window function to each window. The window function is called for each
    +    * evaluation of the window for each key individually. The output of the window function is
    +    * interpreted as a regular non-windowed stream.
    +    *
    +    * Arriving data is incrementally aggregated using the given fold function.
    +    *
    +    * @param initialValue The initial value of the fold
    +    * @param foldFunction The fold function that is used for incremental aggregation
    +    * @param windowFunction The window function.
    +    * @return The data stream that is the result of applying the window function to the window.
    +    */
    +  def applyWith[R: TypeInformation](initialValue: R)
    +                                   (foldFunction: (R, T) => R,
    +                                    windowFunction: (K, W, R) => TraversableOnce[R]):
    +      DataStream[R] =
    --- End diff --
    
    Formatting seems to be a bit off here.
;;;","14/Mar/16 12:53;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55995217
  
    --- Diff: flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/extensions/acceptPartialFunctions/OnWindowedStream.scala ---
    @@ -0,0 +1,78 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.streaming.api.scala.extensions.acceptPartialFunctions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.streaming.api.scala.{DataStream, WindowedStream}
    +import org.apache.flink.streaming.api.windowing.windows.Window
    +
    +class OnWindowedStream[T, K, W <: Window](ds: WindowedStream[T, K, W]) {
    +
    +  /**
    +    * Applies a reduce function to the window. The window function is called for each evaluation
    +    * of the window for each key individually. The output of the reduce function is interpreted
    +    * as a regular non-windowed stream.
    +    *
    +    * This window will try and pre-aggregate data as much as the window policies permit.
    +    * For example,tumbling time windows can perfectly pre-aggregate the data, meaning that only one
    +    * element per key is stored. Sliding time windows will pre-aggregate on the granularity of the
    +    * slide interval, so a few elements are stored per key (one per slide interval).
    +    * Custom windows may not be able to pre-aggregate, or may need to store extra values in an
    +    * aggregation tree.
    +    *
    +    * @param function The reduce function.
    +    * @return The data stream that is the result of applying the reduce function to the window.
    +    */
    +  def reduceWith(function: (T, T) => T) =
    +    ds.reduce(function)
    +
    +  /**
    +    * Applies the given fold function to each window. The window function is called for each
    +    * evaluation of the window for each key individually. The output of the reduce function is
    +    * interpreted as a regular non-windowed stream.
    +    *
    +    * @param function The fold function.
    +    * @return The data stream that is the result of applying the fold function to the window.
    +    */
    +  def foldWith[R: TypeInformation](initialValue: R)(function: (R, T) => R) =
    +    ds.fold(initialValue)(function)
    +
    +  /**
    +    * Applies the given window function to each window. The window function is called for each
    +    * evaluation of the window for each key individually. The output of the window function is
    +    * interpreted as a regular non-windowed stream.
    +    *
    +    * Arriving data is incrementally aggregated using the given fold function.
    +    *
    +    * @param initialValue The initial value of the fold
    +    * @param foldFunction The fold function that is used for incremental aggregation
    +    * @param windowFunction The window function.
    +    * @return The data stream that is the result of applying the window function to the window.
    +    */
    +  def applyWith[R: TypeInformation](initialValue: R)
    +                                   (foldFunction: (R, T) => R,
    +                                    windowFunction: (K, W, R) => TraversableOnce[R]):
    --- End diff --
    
    Why does the `windowFunction` work on a single `R` element and not on all elements of a window?
;;;","14/Mar/16 12:54;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55995289
  
    --- Diff: flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/extensions/package.scala ---
    @@ -0,0 +1,202 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.streaming.api.scala
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.streaming.api.scala.extensions.acceptPartialFunctions._
    +import org.apache.flink.streaming.api.windowing.windows.Window
    +
    +package object extensions {
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataStream with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.keyingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnDataStream[T: TypeInformation](ds: DataStream[T]):
    +      OnDataStream[T] =
    +    new OnDataStream[T](ds)
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataStream with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.keyingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnKeyedStream[T: TypeInformation, K](ds: KeyedStream[T, K]):
    +      OnKeyedStream[T, K] =
    +    new OnKeyedStream[T, K](ds)
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataStream with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.keyingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnJoinedStream
    +      [L: TypeInformation, R: TypeInformation, K, W <: Window](
    +      ds: JoinedStreams[L, R]#Where[K]#EqualTo#WithWindow[W]) =
    +    new OnJoinedStream[L, R, K, W](ds)
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataStream with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.keyingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnConnectedStream[IN1: TypeInformation, IN2: TypeInformation](
    +      ds: ConnectedStreams[IN1, IN2]) =
    +    new OnConnectedStream[IN1, IN2](ds)
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataStream with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.keyingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnWindowedStream[T, K, W <: Window](
    +      ds: WindowedStream[T, K, W]) =
    +    new OnWindowedStream[T, K, W](ds)
    +
    --- End diff --
    
    The same comments as for the extensions for the `DataSet` apply here.
;;;","14/Mar/16 12:56;githubbot;Github user stefanobaghino commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55995517
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/package.scala ---
    @@ -0,0 +1,201 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.extensions.acceptPartialFunctions._
    +
    +import scala.reflect.ClassTag
    +
    +package object extensions {
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataSet with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = ExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.groupingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnDataSet[T: TypeInformation](ds: DataSet[T]): OnDataSet[T] =
    --- End diff --
    
    I've tried but the implicit resolution doesn't seem to be smart enough to recognize which overloaded implicit definition to pick, so I had to fall back to this. I can make a couple of attempts using a common object with several methods or perhaps using type classes.
;;;","14/Mar/16 12:58;githubbot;Github user stefanobaghino commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55995777
  
    --- Diff: docs/apis/scala_api_extensions.md ---
    @@ -0,0 +1,392 @@
    +---
    +title: ""Scala API Extensions""
    +# Top-level navigation
    +top-nav-group: apis
    +top-nav-pos: 11
    +---
    +<!--
    +Licensed to the Apache Software Foundation (ASF) under one
    +or more contributor license agreements.  See the NOTICE file
    +distributed with this work for additional information
    +regarding copyright ownership.  The ASF licenses this file
    +to you under the Apache License, Version 2.0 (the
    +""License""); you may not use this file except in compliance
    +with the License.  You may obtain a copy of the License at
    +
    +  http://www.apache.org/licenses/LICENSE-2.0
    +
    +Unless required by applicable law or agreed to in writing,
    +software distributed under the License is distributed on an
    +""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    +KIND, either express or implied.  See the License for the
    +specific language governing permissions and limitations
    +under the License.
    +-->
    +
    +In order to keep a fair amount of consistency between the Scala and Java APIs, some 
    +of the features that allow a high-level of expressiveness in Scala have been left
    +out from the standard APIs for both batch and streaming.
    +
    +If you want to _enjoy the full Scala experience_ you can choose to opt-in to 
    +extensions that enhance the Scala API via implicit conversions.
    +
    +To use all the available extensions, you can just add a simple `import` for the
    +DataSet API
    +
    +{% highlight scala %}
    +import org.apache.flink.api.scala.extensions._
    +{% endhighlight %}
    +
    +or the DataStream API
    +
    +{% highlight scala %}
    +import org.apache.flink.streaming.api.scala.extensions._
    +{% endhighlight %}
    +
    +Alternatively, you can import individual extensions _a-là-carte_ to only use those
    +you prefer.
    +
    +## Accept partial functions
    +
    +Normally, both the DataSet and DataStream APIs don't accept anonymous pattern
    +matching functions to deconstruct tuples, case classes or collections, like the
    +following:
    +
    +{% highlight scala %}
    +val data: DataSet[(Int, String, Double)] = // [...]
    +data.map {
    +  case (id, name, temperature) => // [...]
    +  // The previous line causes the following compilation error:
    +  // ""The argument types of an anonymous function must be fully known. (SLS 8.5)""
    +}
    +{% endhighlight %}
    +
    +This extension introduces new methods in both the DataSet and DataStream Scala API
    +that have a one-to-one correspondance in the extended API. These delegating methods 
    +do support anonymous pattern matching functions.
    +
    +#### DataSet API
    +
    +<table class=""table table-bordered"">
    +  <thead>
    +    <tr>
    +      <th class=""text-left"" style=""width: 20%"">Method</th>
    +      <th class=""text-left"" style=""width: 20%"">Original</th>
    +      <th class=""text-center"">Example</th>
    +    </tr>
    +  </thead>
    +
    +  <tbody>
    +    <tr>
    +      <td><strong>mapWith</strong></td>
    +      <td><strong>map (DataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.mapWith {
    +  case (_, value) => value.toString
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>mapPartitionWith</strong></td>
    +      <td><strong>mapPartition (DataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.mapPartitionWith {
    +  case head +: _ => head
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>flatMapWith</strong></td>
    +      <td><strong>flatMap (DataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.flatMapWith {
    +  case (_, name, visitTimes) => visitTimes.map(name -> _)
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>filterWith</strong></td>
    +      <td><strong>filter (DataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.filterWith {
    +  case Train(_, isOnTime) => isOnTime
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>reduceWith</strong></td>
    +      <td><strong>reduce (DataSet, GroupedDataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.reduceWith {
    +  case ((_, amount1), (_, amount2)) => amount1 + amount2
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>reduceGroupWith</strong></td>
    +      <td><strong>reduceGroup (GroupedDataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.reduceGroupWith {
    +  case id +: value +: _ => id -> value
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>groupingBy</strong></td>
    +      <td><strong>groupBy (DataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.groupingBy {
    +  case (id, _, _) => id
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>sortGroupWith</strong></td>
    +      <td><strong>sortGroup (GroupedDataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +grouped.sortGroupWith(Order.ASCENDING) {
    +  case House(_, value) => value
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>combineGroupWith</strong></td>
    +      <td><strong>combineGroup (GroupedDataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +grouped.combineGroupWith {
    +  case header +: amounts => amounts.sum
    +}
    +{% endhighlight %}
    +      </td>
    +    <tr>
    +      <td><strong>projecting</strong></td>
    +      <td><strong>apply (JoinDataSet, CrossDataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data1.join(data2).where(0).equalTo(1).projecting {
    +  case ((pk, tx), (products, fk)) => tx -> products
    +}
    +
    +data1.cross(data2).projecting {
    +  case ((a, _), (_, b) => a -> b
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>projecting</strong></td>
    +      <td><strong>apply (CoGroupDataSet)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data1.coGroup(data2).where(0).equalTo(1).projecting {
    +  case (head1 +: _, head2 +: _) => head1 -> head2
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    </tr>
    +  </tbody>
    +</table>
    +
    +#### DataStream API
    +
    +<table class=""table table-bordered"">
    +  <thead>
    +    <tr>
    +      <th class=""text-left"" style=""width: 20%"">Method</th>
    +      <th class=""text-left"" style=""width: 20%"">Original</th>
    +      <th class=""text-center"">Example</th>
    +    </tr>
    +  </thead>
    +
    +  <tbody>
    +    <tr>
    +      <td><strong>mapWith</strong></td>
    +      <td><strong>map (DataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.mapWith {
    +  case (_, value) => value.toString
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>mapPartitionWith</strong></td>
    +      <td><strong>mapPartition (DataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.mapPartitionWith {
    +  case head +: _ => head
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>flatMapWith</strong></td>
    +      <td><strong>flatMap (DataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.flatMapWith {
    +  case (_, name, visits) => visits.map(name -> _)
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>filterWith</strong></td>
    +      <td><strong>filter (DataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.filterWith {
    +  case Train(_, isOnTime) => isOnTime
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>keyingBy</strong></td>
    +      <td><strong>keyBy (DataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.keyingBy {
    +  case (id, _, _) => id
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>mapWith</strong></td>
    +      <td><strong>map (ConnectedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.mapWith(
    +  map1 = case (_, value) => value.toString,
    +  map2 = case (_, _, value, _) => value + 1
    +)
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>flatMapWith</strong></td>
    +      <td><strong>flatMap (ConnectedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.flatMapWith(
    +  flatMap1 = case (_, json) => parse(json),
    +  flatMap2 = case (_, _, json, _) => parse(json)
    +)
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>keyingBy</strong></td>
    +      <td><strong>keyBy (ConnectedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.keyingBy(
    +  key1 = case (_, timestamp) => timestamp,
    +  key2 = case (id, _, _) => id
    +)
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>reduceWith</strong></td>
    +      <td><strong>reduce (KeyedDataStream, WindowedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.reduceWith {
    +  case ((_, sum1), (_, sum2) => sum1 + sum2
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>foldWith</strong></td>
    +      <td><strong>fold (KeyedDataStream, WindowedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.foldWith(User(bought = 0)) {
    +  case (User(b), (_, items)) => User(b + items.size)
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>applyWith</strong></td>
    +      <td><strong>apply (WindowedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data.applyWith(0)(
    +  foldFunction = case (sum, amount) => sum + amount
    +  windowFunction = case (k, w, sum) => // [...]
    +)
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +    <tr>
    +      <td><strong>projecting</strong></td>
    +      <td><strong>apply (JoinedDataStream)</strong></td>
    +      <td>
    +{% highlight scala %}
    +data1.join(data2).where(0).equalTo(1).projecting {
    +  case ((pk, tx), (products, fk)) => tx -> products
    +}
    +{% endhighlight %}
    +      </td>
    +    </tr>
    +  </tbody>
    +</table>
    +
    +
    +
    +For more information on the semantics of each method, please refer to the 
    +[DataStream](batch/index.html) and [DataSet](streaming/index.html) API documentation.
    +
    +To use this extension exclusively, you can add the following `import`:
    +
    +{% highlight scala %}
    +import org.apache.flink.api.scala.extensions.acceptPartialFunctions
    --- End diff --
    
    Yes, I wrote the docs before testing and rewriting the method signatures; good catch, thanks. I'll try to find a way to make a single import for all `acceptPartialFunctions` methods (see my reply to the next comment).
;;;","14/Mar/16 13:04;githubbot;Github user stefanobaghino commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55996335
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/OnDataSet.scala ---
    @@ -0,0 +1,104 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions.acceptPartialFunctions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.{GroupedDataSet, DataSet}
    +
    +import scala.reflect.ClassTag
    +
    +class OnDataSet[T: TypeInformation](ds: DataSet[T]) {
    +
    +  /**
    +    * Applies a function `fun` to each item of the data set
    +    *
    +    * @param fun The function to be applied to each item
    +    * @tparam R The type of the items in the returned data set
    +    * @return A dataset of R
    +    */
    +  def mapWith[R: TypeInformation: ClassTag](fun: T => R): DataSet[R] =
    +    ds.map(fun)
    +
    +  /**
    +    * Applies a function `fun` to a partition as a whole
    +    *
    +    * @param fun The function to be applied on the whole partition
    +    * @tparam R The type of the items in the returned data set
    +    * @return A dataset of R
    +    */
    +  def mapPartitionWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +    ds.mapPartition {
    +      (it, out) =>
    +        out.collect(fun(it.to[Seq]))
    --- End diff --
    
    [Yes, it does.](http://www.scala-lang.org/api/current/index.html#scala.collection.Traversable@to[Col[_]]:Col[A]) I thought in this context I was fairly safe from OOMs but I'll refactor to make it work on each item of the collection individually. Thanks!
;;;","14/Mar/16 13:13;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55997348
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/OnDataSet.scala ---
    @@ -0,0 +1,104 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions.acceptPartialFunctions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.{GroupedDataSet, DataSet}
    +
    +import scala.reflect.ClassTag
    +
    +class OnDataSet[T: TypeInformation](ds: DataSet[T]) {
    +
    +  /**
    +    * Applies a function `fun` to each item of the data set
    +    *
    +    * @param fun The function to be applied to each item
    +    * @tparam R The type of the items in the returned data set
    +    * @return A dataset of R
    +    */
    +  def mapWith[R: TypeInformation: ClassTag](fun: T => R): DataSet[R] =
    +    ds.map(fun)
    +
    +  /**
    +    * Applies a function `fun` to a partition as a whole
    +    *
    +    * @param fun The function to be applied on the whole partition
    +    * @tparam R The type of the items in the returned data set
    +    * @return A dataset of R
    +    */
    +  def mapPartitionWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +    ds.mapPartition {
    +      (it, out) =>
    +        out.collect(fun(it.to[Seq]))
    --- End diff --
    
    I think that `it.to[Seq]` can be problematic, since it can happen that an underlying `Vector` is returned here. This means that the whole iterator will be materialized. It is better imo to define `fun: Iterator[T] => R`.
;;;","14/Mar/16 13:26;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55998933
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/acceptPartialFunctions/OnDataSet.scala ---
    @@ -0,0 +1,104 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions.acceptPartialFunctions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.{GroupedDataSet, DataSet}
    +
    +import scala.reflect.ClassTag
    +
    +class OnDataSet[T: TypeInformation](ds: DataSet[T]) {
    +
    +  /**
    +    * Applies a function `fun` to each item of the data set
    +    *
    +    * @param fun The function to be applied to each item
    +    * @tparam R The type of the items in the returned data set
    +    * @return A dataset of R
    +    */
    +  def mapWith[R: TypeInformation: ClassTag](fun: T => R): DataSet[R] =
    +    ds.map(fun)
    +
    +  /**
    +    * Applies a function `fun` to a partition as a whole
    +    *
    +    * @param fun The function to be applied on the whole partition
    +    * @tparam R The type of the items in the returned data set
    +    * @return A dataset of R
    +    */
    +  def mapPartitionWith[R: TypeInformation: ClassTag](fun: Seq[T] => R): DataSet[R] =
    +    ds.mapPartition {
    +      (it, out) =>
    +        out.collect(fun(it.to[Seq]))
    --- End diff --
    
    @stefanobaghino, the idea of `mapPartition` is to process all elements but also to have the possibility to access them all from within one mapPartition call. Therefore, it should get an `Iterator[T]` as input instead of iterating over the iterator and calling the mapPartition for each element.
;;;","14/Mar/16 13:28;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r55999131
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/package.scala ---
    @@ -0,0 +1,201 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.extensions.acceptPartialFunctions._
    +
    +import scala.reflect.ClassTag
    +
    +package object extensions {
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataSet with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = ExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.groupingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnDataSet[T: TypeInformation](ds: DataSet[T]): OnDataSet[T] =
    --- End diff --
    
    Ah ok, I didn't know that Scala would have problems with implicits and method overloading. If it is not possible or it gets too complicated, then simply leave it as it is. 
;;;","14/Mar/16 13:36;githubbot;Github user stefanobaghino commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r56000181
  
    --- Diff: flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/extensions/acceptPartialFunctions/OnWindowedStream.scala ---
    @@ -0,0 +1,78 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.streaming.api.scala.extensions.acceptPartialFunctions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.streaming.api.scala.{DataStream, WindowedStream}
    +import org.apache.flink.streaming.api.windowing.windows.Window
    +
    +class OnWindowedStream[T, K, W <: Window](ds: WindowedStream[T, K, W]) {
    +
    +  /**
    +    * Applies a reduce function to the window. The window function is called for each evaluation
    +    * of the window for each key individually. The output of the reduce function is interpreted
    +    * as a regular non-windowed stream.
    +    *
    +    * This window will try and pre-aggregate data as much as the window policies permit.
    +    * For example,tumbling time windows can perfectly pre-aggregate the data, meaning that only one
    +    * element per key is stored. Sliding time windows will pre-aggregate on the granularity of the
    +    * slide interval, so a few elements are stored per key (one per slide interval).
    +    * Custom windows may not be able to pre-aggregate, or may need to store extra values in an
    +    * aggregation tree.
    +    *
    +    * @param function The reduce function.
    +    * @return The data stream that is the result of applying the reduce function to the window.
    +    */
    +  def reduceWith(function: (T, T) => T) =
    +    ds.reduce(function)
    +
    +  /**
    +    * Applies the given fold function to each window. The window function is called for each
    +    * evaluation of the window for each key individually. The output of the reduce function is
    +    * interpreted as a regular non-windowed stream.
    +    *
    +    * @param function The fold function.
    +    * @return The data stream that is the result of applying the fold function to the window.
    +    */
    +  def foldWith[R: TypeInformation](initialValue: R)(function: (R, T) => R) =
    +    ds.fold(initialValue)(function)
    +
    +  /**
    +    * Applies the given window function to each window. The window function is called for each
    +    * evaluation of the window for each key individually. The output of the window function is
    +    * interpreted as a regular non-windowed stream.
    +    *
    +    * Arriving data is incrementally aggregated using the given fold function.
    +    *
    +    * @param initialValue The initial value of the fold
    +    * @param foldFunction The fold function that is used for incremental aggregation
    +    * @param windowFunction The window function.
    +    * @return The data stream that is the result of applying the window function to the window.
    +    */
    +  def applyWith[R: TypeInformation](initialValue: R)
    +                                   (foldFunction: (R, T) => R,
    +                                    windowFunction: (K, W, R) => TraversableOnce[R]):
    --- End diff --
    
    The implementation iterates over each `R` item. However, this approach seems to contrast with [the remarks you have on `it.to[Seq]`](https://github.com/apache/flink/pull/1704#discussion_r55998933); I will restore the `Iterator` here as well. Thanks!
;;;","14/Mar/16 13:41;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-196314763
  
    @stefanobaghino, really good work :-) I think we're close to get this merged. I had some minor comments.
    
    Concerning the testing, I agree with @StephanEwen that it's not really necessary to execute a complete program for each new extension method. Instead, I think it is sufficient to check that the right operator/stream transformation has been instantiated. The operators and stream transformations should be already well tested. For the `DataSet` methods you could do something like
    
    ```
    val identityMapDs = ds.mapWith(identity)
    assertTrue(identityMapDs.javaSet.isInstanceOf[MapOperator[String, String]])
    ```
    
    And for the `DataStream` methods
    
    ```
    val identity = stream.mapWith{case x => x}
    assertTrue(identity.javaStream.getTransformation.asInstanceOf[OneInputTransformation[Int, Int]]
          .getOperator.isInstanceOf[StreamMap[Int, Int]])
    ```
;;;","14/Mar/16 13:46;githubbot;Github user tillrohrmann commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r56001580
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/package.scala ---
    @@ -0,0 +1,201 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.extensions.acceptPartialFunctions._
    +
    +import scala.reflect.ClassTag
    +
    +package object extensions {
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataSet with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = ExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.groupingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnDataSet[T: TypeInformation](ds: DataSet[T]): OnDataSet[T] =
    --- End diff --
    
    Can you remember what the problem was? I quickly changed the method names and on a first glance the tests still seem to pass.
;;;","14/Mar/16 13:59;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-196320836
  
    @tillrohrmann Thanks! Outstanding review, it's great to have some guidance when approaching a new project; thank you for the tips on testing as well, it turned out to be way more simple then I thought. :) I'll make sure to apply the changes you mentioned, review the documentation and rewrite and expand the tests.
;;;","14/Mar/16 14:01;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-196321443
  
    @tillrohrmann I just have one minor concern regarding the tests: would `isInstanceOf[StreamMap[Int, Int]]` work? Wouldn't the generic type parameters (the two `Int`s) be erased at runtime?
;;;","14/Mar/16 14:15;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-196325179
  
    Yes I think they will be removed at runtime. Thus, I guess it should also be fine to test for `isInstanceOf[StreamMap[_, _]]`. If you also want to check the input/output types properly, then you can use the `TypeInformations` which are accessible from the `OneInputTransformation`. 
;;;","22/Mar/16 14:24;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-199837531
  
    A quick status update: unfortunately I have had no time to work on this in the past week, I plan to get back to the fixes on Thursday or Friday.
    In the meanwhile, I have a small doubt on the usage of `Iterator`: they are indeed very good to allow the user to have access both to the whole `DataSet`/`DataStream` or accessing it one item at a time; however, they are not particularly useful when used with the case-style partial functions: they offer an edge to de-structure a single item like a tuple or a collection like `Seq` (e.g. using the `_ +: rest` operator to only get the item after the first).
    Are we sure we want to keep the `Iterator`? Is there an advantage in having an `Iterator` with this extension? I see to possible solutions: 
    
    1. the easy one: having two methods, one materializing the `Iterator` into a collection and another one accessing the items one a time: the only issue with this would be the need to have to methods with distinct names (otherwise we would be back to square one); this means the user can use the case-style functions to destructure the collection or each item separately; otherwise we can
    2. adopt a slightly more sophisticated solution: wrap the `Iterator` in a `Stream`, which is lazy but also fully destructurable in case-style functions (e.g.: using the `#::` operator). This would require some work as the `Iterator` is stateful with regards of the traversal while the `Stream` is not and we can't just use a naive solution or the semantic difference could lead to some nasty bugs in user code.
;;;","22/Mar/16 14:32;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-199840360
  
    Your 2. proposal looks more natural to me because what you receive as input to a user function is in fact a stream of data. So if this solution is feasible, then I would vote for it.
;;;","25/Mar/16 15:07;githubbot;Github user stefanobaghino commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r57452214
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/package.scala ---
    @@ -0,0 +1,201 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.extensions.acceptPartialFunctions._
    +
    +import scala.reflect.ClassTag
    +
    +package object extensions {
    +
    +  /**
    +    * acceptPartialFunctions extends the original DataSet with methods with unique names
    +    * that delegate to core higher-order functions (e.g. `map`) so that we can work around
    +    * the fact that overloaded methods taking functions as parameters can't accept partial
    +    * functions as well. This enables the possibility to directly apply pattern matching
    +    * to decompose inputs such as tuples, case classes and collections.
    +    *
    +    * e.g.
    +    * {{{
    +    *   object Main {
    +    *     import org.apache.flink.api.scala.extensions._
    +    *     case class Point(x: Double, y: Double)
    +    *     def main(args: Array[String]): Unit = {
    +    *       val env = ExecutionEnvironment.getExecutionEnvironment
    +    *       val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6))
    +    *       ds.filterWith {
    +    *         case Point(x, _) => x > 1
    +    *       }.reduceWith {
    +    *         case (Point(x1, y1), (Point(x2, y2))) => Point(x1 + y1, x2 + y2)
    +    *       }.mapWith {
    +    *         case Point(x, y) => (x, y)
    +    *       }.flatMapWith {
    +    *         case (x, y) => Seq('x' -> x, 'y' -> y)
    +    *       }.groupingBy {
    +    *         case (id, value) => id
    +    *       }
    +    *     }
    +    *   }
    +    * }}}
    +    *
    +    */
    +  implicit def acceptPartialFunctionsOnDataSet[T: TypeInformation](ds: DataSet[T]): OnDataSet[T] =
    --- End diff --
    
    Ok, in the end the problem was a clash between the `acceptPartialFunctions` package and implicit conversions. I've solved it by moving the actual implementations under the `impl` package under `acceptPartialFunctions`; now the whole set of conversions can be imported with the `acceptPartialFunctions` name. I was wrong regarding the issue on the resolution of overloaded implicit conversions.
;;;","25/Mar/16 15:07;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-201325574
  
    @tillrohrmann I moved forward on the batch extensions: I added support for anonymous partial functions on `where` and `equalsTo` for joins and co-group operations. I also deleted the old tests and provided a full set of unit tests for each method under the same source root (`flink-scala`).
    
    Now I have to work on the streaming and adjust the Scaladoc and documentation. I hope next week I'll have something ready to close this PR. Let me know if you prefer to have the work done so far pushed on this PR already.
;;;","29/Mar/16 08:37;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-202778390
  
    Sounds great @stefanobaghino. I think you can push your work to this PR as well since it is all related to the partial function support. Looking forward having partial function support :-)
;;;","01/Apr/16 09:54;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-204334976
  
    Note on the tests of the streaming extensions: I couldn't find a more specific class than `SingleOutputStreamOperator[_]`, thus assertion may not be very meaningful. However, I'd leave them in place to make sure further work on the extension won't break them at compile time.
;;;","01/Apr/16 18:09;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/flink/pull/1704#discussion_r58244518
  
    --- Diff: flink-scala/src/main/scala/org/apache/flink/api/scala/extensions/impl/acceptPartialFunctions/OnDataSet.scala ---
    @@ -0,0 +1,111 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.api.scala.extensions.impl.acceptPartialFunctions
    +
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.api.scala.{DataSet, GroupedDataSet}
    +
    +import scala.reflect.ClassTag
    +
    +/**
    +  * Wraps a data set, allowing to use anonymous partial functions to
    +  * perform extraction of items in a tuple, case class instance or collection
    +  *
    +  * @param ds The wrapped data set
    +  * @tparam T The type of the data set items, for which the type information must be known
    +  */
    +class OnDataSet[T: TypeInformation](ds: DataSet[T]) {
    --- End diff --
    
    I think you do not need the `TypeInformation` context bound here. Functions should never need the implicit type info for the input (that is contained in the DataSet already), only ever for the return type.
;;;","01/Apr/16 18:17;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-204500365
  
    Looks pretty good. I noticed three remaining things:
    
      - Sometimes, the package and the directory trees are different (example: `OnCoGroupDataSetTest.scala`). While Scala allows that, we usually keep them in sync.
    
      - The classes that implement the ""withX"" functions sometimes have context bounds that I think they should not require (see inline comment above). Would be great to remove them, because removing them later makes it API breaking.
    
      - We need to decide how to label this for the future: stable, or evolving. I would suggest to use `@PublicEvolving` on all the involved classes for now.
;;;","04/Apr/16 10:35;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-205234947
  
    @StephanEwen Thanks for the feedback, I've addressed the points you highlighted and took the time to add some missing Scaladoc to the `DataStream` extension.
;;;","04/Apr/16 12:13;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-205271024
  
    Looks good to me. Will merge this finally!
    
    Thanks for the hard work and the many iterations here...
;;;","04/Apr/16 13:05;githubbot;Github user stefanobaghino commented on the pull request:

    https://github.com/apache/flink/pull/1704#issuecomment-205288351
  
    It's been a pleasure, thanks for the patience and for guiding me through the iterations. :smiley: 
;;;","04/Apr/16 19:32;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/flink/pull/1704
;;;","04/Apr/16 19:33;sewen;Fixed via 5cb84f185963fa89be5d0c4e83bad66bac44d84d

Thank you for the contribution!;;;"
YarnTaskManagerRunner does not shut down,FLINK-1154,12747519,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,trohrmann,trohrmann,11/Oct/14 16:10,19/Dec/16 12:47,14/Jul/23 05:57,29/Nov/14 10:32,,,,,,,Deployment / YARN,,,,0,,,The YarnClient does not shut down the YarnTaskManagerRunner when the client receives a stop command or is terminated. This causes a resource leak on the cluster.,,githubbot,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 29 10:32:32 UTC 2014,,,,,,,,,,"0|i212if:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/14 16:29;rmetzger;Mh. I actually tested exactly this yesterday. I'll test it again.;;;","11/Oct/14 16:43;rmetzger;Which YARN version are you using?
On a Hadoop YARN 2.4 cluster, I see the container allocation on *n* nodes. Once I've stopped the session, all nodes report 0 containers.;;;","13/Oct/14 12:28;trohrmann;I tested it for YARN 2.4.0, 2.4.1 and 2.5.1 and it happened for all of them. Since it is not happening on cloud-11 I suspect that it happens due to configuration on my virtual cluster. Anyways, I think that a graceful shutdown for the task managers could be helpful in cleaning the tmp directories up, for example.;;;","13/Oct/14 12:41;rmetzger;Mh. Under normal conditions I would investigate the issue now. 
But since 
 - I'm currently not able to reproduce it (on both Amazon EMR and my local test cluster) and 
 - the next release will contain a rewritten YARN client and (with the bug fixed)
 - nobody complained about this issue so far (only your test environment)

I would vote to not spend time on fixing this issue now. If a user really runs into the issue, we can make a bugfix release later.;;;","13/Nov/14 17:36;rmetzger;I was able to reproduce the error with Hadoop 2.4.1 (not with 2.4.0).

I'm preparing a fix for this.;;;","17/Nov/14 09:58;githubbot;GitHub user rmetzger opened a pull request:

    https://github.com/apache/incubator-flink/pull/208

    [FLINK-1154] Quickfix to kill TaskManagers in YARN mode.

    This quickfix should probably only go into the 0.7 branch for the 0.7.1 release since the new Akka-based JM/TMs have this issue fixed.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rmetzger/incubator-flink flink1154

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/208.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #208
    
----
commit a95d123fdb9186ee826fc8dfaf088fcecd4d1053
Author: Robert Metzger <rmetzger@apache.org>
Date:   2014-11-13T14:00:01Z

    [FLINK-1154] Quickfix to kill TaskManagers in YARN mode.

----
;;;","17/Nov/14 11:58;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/208#discussion_r20431011
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/instance/InstanceManager.java ---
    @@ -113,6 +113,7 @@ public void shutdown() {
     			this.cleanupStaleMachines.cancel();
     
     			for (Instance i : this.registeredHostsById.values()) {
    +				i.stopInstance();
    --- End diff --
    
    I would strongly vote to remove that. There is no need to kill a TaskManager just because it lost its heartbeat. The TaskManager may very well reconnect later and be available again.
;;;","17/Nov/14 12:00;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/208#issuecomment-63295089
  
    I am not sure that the `stop()` function should go into the TaskManager interface. Until now, the TaskManager was not assumed to run in its own proccess.
    
    I think putting this into the `TaskManagerRunner` would make more sense.
;;;","17/Nov/14 12:24;githubbot;Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/208#discussion_r20431975
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/instance/InstanceManager.java ---
    @@ -113,6 +113,7 @@ public void shutdown() {
     			this.cleanupStaleMachines.cancel();
     
     			for (Instance i : this.registeredHostsById.values()) {
    +				i.stopInstance();
    --- End diff --
    
    I Agree. I'll implement it using a separate method just for YARN.
;;;","20/Nov/14 17:30;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/208#issuecomment-63846318
  
    I've updated the pull request so that the TM killing is happening in a separate call hierarchy.
    
    I also added code to the YARN Client for making it work with the Google Storage file system wrapper. Only the Flink YARN client works with GCloud storage, not the runtime (See FLINK-1266).
;;;","21/Nov/14 12:17;githubbot;Github user rmetzger closed the pull request at:

    https://github.com/apache/incubator-flink/pull/208
;;;","25/Nov/14 14:32;githubbot;GitHub user rmetzger opened a pull request:

    https://github.com/apache/incubator-flink/pull/233

    [FLINK-1154] Quickfix to kill TaskManagers in YARN mode.

    I also added code to the YARN Client for making it work with the Google Storage file system wrapper. Only the Flink YARN client works with GCloud storage, not the runtime (See FLINK-1266).
    


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rmetzger/incubator-flink flink1154

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/233.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #233
    
----
commit c0c0b97073d4233c89757679dc3f65d6dc704031
Author: Robert Metzger <rmetzger@apache.org>
Date:   2014-11-13T14:00:01Z

    [FLINK-1154] Quickfix to kill TaskManagers in YARN mode.

----
;;;","26/Nov/14 17:24;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/233#issuecomment-64680813
  
    It seems that some versions of YARN are not able to kill all YARN containers properly.
    YARN thinks the containers have stopped while they are actually are still running.
;;;","26/Nov/14 17:47;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/233#issuecomment-64684248
  
    I think this is good to merge minus my new comments.
;;;","28/Nov/14 16:51;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/233#issuecomment-64912775
  
    Thank you for the feedback. I'm addressing your comments and merging it.
;;;","29/Nov/14 10:24;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/233
;;;","29/Nov/14 10:32;rmetzger;Resolved in http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/e46d14b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed task cancellation leads to NullPointerException,FLINK-1152,12747229,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,rmetzger,rmetzger,10/Oct/14 08:17,29/Jan/17 14:42,14/Jul/23 05:57,14/Oct/14 13:58,0.7.0-incubating,,,0.7.0-incubating,,,Runtime / Coordination,,,,0,,,"As part of the testing for release 0.7-incubating, I found the following exception:

{code}
20:33:47,737 WARN  org.apache.hadoop.hdfs.DFSClient                              - Failed to connect to /130.149.21.17:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:681)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
        at org.apache.hadoop.hdfs.DFSInputStream.newTcpPeer(DFSInputStream.java:955)
        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:1107)
        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:533)
        at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1273)
        at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:722)
        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:752)
        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:793)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at org.apache.flink.runtime.fs.hdfs.DistributedDataInputStream.read(DistributedDataInputStream.java:66)
        at org.apache.flink.api.common.io.DelimitedInputFormat.fillBuffer(DelimitedInputFormat.java:616)
        at org.apache.flink.api.common.io.DelimitedInputFormat.readLine(DelimitedInputFormat.java:522)
        at org.apache.flink.api.common.io.DelimitedInputFormat.nextRecord(DelimitedInputFormat.java:488)
        at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:214)
        at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:235)
        at java.lang.Thread.run(Thread.java:745)
20:33:47,739 INFO  org.apache.flink.runtime.execution.RuntimeEnvironment         - Canceling CHAIN DataSource (TextInputFormat (hdfs:/datasets/enwiki-latest-pages-meta-current.xml) - UTF-8) -> FlatMap (org.apache.flink.examples.java.wordcount.WordCount$Tokenizer) -> Combine(SUM(1)) (215/400)
[...]
20:34:01,584 INFO  org.apache.flink.runtime.execution.RuntimeEnvironment         - Canceling CHAIN DataSource (TextInputFormat (hdfs:/datasets/generatedKMeans/centers-10mio-10dim) - UTF-8) -> Map (com.github.projectflink.testPlan.KMeansArbitraryDimension$ConvertToCentroid) (164/400)
20:34:01,584 INFO  org.apache.flink.runtime.execution.RuntimeEnvironment         - Canceling CHAIN DataSource (TextInputFormat (hdfs:/datasets/generatedKMeans/centers-10mio-10dim) - UTF-8) -> Map (com.github.projectflink.testPlan.KMeansArbitraryDimension$ConvertToCentroid) (164/400)
20:34:01,634 ERROR org.apache.flink.runtime.taskmanager.TaskManager              - Could not instantiate task
java.lang.Exception: Cannot start task. Task was canceled or failed.
        at org.apache.flink.runtime.taskmanager.TaskManager.submitTask(TaskManager.java:621)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.flink.runtime.ipc.RPC$Server.call(RPC.java:418)
        at org.apache.flink.runtime.ipc.Server$Handler.run(Server.java:947)
20:34:01,644 INFO  org.apache.flink.runtime.execution.RuntimeEnvironment         - Canceling PartialSolution (BulkIteration (Bulk Iteration)) (140/400)
20:34:01,649 ERROR org.apache.flink.runtime.util.ExecutorThreadFactory           - Thread 'Flink Executor Thread - 22' produced an uncaught exception.
java.lang.NullPointerException
        at org.apache.flink.runtime.taskmanager.TaskManager.unregisterTask(TaskManager.java:674)
        at org.apache.flink.runtime.taskmanager.TaskManager.notifyExecutionStateChange(TaskManager.java:709)
        at org.apache.flink.runtime.taskmanager.Task.cancelExecution(Task.java:222)
        at org.apache.flink.runtime.taskmanager.TaskManager$3.run(TaskManager.java:555)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
20:34:01,656 ERROR org.apache.flink.runtime.taskmanager.TaskManager              - Could not instantiate task
java.lang.Exception: Cannot start task. Task was canceled or failed.
        at org.apache.flink.runtime.taskmanager.TaskManager.submitTask(TaskManager.java:621)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.flink.runtime.ipc.RPC$Server.call(RPC.java:418)
        at org.apache.flink.runtime.ipc.Server$Handler.run(Server.java:947)
{code}",,githubbot,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 14 13:58:28 UTC 2014,,,,,,,,,,"0|i210sf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/14 14:49;sewen;The exception {{ERROR org.apache.flink.runtime.taskmanager.TaskManager - Could not instantiate task
java.lang.Exception: Cannot start task. Task was canceled or failed.}} is okay, that may happen when the cancel call intercepts the deployment. I think we should not log that error, as it is an acceptable thing to happen. Throwing an exception at that point ensures that the cleanup logic works, that was the reason for introducing it. We may change that to a {{TaskCanceledException}} and not log those, to keep the logs clean and not introduce confusing messages.;;;","13/Oct/14 14:50;sewen;This exception is a probolem
{code}
20:34:01,649 ERROR org.apache.flink.runtime.util.ExecutorThreadFactory           - Thread 'Flink Executor Thread - 22' produced an uncaught exception.
java.lang.NullPointerException
        at org.apache.flink.runtime.taskmanager.TaskManager.unregisterTask(TaskManager.java:674)
        at org.apache.flink.runtime.taskmanager.TaskManager.notifyExecutionStateChange(TaskManager.java:709)
        at org.apache.flink.runtime.taskmanager.Task.cancelExecution(Task.java:222)
        at org.apache.flink.runtime.taskmanager.TaskManager$3.run(TaskManager.java:555)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}

Happens when canceling is concurrent to startup. We should lock those sections mutually exclusive, or add checks.;;;","13/Oct/14 18:11;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/151

    [FLINK-1152] More robust resource release when tasks are canceled during deployment

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink deploy_fix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/151.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #151
    
----
commit 646323a3637294859ade4b082c3af4b377ccdcc2
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-10-13T18:07:59Z

    [FLINK-1152] More robust resource release when tasks are canceled during deployment

----
;;;","14/Oct/14 13:57;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/151#issuecomment-59048098
  
    Manually merged.
;;;","14/Oct/14 13:57;githubbot;Github user StephanEwen closed the pull request at:

    https://github.com/apache/incubator-flink/pull/151
;;;","14/Oct/14 13:58;sewen;Fixed via 4343e08f3315e83e26bc5f2152c963a75509fd4d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FilterNode set estimated data size to unknown,FLINK-1150,12747023,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fhueske,fhueske,fhueske,09/Oct/14 13:35,28/Feb/19 14:30,14/Jul/23 05:57,09/Oct/14 15:27,0.6.1-incubating,0.7.0-incubating,,0.7.0-incubating,,,API / DataSet,,,,0,,,"The default selectivity estimate of a Filter is 0.5.
During optimization, the FilterNode correctly adapts the estimated cardinality but sets the estimated data size to unknown.",,fhueske,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 15:27:07 UTC 2014,,,,,,,,,,"0|i20zjz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/14 14:16;githubbot;GitHub user fhueske opened a pull request:

    https://github.com/apache/incubator-flink/pull/146

    [FLINK-1150] Fix output size estimate of FilterNode.

    FilterNode implicitly sets the estimated output size to unknown (-1) while adjusting the cardinality by a factor of 0.5.
    This fix changes the estimated output size proportional to cardinality.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fhueske/incubator-flink fixFilterSizeEstimate

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/146.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #146
    
----
commit 913f1face084b82001df12a1bf21b7c785e0cb25
Author: Fabian Hueske <fhueske@apache.org>
Date:   2014-10-09T13:21:20Z

    [FLINK-1150] Fix size estimate for FilterNode. Data size changes proportional to cardinality.

----
;;;","09/Oct/14 15:25;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/146
;;;","09/Oct/14 15:27;fhueske;Fixed with 3eebaa8119fa41d7c673dda6cdac787ca15881a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operators cannot adapt DOP for NonParallelInputs,FLINK-1149,12747022,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fhueske,fhueske,fhueske,09/Oct/14 13:31,28/Feb/19 14:29,14/Jul/23 05:57,09/Oct/14 15:18,0.6.1-incubating,0.7.0-incubating,,0.7.0-incubating,,,,,,,0,,,"InputFormats that cannot be processed in parallel implement the NonParallelInput interface.
During optimization, the optimizer checks for this interface and sets the DOP of an operator to 1 if it is found. Other operators such as Mappers set their DOP during program construction to the DOP of their preceding task (if not specified otherwise). Since non-splittable data sources are only considered later by the optimizer, a Map operator will not have the same DOP as an preceding non-splittable data source.

The simple solution is to set the DOP of a non-splittable data source during program construction.",,fhueske,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 15:18:15 UTC 2014,,,,,,,,,,"0|i20zjr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/14 15:18;fhueske;Fixed with 986f25f6b51e3672865439e9a6581aab121b45ee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POJO Type extractor bug with type variables,FLINK-1145,12746972,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,sewen,sewen,09/Oct/14 09:23,09/Oct/14 13:00,14/Jul/23 05:57,09/Oct/14 13:00,,,,,,,,,,,0,,,"The following program incorrectly states that there are duplicate getters/setters.

{code}
	public static class Vertex<K, V> {
		
		private K key1;
		private K key2;
		private V value;
		
		public Vertex() {}
		
		public Vertex(K key, V value) {
			this.key1 = key;
			this.key2 = key;
			this.value = value;
		}
		
		public Vertex(K key1, K key2, V value) {
			this.key1 = key1;
			this.key2 = key2;
			this.value = value;
		}

		public void setKey1(K key1) {
			this.key1 = key1;
		}
		
		public void setKey2(K key2) {
			this.key2 = key2;
		}
		
		public K getKey1() {
			return key1;
		}
		
		public K getKey2() {
			return key2;
		}
		
		public void setValue(V value) {
			this.value = value;
		}
		
		public V getValue() {
			return value;
		}
	}
	
	public static void main(String[] args) throws Exception {
		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
		
		DataSet<Vertex<Long, Double>> set = env.fromElements(new Vertex<Long, Double>(0L, 3.0), new Vertex<Long, Double>(1L, 1.0));
		
		set.print();
		
		env.execute();
	}
{code}

The exception is
{code}
Exception in thread ""main"" java.lang.IllegalStateException: Detected more than one getters
	at org.apache.flink.api.java.typeutils.TypeExtractor.isValidPojoField(TypeExtractor.java:981)
	at org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo(TypeExtractor.java:1025)
	at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:937)
	at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:863)
	at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForObject(TypeExtractor.java:1146)
	at org.apache.flink.api.java.typeutils.TypeExtractor.getForObject(TypeExtractor.java:1116)
	at org.apache.flink.api.java.ExecutionEnvironment.fromElements(ExecutionEnvironment.java:466)
	at test.Test.main(Test.java:74)

{code}",,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 10:12:05 UTC 2014,,,,,,,,,,"0|i20z8v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/14 09:50;rmetzger;A fix for the getter / setter detection is currently building on travis: https://github.com/rmetzger/incubator-flink/commit/22c370d9b57798b2ad4ccb66526e64300fd73557

I have to look into getting the types for the DataSet, but as far as I know, the type erasure is throwing the types away here.
For my test, I added a VertexTyped class.
{code}
public static class VertexTyped extends Vertex<Long, Double>{
	public VertexTyped(Long l, Double d) {
		super(l, d);
	}
	public VertexTyped() {
	}
}
{code};;;","09/Oct/14 10:09;rmetzger;Fix pushed in http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/22c370d9.
;;;","09/Oct/14 10:12;sewen;I think getting the types can only work in a similar way as with tuples -> forward inferring them.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forced to use Solution Set,FLINK-1143,12746940,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,albermax,albermax,09/Oct/14 06:48,28/Feb/19 14:01,14/Jul/23 05:57,14/Oct/14 13:59,,,,0.7.0-incubating,,,Runtime / Coordination,,,,0,,,"When creating a step function for an iterate with delta function where the solution set is not used, the compiler omits an error.
In some cases this is not desirable. There should be at least a switch to disable this reinforcement.

Exceptiontrace:

org.apache.flink.compiler.CompilerException: Error: The step function does not reference the solution set.
 at org.apache.flink.compiler.PactCompiler$GraphCreatingVisitor.postVisit(PactCompiler.java:868)
 at org.apache.flink.compiler.PactCompiler$GraphCreatingVisitor.postVisit(PactCompiler.java:622)
 at org.apache.flink.api.common.operators.DualInputOperator.accept(DualInputOperator.java:283)
 at org.apache.flink.api.common.operators.SingleInputOperator.accept(SingleInputOperator.java:202)
 at org.apache.flink.api.common.operators.GenericDataSinkBase.accept(GenericDataSinkBase.java:286)
 at org.apache.flink.api.common.Plan.accept(Plan.java:281)
 at org.apache.flink.compiler.PactCompiler.compile(PactCompiler.java:517)
 at org.apache.flink.compiler.PactCompiler.compile(PactCompiler.java:466)
 at org.apache.flink.client.program.Client.getOptimizedPlan(Client.java:196)
 at org.apache.flink.client.program.Client.getOptimizedPlan(Client.java:209)
 at org.apache.flink.client.program.Client.run(Client.java:285)
 at org.apache.flink.client.program.Client.run(Client.java:230)
 at org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:347)
 at org.apache.flink.client.CliFrontend.run(CliFrontend.java:334)
 at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1001)
 at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1025)",,albermax,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 14 13:59:04 UTC 2014,,,,,,,,,,"0|i20z1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/14 19:44;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/153

    [FLINK-1143] Allow delta iterations that do not join with the solution set

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink delta_no_solution_set

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/153.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #153
    
----
commit 145a5c2a6890f13edc557b0cc30e1722bd8c35cb
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-10-13T18:24:35Z

    Fix various deprecation warnings in tests

commit c277435bda432d01a1774d9942f0300bd874bc02
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-10-13T19:21:54Z

    [FLINK-1143] Allow delta iterations that do not join with the solution set

----
;;;","14/Oct/14 13:56;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/153#issuecomment-59048033
  
    Manually merged.
;;;","14/Oct/14 13:56;githubbot;Github user StephanEwen closed the pull request at:

    https://github.com/apache/incubator-flink/pull/153
;;;","14/Oct/14 13:59;sewen;Fixed via bf20591b10373d2e742c0e49b8a9506f530e296e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selfjoin fails after DataSet exceeds certain size,FLINK-1141,12746690,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,rwaury,rwaury,08/Oct/14 11:01,02/Oct/19 17:42,14/Jul/23 05:57,19/Mar/15 10:35,0.6.1-incubating,0.7.0-incubating,,0.9,,,Runtime / Coordination,Runtime / Task,,,0,,,"en.As soon as a DataSet exceeds a certain size (1000000 tuples in my example) a Selfjoin with a FlatJoinFunction no longer works. After around a second the Join, DataSource and DataSink threads are all in Wait and don't perform any work (no output files are created) and the job never finishes.

If I cut the input size in half it works fine.

My current workaround is to create the DataSet twice and join the two identical DataSets.",LocalExecutionEnvironment (dop=4),aljoscha,fhueske,rwaury,sewen,uce,,,,,,,,,,,,,,,,,,,FLINK-1350,"08/Oct/14 11:04;rwaury;LargeSelfJoin.java;https://issues.apache.org/jira/secure/attachment/12673578/LargeSelfJoin.java","16/Oct/14 12:16;rwaury;execution_plan.json;https://issues.apache.org/jira/secure/attachment/12675268/execution_plan.json",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 10:35:30 UTC 2015,,,,,,,,,,"0|i20xjb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/14 11:04;rwaury;You might need to change the value of size to recreate the bug.;;;","08/Oct/14 11:12;uce;I don't know how this applies to a self-join, but it is in general deadlock-prone to send an intermediate result to more than one consumer in a pipelined fashion. If that applies to the self-join as well, this will be resolved when we go for the blocking shuffles (finish intermediate data set, then start sending) for all intermediate data sets, which have more than one consumer. This will happen very soon (would be a good test case actually ;)).

If it is important to you to work with the larger data set right now, we can go for a quick fix. Otherwise, I would ask you to wait a week.;;;","08/Oct/14 12:42;rwaury;I doesn't have a high priority right now since I have a wasteful but easy workaround.

Will the blocking shuffles be included in 0.7 or 0.8?;;;","08/Oct/14 13:18;uce;Unfortunately 0.8. [~sewen] replied to this thread via email (not mirrored to JIRA):

{quote}
Can you add a compiler hint that forces a merge-join? That one is not deadlock prone...
{quote};;;","13/Oct/14 16:36;sewen;Robert, can you try a merge join instead? Also, what is the plan the produces this error? I think we should catch the majority of the simple streaming deadlocks by now...;;;","16/Oct/14 11:38;rwaury;I tried using the REPARTITION_SORT_MERGE hint but even in a LocalExecutionEnvironment with DOP set to 1 it still doesn't work.

So far the only method that worked was creating two new data sets for every join (i.e. no data set is used for a join twice).

But this also caused some issues on the cluster (YARN). It would work fine with 14 machines but deadlock with 16.

But this workaround becomes less feasible the more joins I add (most of them are just broadcast joins to add data from different sources to my main data set).;;;","16/Oct/14 12:02;fhueske;Can you post or attach the JSON plan (of a minimal version) that produces the deadlock?

You get it via {{ExecutionEnvironment.getExecutionPlan();}};;;","16/Oct/14 12:16;rwaury;It seems to ignore my join hints.

This was tested with the current 0.7-incubating-SNAPSHOT.;;;","16/Oct/14 12:56;fhueske;Are you testing with the latest master? 
There was a [commit|https://github.com/apache/incubator-flink/commit/025589f0b1dd64acba8d5e6de066b1555101bafd] two days ago that forwards join hints to the optimizer.

The plan you attached shows that one input of the join goes into a pipeline breaker, which is added by the optimizer to actually avoid deadlocks. 
The plan might deadlock because the optimizer accidentially adds the p-breaker to the build input instead of the probe input. Will need to check what's going on there.;;;","16/Oct/14 13:15;rwaury;Thanks,

that was it. Stephan's workaround now works.;;;","26/Nov/14 16:30;aljoscha;Is this still an issue? Or can we close this?
;;;","27/Nov/14 09:16;rwaury;I just tested it with the current master. It still doesn't work without the workarounds.;;;","19/Mar/15 10:35;uce;Fixed in 9d7acf3, 9c77f07.

Added a test case based on your code. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopOutputFormat fails for DOP > 1,FLINK-1139,12746153,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fhueske,fhueske,fhueske,06/Oct/14 14:22,16/Jun/16 17:07,14/Jul/23 05:57,08/Dec/14 09:32,,,,0.7.1-incubating,0.8.0,,API / DataSet,,,,0,,,"HadoopOutputFormat fails for DOP > 1. The reason is that job finalization (removal of the temp directory) is done after the first task finishes.
All other tasks will fail in that moment, since they cannot continue to write their data to the temp directory.",,fhueske,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 08 09:32:25 UTC 2014,,,,,,,,,,"0|i20uaf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/14 11:33;githubbot;GitHub user qmlmoon opened a pull request:

    https://github.com/apache/incubator-flink/pull/157

    [FLINK-1139] Fix HadoopOutputFormat for DOP>1

    This fix leaves the _temporary directory unremoved. It would be better if we could remove the temp directory after all tasks finished.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/qmlmoon/incubator-flink hadoop

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/157.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #157
    
----
commit 6512e3a2f5e02bf01e5839d36380b9db741e540d
Author: mingliang <qmlmoon@gmail.com>
Date:   2014-10-17T11:17:45Z

    Fix HadoopOutputFormat for DOP>1

----
;;;","17/Oct/14 11:44;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/157#issuecomment-59501331
  
    The problem with this issue is, that you don't know what is happening inside of the {{jobCommit()}} method.
    For Hadoop FileOutputFormats it only removes the temp directory but other formats might have important logic in there. So removing that call is not a good solution.
    
    I am currently working on another solution that adds a hook to the JobManager which is called after all output tasks finished. Will open a PR soon.
;;;","17/Oct/14 11:45;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/157#issuecomment-59501414
  
    And sorry that I forgot to assign the issue to myself :-/
;;;","17/Oct/14 11:45;githubbot;Github user qmlmoon commented on the pull request:

    https://github.com/apache/incubator-flink/pull/157#issuecomment-59501445
  
    ok. then that's better to have the hook.
;;;","20/Oct/14 07:47;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/157#issuecomment-59697722
  
    Would you close this PR? You are the only one who can do that without merging it ;-)
    Thanks!
;;;","20/Oct/14 07:48;githubbot;Github user qmlmoon closed the pull request at:

    https://github.com/apache/incubator-flink/pull/157
;;;","03/Nov/14 11:03;githubbot;GitHub user fhueske opened a pull request:

    https://github.com/apache/incubator-flink/pull/173

    [FLINK-1139] Fix for HadoopOF with DOP > 1 

    This PR allows to execute HadoopOFs with DOP > 1.
    It adds a hook to JM that is executed after the last OutputFormat finished. 
    Hadoop FileOutputFormats need this hook to clean-up temporary files.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fhueske/incubator-flink hadoopOF

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/173.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #173
    
----
commit 8f7ba0869d678d1f2f5340fa52b337a0ce8b7d34
Author: Fabian Hueske <fhueske@apache.org>
Date:   2014-10-06T14:27:25Z

    [FLINK-1139] Added FinalizeOnMaster hook to run code after the last task of an OutputFormat completed

commit 99a1c9ca204c1c9183a537e46c2fa01ad3632e4f
Author: Fabian Hueske <fhueske@apache.org>
Date:   2014-10-06T14:28:00Z

    [FLINK-1139] Fixed HadoopOutputFormat to run with DOP > 1

----
;;;","10/Nov/14 10:27;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/173#issuecomment-62366100
  
    Looks good.
;;;","21/Nov/14 09:14;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/173#issuecomment-63943775
  
    Any further comments? Otherwise, I'll merge it in the next days.
;;;","21/Nov/14 11:06;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/173#discussion_r20709141
  
    --- Diff: flink-addons/flink-hadoop-compatibility/src/main/java/org/apache/flink/hadoopcompatibility/mapred/HadoopOutputFormat.java ---
    @@ -141,7 +142,20 @@ public void close() throws IOException {
     		if (this.fileOutputCommitter.needsTaskCommit(this.context)) {
     			this.fileOutputCommitter.commitTask(this.context);
     		}
    -		this.fileOutputCommitter.commitJob(this.jobContext);
    +	}
    +	
    +	@Override
    +	public void finalizeGlobal(int parallelism) throws IOException {
    +
    +		try {
    +			this.jobContext = HadoopUtils.instantiateJobContext(this.jobConf, new JobID());
    --- End diff --
    
    This method is the last thing that runs from an operator. I would make all variables stack variables, not fields of the class.
;;;","21/Nov/14 11:08;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/173#discussion_r20709227
  
    --- Diff: flink-core/src/main/java/org/apache/flink/api/common/operators/GenericDataSinkBase.java ---
    @@ -292,13 +294,23 @@ public void accept(Visitor<Operator<?>> visitor) {
     	
     	protected void executeOnCollections(List<IN> inputData) throws Exception {
     		OutputFormat<IN> format = this.formatWrapper.getUserCodeObject();
    +		
    +		if(format instanceof InitializeOnMaster) {
    --- End diff --
    
    Good catch!
;;;","21/Nov/14 11:14;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/173#discussion_r20709391
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionJobVertex.java ---
    @@ -282,6 +282,14 @@ private void subtaskInFinalState(int subtask) {
     				numSubtasksInFinalState++;
     				
     				if (numSubtasksInFinalState == parallelism) {
    +					
    --- End diff --
    
    BLOCKER: This here introduces a critical bug. This function is not meant to abort with an exception. It should be
    ```
    try {
        getJobVertex().finalizeOnMaster(getGraph().getUserClassLoader());
    }
    catch (Throwable t) {
        getGraph().fail(t);
    ```
;;;","21/Nov/14 11:16;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/173#issuecomment-63957071
  
    This adds a hook at a very sensitive point. There one blocking issue that can cause the job to hang.
    
    I would also not merge this without a test where the `finalizeGlobal()` method throws an exception. That way we can be sure that the system handles such failures properly.
;;;","22/Nov/14 22:15;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/173#issuecomment-64097626
  
    Replaced exception by missing `ExecutionGraph.fail()`
    Added test case with failing `finalizeOnMaster()` call.
;;;","25/Nov/14 08:37;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/173#issuecomment-64325764
  
    Only the last two commits are part of this PR. The other commit sneaked in because the Github mirror was not synced with the Apache git.
;;;","07/Dec/14 23:32;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/173#issuecomment-65961213
  
    This looks good now.
    
    +1 to merge
;;;","08/Dec/14 08:48;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/173#issuecomment-66038696
  
    Thanks! Will merge now
;;;","08/Dec/14 09:30;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/173
;;;","08/Dec/14 09:32;fhueske;Fixed with 15f58bb23c657951e2de48e6436820a093b393e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"On the client side, the JVM does not terminate using RemoteCollectorOutputFormat",FLINK-1134,12745267,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fatschi,fatschi,fatschi,01/Oct/14 14:44,13/Apr/21 20:39,14/Jul/23 05:57,03/Oct/14 16:25,0.6-incubating,,,0.7.0-incubating,,,,,,,0,,,"The client program does not shutdown after executing a job, because the threads spawned by JAVA RMI prevent it from doing so. The reason is the missing unbind() and UnicastRemoteObject.unexportObject() for the exported collector objects.",,fatschi,githubbot,rmetzger,sewen,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 03 16:25:44 UTC 2014,,,,,,,,,,"0|i20os7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Oct/14 14:59;githubbot;GitHub user fatschi opened a pull request:

    https://github.com/apache/incubator-flink/pull/138

    Flink 1134

    fix for https://issues.apache.org/jira/browse/FLINK-1134

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fatschi/incubator-flink FLINK_1134

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/138.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #138
    
----
commit d28c8412e465088f7077d45106534e8d8ac8b56a
Author: Fabian Tschirschnitz <fatschi@googlemail.com>
Date:   2014-10-01T14:46:54Z

    [FLINK-1134] tried to fix issue with not terminating JVM when using RemoteCollectorOutputFormat
    
    Conflicts:
    	flink-java/src/main/java/org/apache/flink/api/java/io/RemoteCollectorImpl.java

commit b58f337e4e2e07066d6588214d7c2d50cd062fe5
Author: Fabian Tschirschnitz <fatschi@googlemail.com>
Date:   2014-10-01T14:57:25Z

    [FLINK-1134] fixed javadocs

----
;;;","01/Oct/14 14:59;fatschi;please see https://github.com/apache/incubator-flink/pull/138 for possible fix.;;;","03/Oct/14 07:37;rmetzger;Hi Fabian,

thank you for the fix. I've assigned you to this JIRA issue. Therefore, I gave you more permissions on JIRA. You should be able to assign yourself to issues etc.;;;","03/Oct/14 16:25;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/138
;;;","03/Oct/14 16:25;sewen;Fixed via a0057f9d8803be938959be55a826440ea130ac79

Thanks for the patch!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Type extractor cannot determine type of function,FLINK-1133,12745240,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,sewen,sewen,01/Oct/14 12:41,28/Feb/19 14:29,14/Jul/23 05:57,01/Oct/14 21:41,0.7.0-incubating,,,0.7.0-incubating,,,,,,,0,,,"This function fails in the type extractor.

{code}
public static final class DuplicateValue<T> implements MapFunction<Tuple1<T>, Tuple2<T, T>> {
		
	@Override
	public Tuple2<T, T> map(Tuple1<T> vertex) {
		return new Tuple2<T, T>(vertex.f0, vertex.f0);
	}
}
{code}",,githubbot,sewen,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 01 21:41:11 UTC 2014,,,,,,,,,,"0|i20om7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Oct/14 13:03;twalthr;I will look into that.;;;","01/Oct/14 14:51;githubbot;GitHub user twalthr opened a pull request:

    https://github.com/apache/incubator-flink/pull/137

    [FLINK-1133] TypeExtractor resolves also variables inside Tuple-input

    Until now the TypeExtractor only supported simple input derivation from functions like ""Function<T, TupleXY<T,..>>"".
    
    This PR adds the support for functions with variables in input tuples like ""Function<TupleXY<T,...>, T>.""

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/twalthr/incubator-flink FLINK-1133

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/137.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #137
    
----
commit a294da34152c690ab94673ef0d9f9101a6544555
Author: twalthr <info@twalthr.com>
Date:   2014-10-01T14:40:42Z

    TypeExtractor resolves also variables inside Tuple-input

----
;;;","01/Oct/14 15:00;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/137#issuecomment-57477467
  
    Cool! That was a very quick fix!
;;;","01/Oct/14 15:04;githubbot;Github user twalthr commented on the pull request:

    https://github.com/apache/incubator-flink/pull/137#issuecomment-57478197
  
    Too quick. I think I could also do it recursively. A commit will follow..
;;;","01/Oct/14 15:28;githubbot;Github user twalthr commented on the pull request:

    https://github.com/apache/incubator-flink/pull/137#issuecomment-57482859
  
    Now we also support nested tuples: `Function<Tuple1<Tuple1<T>>, T>`
;;;","01/Oct/14 21:27;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/137
;;;","01/Oct/14 21:41;sewen;Fixed in 02c08456410689e10acb53bd7c33d6fdaeb671fb

Thank you for the patch!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Plan Visualizer Cuts of the Lower Part of Certain Operators,FLINK-1129,12744234,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,aljoscha,aljoscha,26/Sep/14 08:23,08/Oct/15 15:39,14/Jul/23 05:57,08/Oct/15 15:39,,,,0.10.0,,,,,,,0,,,,,aljoscha,uce,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/14 08:34;aljoscha;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12671409/screenshot-1.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 08 15:39:12 UTC 2015,,,,,,,,,,"0|i20ihr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/14 10:51;aljoscha;This happens on OSX on Chrome and Safari.;;;","25/Aug/15 14:02;uce;This has been fixed with the new runtime interface for 0.10. I don't think that it will be back ported to 0.9.1. If someone has time to do this, please change the fix version accordingly and port it back.;;;","08/Oct/15 15:39;aljoscha;This is resolved by the new JobManager web interface.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamComponentTest fails with NoResourceAvailableException ,FLINK-1119,12743605,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,fhueske,fhueske,23/Sep/14 20:20,28/Feb/19 11:16,14/Jul/23 05:57,23/Sep/14 23:50,0.7.0-incubating,,,0.7.0-incubating,,,,,,,0,,,"On my machine, the current master branch (commit f329fe2b654eea078de085442d68dd1d5a9fc09a) fails to build due to two tests in error in StreamComponentTest.
Both tests fail with a NoResourceAvailableException:

{code}
22:08:56.779 [IPC Server handler 0 on 1624] ERROR org.apache.flink.runtime.jobmanager.JobManager  - Job submission failed.
org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Not enough free slots available to run the job. You can decrease the operator parallelism or increase the number of slots per TaskManager in the configuration.
	at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleTask(Scheduler.java:201) ~[flink-runtime-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleImmediately(Scheduler.java:116) ~[flink-runtime-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:203) ~[flink-runtime-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:326) ~[flink-runtime-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.scheduleAll(ExecutionJobVertex.java:238) ~[flink-runtime-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.scheduleForExecution(ExecutionGraph.java:293) ~[flink-runtime-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at org.apache.flink.runtime.jobmanager.JobManager.submitJob(JobManager.java:377) ~[flink-runtime-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_67]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_67]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_67]
	at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_67]
	at org.apache.flink.runtime.ipc.RPC$Server.call(RPC.java:418) [flink-runtime-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at org.apache.flink.runtime.ipc.Server$Handler.run(Server.java:947) [flink-runtime-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
{code}",,fhueske,gyfora,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 23 23:50:33 UTC 2014,,,,,,,,,,"0|i20eov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/14 21:02;gyfora;That's very interesting because the the parallelism for all operators are set to 1 and so is the number of slots. It runs for me and it ran on travis too, always. And it hasn't changed for a very long time.;;;","23/Sep/14 21:04;gyfora;Maybe you could try playing with the parallelism settings a little bit, see if anything changes.;;;","23/Sep/14 21:17;fhueske;Yeah, it's a bit mysterious. 
When I run the test in Eclipse it works fine, but if I do {{mvn clean verify}} it failed 4 out of 4 times.

And due to FLINK-1104 it's a bit painful to debug because you need to wait for approx. 15 minutes to find out that nothing changed ;-);;;","23/Sep/14 21:36;gyfora;Hm, when I run mvn clean verify, I get the same results, it fails .

mvn package/install works just fine.

Btw you probably meant [FLINK-1114|https://issues.apache.org/jira/browse/FLINK-1114] :);;;","23/Sep/14 21:39;fhueske;Thanks for the hint! Will try {{mvn clean install}}. :-)
But anyways, we should try to find out what's going on there...

And yes, I meant FLINK-1114 ;-);;;","23/Sep/14 21:44;sewen;I have added additional debug output in the test and it fails saying that not a single task manager is registered.

I suspect that there is a race in the startup of the test minicluster.

Let me push that and a fix to [FLINK-1114].;;;","23/Sep/14 21:53;fhueske;Nice, that would be great!
Thanks!;;;","23/Sep/14 23:09;sewen;Okay, the check {{waitForJobManagerToBecomeReady(int numSlots)}} does not properly wait for the system to be started, because {{numSlots}} is frequently {{-1}}.

That's why it may happen that jobs are started before any taskmanager is internally registered.

TaskManagers register through the heartbeat thread, and this registration and the job submisson may race.;;;","23/Sep/14 23:16;sewen;I think I have a fix, test are running right now...;;;","23/Sep/14 23:50;sewen;Fixed in 68801f2d153a18b7d0fab0238abfd6fbf1091a75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up Avro Project,FLINK-1117,12743563,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,23/Sep/14 17:58,30/Sep/14 18:00,14/Jul/23 05:57,30/Sep/14 18:00,0.7.0-incubating,,,0.7.0-incubating,,,,,,,0,,,I vote to throw out Avro support for the deprecated Record API and remove unnecessary classes like the AvroBaseValue,,sekruse,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 30 18:00:51 UTC 2014,,,,,,,,,,"0|i20efr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Sep/14 08:17;sekruse;I actually still use the records. To me they seem very useful when you have to process ""tuples"" but do not know the arity of that tuples at coding time. I encounter this when profiling arbitrary CSV files.

Tuples have not been an alternative so far, as they only take 25 fields max. and moreover (but I am not completely sure) there have been issues when using the abstract {{Tuple}} class as {{DataSet}} type parameter.;;;","24/Sep/14 09:17;sewen;Are you using the record data type, or the record API?;;;","24/Sep/14 09:22;sekruse;Hm, I am using the {{Record}} datatype and accordingly the {{org.apache.flink.api.java.record.io.CsvInputFormat}}. I am not completely sure what you refer to by record API, but I hope that answers your question.;;;","24/Sep/14 14:57;sewen;I think that means you are using the new API and the Record as the data type. So your stuff would keep working...;;;","30/Sep/14 18:00;sewen;Done in 626d6b785db649e06951e2f336f5ca411b30dce5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spurious Test Failures due to failing file output streams,FLINK-1115,12743386,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,23/Sep/14 00:07,23/Sep/14 21:54,14/Jul/23 05:57,23/Sep/14 21:54,0.7.0-incubating,,,0.7.0-incubating,,,Runtime / Coordination,,,,0,,,"Once in a while, a test fails with the stack trace blow, failing to open a file output stream. I am not fully sure what causes that, but I suspect a race between creating/clearing the output directory, and the file stream creation.

We could simply have one or two retries when the local file system' output stream creation fails with ""FileNotFoundException""

-----

23:39:57.272 [DataSink(Output) (1/4)] ERROR org.apache.flink.runtime.operators.DataSinkTask  - Error in user code: /tmp/org.apache.flink.test.recordJobTests.TPCHQuery10ITCase-result.txt/1 (No such file or directory):  DataSink(Output) (1/4)
java.io.FileNotFoundException: /tmp/org.apache.flink.test.recordJobTests.TPCHQuery10ITCase-result.txt/1 (No such file or directory)
	at java.io.FileOutputStream.open(Native Method) ~[na:1.8.0_05]
	at java.io.FileOutputStream.<init>(FileOutputStream.java:206) ~[na:1.8.0_05]
	at java.io.FileOutputStream.<init>(FileOutputStream.java:156) ~[na:1.8.0_05]
	at org.apache.flink.core.fs.local.LocalDataOutputStream.<init>(LocalDataOutputStream.java:50) ~[flink-core-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at org.apache.flink.core.fs.local.LocalFileSystem.create(LocalFileSystem.java:247) ~[flink-core-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at org.apache.flink.core.fs.local.LocalFileSystem.create(LocalFileSystem.java:254) ~[flink-core-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:239) ~[flink-core-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:174) ~[flink-runtime-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:235) [flink-runtime-0.7-incubating-SNAPSHOT.jar:0.7-incubating-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_05]",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 23 21:54:50 UTC 2014,,,,,,,,,,"0|i20dav:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/14 21:54;sewen;Fixed via 1ddec930a29c1d870d5b5bbde0098d10ff9b45ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScalaStyle prevents projects from being built individually,FLINK-1114,12743375,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,22/Sep/14 23:21,23/Sep/14 21:54,14/Jul/23 05:57,23/Sep/14 21:54,,,,0.7.0-incubating,,,Build System,,,,0,,,"Maven project building and verification fails, unless all are built together.

Message:

Failed to execute goal org.scalastyle:scalastyle-maven-plugin:0.5.0:check (default) on project flink-spargel: Failed during scalastyle execution: Unable to find configuration file at location tools/maven/scalastyle-config.xml",,fhueske,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 23 21:54:30 UTC 2014,,,,,,,,,,"0|i20d8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/14 20:03;fhueske;I ran into this problem as well. 
Very annoying, especially if you're not working on a powerful dev machine... :-(;;;","23/Sep/14 20:25;sewen;I have a fix for this issue;;;","23/Sep/14 21:54;sewen;Fixed via cf80d8627728a719c3ef2be8f31ef77077b9d200;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up JQuery Dependencies,FLINK-1113,12743319,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,sewen,sewen,22/Sep/14 20:33,10/Nov/14 16:11,14/Jul/23 05:57,10/Nov/14 15:35,,,,0.8.0,,,Runtime / Web Frontend,,,,0,,,"There are currently three different versions of JQuery in the webfrontend:

  - 1.4.2 (JobManager WebFrontend - jquery.js)
  - 1.10.2 (JobManager WebFrontend - jquery-1.10.2.js)
  - 2.1.0 (WebClient jquery-2.1.0.js)",,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 10 16:11:53 UTC 2014,,,,,,,,,,"0|i20cw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/14 10:16;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/178#issuecomment-61785999
  
    Looks good.
    
    We also need to update the mentions in the LICENSE and NOTICE files (both in the source root directory and in the distribution directory) and the DEPENDENCIES file.
;;;","07/Nov/14 10:21;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/178#issuecomment-62123722
  
    Looks good, will merge...
;;;","07/Nov/14 10:22;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/178#issuecomment-62123750
  
    Thank you for the patch!
;;;","10/Nov/14 15:35;sewen;Merged in 1e979e90f1fef9a38e6e058658f6cb86db473362

Thank you for the patch!;;;","10/Nov/14 16:11;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/178
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected Histogram accumulator behavior,FLINK-1096,12740543,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sekruse,sekruse,sekruse,10/Sep/14 16:37,13/Apr/21 20:39,14/Jul/23 05:57,21/Sep/14 18:28,,,,0.7.0-incubating,,,,,,,0,,,"The {{Histogram}} accumulator uses the given value as both key and value for the histogram map. Hence, the histogram always contains entries like {{key -> n * key}} where {{n}} is the number of times that {{key}} has been added to the histogram.
Supposedly, adding {{key}} into the histogram {{n}} times should create the entr {{key -> n}} instead.",,fhueske,githubbot,sekruse,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 21 18:28:30 UTC 2014,,,,,,,,,,"0|i1zvzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/14 17:01;githubbot;GitHub user sekruse opened a pull request:

    https://github.com/apache/incubator-flink/pull/117

    [FLINK-1096] Correction to histogram accumulator

    * each key is associated with the number of times it was inserted into the accumulator
    * backed histogram with a tree map to present the entries sorted by key

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sekruse/incubator-flink FLINK-1096

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/117.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #117
    
----
commit 5080784896b8265bb58bf5c3fb471552c1277b37
Author: Sebastian Kruse <sebastian.kruse@hpi.de>
Date:   2014-09-10T16:59:09Z

    [FLINK-1096] Correction to histogram accumulator
    
    * each key is associated with the number of times it was inserted into the accumulator
    * backed histogram with a tree map to present the entries sorted by key

----
;;;","11/Sep/14 13:56;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/117#issuecomment-55266868
  
    Thanks for the PR.
    LGTM
;;;","11/Sep/14 13:59;fhueske;I agree. I would also expect the count instead of the sum.
+1 for changing this.

Does it make sense to make the Histogram more generic and support all comparable data types?;;;","11/Sep/14 15:02;sekruse;That seems to be a good idea. You have to provide a write and read function for accumulators, though. For user-defined keys, you would need user-defined serialization functions as well. Moreover, I think that counted values are in general not comparable, e.g., some kind of arrays or so.

I would suggest to have some standard histograms, e.g. for longs/ints and strings, and maybe an abstract base class for user-defined keys.;;;","11/Sep/14 15:21;githubbot;Github user sekruse commented on the pull request:

    https://github.com/apache/incubator-flink/pull/117#issuecomment-55279816
  
    Oh, indeed, I seem to have missed a test. I changed that one and now ran a {{mvn test}} successfully on my machine.
;;;","11/Sep/14 15:24;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/117#issuecomment-55280405
  
    Integration tests are only triggered with ``mvn verify``.
    Not sure if there are any that depend on the changed code.
;;;","11/Sep/14 15:51;githubbot;Github user sekruse commented on the pull request:

    https://github.com/apache/incubator-flink/pull/117#issuecomment-55284546
  
    Ah okay, the more you know :smiley: 
    
    Unfortunately, on my Windows, there seems to be an issue with the deletion of a temp file, so that I cannot actually run the whole integration tests there. On travis, it seems to be passing now, though.
;;;","17/Sep/14 15:21;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/117#issuecomment-55910128
  
    Do you think its difficult to fix the temp file deletion? Would be cool to have support for Windows as well
;;;","17/Sep/14 15:24;githubbot;Github user sekruse commented on the pull request:

    https://github.com/apache/incubator-flink/pull/117#issuecomment-55910613
  
    I don't think that this issue is caused by the histogram, it is just a problem with one of the integration tests. I can report that issue, though.
;;;","17/Sep/14 15:28;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/117#issuecomment-55911250
  
    I think adding a JIRA ticket for the issue would be great.
;;;","17/Sep/14 18:49;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/117#issuecomment-55941511
  
    I think, there is already a ticket for this issue (FLINK-98).
    Please check and add a comment if you think something is missing.
;;;","21/Sep/14 17:56;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/117#issuecomment-56306783
  
    This looks good to me, I will merge this...
;;;","21/Sep/14 18:28;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/117
;;;","21/Sep/14 18:28;sewen;Fixed in 9463e27bc5142b3f72ee26e5eb25de9dac1b0125;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
./flink info -d command is not working for the examples,FLINK-1095,12740241,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,rmetzger,rmetzger,09/Sep/14 21:48,27/Nov/14 17:58,14/Jul/23 05:57,27/Nov/14 17:58,0.6-incubating,0.7.0-incubating,,0.8.0,,,,,,,0,starter,,"It seems that the {{info -d}} is not working for all of our examples and its probably not documented anywhere.


{code}
./flink info -d ../examples/flink-java-examples-0.6-incubating-rc1-WebLogAnalysis.jar
No description available for this program.
{code}

We should consider removing the option.",,fhueske,githubbot,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 27 17:58:23 UTC 2014,,,,,,,,,,"0|i1zupr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/14 09:15;sewen;This option looks if the program implements ""ProgramDescription"". For the sake of simplicity, we have removed that from the examples.

I vote to remove the option from the CLI.;;;","11/Sep/14 14:09;fhueske;+1 for removing;;;","09/Nov/14 19:05;githubbot;GitHub user shivateja opened a pull request:

    https://github.com/apache/incubator-flink/pull/191

    [FLINK-1095] Remove info option from CliFrontend

    Tested by manually running tests

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/shivateja/incubator-flink flink1905

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/191.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #191
    
----
commit 9afda4128948e542f3194019fd0c03e3f705e57d
Author: Shiva Teja Reddy <shivatejau@gmail.com>
Date:   2014-11-09T18:57:52Z

    [FLINK-1095] Remove info option from CliFrontend

----
;;;","10/Nov/14 09:11;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/191#issuecomment-62358081
  
    Hi,
    
    thank you for working on this issue! I'm not sure if the goal of the issue was to remove the whole ""info"" command. I thought it was only about removing the ""-d"" from ""info"" ?
;;;","10/Nov/14 09:30;githubbot;Github user shivateja commented on the pull request:

    https://github.com/apache/incubator-flink/pull/191#issuecomment-62360021
  
    Hi Robert,
    
    It was not clear from the comments. I'll update the PR to remove just the ""-d"" option.
;;;","17/Nov/14 12:01;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/191#issuecomment-63295216
  
    Is this pull request going to be updated? Otherwise, I would suggest to close it...
;;;","25/Nov/14 10:13;githubbot;Github user aljoscha commented on the pull request:

    https://github.com/apache/incubator-flink/pull/191#issuecomment-64364702
  
    Still nothing happening... :see_no_evil: 
;;;","25/Nov/14 14:45;githubbot;Github user shivateja commented on the pull request:

    https://github.com/apache/incubator-flink/pull/191#issuecomment-64409224
  
    Sorry. I was occupied with exams. Will update this tomorrow.
;;;","26/Nov/14 18:04;githubbot;GitHub user shivateja opened a pull request:

    https://github.com/apache/incubator-flink/pull/238

    [FLINK-1095] Remove '-d' option in info from CliFrontend

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/shivateja/incubator-flink flink1095

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/238.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #238
    
----
commit 7b89430b505e1f4f17e3381b3c7436b49e527c5d
Author: Shiva Teja Reddy <shivatejau@gmail.com>
Date:   2014-11-26T17:58:19Z

    [FLINK-1095] Remove '-d' option in info from CliFrontend

----
;;;","26/Nov/14 18:05;githubbot;Github user shivateja commented on the pull request:

    https://github.com/apache/incubator-flink/pull/191#issuecomment-64686783
  
    Created a new pull request #238
;;;","26/Nov/14 18:05;githubbot;Github user shivateja closed the pull request at:

    https://github.com/apache/incubator-flink/pull/191
;;;","26/Nov/14 18:32;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/238#issuecomment-64690696
  
    Looks good.
    
    +1 to merge
;;;","27/Nov/14 17:56;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/238
;;;","27/Nov/14 17:58;sewen;Fixed via 17bc479c8a66dea9f693035772925151c746a832

Thank you for the patch!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join deadlocks when used inside Delta iteration,FLINK-1090,12739520,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,knub,knub,05/Sep/14 16:48,29/Jan/17 14:43,14/Jul/23 05:57,20/Mar/15 10:21,0.6-incubating,,,0.9,,,API / Scala,Runtime / Coordination,Runtime / Task,,0,,,"I have a join inside a delta iteration, which hangs, i.e.I think it's deadlocked.
If I do the join without a delta iteration, it works.

_Why I think it's a deadlock_:
- no output in the logs
- CPU idles
- no IO (measured using iotop)
- stacktrace (when starting in debug mode and stopping at arbitrary points) locks deadlockish !http://i.imgur.com/4TgSK3x.png!

_Join properties_:
- size of the operands: 6.1 GB, 257 MB
- estimated result size: 50 MB
- the deadlock only occurs for big inputs, if I decrease the size of the first operand to something smaller, e.g. 1MB, it works.

I am using the Scala API.

Let me know, which further information you need. The code is basically the one I posted on the [mailing list|http://apache-flink-incubator-user-mailing-list-archive.2336050.n4.nabble.com/Delta-Iteration-Runtime-Error-quot-Could-not-set-up-runtime-strategy-for-input-channel-to-node-quot-td6.html], but I could provide a compilable version if thats necessary.","Ubuntu 14.04, Flink 0.6-incubating
LocalExecutor
JVM 1.7 with 7 GB RAM assigned",knub,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 20 10:21:28 UTC 2015,,,,,,,,,,"0|i1zqcv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/14 17:16;knub;Scrolling through the open issues, I find this might be a duplicate of [FLINK-1088|https://issues.apache.org/jira/browse/FLINK-1088].;;;","09/Sep/14 21:36;sewen;I think the reason may be very similar to [FLINK-1088].

We are finalizing a new network runtime that can handle these situations properly.

Here is a hint how to temporarily work around that: In general, deadlocks can only occur if there are more the one consumer for a data set. We run a deadlock detection algorithm, but it still has some deficits that we are aware of.
You should always be able to work around that by creating two different DataSets instead of using the same one twice. That is a workaround with overhead, but is is only temporary.
;;;","20/Mar/15 10:21;sewen;Solved via 9c77f0785e43326521da5e535f9ab1f05a9c6280;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Iteration head deadlock,FLINK-1088,12739160,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mbalassi,mbalassi,04/Sep/14 16:13,29/Jan/17 14:43,14/Jul/23 05:57,20/Mar/15 10:21,0.7.0-incubating,,,0.9,,,Runtime / Task,,,,1,,,"Flink hangs up for an iterative algorithm for which Stratosphere 0.5 was working. 

For the code please check out the following repo:
https://github.com/mbalassi/als-comparison

The stacktrace includes the following on Brokers:
""Join(Sends the rows of p with multiple keys)) (1/1)"" daemon prio=10 tid=0x00007f8928014800 nid=0x998 waiting on condition [0x00007f8912eed000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000007d2668ea0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
        at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
        at org.apache.flink.runtime.iterative.concurrent.Broker.get(Broker.java:63)
        at org.apache.flink.runtime.iterative.task.IterationIntermediatePactTask.run(IterationIntermediatePactTask.java:84)
        at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:375)
        at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:265)
        at java.lang.Thread.run(Thread.java:744)

This part waits for the iteration head which has not been started yet and thus induces a deadlock.",,mbalassi,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 20 10:21:47 UTC 2015,,,,,,,,,,"0|i1zot3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/14 13:53;sewen;This is a known issue.

Afaik, [~uce] is working on a patch for the network stack that fixes the streaming deadlocks by introducing blocking channels, which will be used in certain situations.;;;","20/Mar/15 10:21;sewen;Fixed via 9c77f0785e43326521da5e535f9ab1f05a9c6280;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A DeltaIteration fails  with Reducer as Input,FLINK-1087,12739130,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,aljoscha,aljoscha,04/Sep/14 14:09,04/Sep/14 14:51,14/Jul/23 05:57,04/Sep/14 14:51,,,,,,,Runtime / Coordination,,,,0,,,"The following modified WordCount example fails:

{code}
// set up the execution environment
		final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
		
		// get input data
		DataSet<String> text = getTextDataSet(env);
		
		DataSet<Tuple2<String, Integer>> counts = 
				// split up the lines in pairs (2-tuples) containing: (word,1)
				text.flatMap(new Tokenizer())
				// group by the tuple field ""0"" and sum up tuple field ""1""
				.groupBy(0)
				.sum(1);

		DeltaIteration<Tuple2<String, Integer>, Tuple2<String, Integer>> iteration = counts.iterateDelta(counts, 10, 0);

		DataSet<Tuple2<String, Integer>> delta =
				iteration.getSolutionSet().join(iteration.getWorkset()).where(0).equalTo(0).with(
				new JoinFunction<Tuple2<String, Integer>, Tuple2<String, Integer>, Tuple2<String, Integer>>() {

					public Tuple2<String, Integer> join(Tuple2<String, Integer> first, Tuple2<String, Integer> second) throws Exception {
						return second;
					}
				});

		DataSet<Tuple2<String, Integer>> newWorkset = delta.filter(new FilterFunction<Tuple2<String, Integer>>() {
			@Override
			public boolean filter(Tuple2<String, Integer> value) throws Exception {
				return false;
			}
		});

		DataSet<Tuple2<String, Integer>> result = iteration.closeWith(delta, newWorkset);
		result.print();

		// execute program
		env.execute(""WordCount Example"");
{code}

With this Exception:
Exception in thread ""main"" org.apache.flink.runtime.client.JobExecutionException: java.lang.RuntimeException: Initializing the input streams failed in Task Join(org.apache.flink.api.java.operators.JoinOperator$DefaultJoin$WrappingFlatJoinFunction): Illegal input group size in task configuration: -1
	at org.apache.flink.runtime.operators.RegularPactTask.registerInputOutput(RegularPactTask.java:260)
	at org.apache.flink.runtime.execution.RuntimeEnvironment.<init>(RuntimeEnvironment.java:205)
	at org.apache.flink.runtime.taskmanager.TaskManager.submitTasks(TaskManager.java:775)
	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.flink.runtime.ipc.RPC$Server.call(RPC.java:422)
	at org.apache.flink.runtime.ipc.Server$Handler.run(Server.java:958)
Caused by: java.lang.Exception: Illegal input group size in task configuration: -1
	at org.apache.flink.runtime.operators.RegularPactTask.initInputReaders(RegularPactTask.java:739)
	at org.apache.flink.runtime.operators.RegularPactTask.registerInputOutput(RegularPactTask.java:256)
	... 7 more

	at org.apache.flink.runtime.client.JobClient.submitJobAndWait(JobClient.java:361)
	at org.apache.flink.client.LocalExecutor.executePlan(LocalExecutor.java:245)
	at org.apache.flink.api.java.LocalEnvironment.execute(LocalEnvironment.java:58)
	at org.apache.flink.example.java.wordcount.WordCount.main(WordCount.java:100)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)",,aljoscha,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 04 14:51:24 UTC 2014,,,,,,,,,,"0|i1zomn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/14 14:19;sewen;This is a bug in the merging of the iteration auxiliary tasks.

You can deactivate it by the topmost flag in the {{NepheleJobGraphGenerator}}.

Once we introduce the buffer oriented runtime, we can throw out this logic for good.;;;","04/Sep/14 14:28;aljoscha;Ok, I'll check it and commit a fix.;;;","04/Sep/14 14:51;aljoscha;Fixed in https://github.com/apache/incubator-flink/commit/80af60b986f4aaf57ab56b243d3c897dabb0bddd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary failing of GroupReduceCombineDriver,FLINK-1085,12738360,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,fhueske,fhueske,02/Sep/14 07:56,13/Jul/15 15:53,14/Jul/23 05:57,13/Jul/15 15:49,0.6.1-incubating,0.7.0-incubating,,0.10.0,,,Runtime / Task,,,,0,starter,,"With a recent update (commit cbbcf7820885a8a9734ffeba637b0182a6637939) the GroupReduceCombineDriver was changed to not use an asynchronous partial sorter. Instead, the driver fills a sort buffer with records, sorts it, combines them, clears the buffer, and continues to fill it again.

The GroupReduceCombineDriver fails if a record cannot be serialized into an empty sort buffer, i.e., if the record is too large for the buffer.

Alternatively, we should emit a WARN message for the first record that is too large and just forward all records which do not fit into the empty sort buffer (maybe continue to count how many records were simply forwarded and give a second WARN message with this statistic).",,fhueske,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 13 15:53:39 UTC 2015,,,,,,,,,,"0|i1zkg7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/14 11:43;fhueske;The same applies to the {{SynchronousChainedCombineDriver}}.;;;","02/Sep/14 12:58;sewen;Did you already encounter such a case? Because it should only occur with really large records...;;;","02/Sep/14 13:02;fhueske;No, just found it while browsing the code and since forwarding is OK for combine it's not necessary to fail the job.
The job might fail later in Reduce due to the same problem, but that shouldn't be a reason to fail it here IMO.;;;","20/Jun/15 22:40;githubbot;GitHub user dabaitu opened a pull request:

    https://github.com/apache/flink/pull/854

    FLINK-1085: Unnecessary failing of GroupReduceCombineDriver

    I have a unit test failure and it seems it may have been there before my changes:
    
    Tests in error: 
      UtilsTest.testUberjarLocator:39 NullPointer
    
    It's looking for some uber jar and not finding it. Did I need to compile something else first?

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/dabaitu/flink master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/854.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #854
    
----
commit e693ba2398c170b6d65a2365a4cc0fe091f3319b
Author: dabaitu <tom.ssf@gmail.com>
Date:   2015-06-20T22:35:48Z

    FLINK-1085: change exception to warning log and track oversized Record count

----
;;;","20/Jun/15 22:51;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/flink/pull/854#issuecomment-113835793
  
    How did you start the tests?
;;;","20/Jun/15 23:39;githubbot;Github user dabaitu commented on the pull request:

    https://github.com/apache/flink/pull/854#issuecomment-113842215
  
    @rmetzger : 'mvn test'
;;;","20/Jun/15 23:50;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/flink/pull/854#issuecomment-113842515
  
    Can you try to run ""mvn clean verify"" in the project root?
    It will use a lot of resources and run for more than 30 minutes but the tests should pass ;)
    
    Also, the travis (CI system) build for your pull request passed.
    So the tests are all good.
    
    Lets wait for @fhueske to review your proposed changes.
;;;","22/Jun/15 08:50;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/854#issuecomment-114043020
  
    I think this code actually simply drops oversized records, or am I overlooking something?
    
    Also, outputting oversized records on WARN debug level seems heavy, it is not really something that should alert the person that runs the program.
    
    Before merging this, we need to
      - Make sure the records are not lost
      - Add a test that validates exactly that (not as an ITCase, but a unit test)
      - Reduce the log level for large records to DEBUG
;;;","23/Jun/15 02:25;githubbot;Github user dabaitu commented on the pull request:

    https://github.com/apache/flink/pull/854#issuecomment-114331577
  
    @StephanEwen @fhueske - apologies, I forgot to call collect. 
    A dumb question - why do we need to sort the records before combining them? Should I just call collect on the oversized record or still pass it to the combiner? What do combiners do(or what are they suppose to do in general)?
;;;","23/Jun/15 07:57;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/854#issuecomment-114396094
  
    The combiner partially sorts the records, to group subsequences in order to combine (reduce) them.
    
    If a record does not fit into a fresh sort buffer, then we can simply emit it as it is. There is no assumption on records being emitted in sorted order.
;;;","02/Jul/15 04:29;githubbot;Github user dabaitu commented on a diff in the pull request:

    https://github.com/apache/flink/pull/854#discussion_r33746632
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/operators/GroupReduceCombineDriver.java ---
    @@ -170,7 +171,12 @@ public void run() throws Exception {
     
     			// write the value again
     			if (!this.sorter.write(value)) {
    -				throw new IOException(""Cannot write record to fresh sort buffer. Record too large."");
    +				if (oversizedRecordCount == Long.MAX_VALUE) {
    +					LOG.error(""Number of oversized record has exceeded MAX Long"");
    +				} else {
    +					++oversizedRecordCount;
    +					LOG.warn(""Cannot write record to fresh sort buffer. Record too large. Oversized record count: {}"", oversizedRecordCount);
    +				}
    --- End diff --
    
    I tried to call
    ```
    this.output.collect(value)
    ```
    I couldn't figure out how to convert this single value from type IN to type OUT. Could you guys please help me? @StephanEwen @rmetzger 
;;;","02/Jul/15 06:20;githubbot;Github user dabaitu commented on a diff in the pull request:

    https://github.com/apache/flink/pull/854#discussion_r33749807
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/operators/GroupReduceCombineDriver.java ---
    @@ -170,7 +171,14 @@ public void run() throws Exception {
     
     			// write the value again
     			if (!this.sorter.write(value)) {
    -				throw new IOException(""Cannot write record to fresh sort buffer. Record too large."");
    +				if (oversizedRecordCount == Long.MAX_VALUE) {
    +					LOG.debug(""Number of oversized record has exceeded MAX Long"");
    +				} else {
    +					++oversizedRecordCount;
    +					LOG.debug(""Cannot write record to fresh sort buffer. Record too large. Oversized record count: {}"", oversizedRecordCount);
    +				}
    +				// simply forward the record
    +				this.output.collect((OUT)value);
    --- End diff --
    
    I tried to call
    
    this.output.collect(value)
    I couldn't figure out how to convert this single value from type IN to type OUT besides direct casting. Could you guys please help me? @StephanEwen @rmetzger
;;;","02/Jul/15 06:22;githubbot;Github user dabaitu commented on a diff in the pull request:

    https://github.com/apache/flink/pull/854#discussion_r33749843
  
    --- Diff: flink-runtime/src/test/java/org/apache/flink/runtime/operators/CombineTaskTest.java ---
    @@ -92,7 +92,35 @@ public void testCombineTask() {
     		
     		this.outList.clear();
     	}
    -	
    +
    +	@Test
    +	public void testOversizedRecordCombineTask() {
    +		int keyCnt = 1;
    +		int valCnt = 20;
    +
    +		addInput(new UniformRecordGenerator(keyCnt, valCnt, true));
    +		addDriverComparator(this.comparator);
    +		addDriverComparator(this.comparator);
    +		setOutput(this.outList);
    +
    +		getTaskConfig().setDriverStrategy(DriverStrategy.SORTED_GROUP_COMBINE);
    +		getTaskConfig().setRelativeMemoryDriver(combine_frac);
    --- End diff --
    
    I tried tweaking this to configure the memory size of the sorter, but failed. Please help too
;;;","02/Jul/15 18:44;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/flink/pull/854#discussion_r33809590
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/operators/GroupReduceCombineDriver.java ---
    @@ -170,7 +171,14 @@ public void run() throws Exception {
     
     			// write the value again
     			if (!this.sorter.write(value)) {
    -				throw new IOException(""Cannot write record to fresh sort buffer. Record too large."");
    +				if (oversizedRecordCount == Long.MAX_VALUE) {
    +					LOG.debug(""Number of oversized record has exceeded MAX Long"");
    +				} else {
    +					++oversizedRecordCount;
    +					LOG.debug(""Cannot write record to fresh sort buffer. Record too large. Oversized record count: {}"", oversizedRecordCount);
    +				}
    +				// simply forward the record
    +				this.output.collect((OUT)value);
    --- End diff --
    
    I think this is fine, calling `out.collect((OUT) value)`. The generics are a bit confusing here, I think. OUT is for combine functions really the same thing as IN - they are not allowed to change the type. This should really be simplified, otherwise it is confusing, agreed.
;;;","02/Jul/15 18:46;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/flink/pull/854#discussion_r33809780
  
    --- Diff: flink-runtime/src/test/java/org/apache/flink/runtime/operators/CombineTaskTest.java ---
    @@ -92,7 +92,35 @@ public void testCombineTask() {
     		
     		this.outList.clear();
     	}
    -	
    +
    +	@Test
    +	public void testOversizedRecordCombineTask() {
    +		int keyCnt = 1;
    +		int valCnt = 20;
    +
    +		addInput(new UniformRecordGenerator(keyCnt, valCnt, true));
    +		addDriverComparator(this.comparator);
    +		addDriverComparator(this.comparator);
    +		setOutput(this.outList);
    +
    +		getTaskConfig().setDriverStrategy(DriverStrategy.SORTED_GROUP_COMBINE);
    +		getTaskConfig().setRelativeMemoryDriver(combine_frac);
    --- End diff --
    
    I think you cannot reduce the memory further below to enforce records to be oversized. I think you need to change the data generator to provide you with an oversized record. I think the tests have a `UnionIterator` where you can mix an oversized record into the generating iterator.
;;;","03/Jul/15 03:10;githubbot;Github user dabaitu commented on the pull request:

    https://github.com/apache/flink/pull/854#issuecomment-118219396
  
    @StephanEwen please review again. Thanks!
;;;","12/Jul/15 19:56;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/flink/pull/854#discussion_r34425008
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/operators/GroupReduceCombineDriver.java ---
    @@ -170,7 +171,14 @@ public void run() throws Exception {
     
     			// write the value again
     			if (!this.sorter.write(value)) {
    -				throw new IOException(""Cannot write record to fresh sort buffer. Record too large."");
    +				if (oversizedRecordCount == Long.MAX_VALUE) {
    --- End diff --
    
    I think we can skip this test. It's going to be very hard to process 2^63 oversized records in a single Flink thread. Might take a millennium or so ;-)
;;;","12/Jul/15 22:35;githubbot;Github user dabaitu commented on the pull request:

    https://github.com/apache/flink/pull/854#issuecomment-120769914
  
    ProcessFailureStreamingRecoveryITCase passes locally. Is the master branch broken?
;;;","13/Jul/15 07:59;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/854#issuecomment-120842381
  
    I think there was a test instability, but it was fixed a bit back...
;;;","13/Jul/15 07:59;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/854#issuecomment-120842417
  
    Looks good, let me merge this...
;;;","13/Jul/15 15:49;sewen;Fixed via 7271881163d240ad1106a77036dce981dafb82f3 and 01c74338ff44ea7f3735a7eb94b2ce01ababc505.

Thanks you for the patch.;;;","13/Jul/15 15:53;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/flink/pull/854
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in AsynchronousPartialSorterITCase,FLINK-1075,12737493,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,28/Aug/14 16:32,01/Sep/14 17:59,14/Jul/23 05:57,01/Sep/14 17:59,,,,0.7.0-incubating,,,,,,,0,,,I encountered a race condition in the AsynchronousPartialSorterITCase which causes the test to fail non-deterministically. The sorter is legacy code and is only needed in the GroupReduceCombinerDriver. [~StephanEwen] proposed to replace the code in the GroupReduceCombineDriver and to delete the AsynchronousPartialSorter.,,githubbot,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 01 17:59:16 UTC 2014,,,,,,,,,,"0|i1zg3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/14 16:34;githubbot;GitHub user tillrohrmann opened a pull request:

    https://github.com/apache/incubator-flink/pull/104

    [FLINK-1075] Removed the AsynchronousPartialSorter

    As proposed by @StephanEwen, I removed the AsynchronousPartialSorter within the GroupReduceCombinerDriver and deleted the source files and the corresponding test cases.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/tillrohrmann/incubator-flink removeAsynchronousPartialSorter

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/104.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #104
    
----
commit 338dd63a2d6b98f4d3f750ea1ccf689ef001e351
Author: Till Rohrmann <trohrmann@apache.org>
Date:   2014-08-28T16:28:20Z

    Removed the AsynchronousPartialSorter.

----
;;;","29/Aug/14 17:01;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/104#issuecomment-53902820
  
    Good to merge...
;;;","01/Sep/14 17:54;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/104
;;;","01/Sep/14 17:59;sewen;Fixed by removing the asynchronous partial sorted and using the synchronous variant.

Fixed in cbbcf7820885a8a9734ffeba637b0182a6637939;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Project Join fails when input tuple is null,FLINK-1074,12737264,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fhueske,sewen,sewen,27/Aug/14 23:59,28/Feb/19 14:29,14/Jul/23 05:57,28/Aug/14 09:10,0.6-incubating,0.7.0-incubating,,0.6.1-incubating,0.7.0-incubating,,,,,,0,,,"When joining with the solution set (later: in outer joins), one of the input tuple s of the join function may be null. The ProjectJoin fails in that case with a NullPointerException.

{noformat}
Exception in thread ""main"" org.apache.flink.runtime.client.JobExecutionException: java.lang.NullPointerException
    at org.apache.flink.api.java.operators.JoinOperator$ProjectFlatJoinFunction.join(JoinOperator.java:935)
    at org.apache.flink.runtime.operators.JoinWithSolutionSetSecondDriver.run(JoinWithSolutionSetSecondDriver.java:143)
    at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:510)
    at org.apache.flink.runtime.iterative.task.AbstractIterativePactTask.run(AbstractIterativePactTask.java:137)
    at org.apache.flink.runtime.iterative.task.IterationIntermediatePactTask.run(IterationIntermediatePactTask.java:92)
    at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:375)
    at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:265)
    at java.lang.Thread.run(Thread.java:744)
{noformat}",,fhueske,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 28 09:10:40 UTC 2014,,,,,,,,,,"0|i1zf4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/14 09:10;fhueske;Fixed with 00840599a7a498cbd19d524ab5ad698365cbab4f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortGroup() does not sort Combiner input,FLINK-1073,12737242,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fhueske,fhueske,fhueske,27/Aug/14 22:23,28/Feb/19 14:29,14/Jul/23 05:57,24/Sep/14 10:27,0.6.1-incubating,0.7.0-incubating,,0.6.1-incubating,0.7.0-incubating,,,,,,0,,,"Flink supports sorted input for GroupReduce operators by calling for example

{code}
myData.groupBy(1).sortGroup(2, Order.ASCENDING).reduceGroup(new MyReducer());
{code}

This code will sort the input of the function {{MyReducer.reduce()}} on the third field.
However, the input of {{MyReducer.combine()}} is not sorted, which is an  unexpected behavior, IMO.",,fhueske,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 24 10:27:15 UTC 2014,,,,,,,,,,"0|i1zezr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/14 22:25;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/88#issuecomment-53649764
  
    Looks good to me. 
    However, FLINK-1073 does not allow to merge this PR yet. 
    Once that issue is fixed, this PR can be merged.
;;;","02/Sep/14 14:00;githubbot;GitHub user fhueske opened a pull request:

    https://github.com/apache/incubator-flink/pull/109

    [FLINK-1073] Enables sorted group input for GroupReduce combiners

    `GroupReduceCombineDriver` and `SynchronousChainedCombineDriver` use separate comparators for sorting and grouping.
    Enabling multiple comparators for a single input of a driver required some refactoring.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fhueske/incubator-flink fixSortedGroupCombiner

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/109.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #109
    
----
commit d547a27720409584543b0caa4a7a686a82f2a903
Author: Fabian Hueske <fhueske@apache.org>
Date:   2014-09-01T22:08:43Z

    [FLINK-1073] Added test case to reproduce error

commit 190c146ff65a0a09ebe1ca03d32a701159bb2080
Author: Fabian Hueske <fhueske@apache.org>
Date:   2014-09-02T13:51:44Z

    [FLINK-1073] Enables sorted group input for GroupReduce combiners.
    - GroupReduceCombineDriver uses separate comparators for sorting and grouping.
    - Adding support for multiple comparators for a single input driver required some refactoring.

----
;;;","06/Sep/14 20:31;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/88#issuecomment-54727381
  
    PR #109 fixes FLINK-1073. 
    Waiting for that one before we can proceed with this PR.
;;;","22/Sep/14 10:03;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/109#issuecomment-56352515
  
    Ping. Any comments on this PR?
    Otherwise, I'll merge it in the next days to proceed with PR #88.
;;;","24/Sep/14 10:25;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/109
;;;","24/Sep/14 10:27;fhueske;Fixed with e5731e0ed7f7294e6f73970fed8315cb572c9cfd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unknown local strategy 'MAP_PARTITION' in JSON generator.,FLINK-1071,12737136,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,27/Aug/14 15:54,28/Feb/19 14:30,14/Jul/23 05:57,29/Aug/14 18:28,0.7.0-incubating,,,0.7.0-incubating,,,API / DataSet,,,,0,,,Strategy has not been added to the PlanJSONDumpGenerator,,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 29 18:28:54 UTC 2014,,,,,,,,,,"0|i1zecv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/14 18:28;sewen;Fixed in 1e38a636ffb52b19bbf820de1d26f539a2cc24be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Change return type of ""getBroadcastVariable()"" from Collection to List",FLINK-1070,12737132,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,27/Aug/14 15:38,28/Feb/19 14:29,14/Jul/23 05:57,01/Sep/14 16:51,0.7.0-incubating,,,0.7.0-incubating,,,,,,,0,api-breaking,simple,"The type is actually a List.

Collections are a very reduces interface, it is much easier to work with Lists.",,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 01 16:51:07 UTC 2014,,,,,,,,,,"0|i1zebz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/14 17:35;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/105

    [FLINK-1070] Change return type of ""getBroadcastVariable()"" to List.

    `List` is much better to work with than `Collection`. The internal representation was a list anyways.
    
    The change breaks the binary compatibility, but not source compatibility, since `List<T>`can always be cast to `Collection<T>`.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink bc_lists

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/105.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #105
    
----
commit 30ee70a96f663d5aee4ceccfbd601fce34ccf010
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-08-29T17:32:40Z

    [FLINK-1070] Change return type of ""getBroadcastVariable()"" to List.

----
;;;","01/Sep/14 12:49;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/105#issuecomment-54055446
  
    Looks good to me.
    
    Do we need to update the documentation for this change?
;;;","01/Sep/14 16:49;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/105
;;;","01/Sep/14 16:51;sewen;Fixed via 0b100517f1a4f2a74cab06d2fbf00f96a45513f8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""flink-hadoop-compatibility"" fails to build with Hadoop 2.5.0 dependencies",FLINK-1069,12737111,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,rmetzger,rmetzger,27/Aug/14 13:48,30/Sep/14 18:02,14/Jul/23 05:57,30/Sep/14 18:02,0.6-incubating,0.7.0-incubating,,0.7.0-incubating,,,,,,,0,,,"{{mvn clean verify  -Dhadoop.profile=2 -Dhadoop.version=2.5.0}} fails with
{code}
org.apache.flink.runtime.client.JobExecutionException: java.lang.IllegalAccessError: tried to access method com.google.common.base.Stopwatch.<init>()V from class org.apache.hadoop.mapreduce.lib.input.FileInputFormat
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:379)
	at org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat.createInputSplits(HadoopInputFormat.java:156)
	at org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat.createInputSplits(HadoopInputFormat.java:53)
	at org.apache.flink.runtime.jobgraph.JobInputVertex.getInputSplits(JobInputVertex.java:101)
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:495)
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:281)
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:177)
	at org.apache.flink.runtime.jobmanager.JobManager.submitJob(JobManager.java:469)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.flink.runtime.ipc.RPC$Server.call(RPC.java:422)
	at org.apache.flink.runtime.ipc.Server$Handler.run(Server.java:958)

	at org.apache.flink.runtime.client.JobClient.submitJobAndWait(JobClient.java:268)
	at org.apache.flink.test.util.JavaProgramTestBase$TestEnvironment.execute(JavaProgramTestBase.java:148)
	at org.apache.flink.hadoopcompatibility.mapreduce.example.WordCount.main(WordCount.java:86)
	at org.apache.flink.test.hadoopcompatibility.mapreduce.HadoopInputOutputITCase.testProgram(HadoopInputOutputITCase.java:44)
	at org.apache.flink.test.util.JavaProgramTestBase.testJob(JavaProgramTestBase.java:100)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

{code}",,atsikiridis,rmetzger,sewen,srowen,,,,,,,,,,,,FLINK-1072,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 30 18:02:20 UTC 2014,,,,,,,,,,"0|i1ze7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/14 14:16;srowen;Ah, Flink uses Guava 17. Hadoop uses Guava 11. You'll want to shade your Guava dependency in the Flink artifacts, I think. This was the conclusion of a similar, long and complicated discussion in Spark: https://issues.apache.org/jira/browse/SPARK-2420;;;","27/Aug/14 14:21;atsikiridis;And here is the actual Hadoop issue: https://issues.apache.org/jira/browse/HADOOP-10961;;;","27/Aug/14 14:26;srowen;Really, Hadoop should have shaded this a long long time ago. It's not even a matter of Hadoop upgrading. Flink will want to shade too for a similar reason, to stay out of the way of users' Guava usage.;;;","17/Sep/14 15:16;rmetzger;Ok, I'm assigning myself to the issue. I'll look into the shading of Guava.;;;","30/Sep/14 18:02;sewen;Fixed as of 719e2889395d7565bab4e7b0d994e2db32e592ba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Stratosphere"" occurance in docs FAQ",FLINK-1067,12736944,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fhueske,fhueske,fhueske,26/Aug/14 21:47,13/Apr/21 20:38,14/Jul/23 05:57,26/Aug/14 21:52,,,,0.7.0-incubating,,,Documentation,,,,0,,,The name Stratosphere occurs in the answer to the fault-tolerance question of the FAQ.,,fhueske,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 26 21:52:37 UTC 2014,,,,,,,,,,"0|i1zd6f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/14 21:49;githubbot;GitHub user fhueske opened a pull request:

    https://github.com/apache/incubator-flink/pull/103

    [FLINK-1067] Replaced Stratosphere by Flink in FAQ

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fhueske/incubator-flink patch-1

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/103.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #103
    
----
commit ce3ed7e0b769ab3ddc9a96666028ef45f6b01b6d
Author: Fabian Hueske <fhueske@gmail.com>
Date:   2014-08-26T21:48:52Z

    [FLINK-1067] Replaced Stratosphere by Flink in FAQ

----
;;;","26/Aug/14 21:52;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/103
;;;","26/Aug/14 21:52;fhueske;Fixed by ce3ed7e0b769ab3ddc9a96666028ef45f6b01b6d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""flink-streaming-core"" build fails with Hadoop 2.4.0 dependencies",FLINK-1065,12736765,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,rmetzger,rmetzger,26/Aug/14 10:21,28/Feb/19 11:16,14/Jul/23 05:57,30/Aug/14 13:50,0.7.0-incubating,,,0.7.0-incubating,,,,,,,0,,,"Hi,

building Flink with ""mvn clean package -Dhadoop.profile=2 -DskipTests -Dhadoop-two.version=2.4.0""

results in the following error
{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project flink-streaming-core: Compilation failure: Compilation failure:
[ERROR] /home/robert/incubator-flink/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/invokable/operator/BatchReduceInvokable.java:[22,36] package org.apache.commons.math.util does not exist
[ERROR] /home/robert/incubator-flink/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/invokable/operator/WindowReduceInvokable.java:[22,36] package org.apache.commons.math.util does not exist
[ERROR] /home/robert/incubator-flink/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/invokable/operator/BatchReduceInvokable.java:[39,36] cannot find symbol
[ERROR] symbol:   variable MathUtils
[ERROR] location: class org.apache.flink.streaming.api.invokable.operator.BatchReduceInvokable<IN,OUT>
[ERROR] /home/robert/incubator-flink/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/invokable/operator/BatchReduceInvokable.java:[48,36] cannot find symbol
[ERROR] symbol:   variable MathUtils
[ERROR] location: class org.apache.flink.streaming.api.invokable.operator.BatchReduceInvokable<IN,OUT>
[ERROR] /home/robert/incubator-flink/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/invokable/operator/WindowReduceInvokable.java:[40,36] cannot find symbol
[ERROR] symbol:   variable MathUtils
[ERROR] location: class org.apache.flink.streaming.api.invokable.operator.WindowReduceInvokable<IN,OUT>
[ERROR] /home/robert/incubator-flink/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/invokable/operator/WindowReduceInvokable.java:[49,36] cannot find symbol
[ERROR] symbol:   variable MathUtils
[ERROR] location: class org.apache.flink.streaming.api.invokable.operator.WindowReduceInvokable<IN,OUT>
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :flink-streaming-core
{code}

I think older Hadoop versions (its working with Hadoop 2.2.0) are pulling in transitive dependencies that are not present with Hadoop 2.4.0 anymore.",,mbalassi,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 30 13:50:02 UTC 2014,,,,,,,,,,"0|i1zc33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/14 15:00;mbalassi;Just a transitive dependency issue indeed. I'm pushing the fix to the current streaming pull request (#102) if that is sufficient.;;;","27/Aug/14 17:04;rmetzger;I think its okay to append it to the pull request.;;;","30/Aug/14 13:50;rmetzger;Resolved in https://git-wip-us.apache.org/repos/asf?p=incubator-flink.git;a=commit;h=0731f77b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mvn verify fails for quickstarts,FLINK-1064,12736507,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,uce,uce,25/Aug/14 12:00,30/Sep/14 20:47,14/Jul/23 05:57,30/Sep/14 19:10,0.7.0-incubating,,,,,,Build System,,,,0,,,Running {{mvn clean verify}} with the current master results in a failure for the module {{flink-quickstart-java}}. I suspect that {{flink-quickstart-scala}} suffers from the same problem.,,fhueske,rmetzger,sewen,uce,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 30 20:47:13 UTC 2014,,,,,,,,,,"0|i1zain:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/14 15:22;rmetzger;I've asked for help on the maven user mailing list: https://mail-archives.apache.org/mod_mbox/maven-users/201408.mbox/%3CCAGr9p8B%2BwysyMvskxuQnVcqzXBFwudAvWOyE4vqs4OmtFT8fkg%40mail.gmail.com%3E .. No response yet.;;;","30/Sep/14 19:10;fhueske;Checked with the current master.
Could successfully run {{mvn clean verify}} in the root module and all quickstart modules.;;;","30/Sep/14 19:13;sewen;It works only if you ever did an install before. Try re-run it offline after you cleaned your .m2 cache, and I think you'll see the error. Question is how bad it is. ""mvn clean install verify"" will still work, also on a fresh setup and offline.;;;","30/Sep/14 19:34;fhueske;Thanks for the info.
Will check and reopen if it fails.;;;","30/Sep/14 20:47;fhueske;Hmmm, if the .m2 repository is completely empty, it won't be possible to build the system anyway because all dependencies are missing...
Builds are only failing for corner cases and this is not a critical issue, IMO.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in NettyConnectionManager,FLINK-1063,12736490,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,uce,uce,uce,25/Aug/14 09:19,07/Sep/14 10:49,14/Jul/23 05:57,07/Sep/14 10:40,0.6-incubating,0.7.0-incubating,,0.6.1-incubating,0.7.0-incubating,,Runtime / Coordination,,,,0,,,"The TCP channel queuing mechanism in {{NettyConnectionManager}} has a race condition, which may result in re-ordering of envelopes at the receiver (the dreaded {{""Expected data packet X but received Y""}} exception).

Thanks to [AHeise|https://github.com/AHeise] for reporting the problem.

The problem has been introduced with commits 52512636444902497e47ccbfb1cabaffb3e23343 ... 32d168f439bdb5dfab02a3ab2d12e87d0622a67e.

I will revert the respective commits and implement a fall back, which limits TCP channel multiplexing and immediately closes TCP channels.",,uce,,,,,,,,,,,,,,,,,,,,,,FLINK-1093,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 07 10:40:38 UTC 2014,,,,,,,,,,"0|i1zaev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/14 10:40;uce;Fixed in c0c2abda5eaaabc6291f765718d88dcbdc12d2a4 (master) and 828407d58e1894a1e51f74f2c615fcc941e909d2 (release-0.6.1) after [~rmetzger] reported the problem as well.

I will move the 2nd part of a non-multiplexed fall back to a seperate issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTTP 404 for flink-user ML Archive Link ,FLINK-1059,12734997,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,fhueske,fhueske,19/Aug/14 09:41,30/Sep/14 19:27,14/Jul/23 05:57,30/Sep/14 19:27,,,,,,,Project Website,,,,0,,,The link to the archive of the flink-user mailing list on http://flink.incubator.apache.org/community.html#mailing-lists gives a 404.,,fhueske,rmetzger,,,,,,,,,,,,,,,,,,,,,INFRA-8207,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412937,,,Tue Sep 30 19:27:28 UTC 2014,,,,,,,,,,"0|i1z19z:",412923,,,,,,,,,,,,,,,,,,,,"19/Aug/14 09:45;rmetzger;I'm asking the ASF infra why the archive has still not been created.;;;","30/Sep/14 19:27;fhueske;Checked again and the link is working now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows: JM web interface job analze view does not show task details on,FLINK-1054,12734379,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,rmetzger,fhueske,fhueske,15/Aug/14 09:39,15/Aug/14 11:54,14/Jul/23 05:57,15/Aug/14 11:54,,,,0.6-incubating,,,Runtime / Web Frontend,,,,0,,,The job analyze view does not show the start/end times of tasks (neither in stack nor in flow layout). ,"Windows 8.1
Firefox 31, Chrome 36.0.1985.143, IE 11",fhueske,rmetzger,,,,,,,,,,,,,,,,,,,,FLINK-1046,FLINK-1055,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412319,,,Fri Aug 15 11:54:09 UTC 2014,,,,,,,,,,"0|i1yxjj:",412306,,,,,,,,,,,,,,,,,,,,"15/Aug/14 09:41;fhueske;Can somebody check if this problem also affects Linux / MacOS?;;;","15/Aug/14 09:41;rmetzger;Hi,
I also found the issue. I have a fix for it already. It was caused by this change: FLINK-1046;;;","15/Aug/14 11:54;rmetzger;Fixed in http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/a05c5674.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
start-local.bat refers to invalid main class,FLINK-1047,12734055,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fhueske,fhueske,fhueske,14/Aug/14 13:26,14/Aug/14 14:46,14/Jul/23 05:57,14/Aug/14 14:46,0.6-incubating,,,,,,Build System,,,,0,,,The {{./bin/start-local.bat}} script tries to start the main class {{org.apache.flink.nephele.jobmanager.JobManager}} instead of {{org.apache.flink.runtime.jobmanager.JobManager}}.,,fhueske,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412083,,,Thu Aug 14 14:46:56 UTC 2014,,,,,,,,,,"0|i1yw4n:",412073,,,,,,,,,,,,,,,,,,,,"14/Aug/14 14:46;fhueske;Fixed via c0007b0a3348fb489441f3ebb8a9e538306ee0bb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Client does not return after running a job with an accumulator,FLINK-1046,12733701,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,rmetzger,rmetzger,13/Aug/14 07:37,15/Aug/14 09:41,14/Jul/23 05:57,14/Aug/14 12:06,0.6-incubating,,,0.6-incubating,,,,,,,0,,,"cd flink-0.6-incubating
flink-dist/target/flink-0.6-incubating-bin/flink-0.6-incubating/bin/start-local.sh
flink-dist/target/flink-0.6-incubating-bin/flink-0.6-incubating/bin/flink run flink-examples/flink-java-examples/target/flink-java-examples-0.6-incubating.jar -c org.apache.flink.example.java.relational.EmptyFieldsCountAccumulator

The CliFrontend is waiting at the following position:
{code]
""main"" #1 prio=5 os_prio=0 tid=0x00007f102000f000 nid=0x63ea in Object.wait() [0x00007f10274d5000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000076d090620> (a org.apache.flink.runtime.ipc.Client$Call)
	at java.lang.Object.wait(Object.java:502)
	at org.apache.flink.runtime.ipc.Client.call(Client.java:701)
	- locked <0x000000076d090620> (a org.apache.flink.runtime.ipc.Client$Call)
	at org.apache.flink.runtime.ipc.RPC$Invoker.invoke(RPC.java:255)
	at com.sun.proxy.$Proxy1.getAccumulatorResults(Unknown Source)
	at org.apache.flink.runtime.client.JobClient.getAccumulators(JobClient.java:407)
	- locked <0x000000076d78a3f0> (a com.sun.proxy.$Proxy0)
	at org.apache.flink.runtime.client.JobClient.submitJobAndWait(JobClient.java:346)
	at org.apache.flink.client.program.Client.run(Client.java:303)
	at org.apache.flink.client.program.Client.run(Client.java:287)
	at org.apache.flink.client.program.Client.run(Client.java:281)
	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:54)
	at org.apache.flink.example.java.relational.EmptyFieldsCountAccumulator.main(EmptyFieldsCountAccumulator.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:389)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:307)
	at org.apache.flink.client.program.Client.run(Client.java:240)
	at org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:332)
	at org.apache.flink.client.CliFrontend.run(CliFrontend.java:319)
	at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:930)
	at org.apache.flink.client.CliFrontend.main(CliFrontend.java:954)
{code}",,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,FLINK-1054,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411729,,,Thu Aug 14 12:06:34 UTC 2014,,,,,,,,,,"0|i1ytzz:",411720,,,,,,,,,,,,,,,,,,,,"14/Aug/14 12:06;sewen;Fixed via c693f04f4ab3d64b7c8af4b8fc7a8527c23b1cdd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serialization of RuntimeStatefulSerializerFactory causes exception,FLINK-1042,12732665,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fhueske,fhueske,fhueske,07/Aug/14 20:46,28/Feb/19 14:29,14/Jul/23 05:57,13/Aug/14 10:58,,,,,,,,,,,0,,,RuntimeStatefulSerializerFactory cannot be serialized due to a non-serializable field ({{ClassLoader loader}}).,,fhueske,githubbot,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410693,,,Wed Aug 13 10:58:54 UTC 2014,,,,,,,,,,"0|i1ynpz:",410686,,,,,,,,,,,,,,,,,,,,"07/Aug/14 21:34;githubbot;GitHub user fhueske opened a pull request:

    https://github.com/apache/incubator-flink/pull/92

    [FLINK-1042] Changed ClassLoader field to be transient.

    Serialization of the `RuntimeStatefulSerializerFactory` fails because the field `ClassLoader loader` is not serializable. The fix makes the field `transient`.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fhueske/incubator-flink StatefulSerializerFactoryFix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/92.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #92
    
----
commit c01cc279181e48a05e60674e887cfe41341ea8c9
Author: Fabian Hueske <fhueske@apache.org>
Date:   2014-08-07T21:22:34Z

    [FLINK-1042] Changed ClassLoader field to be transient.

----
;;;","08/Aug/14 09:45;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/92#issuecomment-51582286
  
    I tested this change on the cluster with a large KMeans job and it worked.
    I think we can merge this change (also for the 0.6 release)
;;;","12/Aug/14 17:27;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/92#issuecomment-51947425
  
    This seems crucial and makes total sense to me. I would merge it soon.
;;;","13/Aug/14 10:56;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/92
;;;","13/Aug/14 10:58;rmetzger;Merged in commit http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/eb51c880. (Into master and release-0.6);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POJO expression keys do not work for groupReduce functions,FLINK-1039,12731054,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,31/Jul/14 12:37,28/Feb/19 14:29,14/Jul/23 05:57,31/Jul/14 13:24,0.6-incubating,,,0.6-incubating,,,,,,,0,,,I fix is on the way,,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409126,,,Thu Jul 31 13:24:59 UTC 2014,,,,,,,,,,"0|i1ye7b:",409122,,,,,,,,,,,,,,,,,,,,"31/Jul/14 13:24;sewen;Fixed via 23289d6eadeb7ad8798f41e747a4962745e35769;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Projects wiki page link in contribution page is broken,FLINK-1037,12730082,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,rmetzger,gvinayagam,gvinayagam,27/Jul/14 17:27,13/Apr/21 20:39,14/Jul/23 05:57,28/Jul/14 11:40,0.6-incubating,,,0.6-incubating,,,Documentation,,,,0,documentation,,"In http://flink.incubator.apache.org/how-to-contribute.html, there is a link to projects wiki page  (http://the/wiki/url). It is broken.",,gvinayagam,rmetzger,,,,,,,,,,,,,INFRA-8039,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408155,,,Mon Jul 28 11:40:25 UTC 2014,,,,,,,,,,"0|i1y8cn:",408160,,,,,,,,,,,,,,,,,,,,"27/Jul/14 17:59;rmetzger;Thank you for reporting the issue. The link is currently only a placeholder since we do not have a Wiki for Flink set up yet.

We have to wait until the Apache Infra Team has set it up: https://issues.apache.org/jira/browse/INFRA-8039;;;","28/Jul/14 11:40;rmetzger;I resolved the issue in svn http://svn.apache.org/r1613965.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Eclipse M2E cannot compile project,FLINK-1031,12728505,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,sewen,sewen,21/Jul/14 10:22,21/Jul/14 11:57,14/Jul/23 05:57,21/Jul/14 11:57,0.6-incubating,,,0.6-incubating,,,Build System,,,,0,,,Recent additions in [FLINK-1026] (commit 152dcde0bb0d8a56bf878dde6f4aae2d2db6c8ba) prevent the code from being compiled in Eclipse.,,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406581,,,Mon Jul 21 11:57:08 UTC 2014,,,,,,,,,,"0|i1xys7:",406601,,,,,,,,,,,,,,,,,,,,"21/Jul/14 11:57;sewen;Solved via 05677f3d6812a191c54aedf9bcbf152273d6c054
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WordCount POJO fails with null pointer exception,FLINK-1026,12727120,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,sewen,sewen,14/Jul/14 16:45,28/Feb/19 14:29,14/Jul/23 05:57,17/Jul/14 14:37,0.6-incubating,,,0.6-incubating,,,,,,,0,,,"When run from the command line on larger inputs, the WordCountPOJO fails.

{code}
java.lang.NullPointerException
	at org.apache.flink.api.java.typeutils.runtime.PojoComparator.compare(PojoComparator.java:271)
	at org.apache.flink.runtime.operators.sort.NormalizedKeySorter.compareRecords(NormalizedKeySorter.java:319)
	at org.apache.flink.runtime.operators.sort.NormalizedKeySorter.compare(NormalizedKeySorter.java:357)
	at org.apache.flink.runtime.operators.sort.QuickSort.sortInternal(QuickSort.java:96)
	at org.apache.flink.runtime.operators.sort.QuickSort.sort(QuickSort.java:52)
	at org.apache.flink.runtime.operators.sort.QuickSort.sort(QuickSort.java:56)
	at org.apache.flink.runtime.operators.ReduceCombineDriver.sortAndCombine(ReduceCombineDriver.java:166)
	at org.apache.flink.runtime.operators.ReduceCombineDriver.run(ReduceCombineDriver.java:149)
	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:509)
	at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:374)
	at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:265)
	at java.lang.Thread.run(Thread.java:745)
{code}",,aljoscha,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405227,,,Thu Jul 17 14:37:17 UTC 2014,,,,,,,,,,"0|i1xqlb:",405260,,,,,,,,,,,,,,,,,,,,"14/Jul/14 16:55;aljoscha;I will look into it ASAP.;;;","14/Jul/14 17:34;rmetzger;Do we have test coverage for the POJO support?;;;","14/Jul/14 20:00;aljoscha;Yes, but it seems I have to extend them once I find the reason for this.;;;","14/Jul/14 20:21;aljoscha;What are ""Larger Inputs"". Big records, or big input data and small records? Or both?;;;","15/Jul/14 13:37;aljoscha;Hmm, I cannot reproduce the error. Checked it with 124mb of lipsum input data and with 500mb of lipsum input data. Both times I started the WordCountPOJO example as is from within IntelliJ.;;;","15/Jul/14 14:23;sewen;As the initial message said, I got the error by running it through the command line.

Can you reproduce it using the packaged WordCountPOJO.jar that is in the examples directory?;;;","16/Jul/14 07:12;aljoscha;Sorry, I didn't read that part. Now I can reproduce the error, though I still have no idea what is causing it. (Still looking);;;","17/Jul/14 10:21;aljoscha;I found the bug. The PojoComparator is not using the user code class loader and therefore cannot duplicate the serializer it internally uses when it need to be duplicated. The fix is simple but I will also change the PackagedProgramEndToEndITCase to use a Pojo so that this will be tested.;;;","17/Jul/14 14:37;aljoscha;Fixed together with other bugs in https://github.com/apache/incubator-flink/commit/152dcde0bb0d8a56bf878dde6f4aae2d2db6c8ba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix doc link in how-to-contribute page to point to right coding_guidelines,FLINK-1022,12726645,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hsaputra,hsaputra,hsaputra,10/Jul/14 18:14,10/Jul/14 20:27,14/Jul/23 05:57,10/Jul/14 20:27,,,,0.6-incubating,,,,,,,0,,,"Seem like the latest Fllink website page for how-to-contribute page points to wrong coding_guidelines: http://flink.incubator.apache.org/coding_guidelines.html

Here is the correct link. http://flink.incubator.apache.org/docs/0.6-SNAPSHOT/coding_guidelines.html",,hsaputra,rmetzger,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/14 18:16;hsaputra;FLINK-1022.patch;https://issues.apache.org/jira/secure/attachment/12655048/FLINK-1022.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404752,,,Thu Jul 10 20:27:14 UTC 2014,,,,,,,,,,"0|i1xnpj:",404790,,,,,,,,,,,,,,,,,,,,"10/Jul/14 18:16;hsaputra;Proposed patch;;;","10/Jul/14 20:12;rmetzger;+1
Looks good to merge.;;;","10/Jul/14 20:27;hsaputra;Thanks!

svn ci -m ""FLINK-1022 Fix doc link in how-to-contribute page to point to right coding_guidelines""
Sending        how-to-contribute.md
Sending        site/how-to-contribute.html
Transmitting file data ..
Committed revision 1609565.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalStateException at InputGate,FLINK-1021,12726586,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gyfora,gyfora,10/Jul/14 13:33,11/Jul/14 14:16,14/Jul/23 05:57,11/Jul/14 14:16,0.6-incubating,,,,,,Runtime / Task,,,,0,,,"Until now, after every emit to the outputs we flushed them using the .flush() method of the recordwriter. Now we removed this flush() call and we have two interesting observations:

First of all we dont send enough records the source finishes but the output buffer never gets flushed.

Secondly if we generate a simple datastream from lets say the first 1500 numbers we get an exception in the InputGates (after lets say a hundred records): java.lang.IllegalStateException: Channel received an event before completing the current partial record.

java.lang.IllegalStateException: Channel received an event before completing the current partial record.
	at eu.stratosphere.runtime.io.channels.InputChannel.readRecord(InputChannel.java:177)
	at eu.stratosphere.runtime.io.gates.InputGate.readRecord(InputGate.java:173)
	at eu.stratosphere.streaming.api.streamcomponent.StreamRecordReader.hasNext(StreamRecordReader.java:96)
	at eu.stratosphere.streaming.api.streamcomponent.AbstractStreamComponent.invokeRecords(AbstractStreamComponent.java:255)
	at eu.stratosphere.streaming.api.streamcomponent.StreamSink.invoke(StreamSink.java:74)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:260)
	at java.lang.Thread.run(Unknown Source)

To produce the error run this test: https://github.com/stratosphere/stratosphere-streaming/blob/output-flush/stratosphere-streaming-core/src/test/java/eu/stratosphere/streaming/api/PrintTest.java

Please note that this is the output-flush branch in Stratoshpere-streaming

This works perfectly if we flush the outputs after the emits.","Linux/Windows
",gyfora,sewen,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404693,,,Fri Jul 11 14:15:54 UTC 2014,,,,,,,,,,"0|i1xncn:",404731,,,,,,,,,,,,,,,,,,,,"10/Jul/14 15:03;gyfora;Also if using the same setup we run:

https://github.com/stratosphere/stratosphere-streaming/blob/output-flush/stratosphere-streaming-examples/src/main/java/eu/stratosphere/streaming/examples/wordcount/WordCountLocal.java

We get errors during the emits:

java.lang.IllegalStateException: Pending serialization of previous record.
	at eu.stratosphere.runtime.io.serialization.SpanningRecordSerializer.addRecord(SpanningRecordSerializer.java:62)
	at eu.stratosphere.runtime.io.api.RecordWriter.emit(RecordWriter.java:76)
	at eu.stratosphere.streaming.api.streamrecord.StreamCollector.emit(StreamCollector.java:75)
	at eu.stratosphere.streaming.api.streamrecord.StreamCollector.collect(StreamCollector.java:55)
	at eu.stratosphere.streaming.api.streamrecord.StreamCollectorManager.collect(StreamCollectorManager.java:72)
	at eu.stratosphere.streaming.api.streamrecord.StreamCollectorManager.collect(StreamCollectorManager.java:1)
	at eu.stratosphere.streaming.api.invokable.MapInvokable.invoke(MapInvokable.java:37)
	at eu.stratosphere.streaming.api.streamcomponent.AbstractStreamComponent.invokeRecords(AbstractStreamComponent.java:257)
	at eu.stratosphere.streaming.api.streamcomponent.StreamTask.invoke(StreamTask.java:102)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:260)
	at java.lang.Thread.run(Unknown Source)

These two issues might be linked.;;;","10/Jul/14 17:20;sewen;I think [~uce] ist most familiar with that part of the code.

Ufuk, can you take a look at this soon?;;;","11/Jul/14 13:23;uce;Hey Gyula,

I haven't looked into the code, but you should only have to flush once, when the program is finished. Could you try it out with a flush at the end? (The runtime does not do it automatically at the moment, because it is not aware of the writers, which do the serialization; it only knows about the (stateless) output gates -- this will change soon tough. Sorry for the lack of documentation on this part of the system.)

The explanation for the flush call: when emitting, the record is serialized into the current buffer for the respective target channel and the buffer is only dispatched after it is full. At the end of a program, you often end up with a not completely filled buffer. The event, which jumps ahead of the line is probably the channel close event, which is generated after the UDF code is done (see RuntimeEnvironment close gate methods). If the last buffers have not been flushed, the event might be received while there is a partial record at the receiver.

Keep me posted, I can look into your code later this evening if it is still not working with the final flush.;;;","11/Jul/14 13:27;uce;Let me be more precise: with end of program I meant the end of an UDF, basically at the end of the {{invoke}} method.;;;","11/Jul/14 14:15;gyfora;Flushing all the outputs at the end of invoke() solved our problems. 

Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logistic Regression deadlocks,FLINK-1018,12726541,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mholzemer,mholzemer,10/Jul/14 10:04,20/Mar/15 10:19,14/Jul/23 05:57,20/Mar/15 10:19,,,,0.9,,,,,,,0,,,"We are currently running our implementation of logistic regression with batch gradient descent on the cluster.
Unfortunatelly for datasets > 1GB it seems to deadlock inside of the iteration. This means the first iteration is never finished.

The iteration does a map over all points, the map gets the iteration input as broadcast variable. The result of the map is reduced and the result of the reducer (1 tuple) is crossed with the iteration input.

There should be no reason for the deadlock, since the data is still quite small compared to the cluster size (4 nodes a 32GB). Also the datasize stays constant throughout the algorithm.

Here is the generated plan. I will also attach the full algorithm.
{code}
{
	""nodes"": [

	{
		""id"": 2,
		""type"": ""source"",
		""pact"": ""Data Source"",
		""contents"": ""[([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0."",
		""parallelism"": ""1"",
		""subtasks_per_instance"": ""1"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0 B"" },
			{ ""name"": ""CPU"", ""value"": ""0.0 "" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""0.0 B"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0 "" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""step_function"": [
	{
		""id"": 8,
		""type"": ""source"",
		""pact"": ""Data Source"",
		""contents"": ""TextInputFormat (hdfs://cloud-7:45010/tmp/input/higgs.M.txt) - UTF-8"",
		""parallelism"": ""64"",
		""subtasks_per_instance"": ""16"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""8.0.31 GB"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""109.90 M"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Disk I/O"", ""value"": ""8.0.31 GB"" },
			{ ""name"": ""CPU"", ""value"": ""0.0 "" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""8.0.31 GB"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0 "" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 7,
		""type"": ""pact"",
		""pact"": ""Map"",
		""contents"": ""de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$6"",
		""parallelism"": ""64"",
		""subtasks_per_instance"": ""16"",
		""predecessors"": [
			{""id"": 8, ""ship_strategy"": ""Forward""}
		],
		""driver_strategy"": ""Map"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""109.90 M"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0 B"" },
			{ ""name"": ""CPU"", ""value"": ""0.0 "" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""8.0.31 GB"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0 "" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 11,
		""type"": ""pact"",
		""pact"": ""Map"",
		""contents"": ""de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$1"",
		""parallelism"": ""64"",
		""subtasks_per_instance"": ""16"",
		""predecessors"": [
			{""id"": 7, ""ship_strategy"": ""Forward""}
		],
		""driver_strategy"": ""Map"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""109.90 M"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0 B"" },
			{ ""name"": ""CPU"", ""value"": ""0.0 "" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""4.0.15 GB"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0 "" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 10,
		""type"": ""pact"",
		""pact"": ""Reduce"",
		""contents"": ""de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$2"",
		""parallelism"": ""64"",
		""subtasks_per_instance"": ""16"",
		""predecessors"": [
			{""id"": 11, ""ship_strategy"": ""Forward""}
		],
		""driver_strategy"": ""Reduce All"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""109.90 M"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0 B"" },
			{ ""name"": ""CPU"", ""value"": ""0.0 "" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""4.0.15 GB"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0 "" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 9,
		""type"": ""pact"",
		""pact"": ""Reduce"",
		""contents"": ""de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$2"",
		""parallelism"": ""1"",
		""subtasks_per_instance"": ""1"",
		""predecessors"": [
			{""id"": 10, ""ship_strategy"": ""Redistribute""}
		],
		""driver_strategy"": ""Reduce All"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0 B"" },
			{ ""name"": ""CPU"", ""value"": ""0.0 "" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""4.0.15 GB"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0 "" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 12,
		""type"": ""pact"",
		""pact"": ""Bulk Partial Solution"",
		""contents"": ""Partial Solution"",
		""parallelism"": ""64"",
		""subtasks_per_instance"": ""16"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0 B"" },
			{ ""name"": ""CPU"", ""value"": ""0.0 "" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""0.0 B"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0 "" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 6,
		""type"": ""pact"",
		""pact"": ""Map"",
		""contents"": ""de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$3"",
		""parallelism"": ""64"",
		""subtasks_per_instance"": ""16"",
		""predecessors"": [
			{""id"": 7, ""side"": ""first"", ""ship_strategy"": ""Forward"", ""temp_mode"": ""CACHED""},
			{""id"": 9, ""side"": ""second"", ""ship_strategy"": ""Broadcast""},
			{""id"": 12, ""side"": ""second"", ""ship_strategy"": ""Broadcast""}
		],
		""driver_strategy"": ""Map"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""109.90 M"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""CPU"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 5,
		""type"": ""pact"",
		""pact"": ""Reduce"",
		""contents"": ""de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$4"",
		""parallelism"": ""64"",
		""subtasks_per_instance"": ""16"",
		""predecessors"": [
			{""id"": 6, ""ship_strategy"": ""Forward""}
		],
		""driver_strategy"": ""Reduce All"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""109.90 M"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0 B"" },
			{ ""name"": ""CPU"", ""value"": ""0.0 "" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 4,
		""type"": ""pact"",
		""pact"": ""Reduce"",
		""contents"": ""de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$4"",
		""parallelism"": ""1"",
		""subtasks_per_instance"": ""1"",
		""predecessors"": [
			{""id"": 5, ""ship_strategy"": ""Redistribute""}
		],
		""driver_strategy"": ""Reduce All"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0 B"" },
			{ ""name"": ""CPU"", ""value"": ""0.0 "" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 3,
		""type"": ""pact"",
		""pact"": ""Cross"",
		""contents"": ""de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$5"",
		""parallelism"": ""64"",
		""subtasks_per_instance"": ""16"",
		""predecessors"": [
			{""id"": 4, ""side"": ""first"", ""ship_strategy"": ""Broadcast""},
			{""id"": 12, ""side"": ""second"", ""ship_strategy"": ""Forward"", ""temp_mode"": ""PIPELINE_BREAKER""}
		],
		""driver_strategy"": ""Nested Loops (Blocked Outer: de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$4)"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""CPU"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	}
		],
		""partial_solution"": 12,
		""next_partial_solution"": 3,
		""id"": 1,
		""type"": ""bulk_iteration"",
		""pact"": ""Bulk Iteration"",
		""contents"": ""Bulk Iteration"",
		""parallelism"": ""64"",
		""subtasks_per_instance"": ""16"",
		""predecessors"": [
			{""id"": 2, ""ship_strategy"": ""Redistribute""}
		],
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""CPU"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 0,
		""type"": ""sink"",
		""pact"": ""Data Sink"",
		""contents"": ""TextOutputFormat (hdfs://cloud-7:45010/tmp/output/logreg) - UTF-8"",
		""parallelism"": ""64"",
		""subtasks_per_instance"": ""16"",
		""predecessors"": [
			{""id"": 1, ""ship_strategy"": ""Forward""}
		],
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0 B"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0 B"" },
			{ ""name"": ""CPU"", ""value"": ""0.0 "" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	}
	]
}
{code}",,mholzemer,qmlmoon,sewen,,,,,,,,,,,,,,,,,,,,,,"10/Jul/14 10:05;mholzemer;LogisticRegression.java;https://issues.apache.org/jira/secure/attachment/12654963/LogisticRegression.java",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404648,,,Fri Mar 20 10:19:48 UTC 2015,,,,,,,,,,"0|i1xn2n:",404686,,,,,,,,,,,,,,,,,,,,"10/Jul/14 11:24;qmlmoon;I think probably every group in IMPRO3 have this problem. I even created a simple example to illustrate the bug here https://github.com/qmlmoon/test/blob/dev_stratosphere/impro3-ss14-stratosphere/src/main/java/de/tu_berlin/impro3/stratosphere/clustering/kmeanspp/TestBroadcastSet.java .
My solution is to duplicate the points datasource, in order to make the 2 inputs for broadcast or cross operator somehow ""independent"", but strictly speaking it's not. My code is here if you would like to see how it works https://github.com/qmlmoon/test/blob/dev_stratosphere/impro3-ss14-stratosphere/src/main/java/de/tu_berlin/impro3/stratosphere/clustering/kmeanspp/KMeansppGeneric.java .
However, back to the problem. This problem should be fixed as soon as possible in my opinion, because it's really occurs very offen.;;;","10/Jul/14 12:01;mholzemer;Thanks [~qmlmoon]! That workaround seems to be working.
So the deadlock originated in using a datasource both outside and inside of the iteration. By using seperate datasources it is working.

Do you think this solution would also make sense the be used by the optimizer? Just duplicating datasources that appear inside and outside of iterations? Or is there a better solution.;;;","10/Jul/14 12:08;qmlmoon;I would prefer a better solution not just duplicating the datasource..;;;","10/Jul/14 12:53;sewen;I think the reason here is that the streaming deadlock detection does not treat the AllReduce properly. The system has no dam on the path, but the user code effectively puts a dam on the path that is not registered.

Two ways to fix this:
  - 1) No backpressure on operators that are consumed more than once (netstack rework does this, but it will take a bit)
  - 2) Enrich the characteristics of the driver strategy to mark that UDFs can place dams.

I would go for (2).;;;","11/Jul/14 15:53;sewen;The plan is wrong in the sense that the input to mapper with ID 6should be a ""cached pipeline breaker"", but it is only ""cached"".

When I try to reproduce this on the latest code, it correctly assigns that strategy.

Can someone verify that this bug still exists with the latest version (0.6-SNAPSHOT)?;;;","11/Jul/14 19:32;qmlmoon;I tested it with the latest version, the interesting thing is the Est. output size in executionplan looks correctly this time, nonetheless deadlock still exists. 
here is the generated plan for my example which you can find in he previous comment.
```
{
	""nodes"": [

	{
		""id"": 2,
		""type"": ""source"",
		""pact"": ""Data Source"",
		""contents"": ""CSV Input ( ) file:/Users/qml_moon/Downloads/docword.enron.txt"",
		""parallelism"": ""1"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""49.73 M"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""3.80 M"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""49.73 M"" },
			{ ""name"": ""CPU"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""49.73 M"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 3,
		""type"": ""pact"",
		""pact"": ""Reduce"",
		""contents"": ""eu.stratosphere.example.java.TestBroadcastSet$PickOnePoint"",
		""parallelism"": ""1"",
		""predecessors"": [
			{""id"": 2, ""ship_strategy"": ""Forward""}
		],
		""driver_strategy"": ""Reduce All"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""CPU"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""24.86 M"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 1,
		""type"": ""pact"",
		""pact"": ""Cross"",
		""contents"": ""eu.stratosphere.api.java.operators.CrossOperator$DefaultCrossFunction"",
		""parallelism"": ""1"",
		""predecessors"": [
			{""id"": 2, ""side"": ""first"", ""ship_strategy"": ""Forward""},
			{""id"": 3, ""side"": ""second"", ""ship_strategy"": ""Broadcast""}
		],
		""driver_strategy"": ""Nested Loops (Blocked Outer: CSV Input ( ) file:/Users/qml_moon/Downloads/docword.enron.txt)"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""CPU"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 0,
		""type"": ""sink"",
		""pact"": ""Data Sink"",
		""contents"": ""CsvOutputFormat (path: file:/Users/qml_moon/Downloads/oi, delimiter: ,)"",
		""parallelism"": ""1"",
		""predecessors"": [
			{""id"": 1, ""ship_strategy"": ""Forward""}
		],
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""CPU"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	}
	]
}
```;;;","12/Jul/14 12:25;sewen;Ah, i see. There are two issues here:

1) The pipeline breaker at the broadcast variable
2) The pipeline breaker at the cross.

I think (1) is fixed in the latest version.
Let me investigate (2).

;;;","12/Jul/14 13:59;sewen;Okay, I think I have a fix. The cross did not correctly define its pipelined behavior, so the pipeline breakers are incorrect there. I am testing the fix now.;;;","12/Jul/14 14:01;sewen;A strong advise to all people using the cross for this here: If possible, use a map with a broadcast variable. The Cross has a lot of safety nets that are costly and make the performance degrade.;;;","12/Jul/14 17:17;sewen;I have a potential fix at https://github.com/StephanEwen/incubator-flink/tree/master;;;","12/Jul/14 17:18;sewen;Caveat: It is on the latest version of the code, so it has the {{org.apache.flink}} namespace already.;;;","12/Jul/14 17:20;qmlmoon;I tested the latest version, seems to work for me. However, additionally i tried join operator, problem occurs!;;;","13/Jul/14 13:28;sewen;Can you post the plan for that as well?;;;","13/Jul/14 13:47;qmlmoon;{code}
{
""nodes"": [
	{
		""id"": 2,
		""type"": ""source"",
		""pact"": ""Data Source"",
		""contents"": ""CSV Input ( ) file:/Users/qml_moon/Downloads/docword.enron.txt"",
		""parallelism"": ""1"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""49.73 M"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""3.80 M"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""49.73 M"" },
			{ ""name"": ""CPU"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""49.73 M"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 3,
		""type"": ""pact"",
		""pact"": ""Reduce"",
		""contents"": ""org.apache.flink.example.java.TestBroadCastSet$PickOnePoint"",
		""parallelism"": ""1"",
		""predecessors"": [
			{""id"": 2, ""ship_strategy"": ""Forward""}
		],
		""driver_strategy"": ""Reduce All"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""CPU"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""24.86 M"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 1,
		""type"": ""pact"",
		""pact"": ""Join"",
		""contents"": ""org.apache.flink.api.java.operators.JoinOperator$DefaultJoinFunction"",
		""parallelism"": ""1"",
		""predecessors"": [
			{""id"": 2, ""side"": ""first"", ""ship_strategy"": ""Hash Partition on [0]""},
			{""id"": 3, ""side"": ""second"", ""ship_strategy"": ""Hash Partition on [0]"", ""temp_mode"": ""PIPELINE_BREAKER""}
		],
		""driver_strategy"": ""Hybrid Hash (build: CSV Input ( ) file:/Users/qml_moon/Downloads/docword.enron.txt)"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""CPU"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 0,
		""type"": ""sink"",
		""pact"": ""Data Sink"",
		""contents"": ""Print to System.out"",
		""parallelism"": ""1"",
		""predecessors"": [
			{""id"": 1, ""ship_strategy"": ""Forward""}
		],
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""CPU"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	}
	]
}
{code};;;","15/Jul/14 13:12;mholzemer;I tested my original logistic regression code with the current Flink version and it seems to be working now. I also had a look at the plan and it correctly inserts the CACHING_PIPELINE_BREAKER at the mapper now.;;;","15/Jul/14 13:25;sewen;That is good news :-)

Do you have by any chance an insight into the second issue, posted my Mingliang?;;;","15/Jul/14 13:53;mholzemer;I also had a closer look at the example of [~qmlmoon]. I was able to reproduce the deadlock with the join.
Can the deadlock be connected with the fact that a PIPELINE_BREAKER is placed on the probe side of the hybrid hash join? 

I had a look at the threads when deadlocked. The join is waiting for the reduce, the reduce is waiting for the source and the source is waiting for a free buffer, which it seems not to get. ;;;","15/Jul/14 15:51;sewen;I think pipeline breakers for joins need be on the probe side, that looks correct to me. The build side is a pipeline breaker as well.;;;","20/Mar/15 10:19;sewen;Fixed via 9c77f0785e43326521da5e535f9ab1f05a9c6280;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pipeline Breakers and Caches are not displayed in the WebFrontend,FLINK-1014,12726273,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,sewen,sewen,09/Jul/14 09:06,02/Sep/14 13:04,14/Jul/23 05:57,02/Sep/14 13:04,0.6-incubating,,,0.7.0-incubating,,,Runtime / Web Frontend,,,,0,,,"I think this is sort of crucial to fix, as this is important information for plan debugging.",,sewen,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/14 15:08;sewen;messedui.png;https://issues.apache.org/jira/secure/attachment/12655225/messedui.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404380,,,Tue Sep 02 13:04:27 UTC 2014,,,,,,,,,,"0|i1xlfj:",404419,,,,,,,,,,,,,,,,,,,,"11/Jul/14 15:08;sewen;The webfrontend also displays several properties strangely, see attached screenshot file.;;;","02/Sep/14 13:04;sewen;Fixed in the latest master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArithmeticException: / by zero in MutableHashTable,FLINK-1013,12726147,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,trohrmann,trohrmann,08/Jul/14 18:24,06/Aug/15 17:58,14/Jul/23 05:57,06/Aug/15 17:58,,,,0.9,,,,,,,0,,,"I encountered a division by zero exception in the MutableHashTable. It happened when I joined two datasets of relatively big records (approx. 40-50 MB I think). When joining them the buildTableFromSpilledPartition method of the MutableHashTable is called. In case that the available buffers are smaller than the needed number of buffers, the mutable hash table will calculate the bucket count
{code}
bucketCount = (int) (((long) totalBuffersAvailable) * RECORD_TABLE_BYTES / (avgRecordLenPartition + RECORD_OVERHEAD_BYTES));
{code}

If the average record length is sufficiently large, then the bucket count will be 0. Initializing the hash table with a 0 bucket count will cause then the division by 0 exception. I don't know whether this problem can be mitigated but it should at least throw a meaningful exception instead of the ArithmeticException.",,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404254,,,Thu Aug 06 17:58:10 UTC 2015,,,,,,,,,,"0|i1xknz:",404294,,,,,,,,,,,,,,,,,,,,"09/Jul/14 09:04;sewen;Can we simply solve this by assuming a may for the avgRecordLenPartition? It make sense anyways to not let the number of buckets get too low. This formula is only to find a tradeoff between really sort records (like Tuple2<Long, Double>) and longer records.

How about capping that value at 512? That should be fine. The loss in initial partition buffers (in favor of bucket buffers) is rather small then...

Make the formula
{code}
bucketCount = (int) (((long) totalBuffersAvailable) * RECORD_TABLE_BYTES / (Math.min(512, avgRecordLenPartition) + RECORD_OVERHEAD_BYTES));
{code};;;","09/Jul/14 12:13;trohrmann;That should do the trick. However, it did not work when I set the lower bound of bucketCount to 1. Then the system will complain that the recursive descent of the construction process of the hash table is too deep. Thus, we should try they new formula out and see whether it works.;;;","06/Aug/15 17:58;sewen;Fixed already a while back by delegating computation to existing utility function that is also used to build the initial table.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Conflicting Scala Versions,FLINK-1010,12726078,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,sewen,sewen,08/Jul/14 13:51,30/Jul/14 19:20,14/Jul/23 05:57,30/Jul/14 19:19,0.6-incubating,,,0.6-incubating,,,Build System,,,,0,,,"I am seeing the following error. Can we simply set the Scala Version for everything to 2.10.4?
{code}
[WARNING]  Expected all dependencies to require Scala version: 2.10.3
[WARNING]  org.scala-lang:scala-reflect:2.10.3 requires scala version: 2.10.3
[WARNING]  eu.stratosphere:stratosphere-scala:0.6-SNAPSHOT requires scala version: 2.10.3
[WARNING]  eu.stratosphere:stratosphere-scala:0.6-SNAPSHOT requires scala version: 2.10.3
[WARNING]  org.scala-lang:scala-compiler:2.10.3 requires scala version: 2.10.3
[WARNING]  org.scalatest:scalatest_2.10:2.2.0 requires scala version: 2.10.4
[WARNING] Multiple versions of scala libraries detected!
{code}",,aljoscha,sewen,warneke,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404185,,,Wed Jul 30 19:19:23 UTC 2014,,,,,,,,,,"0|i1xk8v:",404225,,,,,,,,,,,,,,,,,,,,"08/Jul/14 15:48;aljoscha;I think it should work.;;;","08/Jul/14 18:38;warneke;I fixed this already as part of FLINK-386 which is subject to be merged.;;;","30/Jul/14 19:18;warneke;FLINK-386 got merged to the master branch, so I propose to close this.;;;","30/Jul/14 19:19;warneke;FLINK-386 got merged to the master branch, so I propose to close this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KMeans algorithm is not visualized by the Web Interface,FLINK-1009,12726076,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,asteriosk,asteriosk,08/Jul/14 13:46,01/Oct/14 14:36,14/Jul/23 05:57,01/Oct/14 14:36,pre-apache-0.5,,,,,,Runtime / Web Frontend,,,,0,,,"I start the web interface on my local machine and upload the example jar file of KMeans-Java. 

I click on the job on the left but cannot see the graph that draws the operators. Same applies for the optimized plan.

Other algos can be visualized e.g. connected components.

",Mac OS X and Ubuntu linux. OpenJDK 1.7.,asteriosk,fhueske,ktzoumas,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404183,,,Wed Oct 01 14:36:28 UTC 2014,,,,,,,,,,"0|i1xk8f:",404223,,,,,,,,,,,,,,,,,,,,"09/Jul/14 20:02;rmetzger;Are you running 0.5 or 0.5.1 ? I think we added a fix into 0.5.1 that fixed this issue.
[~ktzoumas], I think you had a similar issue on a Mac, how did you resolve it?;;;","10/Jul/14 06:44;ktzoumas;I did not resolve it in the end, but I will take a look again;;;","30/Sep/14 16:49;sewen;It works for me with the latest 0.7 and also in the 0.6 release.;;;","01/Oct/14 14:36;fhueske;I checked it as well. It's working.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
createProgramPlan() throws exception,FLINK-1008,12726062,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,bastiank,bastiank,08/Jul/14 12:28,28/Feb/19 14:29,14/Jul/23 05:57,17/Nov/14 13:22,,,,0.8.0,,,,,,,0,,,"After calling createProgramPlan() I get the following exception:
java.lang.RuntimeException: No data sinks have been created yet. A program needs at least one sink that consumes data. Examples are writing the data set or printing it.
	at eu.stratosphere.api.java.ExecutionEnvironment.createProgramPlan(ExecutionEnvironment.java:618)
	at eu.stratosphere.api.java.LocalEnvironment.execute(LocalEnvironment.java:51)
	at eu.stratosphere.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:516)

The plan execute and we also have data sinks, so the error is really confusing.",,bastiank,fhueske,githubbot,knub,mholzemer,sewen,,,,,,,,,,,,,,,,,,,"08/Jul/14 15:04;bastiank;impro3-ss14-stratosphere.zip;https://issues.apache.org/jira/secure/attachment/12654595/impro3-ss14-stratosphere.zip",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404169,,,Mon Nov 17 13:45:07 UTC 2014,,,,,,,,,,"0|i1xk5b:",404209,,,,,,,,,,,,,,,,,,,,"08/Jul/14 13:56;bastiank;getExecutionPlan() also throws an exception, but prints still a plan.;;;","08/Jul/14 14:08;sewen;Are you sure you added a sink? (print(), output(...), writeTo(...) )

Can you paste the program code here?;;;","08/Jul/14 15:04;bastiank;As usual, execute the class HAC ;);;;","08/Jul/14 15:33;sewen;Okay, this is somewhat expected behavior.

Whenever you trigger the plan creation / execution, it works on the part of the program so far. After that, all command create a ""new plan"". It is supposed to allow you things like that:

{code}
DataSet<...> stage = env.doSomething();
stage.print();
env.execute();

DataSet<...> stage2 = stage.doSomethingMore();
stage2.writeTo(...);
env.execute();
{code}

When you call {{createPlan()}} you ""finish"" that part of the program. When you then call execute, the second part of the program is empty.

It is debatable, tough, whether {{getExecutionPlan()}} should finalize a current stage and start a new one.;;;","12/Jul/14 17:23;sewen;Anyone has an opinion on this? Otherwise, I would go and change it such that {{getExecutionPlan()}} does not trigger a new stage of execution.;;;","14/Jul/14 09:13;mholzemer;Yes I think it is a good idea to do so. I see no reason why getExecutionPlan() should finalize the current stage.;;;","05/Nov/14 16:26;knub;I just ran into this issue, and could provide a PR.
Please have a look at [my commit|https://github.com/knub/incubator-flink/commit/7359613e54d2d66e903b3f93a4a0c190aa73e2e3].
Feedback is welcome, as this would be my first PR for Flink.;;;","05/Nov/14 18:58;fhueske;Hi Stefan,

thanks for contributing and welcome to the Flink community!

I had a very brief look at your PR and didn't find any obvious problems with it.
In general, code review is one aspect of pull requests. So it is not really necessary to do a pre-check. However, you should make sure, that your code builds and does not break any tests by running ``mvn clean install`` locally or use a build server like Travis CI. Also rebasing your changes to the lasted master version is appreciated. The [coding guidelines|http://flink.incubator.apache.org/coding_guidelines.html] list a few more points.

Cheers, Fabian;;;","05/Nov/14 20:19;sewen;The commit looks good to me.;;;","05/Nov/14 21:42;githubbot;GitHub user knub opened a pull request:

    https://github.com/apache/incubator-flink/pull/184

    [FLINK-1008] Fix createProgramPlan() throws exception

    Problem was, that ExecutionEnvironment#getExecutionPlan clears the data
    sinks, i.e. a following ExecutionEnvironment#execute will throw an error
    because there are no data sinks.
    
    This introduces a new flag for ExecutionEnvironment#createProgramPlan to
    indicate, that the the sinks shall not be cleared.
    This does not break any existing code.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/knub/incubator-flink knub/flink-1008

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/184.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #184
    
----
commit 7359613e54d2d66e903b3f93a4a0c190aa73e2e3
Author: Stefan Bunk <stefan.bunk@googlemail.com>
Date:   2014-11-05T16:21:24Z

    [FLINK-1008] Fix createProgramPlan() throws exception
    
    Problem was, that ExecutionEnvironment#getExecutionPlan clears the data
    sinks, i.e. a following ExecutionEnvironment#execute will throw an error
    because there are no data sinks.
    
    This introduces a new flag for ExecutionEnvironment#createProgramPlan to
    indicate, that the the sinks shall not be cleared.
    This does not break any existing code.

----
;;;","05/Nov/14 21:58;knub;Fabian: I asked here first, because the second point in the coding guidelines says: ""We consider pull requests as requests to merge the referenced code as is into the current stable master branch [...] If you rather want comments on your code, post a link to your working branch.""
So you say this is fine on GitHub as well?;;;","05/Nov/14 22:26;fhueske;Good point ;-)
We might want to rephrase this section. 
IMO, posting complete PRs is fine, however we should avoid to use PRs for lengthy architecture or design discussions. 

Thanks for the contribution!;;;","07/Nov/14 15:22;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/184#issuecomment-62159097
  
    Looks good to me!
;;;","07/Nov/14 15:22;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/184#issuecomment-62159158
  
    Thank you for the patch!
    
    I'll try and merge this in the next batch...
;;;","17/Nov/14 13:22;sewen;Fixed in 42828f24596ec2432fbd506ce08ac18c23fd5350;;;","17/Nov/14 13:45;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/184
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job fails because an IndexOutOfBoundsException,FLINK-1000,12725389,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rwaury,bastiank,bastiank,03/Jul/14 23:07,16/Jul/14 17:43,14/Jul/23 05:57,16/Jul/14 17:43,0.6-incubating,,,0.6-incubating,,,,,,,0,,,"After some while of running my job I get the following exception:

eu.stratosphere.nephele.client.JobExecutionException: java.lang.IndexOutOfBoundsException: Index: 326, Size: 2
	at java.util.ArrayList.rangeCheck(ArrayList.java:635)
	at java.util.ArrayList.get(ArrayList.java:411)
	at eu.stratosphere.pact.runtime.hash.InMemoryPartition$ReadView.setReadPosition(InMemoryPartition.java:365)
	at eu.stratosphere.pact.runtime.hash.InMemoryPartition.readRecordAt(InMemoryPartition.java:190)
	at eu.stratosphere.pact.runtime.hash.CompactingHashTable$HashTableProber.getMatchFor(CompactingHashTable.java:1148)
	at eu.stratosphere.pact.runtime.task.JoinWithSolutionSetFirstDriver.run(JoinWithSolutionSetFirstDriver.java:134)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:503)
	at eu.stratosphere.pact.runtime.iterative.task.AbstractIterativePactTask.run(AbstractIterativePactTask.java:132)
	at eu.stratosphere.pact.runtime.iterative.task.IterationIntermediatePactTask.run(IterationIntermediatePactTask.java:84)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:368)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:260)
	at java.lang.Thread.run(Thread.java:745)

	at eu.stratosphere.nephele.client.JobClient.submitJobAndWait(JobClient.java:354)
	at eu.stratosphere.client.LocalExecutor.executePlan(LocalExecutor.java:240)
	at eu.stratosphere.api.java.LocalEnvironment.execute(LocalEnvironment.java:55)
	at eu.stratosphere.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:516)

You need to execute the ""de.tu_berlin.impro3.stratosphere.clustering.hac.HAC_Flink_Java"".
One problem is, that this bug occurs after something like 1 and a half hour.",,bastiank,hsaputra,rwaury,,,,,,,,,,,,,,,,,,,,FLINK-955,,"04/Jul/14 17:23;rwaury;flink1000.patch;https://issues.apache.org/jira/secure/attachment/12654128/flink1000.patch","03/Jul/14 23:10;bastiank;src.zip;https://issues.apache.org/jira/secure/attachment/12654007/src.zip",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403549,,,Wed Jul 16 17:43:41 UTC 2014,,,,,,,,,,"0|i1xgdj:",403593,,,,,,,,,,,,,,,,,,,,"03/Jul/14 23:10;bastiank;Source;;;","04/Jul/14 12:19;rwaury;Hi Bastian,

can you give me some details about the job you are trying to run.

- DOP/cluster size
- data set (content, size etc.)
- where exactly does it fail during the delta iteration (first, last or somewhere in between, from the stack trace I can only what part of the runtime code fails)

Can you try to give the job a little more memory and see if that changes anything?

Cheers,
Robert;;;","04/Jul/14 17:23;rwaury;I managed to recreate a similar error.

Tell me if the patch works for you.;;;","05/Jul/14 14:48;bastiank;Okay, this patch seems to fix the problem. But it introduces another bug, now after some time ""DataSet<ClusterHistory> update = linkageSet.map(new HistoryUpdater());"" generates a NullPointer value. This can never happen, especially because the debug printing works. Try to execute my class and you should also get this error.;;;","08/Jul/14 11:24;rwaury;I was unable to reproduce the bug locally but the pull request for FLINK-955 also contains a few bug fixes that may fix the null value issue and it also contains the fix for the IndexOutOfBoundsException.

Can you try out the pull request and post the results.;;;","15/Jul/14 23:08;bastiank;K, this bug seems to be fixed. Thanks for your help!;;;","16/Jul/14 17:43;rwaury;Fixed in https://github.com/apache/incubator-flink/pull/57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in BinaryUnionReplacer for unions in iterations,FLINK-997,12724923,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,trohrmann,trohrmann,02/Jul/14 09:45,06/Jan/15 10:18,14/Jul/23 05:57,06/Jan/15 10:18,,,,0.9,,,Runtime / Task,,,,0,,,The BinaryUnionReplacer throws a NullPointerException when replacing binary unions in an iteration which are also the last node of the iteration. The NullPointerException occurs when the outgoing channels are requested.,,sewen,trohrmann,,,,,,,,,,,,,,FLINK-641,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403106,,,Tue Jan 06 10:18:07 UTC 2015,,,,,,,,,,"0|i1xdpb:",403161,,,,,,,,,,,,,,,,,,,,"02/Jul/14 14:41;trohrmann;The problem is that a union currently cannot be at the root of a data flow. A solution would be to insert an identity mapper in the BinaryUnionReplacer. However, then we have to generate two different plans, one for the record-based API and another for the new operators. Currently, I don't know from where I can retrieve the information whether the plan has been generated using the old or new API....

Maybe we should wait until the Scala API has been moved to the new operators. Then we can easily attach an identity mapper to union roots of iterations.;;;","06/Jan/15 10:18;sewen;Fixed in 259f10c094292085486739c1d1bde5986b61396a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException while translating union node,FLINK-996,12724922,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,02/Jul/14 09:39,03/Jul/14 12:51,14/Jul/23 05:57,03/Jul/14 12:51,,,,0.6-incubating,,,,,,,0,,,The NepheleJobGraphGenerator throws a NullPointerException when translating a binary union operator. The BinaryUnionPlanNode is not replaced by a NAryUnionPlanNode and thus is still treated as a DualInputVertex. Accessing the driver code of the BinaryUnionPlanNode causes then the NullPointerException.,,githubbot,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403105,,,Thu Jul 03 12:51:50 UTC 2014,,,,,,,,,,"0|i1xdp3:",403160,,,,,,,,,,,,,,,,,,,,"02/Jul/14 14:43;trohrmann;To be more precise: The exception only occurs if the union is used for at least two operators as an input.;;;","02/Jul/14 14:57;githubbot;GitHub user tillrohrmann opened a pull request:

    https://github.com/apache/incubator-flink/pull/56

    [FLINK-996] Fix for union replacement

    The problem was that the BinaryUnionNodes were only replaced for the first output channel instead for all.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/tillrohrmann/incubator-flink FLINK-996

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/56.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #56
    
----
commit ccb7ab2a7a21fc3b084313924a39799c19113639
Author: Till Rohrmann <till.rohrmann@gmail.com>
Date:   2014-07-02T14:00:55Z

    Fix for union replacement

----
;;;","03/Jul/14 11:49;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/56#issuecomment-47896648
  
    Looks good, will merge.
;;;","03/Jul/14 12:51;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/56
;;;","03/Jul/14 12:51;sewen;Fixed via 32a003d5c0f507f4634851434e8ffb8ce6b880b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala API: Compiler Hints are not forwarded,FLINK-990,12724370,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,29/Jun/14 22:56,03/Jul/14 16:59,14/Jul/23 05:57,03/Jul/14 16:59,,,,0.6-incubating,,,,,,,0,,,"The Scala API contains functions such as preserve, observe, neglect, uniqueKey etc. which can be used to specify compiler hints. However, they are not forwarded to the compiler. Thus an operation:

DataSet[A] ds = input.map{ x=> x}
ds.preserve(x=>x, y=>y)

which should say that the fields stay constant, does not have an effect. ",,aljoscha,githubbot,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402553,,,Thu Jul 03 16:59:19 UTC 2014,,,,,,,,,,"0|i1xae7:",402620,,,,,,,,,,,,,,,,,,,,"30/Jun/14 10:09;githubbot;GitHub user tillrohrmann opened a pull request:

    https://github.com/apache/incubator-flink/pull/51

    FLINK-990 Scala API: Compiler Hints are not forwarded

    Added constant fields and combinable annotations forwarding to Scala API. The problem is that annotations information is not available when the Operators are created. Thus, we need a operator translation similar to the new Java API which triggers an update of the respective semantic information.  Furthermore, the Scala generated annotations are appended to the operator class, whereas the udf is checked for annotations. That's why Scala's combinableReduce operation does not generate a combinable operator.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/tillrohrmann/incubator-flink FLINK-990

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/51.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #51
    
----
commit 735e93cd17df35b7f7340c777d03e57f00562472
Author: Till Rohrmann <till.rohrmann@gmail.com>
Date:   2014-06-30T09:45:49Z

    Added constant fields and combinable annotations forwarding to Scala API

----
;;;","30/Jun/14 12:21;aljoscha;Why did you only add the {{applyHints}} call in line 108 in CompilerHints.scala and not in the other methods where {{addCardinality}} is called?

Also, the system is a mess there because the way the annotations and hints are handled here is from a time when the system used annotations to specify those hints and did not really provide an API. This stuff will have to be reworked to bring the scala interface up to speed with the java interface.;;;","30/Jun/14 12:53;trohrmann;Good point. I'll add it for addCardinality as well.

You're right, it's more like a temporary fix until the Scala API has been moved to the non-Record based operators. It would make things considerably easier to maintain if they were using the same utility functions. We could probably reuse the OperatorTranslation functionality, which, among others, sets the semantics properties after the Operator plan has been created.;;;","02/Jul/14 09:59;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/incubator-flink/pull/51#issuecomment-47757737
  
    Changed so that all addCardinality calls will trigger an applyHints call.
;;;","03/Jul/14 13:03;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/51#issuecomment-47926329
  
    Looks good to me. I will merge it...
;;;","03/Jul/14 16:58;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/51
;;;","03/Jul/14 16:59;sewen;Fixed via b422fe27879ab2d5ad54f782b7321b1b5808c245;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve not enough slots SchedulingException message,FLINK-989,12724093,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,uce,uce,27/Jun/14 10:14,28/Feb/19 14:02,14/Jul/23 05:57,21/Sep/14 02:15,,,,0.7.0-incubating,,,Runtime / Coordination,,,,3,,,"I think it would be good to have a more detailed message for the following SchedulingException (available slots, required slots etc.):

{code:java}
eu.stratosphere.client.program.ProgramInvocationException: The program execution failed: eu.stratosphere.nephele.jobmanager.scheduler.SchedulingException: Not enough slots to schedule job 04d754249c6c580000cd461d195ca000
        at eu.stratosphere.nephele.jobmanager.scheduler.DefaultScheduler.scheduleJob(DefaultScheduler.java:150)
        at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:504)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:417)
        at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:951)
{code}
",,chesnay,jllopezpino,powibol,sewen,uce,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402278,,,Sun Sep 21 02:15:55 UTC 2014,,,,,,,,,,"0|i1x8of:",402342,,,,,,,,,,,,,,,,,,,,"27/Jun/14 10:54;sewen;Good catch, I'll get onto that...;;;","11/Jul/14 10:09;chesnay;would this be sufficient?

{code:java}
throw new SchedulingException(""Not enough slots to schedule job "" + executionGraph.getJobID()
+ "" Req.: "" + requiredSlots + "" Avail.: "" + availableSlots + "". Either reduce the DOP of your program, ""
+ ""wait for other programs to finish or configure the number of slots in conf/flink-conf.yaml ."");
{code};;;","11/Jul/14 11:47;sewen;Slight edit:

""Not enough available task slots to run job "" + jobName ""("" jobId + "").  Required: "" + requiredSlots + "" Available: "" + availableSlots + "". Either reduce the parallelism of your program, wait for other programs to finish, or increase the number of task slots in the cluster by adding more machines or increasing the number of slots per machine in conf/flink-conf.yaml ."");;;;","15/Jul/14 17:02;sewen;I am fixing this as part of another effort.;;;","16/Jul/14 16:27;jllopezpino;I think this also applies to some other exceptions in the job manager, they are not very descriptive. I can send a pull request that fixes some of them, if you want to.

A couple of examples:
{code:title=DefaultScheduler.java|borderStyle=solid}
		} catch (InstanceException e) {
			final String exceptionMessage = StringUtils.stringifyException(e);
			LOG.error(exceptionMessage);
			this.jobQueue.remove(executionGraph);
			throw new SchedulingException(exceptionMessage);
		}
{code}


{code:title=DefaultScheduler.java|borderStyle=solid}
			switch (outputGate.getChannelType()) {
			case NETWORK:
				deployTarget = false;
				break;
			case IN_MEMORY:
				deployTarget = true;
				break;
			default:
				throw new IllegalStateException(""Unknown channel type"");
			}
{code}


;;;","21/Sep/14 02:15;sewen;Fixed via e2fe9ee6f9a8a5be47b1210d3b609eab1b2ed31e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExtendedManagementProtocol.getAvailableSlots() returns non-serializable type,FLINK-988,12724087,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,rmetzger,rmetzger,27/Jun/14 09:11,28/Feb/19 14:02,14/Jul/23 05:57,06/Jan/15 13:24,0.6-incubating,,,0.9,,,Runtime / Coordination,,,,0,,,"I think the [[ExtendedManagementProtocol.getAvailableSlots()]] method is returning an invalid type.
Our RPC service is casing the return type of the RPC invocation to [[IOReadableWritable]]:
https://github.com/apache/incubator-flink/blob/master/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/RPC.java#L419

This will lead to a ClassCastException at runtime.
The method getAvailableSlots() is not used for any RPC requests (only for local testing, without an actual RPC call).",,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402272,,,Tue Jan 06 13:24:54 UTC 2015,,,,,,,,,,"0|i1x8n3:",402336,,,,,,,,,,,,,,,,,,,,"27/Jun/14 09:44;sewen;So {{int}} is not auto-converted into an {{IntWritable}} in the RPC?;;;","27/Jun/14 11:18;rmetzger;I don't think so. For my YARN RPC service, a method was returning a boolean and it failed at that point with a class cast exception.
This method is also returning a {{IntegerRecord}} instead of an {{int}}: https://github.com/apache/incubator-flink/blob/b4b633eab9a70e14d2e0dd5252f4b092a3689093/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/protocols/JobManagementProtocol.java#L76;;;","06/Jan/15 13:24;sewen;This should be soved with the new Akka based communication;;;","06/Jan/15 13:24;sewen;Fixed through the Akka RPC as of 88e64fc7837a3fb081e15b23a07d7764c91f3b6f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update config.md of the user docs,FLINK-985,12723666,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,25/Jun/14 16:08,25/Jun/14 17:47,14/Jul/23 05:57,25/Jun/14 17:47,0.6-incubating,,,0.6-incubating,,,,,,,0,,,"The configuration reference is not in sync with the current code.

I will start working on this...",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401851,,,Wed Jun 25 17:47:55 UTC 2014,,,,,,,,,,"0|i1x633:",401920,,,,,,,,,,,,,,,,,,,,"25/Jun/14 17:47;sewen;Fixed via 47239b282f8fc350a05a4e8050ea931263140ae4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebClient makes names of functions unreadable,FLINK-983,12723619,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,qmlmoon,sewen,sewen,25/Jun/14 12:07,11/Nov/14 11:11,14/Jul/23 05:57,11/Nov/14 10:57,,,,,,,Runtime / Web Frontend,,,,0,,,"The named of Functions are truncated and abbreviated like ""Properties of ...reeDistribution, delimiter: ) - ID = 7"".

There should be a place where the names are printed fully, not abbreviated/truncated.",,githubbot,sewen,uce,,,,,,,,,,,,,,,,,,,FLINK-954,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401804,,,Tue Nov 11 11:11:05 UTC 2014,,,,,,,,,,"0|i1x5sn:",401873,,,,,,,,,,,,,,,,,,,,"25/Jun/14 12:45;sewen;Also, some names get changed (maybe encoding related problems): {{TextOutputFormat (file:/tmp) - UTF-8}} becomes {{tmp) - UTF-8}};;;","09/Nov/14 14:56;githubbot;GitHub user qmlmoon opened a pull request:

    https://github.com/apache/incubator-flink/pull/190

    [FLINK-983] Fix WebClient truncated function names.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/qmlmoon/incubator-flink webclient-showplan

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/190.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #190
    
----
commit 53ff2d78dfc90cfd1749238ae9421bab471f6e65
Author: mingliang <qmlmoon@gmail.com>
Date:   2014-11-09T14:50:26Z

    [FLINK-983] Fix WebClient truncated function names.

----
;;;","10/Nov/14 09:13;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/190#issuecomment-62358244
  
    Looks good to me
;;;","11/Nov/14 10:44;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/190#issuecomment-62530346
  
    OK. I'm merging this in the current batch
;;;","11/Nov/14 10:57;uce;Fixed in 2f1176a78e4198933fb1c2258f9df21dd38c41ef.;;;","11/Nov/14 11:11;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/190
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Buffer leak in OutputChannel#sendBuffer(Buffer),FLINK-980,12723508,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,uce,uce,uce,24/Jun/14 23:15,25/Jun/14 08:33,14/Jul/23 05:57,25/Jun/14 08:33,pre-apache-0.5.1,,,,,,Runtime / Coordination,,,,0,,,"An empty {{Buffer}} sent via {{OutputChannel}} escapes the respective {{LocalBufferPool}}, because the early return in {{OutputChannel#sendBuffer(Buffer)}} misses the buffer recycling step.

----

The distributed runtime uses a global network buffer pool per TaskManager, which is distributed to local buffer pools on a per task level at runtime. The output side of the task gets a single local sub pool (1 pool for n OutputGates) and each InputGate gets one (m pools for m InputGates).

To ensure deadlock free execution, the output pool needs to have at least one buffer per logical channel ({{OutputChannel}}). (The same reasoning applies to the input side, but this is not relevant here.) When a record is collected by the UDF it is serialized to one or more buffers from the pool and dispatched. After dispatching, a buffer is recycled and becomes available again in the pool.

Each Buffer has a fixed maximum size for the backing MemorySegment. If the network buffer is only partially filled, the size of the Buffer is limited to the respective size. If an empty buffer (size 0) is send, the sending of the buffer is entirely skipped and the dispatcher is never called. 

Because the recycling happens in the dispatcher, this respective buffer escapes the buffer pool.",,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401693,,,Wed Jun 25 08:33:02 UTC 2014,,,,,,,,,,"0|i1x54n:",401765,,,,,,,,,,,,,,,,,,,,"25/Jun/14 08:33;uce;Fixed in f13ad5b415a57e7d1c97319935a04f076cc1776b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler and JobGrph refactoring broke NetworkThroughput test,FLINK-979,12723454,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,uce,uce,uce,24/Jun/14 19:57,25/Jun/14 08:29,14/Jul/23 05:57,25/Jun/14 08:29,pre-apache-0.5.1,,,,,,Runtime / Task,,,,0,,,"The refactoring of the scheduler and the JobGraph broke the NetworkThroughput test.

{code:java}
21:50:27,199 ERROR eu.stratosphere.nephele.client.JobClient                      - ERROR: eu.stratosphere.nephele.executiongraph.GraphConversionException: java.lang.RuntimeException: No input format has been set for job vertex: 35414f433c65380065e604fcce42b800
{code}

The test has been modifed during the merge to use a plain {{JobInputVertex}} instead of the previous {{GenericJobInputVertex}}, which worked without an InputFormat. With {{JobInputVertex}} we need to specify a dummy InputFormat.

The test does not startup (just a local executor is started and shutdown) and we don't see an error during {{mvn verify}}.",,githubbot,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401641,,,Wed Jun 25 08:29:09 UTC 2014,,,,,,,,,,"0|i1x4tj:",401714,,,,,,,,,,,,,,,,,,,,"24/Jun/14 21:30;githubbot;GitHub user uce opened a pull request:

    https://github.com/apache/incubator-flink/pull/41

    [FLINK-979] Fix NetworkThroughput test input and output task config

    - Set DummyInputFormat and DummyOutputFormat via TaskConfig to respect task hierarchy refactoring.
    - Run test via main method instead of JUnit test runner (this was originally a test in order to use RecordAPITestBase for JobGraph submission).

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/uce/incubator-flink FLINK-979

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/41.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #41
    
----
commit 58b7c35ce43cec7a1abbf217f4e15262f76726c9
Author: uce <u.celebi@fu-berlin.de>
Date:   2014-06-24T21:24:17Z

    [FLINK-979] Fix NetworkThroughput test input and output task config
    
    - Set DummyInputFormat and DummyOutputFormat via TaskConfig to respect task
      hierarchy refactoring.
    - Run test via main method instead of JUnit test runner (this was originally
      a test in order to use RecordAPITestBase for JobGraph submission).

----
;;;","25/Jun/14 07:17;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/41#issuecomment-47067902
  
    I think its good to merge.
;;;","25/Jun/14 08:27;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/41
;;;","25/Jun/14 08:29;uce;Merged in 2f0bd8fa26f5ed13d5f116b296557df8207a8260.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManager Webfrontend does not display running jobs since switch to slot-based Scheduler,FLINK-978,12723431,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,JonathanH5,sewen,sewen,24/Jun/14 18:34,26/Jun/14 08:58,14/Jul/23 05:57,25/Jun/14 17:09,0.6-incubating,,,,,,Runtime / Web Frontend,,,,0,,,[~JonathanH5] Could you have a look at this? You are the one most familiar with that component.,,githubbot,JonathanH5,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401618,,,Thu Jun 26 08:58:28 UTC 2014,,,,,,,,,,"0|i1x4of:",401691,,,,,,,,,,,,,,,,,,,,"25/Jun/14 13:08;JonathanH5;Hi,

what is the switch to slot-based Scheduler and what did it change regarding the Jobmanager?
Sadly I am not able to build the current master because of some errors in the scala package.... . Why is that so?;;;","25/Jun/14 13:41;rmetzger;Regarding the slot allocator, this is the pull request that briefly describes the feature: https://github.com/stratosphere/stratosphere/pull/808

Why can't you build the master? (do you have an error message) In addition to that, Ufuk is on a merging spree, so the master is changing every 20 minutes ;);;;","25/Jun/14 14:04;JonathanH5;Ok, so I am going to wait until he is finished ;)

Yes, Maven can not find ""symbols"" in the CharComperatorTest. In an older version of the master everything is fine.;;;","25/Jun/14 14:19;rmetzger;No, our code should always build!

Can you copy paste the whole error message?;;;","25/Jun/14 14:56;JonathanH5;Ok, false alarm ... everything works again ... . This is strange, because i always clean before i package and did not change anything on the code ...;;;","25/Jun/14 15:02;JonathanH5;To the main question:
I guess that the json files now look differently. That's why the interface can not understand it ... :S;;;","25/Jun/14 16:14;JonathanH5;Just to keep you up to date: There is a comma in the json which is not allowed. That is why it fails... . 
I will try to find out where it comes from.;;;","25/Jun/14 16:36;githubbot;Github user JonathanH5 commented on the pull request:

    https://github.com/apache/incubator-flink/pull/43#issuecomment-47126851
  
    Ok, But I will do this after I know what is wrong with https://issues.apache.org/jira/browse/FLINK-978
;;;","25/Jun/14 17:08;githubbot;GitHub user JonathanH5 opened a pull request:

    https://github.com/apache/incubator-flink/pull/46

    Fixed Bug Flink-978

    https://issues.apache.org/jira/browse/FLINK-978

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/JonathanH5/incubator-flink fixWebI

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/46.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #46
    
----
commit 13a7d4182c2a0c40f71730f35c9f624ea8a80770
Author: Jonathan <jonathan@hasenburg.de>
Date:   2014-06-25T17:07:19Z

    Fixed Bug Flink-978

----
;;;","25/Jun/14 17:08;JonathanH5;Ok, fixed it https://github.com/apache/incubator-flink/pull/46.

Can someone else please verify it?

I don't know how to say it in a different way: ""Das war die Nagel im Heuhaufen"";;;","26/Jun/14 07:55;rmetzger;Yes, I'm looking into it right now. Another user already also complained about the issue.
(Btw: your German sentence is not correct (""die Nagel"", its a needle, not a nail). The English version of it is the ""Needle in a haystack"");;;","26/Jun/14 08:10;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/46
;;;","26/Jun/14 08:11;rmetzger;Fixed in https://github.com/apache/incubator-flink/commit/69589fa90ee08bb1c87a56bf57102f5a99d8af79;;;","26/Jun/14 08:58;JonathanH5;BTW Robert, I did not say that it is called nail.... . But in German (as you should know) it is called ""Nadel im Heuhaufen"".... . So I do not really see what is wrong there ... . ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log more of the environment upon JobManager and TaskManager startup,FLINK-969,12723182,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,sewen,sewen,23/Jun/14 18:31,29/Jan/17 14:42,14/Jul/23 05:57,25/Jun/14 09:05,0.6-incubating,,,,,,Runtime / Coordination,,,,0,,,"I would like to include things like Java Version, JVM model and build, heap sizes, and other flags, as far as they can be determined from within the JVM.

Something like discussed here:

http://stackoverflow.com/questions/2541627/how-do-i-get-the-commandline-that-started-the-process",,rmetzger,sewen,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401369,,,Wed Jun 25 09:05:45 UTC 2014,,,,,,,,,,"0|i1x35j:",401444,,,,,,,,,,,,,,,,,,,,"23/Jun/14 19:10;rmetzger;+1
I recently also though about adding this feature.
We can log the output of $JAVA_HOME -version to the stdout-log file, then you don't have to get the stuff out of the JVM.;;;","24/Jun/14 12:27;sewen;I would personally prefer it if all the information about teh environment were in the log file, on startup.

But if it is too clumsy to obtain that information, the out file would be okay as well.;;;","25/Jun/14 09:05;uce;Fixed in 28863ee089bd0f81d6541a631c2d8699eaa71471.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing java-src directory in packaged examples directory,FLINK-966,12723115,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,uce,uce,23/Jun/14 12:12,13/Aug/14 20:34,14/Jul/23 05:57,13/Aug/14 20:34,pre-apache-0.5.1,,,0.6-incubating,,,Build System,,,,0,,,"The packaged examples direcetory include a directory {{scala-src}} with the Scala example sources, but the corresponding {{java-src}} is missing.",,githubbot,rmetzger,sewen,uce,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,401302,,,Wed Aug 13 20:34:11 UTC 2014,,,,,,,,,,"0|i1x2r3:",401379,,,,,,,,,,,,,,,,,,,,"23/Jun/14 12:15;rmetzger;Most likely this is an issue due to the renamed java examples package. The fix has to be placed into the maven assembly configuration of stratosphere-dist.;;;","07/Aug/14 11:56;githubbot;GitHub user zentol opened a pull request:

    https://github.com/apache/incubator-flink/pull/90

    [FLINK-966] Missing java-src directory in packaged examples directory

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/zentol/incubator-flink PackagedJavaExamples

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/90.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #90
    
----
commit e62fb766b191ee328a71061ae0ffa096abba4ac8
Author: zentol <s.motsu@web.de>
Date:   2014-08-07T11:41:11Z

    java examples included in packaged example directory

----
;;;","13/Aug/14 19:22;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/90
;;;","13/Aug/14 20:34;sewen;Fixed via e905c34503547b54fce930422e23d849ee4e3fda;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CollectionDataSource stores Scala collection as one data element of the source,FLINK-960,12722811,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,20/Jun/14 16:17,25/Jun/14 10:26,14/Jul/23 05:57,25/Jun/14 10:26,,,,,,,,,,,0,,,"The CollectionDataSource stores Scala collections as one object in the input format. Therefore, the input format has only one element which is the collection object. The input format should instead store the elements of the input collection.",,githubbot,trohrmann,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400998,,,Wed Jun 25 10:26:37 UTC 2014,,,,,,,,,,"0|i1x0xr:",401083,,,,,,,,,,,,,,,,,,,,"20/Jun/14 16:31;githubbot;GitHub user tillrohrmann opened a pull request:

    https://github.com/apache/incubator-flink/pull/33

    FLINK-960 Fixed bug in CollectionDataSource constructor for Scala Collection inputs

    The bug was a missing bracket.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/tillrohrmann/incubator-flink FLINK-960

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/33.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #33
    
----
commit ed763f07b8be3d3fcd339a581e34aecf35086d7e
Author: Till Rohrmann <till.rohrmann@gmail.com>
Date:   2014-06-20T16:17:42Z

    fixed CollectionDataSource bug

----
;;;","25/Jun/14 10:26;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/33
;;;","25/Jun/14 10:26;uce;Fixed in 515ad3c3362af54efc4a36fe90d9b353c1da85c8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sorting a group when key is a Writable gives a NonSerializable Error,FLINK-958,12722583,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,atsikiridis,atsikiridis,19/Jun/14 20:37,28/Feb/19 14:29,14/Jul/23 05:57,24/Jun/14 13:58,,,,,,,API / DataSet,,,,0,,,"I cannot sort an unsorted group by key when the key Is a Writable. Is it a bug?

Consider:
{code}
Grouping<Tuple2<Text, LongWritable>> res = myData
				.map(new MapFunction<Tuple2<String, Long>, Tuple2<Text, LongWritable>>() {

					@Override
					public Tuple2<Text, LongWritable> map(
							Tuple2<String, Long> value) throws Exception {
						return new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));
					}
				})
				.groupBy(0).sortGroup(0, Order.ASCENDING);
{code}

This gives the following stacktrace:

{code}
Caused by: java.io.NotSerializableException: sun.misc.Launcher$AppClassLoader
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1377)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1173)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at eu.stratosphere.util.InstantiationUtil.serializeObject(InstantiationUtil.java:250)
	at eu.stratosphere.util.InstantiationUtil.writeObjectToConfig(InstantiationUtil.java:231)
	at eu.stratosphere.api.java.typeutils.runtime.RuntimeComparatorFactory.writeParametersToConfig(RuntimeComparatorFactory.java:40)
	... 14 more
{code}

Is it not supported to sort groupings with writables as keys? I understand this is a hadoop serialization error, but so far I am not able to fix it.

Any ideas?",,atsikiridis,fhueske,,,,,,,,,,,,,,,,,,FLINK-777,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400774,,,Tue Jun 24 13:49:42 UTC 2014,,,,,,,,,,"0|i1wzlj:",400866,,,,,,,,,,,,,,,,,,,,"19/Jun/14 21:12;fhueske;I haven't looked at this issue in detail, but one quick comment.
Sorting the elements of groups on the same key as their grouping key does not make sense ;-)
Within a group the grouping key is always the same, so sorting the elements of a group on the grouping key does not change anything.

Anyways, this does of course not invalidate this issue. 
We should 
- give a meaningful error message if a user tries to sort groups on their grouping key.
- check if the problem occurs because the sort key is the same as the grouping key
- fix the issue if that is not the case;;;","19/Jun/14 21:54;atsikiridis;Hello,

I use it in a different context in my application (hadoop abstraction layer) with a custom keyselector. :) . But the problem is not related to the fact that I sort on the same key as even when I try to sort on a different field (e.g. value) it occurs.

Well, it is definitely Hadoop's serialization in Comparators for the sorting. I am currently trying to understand what would be the proper way to make the ComparatorFactory aware of Hadoop's serialization.
;;;","20/Jun/14 06:34;rmetzger;Lets see if [~twalthr] has any comments on this. I guess you have merged FLINK-777 locally.
I know that Timo has spend some time recently debugging the pull request.
The stacktrace you are posting is not complete, right? Can you post the full one?;;;","20/Jun/14 08:19;atsikiridis;Hello, here is the full one:

{code}
java.io.NotSerializableException: sun.misc.Launcher$AppClassLoader
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1377)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1173)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at eu.stratosphere.util.InstantiationUtil.serializeObject(InstantiationUtil.java:256)
	at eu.stratosphere.util.InstantiationUtil.writeObjectToConfig(InstantiationUtil.java:232)
	at eu.stratosphere.api.java.typeutils.runtime.RuntimeComparatorFactory.writeParametersToConfig(RuntimeComparatorFactory.java:40)
	at eu.stratosphere.pact.runtime.task.util.TaskConfig.setTypeComparatorFactory(TaskConfig.java:1055)
	at eu.stratosphere.pact.runtime.task.util.TaskConfig.setInputComparator(TaskConfig.java:404)
	at eu.stratosphere.compiler.plantranslate.NepheleJobGraphGenerator.addLocalInfoFromChannelToConfig(NepheleJobGraphGenerator.java:1114)
	at eu.stratosphere.compiler.plantranslate.NepheleJobGraphGenerator.translateChannel(NepheleJobGraphGenerator.java:693)
	at eu.stratosphere.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:548)
	at eu.stratosphere.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:98)
	at eu.stratosphere.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:146)
	at eu.stratosphere.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:140)
	at eu.stratosphere.compiler.plan.OptimizedPlan.accept(OptimizedPlan.java:159)
	at eu.stratosphere.compiler.plantranslate.NepheleJobGraphGenerator.compileJobGraph(NepheleJobGraphGenerator.java:168)
	at eu.stratosphere.test.util.JavaProgramTestBase$TestEnvironment.execute(JavaProgramTestBase.java:137)
{code};;;","20/Jun/14 09:53;fhueske;While constructing the execution program, the {{NepheleJobGraphGenerator}} configures the program with necessary ComparatorFactories and SerializerFactories. This is done by serializing them into the {{TaskConfig}}. 

The problem seems to be that a comparator which is wrapped by a {{RuntimeComparatorFactory}} cannot be serialized (eu.stratosphere.api.java.typeutils.runtime.RuntimeComparatorFactory.writeParametersToConfig(RuntimeComparatorFactory.java:40))

Which comparator do you use in this example?;;;","20/Jun/14 11:20;atsikiridis;I am using {{eu.stratosphere.api.java.typeutils.runtime.WritableComparator}} wrapaed inside the {{RuntimeComparatorFactory}} (which is the default for Writables).;;;","24/Jun/14 13:49;atsikiridis;Hello this is no longer an issue (after merging FLINK-777) and works fine in https://github.com/apache/incubator-flink/pull/37.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TypeExtractor requires the argument types of the UDF to be identical to the parameter types,FLINK-952,12722268,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,trohrmann,trohrmann,18/Jun/14 17:03,20/Mar/15 10:18,14/Jul/23 05:57,20/Mar/15 10:18,,,,0.9,,,Runtime / Task,,,,0,,,"The TypeExtractor checks for each operation whether the DataSet element types are valid arguments for the UDF. However, it checks for strict equality instead of a subtype relationship. Thus the following computation would not work even though it should semantically be correct.

DataSet[B].map(new MapFunction[A,A](){ A map(A x)}) with B being a sub class of A.",,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400459,,,Fri Mar 20 10:18:49 UTC 2015,,,,,,,,,,"0|i1wxp3:",400558,,,,,,,,,,,,,,,,,,,,"18/Jun/14 17:49;sewen;That is true, there is actually a reason for that:

The data is treated as the type declared in the signature, because the type analysis and serializer generation happens at the ""pre-flight"" time. So if you have a MapFunction<A> and give it a B (subclass of A), it will treat the B as an A at runtime.;;;","18/Jun/14 17:50;sewen;It would be nice to get around that, which is quite a piece of work though.;;;","20/Mar/15 10:18;sewen;Fixed with the introduction of the subclass aware pojo and generic type serializers.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests sometimes fail due to deleted temp files,FLINK-950,12722264,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,18/Jun/14 16:46,18/Jun/14 18:55,14/Jul/23 05:57,18/Jun/14 18:55,0.6-incubating,,,0.6-incubating,,,,,,,0,,,"The tests remove temp files, but it seems to be not immediately visible in some cases, so some tests fail between the checks that determine the existence of a file and getting its status.

I have a fix undergoing tests currently.",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400455,,,Wed Jun 18 18:55:42 UTC 2014,,,,,,,,,,"0|i1wxof:",400554,,,,,,,,,,,,,,,,,,,,"18/Jun/14 18:55;sewen;Fixed via 5c23b8fc332f8f92fda36233b45dfa88fb86ede6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChannelManager startup exception not reported in TaskManager,FLINK-949,12722230,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,uce,uce,uce,18/Jun/14 14:39,25/Jun/14 10:09,14/Jul/23 05:57,25/Jun/14 10:09,pre-apache-0.5.1,,,,,,,,,,0,,,"While debugging a problem with [~rmetzger], we noticded that exception in the constructor of the {{ChannelManager}} are not properly forwarded to the {{TaskManager}}.

The concrete problem was the following: num network buffers was misconfigured and tried to allocate more memory than available on the machine. The respective exception was hiden in the stdout file instead of the log.",,githubbot,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,400421,,,Wed Jun 25 10:09:24 UTC 2014,,,,,,,,,,"0|i1wxh3:",400520,,,,,,,,,,,,,,,,,,,,"19/Jun/14 08:09;githubbot;GitHub user uce opened a pull request:

    https://github.com/apache/incubator-flink/pull/28

    [FLINK-949] Properly report GlobalBufferPool OutOfMemoryError to TaskManager

    This is [FLINK-949](https://issues.apache.org/jira/browse/FLINK-949).
    
    Travis seems to have problems with downloading dependencies from Maven central and hence the [builds fail/timeout](https://travis-ci.org/uce/incubator-flink/builds/27917216).

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/uce/incubator-flink FLINK-949

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/28.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #28
    
----
commit be9f132ec538fba72f9f80cb7aaadf67d845504a
Author: uce <u.celebi@fu-berlin.de>
Date:   2014-06-19T00:42:03Z

    [FLINK-949] Properly report GlobalBufferPool OutOfMemoryError to TaskManager

----
;;;","19/Jun/14 11:44;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/28#issuecomment-46550620
  
    Looks good. 
    Have you tested if the exceptions are properly thrown?
;;;","19/Jun/14 11:49;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/28#issuecomment-46550964
  
    This is what you get:
    
    ```java
    java.lang.Exception: Cannot instantiate local instance manager: Failed to instantiate ChannelManager.
    	at eu.stratosphere.nephele.jobmanager.JobManager.<init>(JobManager.java:223)
    	at eu.stratosphere.client.minicluster.NepheleMiniCluster.start(NepheleMiniCluster.java:197)
    	at eu.stratosphere.test.util.AbstractTestBase.startCluster(AbstractTestBase.java:96)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
    	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
    	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
    	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
    	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:202)
    	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:65)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
    Caused by: java.lang.Exception: Failed to instantiate ChannelManager.
    	at eu.stratosphere.nephele.taskmanager.TaskManager.<init>(TaskManager.java:328)
    	at eu.stratosphere.nephele.instance.local.LocalInstanceManager.<init>(LocalInstanceManager.java:170)
    	at eu.stratosphere.nephele.jobmanager.JobManager.<init>(JobManager.java:221)
    	... 28 more
    Caused by: java.io.IOException: Failed to instantiate GlobalBufferPool.
    	at eu.stratosphere.runtime.io.network.ChannelManager.<init>(ChannelManager.java:85)
    	at eu.stratosphere.nephele.taskmanager.TaskManager.<init>(TaskManager.java:325)
    	... 30 more
    Caused by: java.lang.OutOfMemoryError: Tried to allocate 32768 buffers of size 32768 bytes each (total: 1024 MB) and ran out of memory after 30009 buffers (937 MB).
    	at eu.stratosphere.runtime.io.network.bufferprovider.GlobalBufferPool.<init>(GlobalBufferPool.java:70)
    	at eu.stratosphere.runtime.io.network.ChannelManager.<init>(ChannelManager.java:83)
    	... 31 more
    ```
;;;","19/Jun/14 12:12;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/28#issuecomment-46552826
  
    Very nice. Thank you.
;;;","25/Jun/14 10:08;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/28
;;;","25/Jun/14 10:09;uce;Fixed in 3d6cc5f48dc5a0336f8afdd35f59f1ee25357766.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Iterations sometimes release memory prematurely,FLINK-945,12721730,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,sewen,sewen,17/Jun/14 16:33,29/Jan/17 14:43,14/Jul/23 05:57,08/Aug/14 13:32,0.6-incubating,pre-apache-0.5.2,,,,,Runtime / Task,,,,1,,,"It seems that the iteration tail is sometimes releasing the memory prematurely, causing still working operations to fail.

{code}
java.lang.NullPointerException
	at eu.stratosphere.core.memory.MemorySegment.put(MemorySegment.java:186)
	at eu.stratosphere.nephele.services.memorymanager.AbstractPagedOutputView.writeByte(AbstractPagedOutputView.java:214)
	at eu.stratosphere.nephele.services.memorymanager.AbstractPagedOutputView.write(AbstractPagedOutputView.java:167)
	at eu.stratosphere.types.Record.serialize(Record.java:1208)
	at eu.stratosphere.api.java.typeutils.runtime.record.RecordSerializer.serialize(RecordSerializer.java:79)
	at eu.stratosphere.api.java.typeutils.runtime.record.RecordSerializer.serialize(RecordSerializer.java:27)
	at eu.stratosphere.pact.runtime.iterative.io.WorksetUpdateOutputCollector.collect(WorksetUpdateOutputCollector.java:52)
	at eu.stratosphere.test.recordJobs.kmeans.KMeansBroadcast$RecomputeClusterCenter.reduce(KMeansBroadcast.java:255)
	at eu.stratosphere.pact.runtime.task.GroupReduceDriver.run(GroupReduceDriver.java:103)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:505)
	at eu.stratosphere.pact.runtime.iterative.task.AbstractIterativePactTask.run(AbstractIterativePactTask.java:132)
	at eu.stratosphere.pact.runtime.iterative.task.IterationTailPactTask.run(IterationTailPactTask.java:100)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:370)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:284)
	at java.lang.Thread.run(Thread.java:744)
{code}",,Janani,rmetzger,sewen,uce,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399926,,,Fri Aug 08 13:32:21 UTC 2014,,,,,,,,,,"0|i1wuhb:",400034,,,,,,,,,,,,,,,,,,,,"16/Jul/14 13:08;rmetzger;I came across this issue while testing the 0.5.2 bugfix release with a large KMeans job (27 gb of data). I'm currently trying out if I can reproduce the error.

{code}
12:21:19,844 ERROR eu.stratosphere.nephele.taskmanager.Task                      - java.lang.NullPointerException: in eu.stratosphere.example.java.clustering.KMeans
$.Centroid null of eu.stratosphere.example.java.clustering.KMeans$.Centroid
        at org.apache.avro.reflect.ReflectDatumWriter.write(ReflectDatumWriter.java:145)
        at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:58)
        at eu.stratosphere.api.java.typeutils.runtime.AvroSerializer.serialize(AvroSerializer.java:104)
        at eu.stratosphere.pact.runtime.iterative.io.WorksetUpdateOutputCollector.collect(WorksetUpdateOutputCollector.java:52)
        at eu.stratosphere.pact.runtime.task.chaining.ChainedMapDriver.collect(ChainedMapDriver.java:71)
        at eu.stratosphere.pact.runtime.task.ReduceDriver.run(ReduceDriver.java:121)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:505)
        at eu.stratosphere.pact.runtime.iterative.task.AbstractIterativePactTask.run(AbstractIterativePactTask.java:132)
        at eu.stratosphere.pact.runtime.iterative.task.IterationTailPactTask.run(IterationTailPactTask.java:100)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:370)
        at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:284)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at eu.stratosphere.core.memory.MemorySegment.putInt(MemorySegment.java:432)
        at eu.stratosphere.core.memory.MemorySegment.putIntBigEndian(MemorySegment.java:480)
        at eu.stratosphere.nephele.services.memorymanager.AbstractPagedOutputView.writeInt(AbstractPagedOutputView.java:257)
        at eu.stratosphere.api.java.typeutils.runtime.DataOutputEncoder.writeInt(DataOutputEncoder.java:55)
        at org.apache.avro.reflect.FieldAccessUnsafe$UnsafeIntField.write(FieldAccessUnsafe.java:124)
        at org.apache.avro.reflect.ReflectDatumWriter.writeField(ReflectDatumWriter.java:161)
        at org.apache.avro.generic.GenericDatumWriter.writeRecord(GenericDatumWriter.java:104)
        at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:66)
        at org.apache.avro.reflect.ReflectDatumWriter.write(ReflectDatumWriter.java:143)
        ... 11 more
{code};;;","24/Jul/14 08:46;Janani;I am facing the same exception while using Bulk Iteration. The job is eventually cancelled. I am using stable release version 0.5.1 and I am running on ibm jvm (Big Endian byte order) machine. Is there any fix available for this issue?

10:02:03,835 ERROR eu.stratosphere.nephele.jobmanager.JobManager                 - java.lang.NullPointerException
	at eu.stratosphere.core.memory.MemorySegment.getLong(MemorySegment.java:503)
	at eu.stratosphere.pact.runtime.sort.NormalizedKeySorter$1.next(NormalizedKeySorter.java:405)
	at eu.stratosphere.pact.runtime.resettable.SpillingResettableMutableObjectIterator.consumeAndCacheRemainingData(SpillingResettableMutableObjectIterator.java:163)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.resetAllInputs(RegularPactTask.java:899)
	at eu.stratosphere.pact.runtime.iterative.task.AbstractIterativePactTask.run(AbstractIterativePactTask.java:122)
	at eu.stratosphere.pact.runtime.iterative.task.IterationIntermediatePactTask.run(IterationIntermediatePactTask.java:84)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:370)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:284)
	at java.lang.Thread.run(Thread.java:853) ;;;","24/Jul/14 11:59;sewen;I think that is a different issue. Can you reproduce this error predictably? Can you send us the program and the code?;;;","24/Jul/14 12:58;Janani;Stephan: I just found that I am facing this exception only for a particular data set and for others data sets the program works fine.
I sent you the code to your email and also the link for the data set for which I am still facing this exception. Now I am not sure whether this exception is caused by the data or stratosphere. ;;;","07/Aug/14 20:33;sewen;I have a (prospective) fix for the bug in https://github.com/apache/incubator-flink/pull/91;;;","08/Aug/14 13:32;uce;Fixed in 76a48df00f0fc202bdedefb33d576fd9a18a92c0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serialization problem of CollectionInputFormat,FLINK-944,12721674,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,17/Jun/14 13:04,18/Jun/14 18:56,14/Jul/23 05:57,18/Jun/14 18:56,,,,0.6-incubating,,,,,,,0,,,"The CollectionInputFormat uses only the standard serialization means provided by the JVM. Thus data types which are serializable with a TypeSerializer but does not implement the Serializable interface cannot be used with a CollectionDataSource. Even worse, if one uses an aggregation type such as a tuple, only the top level object will be checked for serializability. Consequently, it will crash at runtime.

It would be more user friendly to not enforce that a used data type has to implement the Serializable interface. Instead we should use the generated TypeSerializer to do the serialization. That way, we are more flexible.",,githubbot,sewen,trohrmann,,,,,,,,,,,,,,,,,,,FLINK-826,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399870,,,Wed Jun 18 18:56:21 UTC 2014,,,,,,,,,,"0|i1wu4v:",399978,,,,,,,,,,,,,,,,,,,,"17/Jun/14 17:26;githubbot;GitHub user tillrohrmann opened a pull request:

    https://github.com/apache/incubator-flink/pull/25

    FLINK-944 Changed serialization logic of CollectionInputFormat to use TypeSerializer

    The CollectionInputFormat did not support collection elements which didn't implement the Serializable interface. By using the TypeSerializer generated from the TypeInformation, we can circumvent this problem. This allows to use arbitrary classes in a collection data source.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/tillrohrmann/incubator-flink FLINK-944

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/25.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #25
    
----
commit 56a0730a4769d74f1a3381a07d0259ad99bdc2c4
Author: Till Rohrmann <till.rohrmann@gmail.com>
Date:   2014-06-17T17:21:02Z

    CollectionInputFormat now uses the TypeSerializer to serialize the collection entries. This allows to use objects not implementing the Serializable interface as collection elements.

----
;;;","18/Jun/14 17:23;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/25#issuecomment-46466239
  
    Looks good, will merge...
;;;","18/Jun/14 18:53;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/25
;;;","18/Jun/14 18:56;sewen;Fixed via 5fa5e50205afb41ebc39f197da07e87c0fdd0334 and 9ecb6df76ca5e7a42d0d62b1c9dca41e9e7ecd1b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configured values with double quotes are not parsed correctly in bash scripts,FLINK-943,12721543,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,uce,uce,16/Jun/14 23:03,18/Jun/14 18:58,14/Jul/23 05:57,18/Jun/14 18:58,,,,0.6-incubating,,,,,,,0,,,"In FLINK-942 I noticed that {{env.java.opts}} are not read by the startup scripts. While doing the fix I discovered a related problem:

This works fine:
{code}
env.java.opts: -Dio.netty.leakDetectionLevel=paranoid
{code}

If you use it with double quotes, the startup scripts get messed up and don't even start up the JobManager/TaskManagers anymore:

{code}
env.java.opts: ""-Dio.netty.leakDetectionLevel=paranoid""
{code}

This applies to all configuration values with double quotes and is there a seperate issue than FLINK-942. I just noticed it by chance for the JVM args.",,githubbot,sewen,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399739,,,Wed Jun 18 18:58:06 UTC 2014,,,,,,,,,,"0|i1wtbr:",399848,,,,,,,,,,,,,,,,,,,,"17/Jun/14 11:52;sewen;Is the issue in the bash-based yaml parser, or in the way that the variables are concatenated to build the java arguments?;;;","17/Jun/14 22:59;githubbot;GitHub user uce opened a pull request:

    https://github.com/apache/incubator-flink/pull/26

    [FLINK-943] Remove leading and ending double quotes from 'env.java.opts' config value

    This is [FLINK-943](https://issues.apache.org/jira/browse/FLINK-943).
    
    The parsing works correctly but the leading/ending double quotes need to be removed before the value is given to the `java` run command, e.g. `java -Xmx1g ""-D ...""` does not work.
    
    I've tested: 1. with double quotes, 2. without double quotes, and 3. not set.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/uce/incubator-flink FLINK-943

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/26.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #26
    
----
commit c29e74399d4c02ac613bb3811afd44e23fd73786
Author: uce <u.celebi@fu-berlin.de>
Date:   2014-06-17T22:48:15Z

    [FLINK-943] Remove leading and ending double quotes from 'env.java.opts' config value in startup scripts

----
;;;","18/Jun/14 16:52;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/26#issuecomment-46462311
  
    Good to merge
;;;","18/Jun/14 18:53;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/26
;;;","18/Jun/14 18:58;sewen;Fixed via b2bd4697af1984a00af3ded443b93528cf146769;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Config key ""env.java.opts"" does not effect JVM args",FLINK-942,12721493,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,uce,uce,16/Jun/14 19:35,18/Jun/14 22:54,14/Jul/23 05:57,18/Jun/14 22:54,,,,,,,,,,,0,,,"Setting custom args for the JVM as in 
{code}
env.java.opts: -Dio.netty.leakDetectionLevel=paranoid
{code}
has no effect for the started JVMs.

A fix is coming up.",,githubbot,sewen,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399689,,,Wed Jun 18 22:54:01 UTC 2014,,,,,,,,,,"0|i1wt0n:",399798,,,,,,,,,,,,,,,,,,,,"16/Jun/14 19:55;sewen;Is there a way we can add tests for the bash scripts? They seem to start being a bit of a weak link...;;;","16/Jun/14 20:19;githubbot;GitHub user uce opened a pull request:

    https://github.com/apache/incubator-flink/pull/22

    [FLINK-942] Fix bug in config.sh to correctly set user-specified JVM args

    This is a fix for [FLINK-942](https://issues.apache.org/jira/browse/FLINK-942).

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/uce/incubator-flink FLINK-942

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/22.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #22
    
----
commit 51e65f8f20611156ecc23e487b28ed545975bfd3
Author: uce <u.celebi@fu-berlin.de>
Date:   2014-06-16T20:17:48Z

    [FLINK-942] Fix bug in config.sh to correctly set user-specified JVM args

----
;;;","16/Jun/14 20:21;uce;I remember [~rmetzger] once mentioning a possible solution to add tests for the bash scripts.;;;","16/Jun/14 21:41;uce;Do not merge this.;;;","16/Jun/14 23:04;uce;OK, I think this is good to merge.

I discovered a related problem in FLINK-943, which needs seperate fixing. Took me quite some time to figure out that the problem was unrelated to this fix. :(;;;","17/Jun/14 11:54;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/22#issuecomment-46297253
  
    Looks good, will merge.
;;;","17/Jun/14 22:19;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/22
;;;","18/Jun/14 22:54;uce;Merged in [6a549325f3c1e98b4c353e9bec6f951affb95bc9|https://github.com/apache/incubator-flink/commit/6a549325f3c1e98b4c353e9bec6f951affb95bc9].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible deadlock after increasing my data set size,FLINK-941,12721396,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,bastiank,bastiank,16/Jun/14 11:55,04/Jul/14 08:36,14/Jul/23 05:57,04/Jul/14 08:36,pre-apache-0.5.1,,,0.6-incubating,,,,,,,0,,,"If I increase my data set, my algorithm stops at some point and doesn't continue anymore. I already waited a quite amount of time, but nothing happens. The linux processor explorer also displays that the process is sleeping and waiting for something to happen, could maybe be a deadlock.

I attached the source of my program, the class HAC_2 is the actual algorithm.
Changing the line 271 from ""if(Integer.parseInt(tokens[0]) > 282)"" to ""if(Integer.parseInt(tokens[0]) > 283)"" at my PC ""enables"" the bug. The numbers 282, 283 are the numbers of the documents in my test data and this line skips all documents with an id greater than that.",,bastiank,sewen,skunert,uce,,,,,,,,,,,,,,,,,,,,,"16/Jun/14 11:59;bastiank;IMPRO-3.SS14.G03.zip;https://issues.apache.org/jira/secure/attachment/12650555/IMPRO-3.SS14.G03.zip",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399592,,,Thu Jul 03 23:03:59 UTC 2014,,,,,,,,,,"0|i1wsf3:",399701,,,,,,,,,,,,,,,,,,,,"16/Jun/14 11:59;bastiank;Source of the program;;;","16/Jun/14 13:03;uce;I could reproduce the deadlock, but didn't immediately figure out why it is happening.

I've commented out lines 284 and 285 of {{HAC_2}}, which just return early for the first flat map and for DOP 1 the group reduce, which outputs the cluster pair (line 354) is blocked while requesting a buffer for the local receiver.

I will look into it later.;;;","17/Jun/14 23:54;sewen;When I change line 271 as you said, I do not get the deadlock.

The deadlock may be dependent on how many buffers are available for intermediate results. What DOPs have you used and what is your maximum head size? I have tried it with DOP 1 and DOP4 on -Xmx192m and it worked.

[~uce] How did you reproduce it?;;;","18/Jun/14 13:07;sewen;I was able to repdroduce the problem, thanks to [~uce].

The issue is that Broadcast variables are not 

I think the correct way to solve that is to make sure that we never allow backpressure on intermediate results that are consumed by multiple targets. That does depend on the redesigned handling of intermediate result partitions (which [~uce] has drafted).;;;","18/Jun/14 13:35;bastiank;""The issue is that Broadcast variables are not"", I think there is something missing ;)
So this means I have to wait for a fix in Flink?;;;","18/Jun/14 13:40;sewen;Right ;-) The issue is that Broadcast variables are not taken into account when deciding where to place pipeline breakers.

You need a fix in flink. I am working on it now.;;;","18/Jun/14 14:05;sewen;I think I fixed the issue. A patch should be in the 0.6-SNAPSHOT version in a few hours (pending tests).

You would have to download and build that version yourself, though.;;;","18/Jun/14 16:43;sewen;Fixed (with test) in 3a452e5bbabe1967ffc04aa34f2959d1efece282;;;","25/Jun/14 09:09;bastiank;Okay, we tried it now with the latest git version, but we still get the Deadlock :/
;;;","03/Jul/14 09:52;uce;Is there an update? I am curious whether you tried it with a version after [f13ad5b415a57e7d1c97319935a04f076cc1776b|https://github.com/apache/incubator-flink/commit/f13ad5b415a57e7d1c97319935a04f076cc1776b]?;;;","03/Jul/14 23:03;bastiank;Yeah, now the Deadlock is gone ;);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in Optimizer when reusing work across iterations,FLINK-935,12721122,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,13/Jun/14 15:08,28/Feb/19 14:30,14/Jul/23 05:57,17/Jun/14 00:10,0.6-incubating,pre-apache-0.5,pre-apache-0.5.1,0.6-incubating,,,API / DataSet,,,,0,,,"The following created plan is invalid

{code}
{
	""nodes"": [

	{
		""id"": 3,
		""type"": ""source"",
		""pact"": ""Data Source"",
		""contents"": ""CSV Input (,) /some/file/path"",
		""parallelism"": ""4"",
		""subtasks_per_instance"": ""1"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""CPU"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 2,
		""type"": ""pact"",
		""pact"": ""Map"",
		""contents"": ""eu.stratosphere.pact.compiler.IterationsCompilerTest$DuplicateValue"",
		""parallelism"": ""4"",
		""subtasks_per_instance"": ""1"",
		""predecessors"": [
			{""id"": 3, ""ship_strategy"": ""Hash Partition on [0]"", ""local_strategy"": ""Sort on [0:ASC]""}
		],
		""driver_strategy"": ""Map"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""HASH_PARTITIONED"" },
			{ ""name"": ""Partitioned on"", ""value"": ""[0]"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""[0:ASC]"" },
			{ ""name"": ""Grouped on"", ""value"": ""[0]"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""CPU"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""step_function"": [
	{
		""id"": 9,
		""type"": ""pact"",
		""pact"": ""Bulk Partial Solution"",
		""contents"": ""Partial Solution"",
		""parallelism"": ""4"",
		""subtasks_per_instance"": ""1"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""HASH_PARTITIONED"" },
			{ ""name"": ""Partitioned on"", ""value"": ""[0]"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""[0:ASC]"" },
			{ ""name"": ""Grouped on"", ""value"": ""[0]"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""CPU"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 10,
		""type"": ""source"",
		""pact"": ""Data Source"",
		""contents"": ""CSV Input (,) /some/file/path"",
		""parallelism"": ""4"",
		""subtasks_per_instance"": ""1"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""CPU"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""0.0"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 8,
		""type"": ""pact"",
		""pact"": ""Join"",
		""contents"": ""eu.stratosphere.pact.compiler.IterationsCompilerTest$Join222"",
		""parallelism"": ""4"",
		""subtasks_per_instance"": ""1"",
		""predecessors"": [
			{""id"": 9, ""side"": ""first"", ""ship_strategy"": ""Forward""},
			{""id"": 10, ""side"": ""second"", ""ship_strategy"": ""Hash Partition on [0]"", ""local_strategy"": ""Sort on [0:ASC]"", ""temp_mode"": ""CACHED""}
		],
		""driver_strategy"": ""Merge"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""CPU"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 7,
		""type"": ""pact"",
		""pact"": ""GroupReduce"",
		""contents"": ""MIN(1)"",
		""parallelism"": ""4"",
		""subtasks_per_instance"": ""1"",
		""predecessors"": [
			{""id"": 8, ""ship_strategy"": ""Forward""}
		],
		""driver_strategy"": ""Sorted Combine"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""CPU"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 6,
		""type"": ""pact"",
		""pact"": ""GroupReduce"",
		""contents"": ""MIN(1)"",
		""parallelism"": ""4"",
		""subtasks_per_instance"": ""1"",
		""predecessors"": [
			{""id"": 7, ""ship_strategy"": ""Hash Partition on [0]"", ""local_strategy"": ""Sort (combining) on [0:ASC]""}
		],
		""driver_strategy"": ""Sorted Group Reduce"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""HASH_PARTITIONED"" },
			{ ""name"": ""Partitioned on"", ""value"": ""[0]"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""[0:ASC]"" },
			{ ""name"": ""Grouped on"", ""value"": ""[0]"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""CPU"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 5,
		""type"": ""pact"",
		""pact"": ""Join"",
		""contents"": ""eu.stratosphere.api.java.operators.JoinOperator$DefaultJoinFunction"",
		""parallelism"": ""4"",
		""subtasks_per_instance"": ""1"",
		""predecessors"": [
			{""id"": 6, ""side"": ""first"", ""ship_strategy"": ""Forward""},
			{""id"": 9, ""side"": ""second"", ""ship_strategy"": ""Forward"", ""temp_mode"": ""PIPELINE_BREAKER""}
		],
		""driver_strategy"": ""Merge"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""CPU"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 4,
		""type"": ""pact"",
		""pact"": ""FlatMap"",
		""contents"": ""eu.stratosphere.pact.compiler.IterationsCompilerTest$FlatMapJoin"",
		""parallelism"": ""4"",
		""subtasks_per_instance"": ""1"",
		""predecessors"": [
			{""id"": 5, ""ship_strategy"": ""Forward""}
		],
		""driver_strategy"": ""Map"",
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""CPU"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	}
		],
		""partial_solution"": 9,
		""next_partial_solution"": 4,
		""id"": 1,
		""type"": ""bulk_iteration"",
		""pact"": ""Bulk Iteration"",
		""contents"": ""Bulk Iteration"",
		""parallelism"": ""4"",
		""subtasks_per_instance"": ""1"",
		""predecessors"": [
			{""id"": 2, ""ship_strategy"": ""Forward""}
		],
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""CPU"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	},
	{
		""id"": 0,
		""type"": ""sink"",
		""pact"": ""Data Sink"",
		""contents"": ""Print to System.out"",
		""parallelism"": ""4"",
		""subtasks_per_instance"": ""1"",
		""predecessors"": [
			{""id"": 1, ""ship_strategy"": ""Forward""}
		],
		""global_properties"": [
			{ ""name"": ""Partitioning"", ""value"": ""RANDOM"" },
			{ ""name"": ""Partitioning Order"", ""value"": ""(none)"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""local_properties"": [
			{ ""name"": ""Order"", ""value"": ""(none)"" },
			{ ""name"": ""Grouping"", ""value"": ""not grouped"" },
			{ ""name"": ""Uniqueness"", ""value"": ""not unique"" }
		],
		""estimates"": [
			{ ""name"": ""Est. Output Size"", ""value"": ""(unknown)"" },
			{ ""name"": ""Est. Cardinality"", ""value"": ""(unknown)"" }		],
		""costs"": [
			{ ""name"": ""Network"", ""value"": ""0.0"" },
			{ ""name"": ""Disk I/O"", ""value"": ""0.0"" },
			{ ""name"": ""CPU"", ""value"": ""0.0"" },
			{ ""name"": ""Cumulative Network"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative Disk I/O"", ""value"": ""(unknown)"" },
			{ ""name"": ""Cumulative CPU"", ""value"": ""(unknown)"" }
		],
		""compiler_hints"": [
			{ ""name"": ""Output Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Output Cardinality"", ""value"": ""(none)"" },
			{ ""name"": ""Avg. Output Record Size (bytes)"", ""value"": ""(none)"" },
			{ ""name"": ""Filter Factor"", ""value"": ""(none)"" }		]
	}
	]
}
{code}",,sewen,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/14 15:10;sewen;screenshot_plan.png;https://issues.apache.org/jira/secure/attachment/12650315/screenshot_plan.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399319,,,Tue Jun 17 00:10:39 UTC 2014,,,,,,,,,,"0|i1wqrb:",399428,,,,,,,,,,,,,,,,,,,,"13/Jun/14 15:10;sewen;!screenshot_plan.png!;;;","13/Jun/14 15:11;sewen;The problem is that the end of the iteration does not produce partitioned data, which the beginning of the next iteration assumes.;;;","17/Jun/14 00:10;sewen;Fixed via commit ca2b287a7a78328ebf43766b9fdf39b56fb5fd4f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Semantic Properties from Project Joins,FLINK-932,12720918,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,skunert,sewen,sewen,12/Jun/14 17:43,03/Jul/14 12:52,14/Jul/23 05:57,03/Jul/14 12:52,,,,0.6-incubating,,,,,,,0,,,"The Project Joins do not create semantic properties right now, which looses optimization potential.",,fhueske,githubbot,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399117,,,Thu Jul 03 12:52:43 UTC 2014,,,,,,,,,,"0|i1wpkn:",399234,,,,,,,,,,,,,,,,,,,,"12/Jun/14 18:36;fhueske;The same applies to the regular project() and the cross project().;;;","19/Jun/14 14:04;githubbot;GitHub user skunert opened a pull request:

    https://github.com/apache/incubator-flink/pull/29

    SemanticProperties for Projections

    This PR deals with
    https://issues.apache.org/jira/browse/FLINK-932
    
    ProjectionJoins, Crosses and the regular project operator get their SemanticProperties generated automatically. Additionally, I disables withConstantSet() for ProjectJoins and Crosses because it can cause conflicts.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/skunert/incubator-flink projection_fix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/29.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #29
    
----
commit 73e14235fb40fd76a7a2bb57ba07eca0f23421d8
Author: sebastian kunert <skunert49@gmail.com>
Date:   2014-06-19T13:59:36Z

    introduced automaitc generation of semantic props for projections

----
;;;","03/Jul/14 12:51;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/29
;;;","03/Jul/14 12:52;sewen;Fixed via 883474ddfd749eedceb85bdd2925a3a76de75206;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Netty Initialization is sometimes very slow,FLINK-930,12720904,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,uce,sewen,sewen,12/Jun/14 16:21,18/Jun/14 22:47,14/Jul/23 05:57,18/Jun/14 22:47,,,,,,,,,,,0,performance,,"I often see the initialization of Netty taking 3 seconds and then failing with the message 

""WARN  io.netty.util.internal.ThreadLocalRandom                      - Failed to generate a seed from SecureRandom within 3 seconds. Not enough entrophy?""

This adds to every single run a 3 seconds delay, which is often more than the test time and causes our builds to run out of time.",,githubbot,sewen,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399103,,,Wed Jun 18 22:47:35 UTC 2014,,,,,,,,,,"0|i1wphj:",399220,,,,,,,,,,,,,,,,,,,,"15/Jun/14 18:35;githubbot;GitHub user uce opened a pull request:

    https://github.com/apache/incubator-flink/pull/19

    [FLINK-930] Netty Initialization is sometimes very slow

    This is [FLINK-930](https://issues.apache.org/jira/browse/FLINK-930).
    
    The issue with the Netty warning was reported in netty/netty#2412 and has been fixed in `4.0.19.Final`. I couldn't reproduce the reported problem (with and without the changes in this PR).
    
    This PR contains the following changes:
    1. Update Netty version from `4.0.19.Final` to `4.0.20.Final` and
    2. Introduce `ExecutionMode` for TaskManagers, where in `LOCAL` mode the `NetworkConnectionManager` is not started (e.g. no ServerSocket binding etc.)
    
    The 2. change speeds up the startup and shutdown time of the TaskManagers noticeably and solves/circumvents the reported problem with each test taking ~ 3 seconds for startup/shutdown.
    


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/uce/incubator-flink FLINK-930

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/19.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #19
    
----
commit a0d3530aa137290bf2d690339bf028f272c12129
Author: uce <u.celebi@fu-berlin.de>
Date:   2014-06-13T08:41:32Z

    [FLINK-930] Netty Initialization is sometimes very slow

----
;;;","16/Jun/14 23:34;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/19#issuecomment-46251209
  
    Can someone please review and merge? Travis is running consistently out of time atm and this PR should ""fix it"". We can then further monitor the Netty ThreadLocalRandom warning.
;;;","17/Jun/14 06:55;githubbot;Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/19#discussion_r13845277
  
    --- Diff: stratosphere-runtime/src/main/java/eu/stratosphere/runtime/io/network/ChannelManager.java ---
    @@ -99,8 +95,13 @@ public ChannelManager(ChannelLookupProtocol channelLookupService, InstanceConnec
     		this.discardBufferPool = new DiscardBufferPool();
     	}
     
    -	public void shutdown() {
    -		this.nettyConnectionManager.shutdown();
    +	public void shutdown()  {
    +		try {
    +			this.networkConnectionManager.shutdown();
    +		} catch (IOException e) {
    +			LOG.warn(""NetworkConnectionManager did not shutdown properly."");
    --- End diff --
    
    I think we should log the exception as well here (I guess the error is unlikely and needs review by the user)
;;;","17/Jun/14 06:56;githubbot;Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/19#discussion_r13845310
  
    --- Diff: stratosphere-runtime/src/main/java/eu/stratosphere/runtime/io/network/netty/NettyConnectionManager.java ---
    @@ -108,9 +130,9 @@ public void initChannel(SocketChannel channel) throws Exception {
     				.option(ChannelOption.SO_KEEPALIVE, true);
     
     		try {
    -			this.in.bind().sync();
    +			in.bind().sync();
     		} catch (InterruptedException e) {
    -			throw new RuntimeException(""Could not bind server socket for incoming connections."");
    +			throw new IOException(""Interrupted while trying to bind server socket."");
    --- End diff --
    
    I would also forward the exception here
;;;","17/Jun/14 09:16;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/19#issuecomment-46284531
  
    I agree with both your points and have addressed them. The thrown exceptions are rethrown as IOExceptions up to the TaskManager's shutdown method, where it is logged. I decided against changing the signature of the TaskManager's shutdown method to rethrow the Exception as I think this should go hand in hand with changes to the other component shutdowns as well.
;;;","17/Jun/14 11:56;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/19#issuecomment-46297333
  
    Nice to see it green again: https://travis-ci.org/uce/incubator-flink/builds/27754618
;;;","17/Jun/14 12:25;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/19#issuecomment-46299700
  
    Looks good to me. Will merge.
;;;","17/Jun/14 22:19;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/19
;;;","18/Jun/14 22:47;uce;Merged in [a70de7e3faef8afea0134c8fed1240d742cccb3d|https://github.com/apache/incubator-flink/commit/a70de7e3faef8afea0134c8fed1240d742cccb3d].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Impossible to pass double with configuration,FLINK-929,12720901,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,powibol,powibol,12/Jun/14 16:10,28/Feb/19 14:29,14/Jul/23 05:57,16/Jul/14 14:37,pre-apache-0.5,,,0.6-incubating,pre-apache-0.5.2,,,,,,0,,,"I discovered the following by using the new Java-API:

When calling getDouble on the configuration given to the open method, the returned value is always the default value.
It seems like an override of getDouble is missing in the class TaskConfig.DelegatingConfiguration.

{code:title=MyClass.java|borderStyle=solid}
//[...]
public void open(Configuration parameters) throws Exception
{
    //This seems to always return the default value
    parameters.getDouble(""myKey"",0.0);
}
//[...]
{code}",,githubbot,powibol,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399100,,,Wed Jul 16 14:37:40 UTC 2014,,,,,,,,,,"0|i1wpgv:",399217,,,,,,,,,,,,,,,,,,,,"12/Jun/14 16:16;rmetzger;Thank you for reporting the issue.

Do you know if the error is specific to double?
I recently fixed an issue where passing the configuration object did not work at all.;;;","12/Jun/14 16:25;rmetzger;Okay, it really seems to be this delegate thing. I will fix the issue.;;;","12/Jun/14 17:42;githubbot;GitHub user rmetzger opened a pull request:

    https://github.com/apache/incubator-flink/pull/13

    Fix for FLINK-929

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rmetzger/incubator-flink confDelegateFix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/13.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #13
    
----
commit 1623fcf0f98dad4fa2c0f23e179e6280e2f8c7dd
Author: Robert Metzger <metzgerr@web.de>
Date:   2014-06-12T17:39:00Z

    Fix DelegatingConfiguration + Test

----
;;;","12/Jun/14 20:59;rmetzger;The fix is included into the 0.5.1 release.;;;","12/Jun/14 21:11;rmetzger;I did a mistake here (I should not work late in the evening).
I forgot to add the commit to the 0.5.1 release.

I'll push the fix to the master (0.6-SNAPSHOT) and add it to the 0.5.2 release.  I'm really sorry that I forgot to include it!;;;","12/Jun/14 21:17;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/13#issuecomment-45949247
  
    Merged into master and release-0.5.2.

;;;","12/Jun/14 21:17;githubbot;Github user rmetzger closed the pull request at:

    https://github.com/apache/incubator-flink/pull/13
;;;","16/Jul/14 14:37;rmetzger;Fixed in 0.5.2 and the upcoming 0.6-incubating.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task Cancellation in CompactingHashTable fails,FLINK-927,12720871,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rwaury,rmetzger,rmetzger,12/Jun/14 13:24,12/Jun/14 21:27,14/Jul/23 05:57,12/Jun/14 21:01,,,,0.6-incubating,pre-apache-0.5.1,,,,,,0,,,"As commented in FLINK-885, this is the proper issue for this problem

{code}
06/10/2014 16:27:44:	Join(T3 Join: Iteration input) (7/8) switched to FAILED
java.lang.ArithmeticException: / by zero
	at eu.stratosphere.pact.runtime.hash.CompactingHashTable$HashTableProber.getMatchFor(CompactingHashTable.java:1089)
	at eu.stratosphere.pact.runtime.task.JoinWithSolutionSetSecondDriver.run(JoinWithSolutionSetSecondDriver.java:134)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:505)
	at eu.stratosphere.pact.runtime.iterative.task.AbstractIterativePactTask.run(AbstractIterativePactTask.java:132)
	at eu.stratosphere.pact.runtime.iterative.task.IterationIntermediatePactTask.run(IterationIntermediatePactTask.java:84)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:370)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:284)
	at java.lang.Thread.run(Thread.java:744)
{code}",,githubbot,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,399070,,,Thu Jun 12 21:27:17 UTC 2014,,,,,,,,,,"0|i1wpa7:",399187,,,,,,,,,,,,,,,,,,,,"12/Jun/14 14:23;githubbot;GitHub user rwaury opened a pull request:

    https://github.com/apache/incubator-flink/pull/11

    quick and dirty fix for FLINK-927 and added more diagnostics information

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rwaury/incubator-flink flink927

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/11.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #11
    
----
commit 1e980dd89b064f2da039dd8ef0af9d7470108a19
Author: rwaury <robert.waury@googlemail.com>
Date:   2014-06-12T14:18:59Z

    quick and dirty fix for FLINK-927 and added more diagnostics information

----
;;;","12/Jun/14 14:45;rmetzger;I suggest to include the fix into the 0.5.1 bugfix release as well.;;;","12/Jun/14 15:57;sewen;It is in the current batch I am testing. Will commit it to the 0.5.1 if tests pass.;;;","12/Jun/14 21:01;rmetzger;Thank you for the pull request. The fix is included in the 0.5.1 release.;;;","12/Jun/14 21:18;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/11#issuecomment-45949335
  
    Merged into master and 0.5.1.
    
    Please close the pull request (only the author of the PR can close it)
;;;","12/Jun/14 21:27;githubbot;Github user rwaury closed the pull request at:

    https://github.com/apache/incubator-flink/pull/11
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
coGroup ordering NullPointerException,FLINK-922,12720565,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,skunert,bastiank,bastiank,11/Jun/14 12:28,17/Jun/14 22:19,14/Jul/23 05:57,12/Jun/14 21:02,pre-apache-0.5,,,0.6-incubating,pre-apache-0.5.1,,,,,,0,,,"I'm running a delta iteration and want to execute an coGroup on the solutionset but I'm getting a NullPointerException.

Here is the stacktrace: 
java.lang.NullPointerException
	at eu.stratosphere.compiler.operators.CoGroupDescriptor.instantiate(CoGroupDescriptor.java:131)
	at eu.stratosphere.compiler.dag.TwoInputNode.instantiate(TwoInputNode.java:555)
	at eu.stratosphere.compiler.dag.TwoInputNode.addLocalCandidates(TwoInputNode.java:523)
	at eu.stratosphere.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:453)
	at eu.stratosphere.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:251)
	at eu.stratosphere.compiler.dag.WorksetIterationNode.instantiate(WorksetIterationNode.java:308)
	at eu.stratosphere.compiler.dag.TwoInputNode.addLocalCandidates(TwoInputNode.java:523)
	at eu.stratosphere.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:453)
	at eu.stratosphere.compiler.dag.DataSinkNode.getAlternativePlans(DataSinkNode.java:192)
	at eu.stratosphere.compiler.PactCompiler.compile(PactCompiler.java:715)
	at eu.stratosphere.compiler.PactCompiler.compile(PactCompiler.java:553)
	at eu.stratosphere.client.LocalExecutor.executePlan(LocalExecutor.java:216)
	at eu.stratosphere.api.java.LocalEnvironment.execute(LocalEnvironment.java:57)
	at eu.stratosphere.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:520)

And here is my code I use to call the coGroup:
DeltaIteration<Document, ClusterPair> iteration = documents.iterateDelta(similarities, 10000, 1);
		
        DataSet<ClusterPair> linkageSet = iteration.getWorkset().reduce(new MinRed());

        DataSet<Document> delta = iteration.getSolutionSet().coGroup(linkageSet).where(0).equalTo(1).with(new PointUpdater());",,bastiank,githubbot,rmetzger,sewen,skunert,,,,,,,,,,,,,,,,,,,,"11/Jun/14 23:01;bastiank;IMPRO-3.SS14.G03-test.zip;https://issues.apache.org/jira/secure/attachment/12649934/IMPRO-3.SS14.G03-test.zip",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398764,,,Tue Jun 17 22:19:30 UTC 2014,,,,,,,,,,"0|i1wngf:",398886,,,,,,,,,,,,,,,,,,,,"11/Jun/14 22:27;rmetzger;Thank you for reporting the issue. We will look into it.
Would it be a big deal for you to share the whole source code of your program with us? (A link to a repo is also okay).
;;;","11/Jun/14 23:01;bastiank;This is our code, it's very hacky. 
You have to execute the class HAC_2. 
(I added an attachment with the code.);;;","11/Jun/14 23:06;rmetzger;Thank you, I'll look into it tomorrow.;;;","12/Jun/14 10:56;skunert;The Bug is located, I will fix it.;;;","12/Jun/14 12:09;githubbot;GitHub user skunert opened a pull request:

    https://github.com/apache/incubator-flink/pull/10

    Bugfix: CoGroup SolutionSetFirst

    This PR deals with the issue described here:
    https://issues.apache.org/jira/browse/FLINK-922

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/skunert/incubator-flink coGroup_solutionSetFirst_bugfix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/10.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #10
    
----
commit 3ec549e29306dc45714d7adee294795119bf8a1d
Author: Sebastian Kunert <skunert49@gmail.com>
Date:   2014-06-12T12:06:24Z

    added own instantiate method to CoGroupDescriptorWithSolutionSetFirst; added Test

----
;;;","12/Jun/14 12:39;sewen;I will merge this now. This should become part of the 0.5.1 bugfix release.;;;","12/Jun/14 21:02;rmetzger;Thank you for the pull request. The fix is included in the 0.5.1 release.;;;","17/Jun/14 22:19;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/10
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Channel initialization does not respect JobGraph channel types,FLINK-921,12720555,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,uce,uce,11/Jun/14 11:38,12/Jun/14 10:32,14/Jul/23 05:57,12/Jun/14 10:32,pre-apache-0.5,,,,,,,,,,0,,,"Initialization of input and output channels has been moved with 2db78a8dc1a4664f3e384005d7e07bea594b835b from `RuntimeEnvironment` to `initializeChannels(GateDeploymentDescriptor)` in InputGate and OutputGate respectively.

When creating a channel, `getChannelType()` of the abstract `Gate` class is called instead of the respective descriptor method, which should actually be called:

```
this.channels[i] = new OutputChannel(this, i, id, connectedId, getChannelType());
```

`getChannelType()` of `Gate` returns `ChannelType.NETWORK`, hence currently all channels are of type `ChannelType.NETWORK`.

This introduces runtime lookups of channel receivers in `ChannelManager` as `ChannelManager.register(Task)` does not add them at task registration.
 ",,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398754,,,Thu Jun 12 10:32:01 UTC 2014,,,,,,,,,,"0|i1wne7:",398876,,,,,,,,,,,,,,,,,,,,"12/Jun/14 10:32;uce;I just saw that this is not true after all. Before calling {{getChannelType()}} of the Gate, the channel type of the Gate is set from the descriptor.

But the Gate channel type is never used again, so it makes sense to remove this confusing detour after all.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added configuration subsection in webinterface,FLINK-919,12720101,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JonathanH5,github-import,github-import,09/Jun/14 13:09,18/Jun/14 12:06,14/Jul/23 05:57,18/Jun/14 12:06,,,,0.6-incubating,pre-apache,,Runtime / Web Frontend,,,,0,github-import,,"I had some troubles with git so I decided to reopen this request in a new branch ([#914|https://github.com/stratosphere/stratosphere/issues/914] | [FLINK-914|https://issues.apache.org/jira/browse/FLINK-914])
@rmetzger

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/919
Created by: [JonathanH5|https://github.com/JonathanH5]
Labels: 
Created at: Mon Jun 09 13:49:29 CEST 2014
State: open
",,github-import,JonathanH5,rmetzger,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:09;github-import;pull-request-919-4098900578714263051.patch;https://issues.apache.org/jira/secure/attachment/12649374/pull-request-919-4098900578714263051.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398300,,,Wed Jun 18 12:06:36 UTC 2014,,,,,,,,,,"0|i1wkmn:",398427,,,,,,,,,,,,,,,,,,,,"09/Jun/14 19:16;rmetzger;I tested the change locally and everything worked well.

--> Good to merge.;;;","13/Jun/14 11:37;JonathanH5;It is now reopened again... . Please press this merge button :).
https://github.com/apache/incubator-flink/pull/16;;;","18/Jun/14 12:06;rmetzger;The issue has been resolved by this pull request: https://github.com/apache/incubator-flink/pull/16
Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename netty IO Thread count parameters,FLINK-917,12720099,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,uce,github-import,github-import,09/Jun/14 13:08,14/Mar/19 14:00,14/Jul/23 05:57,13/Jun/14 14:39,,,,0.6-incubating,,,Runtime / Network,,,,0,github-import,,"How about we rename the config parameters for `taskmanager.netty.numOutThreads` and `taskmanager.netty.numInThreads`? That way we make it ""independent"" of the underlying implementation. The same parameter should also configure the number of I/O threads if we should choose to go with zeroMQ for streaming, or so...

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/917
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Sun Jun 08 16:12:44 CEST 2014
State: open
",,githubbot,github-import,rmetzger,sewen,uce,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398298,,,Tue Jun 17 22:19:30 UTC 2014,,,,,,,,,,"0|i1wkm7:",398425,,,,,,,,,,,,,,,,,,,,"10/Jun/14 21:59;uce;+1

What about `taskmanager.io.numOutThreads` and `numInThreads` respectively?;;;","10/Jun/14 22:13;rmetzger;I agree. Netty should not appear in our config, and [~uce]'s suggestion is good.;;;","10/Jun/14 22:31;githubbot;GitHub user uce opened a pull request:

    https://github.com/apache/incubator-flink/pull/5

    [FLINK-917] Rename netty IO thread count parameters

    Renames config parameters introduced in 4cd4a13415d609a2979c8fa3cf4b797c990ee8c2:
    - `taskmanager.netty.numInThreads => taskmanager.io.numInThreads`, and
    - `taskmanager.netty.numOutThreads => taskmanager.io.numOutThreads`.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/uce/incubator-flink FLINK-917

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/5.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #5
    
----
commit 96dc4bb8f3bdb5a959a769f14e2d9d1262588cf5
Author: uce <u.celebi@fu-berlin.de>
Date:   2014-06-10T22:22:11Z

    [FLINK-917] Rename netty IO thread count parameters

----
;;;","11/Jun/14 07:19;rmetzger;The pull request looks good to merge.;;;","12/Jun/14 13:41;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/5#issuecomment-45892412
  
    `taskmanager.io.numInThreads` is a bit ambiguous with respect to disk I/O versus net I/O threads?
;;;","13/Jun/14 10:38;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/5#issuecomment-45997480
  
    Looks good, will merge
;;;","13/Jun/14 14:39;sewen;Fixed in commit c81d7b02e0affb875af2851fc7c611a1976d69a1;;;","17/Jun/14 22:19;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/5
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement AvroOutputFormat,FLINK-916,12720098,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,qmlmoon,github-import,github-import,09/Jun/14 13:08,13/Jun/14 14:42,14/Jul/23 05:57,13/Jun/14 14:42,,,,0.6-incubating,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/916
Created by: [qmlmoon|https://github.com/qmlmoon]
Labels: 
Created at: Fri Jun 06 21:20:02 CEST 2014
State: open
",,github-import,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:08;github-import;pull-request-916-200075741937579057.patch;https://issues.apache.org/jira/secure/attachment/12649373/pull-request-916-200075741937579057.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398297,,,Fri Jun 13 14:42:17 UTC 2014,,,,,,,,,,"0|i1wklz:",398424,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:08;github-import;[Date: Sat Jun 07 10:06:17 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think we should test that with a few different types of objects (plain
Java, Avro generated).

A few non working cases would also be good to test for proper error
handling.

Also, I think it is useful to have a method that accepts a schema in which
the objects should be written. That's important for ""schema"" evolution.;;;","09/Jun/14 13:08;github-import;[Date: Sat Jun 07 12:03:20 CEST 2014, Author: [qmlmoon|https://github.com/qmlmoon]]

I think I already added the plan java and avro generated test case. Do you mean to have a few more?

For the error handling, actually i just follow the ```AvroInputFormat``` and there's no error handling now. To be honest I can't find some error cases here to be handled:). What error handling do you expect here?

Don't add a method for ""schema"" is also because i want to make it somehow symmetric to the ```AvroInputFormat```;;;","09/Jun/14 13:08;github-import;[Date: Sat Jun 07 13:42:42 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think the output format needs a variant where you pass a schema. In order
to keep the same output data in the presence of evolving objects.;;;","09/Jun/14 20:55;rmetzger;This code from the *AvroOutputFormat.java* needs some reworking:
{code:java}
			datumWriter = new SpecificDatumWriter<E>(avroValueType);
 			try {
 				schema = ((org.apache.avro.specific.SpecificRecordBase)avroValueType.newInstance()).getSchema();
 			} catch (InstantiationException e) {
 				throw new RuntimeException(e.getMessage());
 			} catch (IllegalAccessException e) {
 				throw new RuntimeException(e.getMessage());
 			}
{code}
I would do something like 
{code:java}
throw new RuntimeException(""Unable to instantiate value type"",e);
{code}
The problem is that you are loosing the stack of the exception you've caught.

Can you re-open the pull request against apache/incubator-flink ?
https://github.com/apache/incubator-flink ;;;","13/Jun/14 14:42;sewen;Merged in f82c61fdcf9cc8a77eb2aed0a38abc615e18ff03;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added globalConfig subsection in WebInterface,FLINK-914,12720096,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:08,09/Jun/14 13:08,14/Jul/23 05:57,09/Jun/14 13:08,,,,pre-apache,,,,,,,0,github-import,,"Hi,

I implemented a subsection in the Webinterface for the Global Configurations. It looks like this
![bildschirmfoto 2014-06-06 um 15 53 49|https://cloud.githubusercontent.com/assets/5738978/3200919/24c21c8e-ed82-11e3-846c-4f24ef3687b5.png]
![bildschirmfoto 2014-06-06 um 15 53 58|https://cloud.githubusercontent.com/assets/5738978/3200924/2cbb5d9c-ed82-11e3-8d4c-e0aeb006ccea.png]

@rmetzger

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/914
Created by: [JonathanH5|https://github.com/JonathanH5]
Labels: 
Created at: Fri Jun 06 15:55:43 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:08;github-import;pull-request-914-801996927190680506.patch;https://issues.apache.org/jira/secure/attachment/12649371/pull-request-914-801996927190680506.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398295,,,Mon Jun 09 13:08:44 UTC 2014,,,,,,,,,,"0|i1wklj:",398422,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:08;github-import;[Date: Fri Jun 06 16:18:12 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you.
Can you add the link to the left (next to overview and history) instead of the ""Log Files"" drop down (which does not make any sense anyways) ?

The configuration view should be integrated into the layout (like the overview and history);;;","09/Jun/14 13:08;github-import;[Date: Fri Jun 06 16:18:42 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

This is a fix for issue: https://github.com/stratosphere/stratosphere/issues/849;;;","09/Jun/14 13:08;github-import;[Date: Fri Jun 06 16:24:24 CEST 2014, Author: [JonathanH5|https://github.com/JonathanH5]]

Hm, ok... . I choosed the ""easy way"" (your briefing sounded more like this)  by generating the page with java. If you want me to integrate it in the normal layout I need to create an additional html file and send the information via ajax ... . 
I can't do this today, I am going to do this in the next days... . 
;;;","09/Jun/14 13:08;github-import;[Date: Fri Jun 06 16:25:55 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I mean this solution is fine .. but it would be nicer to have it concise with the rest of the page.
As a next step, I would also like to add information about the connected TaskManagers (memory, cpus etc.)
;;;","09/Jun/14 13:08;github-import;[Date: Mon Jun 09 01:14:13 CEST 2014, Author: [JonathanH5|https://github.com/JonathanH5]]

What do you think now?
![bildschirmfoto 2014-06-09 um 01 09 32|https://cloud.githubusercontent.com/assets/5738978/3212535/96e327a4-ef62-11e3-84f4-fbd670fffe4c.png]
;;;","09/Jun/14 13:08;github-import;[Date: Mon Jun 09 09:49:53 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Amazing!
I suggest to rename it from ""Global Configuration"" to ""Configuration"".
Can you squash together the commits so that your pull request is only one commit? (Use force push to update the branch this PR is based on);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix dependency exclusion for examples in yarn-uberjar,FLINK-913,12720095,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:08,09/Jun/14 13:08,14/Jul/23 05:57,09/Jun/14 13:08,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/913
Created by: [qmlmoon|https://github.com/qmlmoon]
Labels: 
Created at: Fri Jun 06 15:45:24 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:08;github-import;pull-request-913-5515597389136172745.patch;https://issues.apache.org/jira/secure/attachment/12649370/pull-request-913-5515597389136172745.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398294,,,Mon Jun 09 13:08:08 UTC 2014,,,,,,,,,,"0|i1wklb:",398421,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:08;github-import;[Date: Fri Jun 06 15:45:56 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Good to merge.;;;","09/Jun/14 13:08;github-import;[Date: Fri Jun 06 16:00:34 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Will merge in the next batch;;;","09/Jun/14 13:08;github-import;[Date: Sun Jun 08 12:16:34 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [0bb812e431f266a74ff9a668e055209ba5122c7d|https://github.com/stratosphere/stratosphere/commit/0bb812e431f266a74ff9a668e055209ba5122c7d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix maven enforcer plugin warnings in Eclipse,FLINK-912,12720094,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:07,09/Jun/14 13:08,14/Jul/23 05:57,09/Jun/14 13:08,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/912
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Fri Jun 06 15:19:07 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:08;github-import;pull-request-912-7629972960322049136.patch;https://issues.apache.org/jira/secure/attachment/12649369/pull-request-912-7629972960322049136.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398293,,,Mon Jun 09 13:08:02 UTC 2014,,,,,,,,,,"0|i1wkl3:",398420,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:08;github-import;[Date: Fri Jun 06 15:20:51 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Good to merge;;;","09/Jun/14 13:08;github-import;[Date: Fri Jun 06 15:59:06 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, will merge;;;","09/Jun/14 13:08;github-import;[Date: Sun Jun 08 12:15:49 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [2a9ed093f9b462956af87ef09c1d5f67b89f9e1c|https://github.com/stratosphere/stratosphere/commit/2a9ed093f9b462956af87ef09c1d5f67b89f9e1c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add test for broken type extraction in with Java 6 JVM.,FLINK-911,12720093,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,github-import,github-import,09/Jun/14 13:07,18/Jun/14 12:34,14/Jul/23 05:57,18/Jun/14 12:34,,,,0.6-incubating,pre-apache,pre-apache-0.5.1,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/911
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Fri Jun 06 13:43:09 CEST 2014
State: open
",,github-import,rmetzger,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:07;github-import;pull-request-911-8634828199706173291.patch;https://issues.apache.org/jira/secure/attachment/12649368/pull-request-911-8634828199706173291.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398292,,,Wed Jun 18 12:34:40 UTC 2014,,,,,,,,,,"0|i1wkkv:",398419,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:07;github-import;[Date: Fri Jun 06 13:43:22 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Test case for issue ([#910|https://github.com/stratosphere/stratosphere/issues/910] | [FLINK-910|https://issues.apache.org/jira/browse/FLINK-910]) ;;;","18/Jun/14 12:34;rmetzger;The issue has been resolved by https://github.com/apache/incubator-flink/pull/12.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Type extractor fails in Java 6,FLINK-910,12720092,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:07,17/Jun/14 22:19,14/Jul/23 05:57,13/Jun/14 00:09,,,,pre-apache,,,,,,,0,github-import,,"The return type `Tuple2<Integer, double[]>` cannot be extracted in Java 6.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/910
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Fri Jun 06 13:42:48 CEST 2014
State: open
",,githubbot,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398291,,,Tue Jun 17 22:19:30 UTC 2014,,,,,,,,,,"0|i1wkkn:",398418,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:07;github-import;[Date: Fri Jun 06 13:49:53 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

I can work on this issue next week.;;;","09/Jun/14 13:07;github-import;[Date: Fri Jun 06 14:35:19 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Thanks. I added a test case ([#911|https://github.com/stratosphere/stratosphere/issues/911] | [FLINK-911|https://issues.apache.org/jira/browse/FLINK-911]) so it should be easy to reproduce. You
need to run java6, though. 7 & 8 work fine.;;;","12/Jun/14 15:56;githubbot;GitHub user twalthr opened a pull request:

    https://github.com/apache/incubator-flink/pull/12

    Fixes a bug in the Java 6 JVM where primitive arrays have a wrong type.

    Fixes FLINK-910 and updates TypeInfoParser.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/twalthr/incubator-flink TypeExtractorPrimitiveArraysJava6

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/12.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #12
    
----
commit 6f4e55e435306c0fa66e0b8ee3b49b72a1cca545
Author: twalthr <info@twalthr.com>
Date:   2014-06-12T15:45:57Z

    Fixes a bug in the Java 6 JVM where primitive arrays have a wrong type.

----
;;;","13/Jun/14 00:09;sewen;Fixed by twalther's patch https://github.com/apache/incubator-flink/pull/12;;;","17/Jun/14 22:19;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/12
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pitfall due to additional superstep after the iteration has stopped,FLINK-909,12720091,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mholzemer,github-import,github-import,09/Jun/14 13:07,02/Sep/14 17:18,14/Jul/23 05:57,08/Aug/14 13:32,,,,pre-apache,,,,,,,0,github-import,,"Currently, after an iteration has exceeded the maximum number of iterations, all tasks are started again for an additional superstep during which they are stopped. This works if a tasks only waits for dynamic input. However, in the case where one has a task, e.g. a coGroup operation, which gets dynamic and static input the execution is not blocked. This can then lead to erroneous behaviour which the user is not aware of.

I had this problem implementing ALS. Here one has a loop which gets as dynamic input matrix columns and as static input matrix entries. The columns and the entries are used to construct a matrix which represents a system of linear equations. If the set of columns are empty, then the matrix is singular and thus not solvable. During the additional superstep the task won't receive any columns but would still try to solve the now singular matrix.

It would be good to finish the iteration without initiating this additional superstep.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/909
Created by: [tillrohrmann|https://github.com/tillrohrmann]
Labels: 
Created at: Thu Jun 05 17:50:17 CEST 2014
State: open
",,githubbot,github-import,mholzemer,uce,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398290,,,Tue Sep 02 17:18:10 UTC 2014,,,,,,,,,,"0|i1wkkf:",398417,,,,,,,,,,,,,,,,,,,,"23/Jun/14 10:30;mholzemer;I also stumbled over this issue a few times. Since I am currently in the process of refactoring the iterations runtime I will have a look at this issue.
It should be possible to add a barrier at the start of each superstep and wait for an explicit OK message from the iteration head task (that is managing a single iteration instance at one taskmanager) before the next superstep can start.

;;;","01/Jul/14 11:47;mholzemer;I started working on this issue but it seems to be more complicated then I thought. When I wait on a barrier before calling super.run() (that is also calling the open method) inside of the IterationPactTasks I do not receive any channel events.
Has somebody with more experience in the runtime a suggestion why this is the case and what I can do about it?;;;","15/Jul/14 11:46;githubbot;GitHub user markus-h opened a pull request:

    https://github.com/apache/incubator-flink/pull/69

    [FLINK-909] Additional superstep barrier

    This change introduces a new additional barrier at the end of a superstep in iterations. It prevents the execution of an extra iteration at termination and therby fixes[FLINK-909].
    
    This pull request depends on [FLINK-951] and uses the old package names, because [FLINK-951] was not merged yet.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/markus-h/incubator-flink aggregatorsReworkToAccumulatorsRebase4_sync2-pr

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/69.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #69
    
----
commit 9da8aa6442861be17c7658c1dd2c2d9a6943d507
Author: Markus Holzemer <markus.holzemer@gmx.de>
Date:   2014-06-16T12:56:36Z

    Iteration superstep synchronization through RPC and JobManager
    Unification of Accumulators and Aggregators (removal of former Aggregators)
    Adjusted Testcases accordingly

commit f41bf163203621b7dddd5a3073e654f40cb6143b
Author: Markus Holzemer <markus.holzemer@gmx.de>
Date:   2014-07-14T13:59:50Z

    Additional superstep barrier for iterations to prevent execution of an extra superstep at termination

----
;;;","07/Aug/14 20:32;githubbot;GitHub user StephanEwen opened a pull request:

    https://github.com/apache/incubator-flink/pull/91

    [FLINK-909], [FLINK945] Remove additional superstep at the end of iterrations

    Fixes simultaneously the pitfall reported in [FLINK-909] and the memory release bug reported in [FLINK-945].

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink iterbug

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/91.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #91
    
----
commit 76a48df00f0fc202bdedefb33d576fd9a18a92c0
Author: Stephan Ewen <sewen@apache.org>
Date:   2014-08-07T20:00:51Z

    [FLINK-909]  Remove additional empty (and non empty for iterative broadcast variables) superstep.
    [FLINK-945]  Fix early memory release in iterations

----
;;;","08/Aug/14 08:19;githubbot;Github user uce commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/91#discussion_r15982053
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/iterative/concurrent/SuperstepKickoffLatch.java ---
    @@ -0,0 +1,65 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.runtime.iterative.concurrent;
    +
    +public class SuperstepKickoffLatch {
    +	
    +	private final Object monitor = new Object();
    +	
    +	private int superstepNumber = 1;
    +	
    +	private boolean terminated;
    +	
    +	public void triggerNextSuperstep() {
    +		synchronized (monitor) {
    +			if (terminated) {
    +				throw new IllegalStateException(""Already teriminated."");
    --- End diff --
    
    typo: terminated
;;;","08/Aug/14 08:19;githubbot;Github user uce commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/91#discussion_r15982060
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/iterative/concurrent/SuperstepKickoffLatchBroker.java ---
    @@ -0,0 +1,32 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +
    --- End diff --
    
    blank line
;;;","08/Aug/14 08:20;githubbot;Github user uce commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/91#discussion_r15982062
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/iterative/concurrent/SuperstepKickoffLatchBroker.java ---
    @@ -0,0 +1,32 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +
    +package org.apache.flink.runtime.iterative.concurrent;
    +
    +public class SuperstepKickoffLatchBroker extends Broker<SuperstepKickoffLatch> {
    +
    +	private static final SuperstepKickoffLatchBroker INSTANCE = new SuperstepKickoffLatchBroker();
    +
    +	private SuperstepKickoffLatchBroker() {}
    +
    +
    --- End diff --
    
    blank line
;;;","08/Aug/14 08:22;githubbot;Github user uce commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/91#discussion_r15982153
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/iterative/task/AbstractIterativePactTask.java ---
    @@ -210,28 +210,17 @@ public RuntimeAggregatorRegistry getIterationAggregators() {
     		return this.iterationAggregators;
     	}
     
    -	protected void checkForTerminationAndResetEndOfSuperstepState() throws IOException {
    +	protected void verifyEndOfSuperstepState() throws IOException {
    --- End diff --
    
    You removed the sanity check at the end of this method, because it does not/should not happen, right?
;;;","08/Aug/14 08:27;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/91#issuecomment-51575648
  
    Nice addition to get rid of the empty last iteration. Looks good to me :-) I've posted minor comments inline. After @rmetzger tested it, I think it's good to merge.
    
    Could you reproduce the memory issue (FLINK-945) locally? If yes, can you explain what was the exact problem? I am wondering, because in theory the last iteration should not cause any problems, right?

;;;","08/Aug/14 12:34;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/91#issuecomment-51594740
  
    I just saw one test seems to be consistently in error with this patch. I'll have a look at it.
;;;","08/Aug/14 12:35;githubbot;Github user StephanEwen commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/91#discussion_r15990179
  
    --- Diff: flink-runtime/src/main/java/org/apache/flink/runtime/iterative/task/AbstractIterativePactTask.java ---
    @@ -210,28 +210,17 @@ public RuntimeAggregatorRegistry getIterationAggregators() {
     		return this.iterationAggregators;
     	}
     
    -	protected void checkForTerminationAndResetEndOfSuperstepState() throws IOException {
    +	protected void verifyEndOfSuperstepState() throws IOException {
    --- End diff --
    
    The is still a sanity check, but the distinction between end-of-superstep and termination comes no longer from the readers, but from the specific shared sync.
;;;","08/Aug/14 12:50;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/91#issuecomment-51596005
  
    The code was correct, but the test assumed the additional empty superstep.
    
    I adjusted the test and addressed Ufuk's comments.
;;;","08/Aug/14 12:52;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/91#issuecomment-51596116
  
    Thanks. :)
    
    I will merge it in the next batch in order to include it the next RC.
    
    On 08 Aug 2014, at 14:49, Stephan Ewen <notifications@github.com> wrote:
    
    > The code was correct, but the test assumed the additional empty superstep.
    > 
    > I adjusted the test and addressed Ufuk's comments.
    > 
    > —
    > Reply to this email directly or view it on GitHub.
    > 
;;;","08/Aug/14 13:26;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/91
;;;","08/Aug/14 13:32;uce;Fixed in b6ae1ef3d2f95db69c91203b138baf43bd030d93 and 76a48df00f0fc202bdedefb33d576fd9a18a92c0.;;;","01/Sep/14 16:16;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/69#issuecomment-54076052
  
    I have taken the delta of this patch to the pull request #36 and put it into 38cbf0b915dea175f287d7c2a627467a02d28474
    
    Thank you for the patch!
;;;","02/Sep/14 17:18;githubbot;Github user markus-h commented on the pull request:

    https://github.com/apache/incubator-flink/pull/69#issuecomment-54185839
  
    Thanks for mergin!
;;;","02/Sep/14 17:18;githubbot;Github user markus-h closed the pull request at:

    https://github.com/apache/incubator-flink/pull/69
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when compiling from the source,FLINK-906,12720088,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:07,09/Jun/14 13:07,14/Jul/23 05:57,09/Jun/14 13:07,,,,pre-apache,,,,,,,0,github-import,,"I use the ""git clone https://github.com/stratosphere/stratosphere.git"" command, and then I use ""mvn clean package -DskipTests"" to compile but it goes wrong. My java compiler version is javac 1.7.0_55.  The error message is follow:


[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ stratosphere-java ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 209 source files to /Users/wilsoncao/Documents/stratosphere/stratosphere-java/target/classes
?????? (1.6.0_65) ?г????쳣?? ????? Bug Parade ??û???ҵ??ô??????? Java Developer Connection (http://java.sun.com/webapps/bugreport)  ?Ըô?????й鵵?? ???ڱ????и??????ĳ?????????????Ϣ??лл???ĺ?????
java.lang.NullPointerException
	at com.sun.tools.javac.comp.Check.checkCompatibleConcretes(Check.java:1213)
	at com.sun.tools.javac.comp.Check.checkCompatibleSupertypes(Check.java:1567)
	at com.sun.tools.javac.comp.Attr.attribClassBody(Attr.java:2674)
	at com.sun.tools.javac.comp.Attr.attribClass(Attr.java:2628)
	at com.sun.tools.javac.comp.Attr.attribClass(Attr.java:2584)
	at com.sun.tools.javac.comp.Attr.attribClass(Attr.java:2584)
	at com.sun.tools.javac.comp.Attr.attribClass(Attr.java:2584)
	at com.sun.tools.javac.comp.Attr.attribClass(Attr.java:2564)
	at com.sun.tools.javac.main.JavaCompiler.attribute(JavaCompiler.java:1045)
	at com.sun.tools.javac.main.JavaCompiler.compile2(JavaCompiler.java:768)
	at com.sun.tools.javac.main.JavaCompiler.compile(JavaCompiler.java:730)
	at com.sun.tools.javac.main.Main.compile(Main.java:353)
	at com.sun.tools.javac.api.JavacTaskImpl.call(JavacTaskImpl.java:115)
	at org.codehaus.plexus.compiler.javac.JavaxToolsCompiler.compileInProcess(JavaxToolsCompiler.java:126)
	at org.codehaus.plexus.compiler.javac.JavacCompiler.performCompile(JavacCompiler.java:169)
	at org.apache.maven.plugin.compiler.AbstractCompilerMojo.execute(AbstractCompilerMojo.java:785)
	at org.apache.maven.plugin.compiler.CompilerMojo.execute(CompilerMojo.java:129)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:133)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:108)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:76)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:116)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:361)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:157)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] An unknown compilation problem occurred


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/906
Created by: [wilsoncao|https://github.com/wilsoncao]
Labels: 
Created at: Wed Jun 04 17:48:54 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398287,,,Mon Jun 09 13:07:37 UTC 2014,,,,,,,,,,"0|i1wkjr:",398414,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:07;github-import;[Date: Wed Jun 04 17:55:04 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Are you sure that you are using Java 1.7 ?
Because the exception says something about ""1.6.0_65"" and the bug you're mentioning here is only known for java 1.6

What is ""mvn -version"" saying?;;;","09/Jun/14 13:07;github-import;[Date: Wed Jun 04 17:57:07 CEST 2014, Author: [wilsoncao|https://github.com/wilsoncao]]

oh, My fault...., It is java 1.6.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using broadcast variables in UDFs within iterations leads to CompilerException,FLINK-905,12720087,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:07,09/Jun/14 13:07,14/Jul/23 05:57,09/Jun/14 13:07,,,,pre-apache,,,,,,,0,github-import,,"While working with the new Java API I discovered the following behaviour, which I consider to be a bug.

```java
IterativeDataSet<Theta> iteration = theta.iterate(maxIterations);

DataSet<Gradient> gradient = iteration.crossWithHuge(pointsWithLabels).with(new CrossFunction<Theta, Point, Gradient>() {
	@Override
		public Gradient cross(Theta theta, Point pointWithLabel) throws Exception { return ... }
})
.withBroadcastSet(numberOfPoints, ""numberOfPoints"")
.withParameters(config);
```






```java
Exception in thread ""main"" eu.stratosphere.compiler.CompilerException: No plan meeting the requirements could be created @ Cross (de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$4) (1:null)(2:null). Most likely reason: Too restrictive plan hints.
	at eu.stratosphere.compiler.dag.OptimizerNode.prunePlanAlternatives(OptimizerNode.java:755)
	at eu.stratosphere.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:480)
	at eu.stratosphere.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:251)
	at eu.stratosphere.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:251)
	at eu.stratosphere.compiler.dag.BulkIterationNode.instantiateCandidate(BulkIterationNode.java:289)
	at eu.stratosphere.compiler.dag.SingleInputNode.addLocalCandidates(SingleInputNode.java:373)
	at eu.stratosphere.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:316)
	at eu.stratosphere.compiler.dag.DataSinkNode.getAlternativePlans(DataSinkNode.java:192)
	at eu.stratosphere.compiler.PactCompiler.compile(PactCompiler.java:715)
	at eu.stratosphere.compiler.PactCompiler.compile(PactCompiler.java:553)
	at eu.stratosphere.client.LocalExecutor.executePlan(LocalExecutor.java:216)
	at eu.stratosphere.api.java.LocalEnvironment.execute(LocalEnvironment.java:57)
	at de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression.main(LogisticRegression.java:236)
```


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/905
Created by: [powibol|https://github.com/powibol]
Labels: 
Created at: Wed Jun 04 11:19:22 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398286,,,Mon Jun 09 13:07:32 UTC 2014,,,,,,,,,,"0|i1wkjj:",398413,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:07;github-import;[Date: Wed Jun 04 11:21:03 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for reporting the issue.
It has already been reported here: https://github.com/stratosphere/stratosphere/issues/880.
;;;","09/Jun/14 13:07;github-import;[Date: Wed Jun 04 11:21:23 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

What version of Stratosphere have you been using?;;;","09/Jun/14 13:07;github-import;[Date: Wed Jun 04 11:26:44 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

There has been a recent patch that addressed a similar issue. It is
available in the versions ""0.6-SNAPSHOT"" and ""0.5.1-SNAPSHOT"".

Can you try whether that solves your problem?;;;","09/Jun/14 13:07;github-import;[Date: Sun Jun 08 13:58:30 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Any news on this issue?;;;","09/Jun/14 13:07;github-import;[Date: Sun Jun 08 17:50:30 CEST 2014, Author: [powibol|https://github.com/powibol]]

I will test it with the new snapshot, but it's not done by now because of the incredibly good weather and an exam next week. :/
I'll report the results of the test soon!;;;","09/Jun/14 13:07;github-import;[Date: Sun Jun 08 17:52:03 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fair enough ;-);;;","09/Jun/14 13:07;github-import;[Date: Mon Jun 09 13:08:07 CEST 2014, Author: [powibol|https://github.com/powibol]]

Using the new snapshot solved the described problem.
Thank you for your assistance!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Webclient: optimizer plan not showing up,FLINK-901,12720083,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JonathanH5,github-import,github-import,09/Jun/14 13:06,18/Jun/14 12:22,14/Jul/23 05:57,18/Jun/14 12:22,,,,pre-apache,,,,,,,0,github-import,,"It seems that a file is missing, my browser's JavaScript console logs the following: 
```
GET http://localhost:8080/js/progressbar.js 404 (Not Found) 
```
I used the 0.5 release.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/901
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, gui, simple-issue, 
Created at: Tue Jun 03 12:32:55 CEST 2014
State: open
",,github-import,rmetzger,,,,,,,,,,,,,,,,FLINK-711,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398282,,,Wed Jun 18 12:22:05 UTC 2014,,,,,,,,,,"0|i1wkin:",398409,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:06;github-import;[Date: Tue Jun 03 12:51:19 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Does this give an error or simply log to the js console?;;;","09/Jun/14 13:06;github-import;[Date: Tue Jun 03 12:52:41 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It is a deprecated dependency not fully removed...;;;","09/Jun/14 13:06;github-import;[Date: Tue Jun 03 13:06:20 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

The webinterface does not show the plan and there is also no exception or
error. You can click ""Continue"" to submit the job.


On Tue, Jun 3, 2014 at 12:52 PM, Stephan Ewen <notifications@github.com>
wrote:

> It is a deprecated dependency not fully removed...
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/stratosphere/stratosphere/issues/901#issuecomment-44949489>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","18/Jun/14 12:22;rmetzger;The issue has been resolved by this pull request: https://github.com/apache/incubator-flink/pull/15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Webclient does not show System.out messages,FLINK-900,12720081,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,github-import,github-import,09/Jun/14 13:06,18/Jun/14 12:13,14/Jul/23 05:57,18/Jun/14 12:13,,,,0.6-incubating,pre-apache,,,,,,0,github-import,,"When submitting the KMeans Job using the web client, I'm getting the following exception:
```
An error occurred while invoking the program:

The program plan could not be fetched. The program silently swallowed the control flow exceptions.


eu.stratosphere.client.program.ProgramInvocationException: The program plan could not be fetched. The program silently swallowed the control flow exceptions.
	at eu.stratosphere.client.program.Client.getOptimizedPlan(Client.java:154)
	at eu.stratosphere.client.web.JobSubmissionServlet.doGet(JobSubmissionServlet.java:161)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:532)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:453)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:965)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:388)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:187)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:901)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:113)
	at org.eclipse.jetty.server.Server.handle(Server.java:352)
	at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596)
	at org.eclipse.jetty.server.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:1048)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:549)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:211)
	at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:425)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:489)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:436)
	at java.lang.Thread.run(Thread.java:744)
```

The problem is that the main() method just returns (printing a message to `System.out`).
This is not very intuitive, since you have to look into the webclient.out log

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/900
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Milestone: Release 0.5.1
Created at: Tue Jun 03 12:25:04 CEST 2014
State: open
",,github-import,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398280,,,Wed Jun 18 12:13:56 UTC 2014,,,,,,,,,,"0|i1wki7:",398407,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:06;github-import;[Date: Tue Jun 03 12:49:01 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Yes. This is somehow a fundamental problem with ""interactive"" style
programs.

We could temporarily redirect stdout and stderr to display the output in
the web Client.;;;","18/Jun/14 12:13;rmetzger;I resolved the issue according to your suggestion in this commit: https://github.com/apache/incubator-flink/commit/a84f8a1ccfc867499dfaf688b2f658f8a0f87ae4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix data sources and data sinks with splits #766,FLINK-899,12720080,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:06,09/Jun/14 13:06,14/Jul/23 05:57,09/Jun/14 13:06,,,,pre-apache,,,,,,,0,github-import,,"Fix for issue ([#766|https://github.com/stratosphere/stratosphere/issues/766] | [FLINK-766|https://issues.apache.org/jira/browse/FLINK-766]). 

DataSource displays processed input splits. DataSink is not displaying output, though. 

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/899
Created by: [tobwiens|https://github.com/tobwiens]
Labels: 
Created at: Mon Jun 02 17:31:37 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:06;github-import;pull-request-899-4935888483987149530.patch;https://issues.apache.org/jira/secure/attachment/12649363/pull-request-899-4935888483987149530.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398279,,,Mon Jun 09 13:06:40 UTC 2014,,,,,,,,,,"0|i1wkhz:",398406,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:06;github-import;[Date: Mon Jun 02 21:50:43 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Can you squash the commits together into one commit? (every pull request should roughly be one commit (a few more for really large PRs)
After you've squashed (git rebase -i HEAD~11) the commits together, do a force push to update the pull request;;;","09/Jun/14 13:06;github-import;[Date: Fri Jun 06 17:04:53 CEST 2014, Author: [tobwiens|https://github.com/tobwiens]]

I looked into the solution with the new event type. Since it would have been a lot of code for showing a small progress bar I went a different way. 
Through debugging if found: When a DataSouce finishes reading all its input splits it changes into the state FINISHING. It stays in that state until the data is handed over. 
Therefore I could shift the whole logic onto the webpage. 

The DataSources are identified (parsed for string DataSource) to find the correct progress bars to change. 

Therefore the webpage can stay event based. Additionally code is reduced dramatically. 

It can be found in a new pull request: Introduce two in one progress bars ([#915|https://github.com/stratosphere/stratosphere/issues/915] | [FLINK-915|https://issues.apache.org/jira/browse/FLINK-915]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in scheduler for job graph instance sharing,FLINK-897,12720078,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:06,21/Sep/14 02:18,14/Jul/23 05:57,21/Sep/14 02:18,,,,pre-apache,,,,,,,0,github-import,,"I discovered the following problem while writing a low-level task: the scheduler could not bring up the instance for `forwarder3` (state `SCHEDULED`, but not `ASSIGNED`).
```
producer => forwarder0 => forwarder1 => forwarder2 => forwarder3 => consumer
```
```java
producer.setVertexToShareInstancesWith(forwarder0);
forwarder0.setVertexToShareInstancesWith(forwarder1);
forwarder1.setVertexToShareInstancesWith(forwarder2);
forwarder2.setVertexToShareInstancesWith(forwarder3);
forwarder3.setVertexToShareInstancesWith(consumer);
```
Every task should share the vertex. The following equivalent sharing did work though:
```
forwarder0   ================> producer 
forwarder1   ===// // // //
forwarder2   =====// // //
forwarder3   =======// //
consumer     =========//
```
```java
forwarder0.setVertexToShareInstancesWith(producer);
forwarder1.setVertexToShareInstancesWith(producer);
forwarder2.setVertexToShareInstancesWith(producer);
forwarder3.setVertexToShareInstancesWith(producer);
consumer.setVertexToShareInstancesWith(producer);
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/897
Created by: [uce|https://github.com/uce]
Labels: bug, core, 
Created at: Mon Jun 02 14:57:03 CEST 2014
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398277,,,Sun Sep 21 02:18:37 UTC 2014,,,,,,,,,,"0|i1wkhj:",398404,,,,,,,,,,,,,,,,,,,,"21/Sep/14 02:18;sewen;Fixed in new scheduler, as of cdee87501762c092c216adf35fceeea339e0c4c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes bugs where the TypeExtractor throws an NPE instead of the operators,FLINK-896,12720077,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:06,09/Jun/14 13:06,14/Jul/23 05:57,09/Jun/14 13:06,,,,pre-apache,,,,,,,0,github-import,,"If the user accidentally passes ```null``` instead of e.g. a MapFunction. The TypeExtractor throws the NPE instead of the higher layer.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/896
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Mon Jun 02 14:45:14 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:06;github-import;pull-request-896-5355648808718160692.patch;https://issues.apache.org/jira/secure/attachment/12649362/pull-request-896-5355648808718160692.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398276,,,Mon Jun 09 13:06:29 UTC 2014,,,,,,,,,,"0|i1wkhb:",398403,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:06;github-import;[Date: Fri Jun 06 13:19:09 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good to merge;;;","09/Jun/14 13:06;github-import;[Date: Fri Jun 06 15:19:29 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Will merge now...;;;","09/Jun/14 13:06;github-import;[Date: Sun Jun 08 12:16:16 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc|https://github.com/stratosphere/stratosphere/commit/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent Iteration Step Function gives nonesense error message,FLINK-893,12720074,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mholzemer,github-import,github-import,09/Jun/14 13:06,03/Jul/14 10:37,14/Jul/23 05:57,03/Jul/14 10:37,,,,pre-apache,,,,,,,0,github-import,,"
```
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

DataSet<String> input = env.readTextFile(IN_FILE).name(""source1"");
			
IterativeDataSet<String> iteration1 = input.iterate(100);
IterativeDataSet<String> iteration2 = input.iterate(20);
		
iteration1.closeWith(iteration2.map(new IdentityMapper<String>())).print();
```
The code if wrong (closing iteration 1 with the step function of iteration 2), but it should not give the unsupported operation exception.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/893
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Mon Jun 02 03:32:18 CEST 2014
State: open
",,githubbot,github-import,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398273,,,Thu Jul 03 10:37:51 UTC 2014,,,,,,,,,,"0|i1wkgn:",398400,,,,,,,,,,,,,,,,,,,,"30/Jun/14 09:51;githubbot;GitHub user markus-h opened a pull request:

    https://github.com/apache/incubator-flink/pull/50

    [FLINK-893] Inconsistent Iteration Step Function gives nonesense error message

    I adjusted the error message.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/markus-h/incubator-flink iteration_translation_exception

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/50.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #50
    
----
commit 34a6f87055c4ee3453620ea287504dd43882833f
Author: Markus Holzemer <markus.holzemer@gmx.de>
Date:   2014-06-30T09:49:48Z

    adjusted error message when encountering a translation failure for iterations

----
;;;","03/Jul/14 10:37;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/50
;;;","03/Jul/14 10:37;rmetzger;Fixed in https://github.com/apache/incubator-flink/commit/8a8060eaa3a12703b594d4f4c581fa14bd846b19. Thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offer buffer-oriented API for I/O and replace custom Java NIO TCP/IP code with Netty 4 library,FLINK-892,12720073,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:05,09/Jun/14 13:06,14/Jul/23 05:57,09/Jun/14 13:06,,,,pre-apache,,,,,,,0,github-import,,"This PR includes an old PR (([#511|https://github.com/stratosphere/stratosphere/issues/511] | [FLINK-511|https://issues.apache.org/jira/browse/FLINK-511])), which has not been merged and replaces the custom Java NIO TCP/IP code with the [Netty 4 library|http://netty.io/] on top of it. See ([#511|https://github.com/stratosphere/stratosphere/issues/511] | [FLINK-511|https://issues.apache.org/jira/browse/FLINK-511]) and ([#25|https://github.com/stratosphere/stratosphere/issues/25] | [FLINK-25|https://issues.apache.org/jira/browse/FLINK-25]) for some notes on the buffer-oriented API.

Netty replaces our custom NIO code, which was used to setup the TCP connections and queue network envelopes. The main Netty-related stuff is located in the `runtime.io.network.netty` package: `NettyConnectionManager` and `OutboundConnectionQueue` are the entry points for connection setup and envelope queueing. `OutboundEnvelopeEncoder` and `InboundEnvelopeDecoder` take care of serialization and deserialization. All main components have respective test cases. I also went over the buffer-orientation changes but did not add any further tests since the original PR yet.

I will now run the test job on a cluster and get back to confirm that this also fixes ([#652|https://github.com/stratosphere/stratosphere/issues/652] | [FLINK-652|https://issues.apache.org/jira/browse/FLINK-652]).


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/892
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Sun Jun 01 16:43:15 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:06;github-import;pull-request-892-1535228427019480401.patch;https://issues.apache.org/jira/secure/attachment/12649360/pull-request-892-1535228427019480401.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398272,,,Mon Jun 09 13:06:12 UTC 2014,,,,,,,,,,"0|i1wkgf:",398399,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:06;github-import;[Date: Sun Jun 01 16:44:31 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Nice one!

Merging this after merging the ""de-cloudify"" code will be fun :D;;;","09/Jun/14 13:06;github-import;[Date: Sun Jun 08 12:15:26 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [b0f8ba06953f978516044db220a89cacbcbb2d5c|https://github.com/stratosphere/stratosphere/commit/b0f8ba06953f978516044db220a89cacbcbb2d5c]

Nice work :-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential bug due to unnecessary cast from long to int,FLINK-891,12720072,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:05,09/Jun/14 13:05,14/Jul/23 05:57,09/Jun/14 13:05,,,,pre-apache,,,,,,,0,github-import,,"The `FSDataInputStreamWrapper`  accepts an int, but the length is a long (and all methods passing the length are passing long.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/891
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Sun Jun 01 11:57:03 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:05;github-import;pull-request-891-413265463107784310.patch;https://issues.apache.org/jira/secure/attachment/12649359/pull-request-891-413265463107784310.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398271,,,Mon Jun 09 13:05:42 UTC 2014,,,,,,,,,,"0|i1wkg7:",398398,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:05;github-import;[Date: Thu Jun 05 11:18:02 CEST 2014, Author: [uce|https://github.com/uce]]

Merged in [1ce11d3a38f0e310bb8287a51208ac2e42b13d63|https://github.com/stratosphere/stratosphere/commit/1ce11d3a38f0e310bb8287a51208ac2e42b13d63].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code from documentation does not compile [0.5-rc2],FLINK-889,12720070,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:05,09/Jun/14 13:05,14/Jul/23 05:57,09/Jun/14 13:05,,,,pre-apache,,,,,,,0,github-import,,"http://stratosphere.eu/docs/0.5/programming_guides/scala.html#data-sinks
```scala
val out: DataSet[(String, Int)]
val sink = out.write(""file:///some/file"", CsvOutputFormat())

val plan = new ScalaPlan(Seq(sink)) // <-- compile error
```
ScalaPlan expects a `GenericDataSinkBase`, however `out.write` returns a `ScalaSink` which inherits from `AnyRef`.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/889
Created by: [knub|https://github.com/knub]
Labels: 
Created at: Fri May 30 10:46:48 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398269,,,Mon Jun 09 13:05:34 UTC 2014,,,,,,,,,,"0|i1wkfr:",398396,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:05;github-import;[Date: Fri May 30 11:17:49 CEST 2014, Author: [knub|https://github.com/knub]]

Argh, was using Plan(Seq(sink)) instead of ScalaPlan(Seq(sink)). I'm sorry!;;;","09/Jun/14 13:05;github-import;[Date: Fri May 30 11:20:36 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

That's fine...

Let us know if you find inconsistencies in the Documentation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve JobManager heap space memory configuration for YARN,FLINK-887,12720068,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,github-import,github-import,09/Jun/14 13:05,19/Dec/16 12:47,14/Jul/23 05:57,18/Jun/14 12:48,,,,0.6-incubating,pre-apache,pre-apache-0.5.1,Deployment / YARN,,,,0,github-import,,"just saw this while testing for the 0.5 release.
The JM sometimes fails because I forgot to subtract a few %% from the JM heapspace for extra JVM allocations.

{code:xml}
2014-05-29 12:01:53,770 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Process tree for container: container_1401362984347_0002_01_000001 has processes older than 1 iteration running over the configured limit. Limit=1073741824, current usage = 1125031936
2014-05-29 12:01:53,776 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Container [pid=2477,containerID=container_1401362984347_0002_01_000001] is running beyond physical memory limits. Current usage: 1.0 GB of 1 GB physical memory used; 1.7 GB of 5 GB virtual memory used. Killing container.
Dump of the process-tree for container_1401362984347_0002_01_000001 :
	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
	|- 2483 2477 2477 2477 (java) 5021 548 1711837184 274360 /usr/java/latest/bin/java -Xmx1000M -Dlog.file=/mnt/var/log/hadoop/userlogs/application_1401362984347_0002/container_1401362984347_0002_01_000001/jobmanager-log4j.log -Dlog4j.configuration=file:log4j.properties eu.stratosphere.yarn.ApplicationMaster 
	|- 2477 2094 2477 2477 (bash) 0 1 117805056 306 /bin/bash -c /usr/java/latest/bin/java -Xmx1000M  -Dlog.file=/mnt/var/log/hadoop/userlogs/application_1401362984347_0002/container_1401362984347_0002_01_000001/jobmanager-log4j.log -Dlog4j.configuration=file:log4j.properties eu.stratosphere.yarn.ApplicationMaster  1>/mnt/var/log/hadoop/userlogs/application_1401362984347_0002/container_1401362984347_0002_01_000001/jobmanager-stdout.log 2>/mnt/var/log/hadoop/userlogs/application_1401362984347_0002/container_1401362984347_0002_01_000001/jobmanager-stderr.log 

2014-05-29 12:01:53,777 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1401362984347_0002_01_000001 transitioned from RUNNING to KILLING
{code}

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/887
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, YARN, 
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Thu May 29 14:47:59 CEST 2014
State: open
",,github-import,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398267,,,Wed Jun 18 12:48:56 UTC 2014,,,,,,,,,,"0|i1wkfb:",398394,,,,,,,,,,,,,,,,,,,,"10/Jun/14 16:58;rmetzger;I supplied a fix for this issue: https://github.com/apache/incubator-flink/pull/1;;;","18/Jun/14 12:48;rmetzger;The pull request has been merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add missing Usercode Classloader for the ""info"" command in the CliFrontend",FLINK-884,12720065,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:05,09/Jun/14 13:05,14/Jul/23 05:57,09/Jun/14 13:05,,,,pre-apache,,,,,,,0,github-import,,"Rework stratosphere-clients tests to not expect a binary blob in the test resources (build the jar file using maven)

add a test for ensuring the presense of a custom classloader in the CliFrontend
Add missing ClassLoader for the ""info"" command in the CliFrontend

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/884
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Thu May 29 09:57:45 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:05;github-import;pull-request-884-7880368320920543440.patch;https://issues.apache.org/jira/secure/attachment/12649358/pull-request-884-7880368320920543440.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398264,,,Mon Jun 09 13:05:16 UTC 2014,,,,,,,,,,"0|i1wken:",398391,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:05;github-import;[Date: Thu May 29 09:58:56 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Where else in the project are we shipping jar files for testing purposes? If you want, I can also add the mechanism to generate this jar file there.;;;","09/Jun/14 13:05;github-import;[Date: Sat May 31 10:26:45 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I'm currently preparing an update for this pull request to remove all "".jar"" files in our project.;;;","09/Jun/14 13:05;github-import;[Date: Sat May 31 14:23:16 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Very good!;;;","09/Jun/14 13:05;github-import;[Date: Sat May 31 14:34:58 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

:+1: !;;;","09/Jun/14 13:05;github-import;[Date: Sun Jun 01 12:30:22 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

The removal of the jar is done. The only blocker for this pull request is the question regarding the public `setUserCodeClassLoader()` method.;;;","09/Jun/14 13:05;github-import;[Date: Thu Jun 05 11:36:39 CEST 2014, Author: [uce|https://github.com/uce]]

I removed the setter and used mockito to create a partial mock in the test case, which returns the test class loader: https://github.com/uce/stratosphere/tree/rmetzger-infoFixPr.

I'm merging it if Travis gives the OK: https://travis-ci.org/uce/stratosphere/builds/26847638.;;;","09/Jun/14 13:05;github-import;[Date: Thu Jun 05 16:54:40 CEST 2014, Author: [uce|https://github.com/uce]]

Merged in [d80b929f57e3a52a4ce688acd19159236a697c7e|https://github.com/stratosphere/stratosphere/commit/d80b929f57e3a52a4ce688acd19159236a697c7e].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DistCache: executable flag, directories",FLINK-882,12720063,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,github-import,github-import,09/Jun/14 13:04,18/Jun/14 12:55,14/Jul/23 05:57,18/Jun/14 12:55,,,,0.6-incubating,pre-apache,,,,,,0,github-import,,"new features:
- you can now distribute whole directories (this copies all files tracked by ``listStatus`` recursively)
- when distributing single files they can be tagged as executable (automatically checked for local files)

other changes:
- the name/path scheme was changed to preserve the original name. all files are now stored in a job-specific folder, instead of having a job/identifier specific name.
- the arguments passed when registering a file are abstracted away in its own data structure, making it easier to expand the DC the future. (there is no reason why one should have to change ``NepheleJobGraphGenerator`` or ``TaskManager`` for this)
- reworded/expanded some javadocs, and made it clear that local files can only be used when they are accessible form all workers

sidenotes:
- ``FileCache.CopyProcess.create()`` is basically a copy function. i think we could add this as a static method to ``FileSystem`` as ``copy(URI sourcePath, URI targetPath)``.
- the code will be tested with the python interface


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/882
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Wed May 28 23:24:41 CEST 2014
State: open
",,github-import,rmetzger,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:05;github-import;pull-request-882-6600334915844166448.patch;https://issues.apache.org/jira/secure/attachment/12649357/pull-request-882-6600334915844166448.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398262,,,Wed Jun 18 12:55:13 UTC 2014,,,,,,,,,,"0|i1wke7:",398389,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:05;github-import;[Date: Wed May 28 23:31:27 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Cool! Can you add some more tests for the DistributedCache ? 
Especially for bugs you've encountered with the python interface.;;;","09/Jun/14 13:05;github-import;[Date: Thu May 29 14:45:19 CEST 2014, Author: [zentol|https://github.com/zentol]]

sure.;;;","09/Jun/14 13:05;github-import;[Date: Fri Jun 06 23:45:55 CEST 2014, Author: [zentol|https://github.com/zentol]]

verified on the cluster.;;;","09/Jun/14 13:05;github-import;[Date: Sun Jun 08 12:58:53 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think this is good to merge.

Will merge in the next batch;;;","18/Jun/14 12:55;rmetzger;This was a GitHub pull request from the old repository that has been merged: https://github.com/apache/incubator-flink/pull/3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Using another drive C:/D: other than where the stratosphere distribution is, results in file not found",FLINK-881,12720062,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:04,09/Jun/14 13:04,14/Jul/23 05:57,09/Jun/14 13:04,,,,pre-apache,,,,,,,0,github-import,,"I tried to slow down the wordcount example by using a slow sdcard as input, to ""watch"" stratosphere reading the input splits. I checked other partitions and drives and found that it only works with my drive D:. The stratosphere runtime is located on D:.

Command:
```
D:\stratosphereWorkEnvironment\workspace\stratosphere\stratosphere-dist\target\stratosphere-dist-0.5-SNAPSHOT-bin\strato
sphere-0.5-SNAPSHOT\bin>java -Xmx512m -cp ""D:\stratosphereWorkEnvironment\workspace\stratosphere\stratosphere-dist\targe
t\stratosphere-dist-0.5-SNAPSHOT-bin\stratosphere-0.5-SNAPSHOT\bin\..\lib\*"" eu.stratosphere.client.CliFrontend run --ja
rfile D:/stratosphereWorkEnvironment/workspace/stratosphere/stratosphere-dist/target/stratosphere-dist-0.5-SNAPSHOT-bin/
stratosphere-0.5-SNAPSHOT/examples/stratosphere-scala-examples-0.5-SNAPSHOT-WordCount.jar --arguments 10 file://G:/hamle
t.txt file://D:/stratosphereWorkEnvironment/testdata/wordcount/wordcount-result.txt -v -w
```
Exceptions:
```
14/05/28 17:33:27 WARN io.DelimitedInputFormat: Could not determine statistics for file 'file://G:/hamlet.txt' due to an
 io error: File file://G:/hamlet.txt does not exist.
14/05/28 17:33:27 ERROR client.JobClient: ERROR: eu.stratosphere.nephele.executiongraph.GraphConversionException: Cannot
 compute input splits for CHAIN DataSource(File file://G:/hamlet.txt) -> Map (<Unnamed Mapper>): java.io.FileNotFoundExc
eption: File file://G:/hamlet.txt does not exist.
        at eu.stratosphere.core.fs.local.LocalFileSystem.getFileStatus(LocalFileSystem.java:100)
        at eu.stratosphere.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:410)
        at eu.stratosphere.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:70)
        at eu.stratosphere.pact.runtime.task.DataSourceTask.computeInputSplits(DataSourceTask.java:332)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:552)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:273)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:174)
        at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:503)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:417)
        at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:951)

        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:555)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:273)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:174)
        at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:503)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:417)
        at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:951)

Error: The program execution failed: eu.stratosphere.nephele.executiongraph.GraphConversionException: Cannot compute inp
ut splits for CHAIN DataSource(File file://G:/hamlet.txt) -> Map (<Unnamed Mapper>): java.io.FileNotFoundException: File
 file://G:/hamlet.txt does not exist.
        at eu.stratosphere.core.fs.local.LocalFileSystem.getFileStatus(LocalFileSystem.java:100)
        at eu.stratosphere.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:410)
        at eu.stratosphere.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:70)
        at eu.stratosphere.pact.runtime.task.DataSourceTask.computeInputSplits(DataSourceTask.java:332)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:552)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:273)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:174)
        at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:503)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:417)
        at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:951)

        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:555)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:273)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:174)
        at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:503)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:417)
        at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:951)

eu.stratosphere.client.program.ProgramInvocationException: The program execution failed: eu.stratosphere.nephele.executi
ongraph.GraphConversionException: Cannot compute input splits for CHAIN DataSource(File file://G:/hamlet.txt) -> Map (<U
nnamed Mapper>): java.io.FileNotFoundException: File file://G:/hamlet.txt does not exist.
        at eu.stratosphere.core.fs.local.LocalFileSystem.getFileStatus(LocalFileSystem.java:100)
        at eu.stratosphere.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:410)
        at eu.stratosphere.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:70)
        at eu.stratosphere.pact.runtime.task.DataSourceTask.computeInputSplits(DataSourceTask.java:332)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:552)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:273)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:174)
        at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:503)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:417)
        at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:951)

        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:555)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:273)
        at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:174)
        at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:503)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:417)
        at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:951)

        at eu.stratosphere.client.program.Client.run(Client.java:237)
        at eu.stratosphere.client.program.Client.run(Client.java:203)
        at eu.stratosphere.client.program.Client.run(Client.java:197)
        at eu.stratosphere.client.program.Client.run(Client.java:165)
        at eu.stratosphere.client.CliFrontend.run(CliFrontend.java:337)
        at eu.stratosphere.client.CliFrontend.parseParameters(CliFrontend.java:852)
        at eu.stratosphere.client.CliFrontend.main(CliFrontend.java:878)
```



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/881
Created by: [tobwiens|https://github.com/tobwiens]
Labels: 
Created at: Wed May 28 18:18:25 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398261,,,Mon Jun 09 13:04:55 UTC 2014,,,,,,,,,,"0|i1wkdz:",398388,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:04;github-import;[Date: Wed May 28 18:27:39 CEST 2014, Author: [tobwiens|https://github.com/tobwiens]]

When stratosphere is copied to G: and started there (start-local.bat) it can access all files on G:, but no other drive. ;;;","09/Jun/14 13:04;github-import;[Date: Wed May 28 18:34:22 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you debug the class 'LocalFilesystem' and see where the error happens?
I think a good place to start is the 'Filesystem.get()' method.;;;","09/Jun/14 13:04;github-import;[Date: Wed May 28 18:39:55 CEST 2014, Author: [zentol|https://github.com/zentol]]

you need another ``/`` in the file scheme.

this worked for me:
```
DataSet<String> points = env.readTextFile(""file:///F:/test.txt"");
```
this didnt:
```
DataSet<String> points = env.readTextFile(""file://F:/test.txt"");
```
;;;","09/Jun/14 13:04;github-import;[Date: Wed May 28 18:45:15 CEST 2014, Author: [zentol|https://github.com/zentol]]

the reason it worked when on the same drive is the following:
 because due to the missing ``/`` the resulting path is ``/test.txt``, for which the current drive letter is automatically added.;;;","09/Jun/14 13:04;github-import;[Date: Wed May 28 18:51:52 CEST 2014, Author: [zentol|https://github.com/zentol]]

The source of this occurrence is btw. the URI class:
```
URI t = new URI(""file://G:/test.txt"");
System.out.println(t.getScheme());
System.out.println(t.getPath());
```
prints
```
file
/test.txt
```;;;","09/Jun/14 13:04;github-import;[Date: Wed May 28 19:00:31 CEST 2014, Author: [tobwiens|https://github.com/tobwiens]]

That's exactly it. Another / has to be added when the file sits on another drive relative to stratosphere. 

It feels very confusing that, depending on where stratopshere runs the absolute file path need to be changed. 

The tripple notation ""file:///"" does also work if the file is on the same drive. ;;;","09/Jun/14 13:04;github-import;[Date: Wed May 28 19:07:01 CEST 2014, Author: [zentol|https://github.com/zentol]]

you look at it the wrong way. 
``file:///`` is the standard notation for windows systems afaik. 
``file://F:/test.txt`` working is more or less a coincidence.
``file:///test.txt`` behaves the same way as ``file://F:/test.txt``

in short, ``file://`` is straight up wrong.;;;","09/Jun/14 13:04;github-import;[Date: Wed May 28 19:14:07 CEST 2014, Author: [zentol|https://github.com/zentol]]

``It feels very confusing that, depending on where stratopshere runs the absolute file path need to be changed. ``

assigning the letter automatically prevents this for some cases. if you have a txt file inside the stratosphere distribution, and know that the top folder is the same on every node, you can specify ``file:///<path-without-drive-letter>`` and be sure that it runs on any machine, independent of the arbitrary drive letter.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Program with two BroadcastVars causes CompilerException,FLINK-880,12720061,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:04,08/Sep/14 15:53,14/Jul/23 05:57,08/Sep/14 15:53,,,,pre-apache,,,,,,,0,github-import,,"The following CompilerException is raised by

```
eu.stratosphere.compiler.CompilerException: 
No plan meeting the requirements could be created @ GroupReduce (eu.stratosphere.example.java.clustering.Prog$IdentityReduce) (1:null). 
Most likely reason: Too restrictive plan hints.
```

this program.

```java
public class Prog {

	@SuppressWarnings(""unchecked"")
	public static void main(String[] args) throws Exception {

		final ExecutionEnvironment env = ExecutionEnvironment
				.getExecutionEnvironment();

		DataSet<Integer> bc1 = env.fromElements(0);
		DataSet<Integer> bc2 = env.fromElements(0);

		DataSet<Tuple1<Integer>> data = env.fromElements(
				new Tuple1<Integer>(0), new Tuple1<Integer>(0));

		DataSet<Tuple1<Integer>> data2 = data
				.flatMap(new IdentityMap())
				.withBroadcastSet(bc1, ""BC1"");

		DataSet<Tuple1<Integer>> data3 = data2
				.groupBy(0)
				.reduceGroup(new IdentityReduce())
				.withBroadcastSet(bc1, ""BC1"")
				.withBroadcastSet(bc2, ""BC2"");

		data3
			.flatMap(new IdentityMap())
			.print();

		data2
			.flatMap(new IdentityMap())
			.print();

		env.setDegreeOfParallelism(1);
		env.execute(""CustomALS"");
	}

	public static final class IdentityMap extends
			FlatMapFunction<Tuple1<Integer>, Tuple1<Integer>> {
		@Override
		public void flatMap(Tuple1<Integer> value,
				Collector<Tuple1<Integer>> out) {
			out.collect(value);
		}
	}

	public static final class IdentityReduce extends
			GroupReduceFunction<Tuple1<Integer>, Tuple1<Integer>> {
		@Override
		public void reduce(Iterator<Tuple1<Integer>> values,
				Collector<Tuple1<Integer>> out) {
			while (values.hasNext()) {
				out.collect(values.next());
			}
		}
	}
}

```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/880
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, optimizer, 
Milestone: Release 0.5.1
Created at: Wed May 28 15:44:15 CEST 2014
State: open
",,fhueske,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398260,,,Mon Sep 08 15:53:56 UTC 2014,,,,,,,,,,"0|i1wkdr:",398387,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:04;github-import;[Date: Wed Jun 04 11:22:10 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

This commit probably fixed the issue: [28a3dfd723bf370d86c4a4127228058e0218ce51|https://github.com/stratosphere/stratosphere/commit/28a3dfd723bf370d86c4a4127228058e0218ce51]
We should turn this code into a test case to verify the fix.;;;","09/Jun/14 13:04;github-import;[Date: Wed Jun 04 11:28:34 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I have added a similar program as a compiler test.

Would not hurt to have that one as a runtime test well.;;;","09/Jun/14 13:04;github-import;[Date: Sun Jun 08 14:05:20 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Any updates?;;;","08/Sep/14 15:53;fhueske;The example program works with the current master.

So the issue was fixed somehow, sometime ago...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generating a JSON plan with Client leads to NPE,FLINK-878,12720059,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:04,09/Jun/14 13:04,14/Jul/23 05:57,09/Jun/14 13:04,,,,pre-apache,,,,,,,0,github-import,,"Calling ```./bin/stratosphere info ... -p``` with a new JAPI program leads to the following exception:

```java
Exception in thread ""main"" java.lang.NullPointerException
	at eu.stratosphere.compiler.plandump.PlanJSONDumpGenerator.dumpOptimizerPlanAsJSON(PlanJSONDumpGenerator.java:99)
	at eu.stratosphere.compiler.plandump.PlanJSONDumpGenerator.getOptimizerPlanAsJSON(PlanJSONDumpGenerator.java:93)
	at eu.stratosphere.client.program.Client.getOptimizedPlanAsJson(Client.java:112)
	at eu.stratosphere.client.CliFrontend.info(CliFrontend.java:492)
	at eu.stratosphere.client.CliFrontend.parseParameters(CliFrontend.java:856)
	at eu.stratosphere.client.CliFrontend.main(CliFrontend.java:878)
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/878
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Tue May 27 17:27:56 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398258,,,Mon Jun 09 13:04:40 UTC 2014,,,,,,,,,,"0|i1wkdb:",398385,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:04;github-import;[Date: Tue May 27 17:54:40 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

There is a fix already under test ;-);;;","09/Jun/14 13:04;github-import;[Date: Sun Jun 08 14:07:09 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed as part of [abe88b7972c24b85338033042b07cba64ca91054|https://github.com/stratosphere/stratosphere/commit/abe88b7972c24b85338033042b07cba64ca91054];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fixes toString return value of min aggregation,FLINK-874,12720055,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:04,09/Jun/14 13:04,14/Jul/23 05:57,09/Jun/14 13:04,,,,pre-apache,,,,,,,0,github-import,,"This is very minor but was enough to confuse me while trying to make sense of my logs today, so I fixed it :)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/874
Created by: [vasia|https://github.com/vasia]
Labels: 
Created at: Tue May 27 14:56:06 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:04;github-import;pull-request-874-1571852048750535912.patch;https://issues.apache.org/jira/secure/attachment/12649354/pull-request-874-1571852048750535912.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398254,,,Mon Jun 09 13:04:19 UTC 2014,,,,,,,,,,"0|i1wkcf:",398381,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:04;github-import;[Date: Tue May 27 15:35:18 CEST 2014, Author: [uce|https://github.com/uce]]

Indeed, this can be confusing. Good catch :) I'll merge it now.

Since this is a minor String change, you can add `[ci skip]` to the commit message in order to not have Travis build and test your commit (http://docs.travis-ci.com/user/how-to-skip-a-build/).;;;","09/Jun/14 13:04;github-import;[Date: Tue May 27 15:39:30 CEST 2014, Author: [uce|https://github.com/uce]]

Sorry @rmetzger, @StephanEwen, I regret not merging it manually (because of the extra merge commit). Won't happen again. :sob: ;;;","09/Jun/14 13:04;github-import;[Date: Tue May 27 16:46:59 CEST 2014, Author: [vasia|https://github.com/vasia]]

Great, thnx for the tip @uce!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KMeans Scala example does not work with Quick Start,FLINK-870,12720051,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:03,01/Oct/14 15:04,14/Jul/23 05:57,01/Oct/14 15:04,,,,pre-apache,,,,,,,0,github-import,,"The KMeans Scala and Java examples differ in the expected input format - the Scala program expects `(pointID, x, y, z)` as input whereas the Java program expects `(x, y)`, which is the format generated by the `KMeansDataGenerator`.

Consequently, the [Quick Start|http://stratosphere.eu/quickstart/example.html] seems to work only with the Java example.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/870
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Created at: Mon May 26 15:57:45 CEST 2014
State: open
",,fhueske,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398250,,,Wed Oct 01 15:04:22 UTC 2014,,,,,,,,,,"0|i1wkbj:",398377,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:03;github-import;[Date: Mon May 26 15:59:00 CEST 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

I'm working on a fix.;;;","09/Jun/14 13:03;github-import;[Date: Tue May 27 09:28:43 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Will you adapt the Scala version? 
I have also an updated version of the Python script which works for the Java example.;;;","09/Jun/14 13:03;github-import;[Date: Tue May 27 13:33:23 CEST 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

Yes, I adapted the Scala version in PR ([#873|https://github.com/stratosphere/stratosphere/issues/873] | [FLINK-873|https://issues.apache.org/jira/browse/FLINK-873]). I also have modified the Python script so one can plot the input (unclustered points with one color), as well as the output (clustered points, each cluster with distinct color):

```python
#!/usr/bin/python
import sys
import matplotlib.pyplot as plt
import csv
import os

if len(sys.argv) < 4 or not sys.argv[1] in ['points', 'result']:
  print ""Usage: plot-clusters.py (points|result) <src-file> <pdf-file-prefix>""
  sys.exit(1)

inFile = sys.argv[1]
inFile = sys.argv[2]
outFilePx = sys.argv[3]

inFileName = os.path.splitext(os.path.basename(inFile))[0]
outFile = os.path.join(""."", outFilePx+""-plot.pdf"")

########### READ DATA

cs = []
xs = []
ys = []

minX = None
maxX = None
minY = None
maxY = None

if sys.argv[1] == 'points':

  with open(inFile, 'rb') as file:
    for line in file:
      # parse data
      csvData = line.strip().split(' ')

      x = float(csvData[0])
      y = float(csvData[1])
    
      if not minX or minX > x:
        minX = x
      if not maxX or maxX < x:
        maxX = x
      if not minY or minY > y:
        minY = y
      if not maxY or maxY < y:
        maxY = y

      xs.append(x)
      ys.append(y)

    # plot data
    plt.clf()
    plt.scatter(xs, ys, s=25, c=""([#999999|https://github.com/stratosphere/stratosphere/issues/999999] | [FLINK-999999|https://issues.apache.org/jira/browse/FLINK-999999])"", edgecolors='None', alpha=1.0)
    plt.ylim([minY,maxY])
    plt.xlim([minX,maxX])

elif sys.argv[1] == 'result':

  with open(inFile, 'rb') as file:
    for line in file:
      # parse data
      csvData = line.strip().split(',')

      c = int(csvData[0])
      x = float(csvData[1])
      y = float(csvData[2])

      cs.append(c)
      xs.append(x)
      ys.append(y)

    # plot data
    plt.clf()
    plt.scatter(xs, ys, s=25, c=cs, edgecolors='None', alpha=1.0)
    plt.ylim([minY,maxY])
    plt.xlim([minX,maxX])


plt.savefig(outFile, dpi=600)
print ""\nPlotted file: %s"" % outFile

sys.exit(0)
```;;;","09/Jun/14 13:03;github-import;[Date: Mon Jun 02 15:56:55 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

@Alex: is the new python script able to handle the output of the `eu.stratosphere.example.java.clustering.util.KMeansDataGenerator` class? I'm getting this error

```
python2.7 plotPoints.py points input
Traceback (most recent call last):
  File ""plotPoints.py"", line 36, in <module>
    p = int(csvData[0])
ValueError: invalid literal for int() with base 10: '-9.53 -46.26'
```

I'm also getting an error in the web client with the KMeans example:
```
eu.stratosphere.client.program.ProgramInvocationException: The program plan could not be fetched. The program silently swallowed the control flow exceptions.
	at eu.stratosphere.client.program.Client.getOptimizedPlan(Client.java:154)
	at eu.stratosphere.client.web.JobSubmissionServlet.doGet(JobSubmissionServlet.java:161)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:532)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:453)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:965)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:388)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:187)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:901)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:113)
	at org.eclipse.jetty.server.Server.handle(Server.java:352)
	at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596)
	at org.eclipse.jetty.server.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:1048)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:549)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:211)
	at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:425)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:489)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:436)
	at java.lang.Thread.run(Thread.java:744)
```;;;","09/Jun/14 13:03;github-import;[Date: Tue Jun 03 11:24:28 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you @aalexandrov. I will update the python script on the website.;;;","01/Oct/14 15:04;fhueske;The new Scala KMeans example requires the same input format and produce the same output as the Java KMeans.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make aggregations combinable,FLINK-861,12720042,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:03,09/Jun/14 13:03,14/Jul/23 05:57,09/Jun/14 13:03,,,,pre-apache,,,,,,,0,github-import,,"Currently, the Aggregations are not combinable, although all functions can be.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/861
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Sun May 25 21:14:58 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398241,,,Mon Jun 09 13:03:11 UTC 2014,,,,,,,,,,"0|i1wk9j:",398368,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:03;github-import;[Date: Mon May 26 02:50:26 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

A possible fix is in ([#863|https://github.com/stratosphere/stratosphere/issues/863] | [FLINK-863|https://issues.apache.org/jira/browse/FLINK-863]) ;;;","09/Jun/14 13:03;github-import;[Date: Mon May 26 13:07:36 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed with [f834e7d3287f377798ad8b7d13058b5499cdf0db|https://github.com/stratosphere/stratosphere/commit/f834e7d3287f377798ad8b7d13058b5499cdf0db];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
groupBy operator followed by a reduceGroup does not group Writables correctly,FLINK-860,12720041,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:02,09/Jun/14 13:03,14/Jul/23 05:57,09/Jun/14 13:03,,,,pre-apache,,,,,,,0,github-import,,"I am not sure if it is a known issue, but If we consider the following ```DataSet<Tuple2<Text,LongWritable>>``` named ```words``` with the content:

allen 1
allein 1
allein 1
a 1
word 1
word 1
words 1

and apply ```words.groupBy(0).reduceGroup(someReduceFunction)``` (for example one that sums values)





outputs:

a 1
words 3
words 3

The same occurs when we specify a custom ```KeySelector``` that transforms the Text to a String (thw whole key as String). 

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/860
Created by: [atsikiridis|https://github.com/atsikiridis]
Labels: invalid, 
Created at: Sun May 25 19:00:28 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398240,,,Mon Jun 09 13:03:07 UTC 2014,,,,,,,,,,"0|i1wk9b:",398367,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:03;github-import;[Date: Sun May 25 20:59:42 CEST 2014, Author: [zentol|https://github.com/zentol]]

is the code you used available somewhere?;;;","09/Jun/14 13:03;github-import;[Date: Sun May 25 21:08:26 CEST 2014, Author: [atsikiridis|https://github.com/atsikiridis]]

Hello @zentol , https://github.com/atsikiridis/stratosphere/blob/HadoopCompatibilityJAPIReady/stratosphere-addons/hadoop-compatibility/src/main/java/eu/stratosphere/hadoopcompatibility/mapred/example/FullWordCount.java

You can try running this one from my branch. Thanks!;;;","09/Jun/14 13:03;github-import;[Date: Sun May 25 21:12:55 CEST 2014, Author: [atsikiridis|https://github.com/atsikiridis]]

Or actually the testcase HadoopFullJobITCase on the same branch. Should be more convenient;;;","09/Jun/14 13:03;github-import;[Date: Sun May 25 21:41:48 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Hi,

I wrote my own program (below) and could **not** reproduce the error.
Will check your code and see what's different.

```
public static void main(String[] args) throws Exception {
		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
		
		List<Tuple2<String, Long>> data = new ArrayList<Tuple2<String, Long>>();
		data.add(new Tuple2<String, Long>(new String(""allen""), new Long(1)));
		data.add(new Tuple2<String, Long>(new String(""allein""), new Long(1)));
		data.add(new Tuple2<String, Long>(new String(""allein""), new Long(1)));
		data.add(new Tuple2<String, Long>(new String(""a""), new Long(1)));
		data.add(new Tuple2<String, Long>(new String(""word""), new Long(1)));
		data.add(new Tuple2<String, Long>(new String(""word""), new Long(1)));
		data.add(new Tuple2<String, Long>(new String(""words""), new Long(1)));
	
		DataSet<Tuple2<String, Long>> myData = env.fromCollection(data);
		
		DataSet<Tuple2<Text, LongWritable>> res = myData
				.map(new MapFunction<Tuple2<String, Long>, Tuple2<Text, LongWritable>>() {

					@Override
					public Tuple2<Text, LongWritable> map(
							Tuple2<String, Long> value) throws Exception {
						return new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));
					}
				})
				.groupBy(0)
				.reduceGroup(
					new GroupReduceFunction<Tuple2<Text,LongWritable>, Tuple2<Text,LongWritable>>() {

						@Override
						public void reduce(
								Iterator<Tuple2<Text, LongWritable>> values, Collector<Tuple2<Text, LongWritable>> out) {

							long sum = 0;
							Tuple2<Text,LongWritable> v = null;
							while(values.hasNext()) {
								v = values.next();
								sum += v.f1.get();
							}
							v.f1 = new LongWritable(sum);
							out.collect(v);
						}
					}
				);
		res.print();
		env.execute();
	}
```;;;","09/Jun/14 13:03;github-import;[Date: Sun May 25 22:04:37 CEST 2014, Author: [atsikiridis|https://github.com/atsikiridis]]

Hello @fhueske thanks for testing. Maybe this is only when I use generics (in my example) plus I subclass ```HadoopMapFunction```, ```HadoopReduceFunction``` ? I do that in order avoid type erasure issues and the ```TypeExtractor``` complaining (([#845|https://github.com/stratosphere/stratosphere/issues/845] | [FLINK-845|https://issues.apache.org/jira/browse/FLINK-845]) and consequences) .;;;","09/Jun/14 13:03;github-import;[Date: Sun May 25 22:19:06 CEST 2014, Author: [zentol|https://github.com/zentol]]

i doubt it has to do with generics. 
i placed system.outs in the ``ReducerTransformingIterator.next()`` and ``HadoopOutputCollector.collect()`` methods.

the following is an exempt: (ignore the different formatting)
```
iterator: hoechste 1
collector (hoechste, 1)
iterator: hohe 1
collector (hohe, 1)
iterator: hohen 1
iterator: (hohen, 1)
collector (holden, 2)
iterator: holden 1
collector (holden, 1)
```
it seems that sometimes the key is changed between the call to next and collect.;;;","09/Jun/14 13:03;github-import;[Date: Sun May 25 22:40:08 CEST 2014, Author: [atsikiridis|https://github.com/atsikiridis]]

Hi @zentol ,well there is definitely something wrong with the ```ReducerTransformingIterator``` and I think I have misundestood something  about groupBy. Was a bit in a rush to create this issue :-) Thanks for checking everybody.;;;","09/Jun/14 13:03;github-import;[Date: Sun May 25 22:42:29 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I know what the problem is, as usual the mutable objects returned by the reduce iterator. 
So this is a bug in the HadoopReduceFunction and nothing that blocks the release.

I'll share the details later this evening.;;;","09/Jun/14 13:03;github-import;[Date: Sun May 25 22:45:48 CEST 2014, Author: [uce|https://github.com/uce]]

+1 to Stephan's suggestion (outside this discussion... I'll edit this message if I find a reference) to return new objects per default and reuse objects as an ""advanced"" option.

@fhueske You mean later this night? ;-);;;","09/Jun/14 13:03;github-import;[Date: Mon May 26 00:29:34 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Alright ;-) The problem is caused by the iterator that is given to the GroupReduceFunction. This iterator uses two mutable objects and continuously overrides their state on hasNext() calls. In this specific case there are two Tuple2 objects (A and B) which are returned one after the other by the iterator. If next() returns object A, the consecutive hasNext() call overwrites object B by the next element in the iterator. The following next() call returns B and hasNext() overwrites A, and so on. Also the fields of the tuples are reused. Therefore, there are also only two Text objects used during processing.

So what is happening here? At the start of a new group, the ReducerTransformingIterator reads the first tuple and extracts the key. The reference to this Text object is given to the Hadoop Reduce function and used there during the complete reduce() call. While processing the group, every second call of hasNext() on the iterator overwrites the state of the Key object. Since the key is always the same within a group this does not matter. However, the last call of hasNext() loads the first element of the next group in the current mutable object (which is hold back for the next group). Depending on the number of elements in the group (even or odd) the mutable object which is overwritten was the one from which the initial key was extracted and therefore the key object is also overwritten with the key value of the next group.

So a safe and straight-forward way to handle this is to copy the key value into a separate (mutable) object which is given to the Hadoop Reduce function.;;;","09/Jun/14 13:03;github-import;[Date: Mon May 26 03:48:47 CEST 2014, Author: [atsikiridis|https://github.com/atsikiridis]]

@fhueske , thanks a bunch for the detailed explanation. Aaa ```hasNext() ``` is called by my own example in Hadoop and of course, mutable types. Confused me and started blaming the grouping. Thanks :) I'm definitely writing unit tests for this now...;;;","09/Jun/14 13:03;github-import;[Date: Mon May 26 08:36:33 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I think we should have this explanation and a warning in the Javadocs here: http://stratosphere-javadocs.github.io/eu/stratosphere/api/java/functions/GroupReduceFunction.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix hash partitioning for new java api,FLINK-859,12720040,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:02,09/Jun/14 13:02,14/Jul/23 05:57,09/Jun/14 13:02,,,,pre-apache,,,,,,,0,github-import,,"This is a fix for this issue https://github.com/stratosphere/stratosphere/issues/857

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/859
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, runtime, 
Milestone: Release 0.5
Created at: Sun May 25 15:37:14 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:02;github-import;pull-request-859-5040683184356647995.patch;https://issues.apache.org/jira/secure/attachment/12649346/pull-request-859-5040683184356647995.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398239,,,Mon Jun 09 13:02:56 UTC 2014,,,,,,,,,,"0|i1wk93:",398366,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:02;github-import;[Date: Sun May 25 15:44:02 CEST 2014, Author: [uce|https://github.com/uce]]

Can you infer the value which gets hashed to Integer.MIN_VALUE and add a test to `OutputEmitterTest`?;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 15:44:25 CEST 2014, Author: [uce|https://github.com/uce]]

And the problem should also apply to `RecordOutputEmitter`, no?;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 16:45:48 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks, good. Please add the same thing to the RecordOutputEmitter and merge soon.

I order to test it: Right now, the OutputEmitter uses the hash value returned by the TypeComparator. So you can use a custom type comparator that returns Integer.MIN_VALUE for the test.;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 18:17:55 CEST 2014, Author: [uce|https://github.com/uce]]

I wrote before thinking... even better if we can just use a custom Comparator.

@rmetzger Please introduce mockito as a dependency; I will be doing that with an upcoming PR anyways.

```xml
<dependency>
    <groupId>org.mockito</groupId>
    <artifactId>mockito-all</artifactId>
    <version>1.9.5</version>
    <scope>test</scope>
</dependency>
```

Then you can mock the TypeComparator easily:

```java
TypeComparator<Record> comparatorMock = mock(TypeComparator.class);
when(comparatorMock.hash(Matchers.<Record>anyObject()))
    .thenReturn(Integer.MIN_VALUE);

int hash = comparatorMock.hash(new Record(new IntValue(1))); // Integer.MIN_VALUE
```;;;","09/Jun/14 13:02;github-import;[Date: Mon May 26 10:44:07 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

@rmetzger should I continue to fix this or do you want to do it?;;;","09/Jun/14 13:02;github-import;[Date: Mon May 26 10:59:31 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Yeah, I will not have much time in the next two days to finish the fix.
Ufuk also offered to finish the PR. So you should synchronize.


Sent from my iPhone

On 26.05.2014, at 10:50, Fabian Hueske <notifications@github.com> wrote:

@rmetzger <https://github.com/rmetzger> should I continue to fix this or do
you want to do it?

—
Reply to this email directly or view it on
GitHub<https://github.com/stratosphere/stratosphere/pull/859#issuecomment-44169178>
.;;;","09/Jun/14 13:02;github-import;[Date: Mon May 26 11:27:18 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

@uce I'm done with this, tests are currently running.;;;","09/Jun/14 13:02;github-import;[Date: Mon May 26 11:30:41 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Cool. I think I'm pushing the next RC later today.

Sent from my iPhone

On 26.05.2014, at 11:27, Fabian Hueske <notifications@github.com> wrote:

@uce <https://github.com/uce> I'm done with this, tests are currently
running.

—
Reply to this email directly or view it on
GitHub<https://github.com/stratosphere/stratosphere/pull/859#issuecomment-44172413>
.;;;","09/Jun/14 13:02;github-import;[Date: Mon May 26 11:31:50 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

:+1:;;;","09/Jun/14 13:02;github-import;[Date: Mon May 26 11:49:43 CEST 2014, Author: [uce|https://github.com/uce]]

@fhueske Nice. Let's close this PR and merge your fix with a new PR later.;;;","09/Jun/14 13:02;github-import;[Date: Mon May 26 13:07:53 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PlanJSONDumpGenerator.formatNumber()'s punctuation is broken,FLINK-858,12720039,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,github-import,github-import,09/Jun/14 13:02,13/Jun/14 00:08,14/Jul/23 05:57,13/Jun/14 00:08,,,,pre-apache,,,,,,,0,github-import,,"A minor issue I found while testing for the 0.5 release.

In the web frontend, it says ""Cumulative Disk I/O:   1,.04.29 TB""
There is a comma and a dot after the 1.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/858
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, gui, simple-issue, 
Created at: Sat May 24 22:16:29 CEST 2014
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398238,,,Fri Jun 13 00:08:06 UTC 2014,,,,,,,,,,"0|i1wk8v:",398365,,,,,,,,,,,,,,,,,,,,"13/Jun/14 00:08;sewen;Fixed through commit 6c6fc897a284c27dc904de98039814ec786e9fa0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception while executing wordcount example,FLINK-857,12720038,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:02,09/Jun/14 13:02,14/Jul/23 05:57,09/Jun/14 13:02,,,,pre-apache,,,,,,,0,github-import,,"Hi,
while testing the current `master` branch to see if we can release it this way, I found an issue with the new Java API / Wordcount example.

```
05/24/2014 21:56:32:	Job execution switched to status FAILED
Error: The program execution failed: java.lang.ArrayIndexOutOfBoundsException: -128
	at java.util.ArrayList.elementData(ArrayList.java:400)
	at java.util.ArrayList.get(ArrayList.java:413)
	at eu.stratosphere.nephele.io.RuntimeOutputGate.writeRecord(RuntimeOutputGate.java:268)
	at eu.stratosphere.nephele.io.AbstractRecordWriter.emit(AbstractRecordWriter.java:92)
	at eu.stratosphere.pact.runtime.shipping.OutputCollector.collect(OutputCollector.java:82)
	at eu.stratosphere.example.java.wordcount.WordCount$Tokenizer.flatMap(WordCount.java:101)
	at eu.stratosphere.example.java.wordcount.WordCount$Tokenizer.flatMap(WordCount.java:91)
	at eu.stratosphere.pact.runtime.task.chaining.ChainedFlatMapDriver.collect(ChainedFlatMapDriver.java:71)
	at eu.stratosphere.pact.runtime.task.DataSourceTask.invoke(DataSourceTask.java:219)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:351)
	at java.lang.Thread.run(Thread.java:745)

For a more detailed error message use the vebose output option '-v'.
```

I successfully executed the Scala and Record Wordcount implementations with the same configuration (cluster size, data set).
The error is reproducible.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/857
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, java api, runtime, 
Milestone: Release 0.5
Created at: Sat May 24 22:05:29 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398237,,,Mon Jun 09 13:02:44 UTC 2014,,,,,,,,,,"0|i1wk8n:",398364,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:02;github-import;[Date: Sat May 24 23:01:40 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Bug in the hash partitioner.;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 12:37:44 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

The bug is caused by a the following problem: 
`Integer.MIN_VALUE == -Integer.MIN_VALUE`. Also, `Math.abs(Integer.MIN_VALUE) = Integer.MIN_VALUE`.
See also here: http://stackoverflow.com/questions/18565485/why-is-absolute-of-integer-min-value-equivalent-to-integer-min-value

The error only occurs if the hash value is accidentally `Integer.MIN_VALUE`.

The RecordOutputEmitter is doing some hash-salting magic:
```java
for (int i = 0; i < DEFAULT_SALT.length; i++) {
  hash ^= ((hash << 5) + DEFAULT_SALT[i] + (hash >> 2));
}
```
I don't know if this is preventing this case. 
How can we fix this?;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 12:48:56 CEST 2014, Author: [uce|https://github.com/uce]]

Good catch!

The easiest thing will be to test for MIN_VALUE?;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 12:50:47 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Yeah, this will cause a slight skew in the data. Also I'm not sure about the performance impact. This is on the ""critical path"".;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 12:51:16 CEST 2014, Author: [uce|https://github.com/uce]]

PS: This will have an impact on performance, but is there any other way to do it with less skew?;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 13:39:31 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I tested two variants to solve this issue
```java
	public static void main(String[] args) {
		int numberOfChannels = 208;
		int a;
		
		long start = System.currentTimeMillis();
		int c;
		for(int i = Integer.MIN_VALUE; i < Integer.MAX_VALUE; i++) {
			c = ((i % numberOfChannels) + numberOfChannels)  % numberOfChannels;
			if(c < 0 ) {
				System.err.println(""wrong"");
			}
		}
		System.err.println(""First test ""+ (System.currentTimeMillis() - start) );
		
		long start2 = System.currentTimeMillis();
		for(int i = Integer.MIN_VALUE; i < Integer.MAX_VALUE; i++) {
			if(i < 0) {
				if(i == Integer.MIN_VALUE) {
					a = Integer.MAX_VALUE % numberOfChannels;
				} else {
					a = -i % numberOfChannels;
				}
			} else {
				a= i % numberOfChannels;
			}
			if(a < 0 ) {
				System.err.println(""wrong"");
			}
		}
		
		System.err.println(""Second test ""+ (System.currentTimeMillis() - start2) );
	}
```

So for `((i % numberOfChannels) + numberOfChannels)  % numberOfChannels;`
the runtime is 33980

for the `if if else else` variant, I have 18505.
I ran the tests with Java 8.
 ;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 15:32:55 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Something that branches, but follows the same branch almost always should
be faster than modulo, if I recall correctly. Divisions are expensive,
unless they are dividing by powers of two.;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 15:37:03 CEST 2014, Author: [uce|https://github.com/uce]]

I agree. So let's special case the MIN_VALUE and increment the hash in that case.

@rmetzger Do you want to do it or should I? Let's make sure to include a small test.

New RC after the PR?;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 21:21:30 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I would like to see how https://github.com/stratosphere/stratosphere/issues/860 develops. I will not create a new RC until all known issues are addressed.;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 21:33:53 CEST 2014, Author: [uce|https://github.com/uce]]

At the time the issue was not known ;-);;;","09/Jun/14 13:02;github-import;[Date: Mon May 26 13:08:37 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed with [8677ad9ad03cfaa37e654ebd8bd868928fbddff2|https://github.com/stratosphere/stratosphere/commit/8677ad9ad03cfaa37e654ebd8bd868928fbddff2] and [c9589b82d671e0afe76affd01c2e2c71b151a667|https://github.com/stratosphere/stratosphere/commit/c9589b82d671e0afe76affd01c2e2c71b151a667];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve exception for nonexistent local files,FLINK-856,12720037,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:02,09/Jun/14 13:02,14/Jul/23 05:57,09/Jun/14 13:02,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/856
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Sat May 24 20:47:03 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:02;github-import;pull-request-856-6881386888982379660.patch;https://issues.apache.org/jira/secure/attachment/12649345/pull-request-856-6881386888982379660.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398236,,,Mon Jun 09 13:02:34 UTC 2014,,,,,,,,,,"0|i1wk8f:",398363,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:02;github-import;[Date: Sat May 31 10:36:47 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged in [7ad6c8baee710254608c6f4a929cdcc0ba34c7d1|https://github.com/stratosphere/stratosphere/commit/7ad6c8baee710254608c6f4a929cdcc0ba34c7d1].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dokumentation improvement,FLINK-851,12720032,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:02,09/Jun/14 13:02,14/Jul/23 05:57,09/Jun/14 13:02,,,,pre-apache,,,,,,,0,github-import,,"Create a short tutorial for intelliJ users in Readme.md file.
For example: how to run the WordCount example (locally) from the IDE in less then 5 minutes after pulling the latest stratosphere git repo.
=> Encourage the usage of IDE's and local-execution for newbies.

My Idea:
1) File->Import Project->From External Model->Maven->Check all 6 checkboxes->NextNextNext ...
2) Collapse stratosphere-java-examples->eu.stratosphere.example.java.wordcount.WordCount.java -> RightClick on the file -> Click ""Run WordCount.main() -> Enjoy
3) Next: show where to put new projects and data.txt files
(BTW: Great job for making the examples run on a mouse click with default params ! )

Also:
To the readers of [Quickstart|http://stratosphere.eu/quickstart/setup.html#example]:
Ok its great to run the WordCount.jar from the CLI but what next? How can the user analyze/modify WordCount.java or maybe write a new CustomJob.java project?
A side-note there would be great.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/851
Created by: [oresti|https://github.com/oresti]
Labels: 
Created at: Fri May 23 14:16:45 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398231,,,Mon Jun 09 13:02:14 UTC 2014,,,,,,,,,,"0|i1wk7b:",398358,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:02;github-import;[Date: Fri May 23 14:23:31 CEST 2014, Author: [oresti|https://github.com/oresti]]

I just saw that you do have a doku in the Quickstart for that in [here|http://stratosphere.eu/quickstart/java.html]

;;;","09/Jun/14 13:02;github-import;[Date: Fri May 23 17:02:37 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hey,
thanks for creating the issue. 
Since you're obviously tried out Stratosphere recently, do you have any further suggestions on how to improve the overall experience, especially for new users?
I don't know how we can make it more obvious that it is really easy to run Stratosphere from an IDE.;;;","09/Jun/14 13:02;github-import;[Date: Fri May 23 18:16:33 CEST 2014, Author: [oresti|https://github.com/oresti]]

Hi,
for the quick-start introduction I do not have any more suggestions and I am really amazed about how easy it is to run (and work on) Stratosphere.

Maybe a wikipedia article as an additional reference point?
Also: try to point out the obvious fact that ""Stratosphere is much easier/better that MR"" as much as possible :)

Keep up the good work!;;;","09/Jun/14 13:02;github-import;[Date: Fri May 23 18:20:59 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Cool. Thanks for the kind words.

Somebody has actually drafted a Wikipedia article. I'm going to push that
again once we have the new name in the Apache Incubator.

Sent from my iPhone

On 23.05.2014, at 18:16, oresti <notifications@github.com> wrote:

Hi,
for the quick-start introduction I do not have any more suggestions and I
am really amazed about how easy it is to run (and work on) Stratosphere.

Maybe a wikipedia article as an additional reference point?
Also: try to point out the obvious fact that ""Stratosphere is much
easier/better that MR"" as much as possible :)

Keep up the good work!

—
Reply to this email directly or view it on
GitHub<https://github.com/stratosphere/stratosphere/issues/851#issuecomment-44031143>
.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add rCF() call to ContextEnvironment,FLINK-850,12720031,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:02,09/Jun/14 13:02,14/Jul/23 05:57,09/Jun/14 13:02,,,,pre-apache,,,,,,,0,github-import,,"Addendum to ([#728|https://github.com/stratosphere/stratosphere/issues/728] | [FLINK-728|https://issues.apache.org/jira/browse/FLINK-728]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/850
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Thu May 22 18:45:59 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:02;github-import;pull-request-850-2993924954912434557.patch;https://issues.apache.org/jira/secure/attachment/12649344/pull-request-850-2993924954912434557.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398230,,,Mon Jun 09 13:02:09 UTC 2014,,,,,,,,,,"0|i1wk73:",398357,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:02;github-import;[Date: Sun May 25 15:28:47 CEST 2014, Author: [uce|https://github.com/uce]]

LGTM.

Your branch name is `dist-cache-fix`. What was the problem for the distributed cache here?;;;","09/Jun/14 13:02;github-import;[Date: Sun May 25 15:50:59 CEST 2014, Author: [zentol|https://github.com/zentol]]

if you use the distributed cache via ``ExecutionEnvironment.registerCachedFile()`` it straight up doesn't work in an actual setting. When you submit a plan to stratosphere it seems to use ``ContextEnvironment``, which until now never actually registered the files.

(Note that the version that ([#728|https://github.com/stratosphere/stratosphere/issues/728] | [FLINK-728|https://issues.apache.org/jira/browse/FLINK-728]) was supposed to replace does work)
```
Plan p = env.CreateProgramPlan();
p.registerCachedFile(path, ""name"");
PlanExecutor l = new Local-/RemoteExecutor();
l.executePlan(p);
```;;;","09/Jun/14 13:02;github-import;[Date: Sat May 31 11:37:03 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

So this is a bugfix that we should also add to 0.5.1 right?;;;","09/Jun/14 13:02;github-import;[Date: Sat May 31 11:37:32 CEST 2014, Author: [zentol|https://github.com/zentol]]

yup;;;","09/Jun/14 13:02;github-import;[Date: Fri Jun 06 15:56:08 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The fix has been subsumed by [abe88b7972c24b85338033042b07cba64ca91054|https://github.com/stratosphere/stratosphere/commit/abe88b7972c24b85338033042b07cba64ca91054]

The above commit registers files in the `createProgramPlan()` method, which is the common point shared by all environments, etc.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parallelism parameter is not evaluated by CLI client in INFO action,FLINK-847,12720028,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:01,09/Jun/14 13:01,14/Jul/23 05:57,09/Jun/14 13:01,,,,pre-apache,,,,,,,0,github-import,,"The parallelism parameter (-p) is not evaluated if used in the info action together with the execution plan parameter (-e).



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/847
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, simple-issue, 
Created at: Thu May 22 00:46:15 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398227,,,Mon Jun 09 13:01:56 UTC 2014,,,,,,,,,,"0|i1wk6f:",398354,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:01;github-import;[Date: Sun May 25 20:05:41 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

This bug is not CLI Frontend related. The environment parallelism is not evaluated at all for Optimizer Plan command in the new Java API. I have a fix for that...;;;","09/Jun/14 13:01;github-import;[Date: Mon May 26 02:11:24 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [e5f617d1557414456db1f00e7d5adeaded749870|https://github.com/stratosphere/stratosphere/commit/e5f617d1557414456db1f00e7d5adeaded749870];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes an issue with wrong ClassLoader when submitting JobJars.,FLINK-844,12720025,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:01,09/Jun/14 13:01,14/Jul/23 05:57,09/Jun/14 13:01,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/844
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Wed May 21 16:51:22 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:01;github-import;pull-request-844-2587126441236905771.patch;https://issues.apache.org/jira/secure/attachment/12649343/pull-request-844-2587126441236905771.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398224,,,Mon Jun 09 13:01:45 UTC 2014,,,,,,,,,,"0|i1wk5r:",398351,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:01;github-import;[Date: Wed May 21 23:22:16 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [9c06e8cc25062bdf4dcb035bfeee6e594fcf7642|https://github.com/stratosphere/stratosphere/commit/9c06e8cc25062bdf4dcb035bfeee6e594fcf7642];;;","09/Jun/14 13:01;github-import;[Date: Wed May 21 23:23:14 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [9c06e8cc25062bdf4dcb035bfeee6e594fcf7642|https://github.com/stratosphere/stratosphere/commit/9c06e8cc25062bdf4dcb035bfeee6e594fcf7642];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove all System.exit() calls from example programs,FLINK-843,12720024,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:01,09/Jun/14 13:01,14/Jul/23 05:57,09/Jun/14 13:01,,,,pre-apache,,,,,,,0,github-import,,"This kills the CLI client and the WebClient. The firstone ungracefully, for the second one, the WebServer simply goes down. In general, there should never by System.exit() calls anywhere in the code, except for the main executables: JobManager, TaskManager, YarnAppMaster, CliFrontend, WebFrontend.

This is a release blocker in my opinion.

We should think about security managers that prevent calling this method from example programs.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/843
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: java api, 
Milestone: Release 0.5
Assignee: [fhueske|https://github.com/fhueske]
Created at: Wed May 21 14:57:51 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398223,,,Mon Jun 09 13:01:40 UTC 2014,,,,,,,,,,"0|i1wk5j:",398350,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:01;github-import;[Date: Wed May 21 15:04:11 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

eh, shame on me.

Will do that.;;;","09/Jun/14 13:01;github-import;[Date: Thu May 22 00:49:55 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Has been fixed in [8dc3ac4f1cb6a4f53b2fd0be0d7ff35a20cb574d|https://github.com/stratosphere/stratosphere/commit/8dc3ac4f1cb6a4f53b2fd0be0d7ff35a20cb574d] and [7fa8e25c9c6d6be1abb12a376ac9a713fbefe367|https://github.com/stratosphere/stratosphere/commit/7fa8e25c9c6d6be1abb12a376ac9a713fbefe367];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extending a FlatMapFunction with generics leads to a TypeExtractor error,FLINK-842,12720023,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:01,09/Jun/14 13:01,14/Jul/23 05:57,09/Jun/14 13:01,,,,pre-apache,,,,,,,0,github-import,,"When trying to use a ```DummyFlatMapFunction``` that extends ```FlatMapFunction``` with generics I get a type extractor error.

To replicate this consider a mapper with the following signature:

```
public class DummyFlatMapFunction<A,B,C,D> extends FlatMapFunction<Tuple2<A,B>, Tuple2<C,D>> implements Serializable
```

and then try using in the following way:

```
DataSet<Tuple2<Text, LongWritable>> words = text.flatMap(new DummyFlatMapFunction<LongWritable, Text, Text, LongWritable>());
```

This outputs the following stack of exceptions when executed:


```
sun.reflect.generics.reflectiveObjects.ParameterizedTypeImpl cannot be cast to java.lang.reflect.TypeVariable
java.lang.ClassCastException: sun.reflect.generics.reflectiveObjects.ParameterizedTypeImpl cannot be cast to java.lang.reflect.TypeVariable
    at eu.stratosphere.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy(TypeExtractor.java:415)
    at eu.stratosphere.api.java.typeutils.TypeExtractor.createTypeInfo(TypeExtractor.java:117)
    at eu.stratosphere.api.java.typeutils.TypeExtractor.getFlatMapReturnTypes(TypeExtractor.java:48)
    at eu.stratosphere.api.java.operators.FlatMapOperator.<init>(FlatMapOperator.java:34)
    at eu.stratosphere.api.java.DataSet.flatMap(DataSet.java:141)
```

The way I understand it is that the ```TypeExtractor``` does not support this with generics. In the examples of the new Java API all extended ```FlatMapFunction``` are with concrete types and not generics. However,  in ```ConnectedComponents``` the mapper that extends the MapFunction (not ```FlatMapFunction```) is with generics and apparently works.

Is this behaviour expected and generics are not allowed with ```FlatMapFunction``` or am I missing something? Thanks.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/842
Created by: [atsikiridis|https://github.com/atsikiridis]
Labels: bug, java api, question, 
Assignee: [twalthr|https://github.com/twalthr]
Created at: Wed May 21 11:24:14 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398222,,,Mon Jun 09 13:01:35 UTC 2014,,,,,,,,,,"0|i1wk5b:",398349,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:01;github-import;[Date: Wed May 21 12:10:31 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

It seems you are getting an exception while the exception message is being created. This is a bug, I will fix it. However, the intended message should be ""Type of TypeVariable XYZ could not be determined. This is most likely a type erasure problem.""

This is a very common issue with the TypeExtractor, but we can not prevent this Exception. Due to type erasure, Java throws all generic parameters away, if you use:

```java
new Function<ClassA,ClassB>();
```

The parameters stay only present if you extend from it:

```java
new Function<ClassA,ClassB>() { };
```

If you would use a 
```class PythonIdentityMapper<IN> extends MapFunction<IN,IN>```
then the TypeExtractor recognizes the same variable (name) and use the Input type as output type.;;;","09/Jun/14 13:01;github-import;[Date: Thu May 22 01:50:06 CEST 2014, Author: [atsikiridis|https://github.com/atsikiridis]]

Hello, Thank you very much for the input. ;;;","09/Jun/14 13:01;github-import;[Date: Thu May 22 10:28:56 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

So I guess we need to wait for the ResultTypeQueryable interface (([#845|https://github.com/stratosphere/stratosphere/issues/845] | [FLINK-845|https://issues.apache.org/jira/browse/FLINK-845])) to have a proper solution.
In the meantime, you could start to implement the interface for the HadoopFunction Wrappers.
It won't work for now, but once ([#845|https://github.com/stratosphere/stratosphere/issues/845] | [FLINK-845|https://issues.apache.org/jira/browse/FLINK-845]) is fixed, you can check if it is working.;;;","09/Jun/14 13:01;github-import;[Date: Thu May 22 10:31:40 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I close this issue as it is a consequence of ([#845|https://github.com/stratosphere/stratosphere/issues/845] | [FLINK-845|https://issues.apache.org/jira/browse/FLINK-845]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Annotation fix,FLINK-840,12720021,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:01,09/Jun/14 13:01,14/Jul/23 05:57,09/Jun/14 13:01,,,,pre-apache,,,,,,,0,github-import,,"This fixes the issues with the annotation syntax.
The delimiter is "";"".
The following is now possible:

""1,2,3""
""1->2,3;4,5""

I will also update the documentation.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/840
Created by: [skunert|https://github.com/skunert]
Labels: 
Created at: Tue May 20 21:37:18 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:01;github-import;pull-request-840-8869376830993358022.patch;https://issues.apache.org/jira/secure/attachment/12649342/pull-request-840-8869376830993358022.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398220,,,Mon Jun 09 13:01:28 UTC 2014,,,,,,,,,,"0|i1wk4v:",398347,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:01;github-import;[Date: Wed May 21 23:22:45 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [505119b68db9050eabb80516e35fb1f0bba29eb0|https://github.com/stratosphere/stratosphere/commit/505119b68db9050eabb80516e35fb1f0bba29eb0];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration of the CachedBuildSideMatchDriver into the optimizer,FLINK-836,12720017,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mholzemer,github-import,github-import,09/Jun/14 13:01,09/Jul/14 10:32,14/Jul/23 05:57,09/Jul/14 10:32,,,,0.6-incubating,,,,,,,0,github-import,,"I have redone the integration of the cached match driver. It is now integrated into the optimizer instead of just switching the DriverStrategy in the NepheleJobGraphGenerator. I feel it is much cleaner this way.
So this pull request replaces ([#822|https://github.com/stratosphere/stratosphere/issues/822] | [FLINK-822|https://issues.apache.org/jira/browse/FLINK-822]). 
Instead of extending the cost model like proposed in ([#795|https://github.com/stratosphere/stratosphere/issues/795] | [FLINK-795|https://issues.apache.org/jira/browse/FLINK-795]) the cost estimation function get passed the number of iterations, so that you can decide for every operator on its own how to incorporate iterations.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/836
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon May 19 15:48:05 CEST 2014
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:01;github-import;pull-request-836-6275630658372067958.patch;https://issues.apache.org/jira/secure/attachment/12649341/pull-request-836-6275630658372067958.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398216,,,Wed Jul 09 10:32:27 UTC 2014,,,,,,,,,,"0|i1wk3z:",398343,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:01;github-import;[Date: Mon May 19 16:50:19 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

This pull request has not a single test. We cannot merge it like this.
Please make a list of the tests you want to add and how they cover the code.;;;","09/Jun/14 13:01;github-import;[Date: Mon May 19 19:33:17 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I was thinking about tests before I created the pull request but in my opinion it is quite hard to write good compiler tests (perhaps that is also the reason why we have such few).
I could verify that an (by hint) enforced hash join inside of an iteration is correctly replaced, but that would only cover a very small part of the optimizer and even if there is a fault in my code it will most probably not discover it.
An other possibility I was thinking about is constructing a very specific example plan (including manually created statistics), but such an test would only work as long as the cost calculation of all operators is not changed and no new operators are added.;;;","09/Jun/14 13:01;github-import;[Date: Tue May 20 11:23:43 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I added testcases for my changes.
I also found a way to address the break/continue issue. The testcase ""testCorrectChoosing"" fails when there is a break. Because both hash left and hash right always are compatible the break causes that the right variant is never instantiated.;;;","09/Jul/14 10:32;sewen;Fixed via 47a68adc7fa5da7743cb51fe69f81264e908e630;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix binary distribution: add LICENSE and NOTICE,FLINK-835,12720016,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:00,09/Jun/14 13:01,14/Jul/23 05:57,09/Jun/14 13:01,,,,pre-apache,,,,,,,0,github-import,,"source should now build without the presence of a .git directory
included the quickstart-archetypes into the main project.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/835
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon May 19 15:47:02 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:00;github-import;pull-request-835-1462456152469226208.patch;https://issues.apache.org/jira/secure/attachment/12649340/pull-request-835-1462456152469226208.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398215,,,Mon Jun 09 13:01:00 UTC 2014,,,,,,,,,,"0|i1wk3r:",398342,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:00;github-import;[Date: Mon May 19 15:51:44 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I was actually told to push this directly to master, but I'd like to have some feedback on the Job skeleton for the new Java API. 
https://github.com/rmetzger/stratosphere/blob/releasefixes/stratosphere-quickstart/quickstart-java/src/main/resources/archetype-resources/src/main/java/Job.java
;;;","09/Jun/14 13:00;github-import;[Date: Mon May 19 18:08:20 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I like it. The skeleton for the Java Program is very good in my opinion.

I was thinking that we could even simplify the WordCountExample. The current one is the example one that switches between local data and file inputs. We could have the quickstart example even specialize on one of the two.

@fhueske What do you think?;;;","09/Jun/14 13:00;github-import;[Date: Tue May 20 00:09:58 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

@StephanEwen Jep, I agree. I would go for the CollectionDS version of WordCount such that it can be directly executed. 
I would also add a link to the examples page on the website and point to the examples code in the repository. 

Can we also have such a nice skeleton for the Scala quickstart?;;;","09/Jun/14 13:00;github-import;[Date: Tue May 20 09:48:32 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay, I added the links and changed the wordcount.;;;","09/Jun/14 13:00;github-import;[Date: Tue May 20 10:04:02 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for the feedback @fhueske. I addressed all of your comments.;;;","09/Jun/14 13:00;github-import;[Date: Tue May 20 10:15:58 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Looks good to me :+1:;;;","09/Jun/14 13:00;github-import;[Date: Tue May 20 10:16:56 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Ok. WIll merge.


On Tue, May 20, 2014 at 10:15 AM, Fabian Hueske <notifications@github.com>wrote:

> Looks good to me [image: :+1:]
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/835#issuecomment-43597879>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","09/Jun/14 13:01;github-import;[Date: Tue May 20 10:17:40 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged in [656022012916b604a3bf955f0a8514ebe3b8d449|https://github.com/stratosphere/stratosphere/commit/656022012916b604a3bf955f0a8514ebe3b8d449].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Semantic Annotations do not accept simple constant sets,FLINK-832,12720013,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:00,09/Jun/14 13:00,14/Jul/23 05:57,09/Jun/14 13:00,,,,pre-apache,,,,,,,0,github-import,,"The Semantic Annotations on a single input function do not accept simple strings like `""0, 1, 2""`

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/832
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: java api, 
Milestone: Release 0.5
Assignee: [skunert|https://github.com/skunert]
Created at: Mon May 19 01:49:53 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398212,,,Mon Jun 09 13:00:36 UTC 2014,,,,,,,,,,"0|i1wk33:",398339,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:00;github-import;[Date: Mon May 19 01:55:04 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

They also do not accept `""0->0,1->1,2->2""`;;;","09/Jun/14 13:00;github-import;[Date: Mon May 19 11:32:14 CEST 2014, Author: [skunert|https://github.com/skunert]]

Okay, I am on it.

Also some explanation how it currently works:

___ConstantFields___
Here, every input field gets one string. For example this is legit:
    
    @ConstantFields({""1->1"", ""2->2"", ""3->3""})

___ConstantFieldsExcept___
In the '''ConstantFieldsExcept''' annotation there is only one String, for example

    @ConstantFields(""1,2,3"")

I agree with you that this is kind of counterintuitive. I am working on a more streamlined version where you can also use one String for the ConstantFields annotation like your first example.
;;;","09/Jun/14 13:00;github-import;[Date: Tue May 20 21:37:54 CEST 2014, Author: [skunert|https://github.com/skunert]]

This is handled in PR ([#840|https://github.com/stratosphere/stratosphere/issues/840] | [FLINK-840|https://issues.apache.org/jira/browse/FLINK-840]) ;;;","09/Jun/14 13:00;github-import;[Date: Wed May 21 23:23:33 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in ([#840|https://github.com/stratosphere/stratosphere/issues/840] | [FLINK-840|https://issues.apache.org/jira/browse/FLINK-840]).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web job submission client errors when trying to run KMeans,FLINK-830,12720011,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:00,09/Jun/14 13:00,14/Jul/23 05:57,09/Jun/14 13:00,,,,pre-apache,,,,,,,0,github-import,,"I tried the run the quickstart KMeans example with the new example program and followed the instructions on the [website|http://stratosphere.eu/quickstart/example.html].

I had to adapt the Python script and the data generation command, but when I tried to submit the job on the webclient, I got this error message:

```
Unknown local strategy 'SORTED_PARTIAL_REDUCE' in JSON generator.
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/830
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, documentation, user satisfaction, 
Milestone: Release 0.5
Created at: Sat May 17 14:45:50 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398210,,,Mon Jun 09 13:00:26 UTC 2014,,,,,,,,,,"0|i1wk2n:",398337,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:00;github-import;[Date: Mon May 19 10:50:09 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed as part of [2bb3e982a71420caf81fbc9b2459a21ae690f446|https://github.com/stratosphere/stratosphere/commit/2bb3e982a71420caf81fbc9b2459a21ae690f446];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several changes of the website blocking 0.5 release,FLINK-829,12720010,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:00,18/Jun/14 13:15,14/Jul/23 05:57,18/Jun/14 13:15,,,,pre-apache,,,,,,,0,github-import,,"Including:
- [Issue 34|https://github.com/stratosphere/stratosphere.github.io/issues/34]
- [Issue 40|https://github.com/stratosphere/stratosphere.github.io/issues/40]
- [Issue 41|https://github.com/stratosphere/stratosphere.github.io/issues/41]
- [Issue 42|https://github.com/stratosphere/stratosphere.github.io/issues/42]
- [Issue 43|https://github.com/stratosphere/stratosphere.github.io/issues/43]



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/829
Created by: [fhueske|https://github.com/fhueske]
Labels: documentation, website, 
Milestone: Release 0.5
Created at: Sat May 17 13:42:30 CEST 2014
State: open
",,github-import,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398209,,,Wed Jun 18 13:15:35 UTC 2014,,,,,,,,,,"0|i1wk2f:",398336,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:00;github-import;[Date: Wed May 21 14:46:57 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

- Issues [34|https://github.com/stratosphere/stratosphere.github.io/issues/34] and [42|https://github.com/stratosphere/stratosphere.github.io/issues/42] are addressed in PR [44|https://github.com/stratosphere/stratosphere.github.io/pull/44]
- Issue [41|https://github.com/stratosphere/stratosphere.github.io/issues/41] is addressed in PR [46|https://github.com/stratosphere/stratosphere.github.io/pull/46]
- Issue [43|https://github.com/stratosphere/stratosphere.github.io/issues/43] is addressed in PR [45|https://github.com/stratosphere/stratosphere.github.io/pull/45]


Issue [40|https://github.com/stratosphere/stratosphere.github.io/issues/40) (write Java API migration guide] has not been addressed yet.;;;","18/Jun/14 13:15;rmetzger;I'm going to close this ""planning"" issue for the 0.5 release.
It looks like we released 0.5 without having a fix for https://github.com/stratosphere/stratosphere.github.io/issues/40.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Java example program with Accumulators,FLINK-828,12720009,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sekruse,fhueske,github-import,09/Jun/14 13:00,28/Feb/19 14:29,14/Jul/23 05:57,15/Jul/14 18:25,,,,pre-apache,,,Examples,,,,0,github-import,starter,"We need a Java example program that shows the use of Accumulators.

The new example program should follow the general layout of the other example programs, i.e., run without parameters on default data, have the same code structure, etc.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/828
Created by: [fhueske|https://github.com/fhueske]
Labels: documentation, java api, 
Milestone: Release 0.6 (unplanned)
Created at: Sat May 17 12:46:44 CEST 2014
State: open
",,fhueske,githubbot,github-import,rmetzger,sekruse,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398208,,,Tue Jul 15 18:25:41 UTC 2014,,,,,,,,,,"0|i1wk27:",398335,,,,,,,,,,,,,,,,,,,,"01/Jul/14 08:40;sekruse;Do we need a running code example or a documentation update or both?;;;","01/Jul/14 10:01;fhueske;First of all, we need an example program similar as one of [these|https://github.com/apache/incubator-flink/tree/master/stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java]

We have some [documenation|http://stratosphere.eu/docs/0.5/programming_guides/java.html#accumulators_counters] for accumulators. If you think it could be improved, go for it.;;;","01/Jul/14 13:52;githubbot;GitHub user sekruse opened a pull request:

    https://github.com/apache/incubator-flink/pull/55

    providing accumulator example as proposed in FLINK-828

    - shows how to build a custom accumulator (for vectors)
    - employ that accumulator for obtaining filter statistics

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sekruse/incubator-flink FLINK-828

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit 2b12cbca8286b4c9567291afe17f85d6b6f4ecf6
Author: Sebastian Kruse <sebastian.kruse@hpi.de>
Date:   2014-07-01T13:22:18Z

    providing accumulator example as proposed in FLINK-828
    
    - shows how to build a custom accumulator (for vectors)
    - employ that accumulator for obtaining filter statistics

----
;;;","02/Jul/14 06:57;githubbot;Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/55#discussion_r14442947
  
    --- Diff: stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/relational/FilterAndCountIncompleteLines.java ---
    @@ -0,0 +1,281 @@
    +/***********************************************************************************************************************
    + *
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + *
    + **********************************************************************************************************************/
    +package eu.stratosphere.example.java.relational;
    +
    +import java.io.DataInput;
    +import java.io.DataOutput;
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.List;
    +
    +import eu.stratosphere.api.common.JobExecutionResult;
    +import eu.stratosphere.api.common.accumulators.Accumulator;
    +import eu.stratosphere.api.java.DataSet;
    +import eu.stratosphere.api.java.ExecutionEnvironment;
    +import eu.stratosphere.api.java.functions.FilterFunction;
    +import eu.stratosphere.api.java.operators.DataSource;
    +import eu.stratosphere.api.java.tuple.Tuple;
    +import eu.stratosphere.configuration.Configuration;
    +
    +/**
    + * This program filters lines from a CSV file with empty fields. In doing so, it
    + * counts the number of empty fields per column within a CSV file using a custom
    + * accumulator for vectors. In this context, empty fields are those, that at
    + * most contain whitespace characters like space and tab.
    + * 
    + * <p>
    + * The input file is a plain text CSV file with the semicolon as field separator
    + * and double quotes as field delimiters and 9 columns. See
    + * {@link #getDataSet(ExecutionEnvironment)} for configuration.
    + * 
    + * <p>
    + * Usage: <code>FilterAndCountIncompleteLines &lt;input file path&gt; &lt;result path&gt;</code> <br>
    + * 
    + * <p>
    + * This example shows how to use:
    + * <ul>
    + * <li>custom accumulators
    + * <li>tuple data types
    + * <li>inline-defined functions
    + * </ul>
    + */
    +@SuppressWarnings(""serial"")
    +public class FilterAndCountIncompleteLines {
    +
    +	// *************************************************************************
    +	// PROGRAM
    +	// *************************************************************************
    +
    +	private static final String EMPTY_FIELD_ACCUMULATOR = ""empty-fields"";
    +
    +	public static void main(String[] args) throws Exception {
    +
    +		if (!parseParameters(args)) {
    +			return;
    +		}
    +
    +		final ExecutionEnvironment env = ExecutionEnvironment
    +				.getExecutionEnvironment();
    +
    +		// get the data set
    +		DataSet<Tuple> file = getDataSet(env);
    +
    +		// filter lines with empty fields
    +		DataSet<Tuple> filteredLines = file.filter(new FilterFunction<Tuple>() {
    +
    +			// create a new accumulator in each filter function instance
    +			// accumulators can be merged later on
    +			private VectorAccumulator emptyFieldCounter = new VectorAccumulator();
    +
    +			/*
    +			 * (non-Javadoc)
    --- End diff --
    
    Can you remove the ""(non-Javadoc)"" javadoc comments? They don't add any value, because its obvious that there is no Javadoc.
    
    We had quite a lot of these comments in our project in the past and did a huge search/replace session to remove all of them.
    
    As a side note: It seems that you've limited with width of your code. We are currently not limiting the width, so you don't need to wrap long lines.
;;;","02/Jul/14 06:59;githubbot;Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/55#discussion_r14443022
  
    --- Diff: stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/relational/FilterAndCountIncompleteLines.java ---
    @@ -0,0 +1,281 @@
    +/***********************************************************************************************************************
    + *
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + *
    + **********************************************************************************************************************/
    +package eu.stratosphere.example.java.relational;
    +
    +import java.io.DataInput;
    +import java.io.DataOutput;
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.List;
    +
    +import eu.stratosphere.api.common.JobExecutionResult;
    +import eu.stratosphere.api.common.accumulators.Accumulator;
    +import eu.stratosphere.api.java.DataSet;
    +import eu.stratosphere.api.java.ExecutionEnvironment;
    +import eu.stratosphere.api.java.functions.FilterFunction;
    +import eu.stratosphere.api.java.operators.DataSource;
    +import eu.stratosphere.api.java.tuple.Tuple;
    +import eu.stratosphere.configuration.Configuration;
    +
    +/**
    + * This program filters lines from a CSV file with empty fields. In doing so, it
    + * counts the number of empty fields per column within a CSV file using a custom
    + * accumulator for vectors. In this context, empty fields are those, that at
    + * most contain whitespace characters like space and tab.
    + * 
    + * <p>
    + * The input file is a plain text CSV file with the semicolon as field separator
    + * and double quotes as field delimiters and 9 columns. See
    + * {@link #getDataSet(ExecutionEnvironment)} for configuration.
    + * 
    + * <p>
    + * Usage: <code>FilterAndCountIncompleteLines &lt;input file path&gt; &lt;result path&gt;</code> <br>
    + * 
    + * <p>
    + * This example shows how to use:
    + * <ul>
    + * <li>custom accumulators
    + * <li>tuple data types
    + * <li>inline-defined functions
    + * </ul>
    + */
    +@SuppressWarnings(""serial"")
    +public class FilterAndCountIncompleteLines {
    +
    +	// *************************************************************************
    +	// PROGRAM
    +	// *************************************************************************
    +
    +	private static final String EMPTY_FIELD_ACCUMULATOR = ""empty-fields"";
    +
    +	public static void main(String[] args) throws Exception {
    +
    +		if (!parseParameters(args)) {
    +			return;
    +		}
    +
    +		final ExecutionEnvironment env = ExecutionEnvironment
    +				.getExecutionEnvironment();
    +
    +		// get the data set
    +		DataSet<Tuple> file = getDataSet(env);
    +
    +		// filter lines with empty fields
    +		DataSet<Tuple> filteredLines = file.filter(new FilterFunction<Tuple>() {
    +
    +			// create a new accumulator in each filter function instance
    +			// accumulators can be merged later on
    +			private VectorAccumulator emptyFieldCounter = new VectorAccumulator();
    +
    +			/*
    +			 * (non-Javadoc)
    +			 * 
    +			 * @see
    +			 * eu.stratosphere.api.common.functions.AbstractFunction#open(eu
    +			 * .stratosphere.configuration.Configuration)
    +			 */
    +			@Override
    +			public void open(Configuration parameters) throws Exception {
    +				super.open(parameters);
    +
    +				// register the accumulator instance
    +				getRuntimeContext().addAccumulator(EMPTY_FIELD_ACCUMULATOR,
    +						this.emptyFieldCounter);
    +			}
    +
    +			@Override
    +			public boolean filter(Tuple t) {
    +				boolean containsEmptyFields = false;
    +
    +				// iterate over the tuple fields looking for empty ones
    +				for (int pos = 0; pos < t.getArity(); pos++) {
    +
    +					String field = t.getField(pos);
    +					if (field == null || field.trim().isEmpty()) {
    +						containsEmptyFields = true;
    +
    +						// if an empty field is encountered, update the
    +						// accumulator
    +						this.emptyFieldCounter.add(pos);
    +					}
    +				}
    +
    +				return !containsEmptyFields;
    +			}
    +		});
    +
    +		// Here, we could do further processing with the filtered lines...
    +		filteredLines.writeAsCsv(outputPath);
    +
    +		// execute program
    +		JobExecutionResult result = env.execute(""Accumulator example"");
    +
    +		// get the accumulator result via its registration key
    +		List<Integer> emptyFields = result.getAccumulatorResult(EMPTY_FIELD_ACCUMULATOR);
    +		System.out.format(""Number of detected empty fields per column: %s\n"",
    +				emptyFields);
    +
    +	}
    +
    +	// *************************************************************************
    +	// UTIL METHODS
    +	// *************************************************************************
    +
    +	private static String filePath;
    +	private static String outputPath;
    +
    +	private static boolean parseParameters(String[] programArguments) {
    +
    +		if (programArguments.length > 0) {
    +			if (programArguments.length == 2) {
    +				filePath = programArguments[0];
    +				outputPath = programArguments[1];
    +			} else {
    +				System.err
    +						.println(""Usage: FilterAndCountIncompleteLines <input file path> <result path>"");
    --- End diff --
    
    Can you turn this into `System.err.println` without the newline and tabs in between?
;;;","02/Jul/14 07:00;githubbot;Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/55#discussion_r14443047
  
    --- Diff: stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/relational/FilterAndCountIncompleteLines.java ---
    @@ -0,0 +1,281 @@
    +/***********************************************************************************************************************
    + *
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + *
    + **********************************************************************************************************************/
    +package eu.stratosphere.example.java.relational;
    +
    +import java.io.DataInput;
    +import java.io.DataOutput;
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.List;
    +
    +import eu.stratosphere.api.common.JobExecutionResult;
    +import eu.stratosphere.api.common.accumulators.Accumulator;
    +import eu.stratosphere.api.java.DataSet;
    +import eu.stratosphere.api.java.ExecutionEnvironment;
    +import eu.stratosphere.api.java.functions.FilterFunction;
    +import eu.stratosphere.api.java.operators.DataSource;
    +import eu.stratosphere.api.java.tuple.Tuple;
    +import eu.stratosphere.configuration.Configuration;
    +
    +/**
    + * This program filters lines from a CSV file with empty fields. In doing so, it
    + * counts the number of empty fields per column within a CSV file using a custom
    + * accumulator for vectors. In this context, empty fields are those, that at
    + * most contain whitespace characters like space and tab.
    + * 
    + * <p>
    + * The input file is a plain text CSV file with the semicolon as field separator
    + * and double quotes as field delimiters and 9 columns. See
    + * {@link #getDataSet(ExecutionEnvironment)} for configuration.
    + * 
    + * <p>
    + * Usage: <code>FilterAndCountIncompleteLines &lt;input file path&gt; &lt;result path&gt;</code> <br>
    + * 
    + * <p>
    + * This example shows how to use:
    + * <ul>
    + * <li>custom accumulators
    + * <li>tuple data types
    + * <li>inline-defined functions
    + * </ul>
    + */
    +@SuppressWarnings(""serial"")
    +public class FilterAndCountIncompleteLines {
    +
    +	// *************************************************************************
    +	// PROGRAM
    +	// *************************************************************************
    +
    +	private static final String EMPTY_FIELD_ACCUMULATOR = ""empty-fields"";
    +
    +	public static void main(String[] args) throws Exception {
    +
    +		if (!parseParameters(args)) {
    +			return;
    +		}
    +
    +		final ExecutionEnvironment env = ExecutionEnvironment
    +				.getExecutionEnvironment();
    +
    +		// get the data set
    +		DataSet<Tuple> file = getDataSet(env);
    +
    +		// filter lines with empty fields
    +		DataSet<Tuple> filteredLines = file.filter(new FilterFunction<Tuple>() {
    +
    +			// create a new accumulator in each filter function instance
    +			// accumulators can be merged later on
    +			private VectorAccumulator emptyFieldCounter = new VectorAccumulator();
    +
    +			/*
    +			 * (non-Javadoc)
    +			 * 
    +			 * @see
    +			 * eu.stratosphere.api.common.functions.AbstractFunction#open(eu
    +			 * .stratosphere.configuration.Configuration)
    +			 */
    +			@Override
    +			public void open(Configuration parameters) throws Exception {
    +				super.open(parameters);
    +
    +				// register the accumulator instance
    +				getRuntimeContext().addAccumulator(EMPTY_FIELD_ACCUMULATOR,
    +						this.emptyFieldCounter);
    +			}
    +
    +			@Override
    +			public boolean filter(Tuple t) {
    +				boolean containsEmptyFields = false;
    +
    +				// iterate over the tuple fields looking for empty ones
    +				for (int pos = 0; pos < t.getArity(); pos++) {
    +
    +					String field = t.getField(pos);
    +					if (field == null || field.trim().isEmpty()) {
    +						containsEmptyFields = true;
    +
    +						// if an empty field is encountered, update the
    +						// accumulator
    +						this.emptyFieldCounter.add(pos);
    +					}
    +				}
    +
    +				return !containsEmptyFields;
    +			}
    +		});
    +
    +		// Here, we could do further processing with the filtered lines...
    +		filteredLines.writeAsCsv(outputPath);
    +
    +		// execute program
    +		JobExecutionResult result = env.execute(""Accumulator example"");
    +
    +		// get the accumulator result via its registration key
    +		List<Integer> emptyFields = result.getAccumulatorResult(EMPTY_FIELD_ACCUMULATOR);
    +		System.out.format(""Number of detected empty fields per column: %s\n"",
    +				emptyFields);
    +
    +	}
    +
    +	// *************************************************************************
    +	// UTIL METHODS
    +	// *************************************************************************
    +
    +	private static String filePath;
    +	private static String outputPath;
    +
    +	private static boolean parseParameters(String[] programArguments) {
    +
    +		if (programArguments.length > 0) {
    +			if (programArguments.length == 2) {
    +				filePath = programArguments[0];
    +				outputPath = programArguments[1];
    +			} else {
    +				System.err
    +						.println(""Usage: FilterAndCountIncompleteLines <input file path> <result path>"");
    +				return false;
    +			}
    +		} else {
    +			System.err
    +					.println(""This program expects a semicolon-delimited CSV file with nine columns.\n""
    +							+ ""  Usage: FilterAndCountIncompleteLines <input file path> <result path>"");
    +			return false;
    +		}
    +		return true;
    +	}
    +
    +	@SuppressWarnings(""unchecked"")
    +	private static DataSource<Tuple> getDataSet(ExecutionEnvironment env) {
    +
    +		DataSource<? extends Tuple> source = env
    +				.readCsvFile(filePath)
    +				.fieldDelimiter(';')
    +				.includeFields(""111111111"")
    --- End diff --
    
    I don't think you need this line. The reader selects all fields by default.
;;;","02/Jul/14 07:01;githubbot;Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/55#discussion_r14443079
  
    --- Diff: stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/relational/FilterAndCountIncompleteLines.java ---
    @@ -0,0 +1,281 @@
    +/***********************************************************************************************************************
    + *
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + *
    + **********************************************************************************************************************/
    +package eu.stratosphere.example.java.relational;
    +
    +import java.io.DataInput;
    +import java.io.DataOutput;
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.List;
    +
    +import eu.stratosphere.api.common.JobExecutionResult;
    +import eu.stratosphere.api.common.accumulators.Accumulator;
    +import eu.stratosphere.api.java.DataSet;
    +import eu.stratosphere.api.java.ExecutionEnvironment;
    +import eu.stratosphere.api.java.functions.FilterFunction;
    +import eu.stratosphere.api.java.operators.DataSource;
    +import eu.stratosphere.api.java.tuple.Tuple;
    +import eu.stratosphere.configuration.Configuration;
    +
    +/**
    + * This program filters lines from a CSV file with empty fields. In doing so, it
    + * counts the number of empty fields per column within a CSV file using a custom
    + * accumulator for vectors. In this context, empty fields are those, that at
    + * most contain whitespace characters like space and tab.
    + * 
    + * <p>
    + * The input file is a plain text CSV file with the semicolon as field separator
    + * and double quotes as field delimiters and 9 columns. See
    + * {@link #getDataSet(ExecutionEnvironment)} for configuration.
    + * 
    + * <p>
    + * Usage: <code>FilterAndCountIncompleteLines &lt;input file path&gt; &lt;result path&gt;</code> <br>
    + * 
    + * <p>
    + * This example shows how to use:
    + * <ul>
    + * <li>custom accumulators
    + * <li>tuple data types
    + * <li>inline-defined functions
    + * </ul>
    + */
    +@SuppressWarnings(""serial"")
    +public class FilterAndCountIncompleteLines {
    +
    +	// *************************************************************************
    +	// PROGRAM
    +	// *************************************************************************
    +
    +	private static final String EMPTY_FIELD_ACCUMULATOR = ""empty-fields"";
    +
    +	public static void main(String[] args) throws Exception {
    +
    +		if (!parseParameters(args)) {
    +			return;
    +		}
    +
    +		final ExecutionEnvironment env = ExecutionEnvironment
    +				.getExecutionEnvironment();
    +
    +		// get the data set
    +		DataSet<Tuple> file = getDataSet(env);
    +
    +		// filter lines with empty fields
    +		DataSet<Tuple> filteredLines = file.filter(new FilterFunction<Tuple>() {
    +
    +			// create a new accumulator in each filter function instance
    +			// accumulators can be merged later on
    +			private VectorAccumulator emptyFieldCounter = new VectorAccumulator();
    +
    +			/*
    +			 * (non-Javadoc)
    +			 * 
    +			 * @see
    +			 * eu.stratosphere.api.common.functions.AbstractFunction#open(eu
    +			 * .stratosphere.configuration.Configuration)
    +			 */
    +			@Override
    +			public void open(Configuration parameters) throws Exception {
    +				super.open(parameters);
    +
    +				// register the accumulator instance
    +				getRuntimeContext().addAccumulator(EMPTY_FIELD_ACCUMULATOR,
    +						this.emptyFieldCounter);
    +			}
    +
    +			@Override
    +			public boolean filter(Tuple t) {
    +				boolean containsEmptyFields = false;
    +
    +				// iterate over the tuple fields looking for empty ones
    +				for (int pos = 0; pos < t.getArity(); pos++) {
    +
    +					String field = t.getField(pos);
    +					if (field == null || field.trim().isEmpty()) {
    +						containsEmptyFields = true;
    +
    +						// if an empty field is encountered, update the
    +						// accumulator
    +						this.emptyFieldCounter.add(pos);
    +					}
    +				}
    +
    +				return !containsEmptyFields;
    +			}
    +		});
    +
    +		// Here, we could do further processing with the filtered lines...
    +		filteredLines.writeAsCsv(outputPath);
    +
    +		// execute program
    +		JobExecutionResult result = env.execute(""Accumulator example"");
    +
    +		// get the accumulator result via its registration key
    +		List<Integer> emptyFields = result.getAccumulatorResult(EMPTY_FIELD_ACCUMULATOR);
    +		System.out.format(""Number of detected empty fields per column: %s\n"",
    +				emptyFields);
    +
    +	}
    +
    +	// *************************************************************************
    +	// UTIL METHODS
    +	// *************************************************************************
    +
    +	private static String filePath;
    +	private static String outputPath;
    +
    +	private static boolean parseParameters(String[] programArguments) {
    --- End diff --
    
    It would be cool to ship the example with a build-in dataset. This way, users don't need to generate / provide the input data. 
;;;","02/Jul/14 07:03;githubbot;Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/55#discussion_r14443106
  
    --- Diff: stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/relational/FilterAndCountIncompleteLines.java ---
    @@ -0,0 +1,281 @@
    +/***********************************************************************************************************************
    + *
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + *
    + **********************************************************************************************************************/
    +package eu.stratosphere.example.java.relational;
    +
    +import java.io.DataInput;
    +import java.io.DataOutput;
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.List;
    +
    +import eu.stratosphere.api.common.JobExecutionResult;
    +import eu.stratosphere.api.common.accumulators.Accumulator;
    +import eu.stratosphere.api.java.DataSet;
    +import eu.stratosphere.api.java.ExecutionEnvironment;
    +import eu.stratosphere.api.java.functions.FilterFunction;
    +import eu.stratosphere.api.java.operators.DataSource;
    +import eu.stratosphere.api.java.tuple.Tuple;
    +import eu.stratosphere.configuration.Configuration;
    +
    +/**
    + * This program filters lines from a CSV file with empty fields. In doing so, it
    + * counts the number of empty fields per column within a CSV file using a custom
    + * accumulator for vectors. In this context, empty fields are those, that at
    + * most contain whitespace characters like space and tab.
    + * 
    + * <p>
    + * The input file is a plain text CSV file with the semicolon as field separator
    + * and double quotes as field delimiters and 9 columns. See
    + * {@link #getDataSet(ExecutionEnvironment)} for configuration.
    + * 
    + * <p>
    + * Usage: <code>FilterAndCountIncompleteLines &lt;input file path&gt; &lt;result path&gt;</code> <br>
    + * 
    + * <p>
    + * This example shows how to use:
    + * <ul>
    + * <li>custom accumulators
    + * <li>tuple data types
    + * <li>inline-defined functions
    + * </ul>
    + */
    +@SuppressWarnings(""serial"")
    +public class FilterAndCountIncompleteLines {
    +
    +	// *************************************************************************
    +	// PROGRAM
    +	// *************************************************************************
    +
    +	private static final String EMPTY_FIELD_ACCUMULATOR = ""empty-fields"";
    +
    +	public static void main(String[] args) throws Exception {
    +
    +		if (!parseParameters(args)) {
    +			return;
    +		}
    +
    +		final ExecutionEnvironment env = ExecutionEnvironment
    +				.getExecutionEnvironment();
    +
    +		// get the data set
    +		DataSet<Tuple> file = getDataSet(env);
    +
    +		// filter lines with empty fields
    +		DataSet<Tuple> filteredLines = file.filter(new FilterFunction<Tuple>() {
    +
    +			// create a new accumulator in each filter function instance
    +			// accumulators can be merged later on
    +			private VectorAccumulator emptyFieldCounter = new VectorAccumulator();
    +
    +			/*
    +			 * (non-Javadoc)
    +			 * 
    +			 * @see
    +			 * eu.stratosphere.api.common.functions.AbstractFunction#open(eu
    +			 * .stratosphere.configuration.Configuration)
    +			 */
    +			@Override
    +			public void open(Configuration parameters) throws Exception {
    +				super.open(parameters);
    +
    +				// register the accumulator instance
    +				getRuntimeContext().addAccumulator(EMPTY_FIELD_ACCUMULATOR,
    +						this.emptyFieldCounter);
    +			}
    +
    +			@Override
    +			public boolean filter(Tuple t) {
    +				boolean containsEmptyFields = false;
    +
    +				// iterate over the tuple fields looking for empty ones
    +				for (int pos = 0; pos < t.getArity(); pos++) {
    +
    +					String field = t.getField(pos);
    +					if (field == null || field.trim().isEmpty()) {
    +						containsEmptyFields = true;
    +
    +						// if an empty field is encountered, update the
    +						// accumulator
    +						this.emptyFieldCounter.add(pos);
    +					}
    +				}
    +
    +				return !containsEmptyFields;
    +			}
    +		});
    +
    +		// Here, we could do further processing with the filtered lines...
    +		filteredLines.writeAsCsv(outputPath);
    +
    +		// execute program
    +		JobExecutionResult result = env.execute(""Accumulator example"");
    +
    +		// get the accumulator result via its registration key
    +		List<Integer> emptyFields = result.getAccumulatorResult(EMPTY_FIELD_ACCUMULATOR);
    +		System.out.format(""Number of detected empty fields per column: %s\n"",
    +				emptyFields);
    +
    +	}
    +
    +	// *************************************************************************
    +	// UTIL METHODS
    +	// *************************************************************************
    +
    +	private static String filePath;
    +	private static String outputPath;
    +
    +	private static boolean parseParameters(String[] programArguments) {
    +
    +		if (programArguments.length > 0) {
    +			if (programArguments.length == 2) {
    +				filePath = programArguments[0];
    +				outputPath = programArguments[1];
    +			} else {
    +				System.err
    +						.println(""Usage: FilterAndCountIncompleteLines <input file path> <result path>"");
    +				return false;
    +			}
    +		} else {
    +			System.err
    +					.println(""This program expects a semicolon-delimited CSV file with nine columns.\n""
    +							+ ""  Usage: FilterAndCountIncompleteLines <input file path> <result path>"");
    +			return false;
    +		}
    +		return true;
    +	}
    +
    +	@SuppressWarnings(""unchecked"")
    +	private static DataSource<Tuple> getDataSet(ExecutionEnvironment env) {
    +
    +		DataSource<? extends Tuple> source = env
    +				.readCsvFile(filePath)
    +				.fieldDelimiter(';')
    +				.includeFields(""111111111"")
    +				.types(String.class, String.class, String.class, String.class,
    +						String.class, String.class, String.class, String.class,
    +						String.class);
    +		return (DataSource<Tuple>) source;
    +	}
    +
    +	/**
    +	 * This accumulator lets you increase vector components distributedly. The
    +	 * {@link #add(Integer)} method lets you increase the <i>n</i>-th vector
    +	 * component by 1, whereat <i>n</i> is the methods parameter. The size of
    +	 * the vector is automatically managed.
    +	 */
    +	public static class VectorAccumulator implements
    +			Accumulator<Integer, List<Integer>> {
    +
    +		/** Stores the accumulated vector components. */
    +		private final List<Integer> resultVector = new ArrayList<Integer>();
    +
    +		/**
    +		 * Increases the result vector component at the specified position by 1.
    +		 */
    +		@Override
    +		public void add(Integer position) {
    +			updateResultVector(position, 1);
    +		}
    +
    +		/**
    +		 * Increases the result vector component at the specified position by
    +		 * the specified delta.
    +		 */
    +		private void updateResultVector(int position, int delta) {
    +			// inflate the vector to contain the given position
    +			while (resultVector.size() <= position) {
    +				resultVector.add(0);
    +			}
    +
    +			// increment the component value
    +			int component = resultVector.get(position);
    +			this.resultVector.set(position, component + delta);
    +		}
    +
    +		/*
    +		 * (non-Javadoc)
    +		 * 
    +		 * @see
    +		 * eu.stratosphere.api.common.accumulators.Accumulator#getLocalValue()
    +		 */
    +		@Override
    +		public List<Integer> getLocalValue() {
    +			return this.resultVector;
    +		}
    +
    +		/*
    +		 * (non-Javadoc)
    +		 * 
    +		 * @see eu.stratosphere.api.common.accumulators.Accumulator#resetLocal()
    +		 */
    +		@Override
    +		public void resetLocal() {
    +			// clear the result vector if the accumulator instance shall be reused
    +			this.resultVector.clear();
    +		}
    +
    +		/*
    +		 * (non-Javadoc)
    +		 * 
    +		 * @see
    +		 * eu.stratosphere.api.common.accumulators.Accumulator#merge(eu.stratosphere
    +		 * .api.common.accumulators.Accumulator)
    +		 */
    +		@Override
    +		public void merge(Accumulator<Integer, List<Integer>> other) {
    +			
    +			// merge two vector accumulators by adding their up their vector components
    +			List<Integer> otherVector = other.getLocalValue();
    +			for (int index = 0; index < otherVector.size(); index++) {
    +				updateResultVector(index, otherVector.get(index));
    +			}
    +		}
    +
    +		/*
    +		 * (non-Javadoc)
    +		 * 
    +		 * @see
    +		 * eu.stratosphere.core.io.IOReadableWritable#write(java.io.DataOutput)
    --- End diff --
    
    Can you remove these Javadocs? Every modern IDE allows to jump to the method definition in the interface.
;;;","02/Jul/14 07:10;githubbot;Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/55#discussion_r14443290
  
    --- Diff: stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/relational/FilterAndCountIncompleteLines.java ---
    @@ -0,0 +1,281 @@
    +/***********************************************************************************************************************
    + *
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + *
    + **********************************************************************************************************************/
    +package eu.stratosphere.example.java.relational;
    +
    +import java.io.DataInput;
    +import java.io.DataOutput;
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.List;
    +
    +import eu.stratosphere.api.common.JobExecutionResult;
    +import eu.stratosphere.api.common.accumulators.Accumulator;
    +import eu.stratosphere.api.java.DataSet;
    +import eu.stratosphere.api.java.ExecutionEnvironment;
    +import eu.stratosphere.api.java.functions.FilterFunction;
    +import eu.stratosphere.api.java.operators.DataSource;
    +import eu.stratosphere.api.java.tuple.Tuple;
    +import eu.stratosphere.configuration.Configuration;
    +
    +/**
    + * This program filters lines from a CSV file with empty fields. In doing so, it
    + * counts the number of empty fields per column within a CSV file using a custom
    + * accumulator for vectors. In this context, empty fields are those, that at
    + * most contain whitespace characters like space and tab.
    + * 
    + * <p>
    + * The input file is a plain text CSV file with the semicolon as field separator
    + * and double quotes as field delimiters and 9 columns. See
    + * {@link #getDataSet(ExecutionEnvironment)} for configuration.
    + * 
    + * <p>
    + * Usage: <code>FilterAndCountIncompleteLines &lt;input file path&gt; &lt;result path&gt;</code> <br>
    + * 
    + * <p>
    + * This example shows how to use:
    + * <ul>
    + * <li>custom accumulators
    + * <li>tuple data types
    + * <li>inline-defined functions
    + * </ul>
    + */
    +@SuppressWarnings(""serial"")
    +public class FilterAndCountIncompleteLines {
    +
    +	// *************************************************************************
    +	// PROGRAM
    +	// *************************************************************************
    +
    +	private static final String EMPTY_FIELD_ACCUMULATOR = ""empty-fields"";
    +
    +	public static void main(String[] args) throws Exception {
    +
    +		if (!parseParameters(args)) {
    +			return;
    +		}
    +
    +		final ExecutionEnvironment env = ExecutionEnvironment
    +				.getExecutionEnvironment();
    +
    +		// get the data set
    +		DataSet<Tuple> file = getDataSet(env);
    --- End diff --
    
    Why are you not using a `Tuple9<String,String, ...>` for the DataSet ?
    This works also fine but you are loosing the compile-time type checking. Basically the typeparamters will make line 105: https://github.com/apache/incubator-flink/pull/55/files#diff-70cc9ad1c8f8e2e82718452620b0d33eR105 safer.
    
    I think its fine in this case to use the `Tuple` but I want to note that.
;;;","02/Jul/14 07:11;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/55#issuecomment-47743880
  
    Thank you for the contribution.
    
    I have some minor comments regarding comment style / example style. In general, your code is very good!
;;;","02/Jul/14 07:18;sekruse;Thanks, I will incorporate your comments.

Regarding the Tuple datatype: I chose that one so that the code would work with any tuple type to account for varying numbers of columns in CSV files. However, if you want, I can use a concrete TupleX type. Then I would switch over to Tuple3 or something as the many generic parameters of Tuple9 might impair the code readability.;;;","02/Jul/14 08:02;rmetzger;I agree. Tuple9 would make the code very hard to read.
I'd say we keep it with the {{Tuple}}. Nobody else from the project complained about it yet and its good to show one example without type params.;;;","02/Jul/14 08:28;sekruse;Alright, I reworked it.;;;","02/Jul/14 08:34;githubbot;Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/55#discussion_r14445872
  
    --- Diff: stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/relational/FilterAndCountIncompleteLines.java ---
    @@ -0,0 +1,247 @@
    +/***********************************************************************************************************************
    + *
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + *
    + **********************************************************************************************************************/
    +package eu.stratosphere.example.java.relational;
    +
    +import java.io.DataInput;
    +import java.io.DataOutput;
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.Collection;
    +import java.util.List;
    +
    +import eu.stratosphere.api.common.JobExecutionResult;
    +import eu.stratosphere.api.common.accumulators.Accumulator;
    +import eu.stratosphere.api.java.DataSet;
    +import eu.stratosphere.api.java.ExecutionEnvironment;
    +import eu.stratosphere.api.java.functions.FilterFunction;
    +import eu.stratosphere.api.java.tuple.Tuple;
    +import eu.stratosphere.api.java.tuple.Tuple3;
    +import eu.stratosphere.configuration.Configuration;
    +
    +/**
    + * This program filters lines from a CSV file with empty fields. In doing so, it counts the number of empty fields per
    + * column within a CSV file using a custom accumulator for vectors. In this context, empty fields are those, that at
    + * most contain whitespace characters like space and tab.
    + * <p>
    + * The input file is a plain text CSV file with the semicolon as field separator and double quotes as field delimiters
    + * and three columns. See {@link #getDataSet(ExecutionEnvironment)} for configuration.
    + * <p>
    + * Usage: <code>FilterAndCountIncompleteLines &lt;input file path or ""example""&gt; &lt;result path&gt;</code> <br>
    + * <p>
    + * This example shows how to use:
    + * <ul>
    + * <li>custom accumulators
    + * <li>tuple data types
    + * <li>inline-defined functions
    + * </ul>
    + */
    +@SuppressWarnings(""serial"")
    +public class FilterAndCountIncompleteLines {
    +
    +	// *************************************************************************
    +	// PROGRAM
    +	// *************************************************************************
    +
    +	private static final String EMPTY_FIELD_ACCUMULATOR = ""empty-fields"";
    +
    +	public static void main(final String[] args) throws Exception {
    +
    +		if (!parseParameters(args)) {
    +			return;
    +		}
    +
    +		final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
    +
    +		// get the data set
    +		final DataSet<Tuple> file = getDataSet(env);
    +
    +		// filter lines with empty fields
    +		final DataSet<Tuple> filteredLines = file
    +				.filter(new FilterFunction<Tuple>() {
    +
    +					// create a new accumulator in each filter function instance
    +					// accumulators can be merged later on
    +					private final VectorAccumulator emptyFieldCounter = new VectorAccumulator();
    +
    +					@Override
    +					public void open(final Configuration parameters) throws Exception {
    +						super.open(parameters);
    +
    +						// register the accumulator instance
    +						getRuntimeContext().addAccumulator(EMPTY_FIELD_ACCUMULATOR,
    +								this.emptyFieldCounter);
    +					}
    +
    +					@Override
    +					public boolean filter(final Tuple t) {
    +						boolean containsEmptyFields = false;
    +
    +						// iterate over the tuple fields looking for empty ones
    +						for (int pos = 0; pos < t.getArity(); pos++) {
    +
    +							final String field = t.getField(pos);
    +							if (field == null || field.trim().isEmpty()) {
    +								containsEmptyFields = true;
    +
    +								// if an empty field is encountered, update the
    +								// accumulator
    +								this.emptyFieldCounter.add(pos);
    +							}
    +						}
    +
    +						return !containsEmptyFields;
    +					}
    +				});
    +
    +		// Here, we could do further processing with the filtered lines...
    +		filteredLines.writeAsCsv(outputPath);
    +
    +		// execute program
    +		final JobExecutionResult result = env.execute(""Accumulator example"");
    +
    +		// get the accumulator result via its registration key
    +		final List<Integer> emptyFields = result.getAccumulatorResult(EMPTY_FIELD_ACCUMULATOR);
    +		System.out.format(""Number of detected empty fields per column: %s\n"",
    +				emptyFields);
    +
    +	}
    +
    +	// *************************************************************************
    +	// UTIL METHODS
    +	// *************************************************************************
    +
    +	private static String filePath;
    +	private static String outputPath;
    +
    +	private static boolean parseParameters(final String[] programArguments) {
    +
    +		if (programArguments.length > 0) {
    +			if (programArguments.length == 2) {
    +				filePath = programArguments[0];
    +				outputPath = programArguments[1];
    +			} else {
    +				System.err.println(""Usage: FilterAndCountIncompleteLines <input file path or \""example\""> <result path>"");
    +				return false;
    +			}
    +		} else {
    +			System.err.println(""This program expects a semicolon-delimited CSV file with nine columns.\n""
    --- End diff --
    
    Can you change the code that it does not require any parameters at all? (The user still has to figure out how to pass parameters to the program)
    So if the user does not specify any arguments, we use `getExampleInputTuples()` to get some input data. We print the result using dataSet.print() to stdout.

;;;","02/Jul/14 08:50;githubbot;Github user sekruse commented on the pull request:

    https://github.com/apache/incubator-flink/pull/55#issuecomment-47751359
  
    Okay, done.
;;;","02/Jul/14 09:40;githubbot;Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/55#discussion_r14448525
  
    --- Diff: stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/relational/FilterAndCountIncompleteLines.java ---
    @@ -0,0 +1,248 @@
    +/***********************************************************************************************************************
    + *
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + *
    + **********************************************************************************************************************/
    +package eu.stratosphere.example.java.relational;
    +
    +import java.io.DataInput;
    +import java.io.DataOutput;
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.Collection;
    +import java.util.List;
    +
    +import eu.stratosphere.api.common.JobExecutionResult;
    +import eu.stratosphere.api.common.accumulators.Accumulator;
    +import eu.stratosphere.api.java.DataSet;
    +import eu.stratosphere.api.java.ExecutionEnvironment;
    +import eu.stratosphere.api.java.functions.FilterFunction;
    +import eu.stratosphere.api.java.tuple.Tuple;
    +import eu.stratosphere.api.java.tuple.Tuple3;
    +import eu.stratosphere.configuration.Configuration;
    +
    +/**
    + * This program filters lines from a CSV file with empty fields. In doing so, it counts the number of empty fields per
    + * column within a CSV file using a custom accumulator for vectors. In this context, empty fields are those, that at
    + * most contain whitespace characters like space and tab.
    + * <p>
    + * The input file is a plain text CSV file with the semicolon as field separator and double quotes as field delimiters
    + * and three columns. See {@link #getDataSet(ExecutionEnvironment)} for configuration.
    + * <p>
    + * Usage: <code>FilterAndCountIncompleteLines [&lt;input file path&gt; [&lt;result path&gt;]]</code> <br>
    + * <p>
    + * This example shows how to use:
    + * <ul>
    + * <li>custom accumulators
    + * <li>tuple data types
    + * <li>inline-defined functions
    + * </ul>
    + */
    +@SuppressWarnings(""serial"")
    +public class FilterAndCountIncompleteLines {
    +
    +	// *************************************************************************
    +	// PROGRAM
    +	// *************************************************************************
    +
    +	private static final String EMPTY_FIELD_ACCUMULATOR = ""empty-fields"";
    +
    +	public static void main(final String[] args) throws Exception {
    +
    +		if (!parseParameters(args)) {
    +			return;
    +		}
    +
    +		final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
    +
    +		// get the data set
    +		final DataSet<Tuple> file = getDataSet(env);
    +
    +		// filter lines with empty fields
    +		final DataSet<Tuple> filteredLines = file.filter(new FilterFunction<Tuple>() {
    --- End diff --
    
    Would you mind not defining the FilterFunction inline. 
    IMO, separating the data flow and the UDF logic is easier to read, esp. if the UDF implements multiple methods.
;;;","02/Jul/14 09:42;githubbot;Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/55#issuecomment-47756246
  
    Code looks good. :smile:
    Can we change the name of the program? What do you think about EmptyFieldsCountAccumulator?
;;;","02/Jul/14 09:47;githubbot;Github user sekruse commented on the pull request:

    https://github.com/apache/incubator-flink/pull/55#issuecomment-47756694
  
    Since the main purpose of this program is the demonstration of accumulators, that might actually be a good idea :wink:. I'll incorporate your feedback after lunch.
;;;","03/Jul/14 10:14;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/55#issuecomment-47889257
  
    Okay, I think the pull request is good to merge.
;;;","14/Jul/14 17:44;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/55#issuecomment-48932201
  
    I'm going to merge this pull request ...
;;;","15/Jul/14 18:19;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/55
;;;","15/Jul/14 18:25;rmetzger;Thank you. I've merged the pull request (https://github.com/apache/incubator-flink/pull/55).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix windows line endings#791 - Windows delimiter is accepted as standard ,FLINK-825,12720006,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:00,09/Jun/14 13:00,14/Jul/23 05:57,09/Jun/14 13:00,,,,pre-apache,,,,,,,0,github-import,,"This is a fix for issue: ([#791|https://github.com/stratosphere/stratosphere/issues/791] | [FLINK-791|https://issues.apache.org/jira/browse/FLINK-791]) 

If the standard delimiter is \n, a \r\n is accepted as well. Therefore a
marker was introduced to indicate that specific case. The delimiter
length is variably set to the windows delimiter size then. To prevent
failing in reading input with windows line endings.
The DelimitedInputFormat.open() method did not check if it has parsers
registered. Added throwing an exception when no parser is registered
when calling open. CsvInputFormat does now accept windows line endings
with standards settings. The |\r\n"" will be replaced by ""\n_""( _ is a
space)... To prevent from errors the returned length is set to the
space. So that this byte is known as parsed and will not get interpreted
anymore.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/825
Created by: [tobwiens|https://github.com/tobwiens]
Labels: 
Created at: Fri May 16 11:26:34 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:00;github-import;pull-request-825-7522658514205059157.patch;https://issues.apache.org/jira/secure/attachment/12649337/pull-request-825-7522658514205059157.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398205,,,Mon Jun 09 13:00:14 UTC 2014,,,,,,,,,,"0|i1wk1j:",398332,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:00;github-import;[Date: Fri May 16 13:48:03 CEST 2014, Author: [tobwiens|https://github.com/tobwiens]]

Is not stable yet. ;;;","09/Jun/14 13:00;github-import;[Date: Mon May 19 15:01:44 CEST 2014, Author: [tobwiens|https://github.com/tobwiens]]

Thank you Stefan, I addressed the issues!

I left the length == 1 check, to not interfere with arbitrary delimiters which might contain a \n at position 0. ;;;","09/Jun/14 13:00;github-import;[Date: Sat May 31 19:21:00 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed up and merged in [b0557d08865d31762fb2ddb913b4aa7a2f3ed3b3|https://github.com/stratosphere/stratosphere/commit/b0557d08865d31762fb2ddb913b4aa7a2f3ed3b3];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update example documentation on website,FLINK-824,12720005,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 13:00,09/Jun/14 13:00,14/Jul/23 05:57,09/Jun/14 13:00,,,,pre-apache,,,,,,,0,github-import,,"With [4d0f6123626092b9aae388a893a1775fba39bc78|https://github.com/stratosphere/stratosphere/commit/4d0f6123626092b9aae388a893a1775fba39bc78] the Java examples have been refactored. 

We need to update the [example documentation|http://stratosphere.eu/docs/0.5/programming_guides/examples.html] on the website for this.
We should also link from the [API documentation|http://stratosphere.eu/docs/0.5/programming_guides/java.html] to examples that show the API features in action.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/824
Created by: [fhueske|https://github.com/fhueske]
Labels: documentation, website, 
Milestone: Release 0.5
Created at: Thu May 15 23:37:43 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398204,,,Mon Jun 09 13:00:08 UTC 2014,,,,,,,,,,"0|i1wk1b:",398331,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:00;github-import;[Date: Sat May 17 13:39:43 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Was moved to the [website issue tracker|https://github.com/stratosphere/stratosphere.github.io/issues/43];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration of the CachedBuildSideMatchDriver,FLINK-822,12720003,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:59,09/Jun/14 13:00,14/Jul/23 05:57,09/Jun/14 13:00,,,,pre-apache,,,,,,,0,github-import,,"This is a first version of integrating the CachedBuildSideMatchDriver into the Compiler.
In the current implementation it switches a regular hybrid hash join inside of iterations to the cached variant. The switching is done after the optimization in the NepheleJobGraphGenerator.

I debugged myself through a few of the example iterative algorithms we have and in many cases this implementation correctly switches the driver. But there are some cases where the optimizer decides to use a merge join and that can not so easily be replaced.

All in all I don't like this implementation very much. It is quite hacky and I think we should integrate the cached variant at an earlier point inside the optimization. But that would probably require a few extensions to the cost model. I created an issue to discuss this further:  ([#795|https://github.com/stratosphere/stratosphere/issues/795] | [FLINK-795|https://issues.apache.org/jira/browse/FLINK-795])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/822
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Thu May 15 22:07:54 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:00;github-import;pull-request-822-7396058233703880910.patch;https://issues.apache.org/jira/secure/attachment/12649336/pull-request-822-7396058233703880910.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398202,,,Mon Jun 09 13:00:02 UTC 2014,,,,,,,,,,"0|i1wk0v:",398329,,,,,,,,,,,,,,,,,,,,"09/Jun/14 13:00;github-import;[Date: Mon May 19 13:32:57 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I am working on a cleaner integration, so please don't merge this yet.;;;","09/Jun/14 13:00;github-import;[Date: Mon May 19 15:48:51 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I have decided for a different way to integrate this. So this pull request was replace by ([#836|https://github.com/stratosphere/stratosphere/issues/836] | [FLINK-836|https://issues.apache.org/jira/browse/FLINK-836]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some minor fixes,FLINK-821,12720002,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:59,09/Jun/14 12:59,14/Jul/23 05:57,09/Jun/14 12:59,,,,pre-apache,,,,,,,0,github-import,,"- Check if plan is null and die with nicer exception than NPS
- don't die with indexoutofbounds (or so) exception
- fix IllegalMonitorStateException in JM (see also https://github.com/stratosphere/stratosphere/issues/739)
- show exceptions in log, with INFO log lvl (see mailing list).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/821
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Thu May 15 21:51:22 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;pull-request-821-2132902991571131421.patch;https://issues.apache.org/jira/secure/attachment/12649335/pull-request-821-2132902991571131421.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398201,,,Mon Jun 09 12:59:57 UTC 2014,,,,,,,,,,"0|i1wk0n:",398328,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;[Date: Thu May 15 22:35:06 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good aside from the one inline comment. Good to merge.;;;","09/Jun/14 12:59;github-import;[Date: Fri May 16 13:54:39 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I added the changed heapspace setting for YARN to the pull request.
I think Spark has 384 MB that they subtract.
;;;","09/Jun/14 12:59;github-import;[Date: Fri May 16 14:28:26 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

While testing the YARN change on amazon EMR, I found that the YARN client is not compatible with Hadoop 2.4.0

```
Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.hadoop.yarn.util.Apps.addToEnvironment(Ljava/util/Map;Ljava/lang/String;Ljava/lang/String;)V
	at eu.stratosphere.yarn.Utils.setupEnv(Utils.java:160)
	at eu.stratosphere.yarn.ApplicationMaster.run(ApplicationMaster.java:253)
	at eu.stratosphere.yarn.ApplicationMaster.access$000(ApplicationMaster.java:62)
	at eu.stratosphere.yarn.ApplicationMaster$1.run(ApplicationMaster.java:314)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:356)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1528)
	at eu.stratosphere.yarn.ApplicationMaster.main(ApplicationMaster.java:310)
```
But I think the memory calculation is working properly;;;","09/Jun/14 12:59;github-import;[Date: Fri May 16 15:27:26 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Should we cap it at 400 MB then?;;;","09/Jun/14 12:59;github-import;[Date: Sat May 17 19:08:47 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged in commit [b31ae3ac3533441d2aa687ef11d3e637a32846ce|https://github.com/stratosphere/stratosphere/commit/b31ae3ac3533441d2aa687ef11d3e637a32846ce].
I've left the 500 MB. I'll do some tests with 400 to see if its also working well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
small corrections in refactored examples,FLINK-817,12719998,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:59,09/Jun/14 12:59,14/Jul/23 05:57,09/Jun/14 12:59,,,,pre-apache,,,,,,,0,github-import,,"Hey @fhueske, I made some small corrections to the refactored examples. 
I'll write other general comments to the ML.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/817
Created by: [vasia|https://github.com/vasia]
Labels: 
Created at: Thu May 15 12:28:07 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;pull-request-817-8701011252938958372.patch;https://issues.apache.org/jira/secure/attachment/12649334/pull-request-817-8701011252938958372.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398197,,,Mon Jun 09 12:59:42 UTC 2014,,,,,,,,,,"0|i1wjzr:",398324,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;[Date: Thu May 15 12:37:16 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Great, thanks a lot! :smile:;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update JavaDocs of ReduceFunction,FLINK-816,12719997,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:59,09/Jun/14 12:59,14/Jul/23 05:57,09/Jun/14 12:59,,,,pre-apache,,,,,,,0,github-import,,"The JavaDocs are outdated and need to be updated (returning of mutable objects).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/816
Created by: [fhueske|https://github.com/fhueske]
Labels: documentation, 
Milestone: Release 0.5
Created at: Wed May 14 21:51:06 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398196,,,Mon Jun 09 12:59:37 UTC 2014,,,,,,,,,,"0|i1wjzj:",398323,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;[Date: Thu May 15 14:56:39 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Will do this;;;","09/Jun/14 12:59;github-import;[Date: Thu May 15 15:14:39 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Done with [38ce861ff20935a18c9f29f9bba3195a7c404a7d|https://github.com/stratosphere/stratosphere/commit/38ce861ff20935a18c9f29f9bba3195a7c404a7d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve partitioning hash function,FLINK-815,12719996,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:59,18/Jun/14 15:35,14/Jul/23 05:57,18/Jun/14 15:35,,,,0.6-incubating,pre-apache,pre-apache-0.5,,,,,0,github-import,,"Right now, the partitioner (`OutputEmitter`) used directly the hash code produced by the partitioning elements. Types like `Integer` have very weak hash functions, so the hash partitioning is very susceptible to skew there.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/815
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: runtime, 
Milestone: Release 0.6 (unplanned)
Created at: Wed May 14 21:40:36 CEST 2014
State: open
",,github-import,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398195,,,Wed Jun 18 15:35:35 UTC 2014,,,,,,,,,,"0|i1wjzb:",398322,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;[Date: Fri May 16 16:22:47 CEST 2014, Author: [vasia|https://github.com/vasia]]

I'll start working on this. 
The idea so far is to use something similar to [MurmurHash|https://en.wikipedia.org/wiki/MurmurHash]. I also found that there exists a [Guava implementation for the MurmurHash|https://code.google.com/p/guava-libraries/source/browse/guava/src/com/google/common/hash/Murmur3_32HashFunction.java]. 
Let me know if you have other ideas/suggestions.;;;","09/Jun/14 12:59;github-import;[Date: Fri May 16 16:30:22 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The hash implementations are all expecting a byte buffer and hash over the
bytes. Can we use an implementation tailored towards an int (4 bytes) ?

Alternatives could be:

  - http://en.wikipedia.org/wiki/Jenkins_hash_function
  - http://burtleburtle.net/bob/hash/index.html

We took the HashTable hash function from the Jenkins suite. We need to make
sure that we use a different here.;;;","09/Jun/14 12:59;github-import;[Date: Fri May 16 18:00:46 CEST 2014, Author: [vasia|https://github.com/vasia]]

Murmur loops over 4-byte chunks of the input, so I guess we can use it performing just one loop on the int value. Otherwise, I see you have used the ""4-byte integer hash, full avalanche"" from the website you gave, for the HashTable. This seems to be the best among the ones described there and then there is the one that uses 7 shifts (doesn't really have a name).

I can try both, but I'm not really sure how to test which is better for what we want. Any hints on that?;;;","09/Jun/14 12:59;github-import;[Date: Mon May 19 14:38:28 CEST 2014, Author: [vasia|https://github.com/vasia]]

Hey,
[here|https://github.com/vasia/stratosphere/commit/bd21b740dbb24e28892f1a3a0fdfb044e07ba1f1]'s my first take on this. 
I did some very basic tests based on the `OutputEmitterTest` to check the behavior of the hash function.
I'm attaching 3 diagrams, one for integer records, one for strings and one for records with an integer, a string and a double field. Each diagram shows boxplots of the distribution of values to channels. The values are generated as in `OutputEmitterTest`. I'm using 100000 records and varying the number of channels from 10-100, with a step of 10.
Let me know what you think!
![mixed|https://cloud.githubusercontent.com/assets/498957/3014010/3e5a127a-df52-11e3-9793-1489c9a26d9a.png]
![strings|https://cloud.githubusercontent.com/assets/498957/3014011/3e5b59b4-df52-11e3-9d8d-76e212742d8e.png]
![integers|https://cloud.githubusercontent.com/assets/498957/3014012/3e5be24e-df52-11e3-9b89-9f8bc70798ca.png]


;;;","18/Jun/14 15:35;rmetzger;This issue has been resolved in this commit: https://github.com/stratosphere/stratosphere/commit/39fd71ae04faa2761579afdf4f13f97340b34c34;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"(Copyable)ValueComparator use ascending flag, tests added",FLINK-814,12719995,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:59,09/Jun/14 12:59,14/Jul/23 05:57,09/Jun/14 12:59,,,,pre-apache,,,,,,,0,github-import,,"([#812|https://github.com/stratosphere/stratosphere/issues/812] | [FLINK-812|https://issues.apache.org/jira/browse/FLINK-812])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/814
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Wed May 14 21:00:16 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;pull-request-814-8423747089205609614.patch;https://issues.apache.org/jira/secure/attachment/12649333/pull-request-814-8423747089205609614.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398194,,,Mon Jun 09 12:59:30 UTC 2014,,,,,,,,,,"0|i1wjz3:",398321,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;[Date: Thu May 15 01:27:08 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good, will merge!;;;","09/Jun/14 12:59;github-import;[Date: Thu May 15 13:01:41 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [4cdc6335910fcf0612585e8d787b7980641285c0|https://github.com/stratosphere/stratosphere/commit/4cdc6335910fcf0612585e8d787b7980641285c0];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add GenericTypeComparator,FLINK-813,12719994,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:59,09/Jun/14 12:59,14/Jul/23 05:57,09/Jun/14 12:59,,,,pre-apache,,,,,,,0,github-import,,"This PR adds the GenericTypeComparator for generic Comparable types (see `GenericTypeInfo`, `GenericTypeComparator`, and `GenericTypeComparatorTest`).

(While implementing, I noticed some code formatting issues, which I've reformatted.)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/813
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Wed May 14 20:39:54 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;pull-request-813-6268862524804050696.patch;https://issues.apache.org/jira/secure/attachment/12649332/pull-request-813-6268862524804050696.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398193,,,Mon Jun 09 12:59:25 UTC 2014,,,,,,,,,,"0|i1wjyv:",398320,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;[Date: Thu May 15 01:33:10 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good, merging now...;;;","09/Jun/14 12:59;github-import;[Date: Thu May 15 13:01:54 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [1b6a3acfb7d1991ad8e76425e31792d38403b136|https://github.com/stratosphere/stratosphere/commit/1b6a3acfb7d1991ad8e76425e31792d38403b136];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ValueComparator neglects ascending order flag,FLINK-812,12719993,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:59,12/Aug/20 21:18,14/Jul/23 05:57,09/Jun/14 12:59,,,,pre-apache,,,,,,,0,github-import,,"There are no test cases for `ValueComparator`.

Both `compare` and `compareToReference` in `ValueComparator` and `CopyableValueComparator` neglect the ascending comparison flag before returning the comparison result.



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/812
Created by: [uce|https://github.com/uce]
Labels: bug, java api, 
Milestone: Release 0.5
Created at: Wed May 14 18:51:05 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398192,,,Mon Jun 09 12:59:19 UTC 2014,,,,,,,,,,"0|i1wjyn:",398319,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;[Date: Wed May 14 20:00:11 CEST 2014, Author: [zentol|https://github.com/zentol]]

I'm on it.;;;","09/Jun/14 12:59;github-import;[Date: Thu May 15 22:35:29 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in ([#814|https://github.com/stratosphere/stratosphere/issues/814] | [FLINK-814|https://issues.apache.org/jira/browse/FLINK-814]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix version warnign for Maven JAR plugin in root pom.xml,FLINK-811,12719992,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:59,09/Jun/14 12:59,14/Jul/23 05:57,09/Jun/14 12:59,,,,pre-apache,,,,,,,0,github-import,,"I am getting a lot of warnings that the Maven JAR plugin added to fix the ""version in Uberjar"" problem has no version.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/811
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Milestone: Release 0.5
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Wed May 14 18:10:12 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398191,,,Mon Jun 09 12:59:15 UTC 2014,,,,,,,,,,"0|i1wjyf:",398318,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;[Date: Wed May 14 18:24:03 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Yes, I forgot to add the version to the jar plugin. I'll fix this with my
next pull request.


On Wed, May 14, 2014 at 6:10 PM, Stephan Ewen <notifications@github.com>wrote:

> I am getting a lot of warnings that the Maven JAR plugin added to fix the
> ""version in Uberjar"" problem has no version.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/811>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","09/Jun/14 12:59;github-import;[Date: Wed May 14 18:24:39 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Cool, thanks;;;","09/Jun/14 12:59;github-import;[Date: Thu May 15 10:34:04 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed with PR ([#797|https://github.com/stratosphere/stratosphere/issues/797] | [FLINK-797|https://issues.apache.org/jira/browse/FLINK-797]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add JAR packaging for new Java API examples ,FLINK-810,12719991,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:59,09/Jun/14 12:59,14/Jul/23 05:57,09/Jun/14 12:59,,,,pre-apache,,,,,,,0,github-import,,"Currently, all JAR examples that are packaged are from the old Record API

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/810
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Wed May 14 18:09:20 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398190,,,Mon Jun 09 12:59:11 UTC 2014,,,,,,,,,,"0|i1wjy7:",398317,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;[Date: Wed May 14 18:19:12 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Will take care of this.;;;","09/Jun/14 12:59;github-import;[Date: Thu May 15 09:40:29 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

It seems that, due to commit [c41569eb01ac9ab83e17554a50ccdc7e4ac49327|https://github.com/stratosphere/stratosphere/commit/c41569eb01ac9ab83e17554a50ccdc7e4ac49327] (I guess), the example jar files are empty.;;;","09/Jun/14 12:59;github-import;[Date: Thu May 15 11:38:28 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed in [465a6a4df6416b3010f8a11c3113b2d06c2e9d44|https://github.com/stratosphere/stratosphere/commit/465a6a4df6416b3010f8a11c3113b2d06c2e9d44];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix for branching merging logic in BulkIterationPlanNode,FLINK-809,12719989,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:59,09/Jun/14 12:59,14/Jul/23 05:57,09/Jun/14 12:59,,,,pre-apache,,,,,,,0,github-import,,"Fixes a NPE in the BulkIterationPlanNode that occurs when a the output of the bulk iteration is used as a broadcast set with the (branched) input of the bulk iteration.
See the added test case which reproduced the bug.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/809
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, optimizer, 
Milestone: Release 0.5
Created at: Wed May 14 18:09:20 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;pull-request-809-6827653548353825138.patch;https://issues.apache.org/jira/secure/attachment/12649331/pull-request-809-6827653548353825138.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398188,,,Mon Jun 09 12:59:06 UTC 2014,,,,,,,,,,"0|i1wjxr:",398315,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:59;github-import;[Date: Wed May 14 21:02:38 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [ff8bceec7fca1394fb3a223018c6c0380d7eab2a|https://github.com/stratosphere/stratosphere/commit/ff8bceec7fca1394fb3a223018c6c0380d7eab2a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove cloud code,FLINK-808,12719988,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:58,22/Jun/14 21:48,14/Jul/23 05:57,22/Jun/14 21:48,,,,0.6-incubating,,,,,,,0,github-import,,"Removed legacy Nephele cloud code. Among others, all InstanceType related code is removed. Furthermore, a simple slot system for the TaskManager is introduced. Each TaskManager has a set of slots. Either the number is set in the configuration or the number of cores is taken as the default value. This implies that the intra-node parallelism was abandoned. Additionally a relative memory assignment was implemented. During the compiling process, for each task its memory share is calculated. Once deployed this value is translated with respect to the available memory on the TaskManager.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/808
Created by: [tillrohrmann|https://github.com/tillrohrmann]
Labels: 
Created at: Wed May 14 17:55:30 CEST 2014
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:58;github-import;pull-request-808-1284400963159074775.patch;https://issues.apache.org/jira/secure/attachment/12649330/pull-request-808-1284400963159074775.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398187,,,Sun Jun 22 21:48:39 UTC 2014,,,,,,,,,,"0|i1wjxj:",398314,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:58;github-import;[Date: Wed May 14 20:06:14 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Nice. That one is also a contender for ""biggest pull request"" ;-)

I think we will merge that after the 0.5 release, as it changes quite a bit of the model;;;","09/Jun/14 12:58;github-import;[Date: Sun May 25 15:33:33 CEST 2014, Author: [uce|https://github.com/uce]]

+1

I think we should go over this PR as soon as 0.5 is out. Looking forward to it. :-);;;","09/Jun/14 12:58;github-import;[Date: Sun May 25 15:40:26 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Yes, it will be one of the first after the 0.5 release...;;;","09/Jun/14 12:58;github-import;[Date: Sat May 31 19:25:07 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I am trying to merge this in the 0.6-SNAPSHOT master;;;","09/Jun/14 12:58;github-import;[Date: Sat May 31 19:27:10 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

+1

Sent from my iPhone

On 31.05.2014, at 19:25, Stephan Ewen <notifications@github.com> wrote:

I am trying to merge this in the 0.6-SNAPSHOT master

—
Reply to this email directly or view it on GitHub
<https://github.com/stratosphere/stratosphere/pull/808#issuecomment-44754190>
.;;;","09/Jun/14 12:59;github-import;[Date: Sat May 31 19:38:14 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

This changes a lot:

  - The task managers are no longer a single unit that takes ""subtasksPerInstance"". Instead a task manager as a number of ""slots""

  - A slot can take one parallel instance from each operator 

  - It implies that if you have a cluster with 10 node each with 4 slots (40 slots total) and schedule a job with DOP 20, it will potentially run on 5 machines with 4 tasks on each machine (one per slot). Before it ran on all machines and decreased the per-machine DOP. That was a bit unpredictable, it always depended on how much the current cluster load was.

  - This will allow us to simplify the scheduling logic

  - It is a prerequisite for our strategy to reduce memory waste
;;;","09/Jun/14 12:59;github-import;[Date: Sat May 31 20:26:54 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

That one will keep me busy tonight ;-);;;","09/Jun/14 12:59;github-import;[Date: Sat May 31 23:38:04 CEST 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

The most of it should only be of cosmetic nature. Next time, I'll split the
changes...
On May 31, 2014 8:26 PM, ""Stephan Ewen"" <notifications@github.com> wrote:

> That one will keep me busy tonight ;-)
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/stratosphere/stratosphere/pull/808#issuecomment-44755703>
> .
>;;;","09/Jun/14 12:59;github-import;[Date: Sun Jun 01 14:56:48 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The pull request removed the ""connection IDs"" from the edges. I think they are important to decide what can be multiplexed though the same TCP connection and what cannot.;;;","09/Jun/14 12:59;github-import;[Date: Sun Jun 01 14:56:54 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I am re-adding them;;;","09/Jun/14 12:59;github-import;[Date: Sun Jun 01 15:00:14 CEST 2014, Author: [uce|https://github.com/uce]]

@StephanEwen Yes, the netstack relies on the connection IDs.;;;","22/Jun/14 21:48;sewen;Fixed via 86d206c41922a1b7b8c2839b65d3568f9be55e0c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add JavaDoc Comments to ExecutionEnvironment,FLINK-807,12719987,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:58,09/Jun/14 12:58,14/Jul/23 05:57,09/Jun/14 12:58,,,,pre-apache,,,,,,,0,github-import,,"I am on that one...

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/807
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Milestone: Release 0.5
Created at: Wed May 14 15:56:23 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398186,,,Mon Jun 09 12:58:38 UTC 2014,,,,,,,,,,"0|i1wjxb:",398313,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:58;github-import;[Date: Wed May 14 20:03:48 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Part one in [6728ec52ccf8d77225282438056fa46c364709c8|https://github.com/stratosphere/stratosphere/commit/6728ec52ccf8d77225282438056fa46c364709c8];;;","09/Jun/14 12:58;github-import;[Date: Thu May 15 17:58:11 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [1fff513721895f8cf8fe4f2ddad5fa2af64898f7|https://github.com/stratosphere/stratosphere/commit/1fff513721895f8cf8fe4f2ddad5fa2af64898f7];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TuplePairComparatorTests,FLINK-806,12719986,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:58,09/Jun/14 12:58,14/Jul/23 05:57,09/Jun/14 12:58,,,,pre-apache,,,,,,,0,github-import,,"([#804|https://github.com/stratosphere/stratosphere/issues/804] | [FLINK-804|https://issues.apache.org/jira/browse/FLINK-804])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/806
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Wed May 14 12:55:33 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:58;github-import;pull-request-806-6440748340884718970.patch;https://issues.apache.org/jira/secure/attachment/12649329/pull-request-806-6440748340884718970.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398185,,,Mon Jun 09 12:58:35 UTC 2014,,,,,,,,,,"0|i1wjx3:",398312,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:58;github-import;[Date: Wed May 14 17:56:37 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, I will merge the pull request.

I will change the type of one of the test inputs to Tuple4, though, to make sure that different types are used on the two differnt sides. That verifies that the comparators do not assume equal types.;;;","09/Jun/14 12:58;github-import;[Date: Wed May 14 18:29:09 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

BTW: in the tuple construction:

Instead of 
```
new Tuple3<Integer, Double, Long>(5, 20.0, new Long(15))
```
you can write
```
new Tuple3<Integer, Double, Long>(5, 20.0, 15L)
```;;;","09/Jun/14 12:58;github-import;[Date: Wed May 14 21:02:00 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [dc72904849873f817f0ecf3eb16871b21d4c7ec6|https://github.com/stratosphere/stratosphere/commit/dc72904849873f817f0ecf3eb16871b21d4c7ec6];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add PairComparatorTests for TuplePairComparators,FLINK-804,12719984,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:58,09/Jun/14 12:58,14/Jul/23 05:57,09/Jun/14 12:58,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/804
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Milestone: Release 0.5
Created at: Tue May 13 23:48:51 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398183,,,Mon Jun 09 12:58:27 UTC 2014,,,,,,,,,,"0|i1wjwn:",398310,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:58;github-import;[Date: Wed May 14 12:27:14 CEST 2014, Author: [zentol|https://github.com/zentol]]

Ive started working on this.;;;","09/Jun/14 12:58;github-import;[Date: Wed May 14 17:58:46 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Solved in ([#806|https://github.com/stratosphere/stratosphere/issues/806] | [FLINK-806|https://issues.apache.org/jira/browse/FLINK-806]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JAPI String Annotations,FLINK-803,12719983,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:58,09/Jun/14 12:58,14/Jul/23 05:57,09/Jun/14 12:58,,,,pre-apache,,,,,,,0,github-import,,"This PR contains the new String Annotations. Constant sets are not annotated with a string array. 
The parsing is done in the new SemanticPropUtil class. There is also a test case for this class.

What is left is additional functionality like a * operator. Also it would be nice to be able to add the annotation strings to the operators via functions.
Example:
@ConstantFields({""1->2"", ""2->3,4,5""})

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/803
Created by: [skunert|https://github.com/skunert]
Labels: java api, optimizer, 
Milestone: Release 0.5
Created at: Tue May 13 22:58:00 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:58;github-import;pull-request-803-8354763918612454914.patch;https://issues.apache.org/jira/secure/attachment/12649327/pull-request-803-8354763918612454914.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398182,,,Mon Jun 09 12:58:23 UTC 2014,,,,,,,,,,"0|i1wjwf:",398309,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:58;github-import;[Date: Thu May 15 13:43:29 CEST 2014, Author: [skunert|https://github.com/skunert]]

It is now possible to use wildcards. For example

    ""1->*""
    ""*""

Also there is the possibility to add the string annotations directly to the operators.;;;","09/Jun/14 12:58;github-import;[Date: Thu May 15 13:52:29 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

What do the wild-cards mean?;;;","09/Jun/14 12:58;github-import;[Date: Thu May 15 14:29:44 CEST 2014, Author: [skunert|https://github.com/skunert]]

    @ConstantFields{""*""} replaces the AllFieldsConstant
    @ConstantFields{""1->*""} means field 1 is forwarded to all output fields;;;","09/Jun/14 12:58;github-import;[Date: Thu May 15 14:35:40 CEST 2014, Author: [skunert|https://github.com/skunert]]

What do you think about the notation? At the moment it is implemented such that the constantfield annotation takes a String Array. One string for each inputfield. I could also implement that all constantfields are given in one string.

    1. @ConstantFields({""1->2,3"", ""2->3,4""})
    2. @ConstantFields({""1->2,3; 2->3.4"")

At the moment I like the second option better and think I should change it, how about you?;;;","09/Jun/14 12:58;github-import;[Date: Thu May 15 17:31:04 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Would it hurt to support both?;;;","09/Jun/14 12:58;github-import;[Date: Thu May 15 17:31:25 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I am merging the current state now. So please only append commits, do not squash any.;;;","09/Jun/14 12:58;github-import;[Date: Thu May 15 20:46:21 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Hmmm... The checkstyle fails for me when compiling the code.;;;","09/Jun/14 12:58;github-import;[Date: Thu May 15 20:47:06 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Tuples always use zero based indexes, here they use one-based indexes. I have changed that, because it seemed a bit off...;;;","09/Jun/14 12:58;github-import;[Date: Thu May 15 21:10:21 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I have refactored the fluent API methods on the Unary/Binary-UDF Operators. The long methods kind of break the feeling of the API, so I simplified them.;;;","09/Jun/14 12:58;github-import;[Date: Thu May 15 22:18:17 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [1e1101781ef2662a185c3b53befe0a3bb64f550a|https://github.com/stratosphere/stratosphere/commit/1e1101781ef2662a185c3b53befe0a3bb64f550a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
javadoc,FLINK-802,12719982,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:58,09/Jun/14 12:58,14/Jul/23 05:57,09/Jun/14 12:58,,,,pre-apache,,,,,,,0,github-import,,"Was created accidentally. It seems I can not delete anymore...

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/802
Created by: [jvf|https://github.com/jvf]
Labels: 
Created at: Tue May 13 21:40:54 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398181,,,Mon Jun 09 12:58:10 UTC 2014,,,,,,,,,,"0|i1wjw7:",398308,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:58;github-import;[Date: Tue May 13 22:54:49 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

no prob. ;-);;;","09/Jun/14 12:58;github-import;[Date: Wed May 14 09:52:37 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

As a side note: If you were looking for some javadocs, I have setup a website contains the JavaDocs for the latest SNAPSHOT version: http://stratosphere-javadocs.github.io/
We have not made it fully public yet since we are still working on the docs. Note that the website only contains JavaDocs for the user-facing APIs;;;","09/Jun/14 12:58;github-import;[Date: Wed May 14 10:16:43 CEST 2014, Author: [jvf|https://github.com/jvf]]

Awesome, thats what I was looking for!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Moved Java record API example jobs to stratosphere-tests,FLINK-800,12719980,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:57,09/Jun/14 12:58,14/Jul/23 05:57,09/Jun/14 12:58,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/800
Created by: [fhueske|https://github.com/fhueske]
Labels: documentation, 
Milestone: Release 0.5
Created at: Tue May 13 14:13:17 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:58;github-import;pull-request-800-1394740248701308100.patch;https://issues.apache.org/jira/secure/attachment/12649325/pull-request-800-1394740248701308100.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398179,,,Mon Jun 09 12:58:02 UTC 2014,,,,,,,,,,"0|i1wjvr:",398306,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:58;github-import;[Date: Tue May 13 14:16:59 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Will rebase;;;","09/Jun/14 12:58;github-import;[Date: Wed May 14 21:02:18 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [c41569eb01ac9ab83e17554a50ccdc7e4ac49327|https://github.com/stratosphere/stratosphere/commit/c41569eb01ac9ab83e17554a50ccdc7e4ac49327];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change GC flags for JVM,FLINK-799,12719979,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:57,09/Jun/14 12:57,14/Jul/23 05:57,09/Jun/14 12:57,,,,pre-apache,,,,,,,0,github-import,,"Due to reported problems with perm gen space, I suggest to switch the JVM flags from:

  - `-XX:+UseParNewGC`
to
  - `-XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256m`

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/799
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Tue May 13 11:08:18 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398178,,,Mon Jun 09 12:57:38 UTC 2014,,,,,,,,,,"0|i1wjvj:",398305,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:57;github-import;[Date: Wed May 14 18:06:15 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I am adding them to the startup options;;;","09/Jun/14 12:57;github-import;[Date: Wed May 14 20:03:27 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [81522149f834512bd1209d3dd24c503363274cf2|https://github.com/stratosphere/stratosphere/commit/81522149f834512bd1209d3dd24c503363274cf2];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Basic Type and Tuple Comparators,FLINK-798,12719978,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:57,09/Jun/14 12:57,14/Jul/23 05:57,09/Jun/14 12:57,,,,pre-apache,,,,,,,0,github-import,,"Implemented with @zentol 

Set's up on pending PR https://github.com/stratosphere/stratosphere/pull/775
Related to issue https://github.com/stratosphere/stratosphere/issues/772

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/798
Created by: [filiphaase|https://github.com/filiphaase]
Labels: 
Created at: Tue May 13 09:19:34 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:57;github-import;pull-request-798-3483704941246732526.patch;https://issues.apache.org/jira/secure/attachment/12649324/pull-request-798-3483704941246732526.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398177,,,Mon Jun 09 12:57:35 UTC 2014,,,,,,,,,,"0|i1wjvb:",398304,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:57;github-import;[Date: Wed May 14 00:01:10 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good, I will merge it!

BTW: If you want multiple serializer/comparator tests with a TestBase to run in one unit test file, you can have a look at how the serializers do it with `SerializerTestBase` and `SerializerTestInstance`.;;;","09/Jun/14 12:57;github-import;[Date: Wed May 14 05:30:00 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [97ef44db1a5c8ade00a26eb32ff410e35229b5ca|https://github.com/stratosphere/stratosphere/commit/97ef44db1a5c8ade00a26eb32ff410e35229b5ca];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose WriteMode in JAPI. ,FLINK-797,12719977,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:57,09/Jun/14 12:57,14/Jul/23 05:57,09/Jun/14 12:57,,,,pre-apache,,,,,,,0,github-import,,"Fix issue ([#781|https://github.com/stratosphere/stratosphere/issues/781] | [FLINK-781|https://issues.apache.org/jira/browse/FLINK-781]).

Pass Configuration parameters to runtime operators in JAPI (+test)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/797
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue May 13 08:22:00 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:57;github-import;pull-request-797-7366935685409456823.patch;https://issues.apache.org/jira/secure/attachment/12649323/pull-request-797-7366935685409456823.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398176,,,Mon Jun 09 12:57:28 UTC 2014,,,,,,,,,,"0|i1wjv3:",398303,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:57;github-import;[Date: Tue May 13 15:27:09 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I addressed @fhueske's comment.;;;","09/Jun/14 12:57;github-import;[Date: Tue May 13 23:03:36 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Can you add the same functionality for writeAsText() as well?;;;","09/Jun/14 12:57;github-import;[Date: Wed May 14 09:51:06 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Yes, I'll rework the pull request.


On Tue, May 13, 2014 at 11:03 PM, Fabian Hueske <notifications@github.com>wrote:

> Can you add the same functionality for writeAsText() as well?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/797#issuecomment-43012501>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","09/Jun/14 12:57;github-import;[Date: Wed May 14 22:33:59 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay, ready to review again.;;;","09/Jun/14 12:57;github-import;[Date: Wed May 14 22:42:36 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

:+1:
Looks good to me. ;;;","09/Jun/14 12:57;github-import;[Date: Thu May 15 09:37:13 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

the pr now also contains a fix for https://github.com/stratosphere/stratosphere/issues/811;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possibly extend the cost model of the optimizer,FLINK-795,12719975,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mholzemer,github-import,github-import,09/Jun/14 12:57,09/Jul/14 10:33,14/Jul/23 05:57,09/Jul/14 10:33,,,,0.6-incubating,,,,,,,0,github-import,,"I have started the task to integrate the AbstractCachedBuildSideMatchDriver into the optimizer. The driver caches one side of the join and thereby can accellerate iterations if there are joins with static (non-changing) datasets inside the iteration.

The current way of calculating the cost of operators inside of iterations is basically to multiply them by the number of iterations. I would like to propose to extend this to have one static part of costs, that is counted only once for all iterations, and one dynamic part that is multiplied by the number of iterations.
In my opinion that would be the cleanest way to intergrate the cached match, by assigning it a higher starting cost then the regular match and a cheaper dynamic part.

One other approach would be to always use the cached match inside of iterations. For that I would probably have to add a new RequestedLocalProperty that tells the optimizer if the operator is used inside of a iteration.
A simple hacked solution could also be to simply exchange all suitable regular matches inside of an iteration by the cached alternative.

What do you think is the best approach?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/795
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon May 12 18:51:51 CEST 2014
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398174,,,Wed Jul 09 10:33:42 UTC 2014,,,,,,,,,,"0|i1wjun:",398301,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:57;github-import;[Date: Tue May 13 11:12:40 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I had a deeper look at this issue and now I think adjusting the cost model is the wrong way to go.
I will proceed with exchanging regular joins with cached ones inside of iterations when applicable.;;;","09/Jun/14 12:57;github-import;[Date: Tue May 13 11:16:04 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

For the cost calculation, you need to incorporate whether the build side is
on the dynamic or static path.;;;","09/Jun/14 12:57;github-import;[Date: Tue May 13 13:55:08 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

Ok, so if I do understand it correctly I exchange the driver in the NepheleJobGraphGenerator if one side of the match inside of an iteration is on a static path.;;;","09/Jul/14 10:33;sewen;Implemented in 99c888c7b775e1c3c910b9b4a45bbae5102bac14 and 03c6160e47bc2ab613a2734388aa22f83323e6a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Small fix to get test to run under windows,FLINK-794,12719974,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:57,09/Jun/14 12:57,14/Jul/23 05:57,09/Jun/14 12:57,,,,pre-apache,,,,,,,0,github-import,,"This is a small fix to get the current version of stratosphere compiled under windows. I'm using the toUri() method instead of hardcoding the file URI.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/794
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon May 12 17:34:47 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:57;github-import;pull-request-794-564783839917239751.patch;https://issues.apache.org/jira/secure/attachment/12649322/pull-request-794-564783839917239751.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398173,,,Mon Jun 09 12:57:12 UTC 2014,,,,,,,,,,"0|i1wjuf:",398300,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:57;github-import;[Date: Mon May 12 21:37:17 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good, will merge;;;","09/Jun/14 12:57;github-import;[Date: Tue May 13 00:45:56 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [05853623a15cda7932ccf9dbf12691ac6727e8ff|https://github.com/stratosphere/stratosphere/commit/05853623a15cda7932ccf9dbf12691ac6727e8ff];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable LocalExecutor when using Client,FLINK-793,12719973,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:57,09/Jun/14 12:57,14/Jul/23 05:57,09/Jun/14 12:57,,,,pre-apache,,,,,,,0,github-import,,"If you submit a job using the Client, but use the LocalExecutor in your job by accident you can get weird results.
This pull requests makes the LocalExecutor throw an exception if an instance of Client was created before.
I'm not 100% happy with this implementation, since it absolutely prevents you from using LocalExecutor when you also want to create an Client instance, and perhaps there are some cases when you want to do this. What do you think?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/793
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon May 12 17:33:07 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:57;github-import;pull-request-793-3810583623395346785.patch;https://issues.apache.org/jira/secure/attachment/12649321/pull-request-793-3810583623395346785.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398172,,,Mon Jun 09 12:57:07 UTC 2014,,,,,,,,,,"0|i1wju7:",398299,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:57;github-import;[Date: Mon May 12 21:46:57 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you make the methods to deactivate the local and remote environment protected methods? That way they are not visible for the users. You can still call them from the `ContextEnvironment`. Have a look at how the `ContextEnvironment` is set.;;;","09/Jun/14 12:57;github-import;[Date: Tue May 13 10:26:13 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I made the method in ExecutionEnvironment protected and call it from another method in ContextEnvironment, hope that is what you meant.
Currently there is only a method to deactivate the local environmnet.;;;","09/Jun/14 12:57;github-import;[Date: Wed May 14 15:07:05 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good, will merge this now.;;;","09/Jun/14 12:57;github-import;[Date: Wed May 14 20:02:56 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

merged in [4b8c3e899bb2f2b9ab801a455f7f4b052537af34|https://github.com/stratosphere/stratosphere/commit/4b8c3e899bb2f2b9ab801a455f7f4b052537af34];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve exceptions of the GenericCsvInputFormat,FLINK-792,12719972,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:56,09/Jun/14 12:57,14/Jul/23 05:57,09/Jun/14 12:57,,,,pre-apache,,,,,,,0,github-import,,"While debugging the issues of the user mentioned here: https://github.com/stratosphere/stratosphere/issues/791 I found out that the exceptions are not very helpful.

The exception will now contain the file path and the expected types.
I also added a very simple testcase (which is not really relevant but covers the ""standard operation""). (and needs no time)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/792
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon May 12 15:43:15 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:57;github-import;pull-request-792-3271513799964119159.patch;https://issues.apache.org/jira/secure/attachment/12649319/pull-request-792-3271513799964119159.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398171,,,Mon Jun 09 12:57:01 UTC 2014,,,,,,,,,,"0|i1wjtz:",398298,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:57;github-import;[Date: Mon May 12 21:50:29 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good, will merge;;;","09/Jun/14 12:57;github-import;[Date: Tue May 13 00:46:15 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [1c142484a3716712920867dd6faf8cd933696408|https://github.com/stratosphere/stratosphere/commit/1c142484a3716712920867dd6faf8cd933696408];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows line endings not working in CSVInputFormat,FLINK-791,12719971,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:56,30/Jun/14 09:39,14/Jul/23 05:57,30/Jun/14 09:39,,,,pre-apache,,,,,,,0,github-import,,"A user reported this issue, who is running stratosphere under windows. After converting the input file to UNIX line endings it worked.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/791
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon May 12 15:34:40 CEST 2014
State: open
",,github-import,mholzemer,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398170,,,Mon Jun 30 09:39:15 UTC 2014,,,,,,,,,,"0|i1wjtr:",398297,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:56;github-import;[Date: Mon May 12 15:37:53 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I'm currently preparing a pull request to ease debugging these issues.;;;","09/Jun/14 12:56;github-import;[Date: Mon May 12 15:57:36 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I had a look at this issue and for me CsvInputFormat seems to work as intended. The problem probably is, that the DEFAULT_LINE_DELIMITER is set to ""\n"", but files created under windows are using ""\r\n"".
I am not sure how to deal with this. We could set the default delimiter to System.getProperty(""line.separator""); but then for example files checkout out from git would not be running by default because git is keeping them in linux format.
Another possibility would be to accept both by default and dynamically remove a tailing ""\r"" if it is present.;;;","09/Jun/14 12:56;github-import;[Date: Mon May 12 16:32:29 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I think its okay to accept both IF the user is Windows.;;;","09/Jun/14 12:56;github-import;[Date: Mon May 12 17:26:57 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

We had a fix for this. Did it get lost during some merge?;;;","09/Jun/14 12:56;github-import;[Date: Mon May 12 17:29:21 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thats the pull request: https://github.com/stratosphere/stratosphere/pull/582;;;","09/Jun/14 12:56;github-import;[Date: Mon May 12 17:30:42 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Have a look at ([#582|https://github.com/stratosphere/stratosphere/issues/582] | [FLINK-582|https://issues.apache.org/jira/browse/FLINK-582]);;;","18/Jun/14 15:38;rmetzger;[~mholzemer], whats the status of this issue?;;;","30/Jun/14 09:39;mholzemer;This issue was fixed some time ago by https://github.com/stratosphere/stratosphere/pull/825;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortedGrouping to move the group order from the Grouping,FLINK-790,12719970,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:56,09/Jun/14 12:56,14/Jul/23 05:57,09/Jun/14 12:56,,,,pre-apache,,,,,,,0,github-import,,"See ([#783|https://github.com/stratosphere/stratosphere/issues/783] | [FLINK-783|https://issues.apache.org/jira/browse/FLINK-783])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/790
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon May 12 12:07:24 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:56;github-import;pull-request-790-3090687912416189251.patch;https://issues.apache.org/jira/secure/attachment/12649318/pull-request-790-3090687912416189251.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398169,,,Mon Jun 09 12:56:50 UTC 2014,,,,,,,,,,"0|i1wjtj:",398296,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:56;github-import;[Date: Thu May 15 13:50:04 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Will merge this.;;;","09/Jun/14 12:56;github-import;[Date: Thu May 15 14:08:24 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged in [698bcc6f3e1dda5deba3eff4b46fe1c6b6391b7e|https://github.com/stratosphere/stratosphere/commit/698bcc6f3e1dda5deba3eff4b46fe1c6b6391b7e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Created new SortedGrouping to move the group order from the Grouping,FLINK-789,12719969,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:56,09/Jun/14 12:56,14/Jul/23 05:57,09/Jun/14 12:56,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/789
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon May 12 09:55:30 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:56;github-import;pull-request-789-2708587164679066465.patch;https://issues.apache.org/jira/secure/attachment/12649317/pull-request-789-2708587164679066465.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398168,,,Mon Jun 09 12:56:44 UTC 2014,,,,,,,,,,"0|i1wjtb:",398295,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:56;github-import;[Date: Mon May 12 10:00:02 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I accidentally clicked on create pull request when I just wanted to review my changes. I will reopen when my tests passed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkstyle marks imports as unused when only referred to in javadocs,FLINK-786,12719966,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:56,09/Jun/14 12:56,14/Jul/23 05:57,09/Jun/14 12:56,,,,pre-apache,,,,,,,0,github-import,,"You can import a class to use its short name in javadocs links. Checkstyle fails with an unused import error then.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/786
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Sun May 11 18:16:47 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398165,,,Mon Jun 09 12:56:35 UTC 2014,,,,,,,,,,"0|i1wjsn:",398292,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:56;github-import;[Date: Sun May 11 18:17:50 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

This pull request contains a fix for that: https://github.com/stratosphere/stratosphere/pull/779/files#diff-15;;;","09/Jun/14 12:56;github-import;[Date: Sun May 11 18:43:57 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Ah, nice! Will merge as soon as the patch to divide reduce and groupReduce is done.;;;","09/Jun/14 12:56;github-import;[Date: Wed May 14 05:32:53 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Solved in [340ebde779d5028dfdf38b2bed40566d57985335|https://github.com/stratosphere/stratosphere/commit/340ebde779d5028dfdf38b2bed40566d57985335];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bug for tuple size greater than 22,FLINK-784,12719964,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:56,09/Jun/14 12:56,14/Jul/23 05:57,09/Jun/14 12:56,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/784
Created by: [qmlmoon|https://github.com/qmlmoon]
Labels: 
Created at: Sun May 11 10:01:16 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:56;github-import;pull-request-784-6257000790954216346.patch;https://issues.apache.org/jira/secure/attachment/12649316/pull-request-784-6257000790954216346.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398163,,,Mon Jun 09 12:56:27 UTC 2014,,,,,,,,,,"0|i1wjs7:",398290,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:56;github-import;[Date: Tue May 13 00:45:32 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [cbed2a13b8417c5f78f4f0ac1cea2f9b6e4f5526|https://github.com/stratosphere/stratosphere/commit/cbed2a13b8417c5f78f4f0ac1cea2f9b6e4f5526];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move the group order from the Grouping,FLINK-783,12719963,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:56,09/Jun/14 12:56,14/Jul/23 05:57,09/Jun/14 12:56,,,,pre-apache,,,,,,,0,github-import,,"Currently, the `Grouping` class (used for grouped reduce, reduceGroup, aggregate, coGroup) has the option to specify a group order. Half of the operators need to throw an exception when encountering it because they do not support group order.

I would suggest to move this either to the operators that support group order (`reduceGroup`, `coGroup`) or to a subclass of `Grouping` that is created for these operators.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/783
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Sat May 10 21:11:52 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398162,,,Mon Jun 09 12:56:23 UTC 2014,,,,,,,,,,"0|i1wjrz:",398289,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:56;github-import;[Date: Sat May 10 21:12:54 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I would vote to make a subclass of `Grouping`.;;;","09/Jun/14 12:56;github-import;[Date: Sat May 10 21:52:24 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

:+1:;;;","09/Jun/14 12:56;github-import;[Date: Mon May 12 09:38:11 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

Good idea! Just a small remark: I think coGroup is at the moment not implemented through Grouping but as a first class function at DataSet. So reduceGroup is currently the only operator accessed through Grouping that can be sorted.;;;","09/Jun/14 12:56;github-import;[Date: Mon May 12 10:16:02 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Very good point. We should also support sorted groups for CoGroup. 
I would make this a separate issue though.;;;","09/Jun/14 12:56;github-import;[Date: Thu May 15 14:08:49 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed with ([#790|https://github.com/stratosphere/stratosphere/issues/790] | [FLINK-790|https://issues.apache.org/jira/browse/FLINK-790]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parametrize aggregators and convergence criteria,FLINK-782,12719962,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,github-import,github-import,09/Jun/14 12:56,13/Jun/14 14:45,14/Jul/23 05:57,13/Jun/14 14:45,,,,0.6-incubating,,,,,,,0,github-import,,"Changes so that aggregators and convergence criteria are registered using an object, rather than a class. 
This allows to pass parameters to aggregators and convergence criteria (see ([#731|https://github.com/stratosphere/stratosphere/issues/731] | [FLINK-731|https://issues.apache.org/jira/browse/FLINK-731])).
Also, enables registering aggregators in Delta iterations.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/782
Created by: [vasia|https://github.com/vasia]
Labels: 
Created at: Fri May 09 20:06:32 CEST 2014
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:56;github-import;pull-request-782-5777379156518943406.patch;https://issues.apache.org/jira/secure/attachment/12649315/pull-request-782-5777379156518943406.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398161,,,Fri Jun 13 14:45:53 UTC 2014,,,,,,,,,,"0|i1wjrr:",398288,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:56;github-import;[Date: Wed May 14 20:09:13 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good. I think we will merge this after the 0.5 release. Feature freeze for now ;-);;;","13/Jun/14 14:45;sewen;Merged in 08f189ad37a64b094ba86def8544687419c131ce

This was a contribution from Vasia Kalavri. The merge/rebase screwed up the commit author.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Introduce TypeInformation at lowest Operator level,FLINK-780,12719960,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:55,09/Jun/14 12:56,14/Jul/23 05:57,09/Jun/14 12:56,,,,pre-apache,,,,,,,0,github-import,,"This touches a LOT of stuff but will allow the removal of the separate
JavaAPI plan operators and other redundant code.

There is a new RecordTypeInfo that operators create behind the scenes.
The *Source and *Sink operators are now Record specific as to not break
existing programs. There are agnostic base classes with the *Base
suffix. Same is true for the iteration operators. Those operators and
sources/sinks are now in stratosphere-java, only the agnostic ones are
in stratosphere-core.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/780
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri May 09 16:21:50 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:55;github-import;pull-request-780-3868966401694297281.patch;https://issues.apache.org/jira/secure/attachment/12649314/pull-request-780-3868966401694297281.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398159,,,Mon Jun 09 12:56:05 UTC 2014,,,,,,,,,,"0|i1wjrb:",398286,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:56;github-import;[Date: Fri May 09 16:55:11 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

Please do not merge this, I think I came upon a way to also add input type information in a not to annoying way. Let me see how that goes.;;;","09/Jun/14 12:56;github-import;[Date: Fri May 09 17:03:08 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

Instead of adding only a TypeInformation<T> to Operator I would like to create a new OperatorType (plus BinaryOperatorType and UnaryOperatorType) that gets attached to an Operator. This can then hold the information that we want operators to have. Otherwise, this stuff will keep blowing up all the constructors if we decide to add input type information or other stuff later.

Alternatively, I could extend the user code wrapper to hold this information, similarly to how the scala API does it.

What do you think?

(The only problem I see with the user code wrapper approach is that not all types of operators have user code attached. They could retrieve it from the actual user code operator that represents their input.);;;","09/Jun/14 12:56;github-import;[Date: Tue May 13 20:19:49 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

I added another commit that introduces OperatorInformation, so that they can have more than just a TypeInformation for the output type.;;;","09/Jun/14 12:56;github-import;[Date: Tue May 13 22:38:03 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

I squashed it into one commit so that it is easier for people to see the differences.;;;","09/Jun/14 12:56;github-import;[Date: Thu May 15 13:38:10 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

Rebased on top of current master.;;;","09/Jun/14 12:56;github-import;[Date: Fri May 16 14:41:48 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

Rebased it on top of current master again. This is getting somewhat annoying, because people keep adding stuff to the now-removed Plan*Operators.;;;","09/Jun/14 12:56;github-import;[Date: Fri May 16 15:05:50 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Don't rebase it again. Will merge it today...;;;","09/Jun/14 12:56;github-import;[Date: Tue May 20 15:41:41 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

I incorporated @StephanEwen's fixes, rebased it on top of HEAD again and fixed scala warnings.;;;","09/Jun/14 12:56;github-import;[Date: Wed May 21 22:14:17 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Ouch. I was not aware that this pull request is again breaking the existing codebase for our old java API users. `FileDataSink` and `FileDataSource` were moved out of the `eu.stratosphere.api.common.operators` package, right?

Users of the old java API have to change their packages from
0.2 to 0.4
0.4 to 0.5
and since everybody has to:
0.5 to 0.6

I thought we are able to avoid this mess at least for the 0.5 release.

Do you think we should not add this change to the 0.5 release?;;;","09/Jun/14 12:56;github-import;[Date: Wed May 21 22:21:47 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think this is only a package move.

If it eases the migration, lets add a copy of the record API FileDataSource
in eu.stratosphere.api.common.operators in addition to the
GenericDataSourceBase.

We could mark it as deprecated and carry it along for one release.

I would prefer that rather than undoing that monster patch. It was a lot of
work and affects so much that is almost immediately goes out of sync.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add some more javadocs documentation to user-facing classes.,FLINK-779,12719959,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:55,09/Jun/14 12:55,14/Jul/23 05:57,09/Jun/14 12:55,,,,pre-apache,,,,,,,0,github-import,,"Some of the comments are copied (and adopted) from the old API.
I'm not so sure if this pull request is really helpful, but I already made the work, so I should at least try it ;)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/779
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Fri May 09 13:32:37 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:55;github-import;pull-request-779-1022563524204602474.patch;https://issues.apache.org/jira/secure/attachment/12649313/pull-request-779-1022563524204602474.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398158,,,Mon Jun 09 12:55:41 UTC 2014,,,,,,,,,,"0|i1wjr3:",398285,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:55;github-import;[Date: Wed May 14 05:30:22 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [340ebde779d5028dfdf38b2bed40566d57985335|https://github.com/stratosphere/stratosphere/commit/340ebde779d5028dfdf38b2bed40566d57985335];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add test for deflate input,FLINK-778,12719958,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:55,09/Jun/14 12:55,14/Jul/23 05:57,09/Jun/14 12:55,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/778
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Fri May 09 13:23:46 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:55;github-import;pull-request-778-8091699443719534255.patch;https://issues.apache.org/jira/secure/attachment/12649312/pull-request-778-8091699443719534255.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398157,,,Mon Jun 09 12:55:37 UTC 2014,,,,,,,,,,"0|i1wjqv:",398284,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:55;github-import;[Date: Tue May 13 00:45:12 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [e2fb213adcf7a6c110b0f5e399537e6fdde24fad|https://github.com/stratosphere/stratosphere/commit/e2fb213adcf7a6c110b0f5e399537e6fdde24fad];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TupleComparator fixed,FLINK-775,12719955,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:55,09/Jun/14 12:55,14/Jul/23 05:57,09/Jun/14 12:55,,,,pre-apache,,,,,,,0,github-import,,"The TupleComparator now:
- actually uses the key positions passed
- can skip positions
- supports any ordering ( you can compare on the second, and afterward on the first field )

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/775
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Thu May 08 18:03:56 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:55;github-import;pull-request-775-6046479862917886571.patch;https://issues.apache.org/jira/secure/attachment/12649309/pull-request-775-6046479862917886571.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398154,,,Mon Jun 09 12:55:06 UTC 2014,,,,,,,,,,"0|i1wjq7:",398281,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:55;github-import;[Date: Thu May 08 18:05:21 CEST 2014, Author: [zentol|https://github.com/zentol]]

@StephanEwen the special comparison we talked about, that we should use a faster compare method if you only use the first field as a key, was already implemented in the TupleTypeInfo.createComparator() method. In this case it returns a TupleSingleFieldComparator.;;;","09/Jun/14 12:55;github-import;[Date: Thu May 08 19:27:40 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, very well. I remember that I did this a while ago, now that you say it :D

Will merge this.;;;","09/Jun/14 12:55;github-import;[Date: Thu May 08 21:42:00 CEST 2014, Author: [zentol|https://github.com/zentol]]

i accidentally clicked close...;;;","09/Jun/14 12:55;github-import;[Date: Fri May 09 23:26:45 CEST 2014, Author: [zentol|https://github.com/zentol]]

in case it was missed, Ive added a commit that fixed a few issues causing some tests to fail.;;;","09/Jun/14 12:55;github-import;[Date: Wed May 14 05:31:02 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [18f95d2c6212c3306619663571ad3b73b8f7dd5d|https://github.com/stratosphere/stratosphere/commit/18f95d2c6212c3306619663571ad3b73b8f7dd5d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WordCount examples are documented with many comments in order to be very user-friendly.,FLINK-774,12719954,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:54,09/Jun/14 12:55,14/Jul/23 05:57,09/Jun/14 12:55,,,,pre-apache,,,,,,,0,github-import,,"See ([#773|https://github.com/stratosphere/stratosphere/issues/773] | [FLINK-773|https://issues.apache.org/jira/browse/FLINK-773]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/774
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Thu May 08 17:25:39 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;pull-request-774-8180002997296450448.patch;https://issues.apache.org/jira/secure/attachment/12649308/pull-request-774-8180002997296450448.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398153,,,Mon Jun 09 12:54:59 UTC 2014,,,,,,,,,,"0|i1wjpz:",398280,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;[Date: Fri May 09 13:28:01 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

Good idea to add some comment! Looks good to merge for me.;;;","09/Jun/14 12:54;github-import;[Date: Fri May 09 13:39:39 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [aa289b793a00ea9a653d16197beca5ed7e335aa2|https://github.com/stratosphere/stratosphere/commit/aa289b793a00ea9a653d16197beca5ed7e335aa2];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document WordCount example,FLINK-773,12719953,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:54,09/Jun/14 12:54,14/Jul/23 05:57,09/Jun/14 12:54,,,,pre-apache,,,,,,,0,github-import,,"The word count examples in ```eu.stratosphere.example.java.wordcount``` neither have description nor comments.

Additionally ```.split(""\\W"");``` should better be ```.split(""\\W+"");``` for better results.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/773
Created by: [twalthr|https://github.com/twalthr]
Labels: documentation, java api, 
Milestone: Release 0.5
Assignee: [twalthr|https://github.com/twalthr]
Created at: Thu May 08 12:42:41 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398152,,,Mon Jun 09 12:54:54 UTC 2014,,,,,,,,,,"0|i1wjpr:",398279,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;[Date: Thu May 08 12:47:56 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you make a pull request for this?;;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 12:50:57 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

Yes I will do it later. You can assign this issue to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests for the type comparators,FLINK-772,12719952,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:54,09/Jun/14 12:54,14/Jul/23 05:57,09/Jun/14 12:54,,,,pre-apache,,,,,,,0,github-import,,"We need tests for the comparators (all subclasses of `TypeComparator`)

  - Basic types
  - Value types
  - CopyableValue types
  - Generic types

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/772
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: java api, runtime, testing, 
Milestone: Release 0.5
Created at: Thu May 08 00:35:06 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398151,,,Mon Jun 09 12:54:49 UTC 2014,,,,,,,,,,"0|i1wjpj:",398278,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;[Date: Wed May 14 05:33:53 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

 - BasicType is done
 - Tuple Type is done

Pending:
  - Value / Generic (should be equivalent)
;;;","09/Jun/14 12:54;github-import;[Date: Thu May 15 14:39:23 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Added generic comparator tests with [1b6a3acfb7d1991ad8e76425e31792d38403b136|https://github.com/stratosphere/stratosphere/commit/1b6a3acfb7d1991ad8e76425e31792d38403b136] and value comparator tests with [4cdc6335910fcf0612585e8d787b7980641285c0|https://github.com/stratosphere/stratosphere/commit/4cdc6335910fcf0612585e8d787b7980641285c0].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More detailed error messages for CompactingHashTable,FLINK-771,12719951,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:54,09/Jun/14 12:54,14/Jul/23 05:57,09/Jun/14 12:54,,,,pre-apache,,,,,,,0,github-import,,"Every time compaction or insertion fails because of too few MemorySegments exception now contains number of partitions, number of segments in largest and smallest partition and number of segments used for buckets.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/771
Created by: [rwaury|https://github.com/rwaury]
Labels: 
Created at: Wed May 07 23:04:32 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;pull-request-771-7787555855351671503.patch;https://issues.apache.org/jira/secure/attachment/12649307/pull-request-771-7787555855351671503.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398150,,,Mon Jun 09 12:54:45 UTC 2014,,,,,,,,,,"0|i1wjpb:",398277,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;[Date: Wed May 07 23:25:00 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Wow, that was quick, thanks!;;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 00:45:09 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [34ab689ad210b7448d16e647296dc6c671c9bf76|https://github.com/stratosphere/stratosphere/commit/34ab689ad210b7448d16e647296dc6c671c9bf76];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test case for showing the still existing BulkIteration bug.,FLINK-770,12719950,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:54,09/Jun/14 12:54,14/Jul/23 05:57,09/Jun/14 12:54,,,,pre-apache,,,,,,,0,github-import,,"Shows the bug mentioned in ([#769|https://github.com/stratosphere/stratosphere/issues/769] | [FLINK-769|https://issues.apache.org/jira/browse/FLINK-769]).

The lines 

```java
while(records.hasNext()) {
    records.next();
}
```
should not be necessary.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/770
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Wed May 07 19:50:47 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;pull-request-770-5430126342919175870.patch;https://issues.apache.org/jira/secure/attachment/12649306/pull-request-770-5430126342919175870.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398149,,,Mon Jun 09 12:54:40 UTC 2014,,,,,,,,,,"0|i1wjp3:",398276,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;[Date: Wed May 07 23:36:21 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, there is a different issue than the last time.
This here is about non exhausted non-iterative inputs. The cache caches ""to the side"", so it only caches what the first iteration consumed. The second iteration has only that data available.;;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 00:44:25 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [982e1e0905dcdce71977cf408b6fac97b6f8f45c|https://github.com/stratosphere/stratosphere/commit/982e1e0905dcdce71977cf408b6fac97b6f8f45c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkIteration does not work correctly if not all records are consumed,FLINK-769,12719949,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:54,09/Jun/14 12:54,14/Jul/23 05:57,09/Jun/14 12:54,,,,pre-apache,,,,,,,0,github-import,,"This is a old but still existing bug. A similar bug already occurred with DeltaIteration.

A test case that shows the bug is following in a minute.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/769
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Wed May 07 19:48:49 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398148,,,Mon Jun 09 12:54:35 UTC 2014,,,,,,,,,,"0|i1wjov:",398275,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;[Date: Thu May 08 00:44:42 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [d56565c31e3cf4af8888e77a4de9e0b0dfa08dea|https://github.com/stratosphere/stratosphere/commit/d56565c31e3cf4af8888e77a4de9e0b0dfa08dea];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for deflate files,FLINK-768,12719948,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:54,09/Jun/14 12:54,14/Jul/23 05:57,09/Jun/14 12:54,,,,pre-apache,,,,,,,0,github-import,,"See issue https://github.com/stratosphere/stratosphere/issues/763
Made Avro input formats unsplittable
Implement clear distinction between unsplittable and serial inputs

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/768
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Wed May 07 18:18:26 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;pull-request-768-8907423822488858410.patch;https://issues.apache.org/jira/secure/attachment/12649305/pull-request-768-8907423822488858410.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398147,,,Mon Jun 09 12:54:31 UTC 2014,,,,,,,,,,"0|i1wjon:",398274,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;[Date: Wed May 07 18:48:22 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

The optimizer / statistics problem has been resolved (and tested).
I need to push a newer version because I forgot to refactor in the `jdbc` package.

I'm now testing for correctness.;;;","09/Jun/14 12:54;github-import;[Date: Wed May 07 22:14:49 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Do not merge this. There seems to be an issue with reading the compressed files.;;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 13:34:32 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Any news here?;;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 13:36:47 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Yes ;) I resolved the issue with the missing data: The DelimitedInputFormat is reading until ""file size"" basically. Since the file is compressed, it will not ready everything. But I resolved this.
Now the AvroInputFormat has some issues, but I'm on it.;;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 14:00:41 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I resolved all known issues. 
Now I'm waiting for travis for executing all test cases.
I also tested the code on the cluster. (not for Avro);;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 16:17:37 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Build has passed;;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 17:59:37 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [ef040d46347d7d9cdd7c11526200d74bbe100fe4|https://github.com/stratosphere/stratosphere/commit/ef040d46347d7d9cdd7c11526200d74bbe100fe4];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement extractInputFormatTypes() for TypeExtractor ,FLINK-767,12719947,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:54,09/Jun/14 12:54,14/Jul/23 05:57,09/Jun/14 12:54,,,,pre-apache,,,,,,,0,github-import,,"TypeExtractor.extractInputFormatTypes() is not implemented yet.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/767
Created by: [twalthr|https://github.com/twalthr]
Labels: java api, 
Milestone: Release 0.5
Assignee: [twalthr|https://github.com/twalthr]
Created at: Wed May 07 13:45:18 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398146,,,Mon Jun 09 12:54:22 UTC 2014,,,,,,,,,,"0|i1wjof:",398273,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;[Date: Thu May 08 13:26:15 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I will take a quick stab at this.;;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 13:32:52 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

I also can do it since I need it later this afternoon for the Hadoop Input Format.;;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 13:34:04 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Sounds good.;;;","09/Jun/14 12:54;github-import;[Date: Thu May 15 22:36:18 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in ([#776|https://github.com/stratosphere/stratosphere/issues/776] | [FLINK-776|https://issues.apache.org/jira/browse/FLINK-776]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generalised the name of the expected InputSplit class in HadoopDataSource.,FLINK-765,12719945,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:54,09/Jun/14 12:54,14/Jul/23 05:57,09/Jun/14 12:54,,,,pre-apache,,,,,,,0,github-import,,"In order to get the name of an ```InputSplit``` class  ```getCanonicalName()``` has been used so far in ```HadoopInputSplitWrapper```. This is ok for most cases, except for the case when the ```InputSplit``` is a nested class inside its ```InputFormat``` class. 

Specifically, here is an exact case:  https://github.com/Parquet/parquet-mr/blob/master/parquet-hadoop/src/main/java/parquet/hadoop/mapred/DeprecatedParquetInputFormat.java

This fixes https://github.com/stratosphere/stratosphere-experimental/issues/3

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/765
Created by: [atsikiridis|https://github.com/atsikiridis]
Labels: 
Created at: Wed May 07 02:11:39 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;pull-request-765-4393504822179344615.patch;https://issues.apache.org/jira/secure/attachment/12649304/pull-request-765-4393504822179344615.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398144,,,Mon Jun 09 12:54:12 UTC 2014,,,,,,,,,,"0|i1wjnz:",398271,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;[Date: Wed May 07 08:23:51 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thanks for your pull request.
Good to merge.;;;","09/Jun/14 12:54;github-import;[Date: Wed May 07 12:35:20 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [55a19fb2a39d351ee2a08ff572078e83d6e4a5a1|https://github.com/stratosphere/stratosphere/commit/55a19fb2a39d351ee2a08ff572078e83d6e4a5a1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BasicType-/BooleanComparator consistent comparison,FLINK-764,12719944,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:54,09/Jun/14 12:54,14/Jul/23 05:57,09/Jun/14 12:54,,,,pre-apache,,,,,,,0,github-import,,"([#761|https://github.com/stratosphere/stratosphere/issues/761] | [FLINK-761|https://issues.apache.org/jira/browse/FLINK-761])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/764
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Tue May 06 13:53:19 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;pull-request-764-221736581784718331.patch;https://issues.apache.org/jira/secure/attachment/12649303/pull-request-764-221736581784718331.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398143,,,Mon Jun 09 12:54:06 UTC 2014,,,,,,,,,,"0|i1wjnr:",398270,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:54;github-import;[Date: Tue May 06 14:34:41 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The current order is important to make sure that a requested ascending comparison is really ascending. If you switch the order in this one place, then a group order of ascending over boolean will no longer be a ascending, because the code assumes the current behavior.;;;","09/Jun/14 12:54;github-import;[Date: Tue May 06 19:29:44 CEST 2014, Author: [zentol|https://github.com/zentol]]

This is actually a larger issue than i expected. for *any* class extending from TypeComparator, or any of its subclasses, it is basically a coin flip how compareToReference is implemented. Even classes at the very top (for example the TupleComparator) can disagree.;;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 00:36:13 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It may be that this comes from the fact that different people implemented those comparators. Since they are not yet tested, the build accepted the behavior.

To  fix this, have a look at the way the `RecordComparator` does it. That one is consistent with the way the runtime expects the behavior to correctly produce ascending/descending orders.;;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 14:23:11 CEST 2014, Author: [zentol|https://github.com/zentol]]

it seems like i made a mistake and created an issue were none actually exists. 

The TypeComparator.cTF() methods are fine. The BooleanComparator should be fixed though.

Turns out there are only 2 groups of Comparators that uses a different order than the TypeComparator, namely TupleComparators and a few in the test packages of runtime. 
Since the TupleComparators only delegate the comparisons to others, no change is necessary here. 
The ones used in the tests *could* be changed, but i doubt its necessary.;;;","09/Jun/14 12:54;github-import;[Date: Thu May 08 17:59:48 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

merged in [8e72db9f1935ce6ee063f5a23cbc544cc1dc3210|https://github.com/stratosphere/stratosphere/commit/8e72db9f1935ce6ee063f5a23cbc544cc1dc3210];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More generated Tuples up to 25.,FLINK-762,12719942,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:53,09/Jun/14 12:53,14/Jul/23 05:57,09/Jun/14 12:53,,,,pre-apache,,,,,,,0,github-import,,"I had a use case where I needed 24-tuples. @qmlmoon also needed a larger arity for the testjob. 
(I can't find the issue with the discussion at the moment.)

I think 25 is a nice number.

This PR also corrects a small issue of the TupleGenerator together with the new checkstyle.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/762
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Tue May 06 13:01:11 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:53;github-import;pull-request-762-6716838508788063025.patch;https://issues.apache.org/jira/secure/attachment/12649302/pull-request-762-6716838508788063025.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398141,,,Mon Jun 09 12:53:56 UTC 2014,,,,,,,,,,"0|i1wjnb:",398268,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:53;github-import;[Date: Wed May 07 19:14:36 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

I introduced the possibility to skip the checkstyle for specific regions. The syntax is as follows:

```java
//CHECKSTYLE.OFF: AvoidStarImport - Needed for TupleGenerator
import eu.stratosphere.api.java.tuple.*;
//CHECKSTYLE.ON: AvoidStarImport
```

The developer can only disable one module at a time and must leave a comment, why he/she disables the checkstyle module.;;;","09/Jun/14 12:53;github-import;[Date: Thu May 08 13:10:16 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Is it actually possible to create a checkstyle rule to allow the import of `eu.stratosphere.api.java.tuple.*` ?;;;","09/Jun/14 12:53;github-import;[Date: Thu May 08 13:27:02 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Will merge now. Let's keep looking for the exception rule;;;","09/Jun/14 12:53;github-import;[Date: Thu May 08 18:00:29 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [5b34bef7dba5c3842059a934e1c00d7c32e40979|https://github.com/stratosphere/stratosphere/commit/5b34bef7dba5c3842059a934e1c00d7c32e40979];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent Boolean comparison,FLINK-761,12719941,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:53,09/Jun/14 12:53,14/Jul/23 05:57,09/Jun/14 12:53,,,,pre-apache,,,,,,,0,github-import,,"The BooleanComparator.compare() works the opposite way of Boolean/BooleanValue.compareTo().

Consider the following piece of code:
```
public static void main(String[] args) throws IOException{
		Boolean x = true;
		Boolean y = false;
		System.out.println(x.compareTo(y));

		BooleanValue t = new BooleanValue(true);
		BooleanValue s = new BooleanValue(false);
		System.out.println(t.compareTo(s));
		
		BooleanSerializer bs = new BooleanSerializer();
		TestOutputView out1 = new TestOutputView();
		bs.serialize(true, out1);
		TestInputView in1 = out1.getInputView();
		TestOutputView out2 = new TestOutputView();
		bs.serialize(false, out2);
		TestInputView in2 = out2.getInputView();
		
		System.out.println(new BooleanComparator(true).compare(in1, in2));
	}
```
This prints
```
1
1
-1
```

intended?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/761
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Tue May 06 12:56:49 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398140,,,Mon Jun 09 12:53:48 UTC 2014,,,,,,,,,,"0|i1wjn3:",398267,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:53;github-import;[Date: Tue May 06 13:25:42 CEST 2014, Author: [zentol|https://github.com/zentol]]

the basicTypeComparator.compareToReference() is also odd. 
```
@Override
	public int compareToReference(TypeComparator<T> referencedComparator) {
		int comp = ((BasicTypeComparator<T>) referencedComparator).reference.compareTo(reference);
		return ascendingComparison ? comp : -comp;
	}
```
shouldnt it compare its own reference with the passed one instead of the other way around?;;;","09/Jun/14 12:53;github-import;[Date: Tue May 06 13:36:10 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I think everything should be consistent to the Boolean implementation of Java, so BooleanComparator and the basicTypeComparator seems to be wrong for me.;;;","09/Jun/14 12:53;github-import;[Date: Tue May 06 14:31:04 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The TypeComparators kind of swap the order of arguments internally. I think
all of the TypeComparators do it like that (but please verify that
statement).

If you want to change it, you would need to adjust every place where it is
invoked.;;;","09/Jun/14 12:53;github-import;[Date: Tue May 06 14:45:45 CEST 2014, Author: [zentol|https://github.com/zentol]]

only the boolean comparator works like this.
exempt from the int comparator (i did check them all):
```
int i1 = firstSource.readInt();
int i2 = secondSource.readInt();
int comp = (i1 < i2 ? -1 : (i1 == i2 ? 0 : 1)); 
```
in contrast the boolean one:
```
final int fs = firstSource.readBoolean() ? 1 : 0;
final int ss = secondSource.readBoolean() ? 1 : 0;
int comp = ss - fs; 
```;;;","09/Jun/14 12:53;github-import;[Date: Tue May 06 17:26:12 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, then it seems like a bug there. So, do the other comparators return a
consistent result in your sample test?;;;","09/Jun/14 12:53;github-import;[Date: Thu May 08 18:21:53 CEST 2014, Author: [zentol|https://github.com/zentol]]

resolved in ([#764|https://github.com/stratosphere/stratosphere/issues/764] | [FLINK-764|https://issues.apache.org/jira/browse/FLINK-764]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests for aggregate operator,FLINK-759,12719939,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:53,09/Jun/14 12:53,14/Jul/23 05:57,09/Jun/14 12:53,,,,pre-apache,,,,,,,0,github-import,,"See ([#548|https://github.com/stratosphere/stratosphere/issues/548] | [FLINK-548|https://issues.apache.org/jira/browse/FLINK-548])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/759
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Tue May 06 11:26:54 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:53;github-import;pull-request-759-8233427659989801231.patch;https://issues.apache.org/jira/secure/attachment/12649301/pull-request-759-8233427659989801231.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398138,,,Mon Jun 09 12:53:39 UTC 2014,,,,,,,,,,"0|i1wjmn:",398265,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:53;github-import;[Date: Wed May 07 11:27:19 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good to merge.

By the way, for future tests, it is easier to use the `LocalCollectionOutputFormat` than working with temp files. Have a look at how the `WordCountWithCollectionITCase`or the `MultipleSolutionSetJoinsITCase` do it.;;;","09/Jun/14 12:53;github-import;[Date: Wed May 07 11:44:52 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I am merging this now. In general: When you fail a test, make sure the error message gets written out. Like this it becomes a bit hard to debug.;;;","09/Jun/14 12:53;github-import;[Date: Wed May 07 11:47:02 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Hmmm... this test is a bit weird, it cannot succeed. It tries to compute the avgerage of a string?;;;","09/Jun/14 12:53;github-import;[Date: Wed May 07 12:32:15 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [ff96f6ca0815cf0abb5f67cd03487b5307ba2217|https://github.com/stratosphere/stratosphere/commit/ff96f6ca0815cf0abb5f67cd03487b5307ba2217] and [10d288688cce8aa435f60109279b88e096c420ef|https://github.com/stratosphere/stratosphere/commit/10d288688cce8aa435f60109279b88e096c420ef];;;","09/Jun/14 12:53;github-import;[Date: Wed May 07 12:32:26 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

And fixed ;-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Aggregations producing wrong result,FLINK-757,12719937,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:53,09/Jun/14 12:53,14/Jul/23 05:57,09/Jun/14 12:53,,,,pre-apache,,,,,,,0,github-import,,"I started writing the integration tests for the aggregate operator and have some trouble with my first example:

```	
DataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);			
DataSet<Tuple2<Integer, Long>> aggregateDs = ds
						.aggregate(Aggregations.SUM, 0)
						.aggregate(Aggregations.MAX, 1);
````

What I expect is the SUM and MAX of all fields. I get always the correct sum but for the MAX the result is changing, sometimes producing 4 or 5 instead of the correct 6.
Has somebody an idea where the problem could lie?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/757
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Tue May 06 10:09:43 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398136,,,Mon Jun 09 12:53:22 UTC 2014,,,,,,,,,,"0|i1wjm7:",398263,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:53;github-import;[Date: Tue May 06 10:56:25 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

You Aggregate over an aggregate.

DataSet.aggregare(a).aggregate(b) = b(a(dataset)).

DataSet.aggreage(a).and(b) = a,b(dataset)
Am 06.05.2014 10:09 schrieb ""markus-h"" <notifications@github.com>:

> I started writing the integration tests for the aggregate operator and
> have some trouble with my first example:
>
> DataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);
> DataSet<Tuple2<Integer, Long>> aggregateDs = ds
>                         .aggregate(Aggregations.SUM, 0)
>                         .aggregate(Aggregations.MAX, 1);
>
> What I expect is the SUM and MAX of all fields. I get always the correct
> sum but for the MAX the result is changing, sometimes producing 4 or 5
> instead of the correct 6.
> Has somebody an idea where the problem could lie?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/757>
> .
>;;;","09/Jun/14 12:53;github-import;[Date: Tue May 06 10:59:40 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

Oh I see, I overlooked the and operator.;;;","09/Jun/14 12:53;github-import;[Date: Tue May 06 12:18:10 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I guess we need good comments or docs for that,;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getAggregators() for SpargleIteration,FLINK-756,12719936,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:53,09/Jun/14 12:53,14/Jul/23 05:57,09/Jun/14 12:53,,,,pre-apache,,,,,,,0,github-import,,"See ([#704|https://github.com/stratosphere/stratosphere/issues/704] | [FLINK-704|https://issues.apache.org/jira/browse/FLINK-704])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/756
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon May 05 15:11:23 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:53;github-import;pull-request-756-7262664237937218948.patch;https://issues.apache.org/jira/secure/attachment/12649299/pull-request-756-7262664237937218948.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398135,,,Mon Jun 09 12:53:17 UTC 2014,,,,,,,,,,"0|i1wjlz:",398262,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:53;github-import;[Date: Mon May 05 15:44:16 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I think this pull request is good to merge.;;;","09/Jun/14 12:53;github-import;[Date: Wed May 07 12:06:31 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Can we merge this? An user is waiting for it, and its a trivial change.;;;","09/Jun/14 12:53;github-import;[Date: Wed May 07 12:30:23 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It is merged, tests are running.;;;","09/Jun/14 12:53;github-import;[Date: Wed May 07 12:31:51 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [f65c65b32c36ed687a5e8beffee74b13772bea01|https://github.com/stratosphere/stratosphere/commit/f65c65b32c36ed687a5e8beffee74b13772bea01];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extension of the TypeExtractor to validate input type information and recognizing input mismatches.,FLINK-755,12719935,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:53,09/Jun/14 12:53,14/Jul/23 05:57,09/Jun/14 12:53,,,,pre-apache,,,,,,,0,github-import,,"With this PR the TypeExtractor also validates the input parameters of a Function and compares them with the given TypeInformation. If there is an input mismatch, an Exception is thrown which helps the user to identify the problem e.g. ""Basic type expected"" etc.

See also ([#683|https://github.com/stratosphere/stratosphere/issues/683] | [FLINK-683|https://issues.apache.org/jira/browse/FLINK-683])

The PR also corrects the whitespace format and fixes a small TypeExtractor bug.

Test cases are also included.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/755
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Mon May 05 15:03:09 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:53;github-import;pull-request-755-1085468190591692097.patch;https://issues.apache.org/jira/secure/attachment/12649298/pull-request-755-1085468190591692097.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398134,,,Mon Jun 09 12:53:11 UTC 2014,,,,,,,,,,"0|i1wjlr:",398261,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:53;github-import;[Date: Thu May 08 13:11:09 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good, will merge.;;;","09/Jun/14 12:53;github-import;[Date: Thu May 08 16:46:14 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

Would be great if you could merge it today, then I can do my Writable types PR.;;;","09/Jun/14 12:53;github-import;[Date: Thu May 08 18:00:16 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [4ab2d9ac1f8f531bc5b5022c38ff1e0157c3a7bb|https://github.com/stratosphere/stratosphere/commit/4ab2d9ac1f8f531bc5b5022c38ff1e0157c3a7bb];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Value comparators,FLINK-754,12719934,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:52,09/Jun/14 12:53,14/Jul/23 05:57,09/Jun/14 12:53,,,,pre-apache,,,,,,,0,github-import,,"See ([#738|https://github.com/stratosphere/stratosphere/issues/738] | [FLINK-738|https://issues.apache.org/jira/browse/FLINK-738]).
This pull request depends on ([#741|https://github.com/stratosphere/stratosphere/issues/741] | [FLINK-741|https://issues.apache.org/jira/browse/FLINK-741]).

I did a quick test with a simple reduce on tuples of value types. Once ([#744|https://github.com/stratosphere/stratosphere/issues/744] | [FLINK-744|https://issues.apache.org/jira/browse/FLINK-744]) is done I will integrate further tests.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/754
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon May 05 15:00:43 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:53;github-import;pull-request-754-3893022699773086779.patch;https://issues.apache.org/jira/secure/attachment/12649297/pull-request-754-3893022699773086779.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398133,,,Mon Jun 09 12:53:04 UTC 2014,,,,,,,,,,"0|i1wjlj:",398260,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:53;github-import;[Date: Thu May 08 00:16:41 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good, but needs tests.;;;","09/Jun/14 12:53;github-import;[Date: Thu May 08 12:58:58 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [808ae18f66614ba4bc81c2f3007f913e04ce7b80|https://github.com/stratosphere/stratosphere/commit/808ae18f66614ba4bc81c2f3007f913e04ce7b80];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix for Tuple Comparator,FLINK-752,12719932,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:52,09/Jun/14 12:52,14/Jul/23 05:57,09/Jun/14 12:52,,,,pre-apache,,,,,,,0,github-import,,"I thought about a fix for the bug in the TupleComparator (mentioned in https://github.com/stratosphere/stratosphere/issues/744#issuecomment-42018637) which currently leads to the wrong result in sorts. 

My idea was to change the TupleSeriailizer to serialize the fields in the same order as the comparator needs them. Then the current implementation of the TupleComparator runs without any changes and without any serializiation overhead at runtime. Since it wasn't much code I did just write and tested it and it seems to work. Or do you prefer an other fix?

If this fix is okay, I got an other question: I added the KeyPositions to the TypeInformation and set them in the JavaAPiPostPass. I get the KeyPositions over channel.getLocalStrategyKeys() and pass it to the TypeInformation, but there are also the channel.getShipStrategyKeys(). In my tests the ShipStrategyKeys were a subset of the LocalStrategyKeys therefore I took the LocalStrategyKeys, but is this always the case?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/752
Created by: [filiphaase|https://github.com/filiphaase]
Labels: 
Created at: Mon May 05 09:32:44 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:52;github-import;pull-request-752-5931770225430775964.patch;https://issues.apache.org/jira/secure/attachment/12649296/pull-request-752-5931770225430775964.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398131,,,Mon Jun 09 12:52:55 UTC 2014,,,,,,,,,,"0|i1wjl3:",398258,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:52;github-import;[Date: Tue May 06 19:47:01 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

This pull request would introduce quite a bit of instability and potential errors for tasks with multiple outputs.

@filiphaase and me decided to fix this in a different way.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactored with maven checkstyle plugin,FLINK-751,12719931,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:52,09/Jun/14 12:52,14/Jul/23 05:57,09/Jun/14 12:52,,,,pre-apache,,,,,,,0,github-import,,"Added suppressions.xml file with one file to be omitted and refactored all checkstyle errors. Related to pull-req. ([#636|https://github.com/stratosphere/stratosphere/issues/636] | [FLINK-636|https://issues.apache.org/jira/browse/FLINK-636]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/751
Created by: [alexff91|https://github.com/alexff91]
Labels: 
Created at: Fri May 02 22:13:57 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:52;github-import;pull-request-751-2578152067098192700.patch;https://issues.apache.org/jira/secure/attachment/12649295/pull-request-751-2578152067098192700.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398130,,,Mon Jun 09 12:52:50 UTC 2014,,,,,,,,,,"0|i1wjkv:",398257,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:52;github-import;[Date: Mon May 05 10:29:38 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I merged the pull request ([938a2af4f48fe2dc6caa6470ceec99b4089931ca|https://github.com/stratosphere/stratosphere/commit/938a2af4f48fe2dc6caa6470ceec99b4089931ca]) after some minor refinements ([ba8f2d2fac41a0cb7137f0b00f64f49ff718b1d8|https://github.com/stratosphere/stratosphere/commit/ba8f2d2fac41a0cb7137f0b00f64f49ff718b1d8]);;;","09/Jun/14 12:52;github-import;[Date: Mon May 05 10:31:29 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Great job guys!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add getSolutionSet() and .getWorkset() for delta iterations,FLINK-750,12719930,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:52,09/Jun/14 12:52,14/Jul/23 05:57,09/Jun/14 12:52,,,,pre-apache,,,,,,,0,github-import,,"Before, the result of dataset.iterateDelta() would be the workset and
the solution set could be obtained via iteration.getSolutionSet().

This is counterintuitive for two reasons: 1. in BulkIteration the result of
the dataset.iterate() is the solution set. 2. you call .iterateDelta()
on what is to be the initial solution set and pass the workset as
an argument, so it makes sense that what you get as a result is also the
solution set placeholder.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/750
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri May 02 15:34:16 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:52;github-import;pull-request-750-4373329867343111360.patch;https://issues.apache.org/jira/secure/attachment/12649294/pull-request-750-4373329867343111360.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398129,,,Mon Jun 09 12:52:31 UTC 2014,,,,,,,,,,"0|i1wjkn:",398256,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:52;github-import;[Date: Fri May 02 16:25:56 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I am not sure about this. The reason why we did it as we did is that you cannot really do anything on the solution set, except joining with it. So you cannot take the set and run some functions on it, as one might expect seeing the syntax of the bulk iterations. So

```
DeltaIterativeSet<...> set = ...
set.map.reduce.something();
```

would not work then. The way we have it right now, the delta iterations are like the bulk ones, except that you can access a state in addition.;;;","09/Jun/14 12:52;github-import;[Date: Fri May 02 16:56:14 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

Hmm ok, it's just that I immediately was confused about what the result from the iterateDelta() call was supposed to be. Maybe we should have .getWorkset() and .getSolutionSet() then?;;;","09/Jun/14 12:52;github-import;[Date: Tue May 06 18:09:20 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

We could do that. That would at least make it very clear what is what.

The way it is now is with the patch confusing to me as well, to be honest.;;;","09/Jun/14 12:52;github-import;[Date: Wed May 07 10:38:17 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

Ok, then I will change this and update the pull request.
;;;","09/Jun/14 12:52;github-import;[Date: Wed May 07 11:24:42 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

I updated it with the following commit message:

Before, the result of dataset.iterateDelta() would be the workset and
    the solution set could be obtained via iteration.getSolutionSet().
    
    This is confusing since you are not told what the result of
    .iterateDelta() is supposed to be. Now, when using the result an
    exception is thrown during pre-flight that advises the user to use
    the two methods to get the correct dataset (work, solution).;;;","09/Jun/14 12:52;github-import;[Date: Wed May 07 11:31:24 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Would it make sense to let the `DeltaIterativeDataSet` not inherit from `DataSet` such that you cannot use it directly? Also, we should rename it then, many to simply DeltaIteration.;;;","09/Jun/14 12:52;github-import;[Date: Wed May 07 11:48:46 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

Yes, you are right. I did that now.;;;","09/Jun/14 12:52;github-import;[Date: Wed May 07 18:41:31 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Good to merge.;;;","09/Jun/14 12:52;github-import;[Date: Wed May 07 20:54:03 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [7f32534cf1110e489f38befe1128aba6e42c97a3|https://github.com/stratosphere/stratosphere/commit/7f32534cf1110e489f38befe1128aba6e42c97a3];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StringComparator incompatible read/write,FLINK-749,12719929,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:52,09/Jun/14 12:52,14/Jul/23 05:57,09/Jun/14 12:52,,,,pre-apache,,,,,,,0,github-import,,"The StringComparator uses a mixture of static and non-static read/write methods which are not compatible with each other due to different offsets.

This PR makes sure that it only uses the static methods (similar to the StringSerializer).

this fixes the issue mentioned in ([#748|https://github.com/stratosphere/stratosphere/issues/748] | [FLINK-748|https://issues.apache.org/jira/browse/FLINK-748]) .

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/749
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Fri May 02 14:50:40 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:52;github-import;pull-request-749-5713524883275433715.patch;https://issues.apache.org/jira/secure/attachment/12649293/pull-request-749-5713524883275433715.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398128,,,Mon Jun 09 12:52:21 UTC 2014,,,,,,,,,,"0|i1wjkf:",398255,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:52;github-import;[Date: Fri May 02 16:29:36 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The purpose of the StringValue is to support mutable objects that can be reused. Otherwise, one could use String directly. This purpose is voided with this implementation.;;;","09/Jun/14 12:52;github-import;[Date: Fri May 02 18:12:38 CEST 2014, Author: [zentol|https://github.com/zentol]]

I'm a bit confused. The StringComparator is only used for strings, correct? and not for StringValues, which is why ([#738|https://github.com/stratosphere/stratosphere/issues/738] | [FLINK-738|https://issues.apache.org/jira/browse/FLINK-738]) exists. So why is this change connected to the general purpose of StringValues?;;;","09/Jun/14 12:52;github-import;[Date: Fri May 02 19:54:52 CEST 2014, Author: [zentol|https://github.com/zentol]]

instead of the StringComparator i now modify the StringValue class, adding an offset to the read()/write() method. They now behave more similarly to the StringSerializer methods, fixing the issue in the StringComparator just as well.;;;","09/Jun/14 12:52;github-import;[Date: Sat May 03 14:55:26 CEST 2014, Author: [zentol|https://github.com/zentol]]

hmm this turned out trickier than i thought. i tried different positions for adding the offsets, but even the ""best"" version broke several tests. :/

eu.stratosphere.RecordTest.blackBoxTests
eu.stratosphere.pact.runtime.io.ChannelViewsTest (timeout);;;","09/Jun/14 12:52;github-import;[Date: Tue May 06 14:04:48 CEST 2014, Author: [zentol|https://github.com/zentol]]

```
Running eu.stratosphere.pact.runtime.hash.HashTableITCase

No output has been received in the last 10 minutes, this potentially indicates a stalled build or something wrong with the build itself.

The build has been terminated
```
can this be related to this PR? it only happens on a single build.;;;","09/Jun/14 12:52;github-import;[Date: Tue May 06 15:07:00 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

No, its a common error.


On Tue, May 6, 2014 at 2:04 PM, zentol <notifications@github.com> wrote:

> Running eu.stratosphere.pact.runtime.hash.HashTableITCase
>
> No output has been received in the last 10 minutes, this potentially indicates a stalled build or something wrong with the build itself.
>
> The build has been terminated
>
> can this be related to this PR? it only happens on a single build.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/749#issuecomment-42294207>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","09/Jun/14 12:52;github-import;[Date: Wed May 07 20:54:23 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [d2a5178014470591c155989158d3d8629a6ee0ef|https://github.com/stratosphere/stratosphere/commit/d2a5178014470591c155989158d3d8629a6ee0ef];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inconsistent read/write in StringValue,FLINK-748,12719928,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:52,09/Jun/14 12:52,14/Jul/23 05:57,09/Jun/14 12:52,,,,pre-apache,,,,,,,0,github-import,,"the static writeString() method in StringValue adds an offset of 1 to differentiate between null and """". while the readString() method accounts for this, the non-static read()/write() methods do not.

this currently causes issues in the StringComparator, which uses writeString() to serialize but read() to deserialize, causing it to read an additional character.

is there a reason why these methods behave differently?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/748
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Fri May 02 13:21:53 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398127,,,Mon Jun 09 12:52:14 UTC 2014,,,,,,,,,,"0|i1wjk7:",398254,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:52;github-import;[Date: Fri May 02 14:14:38 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The reason is that you can statically read a null. But if you have already
a StringValue instance which deserializes itself  from the stream, it is
naturally not null.;;;","09/Jun/14 12:52;github-import;[Date: Fri May 02 18:12:28 CEST 2014, Author: [zentol|https://github.com/zentol]]

would it be a problem to add the offsets to the non static methods?;;;","09/Jun/14 12:52;github-import;[Date: Fri May 02 19:39:41 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think you can add the offsets to the non static method as well;;;","09/Jun/14 12:52;github-import;[Date: Thu May 08 13:35:07 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [d2a5178014470591c155989158d3d8629a6ee0ef|https://github.com/stratosphere/stratosphere/commit/d2a5178014470591c155989158d3d8629a6ee0ef];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
use Kryo for object copy in AvroSerializer. Enable CrossITCase for JAPI,FLINK-746,12719926,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:52,09/Jun/14 12:52,14/Jul/23 05:57,09/Jun/14 12:52,,,,pre-apache,,,,,,,0,github-import,,"Fix for issue https://github.com/stratosphere/stratosphere/issues/691

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/746
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Thu May 01 11:20:49 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:52;github-import;pull-request-746-7145426055487631271.patch;https://issues.apache.org/jira/secure/attachment/12649292/pull-request-746-7145426055487631271.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398125,,,Mon Jun 09 12:52:03 UTC 2014,,,,,,,,,,"0|i1wjjr:",398252,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:52;github-import;[Date: Tue May 06 17:04:57 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [2a3ced46f2b7c7289221d6e6cbfcb4d3968dd947|https://github.com/stratosphere/stratosphere/commit/2a3ced46f2b7c7289221d6e6cbfcb4d3968dd947];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Added Boolean, Byte, Char Serializer Tests",FLINK-745,12719925,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:51,09/Jun/14 12:52,14/Jul/23 05:57,09/Jun/14 12:52,,,,pre-apache,,,,,,,0,github-import,,"([#589|https://github.com/stratosphere/stratosphere/issues/589] | [FLINK-589|https://issues.apache.org/jira/browse/FLINK-589])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/745
Created by: [aaronchlam|https://github.com/aaronchlam]
Labels: 
Created at: Thu May 01 10:08:06 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:51;github-import;pull-request-745-5668685325603479445.patch;https://issues.apache.org/jira/secure/attachment/12649291/pull-request-745-5668685325603479445.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398124,,,Mon Jun 09 12:51:59 UTC 2014,,,,,,,,,,"0|i1wjjj:",398251,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:51;github-import;[Date: Fri May 02 11:42:15 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

@aaronchlam Thank you!
Will merge this PR.
;;;","09/Jun/14 12:51;github-import;[Date: Fri May 02 15:01:11 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor test utils into own package,FLINK-743,12719923,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:51,09/Jun/14 12:51,14/Jul/23 05:57,09/Jun/14 12:51,,,,pre-apache,,,,,,,0,github-import,,"When trying to use packages that use stratosphere-tests in
stratosphere-examples you used to get a circular dependency. Now
packages can use the testing utilities without pulling in all the
transitive dependencies of stratosphere-tests.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/743
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Wed Apr 30 16:48:01 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:51;github-import;pull-request-743-8311070813924845556.patch;https://issues.apache.org/jira/secure/attachment/12649290/pull-request-743-8311070813924845556.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398122,,,Mon Jun 09 12:51:47 UTC 2014,,,,,,,,,,"0|i1wjj3:",398249,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:51;github-import;[Date: Wed Apr 30 16:54:27 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Sounds like a good idea to me.

While at that, can we get rid of the old TestBase?;;;","09/Jun/14 12:51;github-import;[Date: Wed Apr 30 18:03:38 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

OK, i'll work on removing that.;;;","09/Jun/14 12:51;github-import;[Date: Fri May 02 11:41:58 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

This has grown a bit. Now it's the refactoring of the testing utils into a separate package plus the removal of the old TestBase and the renaming of TestBase2 to RecordAPITestBase.;;;","09/Jun/14 12:51;github-import;[Date: Tue May 06 17:09:43 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good, will merge.;;;","09/Jun/14 12:51;github-import;[Date: Tue May 06 20:25:34 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [dcda680f89dd055800dda89c6bd3b16567d4f35c|https://github.com/stratosphere/stratosphere/commit/dcda680f89dd055800dda89c6bd3b16567d4f35c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changed Key to Key<T>,FLINK-741,12719921,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:51,09/Jun/14 12:51,14/Jul/23 05:57,09/Jun/14 12:51,,,,pre-apache,,,,,,,0,github-import,,"See https://groups.google.com/forum/#!topic/stratosphere-dev/N-khBR8978w for a description and discussion.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/741
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Tue Apr 29 23:14:18 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:51;github-import;pull-request-741-5832581290991877334.patch;https://issues.apache.org/jira/secure/attachment/12649289/pull-request-741-5832581290991877334.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398120,,,Mon Jun 09 12:51:27 UTC 2014,,,,,,,,,,"0|i1wjin:",398247,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:51;github-import;[Date: Wed Apr 30 01:18:20 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Pull request is not meant to be merged just now, it still fails in the Scala API.

The point is to give an overview of what would change.;;;","09/Jun/14 12:51;github-import;[Date: Fri May 02 14:51:40 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

Ooops, I didn't want to close this, klicked by accident. I fixed the scala side in [96a6a9e5b946fac37cb73d704580f343b1a6067a|https://github.com/stratosphere/stratosphere/commit/96a6a9e5b946fac37cb73d704580f343b1a6067a]. Travis is still running: https://travis-ci.org/aljoscha/stratosphere/builds/24263218;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Many small YARN improvements.,FLINK-740,12719920,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:51,09/Jun/14 12:51,14/Jul/23 05:57,09/Jun/14 12:51,,,,pre-apache,,,,,,,0,github-import,,"This pull request fixes some YARN-related bugs:
* Fix temporary directory behavior of YARN and .yarn-jobmanager permissions for different users. ([#736|https://github.com/stratosphere/stratosphere/issues/736] | [FLINK-736|https://issues.apache.org/jira/browse/FLINK-736])
* YARN session client should use ""fs.hdfs.hadoopconf"" if property set in conf ([#733|https://github.com/stratosphere/stratosphere/issues/733] | [FLINK-733|https://issues.apache.org/jira/browse/FLINK-733])
* warning if the user is using ""file:"" as the distributed file system
* Don't show JobManager URL in stratosphere-client if submitting to YARN. 
* Suggested fix for: IllegalMonitorStateException in TaskManager. ([#739|https://github.com/stratosphere/stratosphere/issues/739] | [FLINK-739|https://issues.apache.org/jira/browse/FLINK-739])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/740
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue Apr 29 18:54:29 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:51;github-import;pull-request-740-3141453636679336553.patch;https://issues.apache.org/jira/secure/attachment/12649288/pull-request-740-3141453636679336553.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398119,,,Mon Jun 09 12:51:20 UTC 2014,,,,,,,,,,"0|i1wjif:",398246,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:51;github-import;[Date: Wed Apr 30 08:28:37 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Pushed some more improvements in [68100e5c9414da6826ab04fc1d40f2ea6e8744c2|https://github.com/stratosphere/stratosphere/commit/68100e5c9414da6826ab04fc1d40f2ea6e8744c2];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalMonitorStateException in TaskManager.,FLINK-739,12719919,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:51,09/Jun/14 12:51,14/Jul/23 05:57,09/Jun/14 12:51,,,,pre-apache,,,,,,,0,github-import,,"The code has been introduced by commit [2cbb93a436eaa78591677ca5ac3886f94471f485|https://github.com/stratosphere/stratosphere/commit/2cbb93a436eaa78591677ca5ac3886f94471f485].

Snip:
```java
// Create a new task manager object
try {
	new TaskManager();
} catch (Exception e) {
	LOG.fatal(""Taskmanager startup failed: "" + e.getMessage(), e);
	System.exit(FAILURE_RETURN_CODE);
}

// park the main thread to keep the JVM alive (all other threads may be daemon threads)
try {
	new Object().wait(); // line 421
} catch (InterruptedException ex) {}
```

The exception

```
java.lang.IllegalMonitorStateException
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at eu.stratosphere.nephele.taskmanager.TaskManager.main(TaskManager.java:421)
	at eu.stratosphere.yarn.YarnTaskManagerRunner$1.run(YarnTaskManagerRunner.java:55)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:357)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1471)
	at eu.stratosphere.yarn.YarnTaskManagerRunner.main(YarnTaskManagerRunner.java:51)
```

I think you can only call `.wait()` inside a synchronized block?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/739
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, runtime, simple-issue, 
Created at: Tue Apr 29 18:10:18 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398118,,,Mon Jun 09 12:51:15 UTC 2014,,,,,,,,,,"0|i1wji7:",398245,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:51;github-import;[Date: Tue Apr 29 20:10:43 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Right. Hmmm, I thought I tested it with start-cluster, but apparently I did not. This is pretty critical.;;;","09/Jun/14 12:51;github-import;[Date: Wed Apr 30 10:56:33 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Fixed in ([#740|https://github.com/stratosphere/stratosphere/issues/740] | [FLINK-740|https://issues.apache.org/jira/browse/FLINK-740]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement comparators for Value types,FLINK-738,12719918,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:51,09/Jun/14 12:51,14/Jul/23 05:57,09/Jun/14 12:51,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/738
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Tue Apr 29 11:51:37 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398117,,,Mon Jun 09 12:51:11 UTC 2014,,,,,,,,,,"0|i1wjhz:",398244,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:51;github-import;[Date: Mon May 05 09:53:48 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I think  ([#741|https://github.com/stratosphere/stratosphere/issues/741] | [FLINK-741|https://issues.apache.org/jira/browse/FLINK-741]) is quite important for a good implementation. ;;;","09/Jun/14 12:51;github-import;[Date: Mon May 12 08:55:14 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

This issue can be closed. Implemented in ([#754|https://github.com/stratosphere/stratosphere/issues/754] | [FLINK-754|https://issues.apache.org/jira/browse/FLINK-754]) merged in [808ae18f66614ba4bc81c2f3007f913e04ce7b80|https://github.com/stratosphere/stratosphere/commit/808ae18f66614ba4bc81c2f3007f913e04ce7b80] and [01583434ea14d1a73b538d72940cc5c77e009153|https://github.com/stratosphere/stratosphere/commit/01583434ea14d1a73b538d72940cc5c77e009153].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests to check for valid parameters of cogroup,FLINK-737,12719917,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:51,09/Jun/14 12:51,14/Jul/23 05:57,09/Jun/14 12:51,,,,pre-apache,,,,,,,0,github-import,,"See ([#548|https://github.com/stratosphere/stratosphere/issues/548] | [FLINK-548|https://issues.apache.org/jira/browse/FLINK-548])
I basically copied the test for join and used it also for cogroup.
I also activated the tests for mixed key types.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/737
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Tue Apr 29 10:17:37 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:51;github-import;pull-request-737-6257858387975329220.patch;https://issues.apache.org/jira/secure/attachment/12649287/pull-request-737-6257858387975329220.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398116,,,Mon Jun 09 12:51:07 UTC 2014,,,,,,,,,,"0|i1wjhr:",398243,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:51;github-import;[Date: Wed Apr 30 18:32:08 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Will merge after rebase passed Travis.;;;","09/Jun/14 12:51;github-import;[Date: Wed Apr 30 20:42:00 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix temporary directory behavior of YARN and .yarn-jobmanager permissions for different users.,FLINK-736,12719916,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:50,09/Jun/14 12:51,14/Jul/23 05:57,09/Jun/14 12:51,,,,pre-apache,,,,,,,0,github-import,,"yarn-session: root
strato client: not-root.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/736
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, enhancement, user satisfaction, YARN, 
Milestone: Release 0.5
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Mon Apr 28 22:31:35 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398115,,,2014-06-09 12:50:59.0,,,,,,,,,,"0|i1wjhj:",398242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
#722 - Removed faulty constructor,FLINK-732,12719912,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:50,09/Jun/14 12:50,14/Jul/23 05:57,09/Jun/14 12:50,,,,pre-apache,,,,,,,0,github-import,,"([#722|https://github.com/stratosphere/stratosphere/issues/722] | [FLINK-722|https://issues.apache.org/jira/browse/FLINK-722])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/732
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Mon Apr 28 16:33:19 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;pull-request-732-609182485207651285.patch;https://issues.apache.org/jira/secure/attachment/12649285/pull-request-732-609182485207651285.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398111,,,Mon Jun 09 12:50:48 UTC 2014,,,,,,,,,,"0|i1wjgn:",398238,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;[Date: Mon Apr 28 18:13:42 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good, will merge.;;;","09/Jun/14 12:50;github-import;[Date: Mon Apr 28 23:00:50 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [ea530c32ef84a4536f0a99dab4c208d6eebd0d33|https://github.com/stratosphere/stratosphere/commit/ea530c32ef84a4536f0a99dab4c208d6eebd0d33];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a way to parametrize aggregators and convergence criteria,FLINK-731,12719911,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,github-import,github-import,09/Jun/14 12:50,13/Jun/14 14:45,14/Jul/23 05:57,13/Jun/14 14:45,,,,0.6-incubating,,,,,,,0,github-import,,"Currently, there is no way to pass parameters to an aggregator or a convergence criterion. @StephanEwen has suggested to add support for this by changing the code for aggregators and convergence criterion to take an object, rather than a class, i.e. instead of 
`iteration.registerAggregator(""my aggregator"", Customaggregator.class)`
do something like this:
`iteration.registerAggregator(""my aggregator"", new CustomAggregator(params));`

I can start working on this, but I have a question: What should the type of `params` be?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/731
Created by: [vasia|https://github.com/vasia]
Labels: 
Created at: Mon Apr 28 16:26:56 CEST 2014
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398110,,,Fri Jun 13 14:45:09 UTC 2014,,,,,,,,,,"0|i1wjgf:",398237,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;[Date: Mon Apr 28 17:23:04 CEST 2014, Author: [zentol|https://github.com/zentol]]

I don't see how the type (and number) of params are relevant for this issue.;;;","09/Jun/14 12:50;github-import;[Date: Mon Apr 28 17:32:51 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I agree, Params can actually be arbitary. I would make it a regular java
class, and an extension of IOReadableWritable for the start, to deal with
serialization. The later, we can think about getting rid of the
IOReadbaleWritable.;;;","09/Jun/14 12:50;github-import;[Date: Mon Apr 28 17:39:22 CEST 2014, Author: [vasia|https://github.com/vasia]]

Thanks, that's what I meant by type. I was actually thinking of Configuration, but I can just make an extension of IOReadableWritable, sure.;;;","09/Jun/14 12:50;github-import;[Date: Mon Apr 28 17:42:54 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Sorry, what I said was confusing. I would not make params an extension of
IOReadableWritable. The same way as MapFunction or JoinFunction can have a
constructors with arbitrary parameters, the Aggregator can have a
constructor with arbitrary parameters.;;;","09/Jun/14 12:50;github-import;[Date: Mon Apr 28 18:27:28 CEST 2014, Author: [vasia|https://github.com/vasia]]

OK, I think I understand now. 
I assumed that since there are several common aggregators that have standard implementations, one would want a common way of instantiating them, in order to avoid defining multiple constructors or sub-classing.
For example, several programs will use the already provided LongSumAggregator, but if they need a different number of parameters, they would either have to add a constructor or subclass.
Do I make any sense? o.O
;;;","09/Jun/14 12:50;github-import;[Date: Mon Apr 28 18:36:04 CEST 2014, Author: [zentol|https://github.com/zentol]]

yes you do make sense. but consider this: if someone uses the LongSumAggregator, but wants to use additional parameters to do *something different*, he has to create a subclass anyway to add that new behaviour.;;;","09/Jun/14 12:50;github-import;[Date: Tue May 06 19:20:54 CEST 2014, Author: [vasia|https://github.com/vasia]]

Hi,
while working on this issue, I noticed that there are no `getAggregators()`, `registerAggregator()` and `registerAggregationConvergenceCriterion()` methods in `DeltaIterativeDataSet`. Shall I implement these?;;;","09/Jun/14 12:50;github-import;[Date: Wed May 07 11:21:23 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Good idea, please do so!;;;","09/Jun/14 12:50;github-import;[Date: Wed May 07 16:29:25 CEST 2014, Author: [vasia|https://github.com/vasia]]

Hey,
there exists a check in `NepheleJobGraphGenerator` that throws the following error if a custom convergence criterion is set in a DeltaIteration: ""Cannot use custom convergence criterion with workset iteration. Workset iterations have implicit convergence criterion where workset is empty.""
What is the logic behind this and should I remove it?;;;","09/Jun/14 12:50;github-import;[Date: Wed May 07 17:50:21 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The Delta Iteration terminates when the workset is empty. That is its
Termination Criterion and it comes with an Aggregator, which counts the
number of elements in the workset.

If you would set another TerminationCriterion, you would override that
""workset empty"" termination criterion.

It makes sense to have more Aggregators available in the delta iterations.
In order to set an additional Termination Critertion, we would need to
change the code such that multiple criterions are allowed and that
convergence happens once one of them signals so.

For a start, I would suggest to allow the registration of additional
Aggregators, but not additional convergence criteria.;;;","09/Jun/14 12:50;github-import;[Date: Wed May 07 19:57:00 CEST 2014, Author: [vasia|https://github.com/vasia]]

OK, so should I just implement the `registerAggregator()` method in the DeltaIterativeDataSet or also the `getAggregators()`? The 2nd one will also allow un-registering aggregators and get all registered aggregators, but also has a `registerAggregationConvergenceCriterion()` method (which will give an exception). 
I will open an issue for additional termination criteria in delta iteration, for future consideration :);;;","09/Jun/14 12:50;github-import;[Date: Wed May 07 20:57:20 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Sounds good!;;;","13/Jun/14 14:45;sewen;Merged in 08f189ad37a64b094ba86def8544687419c131ce

This was a contribution from Vasia Kalavri. The merge/rebase screwed up the  commit author.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache: exists() called for files / relative path support,FLINK-730,12719910,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:50,09/Jun/14 12:50,14/Jul/23 05:57,09/Jun/14 12:50,,,,pre-apache,,,,,,,0,github-import,,"The distributed cache now supports relative paths, and checks whether the file actually exists.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/730
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Mon Apr 28 12:39:00 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;pull-request-730-8089559167813254845.patch;https://issues.apache.org/jira/secure/attachment/12649284/pull-request-730-8089559167813254845.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398109,,,Mon Jun 09 12:50:34 UTC 2014,,,,,,,,,,"0|i1wjg7:",398236,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;[Date: Mon Apr 28 13:12:32 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Checking if the file exists is a good idea, but this code here only works for local files. 
I think users can also add files such as ""hdfs:///blabla/"" or ""file:///tmp/abc"". 
But you can probably do the same check using the `FileSystems` class of Stratosphere.;;;","09/Jun/14 12:50;github-import;[Date: Mon Apr 28 13:36:45 CEST 2014, Author: [zentol|https://github.com/zentol]]

good call. it now uses the FileSystem class, and if that fails (which happens if you pass something like ""src/main/..."") it falls back to File class.;;;","09/Jun/14 12:50;github-import;[Date: Mon Apr 28 14:31:54 CEST 2014, Author: [zentol|https://github.com/zentol]]

hmm its not really a nice solution, Ill keep working on it.;;;","09/Jun/14 12:50;github-import;[Date: Wed May 07 11:19:47 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good to me.

@zentol Is this ready, or do you want to further add to it?;;;","09/Jun/14 12:50;github-import;[Date: Wed May 07 11:28:51 CEST 2014, Author: [zentol|https://github.com/zentol]]

its ready.;;;","09/Jun/14 12:50;github-import;[Date: Wed May 07 12:31:39 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [8bc853ac71c5abe6476693360043f518e8c4c44e|https://github.com/stratosphere/stratosphere/commit/8bc853ac71c5abe6476693360043f518e8c4c44e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
added ExecutionEnvironment.registerCachedFile(),FLINK-728,12719908,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:50,09/Jun/14 12:50,14/Jul/23 05:57,09/Jun/14 12:50,,,,pre-apache,,,,,,,0,github-import,,"Instead of
```
Plan p = env.CreateProgramPlan();
p.registerCachedFile(path, ""name"");
PlanExecutor l = new Local-/RemoteExecutor();
l.executePlan(p);
```
you can now write
```
env.registerCachedFile(path, ""name"");
env.execute();
```


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/728
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Mon Apr 28 12:18:49 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;pull-request-728-7535894435221399758.patch;https://issues.apache.org/jira/secure/attachment/12649282/pull-request-728-7535894435221399758.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398107,,,Mon Jun 09 12:50:22 UTC 2014,,,,,,,,,,"0|i1wjfr:",398234,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;[Date: Mon Apr 28 12:19:50 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Very good!;;;","09/Jun/14 12:50;github-import;[Date: Wed May 07 22:49:43 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [06baf89844af2cb07f08a1a55e6c6367afec28d1|https://github.com/stratosphere/stratosphere/commit/06baf89844af2cb07f08a1a55e6c6367afec28d1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default cross operator,FLINK-727,12719907,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:50,09/Jun/14 12:50,14/Jul/23 05:57,09/Jun/14 12:50,,,,pre-apache,,,,,,,0,github-import,,"See ([#659|https://github.com/stratosphere/stratosphere/issues/659] | [FLINK-659|https://issues.apache.org/jira/browse/FLINK-659])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/727
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon Apr 28 10:49:20 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;pull-request-727-6023159630910503956.patch;https://issues.apache.org/jira/secure/attachment/12649281/pull-request-727-6023159630910503956.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398106,,,Mon Jun 09 12:50:17 UTC 2014,,,,,,,,,,"0|i1wjfj:",398233,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;[Date: Wed Apr 30 18:32:13 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Will merge after rebase passed Travis.;;;","09/Jun/14 12:50;github-import;[Date: Wed Apr 30 20:41:52 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Divided Reduce and GroupReduce,FLINK-726,12719906,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:50,09/Jun/14 12:50,14/Jul/23 05:57,09/Jun/14 12:50,,,,pre-apache,,,,,,,0,github-import,,"Divided the Reduce and the GroupReduce classes from each other. So far Reduce was just wrapped into a GroupReduce. Now there are OptimizerClasses, Pact-Driver,... for both. 

I only use the same drivers for the combine case (PARTIAL_GROUP_COMBINE), since the combine logic is exactly the same for both cases so far. (Therefore both Operators implement the new interface GenericCombine now)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/726
Created by: [filiphaase|https://github.com/filiphaase]
Labels: 
Created at: Mon Apr 28 09:54:04 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;pull-request-726-5465959842545378649.patch;https://issues.apache.org/jira/secure/attachment/12649280/pull-request-726-5465959842545378649.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398105,,,Mon Jun 09 12:50:10 UTC 2014,,,,,,,,,,"0|i1wjfb:",398232,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;[Date: Mon May 12 22:18:24 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [de80196067f6a46699e3ecb9d45ebdc68c5da5e8|https://github.com/stratosphere/stratosphere/commit/de80196067f6a46699e3ecb9d45ebdc68c5da5e8];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Added Int, Short and Long Serializer Tests",FLINK-725,12719905,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:50,09/Jun/14 12:50,14/Jul/23 05:57,09/Jun/14 12:50,,,,pre-apache,,,,,,,0,github-import,,"([#589|https://github.com/stratosphere/stratosphere/issues/589] | [FLINK-589|https://issues.apache.org/jira/browse/FLINK-589])

Will continue to add commits for other types in this pull request.

For number types, the test cases are {0, 1, -1, Type.MAX_VALUE, Type.MIN_VALUE, random_value, -random_value}. 

The Type.MIN_VALUE is negative.

random_value may be positive or negative depending on how it was generated. If random_value is negative , then -random_value is positive; and, vice versa. So both negative and positive cases are covered.

I left unused imports in the StringSerializerTest. Is it appropriate to use this pull request to fix it too?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/725
Created by: [aaronchlam|https://github.com/aaronchlam]
Labels: 
Created at: Fri Apr 25 20:52:41 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;pull-request-725-1837277017120844864.patch;https://issues.apache.org/jira/secure/attachment/12649279/pull-request-725-1837277017120844864.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398104,,,Mon Jun 09 12:50:05 UTC 2014,,,,,,,,,,"0|i1wjf3:",398231,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:50;github-import;[Date: Fri Apr 25 21:25:54 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Thanks for this PR. Will look into it later.
Sure, fixing the imports is very welcome!;;;","09/Jun/14 12:50;github-import;[Date: Mon Apr 28 13:38:31 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks, good, I am merging it now.;;;","09/Jun/14 12:50;github-import;[Date: Mon Apr 28 18:18:05 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [afe4d876adcfa7075542b456c472d602e931a6e5|https://github.com/stratosphere/stratosphere/commit/afe4d876adcfa7075542b456c472d602e931a6e5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
./bin/stratosphere client is unable to submit record-API jobs,FLINK-724,12719904,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:49,09/Jun/14 12:49,14/Jul/23 05:57,09/Jun/14 12:49,,,,pre-apache,,,,,,,0,github-import,,"Because it calls the main method instead of getPlan().

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/724
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, java api, 
Created at: Fri Apr 25 16:55:44 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398103,,,Mon Jun 09 12:49:58 UTC 2014,,,,,,,,,,"0|i1wjev:",398230,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:49;github-import;[Date: Fri Apr 25 17:00:06 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

The error has probably been introduced by https://github.com/stratosphere/stratosphere/commit/ca41f6aec9d87d57c08aae38a9516754683cbfc7
```
./bin/stratosphere run -j ../../rmetzger/testjob/target/testjob-0.1-SNAPSHOT.jar -c eu.stratosphere.test.testPlan.DistributedCacheTest  -a 2 hdfs:///user/robert/datasets/a hdfs:///user/robert/datasets/b hdfs:///user/robert/test/distCa
```;;;","09/Jun/14 12:49;github-import;[Date: Fri Apr 25 17:04:29 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

The issue is invalid, sorry.;;;","09/Jun/14 12:49;github-import;[Date: Fri Apr 25 17:45:46 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

What was the source of the problem then? I am curious...;;;","09/Jun/14 12:49;github-import;[Date: Fri Apr 25 17:48:45 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Well, its unbelievably stupid. 
I forgot to implement the `Program` interface for the main class.;;;","09/Jun/14 12:49;github-import;[Date: Fri Apr 25 17:50:05 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Happens ;-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"(Re)add TransitiveClosureNaive, fix join, add ITCase",FLINK-723,12719903,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:49,09/Jun/14 12:49,14/Jul/23 05:57,09/Jun/14 12:49,,,,pre-apache,,,,,,,0,github-import,,"Scala Join took left key selector for both sides of join. Re-enable
union. TransitiveClosureNaive uses union, this is tested in the ITCase.

Properly disable GlobalSchemaGenerator, the information it created has
not been used, only local field positions have been used.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/723
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri Apr 25 15:51:25 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:49;github-import;pull-request-723-7076498480447930416.patch;https://issues.apache.org/jira/secure/attachment/12649278/pull-request-723-7076498480447930416.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398102,,,Mon Jun 09 12:49:53 UTC 2014,,,,,,,,,,"0|i1wjen:",398229,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:49;github-import;[Date: Fri Apr 25 15:52:20 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

@StephanEwen  I could not think of an example where union fails, since we only use local positions and I completely disabled the ""global schema generator"".;;;","09/Jun/14 12:49;github-import;[Date: Fri Apr 25 16:01:22 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, very good. I will merge this soon.;;;","09/Jun/14 12:49;github-import;[Date: Fri Apr 25 19:10:42 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]



I am getting errors in the integration tests.

```
eu.stratosphere.types.KeyFieldOutOfBoundsException: Field 1 is accessed for a key, but out of bounds in the record.
	at eu.stratosphere.pact.runtime.plugable.pactrecord.RecordComparator.hash(RecordComparator.java:199)
	at eu.stratosphere.pact.runtime.shipping.RecordOutputEmitter.hashPartitionDefault(RecordOutputEmitter.java:145)
	at eu.stratosphere.pact.runtime.shipping.RecordOutputEmitter.selectChannels(RecordOutputEmitter.java:114)
	at eu.stratosphere.pact.runtime.shipping.RecordOutputEmitter.selectChannels(RecordOutputEmitter.java:23)
	at eu.stratosphere.nephele.io.RuntimeOutputGate.writeRecord(RuntimeOutputGate.java:259)
	at eu.stratosphere.nephele.io.AbstractRecordWriter.emit(AbstractRecordWriter.java:92)
	at eu.stratosphere.pact.runtime.shipping.RecordOutputCollector.collect(RecordOutputCollector.java:77)
	at eu.stratosphere.pact.runtime.shipping.RecordOutputCollector.collect(RecordOutputCollector.java:29)
	at eu.stratosphere.examples.scala.relational.WebLogAnalysis$$anon$18.map(WebLogAnalysis.scala:105)
	at eu.stratosphere.examples.scala.relational.WebLogAnalysis$$anon$18.map(WebLogAnalysis.scala:105)
	at eu.stratosphere.pact.runtime.task.chaining.ChainedCollectorMapDriver.collect(ChainedCollectorMapDriver.java:71)
	at eu.stratosphere.examples.scala.relational.WebLogAnalysis$$anon$17.map(WebLogAnalysis.scala:105)
	at eu.stratosphere.examples.scala.relational.WebLogAnalysis$$anon$17.map(WebLogAnalysis.scala:105)
	at eu.stratosphere.pact.runtime.task.chaining.ChainedCollectorMapDriver.collect(ChainedCollectorMapDriver.java:71)
	at eu.stratosphere.examples.scala.relational.WebLogAnalysis$$anon$13.map(WebLogAnalysis.scala:90)
	at eu.stratosphere.examples.scala.relational.WebLogAnalysis$$anon$13.map(WebLogAnalysis.scala:90)
	at eu.stratosphere.pact.runtime.task.chaining.ChainedCollectorMapDriver.collect(ChainedCollectorMapDriver.java:71)
	at eu.stratosphere.pact.runtime.task.DataSourceTask.invoke(DataSourceTask.java:168)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:345)
	at java.lang.Thread.run(Thread.java:744)
```;;;","09/Jun/14 12:49;github-import;[Date: Sat Apr 26 13:22:16 CEST 2014, Author: [aljoscha|https://github.com/aljoscha]]

Yes I did too. Disabling the global schema generator made a bug appear. For some reason (my oversight) join and coGroup did not correctly set the keys retrieved from the key selectors. But now it works, travis only failed in some cleanup test in one of the 6 different verify runs.
 ;;;","09/Jun/14 12:49;github-import;[Date: Mon Apr 28 18:19:14 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [799f7a9f64d6b06b1bf73749f273e4759218c317|https://github.com/stratosphere/stratosphere/commit/799f7a9f64d6b06b1bf73749f273e4759218c317];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Example - Triangles - DataType indice error,FLINK-722,12719902,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:49,09/Jun/14 12:49,14/Jul/23 05:57,09/Jun/14 12:49,,,,pre-apache,,,,,,,0,github-import,,"The defined indices in the EdgeWithDegrees class don't match the indizes used in the constructor. If you were to group over V2, you would use the first degree instead of the second vertex if the edge was build with the constructor.
https://github.com/stratosphere/stratosphere/blob/master/stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/triangles/util/EdgeDataTypes.java

```
public static class EdgeWithDegrees extends Tuple4<Integer, Integer, Integer, Integer> {
	public static final int V1 = 0;
	public static final int V2 = 1;
	public static final int D1 = 2;
	public static final int D2 = 3;

	public EdgeWithDegrees(final Integer vertex1, final Integer degree1, final Integer vertex2, final Integer degree2) {
		super(vertex1, degree1, vertex2, degree2);
	}
...
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/722
Created by: [zentol|https://github.com/zentol]
Labels: bug, documentation, 
Milestone: Release 0.5
Created at: Fri Apr 25 14:57:02 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398101,,,Mon Jun 09 12:49:46 UTC 2014,,,,,,,,,,"0|i1wjef:",398228,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:49;github-import;[Date: Fri Apr 25 15:14:17 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Oh, yes, you're right!
The constructor isn't used in the code, that's probably why the bug did not cause the tests to fail.
We could simply remove it...;;;","09/Jun/14 12:49;github-import;[Date: Mon Apr 28 18:27:33 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fix provided by ([#722|https://github.com/stratosphere/stratosphere/issues/722] | [FLINK-722|https://issues.apache.org/jira/browse/FLINK-722]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A Connected Components test case which requires 2 joins with the solution set,FLINK-721,12719900,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:49,09/Jun/14 12:49,14/Jul/23 05:57,09/Jun/14 12:49,,,,pre-apache,,,,,,,0,github-import,,"This is a Connected Components test case, which recomputes only the elements of the solution set whose at least one dependency (in-neighbor) has changed since the last iteration.
It is used to test the multiple joins with solution set feature (see ([#162|https://github.com/stratosphere/stratosphere/issues/162] | [FLINK-162|https://issues.apache.org/jira/browse/FLINK-162])).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/721
Created by: [vasia|https://github.com/vasia]
Labels: 
Created at: Fri Apr 25 12:44:14 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:49;github-import;pull-request-721-6616767509133088687.patch;https://issues.apache.org/jira/secure/attachment/12649277/pull-request-721-6616767509133088687.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398099,,,Mon Jun 09 12:49:42 UTC 2014,,,,,,,,,,"0|i1wjdz:",398226,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:49;github-import;[Date: Sat Apr 26 02:02:27 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [c8dbe684b229cb463359b819e898de6e7d248f32|https://github.com/stratosphere/stratosphere/commit/c8dbe684b229cb463359b819e898de6e7d248f32]

Thanks for the contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add projection for cross,FLINK-719,12719898,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:49,09/Jun/14 12:49,14/Jul/23 05:57,09/Jun/14 12:49,,,,pre-apache,,,,,,,0,github-import,,"Almost same as projection for join in PR ([#707|https://github.com/stratosphere/stratosphere/issues/707] | [FLINK-707|https://issues.apache.org/jira/browse/FLINK-707])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/719
Created by: [qmlmoon|https://github.com/qmlmoon]
Labels: 
Created at: Thu Apr 24 22:35:34 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:49;github-import;pull-request-719-4431013406687567865.patch;https://issues.apache.org/jira/secure/attachment/12649276/pull-request-719-4431013406687567865.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398097,,,Mon Jun 09 12:49:33 UTC 2014,,,,,,,,,,"0|i1wjdj:",398224,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:49;github-import;[Date: Fri Apr 25 09:54:55 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Thanks @qmlmoon! Will have a look at it later.;;;","09/Jun/14 12:49;github-import;[Date: Wed Apr 30 18:32:18 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Will merge after rebase passed Travis.;;;","09/Jun/14 12:49;github-import;[Date: Wed Apr 30 20:41:46 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Various minor fixes:,FLINK-718,12719897,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:49,09/Jun/14 12:49,14/Jul/23 05:57,09/Jun/14 12:49,,,,pre-apache,,,,,,,0,github-import,,"- better exceptions for faulty temporary directories
- better exception if client can not connect to JobManager

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/718
Created by: [rmetzger|https://github.com/rmetzger]
Labels: build system, 
Milestone: Release 0.5
Created at: Wed Apr 23 14:31:14 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:49;github-import;pull-request-718-8423982879526460878.patch;https://issues.apache.org/jira/secure/attachment/12649275/pull-request-718-8423982879526460878.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398096,,,Mon Jun 09 12:49:26 UTC 2014,,,,,,,,,,"0|i1wjdb:",398223,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:49;github-import;[Date: Wed Apr 23 14:32:09 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Also, the PR correctly enforces maven 3.0.3.
I was unable to create a maven enforcer rule that fails on oraclejdk6. (there is a way by writing a custom rule in Java).;;;","09/Jun/14 12:49;github-import;[Date: Thu Apr 24 11:03:27 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I have only one minor comment.
Otherwise good to merge.;;;","09/Jun/14 12:49;github-import;[Date: Thu Apr 24 15:21:37 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Agree, good to merge;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mixed key types for join,FLINK-715,12719894,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:49,09/Jun/14 12:49,14/Jul/23 05:57,09/Jun/14 12:49,,,,pre-apache,,,,,,,0,github-import,,"See ([#675|https://github.com/stratosphere/stratosphere/issues/675] | [FLINK-675|https://issues.apache.org/jira/browse/FLINK-675])
For some reasons one of the tests is already running because copy() in the Avro Serializer is not called for it. I suppose it has something to do with one dataset being very small so that the optimizer decides for some different strategy.
The other test has copy() support for the Avro Serializer pending.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/715
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Wed Apr 23 13:10:31 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:49;github-import;pull-request-715-768320518275575595.patch;https://issues.apache.org/jira/secure/attachment/12649272/pull-request-715-768320518275575595.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398093,,,Mon Jun 09 12:49:10 UTC 2014,,,,,,,,,,"0|i1wjcn:",398220,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:49;github-import;[Date: Wed Apr 23 18:36:50 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I checked the other tests by forcing the input with the custom type to be the build side of the join. That way, the custom types do not need to be copied.
The test passed.
I will merge this PR.;;;","09/Jun/14 12:49;github-import;[Date: Thu Apr 24 00:19:28 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged in [d056a43b13e23fb2d2e2db0cf0577f341c0daa09|https://github.com/stratosphere/stratosphere/commit/d056a43b13e23fb2d2e2db0cf0577f341c0daa09];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Combinable GroupReduce for new Java API,FLINK-713,12719892,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:48,09/Jun/14 12:48,14/Jul/23 05:57,09/Jun/14 12:48,,,,pre-apache,,,,,,,0,github-import,,"See ([#700|https://github.com/stratosphere/stratosphere/issues/700] | [FLINK-700|https://issues.apache.org/jira/browse/FLINK-700])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/713
Created by: [markus-h|https://github.com/markus-h]
Labels: bug, java api, 
Milestone: Release 0.5
Created at: Tue Apr 22 18:37:38 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:48;github-import;pull-request-713-8971251198972908462.patch;https://issues.apache.org/jira/secure/attachment/12649271/pull-request-713-8971251198972908462.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398091,,,Mon Jun 09 12:48:57 UTC 2014,,,,,,,,,,"0|i1wjc7:",398218,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:48;github-import;[Date: Wed Apr 23 17:45:38 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I added a few comments mainly regarding improving the runtime overhead of combiners.
The tests look good!;;;","09/Jun/14 12:48;github-import;[Date: Thu Apr 24 11:44:16 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

Thanks for your comments! I implemented your proposed changes.;;;","09/Jun/14 12:48;github-import;[Date: Thu Apr 24 11:58:19 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Great, thanks for the update! There's just one tiny check that should be removed. 
Otherwise, the PR is ready to merge.;;;","09/Jun/14 12:48;github-import;[Date: Mon Apr 28 09:16:22 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I removed the check;;;","09/Jun/14 12:48;github-import;[Date: Wed Apr 30 18:32:26 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Will merge after rebase passed Travis.;;;","09/Jun/14 12:48;github-import;[Date: Wed Apr 30 20:41:37 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoGroup does not call combineFirst/Second,FLINK-712,12719891,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:48,09/Jun/14 12:48,14/Jul/23 05:57,09/Jun/14 12:48,,,,pre-apache,,,,,,,0,github-import,,"I tried to write a test for combinable cogroup (see  ([#548|https://github.com/stratosphere/stratosphere/issues/548] | [FLINK-548|https://issues.apache.org/jira/browse/FLINK-548])). I extended the new API to set the correct flag for combinableFirst/Seconed so that it is correctly passed to the optimizer, but the combine methods are not called. I tried to right click in eclipse and ""open call hierarchy"" on the combine methods (usually that gives me the place where the method is called) but it stayed empty. Is there runtime support von combineFirst/Second on cogroup? If yes, in what driver are the combine methods called? 
I do not find any example where combineFirst/Second are used. I tested it with the CoGroupConnectedComponentsITCase and set a breakpoint inside the combineFirst but it is not called.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/712
Created by: [markus-h|https://github.com/markus-h]
Labels: bug, java api, 
Milestone: Release 0.5
Created at: Tue Apr 22 17:47:44 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398090,,,Mon Jun 09 12:48:49 UTC 2014,,,,,,,,,,"0|i1wjbz:",398217,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:48;github-import;[Date: Tue Apr 22 18:40:15 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

There is currently no runtime or optimizer support for CoGroup combining.
The functioned are placeholders for now.;;;","09/Jun/14 12:48;github-import;[Date: Tue Apr 22 18:45:12 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

Ok, I didn't know that.;;;","09/Jun/14 12:48;github-import;[Date: Tue Apr 22 22:37:16 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I think in this case we should remove / deactivate the combine methods in the `CoGroupFunction` class.;;;","09/Jun/14 12:48;github-import;[Date: Thu May 15 18:04:27 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I'll take care of this;;;","09/Jun/14 12:48;github-import;[Date: Thu May 15 23:42:09 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

`combineFirst()` and `combineSecond()` have been removed in this commit [a6d188cda8ba527db6e0f3cee57df85821be53af|https://github.com/stratosphere/stratosphere/commit/a6d188cda8ba527db6e0f3cee57df85821be53af].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Testcase for filter with broadcast variable,FLINK-709,12719888,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:48,09/Jun/14 12:48,14/Jul/23 05:57,09/Jun/14 12:48,,,,pre-apache,,,,,,,0,github-import,,"See ([#548|https://github.com/stratosphere/stratosphere/issues/548] | [FLINK-548|https://issues.apache.org/jira/browse/FLINK-548])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/709
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Tue Apr 22 09:15:10 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:48;github-import;pull-request-709-2884496249615022605.patch;https://issues.apache.org/jira/secure/attachment/12649270/pull-request-709-2884496249615022605.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398087,,,Mon Jun 09 12:48:16 UTC 2014,,,,,,,,,,"0|i1wjbb:",398214,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:48;github-import;[Date: Tue Apr 22 10:11:26 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Thanks for extending the tests!;;;","09/Jun/14 12:48;github-import;[Date: Wed Apr 23 16:20:59 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Will merge this PR;;;","09/Jun/14 12:48;github-import;[Date: Thu Apr 24 00:18:33 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged in [63b0d8c8b147dfbb9277f6277aba2d8f959729ef|https://github.com/stratosphere/stratosphere/commit/63b0d8c8b147dfbb9277f6277aba2d8f959729ef];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Flink with Apache Hadoop 2.4,FLINK-708,12719887,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,github-import,09/Jun/14 12:48,10/Jun/14 19:44,14/Jul/23 05:57,10/Jun/14 19:44,pre-apache,pre-apache-0.5,,pre-apache-0.5.1,,,,,,,0,github-import,,"Hadoop has released version 2.4, we need to test Stratosphere regarding YARN and HDFS with that version.

http://hortonworks.com/blog/apache-hadoop-2-4-0-released/


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/708
Created by: [rmetzger|https://github.com/rmetzger]
Labels: build system, simple-issue, YARN, 
Milestone: Release 0.5.1
Created at: Tue Apr 22 09:10:29 CEST 2014
State: open
",,githubbot,github-import,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398086,,,Tue Jun 10 19:44:45 UTC 2014,,,,,,,,,,"0|i1wjb3:",398213,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:48;github-import;[Date: Fri May 02 09:26:30 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

First experiments indicate that our YARN client is not working with Hadoop 2.4;;;","09/Jun/14 12:48;github-import;[Date: Tue May 20 19:58:11 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Are you planning to fix this for 0.5, or should we postpone that for 0.5.1?;;;","09/Jun/14 12:48;github-import;[Date: Tue May 20 20:01:04 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I think we should postpone this to 0.6 
(I will be strictly against making 0.5.1 a feature release);;;","09/Jun/14 12:48;github-import;[Date: Tue May 20 20:02:23 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

(The question is whether hadoop 2.4 support is a feature or a bugfix ;) );;;","09/Jun/14 12:48;github-import;[Date: Tue May 20 20:04:41 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I would consider that a bugfix, not a feature release.;;;","09/Jun/14 12:48;github-import;[Date: Tue May 20 22:19:12 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I'm fine either way. Let's postpone this one...;;;","09/Jun/14 12:48;github-import;[Date: Wed May 21 00:11:37 CEST 2014, Author: [uce|https://github.com/uce]]

Imo it would also be a bugfix.

> On 20 May 2014, at 22:19, Fabian Hueske <notifications@github.com> wrote:
> 
> I'm fine either way. Let's postpone this one...
> 
> —
> Reply to this email directly or view it on GitHub.;;;","10/Jun/14 16:51;githubbot;GitHub user rmetzger opened a pull request:

    https://github.com/apache/incubator-flink/pull/1

    Fix for FLINK-708 and FLINK-887 

    I tested Flink on Amazon EMR (Hadoop 2.4.0). It was necessary to change some code due to a API change in YARN. Flink should now support all Hadoop versions >= 2.2.0.
    
    I also fixed the JobManager heap space calculation for YARN. The JM sometimes failed one large jobs.
    
    I would like to include the commit into the 0.5.1 release.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rmetzger/incubator-flink yarnJMMem

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/1.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1
    
----
commit 3842385a4c5a4dcaa844b94ba5897d022b780c6e
Author: Robert Metzger <rmetzger@apache.org>
Date:   2014-06-10T15:34:00Z

    Fix for FLINK-708 (Hadoop 2.4 compatibility) and FLINK-887 (YARN Jobmanager heapspace calc)

----
;;;","10/Jun/14 16:51;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/1#issuecomment-45640561
  
    Travis build (in my account) https://travis-ci.org/rmetzger/incubator-flink/builds/27237521
;;;","10/Jun/14 19:42;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/1#issuecomment-45661714
  
    merged in c0e76bc0cc296b23df98491ea730a73b43577ddf
;;;","10/Jun/14 19:42;githubbot;Github user rmetzger closed the pull request at:

    https://github.com/apache/incubator-flink/pull/1
;;;","10/Jun/14 19:44;rmetzger;I successfully tested Flink on Amazon EMR with Hadoop 2.4.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added kryo-serializer and simple test,FLINK-706,12719885,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:47,09/Jun/14 12:47,14/Jul/23 05:57,09/Jun/14 12:47,,,,pre-apache,,,,,,,0,github-import,,"Related to issue ([#610|https://github.com/stratosphere/stratosphere/issues/610] | [FLINK-610|https://issues.apache.org/jira/browse/FLINK-610]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/706
Created by: [alexff91|https://github.com/alexff91]
Labels: 
Created at: Fri Apr 18 22:33:31 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:47;github-import;pull-request-706-6936793392381193298.patch;https://issues.apache.org/jira/secure/attachment/12649268/pull-request-706-6936793392381193298.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398084,,,Mon Jun 09 12:47:57 UTC 2014,,,,,,,,,,"0|i1wjan:",398211,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:47;github-import;[Date: Fri Apr 25 10:45:22 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi @alexff91,

thank you for the pull request. I'm sorry that you didn't hear anything from us yet. We are all quite busy right know, but I know from a personal conversation with @StephanEwen that he'll soon take a look over your pull request.;;;","09/Jun/14 12:47;github-import;[Date: Thu May 01 12:06:49 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I am merging this together with ([#746|https://github.com/stratosphere/stratosphere/issues/746] | [FLINK-746|https://issues.apache.org/jira/browse/FLINK-746]) later;;;","09/Jun/14 12:47;github-import;[Date: Tue May 06 17:04:30 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [9050111bd30f61c7fed6e545b285c0e945673a82|https://github.com/stratosphere/stratosphere/commit/9050111bd30f61c7fed6e545b285c0e945673a82]

Thanks for the pull request!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add test for String Serializer for #589,FLINK-702,12719881,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:47,09/Jun/14 12:47,14/Jul/23 05:57,09/Jun/14 12:47,,,,pre-apache,,,,,,,0,github-import,,"([#589|https://github.com/stratosphere/stratosphere/issues/589] | [FLINK-589|https://issues.apache.org/jira/browse/FLINK-589])

First test is the String Serializer. One thing that I'm wondering about the StringSerializer is whether it should be able to handle ```null``` values. I took the test strings from ```stratosphere-core.src.test.java.eu.stratosphere.types.StringSerializationTest.java``` as the test strings for the StringSerializer.

It was able to handle ```new String[] {""a"", """", ""bcd"", ""jbmbmner8 jhk hj \n \t üäßß@µ"", """", ""non-empty""}```
 but not ```new String[] {""a"", null, """", null, ""bcd"", null, ""jbmbmner8 jhk hj \n \t üäßß@µ"", null, """", null, ""non-empty""}```.

I am not sure if this is intended, so I have kept the array that does not include null for now.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/702
Created by: [aaronchlam|https://github.com/aaronchlam]
Labels: 
Created at: Thu Apr 17 22:57:58 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:47;github-import;pull-request-702-3116977208101206320.patch;https://issues.apache.org/jira/secure/attachment/12649267/pull-request-702-3116977208101206320.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398080,,,Mon Jun 09 12:47:40 UTC 2014,,,,,,,,,,"0|i1wj9r:",398207,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:47;github-import;[Date: Sat Apr 19 23:59:33 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Hi @aaronchlam,

thanks for submitting this PR! 
Stratosphere does not support handling of null-valued data objects. So, it is fine that the tests do not include null-valued Strings.
I am currently on easter vacations and will have a detailed look at the PR next week.;;;","09/Jun/14 12:47;github-import;[Date: Wed Apr 23 13:45:01 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Thanks, that one is good to merge.;;;","09/Jun/14 12:47;github-import;[Date: Wed Apr 23 18:39:07 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I will merge this PR;;;","09/Jun/14 12:47;github-import;[Date: Thu Apr 24 00:18:55 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged in [09632ed2372784806e5602f298ae5b7e7cb16259|https://github.com/stratosphere/stratosphere/commit/09632ed2372784806e5602f298ae5b7e7cb16259];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GroupReduce with Key Selection Function does not call combine(),FLINK-700,12719879,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:47,09/Jun/14 12:47,14/Jul/23 05:57,09/Jun/14 12:47,,,,pre-apache,,,,,,,0,github-import,,"The `TupleUnwrappingGroupReducer` implements an empty `combine()` which means that a `GroupReduceFunction` that is used on a `Grouping` with `KeySelector` cannot use its combiner.

While digging a bit deeper into the code, I am not sure, if a `GroupReduceFunction` will ever use its combiner (even in `Grouping` with field positions and maybe even when applied without a `Grouping`). The `GroupReduceOperatorBase` has a flag attribute `combinable` which seems to be always false, hence telling the optimizer that all `GroupReduceFunction`s do not support `combine()`.

We should check this!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/700
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, java api, 
Milestone: Release 0.5
Created at: Wed Apr 16 16:38:40 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398078,,,Mon Jun 09 12:47:32 UTC 2014,,,,,,,,,,"0|i1wj9b:",398205,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:47;github-import;[Date: Wed Apr 16 16:42:47 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

The `GroupReduceITCase` should be extended to check if the combiner of a combinable `GroupReduceFunction` is called for non-grouped, field-pos-grouped, and key-selector-grouped datasets.;;;","09/Jun/14 12:47;github-import;[Date: Tue Apr 22 09:53:09 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I started working on this issue.;;;","09/Jun/14 12:47;github-import;[Date: Tue Apr 22 17:03:12 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

Is there runtime support for combine in combination with an all-reduce?;;;","09/Jun/14 12:47;github-import;[Date: Tue Apr 22 17:58:21 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The old Java API had that. Once we add the Combinable annotation/flag to
the new API, it should work for both the grouped reduce and the all-reduce.;;;","09/Jun/14 12:47;github-import;[Date: Tue Apr 22 18:28:48 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I am working on this that is why I'm asking. I set the flag, but as far as I can see only the AllReduceDriver is used and it does not call the combine method. 
After post pass the source has driver strategy NONE and the reduce has ALL_GROUP. For the pact contract combinable is true. The combine method in my udf is not called.;;;","09/Jun/14 12:47;github-import;[Date: Tue Apr 22 18:41:38 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The optimizer plan does not inject a combine node?;;;","09/Jun/14 12:47;github-import;[Date: Tue Apr 22 18:46:23 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

As far as I can see: no.;;;","09/Jun/14 12:47;github-import;[Date: Tue Apr 29 10:50:14 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I spent some more time on this issue. I think the compiler sees that the DataSource is directly locally connected to the all-reduce UDF function, therefore it would not be of any use to plug in a combiner locally. So in my opinion it is not a bug, but a feature. ;;;","09/Jun/14 12:47;github-import;[Date: Tue Apr 29 10:58:25 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Can you try to increase the DOP (> 1) of the DataSource? Then, the data must be shuffled and a Combiner would be important to reduce the shipped data volume.;;;","09/Jun/14 12:47;github-import;[Date: Tue Apr 29 11:22:27 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

Even with DOP > 1 the data is not shipped through the network and therefor the shipping strategy remains FORWARD and everything is passed locally. Since we have an all-reduce there is no real shuffling as long as we stay on one node.
I think in a cluster environment it should work.;;;","09/Jun/14 12:47;github-import;[Date: Tue Apr 29 11:30:15 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you try it with a LocalDistributedExecutor? Then you should see
shipping behavior.;;;","09/Jun/14 12:47;github-import;[Date: Tue Apr 29 12:26:34 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I have some trouble to get the testcase to run with LocalDistributedExecutor. I tried a naive
```LocalDistributedExecutor lde = new LocalDistributedExecutor();
				lde.start(2);
				lde.run(env.createProgramPlan());```
but I get different kind of errors.;;;","09/Jun/14 12:47;github-import;[Date: Tue Apr 29 12:44:15 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you post those?;;;","09/Jun/14 12:47;github-import;[Date: Wed Apr 30 20:44:35 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

The original issue (combine for key-selector reduce operators) has been resolved.

@markus-h Can you open an issue for combine support for all-reduce transformations?
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added Java 8,FLINK-699,12719878,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:47,09/Jun/14 12:47,14/Jul/23 05:57,09/Jun/14 12:47,,,,pre-apache,,,,,,,0,github-import,,"Fixes issue ([#649|https://github.com/stratosphere/stratosphere/issues/649] | [FLINK-649|https://issues.apache.org/jira/browse/FLINK-649])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/699
Created by: [JonathanH5|https://github.com/JonathanH5]
Labels: 
Created at: Wed Apr 16 15:14:29 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:47;github-import;pull-request-699-2557790000919289477.patch;https://issues.apache.org/jira/secure/attachment/12649266/pull-request-699-2557790000919289477.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398077,,,Mon Jun 09 12:47:21 UTC 2014,,,,,,,,,,"0|i1wj93:",398204,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:47;github-import;[Date: Wed Apr 16 22:12:42 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged in [ef72b32051e4d906b591e8ef98a79d2768b78f4a|https://github.com/stratosphere/stratosphere/commit/ef72b32051e4d906b591e8ef98a79d2768b78f4a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Moved SerializerTestBase to stratosphere-core and resolved Maven dependencies,FLINK-697,12719876,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:47,09/Jun/14 12:47,14/Jul/23 05:57,09/Jun/14 12:47,,,,pre-apache,,,,,,,0,github-import,,"Moved the SerializerTestBase from stratosphere-java to stratosphere-core where the abstract TypeSerializer class and all base type serializers are defined.
Had add some Maven stuff to get the dependency on test code working.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/697
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Wed Apr 16 10:58:52 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:47;github-import;pull-request-697-2109490828705377140.patch;https://issues.apache.org/jira/secure/attachment/12649264/pull-request-697-2109490828705377140.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398075,,,Mon Jun 09 12:47:09 UTC 2014,,,,,,,,,,"0|i1wj8n:",398202,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:47;github-import;[Date: Wed Apr 16 23:51:55 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Was merged in [4fe742635f3a6dced7b9bf66f9c34ef7c1e053dc|https://github.com/stratosphere/stratosphere/commit/4fe742635f3a6dced7b9bf66f9c34ef7c1e053dc];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GroupReduce testcases for the new java api,FLINK-695,12719874,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:46,09/Jun/14 12:46,14/Jul/23 05:57,09/Jun/14 12:46,,,,pre-apache,,,,,,,0,github-import,,"See ([#548|https://github.com/stratosphere/stratosphere/issues/548] | [FLINK-548|https://issues.apache.org/jira/browse/FLINK-548]).
I had to do a small fix since for some reason the TupleUnwrappingIterator was not instantiated.

The descending sorting of tuples inside of the groups is not working yet. I think this is connected to ([#588|https://github.com/stratosphere/stratosphere/issues/588] | [FLINK-588|https://issues.apache.org/jira/browse/FLINK-588]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/695
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Tue Apr 15 12:26:11 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:46;github-import;pull-request-695-3093703553796604952.patch;https://issues.apache.org/jira/secure/attachment/12649262/pull-request-695-3093703553796604952.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398073,,,Mon Jun 09 12:46:57 UTC 2014,,,,,,,,,,"0|i1wj87:",398200,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:46;github-import;[Date: Wed Apr 16 22:13:44 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged in [9e5e73a0ae26f0471ede62025f2751fdb3685dea|https://github.com/stratosphere/stratosphere/commit/9e5e73a0ae26f0471ede62025f2751fdb3685dea];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implements the ascending flag for basic type comparators.,FLINK-693,12719872,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:46,09/Jun/14 12:46,14/Jul/23 05:57,09/Jun/14 12:46,,,,pre-apache,,,,,,,0,github-import,,"Solves ([#588|https://github.com/stratosphere/stratosphere/issues/588] | [FLINK-588|https://issues.apache.org/jira/browse/FLINK-588]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/693
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Tue Apr 15 10:24:06 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:46;github-import;pull-request-693-3474699323716856249.patch;https://issues.apache.org/jira/secure/attachment/12649261/pull-request-693-3474699323716856249.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398071,,,Mon Jun 09 12:46:48 UTC 2014,,,,,,,,,,"0|i1wj7r:",398198,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:46;github-import;[Date: Wed Apr 16 23:36:35 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merged in [c9fdac03f848d9c231cf0460eb34d832831ef11e|https://github.com/stratosphere/stratosphere/commit/c9fdac03f848d9c231cf0460eb34d832831ef11e]
Does not completely solve ([#588|https://github.com/stratosphere/stratosphere/issues/588] | [FLINK-588|https://issues.apache.org/jira/browse/FLINK-588]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Testcases for cross operator in new java API,FLINK-692,12719871,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:46,09/Jun/14 12:46,14/Jul/23 05:57,09/Jun/14 12:46,,,,pre-apache,,,,,,,0,github-import,,"See ([#548|https://github.com/stratosphere/stratosphere/issues/548] | [FLINK-548|https://issues.apache.org/jira/browse/FLINK-548]).
Due to a UnsupportedOperationException in the AvroSerializer the cross is currently not working for custom type objects. I created an issue for that: ([#691|https://github.com/stratosphere/stratosphere/issues/691] | [FLINK-691|https://issues.apache.org/jira/browse/FLINK-691])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/692
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon Apr 14 15:56:30 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:46;github-import;pull-request-692-4149741485799239719.patch;https://issues.apache.org/jira/secure/attachment/12649260/pull-request-692-4149741485799239719.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398070,,,Mon Jun 09 12:46:43 UTC 2014,,,,,,,,,,"0|i1wj7j:",398197,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:46;github-import;[Date: Wed Apr 16 22:15:19 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Merge at current state in [c9aa4fd453ef48e62fdf66d7e8ed519f282152a0|https://github.com/stratosphere/stratosphere/commit/c9aa4fd453ef48e62fdf66d7e8ed519f282152a0]
Tests for custom objects need to be activated once ([#691|https://github.com/stratosphere/stratosphere/issues/691] | [FLINK-691|https://issues.apache.org/jira/browse/FLINK-691]) has been resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cross (and probably Join) not working for custom type objects,FLINK-691,12719870,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:46,09/Jun/14 12:46,14/Jul/23 05:57,09/Jun/14 12:46,,,,pre-apache,,,,,,,0,github-import,,"By creating the testcases for the cross operator I noticed that it is currently not possible to use cross on custom type objects.
The reason for that is that the two copy methods in the AvroSerializer are currently not implemented.
The same problem probably applies for the join operator.

I think this issue is quite critical. Testcases for the cross are already created (withoud warranty for correctness) but commented out.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/691
Created by: [markus-h|https://github.com/markus-h]
Labels: bug, java api, 
Milestone: Release 0.5
Created at: Mon Apr 14 15:56:09 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398069,,,Mon Jun 09 12:46:38 UTC 2014,,,,,,,,,,"0|i1wj7b:",398196,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:46;github-import;[Date: Wed Apr 16 22:16:28 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

`CrossITCase` contains a deactivated test case to check for the problem.
Once, this issue is fixed, the test should be activated.;;;","09/Jun/14 12:46;github-import;[Date: Fri Apr 25 09:53:31 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Moving from Avro to [Kryo|https://github.com/EsotericSoftware/kryo] might be the way to go here.
Kryo's website states: 

*""Kryo is a fast and efficient object graph serialization framework for Java. The goals of the project are speed, efficiency, and an easy to use API. The project is useful any time objects need to be persisted, whether to a file, database, or over the network. Kryo can also perform automatic deep and shallow copying/cloning. This is direct copying from object to object, not object->bytes->object.""*

There is also an issue ([#610|https://github.com/stratosphere/stratosphere/issues/610] | [FLINK-610|https://issues.apache.org/jira/browse/FLINK-610]) and a PR for this ([#706|https://github.com/stratosphere/stratosphere/issues/706] | [FLINK-706|https://issues.apache.org/jira/browse/FLINK-706]) ;;;","09/Jun/14 12:46;github-import;[Date: Thu May 01 10:41:39 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I'm looking into this issue.;;;","09/Jun/14 12:46;github-import;[Date: Thu May 01 12:02:43 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I actually have a kryo serializer running (more or less. still need to test how it behaves in corner cases). Will merge it today hopefully.;;;","09/Jun/14 12:46;github-import;[Date: Thu May 01 12:04:22 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

So its an alternative implementation to https://github.com/stratosphere/stratosphere/pull/706 ?
If yes, tell the student to avoid any additional work on his side.;;;","09/Jun/14 12:46;github-import;[Date: Thu May 01 12:05:34 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It is baesd on his pull request, but i fixed it up similarly as you did. I
still want to investigate how it behaves with generic classes and so. I
think I will make a mix of his, your, and my pull request ;-)


On Thu, May 1, 2014 at 12:04 PM, Robert Metzger <notifications@github.com>wrote:

> So its an alternative implementation to ([#706|https://github.com/stratosphere/stratosphere/issues/706] | [FLINK-706|https://issues.apache.org/jira/browse/FLINK-706])<https://github.com/stratosphere/stratosphere/pull/706>?
> If yes, tell the student to avoid any additional work on his side.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/691#issuecomment-41896931>
> .
>;;;","09/Jun/14 12:46;github-import;[Date: Wed May 14 17:58:23 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed with the introducetion of Kryo in the generic serializers/comparators;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactingHashTable Integration,FLINK-690,12719869,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:46,09/Jun/14 12:46,14/Jul/23 05:57,09/Jun/14 12:46,,,,pre-apache,,,,,,,0,github-import,,"CompactingHashTable and integration in iterations code

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/690
Created by: [rwaury|https://github.com/rwaury]
Labels: 
Created at: Mon Apr 14 14:28:08 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:46;github-import;pull-request-690-6564454118026048574.patch;https://issues.apache.org/jira/secure/attachment/12649259/pull-request-690-6564454118026048574.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398068,,,2014-06-09 12:46:25.0,,,,,,,,,,"0|i1wj73:",398195,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added support for local distributed execution environments.,FLINK-686,12719864,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:45,09/Jun/14 12:46,14/Jul/23 05:57,09/Jun/14 12:46,,,,pre-apache,,,,,,,0,github-import,,"I added support for local distributed execution environments similar to the LocalDistributedExecutor. For this purpose I generalized the LocalInstanceManager to be able to host multiple TaskManager threads. Local distributed execution should be useful for easy local testing of scheduler strategies. Moreover, this feature might be interesting for local execution on a multi-core system.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/686
Created by: [tillrohrmann|https://github.com/tillrohrmann]
Labels: 
Created at: Mon Apr 14 12:05:39 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:45;github-import;pull-request-686-2329953294716287156.patch;https://issues.apache.org/jira/secure/attachment/12649257/pull-request-686-2329953294716287156.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398063,,,Mon Jun 09 12:46:08 UTC 2014,,,,,,,,,,"0|i1wj5z:",398190,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:45;github-import;[Date: Mon Apr 14 13:52:44 CEST 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

I made the hardware description of the task manager a static variable to fix a bug in the memory calculation if multiple task manager run on the same jvm. Before the hardware description was calculated for each task manager individually. If multiple task manager run on the same jvm then the free memory is simply divided by their number. However, once created the memory manager, the free memory was directly reduced so that a subsequently instantiated task manager saw only the reduced free memory. Dividing this amount by the number of task managers running on the jvm leads now to a wrong result.;;;","09/Jun/14 12:45;github-import;[Date: Mon Apr 14 14:18:00 CEST 2014, Author: [uce|https://github.com/uce]]

I noticed some minor stuff (see comments). Very nice to have this. :-);;;","09/Jun/14 12:45;github-import;[Date: Mon Apr 14 14:42:16 CEST 2014, Author: [uce|https://github.com/uce]]

I just noticed a wrong error message (while doing s.th. else) in `createRemoteExecutor` of `PlanExecutor`:

```java
catch (Throwable t) {
			throw new RuntimeException(""An error occurred while loading the local executor ("" + LOCAL_EXECUTOR_CLASS + "")."", t);
}
```

Can you also fix this before we merge this?;;;","09/Jun/14 12:46;github-import;[Date: Mon Apr 14 14:44:05 CEST 2014, Author: [uce|https://github.com/uce]]

Oh wait, I think it's not just the error message... it's a real problem. I'm calling
```java
final ExecutionEnvironment env = ExecutionEnvironment.createRemoteEnvironment(""localhost"", 6123);
```
and get the following message:
```
Exception in thread ""main"" java.lang.RuntimeException: An error occurred while loading the local executor (eu.stratosphere.client.LocalExecutor).
	at eu.stratosphere.api.common.PlanExecutor.createRemoteExecutor(PlanExecutor.java:75)
	at eu.stratosphere.api.java.RemoteEnvironment.execute(RemoteEnvironment.java:51)
	at eu.stratosphere.example.java.wordcount.WordCount.main(WordCount.java:61)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
Caused by: java.lang.NoSuchMethodException: eu.stratosphere.client.LocalExecutor.<init>(java.lang.String, int, java.util.List)
	at java.lang.Class.getConstructor0(Class.java:2810)
	at java.lang.Class.getConstructor(Class.java:1718)
	at eu.stratosphere.api.common.PlanExecutor.createRemoteExecutor(PlanExecutor.java:72)
	... 7 more
```;;;","09/Jun/14 12:46;github-import;[Date: Mon Apr 14 14:44:43 CEST 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

I'll look into it.;;;","09/Jun/14 12:46;github-import;[Date: Mon Apr 14 16:19:23 CEST 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

The error didn't occur on my machine. I could instantiate a remote environment.

I removed the local distributed executor environment from the user api. For testing purposes one can still use. See the WordCountITCase.;;;","09/Jun/14 12:46;github-import;[Date: Fri May 02 09:31:34 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I guess @uce's error is caused by some outdated maven dependencies? I think a `mvn clean install` will resolve the issue.

@tillrohrmann: Why can't we just remove the LocalDistributedExecutor class? I guess your implementation is better integrated into the system than the LDE impl.;;;","09/Jun/14 12:46;github-import;[Date: Fri May 02 11:32:27 CEST 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

If I'm not mistaken, then the LocalDistributedExecutor is only used in the ```PackagedProgramEndToEndITCase```. Thus we only need to replace there the LDE with the NepheleMiniCluster and it should be fine. Apart from that, everything should be working. I'll do the reworking.;;;","09/Jun/14 12:46;github-import;[Date: Fri May 02 12:45:32 CEST 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

I removed the LDE.;;;","09/Jun/14 12:46;github-import;[Date: Wed May 07 18:14:26 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I would like to merge this.

One question, though: Why configure the number of task managers through the config and not simply through a setter, like the other parameters (memory, ports, overwrite)?;;;","09/Jun/14 12:46;github-import;[Date: Thu May 08 08:43:00 CEST 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

Initially, I wanted to integrate this feature as transparent as possible for the different components, because it should only be used for testing purposes. But you are right that it wouldn't actually be exposed to the user API if the NepheleMiniCluster gets a setter for the number of task managers. And additionally it is more elegant.;;;","09/Jun/14 12:46;github-import;[Date: Thu May 08 08:59:46 CEST 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

Ok, I refactored it.;;;","09/Jun/14 12:46;github-import;[Date: Thu May 08 13:08:44 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Sorry, to ping you again. Can you rebase this to the master? There have been a few changes to the Task Manager in the last week.

One thing that has an implication on this code is that the task managers do not get the ""numTaskmanagers"" parameter any more. The LocalDistributedExecuter in the curret HEAD configures them with memory that is scaled down by the number of TaskManagers to start.

I think you need something similar here...;;;","09/Jun/14 12:46;github-import;[Date: Thu May 08 15:15:20 CEST 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

I rebased on the current master. Now, the memory size which one can define for the NepheleMiniCluster is considered to be the total available memory and thus is split among the TaskManagers. If the memory size is not set, then he takes the available free memory minus the memory space for the buffers.;;;","09/Jun/14 12:46;github-import;[Date: Thu May 08 17:51:47 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Wow, great! That should work! Sorry for making you go though all the
changes...;;;","09/Jun/14 12:46;github-import;[Date: Thu May 08 19:09:42 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [93c4b9ae8d633efd49d02dc1702185f670f9c6e7|https://github.com/stratosphere/stratosphere/commit/93c4b9ae8d633efd49d02dc1702185f670f9c6e7];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mixed key types for cogroup,FLINK-684,12719862,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:45,09/Jun/14 12:45,14/Jul/23 05:57,09/Jun/14 12:45,,,,pre-apache,,,,,,,0,github-import,,"See ([#675|https://github.com/stratosphere/stratosphere/issues/675] | [FLINK-675|https://issues.apache.org/jira/browse/FLINK-675]).
I plan to do the same for join when the testcases for join are finished.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/684
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon Apr 14 10:54:34 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:45;github-import;pull-request-684-6686054445325250404.patch;https://issues.apache.org/jira/secure/attachment/12649256/pull-request-684-6686054445325250404.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398061,,,Mon Jun 09 12:45:49 UTC 2014,,,,,,,,,,"0|i1wj5j:",398188,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:45;github-import;[Date: Wed Apr 16 21:37:39 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Thanks for this PR! It was a bit unlucky, that the tests did not catch the bug.
I fixed the bugs in a commit and merged this PR.;;;","09/Jun/14 12:45;github-import;[Date: Wed Apr 16 22:11:18 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

merged in [b6c3a79f968110fbb9bbe0fe555e3e59688984ef|https://github.com/stratosphere/stratosphere/commit/b6c3a79f968110fbb9bbe0fe555e3e59688984ef];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Type extractor should detect type mismatches,FLINK-683,12719861,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:45,09/Jun/14 12:45,14/Jul/23 05:57,09/Jun/14 12:45,,,,pre-apache,,,,,,,0,github-import,,"Currently, the type extractor does not check whether the input type(s) of functions and the TypeInformation of the input data sets match. It would be good to check that, in order to catch cases where programmers omit the generics on the data set and can compile programs where the input type does not fit.

The code below should not result in a successful test.

```
public void testFunctionDependingOnInputWithTupleInputWithTypeMismatch() {
	IdentityMapper2<Boolean> function = new IdentityMapper2<Boolean>();

	TypeInformation<Tuple2<Boolean, String>> inputType = new TupleTypeInfo<Tuple2<Boolean, String>>(BasicTypeInfo.BOOLEAN_TYPE_INFO,
			BasicTypeInfo.INT_TYPE_INFO);

	TypeInformation<?> ti = TypeExtractor.getMapReturnTypes(function, inputType);

	Assert.assertTrue(ti.isBasicType());
	Assert.assertEquals(BasicTypeInfo.BOOLEAN_TYPE_INFO, ti);
}

public class IdentityMapper2<T> extends MapFunction<Tuple2<T, String>, T> {
	private static final long serialVersionUID = 1L;

	@Override
	public T map(Tuple2<T, String> value) throws Exception {
		return null;
	}
}
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/683
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: java api, 
Milestone: Release 0.5
Assignee: [twalthr|https://github.com/twalthr]
Created at: Mon Apr 14 01:00:29 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398060,,,Mon Jun 09 12:45:44 UTC 2014,,,,,,,,,,"0|i1wj5b:",398187,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:45;github-import;[Date: Mon Apr 14 10:25:50 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

I'm working on this.;;;","09/Jun/14 12:45;github-import;[Date: Thu May 08 20:25:11 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

This issue can be closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TypeExtractor.getForClass(...) should work for subclasses of tuples,FLINK-682,12719860,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:45,09/Jun/14 12:45,14/Jul/23 05:57,09/Jun/14 12:45,,,,pre-apache,,,,,,,0,github-import,,"The method currently rejects subclasses of tuple. If the subclass defines all generic parameters, it should be accepted by the type extractor.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/682
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: java api, 
Milestone: Release 0.5
Assignee: [twalthr|https://github.com/twalthr]
Created at: Mon Apr 14 00:26:00 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398059,,,Mon Jun 09 12:45:39 UTC 2014,,,,,,,,,,"0|i1wj53:",398186,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:45;github-import;[Date: Mon Apr 14 00:37:22 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

I can do that tomorrow. But than I have to change the API, since a tuple is a `ParameterizedType` not a `Class`.;;;","09/Jun/14 12:45;github-import;[Date: Mon Apr 14 00:56:28 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Which part of the API is affected by that? The method getForClass(...) would still accept a class, right? Are the methods that need to change user facing, or internal?;;;","09/Jun/14 12:45;github-import;[Date: Mon Apr 14 10:23:48 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

Your right, we could change the argument to ```Type```. But actually ```getForClass()``` is intended for Classes only. One should use ```TypeExtractor.createTypeInfo(Type t)``` as this supports all types not only Classes.;;;","09/Jun/14 12:45;github-import;[Date: Wed Apr 30 11:57:03 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Group Order for incremental Reduce,FLINK-681,12719859,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:45,09/Jun/14 12:45,14/Jul/23 05:57,09/Jun/14 12:45,,,,pre-apache,,,,,,,0,github-import,,"I think the group order for the ReduceFunction (not the GroupReduceFunction) makes no sense. 

The ReduceITCase also has a test for that which shows how this can really only be exploited with variables outside the function.

I think we should not support/encourage that and define a group order only for the group reduce.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/681
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: java api, 
Milestone: Release 0.5
Assignee: [fhueske|https://github.com/fhueske]
Created at: Sun Apr 13 23:55:20 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398058,,,Mon Jun 09 12:45:34 UTC 2014,,,,,,,,,,"0|i1wj4v:",398185,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:45;github-import;[Date: Sun Apr 13 23:55:46 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

@fhueske Can you comment on this?;;;","09/Jun/14 12:45;github-import;[Date: Mon Apr 14 12:19:42 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I am not so sure about this. 
Providing a sorted input to a ReduceFunction can help to have deterministic values for fields that depend on the order of the reduced elements. 
Of course you can explicitly look for the minimum or maximum value of such a field in the Reduce function. Not sure if there might exist other usecases.;;;","09/Jun/14 12:45;github-import;[Date: Mon Apr 14 20:38:14 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I thought about this.
You are right. We should remove group sorting for reduce.
Will take it out and adapt documentation and tests.;;;","09/Jun/14 12:45;github-import;[Date: Tue Apr 15 00:46:09 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

done with [e2818097c8f704f79fa85c5e07b34da86f081857|https://github.com/stratosphere/stratosphere/commit/e2818097c8f704f79fa85c5e07b34da86f081857];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Map and FlatMap operator tests for the new Java API,FLINK-679,12719857,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:45,09/Jun/14 12:45,14/Jul/23 05:57,09/Jun/14 12:45,,,,pre-apache,,,,,,,0,github-import,,"See issue ([#548|https://github.com/stratosphere/stratosphere/issues/548] | [FLINK-548|https://issues.apache.org/jira/browse/FLINK-548])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/679
Created by: [vasia|https://github.com/vasia]
Labels: java api, testing, 
Created at: Fri Apr 11 16:42:16 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:45;github-import;pull-request-679-8688356641995172590.patch;https://issues.apache.org/jira/secure/attachment/12649254/pull-request-679-8688356641995172590.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398056,,,Mon Jun 09 12:45:20 UTC 2014,,,,,,,,,,"0|i1wj4f:",398183,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:45;github-import;[Date: Mon Apr 14 01:43:31 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Very nice, thanks!

Merged in [e11a040b6c4502c7dc50529143554e9be6f7daad|https://github.com/stratosphere/stratosphere/commit/e11a040b6c4502c7dc50529143554e9be6f7daad];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Testcases for the CoGroup operator in the new Java API,FLINK-676,12719854,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:44,09/Jun/14 12:44,14/Jul/23 05:57,09/Jun/14 12:44,,,,pre-apache,,,,,,,0,github-import,,"See ([#548|https://github.com/stratosphere/stratosphere/issues/548] | [FLINK-548|https://issues.apache.org/jira/browse/FLINK-548])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/676
Created by: [markus-h|https://github.com/markus-h]
Labels: java api, testing, 
Created at: Thu Apr 10 13:57:36 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:44;github-import;pull-request-676-1108271452577525262.patch;https://issues.apache.org/jira/secure/attachment/12649251/pull-request-676-1108271452577525262.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398053,,,Mon Jun 09 12:44:55 UTC 2014,,,,,,,,,,"0|i1wj3r:",398180,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:44;github-import;[Date: Fri Apr 11 11:52:01 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good to me, I think this can be merged.;;;","09/Jun/14 12:44;github-import;[Date: Mon Apr 14 01:44:02 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Good patch!

Merged in [8d2be3399d2ded71a26896e2e162bf942f3f156f|https://github.com/stratosphere/stratosphere/commit/8d2be3399d2ded71a26896e2e162bf942f3f156f];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New java Api Union,FLINK-674,12719852,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:44,09/Jun/14 12:44,14/Jul/23 05:57,09/Jun/14 12:44,,,,pre-apache,,,,,,,0,github-import,,"Corr. Issue: https://github.com/stratosphere/stratosphere/issues/621

A union operator for the new Java-API callable: dataSet1.union(dataSet2)
There exists a binary union operator from the java-api down to the optimizer level. 
This means that unions do not need to be handled over lists of operators by the commons-api anymore, but a cascade of union-operators can be used for this case. Therefore my next step is to clean the code from the operator lists to make the code ""slimmer"".

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/674
Created by: [filiphaase|https://github.com/filiphaase]
Labels: 
Created at: Thu Apr 10 02:56:07 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:44;github-import;pull-request-674-3070191095977098862.patch;https://issues.apache.org/jira/secure/attachment/12649250/pull-request-674-3070191095977098862.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398051,,,Mon Jun 09 12:44:45 UTC 2014,,,,,,,,,,"0|i1wj3b:",398178,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:44;github-import;[Date: Thu Apr 10 08:37:57 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I will merge this version, because we need the union. I added some inline comments. Can you have a look at them and make a cleanup patch?;;;","09/Jun/14 12:44;github-import;[Date: Thu Apr 10 09:32:44 CEST 2014, Author: [filiphaase|https://github.com/filiphaase]]

Hey, since you havn't merged so far I updated the PR with your comments. Also added the license to my files so hopefully travis won't fail now.;;;","09/Jun/14 12:44;github-import;[Date: Thu Apr 10 09:44:44 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I merged in my repository. Please add commits on top of this commit and
don't rebase or squash.;;;","09/Jun/14 12:44;github-import;[Date: Thu Apr 10 10:27:28 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Please also try and build with maven.

RAT complains about missing headers in certain files;;;","09/Jun/14 12:44;github-import;[Date: Mon Apr 14 01:44:49 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [4bcbd133ac43289c268ba776efdf9b7de88d1dca|https://github.com/stratosphere/stratosphere/commit/4bcbd133ac43289c268ba776efdf9b7de88d1dca];;;","09/Jun/14 12:44;github-import;[Date: Mon Apr 14 10:12:45 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

@filiphaase Can prepare a patch to address my comments? Thanks!;;;","09/Jun/14 12:44;github-import;[Date: Mon Apr 14 12:23:57 CEST 2014, Author: [filiphaase|https://github.com/filiphaase]]

@fhueske, it was a little bit git-messy in my repo. Therefore I added a second PR https://github.com/stratosphere/stratosphere/pull/680 which covers your comments;;;","09/Jun/14 12:44;github-import;[Date: Mon Apr 14 12:37:18 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

OK, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Failure - Compilation error in stratosphere-java,FLINK-673,12719851,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:44,09/Jun/14 12:44,14/Jul/23 05:57,09/Jun/14 12:44,,,,pre-apache,,,,,,,0,github-import,,"I followed instructions according to the Build from Source instruction in README.md. Encountered a compilation error at stratosphere-java.
Maven output at this link https://gist.github.com/aaronchlam/2633b99ab65a262deec7
```
Aarons-MacBook-Pro:Stratosphere aaronlam$ java -version
java version ""1.6.0_65""
Java(TM) SE Runtime Environment (build 1.6.0_65-b14-462-11M4609)
Java HotSpot(TM) 64-Bit Server VM (build 20.65-b04-462, mixed mode)
Aarons-MacBook-Pro:Stratosphere aaronlam$ mvn -version
Apache Maven 3.2.1 ([ea8b2b07643dbb1b84b6d16e1f08391b666bc1e9|https://github.com/stratosphere/stratosphere/commit/ea8b2b07643dbb1b84b6d16e1f08391b666bc1e9]; 2014-02-14T12:37:52-05:00)
Maven home: /usr/local/Cellar/maven/3.2.1/libexec
Java version: 1.6.0_65, vendor: Apple Inc.
Java home: /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home
Default locale: en_US, platform encoding: MacRoman
OS name: ""mac os x"", version: ""10.9.2"", arch: ""x86_64"", family: ""mac""
```

I've checked the pom.xml and the versions match:
```
<plugin>
	<!-- just define the Java version to be used for compiling and plugins -->
	<groupId>org.apache.maven.plugins</groupId>
	<artifactId>maven-compiler-plugin</artifactId>
	<version>3.1</version>
	<configuration>
		<source>1.6</source>
		<target>1.6</target>
		<!-- High optimization, no debugging <compilerArgument>-g:none -O</compilerArgument> -->
	</configuration>
</plugin>
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/673
Created by: [aaronchlam|https://github.com/aaronchlam]
Labels: duplicate, 
Created at: Thu Apr 10 02:32:38 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398050,,,Mon Jun 09 12:44:35 UTC 2014,,,,,,,,,,"0|i1wj33:",398177,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:44;github-import;[Date: Thu Apr 10 08:02:50 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Does this occur reproducibility every time? Looks like a bug in the Java
compiler.

You can probably fix this by updating your Java version to 1.7
Unfortunately, Oracle does not provide patches for 1.6 any more.
Am 10.04.2014 02:32 schrieb ""Aaron Lam"" <notifications@github.com>:

> I followed instructions according to the Build from Source instruction in
> README.md. Encountered a compilation error at stratosphere-java.
> Maven output at this link
> https://gist.github.com/aaronchlam/2633b99ab65a262deec7
>
> Aarons-MacBook-Pro:Stratosphere aaronlam$ java -version
> java version ""1.6.0_65""
> Java(TM) SE Runtime Environment (build 1.6.0_65-b14-462-11M4609)
> Java HotSpot(TM) 64-Bit Server VM (build 20.65-b04-462, mixed mode)
> Aarons-MacBook-Pro:Stratosphere aaronlam$ mvn -version
> Apache Maven 3.2.1 ([ea8b2b07643dbb1b84b6d16e1f08391b666bc1e9|https://github.com/stratosphere/stratosphere/commit/ea8b2b07643dbb1b84b6d16e1f08391b666bc1e9]; 2014-02-14T12:37:52-05:00)
> Maven home: /usr/local/Cellar/maven/3.2.1/libexec
> Java version: 1.6.0_65, vendor: Apple Inc.
> Java home: /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home
> Default locale: en_US, platform encoding: MacRoman
> OS name: ""mac os x"", version: ""10.9.2"", arch: ""x86_64"", family: ""mac""
>
> I've checked the pom.xml and the versions match:
>
> <plugin>
>     <!-- just define the Java version to be used for compiling and plugins -->
>     <groupId>org.apache.maven.plugins</groupId>
>     <artifactId>maven-compiler-plugin</artifactId>
>     <version>3.1</version>
>     <configuration>
>         <source>1.6</source>
>         <target>1.6</target>
>         <!-- High optimization, no debugging <compilerArgument>-g:none -O</compilerArgument> -->
>     </configuration>
> </plugin>
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/673>
> .
>;;;","09/Jun/14 12:44;github-import;[Date: Thu Apr 10 10:10:40 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Hi Aaron, 

thanks for reporting this issue!
It was identified before (([#575|https://github.com/stratosphere/stratosphere/issues/575] | [FLINK-575|https://issues.apache.org/jira/browse/FLINK-575])) but unfortunately we did not update the pom files. :worried: 

Since you asked on the ML for a task to start with Stratosphere, you might want to fix the bug you caught as a first contribution. I will in the mean time update the list of starter tasks and help you to pick on.

Does that sound good to you?;;;","09/Jun/14 12:44;github-import;[Date: Thu Apr 10 14:47:56 CEST 2014, Author: [aaronchlam|https://github.com/aaronchlam]]

@StephanEwen I reran the build and it gave the same compilation error. See gist  here https://gist.github.com/aaronchlam/379d76443e58a426a8ab#file-stratosphere_compile_failure_rerun
I cloned and ran in a separate directory and ran into the same problem. See gist here https://gist.github.com/aaronchlam/379d76443e58a426a8ab#file-stratosphere_compile_failure_clone_and_build

I'm upgrading my Java now. I will post an update.;;;","09/Jun/14 12:44;github-import;[Date: Thu Apr 10 15:35:46 CEST 2014, Author: [aaronchlam|https://github.com/aaronchlam]]

I've updates to java 7 and updated the pom.xml
```
			<plugin>
				<!-- just define the Java version to be used for compiling and plugins -->
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-compiler-plugin</artifactId>
				<version>3.1</version>
				<configuration>
					<source>1.7</source>
					<target>1.7</target>
					<!-- High optimization, no debugging <compilerArgument>-g:none -O</compilerArgument> -->
				</configuration>
			</plugin>
			<plugin>
```
Since this is a duplicate issue, do I put the pull request on this issue or on the issue ([#575|https://github.com/stratosphere/stratosphere/issues/575] | [FLINK-575|https://issues.apache.org/jira/browse/FLINK-575]) ?

Also, the README.md's Build from Source Requirements currently states ""Java 6 or 7"". So I guess this needs to be updated too. Should I open a new issue, modify the REAMDE.md to say just Java 7 as my 2nd contribution?;;;","09/Jun/14 12:44;github-import;[Date: Thu Apr 10 15:38:22 CEST 2014, Author: [sscdotopen|https://github.com/sscdotopen]]

Is it really necessary to already update to java7 ? If not, I'd vote for
staying at 6, because at lot people haven't updated their clusters yet to
my knowledge.
Am 10.04.2014 15:35 schrieb ""Aaron Lam"" <notifications@github.com>:

> I've updates to java 7 and updated the pom.xml
>
>             <plugin>
>                 <!-- just define the Java version to be used for compiling and plugins -->
>                 <groupId>org.apache.maven.plugins</groupId>
>                 <artifactId>maven-compiler-plugin</artifactId>
>                 <version>3.1</version>
>                 <configuration>
>                     <source>1.7</source>
>                     <target>1.7</target>
>                     <!-- High optimization, no debugging <compilerArgument>-g:none -O</compilerArgument> -->
>                 </configuration>
>             </plugin>
>             <plugin>
>
> Since this is a duplicate issue, do I put the pull request on this issue
> or on the issue ([#575|https://github.com/stratosphere/stratosphere/issues/575] | [FLINK-575|https://issues.apache.org/jira/browse/FLINK-575])<https://github.com/stratosphere/stratosphere/issues/575>?
>
> Also, the README.md's Build from Source Requirements currently states
> ""Java 6 or 7"". So I guess this needs to be updated too. Should I open a new
> issue, modify the REAMDE.md to say just Java 7 as my 2nd contribution?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/673#issuecomment-40081924>
> .
>;;;","09/Jun/14 12:44;github-import;[Date: Thu Apr 10 15:45:45 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I agree, I would like to keep the pom file on Java 6. You can still use the
Java 7 compiler that fixed the bug. It will produce Java 6 compatible code
with the old pom.;;;","09/Jun/14 12:44;github-import;[Date: Thu Apr 10 17:31:25 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

The problem only applies to oracle java 6, not openjdk6.
Also, many users are still at java 6. I guess there is a maven plugin to
check these versions.


Sent from my iPhone

On 10.04.2014, at 16:45, Stephan Ewen <notifications@github.com> wrote:

I agree, I would like to keep the pom file on Java 6. You can still use the
Java 7 compiler that fixed the bug. It will produce Java 6 compatible code
with the old pom.

--
Reply to this email directly or view it on
GitHub<https://github.com/stratosphere/stratosphere/issues/673#issuecomment-40082992>
.;;;","09/Jun/14 12:44;github-import;[Date: Thu Apr 24 23:47:39 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think that we can close this issue.

This is clearly a java compiler bug in a no longer maintained version. We support Java 6 runtimes, but we support not all versions of Java 6 compilers. That affects only developers that compile the system themselves: They need to upgrade their local JDK;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the BroadcastVariableJoin example,FLINK-667,12719845,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:43,09/Jun/14 12:43,14/Jul/23 05:57,09/Jun/14 12:43,,,,pre-apache,,,,,,,0,github-import,,"I think that one is not necessary

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/667
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: documentation, simple-issue, 
Created at: Tue Apr 08 13:01:49 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398044,,,Mon Jun 09 12:43:41 UTC 2014,,,,,,,,,,"0|i1wj1r:",398171,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:43;github-import;[Date: Mon Apr 28 23:02:38 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Done in [dd6694e5660ff6317661fb4250c8f871cf02aa2c|https://github.com/stratosphere/stratosphere/commit/dd6694e5660ff6317661fb4250c8f871cf02aa2c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove some of the pethora of WordCount examples,FLINK-666,12719844,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:43,09/Jun/14 12:43,14/Jul/23 05:57,09/Jun/14 12:43,,,,pre-apache,,,,,,,0,github-import,,"I would suggest to keep the following examples:

 - WordCount
 - WordCount with collection and accumulators

That should suffice, no?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/666
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: documentation, java api, simple-issue, 
Created at: Tue Apr 08 12:59:53 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398043,,,Mon Jun 09 12:43:37 UTC 2014,,,,,,,,,,"0|i1wj1j:",398170,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:43;github-import;[Date: Tue Apr 08 13:05:02 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

:+1: 
The `RelQuery` example should also be removed because it is broken.;;;","09/Jun/14 12:43;github-import;[Date: Wed May 14 13:46:11 CEST 2014, Author: [zentol|https://github.com/zentol]]

This issue can be closed. 
78a4283e64
45d9b36e5f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove record examples,FLINK-665,12719843,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:43,09/Jun/14 12:43,14/Jul/23 05:57,09/Jun/14 12:43,,,,pre-apache,,,,,,,0,github-import,,"Now that we are having more and more examples for the new Java API, I would like to remove the examples for the record api. Too many (quasi duplicate) examples can be really confusing.

I would suggest to move the record examples to the stratosphere-artifacts repository.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/665
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Tue Apr 08 12:58:05 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398042,,,Mon Jun 09 12:43:33 UTC 2014,,,,,,,,,,"0|i1wj1b:",398169,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:43;github-import;[Date: Tue Apr 08 13:01:18 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

In general I am in favor of doing this.
However, we should make sure that we do not cut on test coverage here since many integration tests are based on these examples.;;;","09/Jun/14 12:43;github-import;[Date: Thu Apr 24 23:49:00 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

We could also move them to stratosphere-tests, to keep using them as end-to-end integration tests. That way we loose no test coverage and keep a slim and clean examples project.;;;","09/Jun/14 12:43;github-import;[Date: Fri Apr 25 08:23:38 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

:+1: 
In the long run, we should try to remove some overlap in the integration tests. A lot of them are very similar, do not add any benefit, but increase build time by several minutes.
But for now, moving is fine with me.;;;","09/Jun/14 12:43;github-import;[Date: Thu May 15 11:40:17 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Record examples were moved in [c41569eb01ac9ab83e17554a50ccdc7e4ac49327|https://github.com/stratosphere/stratosphere/commit/c41569eb01ac9ab83e17554a50ccdc7e4ac49327];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make delta iterations use Keys.FieldPositionKeys,FLINK-664,12719842,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:43,09/Jun/14 12:43,14/Jul/23 05:57,09/Jun/14 12:43,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/664
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Tue Apr 08 12:37:22 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:43;github-import;pull-request-664-1509031116700463435.patch;https://issues.apache.org/jira/secure/attachment/12649247/pull-request-664-1509031116700463435.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398041,,,Mon Jun 09 12:43:28 UTC 2014,,,,,,,,,,"0|i1wj13:",398168,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:43;github-import;[Date: Wed Apr 09 17:35:49 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [7ad677b2d8392b8e83da2f898586a33d47939097|https://github.com/stratosphere/stratosphere/commit/7ad677b2d8392b8e83da2f898586a33d47939097];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DOP of sink after all reduce higher than expected,FLINK-663,12719841,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:43,09/Jun/14 12:43,14/Jul/23 05:57,09/Jun/14 12:43,,,,pre-apache,,,,,,,0,github-import,,"Consider the following situation: the program DOP is 8; the last transformation is an all reduce (i.e. reduce without a grouping); after this, the result is written out.

How many files should this result in? I was expecting a single file, but found that it will create 8 files.

ATM I haven't checked if the sink still has a higher DOP or if there is a problem with the reduce op.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/663
Created by: [uce|https://github.com/uce]
Labels: bug, java api, 
Created at: Mon Apr 07 16:47:06 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398040,,,Mon Jun 09 12:43:24 UTC 2014,,,,,,,,,,"0|i1wj0v:",398167,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:43;github-import;[Date: Mon Apr 07 17:51:54 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

At the moment, all nodes will use the default degree-of-parallelism, unless they explicitly state a different one. The AllReduce states a DOP of 1 (by definition). The sinks then have the default DOP again.

I agree that this may be not what you want in that case. Do we need different rules in how to determine the DOP of an operator, or is this simply a special case for sinks?

;;;","09/Jun/14 12:43;github-import;[Date: Tue Apr 08 09:33:25 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I'd say this should be a specific rule for DataSinks. A DataSink should adopt the DOP of its preceding task if its DOP was not explicitly defined.
If an AllReduce is followed by a regular operator it might make sense that this operator is run in parallel and we should not change this default behavior.

What do you think?;;;","09/Jun/14 12:43;github-import;[Date: Tue May 06 20:27:23 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

This suggestion was implemented in [44ccde6503469391bca67cc0c71aedca13435f3a|https://github.com/stratosphere/stratosphere/commit/44ccde6503469391bca67cc0c71aedca13435f3a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
added TPCH3 example,FLINK-661,12719839,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:43,09/Jun/14 12:43,14/Jul/23 05:57,09/Jun/14 12:43,,,,pre-apache,,,,,,,0,github-import,,"I implemented the tpch 3 query with custom objects as an example.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/661
Created by: [skunert|https://github.com/skunert]
Labels: documentation, java api, 
Milestone: Release 0.5
Created at: Thu Apr 03 16:45:58 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:43;github-import;pull-request-661-6708350896503102046.patch;https://issues.apache.org/jira/secure/attachment/12649246/pull-request-661-6708350896503102046.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398038,,,Mon Jun 09 12:43:15 UTC 2014,,,,,,,,,,"0|i1wj0f:",398165,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:43;github-import;[Date: Mon Apr 14 20:19:48 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think we need a little cleanup here, because the examples should look nice. They are representatives of what it feels like working with the system.

I think this can also benefit bigtime from an extension to the CSVReader, that supports reading into subclasses of tuples directly. You can extract the types of the fields from the class. @twalthr has added support for that in the type extractor.

Could you add that? I think it would be a great addition.;;;","09/Jun/14 12:43;github-import;[Date: Tue Apr 15 12:15:19 CEST 2014, Author: [skunert|https://github.com/skunert]]

I did the cleanup work you recommended. I will look at the CSVReader.;;;","09/Jun/14 12:43;github-import;[Date: Tue Apr 15 13:46:05 CEST 2014, Author: [skunert|https://github.com/skunert]]

The CsvReader Extension is handled in PR ([#696|https://github.com/stratosphere/stratosphere/issues/696] | [FLINK-696|https://issues.apache.org/jira/browse/FLINK-696]).;;;","09/Jun/14 12:43;github-import;[Date: Mon May 05 17:19:55 CEST 2014, Author: [skunert|https://github.com/skunert]]

This example is done.;;;","09/Jun/14 12:43;github-import;[Date: Tue May 06 22:52:39 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [7aec7bcd8d37b21fe1ebe308a12d06c4c5a7f2cc|https://github.com/stratosphere/stratosphere/commit/7aec7bcd8d37b21fe1ebe308a12d06c4c5a7f2cc];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add JavaDocs to Function classes,FLINK-657,12719835,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:42,09/Jun/14 12:42,14/Jul/23 05:57,09/Jun/14 12:42,,,,pre-apache,,,,,,,0,github-import,,"The function classes such as `MapFunction`, `FlatMapFunction`, `JoinFunction`, etc. do not have any JavaDocs.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/657
Created by: [fhueske|https://github.com/fhueske]
Labels: documentation, java api, user satisfaction, 
Milestone: Release 0.5
Created at: Wed Apr 02 21:14:26 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398034,,,Mon Jun 09 12:42:54 UTC 2014,,,,,,,,,,"0|i1wizj:",398161,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:42;github-import;[Date: Wed Apr 02 21:15:22 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

related https://github.com/stratosphere/stratosphere/issues/642;;;","09/Jun/14 12:42;github-import;[Date: Tue May 20 19:55:06 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Done in [af2a765a5723532c6a87349de4c74ec613a7ee24|https://github.com/stratosphere/stratosphere/commit/af2a765a5723532c6a87349de4c74ec613a7ee24];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Runtime Exception when shuffling large Records,FLINK-652,12719830,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:42,18/Jun/14 15:54,14/Jul/23 05:57,18/Jun/14 15:54,,,,pre-apache,pre-apache-0.5.1,,,,,,0,github-import,,"Hi,

I have a job that reads (sometimes) large web documents from HBase and performs entity extraction in those docs.
Between the HBase source and the NER Mapper I randomly shuffle the records to different mapper to compensate for skew in the data.
```Java
mapper.setParameter(""INPUT_SHIP_STRATEGY"", ""SHIP_REPARTITION"");
```
Look here for details: https://groups.google.com/forum/#!topic/stratosphere-users/pIw0eyMkLd8

I'm fairly sure the issue is caused by the records being large and by shuffling them over the network in the fist place because:
1. the problem didn't occur when the source and NER mapper were chained
2. the problem didn't occur when I enabled shuffling but accidentally omitted the document content which made for very small records
3. the exception says so, duh!
```
eu.stratosphere.client.program.ProgramInvocationException: The program execution failed: java.io.IOException: An error occurred in the channel: Expected data packet 82 but received 84
        at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedInputChannel.isClosed(AbstractByteBufferedInputChannel.java:149)
        at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedInputChannel.readRecord(AbstractByteBufferedInputChannel.java:92)
        at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:192)
        at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:78)
        at eu.stratosphere.pact.runtime.task.util.RecordReaderIterator.next(RecordReaderIterator.java:41)
        at eu.stratosphere.pact.runtime.task.util.RecordReaderIterator.next(RecordReaderIterator.java:25)
        at eu.stratosphere.pact.runtime.task.MapDriver.run(MapDriver.java:79)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:403)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:294)
        at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:342)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: Expected data packet 82 but received 84
        at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputChannelContext.queueTransferEnvelope(RuntimeInputChannelContext.java:147)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeWithBuffer(ByteBufferedChannelManager.java:363)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelope(ByteBufferedChannelManager.java:329)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeFromNetwork(ByteBufferedChannelManager.java:636)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnection.read(IncomingConnection.java:98)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.doRead(IncomingConnectionThread.java:185)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.run(IncomingConnectionThread.java:124)

        at eu.stratosphere.client.program.Client.run(Client.java:338)
        at eu.stratosphere.client.program.Client.run(Client.java:282)
        at eu.stratosphere.client.program.Client.run(Client.java:250)
        at eu.stratosphere.client.RemoteExecutor.executePlan(RemoteExecutor.java:84)
        at org.okkam.dopa.ner.ImrNerTaskDriver.runNerTask(ImrNerTaskDriver.java:65)
        at org.okkam.dopa.ner.ImrNerTaskDriver.main(ImrNerTaskDriver.java:150)
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/652
Created by: [mleich|https://github.com/mleich]
Labels: bug, runtime, 
Created at: Wed Apr 02 15:54:51 CEST 2014
State: open
",,github-import,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398029,,,Wed Jun 18 15:54:37 UTC 2014,,,,,,,,,,"0|i1wiyf:",398156,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:42;github-import;[Date: Wed Apr 02 16:16:04 CEST 2014, Author: [uce|https://github.com/uce]]

Indeed, it seems to be a problem with the network stack.

We reworked large parts of the network stack, but didn't merge it yet. While doing this, I experienced the same problem of missing/skipped envelopes when sending out empty buffers and fixed the problem in the reworked version.

Can you checkout out the my [netstack_rework branch|https://github.com/uce/stratosphere/tree/netstack_rework] and report if it is working with it? It is not based on the newest snapshot version, but I hope that it will be OK for your program.

Please let me know, if your program doesn't work with the Stratosphere version of the branch or the problem still exists there.;;;","09/Jun/14 12:42;github-import;[Date: Wed Apr 02 17:46:40 CEST 2014, Author: [mleich|https://github.com/mleich]]

Thanks @uce, good to know it's on the radar.
In the meantime I pushed down some simple filters we had in our program anyways. Turns the large documents that caused to issue should have been pruned anyways.

Time on this specific cluster with this dataset is limited, I'll try to come up with some minimal code snippet so we can reproduce the issue in any other environment.;;;","09/Jun/14 12:42;github-import;[Date: Wed Apr 02 20:21:28 CEST 2014, Author: [mleich|https://github.com/mleich]]

It turns out that pruning certain large records doesn't fix the problem.
So I tested the branch form @uce, unfortunately, the issue still persists.

```
j20:13:48,616 INFO  eu.stratosphere.nephele.client.JobClient                      - 04/02/2014 20:13:46: CHAIN Map (Run NER) -> Combine(Group Entities) (8/16) switched to FAILED
java.io.IOException: An error occurred in the channel: Expected data packet 24 but received 27
        at eu.stratosphere.runtime.io.channels.InputChannel.isClosed(InputChannel.java:233)
        at eu.stratosphere.runtime.io.channels.InputChannel.readRecord(InputChannel.java:163)
        at eu.stratosphere.runtime.io.gates.InputGate.readRecord(InputGate.java:237)
        at eu.stratosphere.runtime.io.api.MutableRecordReader.next(MutableRecordReader.java:79)
        at eu.stratosphere.pact.runtime.task.util.RecordReaderIterator.next(RecordReaderIterator.java:41)
        at eu.stratosphere.pact.runtime.task.util.RecordReaderIterator.next(RecordReaderIterator.java:25)
        at eu.stratosphere.pact.runtime.task.CollectorMapDriver.run(CollectorMapDriver.java:77)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:503)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:371)
        at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:312)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.IOException: Expected data packet 24 but received 27
        at eu.stratosphere.runtime.io.channels.InputChannel.queueEnvelope(InputChannel.java:333)
        at eu.stratosphere.runtime.io.network.ChannelManager.dispatchFromNetwork(ChannelManager.java:566)
        at eu.stratosphere.runtime.io.network.tcp.IncomingConnection.read(IncomingConnection.java:62)
        at eu.stratosphere.runtime.io.network.tcp.IncomingConnectionThread.doRead(IncomingConnectionThread.java:188)
        at eu.stratosphere.runtime.io.network.tcp.IncomingConnectionThread.run(IncomingConnectionThread.java:121)
```;;;","09/Jun/14 12:42;github-import;[Date: Wed Apr 02 20:26:13 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, that's unfortunate. Is it by any chance deterministic, does it always
occur between packets 24 and 27 when you re-run the job?;;;","09/Jun/14 12:42;github-import;[Date: Wed Apr 02 20:43:45 CEST 2014, Author: [mleich|https://github.com/mleich]]

Not between the same packets, but more or less in the same timeframe (in the first three minutes):
```
20:11:14,462 scheduled -> 20:13:48,616 ... 24 but received 27
20:33:30,578 scheduled -> 20:35:46,786 ... 22 but received 28
20:37:04,263 scheduled -> 20:39:18,382 ... 9 but received 10
```;;;","09/Jun/14 12:42;github-import;[Date: Wed Apr 02 21:22:10 CEST 2014, Author: [uce|https://github.com/uce]]

 Will you have time to extract a minimal code snippet with some sample data soon? It's crucial that we fix this.

Btw: How large are the documents/records? I could also try to reproduce the problem.

Sent from my iPhone

> On 02 Apr 2014, at 20:43, mleich <notifications@github.com> wrote:
> 
> Not between the same packets, but more or less in the same timeframe (in the first three minutes):
> 
> 20:11:14,462 scheduled -> 20:13:48,616 ... 24 but received 27
> 20:33:30,578 scheduled -> 20:35:46,786 ... 22 but received 28
> 20:37:04,263 scheduled -> 20:39:18,382 ... 9 but received 10
> —
> Reply to this email directly or view it on GitHub.;;;","09/Jun/14 12:42;github-import;[Date: Wed Apr 02 21:39:39 CEST 2014, Author: [mleich|https://github.com/mleich]]

I did some more testing
I have 8 nodes and used a DOP of 1, 2, 4, 8, 16, 32.
For dop >= 4 the error occurred within the first 3 minutes.
For dop < 4 I stopped execution after 6-10 minutes, the error didn't occur so far. I assume  that a certain HBase region in the input splits generates a sequence of records that is large enough.

Running the the locally with DOP 4 didn't reproduce the issue.I'm trying to build the minimal snippet now.;;;","09/Jun/14 12:42;github-import;[Date: Wed Apr 02 21:58:15 CEST 2014, Author: [uce|https://github.com/uce]]

Ok, thank you!

You could also try to run it locally with the LocalDistributedExecutor (I suggest to look into the LocalDistributedExecutorITCase to see how it's used), which will also use the TCP stack locally instead of just the in-memory channels.

Sent from my iPhone

> On 02 Apr 2014, at 21:39, mleich <notifications@github.com> wrote:
> 
> I did some more testing
> I have 8 nodes and used a DOP of 1, 2, 4, 8, 16, 32.
> For dop >= 4 the error occurred within the first 3 minutes.
> For dop < 4 I stopped execution after 6-10 minutes, the error didn't occur so far. I assume that a certain HBase region in the input splits generates a sequence of records that is large enough.
> 
> Running the the locally with DOP 4 didn't reproduce the issue.I'm trying to build the minimal snippet now.
> 
> —
> Reply to this email directly or view it on GitHub.;;;","09/Jun/14 12:42;github-import;[Date: Wed Apr 02 23:51:30 CEST 2014, Author: [mleich|https://github.com/mleich]]

I managed to make most of my cluster nodes unresponsive, have to wait until the admin fixes them again.
So far my test doesn't reproduce the issue.
Must be some strange mixture of varying records sizes and record inter arrival times.
Going to continue tomorrow.;;;","09/Jun/14 12:42;github-import;[Date: Fri May 16 08:24:19 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Just FYI, I am able to reproduce this error on the 26 node DIMA cluster with the large test job. (However, it takes around 4 hours)

```
Error: java.io.IOException: An error occurred in the channel: Expected data packet 8 but received 9
at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedInputChannel.isClosed(AbstractByteBufferedInputChannel.java:149)
at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedInputChannel.readRecord(AbstractByteBufferedInputChannel.java:92)
at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:192)
at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:78)
at eu.stratosphere.pact.runtime.task.util.RecordReaderIterator.next(RecordReaderIterator.java:41)
at eu.stratosphere.pact.runtime.task.util.RecordReaderIterator.next(RecordReaderIterator.java:25)
at eu.stratosphere.pact.runtime.resettable.SpillingResettableMutableObjectIterator.next(SpillingResettableMutableObjectIterator.java:144)
at eu.stratosphere.pact.runtime.task.CrossDriver.runBlockedOuterFirst(CrossDriver.java:212)
at eu.stratosphere.pact.runtime.task.CrossDriver.run(CrossDriver.java:146)
at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:511)
at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:376)
at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:351)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Expected data packet 8 but received 9
at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputChannelContext.queueTransferEnvelope(RuntimeInputChannelContext.java:147)
at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeWithBuffer(ByteBufferedChannelManager.java:364)
at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelope(ByteBufferedChannelManager.java:330)
at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeFromNetwork(ByteBufferedChannelManager.java:637)
at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnection.read(IncomingConnection.java:98)
at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.doRead(IncomingConnectionThread.java:185)
at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.run(IncomingConnectionThread.java:124)
```;;;","09/Jun/14 12:42;github-import;[Date: Thu May 29 11:03:52 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

In 0.5-rc3, the error occurs almost instantly with the large testjob
```
05/29/2014 10:59:10:	Job execution switched to status FAILED
Error: The program execution failed: java.lang.Exception: The data preparation for task 'Join(T4 Join: All order keys)' , caused an error: An error occurred creating the temp table.
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:483)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:373)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:351)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: An error occurred creating the temp table.
	at eu.stratosphere.pact.runtime.task.TempBarrier.getIterator(TempBarrier.java:92)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.getInput(RegularPactTask.java:1122)
	at eu.stratosphere.pact.runtime.task.MatchDriver.prepare(MatchDriver.java:93)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:478)
	... 3 more
Caused by: java.io.IOException: An error occurred in the channel: Expected data packet 0 but received 1
	at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedInputChannel.isClosed(AbstractByteBufferedInputChannel.java:149)
	at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedInputChannel.readRecord(AbstractByteBufferedInputChannel.java:92)
	at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:192)
	at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:78)
	at eu.stratosphere.pact.runtime.task.util.RecordReaderIterator.next(RecordReaderIterator.java:41)
	at eu.stratosphere.pact.runtime.task.util.RecordReaderIterator.next(RecordReaderIterator.java:25)
	at eu.stratosphere.pact.runtime.task.TempBarrier$TempWritingThread.run(TempBarrier.java:171)
Caused by: java.io.IOException: Expected data packet 0 but received 1
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputChannelContext.queueTransferEnvelope(RuntimeInputChannelContext.java:147)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeEnvelopeWithoutBuffer(ByteBufferedChannelManager.java:447)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelope(ByteBufferedChannelManager.java:328)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeFromNetwork(ByteBufferedChannelManager.java:637)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnection.read(IncomingConnection.java:96)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.doRead(IncomingConnectionThread.java:185)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.run(IncomingConnectionThread.java:124)
```;;;","18/Jun/14 15:54;rmetzger;The issue has been addressed by the new netty-based network stack, which is included starting from 0.5.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor and add comments to broadcast variable example,FLINK-651,12719829,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:42,09/Jun/14 12:42,14/Jul/23 05:57,09/Jun/14 12:42,,,,pre-apache,,,,,,,0,github-import,,"While going over the docs for the 0.5 release, I felt the need to refactor our current broadcast variable example.

Since the examples also act as documentation, I've added some comments.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/651
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Wed Apr 02 13:16:22 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:42;github-import;pull-request-651-1343799736373756828.patch;https://issues.apache.org/jira/secure/attachment/12649244/pull-request-651-1343799736373756828.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398028,,,Mon Jun 09 12:42:25 UTC 2014,,,,,,,,,,"0|i1wiy7:",398155,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:42;github-import;[Date: Wed Apr 02 15:58:16 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Looks good to me. Can be merged IMHO.;;;","09/Jun/14 12:42;github-import;[Date: Thu Apr 03 12:11:45 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Looks good. I'm going to merge;;;","09/Jun/14 12:42;github-import;[Date: Thu Apr 03 12:16:55 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged [3def816a432e48bd0ea2b9fbd2cc85a855a44cd2|https://github.com/stratosphere/stratosphere/commit/3def816a432e48bd0ea2b9fbd2cc85a855a44cd2].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Java 8 to README.md,FLINK-649,12719827,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:42,09/Jun/14 12:42,14/Jul/23 05:57,09/Jun/14 12:42,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/649
Created by: [fhueske|https://github.com/fhueske]
Labels: documentation, simple-issue, website, 
Created at: Wed Apr 02 09:29:18 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398026,,,Mon Jun 09 12:42:17 UTC 2014,,,,,,,,,,"0|i1wixr:",398153,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:42;github-import;[Date: Wed Apr 16 22:13:03 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Done with ([#699|https://github.com/stratosphere/stratosphere/issues/699] | [FLINK-699|https://issues.apache.org/jira/browse/FLINK-699]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow .yml file extension for config files (fixes #645),FLINK-646,12719822,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:41,09/Jun/14 12:42,14/Jul/23 05:57,09/Jun/14 12:42,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/646
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Tue Apr 01 18:23:48 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:41;github-import;pull-request-646-1200758461879435153.patch;https://issues.apache.org/jira/secure/attachment/12649242/pull-request-646-1200758461879435153.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398021,,,Mon Jun 09 12:41:59 UTC 2014,,,,,,,,,,"0|i1wiwn:",398148,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:41;github-import;[Date: Tue Apr 01 18:47:40 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Good to merge (once the issue is resolved);;;","09/Jun/14 12:41;github-import;[Date: Mon Apr 07 19:55:04 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [1c1be8cb051f3b97e43a4b6d36f0bc35b459d63b|https://github.com/stratosphere/stratosphere/commit/1c1be8cb051f3b97e43a4b6d36f0bc35b459d63b];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add .yml as config file extension,FLINK-645,12719821,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:41,09/Jun/14 12:41,14/Jul/23 05:57,09/Jun/14 12:41,,,,pre-apache,,,,,,,0,github-import,,"Currently, `GlobalConfiguration` searches the config dir for files ending in XML or YAML.

We should also allow YML as file extension for YAML files as it is quite common.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/645
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Tue Apr 01 16:31:58 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398020,,,2014-06-09 12:41:53.0,,,,,,,,,,"0|i1wiwf:",398147,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delta Iterations for new Java API,FLINK-643,12719819,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:41,09/Jun/14 12:41,14/Jul/23 05:57,09/Jun/14 12:41,,,,pre-apache,,,,,,,0,github-import,,"I added Delta Iterations to the new Java API (([#607|https://github.com/stratosphere/stratosphere/issues/607] | [FLINK-607|https://issues.apache.org/jira/browse/FLINK-607])). Moreover I implemented a simple Delta PageRank Example and created a testcase with it. The example also shows how the new syntax for delta iterations looks like.

My branch is based on PullRequest ([#638|https://github.com/stratosphere/stratosphere/issues/638] | [FLINK-638|https://issues.apache.org/jira/browse/FLINK-638]), since it already included some of the compiler post pass logic for delta iterations.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/643
Created by: [markus-h|https://github.com/markus-h]
Labels: java api, 
Created at: Tue Apr 01 13:14:12 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:41;github-import;pull-request-643-8588008789466010529.patch;https://issues.apache.org/jira/secure/attachment/12649241/pull-request-643-8588008789466010529.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398018,,,Mon Jun 09 12:41:46 UTC 2014,,,,,,,,,,"0|i1wivz:",398145,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:41;github-import;[Date: Mon Apr 07 19:11:14 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [74ce8d3b3a202aadb59b843582068c5ed21ca4d9|https://github.com/stratosphere/stratosphere/commit/74ce8d3b3a202aadb59b843582068c5ed21ca4d9];;;","09/Jun/14 12:41;github-import;[Date: Wed Apr 23 14:28:14 CEST 2014, Author: [vasia|https://github.com/vasia]]

Hey,
the deltas initialization can now actually be included into the same program (when I first wrote this delta version, that wasn't possible, so I would first run one separate bulk iteration to get the initial deltas input). 
So, now both the SimpleDeltaPageRank example and the test case can be simplified to receive only the initial ranks and the edges as input and not the deltas explicitly.
Not sure if you think it's important, but I could change it if you want :);;;","09/Jun/14 12:41;github-import;[Date: Thu May 08 18:58:41 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It would make a nice example, but it is not critical. Up to you ;-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added automated javadocs for new Java API,FLINK-642,12719818,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:41,09/Jun/14 12:41,14/Jul/23 05:57,09/Jun/14 12:41,,,,pre-apache,,,,,,,0,github-import,,"With this commit [33525af0b76e260e4237a1bdb1b217e2d7b2460a|https://github.com/stratosphere/stratosphere/commit/33525af0b76e260e4237a1bdb1b217e2d7b2460a] travis will automatically create javadocs for the new Java API here: http://stratosphere-javadocs.github.io/

I'm planning to manually add the javadocs of the 0.5 release to the regular website.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/642
Created by: [rmetzger|https://github.com/rmetzger]
Labels: documentation, java api, website, 
Created at: Tue Apr 01 11:09:24 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398017,,,2014-06-09 12:41:34.0,,,,,,,,,,"0|i1wivr:",398144,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
added annotation support for new java api,FLINK-640,12719816,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:41,09/Jun/14 12:41,14/Jul/23 05:57,09/Jun/14 12:41,,,,pre-apache,,,,,,,0,github-import,,"The operators now read the annotations and wrap them in semanticproperties. This implementation should work fine for operator implementation which use tuples. 

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/640
Created by: [skunert|https://github.com/skunert]
Labels: java api, 
Created at: Mon Mar 31 09:39:21 CEST 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:41;github-import;pull-request-640-4318228422859693369.patch;https://issues.apache.org/jira/secure/attachment/12649240/pull-request-640-4318228422859693369.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398015,,,Mon Jun 09 12:41:29 UTC 2014,,,,,,,,,,"0|i1wivb:",398142,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:41;github-import;[Date: Thu Apr 03 11:48:59 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

What happens if the annotations are incorrectly used for UDFs with non-Tuple types? 
Should we allow annotations only on UDFs with Tuple types, check that at program assembly time, give a warning, and ignore the annotations?;;;","09/Jun/14 12:41;github-import;[Date: Thu Apr 10 10:43:19 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Hi @skunert, what happened to this PR?
Are you reworking it?;;;","09/Jun/14 12:41;github-import;[Date: Thu Apr 10 13:51:16 CEST 2014, Author: [skunert|https://github.com/skunert]]

Yeah, I talked to stephan and am reworking it at this moment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
First Version of Spargel (Pregel style API) for Java,FLINK-638,12719814,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:41,09/Jun/14 12:41,14/Jul/23 05:57,09/Jun/14 12:41,,,,pre-apache,,,,,,,0,github-import,,"To get a feel of the api, check out this example (connected components)

```
public class SpargelConnectedComponents {

	public static void main(String[] args) throws Exception {
		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

		DataSet<Long> vertexIds = env.generateSequence(0, 10);
		DataSet<Tuple2<Long, Long>> edges = env.fromElements(new Tuple2<Long, Long>(0L, 2L), new Tuple2<Long, Long>(2L, 4L), new Tuple2<Long, Long>(4L, 8L),
															new Tuple2<Long, Long>(1L, 5L), new Tuple2<Long, Long>(3L, 7L), new Tuple2<Long, Long>(3L, 9L));

		DataSet<Tuple2<Long, Long>> initialVertices = vertexIds.map(new IdAssigner());

		DataSet<Tuple2<Long, Long>> result = initialVertices.runOperation(SpargelIteration.withPlainEdges(edges, new CCUpdater(), new CCMessager(), 100));

		result.print();
		env.execute(""Spargel Connected Components"");
	}


	public static final class CCUpdater extends VertexUpdateFunction<Long, Long, Long> {
		@Override
		public void updateVertex(Long vertexKey, Long vertexValue, MessageIterator<Long> inMessages) {
			long min = Long.MAX_VALUE;
			for (long msg : inMessages) {
				min = Math.min(min, msg);
			}
			if (min < vertexValue) {
				setNewVertexValue(min);
			}
		}
	}

	public static final class CCMessager extends MessagingFunction<Long, Long, Long, NullValue> {
		@Override
		public void sendMessages(Long vertexId, Long componentId) {
			sendMessageToAllNeighbors(componentId);
		}
	}

	public static final class IdAssigner extends MapFunction<Long, Tuple2<Long, Long>> {

		@Override
		public Tuple2<Long, Long> map(Long value) throws Exception {
			return new Tuple2<Long, Long>(value, value);
		}
	}
}
```
From https://github.com/StephanEwen/stratosphere/blob/spargel/stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/examples/SpargelConnectedComponents.java



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/638
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: java api, 
Created at: Fri Mar 28 03:01:25 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:41;github-import;pull-request-638-784526149979384146.patch;https://issues.apache.org/jira/secure/attachment/12649239/pull-request-638-784526149979384146.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398013,,,Mon Jun 09 12:41:19 UTC 2014,,,,,,,,,,"0|i1wiuv:",398140,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:41;github-import;[Date: Mon Apr 07 19:11:32 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [253b4cdadacd71247aedf2d647aef34f5d4e0c13|https://github.com/stratosphere/stratosphere/commit/253b4cdadacd71247aedf2d647aef34f5d4e0c13];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add DateValue,FLINK-637,12719813,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:41,09/Jun/14 12:41,14/Jul/23 05:57,09/Jun/14 12:41,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/637
Created by: [zerolevel|https://github.com/zerolevel]
Labels: 
Created at: Thu Mar 27 23:48:14 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:41;github-import;pull-request-637-6561165484829601738.patch;https://issues.apache.org/jira/secure/attachment/12649238/pull-request-637-6561165484829601738.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398012,,,Mon Jun 09 12:41:11 UTC 2014,,,,,,,,,,"0|i1wiun:",398139,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:41;github-import;[Date: Fri Mar 28 06:52:38 CET 2014, Author: [zerolevel|https://github.com/zerolevel]]

I am not sure if the above error is due to my code!;;;","09/Jun/14 12:41;github-import;[Date: Fri Mar 28 13:04:16 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi, I merged your changes, my change and some other changes into this (new!) branch: https://github.com/rmetzger/stratosphere/tree/sql_mainline_changes_update1
I'm not sure if I merged everything correctly;;;","09/Jun/14 12:41;github-import;[Date: Fri Mar 28 14:16:32 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I will resubmit a pull request for all Stratosphere-SQL related mainline changes once the SQL interface reached alpha status.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cost estimator,FLINK-632,12719808,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:38,09/Jun/14 12:38,14/Jul/23 05:57,09/Jun/14 12:38,,,,pre-apache,,,,,,,0,github-import,,"This fixes ([#613|https://github.com/stratosphere/stratosphere/issues/613] | [FLINK-613|https://issues.apache.org/jira/browse/FLINK-613]). First the Costs members are now doubles to support divisions. The cumulative costs of a node are now computed in the following way: Costs for the node + adjusted costs of all predecessor nodes, where adjusted costs means that the costs of the predecessor node is divided by its out degree. This achieves that the costs of a node is propagated equally among its successors.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/632
Created by: [tillrohrmann|https://github.com/tillrohrmann]
Labels: 
Created at: Wed Mar 26 12:10:19 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:38;github-import;pull-request-632-4179507864274648324.patch;https://issues.apache.org/jira/secure/attachment/12649234/pull-request-632-4179507864274648324.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398007,,,Mon Jun 09 12:38:48 UTC 2014,,,,,,,,,,"0|i1witj:",398134,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:38;github-import;[Date: Wed Mar 26 14:26:20 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I like the principle idea of dividing costs. I am a bit skeptical concerning the switch to double actually. 

With the large numbers that we represent in the doubles, the domain starts to become very sparse and it may be that computations like `costs += 1` do not change the value, because that specific value could not be represented. We may need to do such computations to introduce biases, though that's not clear yet.

On the other hand, the same would occur when we divide the integer valued costs by the number of outputs. Although I guess that is okay, as we never compare plans across different candidates at such ""branching points"".

All in all I think that representing the costs as `long` is a bit safer right now.

What do you think?;;;","09/Jun/14 12:38;github-import;[Date: Thu Mar 27 01:34:18 CET 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

You are right that the problem of absorption might occur if the costs get really huge. And you are also right that using ```long``` would not be a problem if one reasons about one group of plans with the same interesting properties. Only the best plan is kept at the branching point and thus prior to the actual division of the costs. 

But what about the case where one has two groups of plans with different interesting properties at a branching node ```x```. Let's assume their costs are ```p``` for interesting properties A and ```p+1``` for interesting properties B. Let's assume the node costs at the successor node ```y``` is ```c``` for both interesting properties A and B. Furthermore ```x``` has a branching degree of 2. Then the cumulative costs of ```y``` for A would be ```c +  p/2``` and for B would be ```c+(p+1)/2```. Then you couldn't distinguish anymore which plan is the cheapest. (I hope that I got the pruning mechanism right....) I see the problem of this scenario that there cannot be two groups of plans at the end of the getAlternativePlans method, but I haven't found the source code location yet where this is assured. Does it have any effect if one doesn't know anymore which plan is the cheapest?

Another problem might be that other programmers could stumble upon this design decision wondering why one can forget about the decimal fraction. This can of course be alleviated by some explanatory comments.

How big does the costs actually get? Using doubles, one has roughly 16 digits until the problem with ```costs+1``` occurs. A long is of course better with roughly 20 digits but if one talks about numbers of this magnitude the jump from doubles to longs might not be sufficient and one could end up with a overflow when using longs.;;;","09/Jun/14 12:38;github-import;[Date: Sat Mar 29 21:20:20 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

During pruning, plans are not compared across different candidates at branching points. Branch points act like an additional 'super' interesting property.

Plans across candidates at branch points are only compared at the point where the branch has been closed. At that point, it may happen that plan costs appear equal due to rounding loss.

I think the problem will occur in both ways, though at different points. It will most likely not matter for the operator costs themselves, only for small values we add to introduce a bias.

The costs actually get fairly big. Especially the heuristic costs (see cost estimator classes).;;;","09/Jun/14 12:38;github-import;[Date: Sat Mar 29 21:22:00 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think we can go ahead with this for now.;;;","09/Jun/14 12:38;github-import;[Date: Sun Mar 30 23:23:36 CEST 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

Do you mean going ahead with ```double``` or ```long```?;;;","09/Jun/14 12:38;github-import;[Date: Mon Mar 31 09:24:30 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Double
Am 30.03.2014 23:23 schrieb ""Till Rohrmann"" <notifications@github.com>:

> Do you mean going ahead with
>
> double
>
> or
>
> long
>
> ?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/632#issuecomment-39040193>
> .
>;;;","09/Jun/14 12:38;github-import;[Date: Mon Apr 07 19:12:02 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [84c2bcb0d8cb4c2ce91f93686cf646a786a3fc60|https://github.com/stratosphere/stratosphere/commit/84c2bcb0d8cb4c2ce91f93686cf646a786a3fc60];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add WebLogAnalysis example,FLINK-631,12719807,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:38,09/Jun/14 12:38,14/Jul/23 05:57,09/Jun/14 12:38,,,,pre-apache,,,,,,,0,github-import,,"filter factor is still missing

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/631
Created by: [qmlmoon|https://github.com/qmlmoon]
Labels: documentation, java api, 
Milestone: Release 0.5
Created at: Wed Mar 26 12:03:37 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:38;github-import;pull-request-631-4666827703431788728.patch;https://issues.apache.org/jira/secure/attachment/12649233/pull-request-631-4666827703431788728.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398006,,,Mon Jun 09 12:38:41 UTC 2014,,,,,,,,,,"0|i1witb:",398133,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:38;github-import;[Date: Tue Apr 01 13:44:38 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I tested it locally on Windows and everything seems to run smoothly.
@qmlmoon: Very nice comments!;;;","09/Jun/14 12:38;github-import;[Date: Wed Apr 16 22:48:58 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Hi @qmlmoon, just an advice for the next time when you update a PR to address comments:
Please add additional commits and do not squeeze them and force a push. Otherwise, the previous comments can no longer be matched to the code and we cannot check how the comments were addressed.
The commits can be squeezed during merging.;;;","09/Jun/14 12:38;github-import;[Date: Wed Apr 30 20:50:50 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Good to merge once the UDF join has been replaced by a projection join. ;;;","09/Jun/14 12:38;github-import;[Date: Wed Apr 30 20:53:14 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Please rebase the PR to the master to check for any other API changes.
Thank you!;;;","09/Jun/14 12:38;github-import;[Date: Tue May 06 22:52:25 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [18b654fa2f6d495a6ad0c8050dd3abe870a1ed2a|https://github.com/stratosphere/stratosphere/commit/18b654fa2f6d495a6ad0c8050dd3abe870a1ed2a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
small change to write function for new java api to set the write mode,FLINK-630,12719806,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:38,09/Jun/14 12:38,14/Jul/23 05:57,09/Jun/14 12:38,,,,pre-apache,,,,,,,0,github-import,,"See ([#626|https://github.com/stratosphere/stratosphere/issues/626] | [FLINK-626|https://issues.apache.org/jira/browse/FLINK-626])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/630
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Wed Mar 26 11:59:22 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:38;github-import;pull-request-630-7078755521038380998.patch;https://issues.apache.org/jira/secure/attachment/12649232/pull-request-630-7078755521038380998.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398005,,,Mon Jun 09 12:38:33 UTC 2014,,,,,,,,,,"0|i1wit3:",398132,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:38;github-import;[Date: Wed Apr 02 22:45:39 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The way this pull request changes the behavior is that the system always uses the `NO_OVERWRITE` mode unless something else is specified. The configured default write mode is therefore always overridden and never used.

I think the method that takes no write mode parameter should not set a write mode. If the write mode is not set, the configured default write mode (in stratosphere-conf.yml) of the executing environment is used. If you throw it to the cluster, then the cluster decided whether it accepts overwriting files.;;;","09/Jun/14 12:38;github-import;[Date: Mon Apr 07 22:29:39 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

Thanks for your feedback, Stephan! I changed the implementation accordingly.;;;","09/Jun/14 12:38;github-import;[Date: Mon Apr 14 01:44:19 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [5cc81cdd1f71c740669c7c4284524499d6e8617a|https://github.com/stratosphere/stratosphere/commit/5cc81cdd1f71c740669c7c4284524499d6e8617a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add parser for BigDecimal,FLINK-628,12719804,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:38,09/Jun/14 12:38,14/Jul/23 05:57,09/Jun/14 12:38,,,,pre-apache,,,,,,,0,github-import,,"I [need|https://github.com/rmetzger/stratosphere-sql/issues/17) BigDecimal support the Stratosphere SQL interface.
The pull request adds a `DecimalValue` to Stratosphere, including a parser for the CSVInputFormat and a little test.

I also added a `GenericCsvInputFormat.getFieldParser(]` method to allow external configuration of the parser. I will also need this for the [planned `DateValue` type|https://github.com/rmetzger/stratosphere-sql/issues/16] to configure different Date parsers.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/628
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue Mar 25 18:07:08 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:38;github-import;pull-request-628-71130037844924232.patch;https://issues.apache.org/jira/secure/attachment/12649231/pull-request-628-71130037844924232.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398003,,,Mon Jun 09 12:38:24 UTC 2014,,,,,,,,,,"0|i1wisn:",398130,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:38;github-import;[Date: Tue Mar 25 21:18:06 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Rat doesn't like your code ;-) I think you forgot a header somewhere...;;;","09/Jun/14 12:38;github-import;[Date: Fri Mar 28 14:16:37 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I will resubmit a pull request for all Stratosphere-SQL related mainline changes once the SQL interface reached alpha status.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add a fail(Exception) method to the job graph, to report problems detected in RPC calls",FLINK-625,12719801,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,github-import,github-import,09/Jun/14 12:38,21/Sep/14 02:19,14/Jul/23 05:57,21/Sep/14 02:19,,,,0.7.0-incubating,,,Runtime / Task,,,,0,github-import,,"Inside RPC calls, throwing an exception kills only the RPC service. Instead, the job graph needs to be failed by the exception.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/625
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Mon Mar 24 18:58:23 CET 2014
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,FLINK-1019,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,398000,,,Sun Sep 21 02:19:56 UTC 2014,,,,,,,,,,"0|i1wirz:",398127,,,,,,,,,,,,,,,,,,,,"21/Sep/14 02:19;sewen;Fixed in ae139f5ae2199a52e8d7f561f94db51631107d00;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cancelling a task may cause the IncomingConnectionThread to die,FLINK-624,12719800,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:38,18/Jun/14 23:00,14/Jul/23 05:57,18/Jun/14 23:00,,,,pre-apache,,,,,,,0,github-import,,"If the receiver of a transfer envelope is no longer available, the incoming connection thread can no longer find target buffers to fill and will die with an exception.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/624
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Mon Mar 24 17:39:44 CET 2014
State: open
",,github-import,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397999,,,Wed Jun 18 23:00:17 UTC 2014,,,,,,,,,,"0|i1wirr:",398126,,,,,,,,,,,,,,,,,,,,"18/Jun/14 23:00;uce;Fixed as part of [4cd4a13415d609a2979c8fa3cf4b797c990ee8c2|https://github.com/apache/incubator-flink/commit/4cd4a13415d609a2979c8fa3cf4b797c990ee8c2].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add CollectionOutputFormat for local executor,FLINK-623,12719799,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:37,09/Jun/14 12:38,14/Jul/23 05:57,09/Jun/14 12:38,,,,pre-apache,,,,,,,0,github-import,,"implement issue ([#602|https://github.com/stratosphere/stratosphere/issues/602] | [FLINK-602|https://issues.apache.org/jira/browse/FLINK-602])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/623
Created by: [qmlmoon|https://github.com/qmlmoon]
Labels: 
Created at: Fri Mar 21 18:15:22 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:37;github-import;pull-request-623-1625180174695485718.patch;https://issues.apache.org/jira/secure/attachment/12649229/pull-request-623-1625180174695485718.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397998,,,Mon Jun 09 12:38:02 UTC 2014,,,,,,,,,,"0|i1wirj:",398125,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:37;github-import;[Date: Sun Mar 23 12:24:04 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It is a good start. There are two things I would like to improve:

  - I don't think we should have a dedicated method for this on `DataSet`. because this will only work for local testing. I would prefer to use it only through `result.output(new CollectionOutputFormat(list))`.
  - This will not work for jobs with multiple sinks, because you have only one static reference. If you had two CollectionOutputFormats in the same program, both would get the union of the results. I think you can work around that by giving each CollectionOutputFormat a generated id and have a static map of collections, rather than only one static field.;;;","09/Jun/14 12:38;github-import;[Date: Sun Mar 23 15:07:15 CET 2014, Author: [qmlmoon|https://github.com/qmlmoon]]

```CollectionOutputFormat``` needs a ```TypeSerializer``` from ```DataSet``` to make a copy of record. To remove the method on ```DataSet```, I would do something like ```CollectionOutputFormat<T extends Tuple>``` to use ```SerializationUtil.clone()```, which forces the element of the collection to be a tuple. But it's not good for type safty as Fabian pointed.;;;","09/Jun/14 12:38;github-import;[Date: Sun Mar 23 19:42:46 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I see. But I don't think the workaround can be to add a method to data set for every possible output format.
Let's add an interface called `TypeConfigurable` or so. Then the output() function can call a method on corresponding output formats and pass the type.;;;","09/Jun/14 12:38;github-import;[Date: Sun Mar 23 20:24:36 CET 2014, Author: [qmlmoon|https://github.com/qmlmoon]]

What about to use the collection like ```result.output(new CollectionOutputFormat(list, result.getType().createSerializer))``` ,just specify the serialiser manually;;;","09/Jun/14 12:38;github-import;[Date: Tue Mar 25 22:40:52 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks nice to me :-)

I think we should rename it, just to make sure that it is obvious that this thing is only intended for local executor use. Maybe call it `LocalCollectionOutputFormat` or so.;;;","09/Jun/14 12:38;github-import;[Date: Thu Apr 03 21:26:30 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Good to merge, thanks! :+1:;;;","09/Jun/14 12:38;github-import;[Date: Thu Apr 03 21:31:36 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Will merge this now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added static method for DecimalTextIntParser,FLINK-618,12719794,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:37,09/Jun/14 12:37,14/Jul/23 05:57,09/Jun/14 12:37,,,,pre-apache,,,,,,,0,github-import,,"I added the static method for DecimalTextIntParser (issue ([#427|https://github.com/stratosphere/stratosphere/issues/427] | [FLINK-427|https://issues.apache.org/jira/browse/FLINK-427])) the same way is done in DecimalTextLongParser.
I'm planning on adding static methods for the rest of the parsers.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/618
Created by: [gruay|https://github.com/gruay]
Labels: 
Created at: Thu Mar 20 14:36:15 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:37;github-import;pull-request-618-955318392105901085.patch;https://issues.apache.org/jira/secure/attachment/12649227/pull-request-618-955318392105901085.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397993,,,Mon Jun 09 12:37:30 UTC 2014,,,,,,,,,,"0|i1wiqf:",398120,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:37;github-import;[Date: Thu Mar 20 14:54:30 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hey,
thank you for your pull request.
Is there a way to use only one method for the parsing? I have the impression that `public int parseField()` and `public static final int parseField()` are quite similar.;;;","09/Jun/14 12:37;github-import;[Date: Thu Mar 20 16:59:47 CET 2014, Author: [gruay|https://github.com/gruay]]

Right now the non static one is returning the position in the byte array where it ended parsing the int (or -1 if something went wrong) and then the parsed number is put in a parameter given, whereas the static one simply returns the parsed int.
So, they both do the same but the non-static one gives more information.
I'm not sure if that answer your question now that I re-read it;;;","09/Jun/14 12:37;github-import;[Date: Thu Mar 20 17:05:31 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I was just wondering if there would be a way to avoid duplicate code, for example by introducing an ""internal"" method that can handle both cases?;;;","09/Jun/14 12:37;github-import;[Date: Thu Mar 20 17:35:01 CET 2014, Author: [gruay|https://github.com/gruay]]

Sure, I could move the non-static code to a new static method that would do the same. Then the non-static method would call that one for getting both the ending position and the value and the static method would just care about the parsed value.
Do you mean something like this?;;;","09/Jun/14 12:37;github-import;[Date: Fri Mar 21 17:19:15 CET 2014, Author: [gruay|https://github.com/gruay]]

Hey, I just created the new static method that is called from both methods.
Did you mean something like this?;;;","09/Jun/14 12:37;github-import;[Date: Tue Mar 25 22:21:57 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I agree with Robert. That is very fragile code, the way the error messages and return codes are handled. Also, throwing an exception and then turning that into a state seems reversed. Why not parse and set the state, and then react with an exception to the state (if it is not the ""OKAY"" state)?;;;","09/Jun/14 12:37;github-import;[Date: Tue Mar 25 22:24:37 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I am not sure about the use case of this method, but I have a feeling that the delimiter is odd. I think @sscdotopen 's initial case was that it is odd to come up with some delimiter only to have a standalone array parsed.

The original parse method is used to parse a field as part of a CSV row. The standalone method ,seems to me, it rather intended to parse an isolated value inside an array.

@sscdotopen Can you comment on this?;;;","09/Jun/14 12:37;github-import;[Date: Mon Apr 14 20:32:00 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

What do we do with this pull request? I don't want to merge it in the state that it is in, I think it needs to be improved.

Is anything going to happen here?;;;","09/Jun/14 12:37;github-import;[Date: Mon Apr 14 20:33:45 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I think the problem is, that it is not really clear what issue ([#427|https://github.com/stratosphere/stratosphere/issues/427] | [FLINK-427|https://issues.apache.org/jira/browse/FLINK-427]) is about...;;;","09/Jun/14 12:37;github-import;[Date: Mon Apr 14 20:37:23 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think this issue simply asks for a utility to parse binary text data (as
read from a file stream) directly into a primitive type.

Is it fair to ask whoever really needs it should implement it? I think it
is not a priority thing right now.;;;","09/Jun/14 12:37;github-import;[Date: Wed Apr 16 19:00:54 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I will close this pull request, because the code is not in shape to be merged and the functionality has by now been subsumed by [a1555ce84ae47edb8cd696f672935cbeffd3481b|https://github.com/stratosphere/stratosphere/commit/a1555ce84ae47edb8cd696f672935cbeffd3481b] including tests.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change of java version without effect.,FLINK-616,12719791,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:37,08/Sep/16 08:12,14/Jul/23 05:57,09/Jun/14 12:37,,,,pre-apache,,,,,,,0,github-import,,"The compiler plugin was configured under the wrong section. It used to
be a child of reporting but is now a child of build. Changing the source
and target properties in the old pom were without any effect. They were
simply ignored. 

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/616
Created by: [carabolic|https://github.com/carabolic]
Labels: 
Created at: Thu Mar 20 13:49:44 CET 2014
State: closed
",,githubbot,github-import,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:37;github-import;pull-request-616-8025446799688161987.patch;https://issues.apache.org/jira/secure/attachment/12649225/pull-request-616-8025446799688161987.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397990,,,Thu Sep 08 08:12:04 UTC 2016,,,,,,,,,,"0|i1wipr:",398117,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:37;github-import;[Date: Thu Mar 20 13:51:31 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Oh nice ;) Thank you, very good catch.

We are using tabs instead of spaces. I think your indentation are spaces.;;;","09/Jun/14 12:37;github-import;[Date: Thu Mar 20 13:53:01 CET 2014, Author: [carabolic|https://github.com/carabolic]]

Really TABS? Alright I'm fixing it.;;;","09/Jun/14 12:37;github-import;[Date: Thu Mar 20 13:53:49 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I think we had the the compiler plugin twice in the pom file.
Have a look at the travis output:

```
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for eu.stratosphere:stratosphere-core:jar:0.5-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-compiler-plugin @ eu.stratosphere:stratosphere:0.5-SNAPSHOT, /home/travis/build/stratosphere/stratosphere/pom.xml, line 367, column 12
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for eu.stratosphere:stratosphere-java:jar:0.5-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-compiler-plugin @ eu.stratosphere:stratosphere:0.5-SNAPSHOT, /home/travis/build/stratosphere/stratosphere/pom.xml, line 367, column 12
[WARNING] 
```

I think it is sufficient to remove the maven compiler plugin in the reporting section.;;;","09/Jun/14 12:37;github-import;[Date: Thu Mar 20 14:40:37 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Can you make one commit out of your pull request?  (force push into the branch);;;","09/Jun/14 12:37;github-import;[Date: Thu Mar 20 15:20:08 CET 2014, Author: [carabolic|https://github.com/carabolic]]

Done!;;;","09/Jun/14 12:37;github-import;[Date: Thu Mar 20 15:30:41 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you.
I'll merge it later.;;;","06/Sep/16 08:30;githubbot;GitHub user aljoscha opened a pull request:

    https://github.com/apache/incubator-beam/pull/921

    [FLINK-616] Update Flink Runner to Flink 1.1.2

    R: @mxm for review


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/aljoscha/incubator-beam flink-1.1.2

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-beam/pull/921.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #921
    
----
commit 129ff811754f33313ddfe7021d6c2d49f6c57f35
Author: Aljoscha Krettek <aljoscha.krettek@gmail.com>
Date:   2016-09-05T16:17:11Z

    [FLINK-616] Update Flink Runner to Flink 1.1.2

----
;;;","08/Sep/16 08:12;githubbot;Github user aljoscha closed the pull request at:

    https://github.com/apache/incubator-beam/pull/921
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix for ReduceOperator mutable object return bug.,FLINK-614,12719789,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:36,09/Jun/14 12:36,14/Jul/23 05:57,09/Jun/14 12:36,,,,pre-apache,,,,,,,0,github-import,,"This PR proposes a fix for ([#563|https://github.com/stratosphere/stratosphere/issues/563] | [FLINK-563|https://issues.apache.org/jira/browse/FLINK-563]) and is a rebased version of PR ([#578|https://github.com/stratosphere/stratosphere/issues/578] | [FLINK-578|https://issues.apache.org/jira/browse/FLINK-578]).

Performance implications should be tolerable. For Sort-Group strategy only the first value of a group is copied. For Hash-Aggregate strategy (not supported yet), there will be no performance implications.

The fix assumes that the object of the first input value (```val1```) of a ```T reduce(T val1, T val2)``` may be returned and that the object of the second input value (```val2```) may NOT be returned.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/614
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, java api, user satisfaction, 
Created at: Thu Mar 20 11:50:58 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:36;github-import;pull-request-614-7731991632494210095.patch;https://issues.apache.org/jira/secure/attachment/12649224/pull-request-614-7731991632494210095.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397988,,,Mon Jun 09 12:36:53 UTC 2014,,,,,,,,,,"0|i1wipb:",398115,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:36;github-import;[Date: Sun Mar 23 12:59:16 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think this is a flavor of the problem found in ([#131|https://github.com/stratosphere/stratosphere/issues/131] | [FLINK-131|https://issues.apache.org/jira/browse/FLINK-131]).

If I am not mistaken, then the problem is the following: Modifying a value may potentially overwrite the key in that value and cause further KeyGroupIterator comparisons to chunk the stream into key groups to fail.

If that is the only problem, then I think we can fix it without copying by introducing a third value in the KeyGroupedIterator.

Are there other problems that I overlooked?;;;","09/Jun/14 12:36;github-import;[Date: Sun Mar 30 22:00:20 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Well, it is related to ([#131|https://github.com/stratosphere/stratosphere/issues/131] | [FLINK-131|https://issues.apache.org/jira/browse/FLINK-131]) because it is a problem in the context of the mutable object iterator of the Reduce operator. However, the problem itself is different. In ([#131|https://github.com/stratosphere/stratosphere/issues/131] | [FLINK-131|https://issues.apache.org/jira/browse/FLINK-131]), I proposed to return the same object instance for every ```next()``` call of the iterator. That would make this issue even more problematic...

The problem of this issue is, that an implementation of ```T reduce(T t1, T t2)``` might return ```t1``` or ```t2``` (either the same one every time or picking one depending on the context)
Let's have a look at the different cases:

1. **The function returns ```t1``` in every call.** In this case, the object in which the result is ""aggregated"" will be the same for the whole group because ```ReduceFunction``` performs the next call of ```reduce(t1, t2)``` with ```t1``` being the result of the previous call (or the first element of the group). If the mutable object iterator alternates between two objects, every second call of ```hasNext()``` will cause that the ""aggregated"" result will be overwritten by the next element of the group (or even the first element of the next group). Adding another mutable object to the iterator will only increase the interval but not fix this issue. This PR proposes to copy the first element of the group into a dedicated object which will hold the aggregated result while the whole group is processed and thereby fix the issue for this case.

2. **The function returns ```t2``` in every call.** In this case we should be fine, because the object that holds the aggregated value is always different from the object that will hold the next element of the group. I have not verified this with a unit test. This would contradict with the Java Doc comment added by this PR but would not require any code changes for this case.

3. **The function returns either ```t1``` or ```t2```.** In this case, copying the first object does not help because the dedicated object might get replaced by one of the iterators mutable objects. 

So this PR does not solve the problem but what can we do?

1. Leave the code as it is (after checking that it works) and add a Java Doc that never the first parameter (```t1```) might be returned by the function. This requires that the programmers read and follow the java docs.
1. Try to make the code work for any user function by checking if the returned object is the same as the object which is returned next by the iterator and make a copy in this case. This will cause high overhead due to checks and potential copy operations.
1. Change the signature of reduce to ```void reduce(T t1, T t2, T out)``` and explicitly provide the object that should hold the result of reduce.
1. Something else???;;;","09/Jun/14 12:36;github-import;[Date: Fri Apr 11 15:23:34 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Commit [cf0eff3b1386486ed656aed1c1e8260c0cba5a09|https://github.com/stratosphere/stratosphere/commit/cf0eff3b1386486ed656aed1c1e8260c0cba5a09] goes for the first option (leave code as it is and add documentation).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cost estimation problem for branched plans,FLINK-613,12719788,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:36,09/Jun/14 12:36,14/Jul/23 05:57,09/Jun/14 12:36,,,,pre-apache,,,,,,,0,github-import,,"I encountered the problem of negative cumulative costs for a plan which contained branched nodes. The problem is that the costs for branched nodes are not correctly calculated if one branched node x is contained in a cycle of another branched node y. Assuming that y is closed first, then closing of x will calculate too high double counted costs. To illustrate this case consider the following flow:

<pre>
     A ---- D
   /  \   /  \
SRC     \   SINK
   \  /   \  /
     B ---- C
</pre>
Assuming that the costs of all nodes except for SRC is 0 and the cost of SRC is 1, then SINK should have cumulative costs of 1 at the end. But executing the algorithm will calculate the following sequence.

SRC = 1
A = SRC = 1
B = SRC = 1
D = A + B - SRC (as a joined branching node) = 1
C = A + B - SRC = 1
SINK = D + C - A (as a joined branching node) - B (as a joined branching node) = 0

Thus the problem originates from a wrong accounting of branching nodes. A possible solution for this problem could be to split the predecessor costs according to the out degree of the predecessor node. By doing this, we wouldn't need to subtract double counted costs. But I'm not aware of any side effects this could have....

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/613
Created by: [tillrohrmann|https://github.com/tillrohrmann]
Labels: bug, optimizer, 
Created at: Wed Mar 19 18:53:44 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397987,,,Mon Jun 09 12:36:47 UTC 2014,,,,,,,,,,"0|i1wip3:",398114,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:36;github-import;[Date: Wed Mar 19 21:23:14 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I see the problem that you encounter. The branching logic tracks ""already
closed branches"" to not close them multiple times. But that does not work
properly for the cumulative costs.

I really like the idea of splitting the costs according and distributing
them over the outputs. The implication is that the cumulative costs at some
points are not the proper cumulative costs. Since the cumulative costs are
used for pruning, this might change things. But from the top of my head, I
cannot think of a pathological case.

So I would agree with the solution.
 Am 19.03.2014 18:53 schrieb ""Till Rohrmann"" <notifications@github.com>:

> I encountered the problem of negative cumulative costs for a plan which
> contained branched nodes. The problem is that the costs for branched nodes
> are not correctly calculated if one branched node x is contained in a cycle
> of another branched node y. Assuming that y is closed first, then closing
> of x will calculate too high double counted costs. To illustrate this case
> consider the following flow:
>
>  A ---- D
>
> / \ / \
> SRC \ SINK
> \ / \ /
> B ---- C
>
> Assuming that the costs of all nodes except for SRC is 0 and the cost of
> SRC is 1, then SINK should have cumulative costs of 1 at the end. But
> executing the algorithm will calculate the following sequence.
>
> SRC = 1
> A = SRC = 1
> B = SRC = 1
> D = A + B - SRC (as a joined branching node) = 1
> C = A + B - SRC = 1
> SINK = D + C - A (as a joined branching node) - B (as a joined branching
> node) = 0
>
> Thus the problem originates from a wrong accounting of branching nodes. A
> possible solution for this problem could be to split the predecessor costs
> according to the out degree of the predecessor node. By doing this, we
> wouldn't need to subtract double counted costs. But I'm not aware of any
> side effects this could have....
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/613>
> .
>;;;","09/Jun/14 12:36;github-import;[Date: Thu Mar 20 00:55:38 CET 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

I tried to split the costs among the successors and all test cases pass so far. However, I suspect that there are not many test cases testing the optimiser and the cost estimation and thus I might miss something. If you want to, I can just make a pull request out of it.;;;","09/Jun/14 12:36;github-import;[Date: Wed Mar 26 01:01:45 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The solution sounds good to me. I'd be happy to take a pull request for this :-);;;","09/Jun/14 12:36;github-import;[Date: Thu Apr 10 10:38:07 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Closed through ([#632|https://github.com/stratosphere/stratosphere/issues/632] | [FLINK-632|https://issues.apache.org/jira/browse/FLINK-632]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace Avro serialization by Kryo ,FLINK-610,12719785,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,github-import,github-import,09/Jun/14 12:36,28/Feb/19 14:29,14/Jul/23 05:57,18/Dec/14 11:01,,,,0.8.0,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/610
Created by: [rmetzger|https://github.com/rmetzger]
Labels: java api, 
Milestone: Release 0.6 (unplanned)
Created at: Tue Mar 18 17:29:28 CET 2014
State: open
",,githubbot,github-import,rmetzger,,,,,,,,,,,,,,,,,,,,FLINK-1391,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397984,,,Thu Dec 18 11:01:17 UTC 2014,,,,,,,,,,"0|i1wiof:",398111,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:36;github-import;[Date: Tue Mar 18 20:07:21 CET 2014, Author: [alexff91|https://github.com/alexff91]]

You want to add addon - Kryo? 
and change the following code:
 ```java
@Override
	public void write(DataOutput out) throws IOException {
		// the null flag
		if (datum() == null) {
			out.writeBoolean(false);
		} else {
			out.writeBoolean(true);
			
			DataOutputEncoder encoder = getEncoder();
			encoder.setOut(out);
			getWriter().write(datum(), encoder);
		}
	}
```

To something like:
 ```java
@Override
    public
    void write(
            Kryo kryo,
            Output output) {
        kryo.writeObject(output,
                         blockId);
    }

    @Override
    public
    void read(
            Kryo kryo,
            Input input) {
        Integer blockId = kryo.readObject(input,
                                          Integer.class);
        this.blockId = blockId;
    }
 ```;;;","09/Jun/14 12:36;github-import;[Date: Wed Mar 19 09:53:49 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

As far as I understand the issue, the goal is to use the much faster Kryo serialization framework instead of Avro.
Right now, we use the serializers from Avro for the new Java API's support to use arbitrary objects. We hope to increase the performance by using Kryo for that task.

Where have you taken the first code-snipped from?
;;;","09/Jun/14 12:36;github-import;[Date: Wed Mar 19 10:52:07 CET 2014, Author: [alexff91|https://github.com/alexff91]]

I have taken this from avro-addon for stratosphere project.l, by searching
usages of Avro in API.
19.03.2014 12:53 ÐÏÌØÚÏ×ÁÔÅÌØ ""Robert Metzger"" <notifications@github.com>
ÎÁÐÉÓÁÌ:

> As far as I understand the issue, the goal is to use the much faster Kryo
> serialization framework instead of Avro.
> Right now, we use the serializers from Avro for the new Java API's support
> to use arbitrary objects. We hope to increase the performance by using Kryo
> for that task.
>
> Where have you taken the first code-snipped from?
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/610#issuecomment-38028827>
> .
>;;;","09/Jun/14 12:36;github-import;[Date: Wed Mar 19 11:12:21 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Have a look at the
eu.stratosphere.api.java.typeutils.runtime.AvroSerializer

A Kryo Serializer would implement the same interface.


On Wed, Mar 19, 2014 at 10:52 AM, Aleksandr <notifications@github.com>wrote:

> I have taken this from avro-addon for stratosphere project.l, by searching
> usages of Avro in API.
> 19.03.2014 12:53 ÐÏÌØÚÏ×ÁÔÅÌØ ""Robert Metzger"" <notifications@github.com>
> ÎÁÐÉÓÁÌ:
>
> > As far as I understand the issue, the goal is to use the much faster
> Kryo
> > serialization framework instead of Avro.
> > Right now, we use the serializers from Avro for the new Java API's
> support
> > to use arbitrary objects. We hope to increase the performance by using
> Kryo
> > for that task.
> >
> > Where have you taken the first code-snipped from?
> >
> > --
> > Reply to this email directly or view it on GitHub<
> https://github.com/stratosphere/stratosphere/issues/610#issuecomment-38028827>
>
> > .
> >
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/610#issuecomment-38033001>
> .
>;;;","09/Jun/14 12:36;github-import;[Date: Wed Mar 19 13:57:12 CET 2014, Author: [alexff91|https://github.com/alexff91]]

I will add KryoSerializer near AvroSerializer and  test for testing which of them faster.;;;","09/Jun/14 12:36;github-import;[Date: Tue Apr 08 10:32:16 CEST 2014, Author: [fabiofumarola|https://github.com/fabiofumarola]]

It is not need to try yourself which is the fast. Have a look at https://github.com/eishay/jvm-serializers/wiki. There is a deeply analysis on jvm serializers performance. My option is that would be better to do overloading on the methods in order to support different serialization frameworks. ;;;","09/Jun/14 12:36;github-import;[Date: Tue Apr 08 10:44:07 CEST 2014, Author: [alexff91|https://github.com/alexff91]]

Thanks for the link @fabiofumarola. I'm trying to do KryoSerializer like AvroSerializer by overloading methods, but I can't find any test which uses AvroSerializer to check if everything is ok. ;;;","17/Jul/14 17:14;githubbot;GitHub user tillrohrmann opened a pull request:

    https://github.com/apache/incubator-flink/pull/74

    [FLINK-610] Added KryoSerializer

    I added the KryoSerializer and replaced the AvroSerializer as the standard generic type serializer. Due to the way Flink's serialization works, we cannot exploit Kryo's Inputs which buffer data for serialization. Instead, we have to read byte after byte if we do not know the length of the underlying data. This has consequences for variable length encodings. Kryo will fall back to a slow deserialization of these types, since it will read only one byte at a time. I do not know the implications of this on Kryo's performance. Therefore, I did not remove Avro completely yet. Furthermore, Kryo should be tested on the cluster and if possible its performance should be compared to Avro.
    
    We could mitigate the problem if we don't use variable length encodings for types such as ```int```s and ```long```s. This, however, comes at the price of larger serialized objects. For UTF8 strings, the problem would  persist.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/tillrohrmann/incubator-flink FLINK-610

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/74.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #74
    
----
commit 0c618c20bc7be64de82e0e7d3f7734e8c68593bb
Author: Till Rohrmann <till.rohrmann@gmail.com>
Date:   2014-07-17T15:21:59Z

    Added KryoSerializer

----
;;;","18/Jul/14 09:27;githubbot;Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/74#issuecomment-49411918
  
    Thanks for doing this so fast.
    
    Do you know whether the problems you mentioned will be resolved by [FLINK-987](https://issues.apache.org/jira/browse/FLINK-987)? In other words, are the problems only temporary until FLINK-987 is done?
;;;","18/Jul/14 10:22;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/incubator-flink/pull/74#issuecomment-49416463
  
    The underlying problem is that Flink can use different serializers for serialized objects in one stream. Thus, it is not possible to use the Kryo serializer all the time. If that weren't the case, it would not matter whether Kryo already read some data into its buffer belonging to the next object.
    
    If we have a memory abstraction instead of a stream, as proposed in [FLINK-987](https://issues.apache.org/jira/browse/FLINK-987), which allows to seek the current pointer, we should be able to virtually write data back which has been read from a different object. However, that would require that we can seek backwards over memory segment borders.
;;;","18/Jul/14 10:33;githubbot;Github user aljoscha commented on the pull request:

    https://github.com/apache/incubator-flink/pull/74#issuecomment-49417330
  
    I might be able to provide that as part of [FLINK-987](https://issues.apache.org/jira/browse/FLINK-987).
;;;","20/Aug/14 04:59;githubbot;Github user hsaputra commented on the pull request:

    https://github.com/apache/incubator-flink/pull/74#issuecomment-52734136
  
    Do you use tab in the patch source or is it 4 spaces?
;;;","20/Aug/14 09:31;githubbot;Github user tillrohrmann commented on the pull request:

    https://github.com/apache/incubator-flink/pull/74#issuecomment-52753332
  
    We agreed to use single tab spaces for indentation. Due to that the code on Github does not look so compactly. Did I mix it up somewhere in the code?
;;;","20/Aug/14 13:53;githubbot;Github user hsaputra commented on the pull request:

    https://github.com/apache/incubator-flink/pull/74#issuecomment-52780715
  
    No, you were right. It caught me by surprise because most of the Java/ Scala code I have been working with use spaces for indentation.
;;;","02/Sep/14 17:26;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/74
;;;","13/Oct/14 09:55;rmetzger;I just found out that Avro is not even able to serialize java.util.Date (it took me some time to figure out why my join is not working). They have already fixed the issue in the upcoming 1.8.0 release: https://issues.apache.org/jira/browse/AVRO-739
Still, I think this issue is very important if we claim to support arbitrary java objects in our API.;;;","16/Dec/14 10:13;rmetzger;Another user is affected by this issue.
I'll replace the generic serialization with Kryo for the 0.8 release. ;;;","16/Dec/14 20:18;githubbot;GitHub user rmetzger opened a pull request:

    https://github.com/apache/incubator-flink/pull/271

    [FLINK-610] Replace Avro with Kryo as the GenericType serializer

    This pull request is basically a collection of the work of others ;)
    @tillrohrmann contributed some of the tests (in particular for the serializer itself). He did the integration with [Chill](https://github.com/twitter/chill).
    @twalthr contributed one commit for fixing a TypeExtractor bug.
    
    I added two IT cases, checking it to work with the following types:
    
    ```java
    java.util.List<String>
    scala.math.BigInt
    java.io.File
    java.math.BigDecimal;
    java.math.BigInteger;
    java.util.HashMap;
    java.io.Serializable;
    ```
    
    I also tested it on a cluster with a Collection of Objects (containing Strings and Integers). It worked. I didn't test it for performance yet. The purpose of the pull request is to extend the type support for cases we don't cover by our own serialization framework.
    
    I'll now test the change with the code that brought the issue on top of my TODO list: https://issues.apache.org/jira/browse/FLINK-629?focusedCommentId=14241856&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14241856

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rmetzger/incubator-flink flink610-master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/271.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #271
    
----
commit b9dac6bad47e41fc3bc1dafc3eaede1a3bf63496
Author: twalthr <info@twalthr.com>
Date:   2014-12-10T21:02:07Z

    Fix invalid type hierarchy creation by Pojo logic

commit 64d2e52970f761ad117aec0080d0618f7f7ace95
Author: Robert Metzger <rmetzger@apache.org>
Date:   2014-12-16T10:30:52Z

    [FLINK-610] Replace Avro by Kryo as the GenericType serializer
    
    The performance of data-intensive jobs using Kryo is probably going to be slow.

----
;;;","17/Dec/14 09:53;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/271#issuecomment-67299689
  
    I rebased the pull request to the 0.8 branch and added a fix for https://issues.apache.org/jira/browse/FLINK-1333.
;;;","17/Dec/14 09:58;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/271#issuecomment-67300275
  
    Very good idea. I think we could fix this still a bit and make it better:
    
      - Avro created a schema from the objects to serialize (somewhat a simple thing as our POJO analysis). That way, it does not need to write all classnames. Kryo writes all classnames by default, which is very painful, unless they are registered. I think that was one of the reasons why Avro was actually faster for us out of the box.
         - We should register the types at Kryo that we see as being (recursively contained in the type information)
         - We should also allow at the execution environment to manually register types or serializers.
    
      - Why is the `copy()` method not using Kryo's copy functionality? The current implementation seems painfully inefficient, and will still be used until @aljoscha 's change about reuse/non-reuse mode is in.
    
      - The tests that have a hardwired check for the KryoSerializer break design...
    
    
    It seems that the Java/Scala code is moving closer together still, with the common serializer being configured in the Java API for Scala classes. That means we might really collapse the projects at some point.
;;;","17/Dec/14 10:59;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/271#issuecomment-67307001
  
    Thank you for the feedback.
    * We are using the Twitter Chill library. It registers many commonly used classes with Kryo (Collections, Date, BigInt, ...). So the issue with class names being written does only apply to userdefined types.
    You are indeed right that we should provide facilities to register custom classes and that we should register all classes we see during type analysis.
    I didn't implement these features because Timo is actually working on this and I wanted to have at least basic Kryo support in the release. Timo will integrate Kryo more tightly with the type analysis.
    
    * The `copy()` method of Kryo is not implemented for many types (in particular those by Chill). For example the java.sql.Date type (https://github.com/twitter/chill/blob/develop/chill-java/src/main/java/com/twitter/chill/java/SqlDateSerializer.java) doesn't have a copy() method. 
    Kryo provides a default copy() method which fails on mutable types. (https://github.com/EsotericSoftware/kryo/blob/master/src/com/esotericsoftware/kryo/Serializer.java#L102). I thought about contributing the missing copy() methods for chill.
    We could also whitelist some classes for kryo's copy and fall back to the slow variant.
    
    * The hardwired check for the KryoSerializer is very ugly. Maybe I can come up with a different solution. Till suggested to add a method (`canCreateInstance`) to all serializers .. but thats a lot of code for one test case.
;;;","17/Dec/14 12:57;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/271#issuecomment-67318300
  
    How about we try to use Kryo's copy method and fall back to serialization
    copies upon failure?
    Am 17.12.2014 11:59 schrieb ""Robert Metzger"" <notifications@github.com>:
    
    > Thank you for the feedback.
    >
    >    -
    >
    >    We are using the Twitter Chill library. It registers many commonly
    >    used classes with Kryo (Collections, Date, BigInt, ...). So the issue with
    >    class names being written does only apply to userdefined types.
    >    You are indeed right that we should provide facilities to register
    >    custom classes and that we should register all classes we see during type
    >    analysis.
    >    I didn't implement these features because Timo is actually working on
    >    this and I wanted to have at least basic Kryo support in the release. Timo
    >    will integrate Kryo more tightly with the type analysis.
    >    -
    >
    >    The copy() method of Kryo is not implemented for many types (in
    >    particular those by Chill). For example the java.sql.Date type (
    >    https://github.com/twitter/chill/blob/develop/chill-java/src/main/java/com/twitter/chill/java/SqlDateSerializer.java)
    >    doesn't have a copy() method.
    >    Kryo provides a default copy() method which fails on mutable types. (
    >    https://github.com/EsotericSoftware/kryo/blob/master/src/com/esotericsoftware/kryo/Serializer.java#L102).
    >    I thought about contributing the missing copy() methods for chill.
    >    We could also whitelist some classes for kryo's copy and fall back to
    >    the slow variant.
    >    -
    >
    >    The hardwired check for the KryoSerializer is very ugly. Maybe I can
    >    come up with a different solution. Till suggested to add a method (
    >    canCreateInstance) to all serializers .. but thats a lot of code for
    >    one test case.
    >
    > —
    > Reply to this email directly or view it on GitHub
    > <https://github.com/apache/incubator-flink/pull/271#issuecomment-67307001>
    > .
    >
;;;","17/Dec/14 12:58;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/271#issuecomment-67318414
  
    Okay, I'll add that
;;;","17/Dec/14 15:46;githubbot;Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/271#issuecomment-67341195
  
    That makes sense.
    
    +1 to merge
;;;","18/Dec/14 11:00;githubbot;Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/271#issuecomment-67470974
  
    Manually closing it. Has been merged
;;;","18/Dec/14 11:00;githubbot;Github user rmetzger closed the pull request at:

    https://github.com/apache/incubator-flink/pull/271
;;;","18/Dec/14 11:01;rmetzger;http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/a70aa67a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Write tests for new java api,FLINK-609,12719783,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:36,09/Jun/14 12:36,14/Jul/23 05:57,09/Jun/14 12:36,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/609
Created by: [rmetzger|https://github.com/rmetzger]
Labels: duplicate, java api, 
Milestone: Release 0.5
Assignee: [fhueske|https://github.com/fhueske]
Created at: Tue Mar 18 17:12:53 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397982,,,Mon Jun 09 12:36:23 UTC 2014,,,,,,,,,,"0|i1winz:",398109,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:36;github-import;[Date: Wed Mar 19 00:48:31 CET 2014, Author: [fhueske|https://github.com/fhueske]]

There are several more detailed issues for this (([#547|https://github.com/stratosphere/stratosphere/issues/547] | [FLINK-547|https://issues.apache.org/jira/browse/FLINK-547]), ([#548|https://github.com/stratosphere/stratosphere/issues/548] | [FLINK-548|https://issues.apache.org/jira/browse/FLINK-548]),  ([#608|https://github.com/stratosphere/stratosphere/issues/608] | [FLINK-608|https://issues.apache.org/jira/browse/FLINK-608]), ([#579|https://github.com/stratosphere/stratosphere/issues/579] | [FLINK-579|https://issues.apache.org/jira/browse/FLINK-579]), ([#589|https://github.com/stratosphere/stratosphere/issues/589] | [FLINK-589|https://issues.apache.org/jira/browse/FLINK-589]));;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate examples with test base ,FLINK-608,12719782,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:36,09/Jun/14 12:36,14/Jul/23 05:57,09/Jun/14 12:36,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/608
Created by: [rmetzger|https://github.com/rmetzger]
Labels: java api, 
Created at: Tue Mar 18 17:12:32 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397981,,,Mon Jun 09 12:36:20 UTC 2014,,,,,,,,,,"0|i1winr:",398108,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:36;github-import;[Date: Thu May 15 15:23:51 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Done, I believe.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delta iterations in new Java API,FLINK-607,12719781,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:36,09/Jun/14 12:36,14/Jul/23 05:57,09/Jun/14 12:36,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/607
Created by: [rmetzger|https://github.com/rmetzger]
Labels: java api, 
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Tue Mar 18 17:09:40 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397980,,,Mon Jun 09 12:36:17 UTC 2014,,,,,,,,,,"0|i1winj:",398107,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:36;github-import;[Date: Wed Mar 26 13:27:47 CET 2014, Author: [markus-h|https://github.com/markus-h]]

I'm on it.;;;","09/Jun/14 12:36;github-import;[Date: Fri Mar 28 00:50:42 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I have implemented a port of Spargel to the new java API. It does not use the new Java API classes for delta iterations, but rather directly instantiates the `eu.stratosphere.api.common` classes in the background.

It does however need the java api postpass to be able to handle delta iteration plan operators. Any chance you have gotten to that part already?;;;","09/Jun/14 12:36;github-import;[Date: Fri Mar 28 10:25:40 CET 2014, Author: [markus-h|https://github.com/markus-h]]

No, sorry, not yet. But it should be very similiar to BulkIterations with
termination criterion, so you can try to do a quick copy paste.


2014-03-28 0:50 GMT+01:00 Stephan Ewen <notifications@github.com>:

> I have implemented a port of Spargel to the new java API. It does not use
> the new Java API classes for delta iterations, but rather directly
> instantiates the eu.stratosphere.api.common classes in the background.
>
> It does however need the java api postpass to be able to handle delta
> iteration plan operators. Any chance you have gotten to that part already?
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/607#issuecomment-38875257>
> .
>;;;","09/Jun/14 12:36;github-import;[Date: Wed Apr 23 13:16:25 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

This issue can be closed.
See ([#643|https://github.com/stratosphere/stratosphere/issues/643] | [FLINK-643|https://issues.apache.org/jira/browse/FLINK-643]) merged in 74ce8d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Constant field annotations for tuples on new java api,FLINK-606,12719780,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:36,09/Jun/14 12:36,14/Jul/23 05:57,09/Jun/14 12:36,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/606
Created by: [rmetzger|https://github.com/rmetzger]
Labels: java api, 
Milestone: Release 0.5
Assignee: [skunert|https://github.com/skunert]
Created at: Tue Mar 18 17:01:48 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397979,,,Mon Jun 09 12:36:12 UTC 2014,,,,,,,,,,"0|i1winb:",398106,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:36;github-import;[Date: Mon Apr 28 18:25:07 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Provided by ([#677|https://github.com/stratosphere/stratosphere/issues/677] | [FLINK-677|https://issues.apache.org/jira/browse/FLINK-677]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spargel to new Java API,FLINK-604,12719778,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:36,09/Jun/14 12:36,14/Jul/23 05:57,09/Jun/14 12:36,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/604
Created by: [rmetzger|https://github.com/rmetzger]
Labels: java api, 
Milestone: Release 0.5
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Tue Mar 18 16:58:11 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397977,,,Mon Jun 09 12:36:06 UTC 2014,,,,,,,,,,"0|i1wimv:",398104,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:36;github-import;[Date: Sat Mar 22 11:54:08 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Depends on ([#607|https://github.com/stratosphere/stratosphere/issues/607] | [FLINK-607|https://issues.apache.org/jira/browse/FLINK-607]) ;;;","09/Jun/14 12:36;github-import;[Date: Tue Apr 08 13:59:32 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [253b4cdadacd71247aedf2d647aef34f5d4e0c13|https://github.com/stratosphere/stratosphere/commit/253b4cdadacd71247aedf2d647aef34f5d4e0c13];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collecting Output Format for new Java API (only for local executor),FLINK-602,12719776,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:35,09/Jun/14 12:36,14/Jul/23 05:57,09/Jun/14 12:36,,,,pre-apache,,,,,,,0,github-import,,"```java
List<String> resultsCollected = new ArrayList<String>();
result.output(resultsCollected);
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/602
Created by: [rmetzger|https://github.com/rmetzger]
Labels: java api, 
Milestone: Release 0.5
Assignee: [qmlmoon|https://github.com/qmlmoon]
Created at: Tue Mar 18 16:56:23 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397975,,,Mon Jun 09 12:36:00 UTC 2014,,,,,,,,,,"0|i1wimf:",398102,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:35;github-import;[Date: Thu Mar 20 22:45:18 CET 2014, Author: [qmlmoon|https://github.com/qmlmoon]]

When dop = 2, then we'll get 2 collection outputs. How to merge them? They have no idea about how the record fields are grouped. So, do we just simply concat them or leave them like  ``` List<List<String>> resultCollected ``` ?;;;","09/Jun/14 12:35;github-import;[Date: Thu Mar 20 22:51:13 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Good observation.
In most cases, you can simply concatenate them since the order of elements is not important.
If the user specifies sorted output, going for nested lists would be the best thing to do.

For now, I would go with the concatenated lists as it is easier to handle for the user.;;;","09/Jun/14 12:35;github-import;[Date: Fri Mar 21 21:19:03 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

This is related: https://github.com/stratosphere/stratosphere/issues/478 (for an implementation into the runtime);;;","09/Jun/14 12:36;github-import;[Date: Thu Apr 03 22:25:48 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Issue solved in [b032ff3afb10f9e0158e133a8f5c447ad89a90b6|https://github.com/stratosphere/stratosphere/commit/b032ff3afb10f9e0158e133a8f5c447ad89a90b6]
Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add JavaDocs to user-facing methods of new Java API,FLINK-601,12719775,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:35,09/Jun/14 12:35,14/Jul/23 05:57,09/Jun/14 12:35,,,,pre-apache,,,,,,,0,github-import,,"We need to add JavaDocs to the user-facing methods of the new Java API to describe the operators and guide a user through sequences of function calls as for example required for a join:

```
DataSet.join(DataSet).where(int...).equalTo(int...).with(UDF)
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/601
Created by: [fhueske|https://github.com/fhueske]
Labels: documentation, java api, user satisfaction, 
Milestone: Release 0.5
Created at: Tue Mar 18 14:09:50 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397974,,,Mon Jun 09 12:35:56 UTC 2014,,,,,,,,,,"0|i1wim7:",398101,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:35;github-import;[Date: Tue Mar 18 14:40:46 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

:+1: 
I would also like to publish them on our website.;;;","09/Jun/14 12:35;github-import;[Date: Mon Apr 07 06:31:03 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I pushed JavaDocs for DataSet and Grouping. Will continue with the Join classes.;;;","09/Jun/14 12:35;github-import;[Date: Fri May 16 15:25:00 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I looked over the JavaDocs of DataSet, Operators, and ExecutionEnvironment and completed them.
This issue should be resolved by that. Note there are still JavaDocs missing for the Function classes see ([#657|https://github.com/stratosphere/stratosphere/issues/657] | [FLINK-657|https://issues.apache.org/jira/browse/FLINK-657]).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add SetFields() Method to Tuples,FLINK-600,12719774,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:35,09/Jun/14 12:35,14/Jul/23 05:57,09/Jun/14 12:35,,,,pre-apache,,,,,,,0,github-import,,"Add a ```setFields()``` method to tuples to set all fields of a tuple in one method call:
```public void Tuple3.setFields(T1 value1, T2 value2, T3 value3)```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/600
Created by: [fhueske|https://github.com/fhueske]
Labels: java api, user satisfaction, 
Assignee: [twalthr|https://github.com/twalthr]
Created at: Tue Mar 18 09:18:28 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397973,,,Mon Jun 09 12:35:51 UTC 2014,,,,,,,,,,"0|i1wilz:",398100,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:35;github-import;[Date: Tue Mar 18 10:12:38 CET 2014, Author: [twalthr|https://github.com/twalthr]]

I will implement it.;;;","09/Jun/14 12:35;github-import;[Date: Tue Mar 18 10:40:12 CET 2014, Author: [fhueske|https://github.com/fhueske]]

You know that you only need to change TupleGenerator for that, right?;;;","09/Jun/14 12:35;github-import;[Date: Tue Mar 18 10:53:34 CET 2014, Author: [twalthr|https://github.com/twalthr]]

yes ;-);;;","09/Jun/14 12:35;github-import;[Date: Thu Mar 20 23:49:55 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed in [b6edba26c3443ebe33a3de4f482900fa424430f2|https://github.com/stratosphere/stratosphere/commit/b6edba26c3443ebe33a3de4f482900fa424430f2];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tuple field access indexes are off by one,FLINK-599,12719773,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:35,09/Jun/14 12:35,14/Jul/23 05:57,09/Jun/14 12:35,,,,pre-apache,,,,,,,0,github-import,,"Tuple fields start at 1 (```Tuple1._1```) but are accessed with indexes starting at 0:
```Tuple1.getField(0)``` and ```Tuple1.T1()``` return the same field ```Tuple1._1```.

I find this confusing as would make it consistent for example by starting the ```getField()``` at 1 as well.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/599
Created by: [fhueske|https://github.com/fhueske]
Labels: java api, user satisfaction, 
Milestone: Release 0.5
Assignee: [twalthr|https://github.com/twalthr]
Created at: Tue Mar 18 09:06:38 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397972,,,Mon Jun 09 12:35:46 UTC 2014,,,,,,,,,,"0|i1wilr:",398099,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:35;github-import;[Date: Tue Mar 18 09:10:35 CET 2014, Author: [fhueske|https://github.com/fhueske]]

The same applies for the key definitions, e.g., ```DataSet.groupBy(0)``` and ```JoinOperatorSets.where(0)``` (and so on) all refer to ```Tuple._1```;;;","09/Jun/14 12:35;github-import;[Date: Tue Mar 18 09:21:12 CET 2014, Author: [twalthr|https://github.com/twalthr]]

I also ran into this issue at the beginning. I would prefer to start all indexes at 0 as it is usual in computer science.;;;","09/Jun/14 12:35;github-import;[Date: Tue Mar 18 09:25:31 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I am fine either way as long as it is consistent :smirk:;;;","09/Jun/14 12:35;github-import;[Date: Tue Mar 18 13:10:17 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The tuples are inspired by the scala tuples. Those start at one. So the
choice is not easy...
Am 18.03.2014 09:25 schrieb ""Fabian Hueske"" <notifications@github.com>:

> I am fine either way as long as it is consistent [image: :smirk:]
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/599#issuecomment-37908785>
> .
>;;;","09/Jun/14 12:35;github-import;[Date: Tue Mar 18 13:17:33 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Right, but I think we should have a decision rather soon, i.e., before we announce / release the API. Otherwise, we might silently break existing stuff.;;;","09/Jun/14 12:35;github-import;[Date: Tue Mar 18 13:19:31 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I'm also in favor of zero-indexed tuples.
I would say Scala is inconsistent and we should not support that. The majority of our users are Java users, and not Scala users. ;;;","09/Jun/14 12:35;github-import;[Date: Tue Mar 18 16:55:43 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Rename it to ""v0, v1, ..."" ;;;","09/Jun/14 12:35;github-import;[Date: Thu Mar 20 10:50:37 CET 2014, Author: [twalthr|https://github.com/twalthr]]

Sorry, I need to ask again. I'm currently working in the TupleGenerator.
So v0,v1,.. was our final decision? I thought f0, f1,..
But _1,_2,.. should still exist for scala developers, right?;;;","09/Jun/14 12:35;github-import;[Date: Thu Mar 20 11:29:28 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I am in favor of replacing ```private T1 _1;``` by ```public T0 f0;``` and so on.
Also remove the getter ```public T1 T1()``` and setter ```public void T1(T1 value)```.

This is a Java API and we should not think too much about the 1% Scala developers out there. These guys are probably smart enough and know that other languages start counting at 0 :smirk:;;;","09/Jun/14 12:35;github-import;[Date: Thu Mar 20 23:50:06 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed in [b6edba26c3443ebe33a3de4f482900fa424430f2|https://github.com/stratosphere/stratosphere/commit/b6edba26c3443ebe33a3de4f482900fa424430f2];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extend the TupleGenerator for generating Tuple-dependent code in other classes,FLINK-597,12719771,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:35,09/Jun/14 12:35,14/Jul/23 05:57,09/Jun/14 12:35,,,,pre-apache,,,,,,,0,github-import,,"Some classes also depend on the number of tuples. 
E.g. the `types()` of `CsvReader` or `CLASSES` of `TupleTypeInfo`

@StephanEwen already suggested to extend the TupleGenerator. I would like to do this but I'm not sure how.

What do you think about adding a special comment to these classes so that the tuple generator knows where to put Tuple-dependent code, instead of generating the whole class?

    class CsvReader {
        // Constructor, methods etc.
        // ......
        
        // BEGIN_OF_TUPLE_DEPENDENT_CODE
        
        // END_OF_TUPLE_DEPENDENT_CODE
    }

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/597
Created by: [twalthr|https://github.com/twalthr]
Labels: java api, 
Assignee: [twalthr|https://github.com/twalthr]
Created at: Mon Mar 17 13:03:12 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397970,,,Mon Jun 09 12:35:35 UTC 2014,,,,,,,,,,"0|i1wilb:",398097,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:35;github-import;[Date: Tue Mar 18 15:05:39 CET 2014, Author: [alexff91|https://github.com/alexff91]]

Hi, maybe better to do this:
Check if there the same methods, if no add tuple-methods to end of class.
This will help if someone will forgot to add // BEGIN_OF_TUPLE_DEPENDENT_CODE at first to the class.;;;","09/Jun/14 12:35;github-import;[Date: Thu Mar 20 23:50:22 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed in [b6edba26c3443ebe33a3de4f482900fa424430f2|https://github.com/stratosphere/stratosphere/commit/b6edba26c3443ebe33a3de4f482900fa424430f2];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release 0.4,FLINK-595,12719769,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:35,09/Jun/14 12:35,14/Jul/23 05:57,09/Jun/14 12:35,,,,pre-apache,,,,,,,0,github-import,,"I am applying for the New Example Jobs / Development of a Stratosphere Standard Library with common operators.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/595
Created by: [wilsoncao|https://github.com/wilsoncao]
Labels: 
Created at: Sun Mar 16 15:13:51 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:35;github-import;pull-request-595-5363574778355904309.patch;https://issues.apache.org/jira/secure/attachment/12649223/pull-request-595-5363574778355904309.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397968,,,Mon Jun 09 12:35:15 UTC 2014,,,,,,,,,,"0|i1wikv:",398095,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:35;github-import;[Date: Sun Mar 16 15:36:15 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,

why did you create a pull request of a release branch to our main branch?

Pull requests exist to offer your changes to our code. For a successful GSoC application, we expect you to write code for our system.
The easiest way for you to give us the code you've written is a pull request.
More information:
http://oss-watch.ac.uk/resources/pullrequest
https://help.github.com/articles/using-pull-requests;;;","09/Jun/14 12:35;github-import;[Date: Sun Mar 16 16:32:45 CET 2014, Author: [wilsoncao|https://github.com/wilsoncao]]

Hi,


   I am sorry to make a mistake. And I am wondering whether I can pull request after the proposal deadline? Because recently I am busy with my TOEFL exam and maybe don't have much time to write the code.


Yours,
Wilson Cao


------------------ 原始邮件 ------------------
发件人: ""Robert Metzger"";<notifications@github.com>;
发送时间: 2014年3月16日(星期天) 晚上10:36
收件人: ""stratosphere/stratosphere""<stratosphere@noreply.github.com>; 
抄送: ""275239608""<275239608@qq.com>; 
主题: Re: [stratosphere] Release 0.4 (([#595|https://github.com/stratosphere/stratosphere/issues/595] | [FLINK-595|https://issues.apache.org/jira/browse/FLINK-595]))




Hi,
 
why did you create a pull request of a release branch to our main branch?
 
Pull requests exist to offer your changes to our code. For a successful GSoC application, we expect you to write code for our system.
 The easiest way for you to give us the code you've written is a pull request.
 More information:
http://oss-watch.ac.uk/resources/pullrequest
https://help.github.com/articles/using-pull-requests
 
—
Reply to this email directly or view it on GitHub.;;;","09/Jun/14 12:35;github-import;[Date: Sun Mar 16 17:25:03 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I am closing this one as a confused pull request.;;;","09/Jun/14 12:35;github-import;[Date: Mon Mar 17 11:12:35 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

@wilsoncao: Regarding your question: As Fabian said on the mailing list, we can not make any exceptions to this deadline. Google will not accept late proposals for any reason and we want to start reviewing the applications once the deadline is over.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release 0.4 - For GSOC Project Add Web-based visualization of tasks in execution,FLINK-594,12719768,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:35,09/Jun/14 12:35,14/Jul/23 05:57,09/Jun/14 12:35,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/594
Created by: [liutuo|https://github.com/liutuo]
Labels: 
Created at: Sun Mar 16 11:35:46 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:35;github-import;pull-request-594-6219355868234141973.patch;https://issues.apache.org/jira/secure/attachment/12649222/pull-request-594-6219355868234141973.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397967,,,2014-06-09 12:35:00.0,,,,,,,,,,"0|i1wikn:",398094,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added unit-tests for utils classes,FLINK-593,12719767,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:34,09/Jun/14 12:34,14/Jul/23 05:57,09/Jun/14 12:34,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/593
Created by: [alexff91|https://github.com/alexff91]
Labels: 
Created at: Sun Mar 16 11:10:40 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:34;github-import;pull-request-593-1395000439795134914.patch;https://issues.apache.org/jira/secure/attachment/12649221/pull-request-593-1395000439795134914.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397966,,,Mon Jun 09 12:34:57 UTC 2014,,,,,,,,,,"0|i1wikf:",398093,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:34;github-import;[Date: Sun Mar 16 11:21:24 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,

thank you for your pull request!
I think we don't have an issue for this pull request, right?
Is there a particular reason why you added a test for the `AsciiUtils` and the `InstantiationUtil` ? Did you see any bugs related to the two tools?

Can you make a single commit out of your pull request?
I think you can do a 
```
git rebase -i HEAD~3
```
and then squash or fixup the commits together into one.
You can update a pending pull request by (force) pushing into it.
;;;","09/Jun/14 12:34;github-import;[Date: Sun Mar 16 20:06:49 CET 2014, Author: [alexff91|https://github.com/alexff91]]

Hello! I want to participate in GSOC-14(https://groups.google.com/forum/#!topic/stratosphere-dev/tD8pOje3IWg), so I started my small contribution from tests. Test related to https://github.com/stratosphere/stratosphere/issues/589, or tests needed for another Utils JAPI?
I used eclemma-plugin in Eclipse for showing how many bunches are covered in classes, and there are a lot of brunches that don't have test and because of it I started from them. I will make this magic with joining all commits into one.;;;","09/Jun/14 12:34;github-import;[Date: Sun Mar 16 20:27:44 CET 2014, Author: [alexff91|https://github.com/alexff91]]

I rebased,squashed commits and then pushed? Everything right?;;;","09/Jun/14 12:34;github-import;[Date: Sun Mar 16 20:45:55 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi, sorry, no. I would like your pull request to be one commit only! (The reason is not to torture you, but we want to have a somewhat nice commit history)
;;;","09/Jun/14 12:34;github-import;[Date: Sun Mar 16 21:33:29 CET 2014, Author: [alexff91|https://github.com/alexff91]]

I made it! with rebase was a problem because it always left 2 commits, so I reseted to the first commit and used amend  to add all files.;;;","09/Jun/14 12:34;github-import;[Date: Mon Mar 17 07:50:34 CET 2014, Author: [alexff91|https://github.com/alexff91]]

Strange, 
Travis log says:
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.008 sec - in eu.stratosphere.util.InstantiationUtilsTest
Running eu.stratosphere.util.AsciiUtilsTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec - in eu.stratosphere.util.AsciiUtilsTest
Running eu.stratosphere.util.NumberSequenceIteratorTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec - in eu.stratosphere.util.NumberSequenceIteratorTest


And it ends with message:

I'm sorry but your test run exceeded 50.0 minutes. 
One possible solution is to split up your test run.;;;","09/Jun/14 12:34;github-import;[Date: Mon Mar 17 11:00:40 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Travis stops builds that exceed 50 minutes. 
We have quite a lot of dependencies (which need to be downloaded for every build) and many unit and integration tests. 
Unfortunately, this leads to timed out builds from time to time :worried: 

So this does not indicate that there is something wrong with your PR. It is ""just"" a problem with our build system and Travis...;;;","09/Jun/14 12:34;github-import;[Date: Mon Mar 17 13:13:34 CET 2014, Author: [alexff91|https://github.com/alexff91]]

Got it, thanks for the answer!;;;","09/Jun/14 12:34;github-import;[Date: Mon Mar 17 21:22:17 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I think the pull request is good to merge.;;;","09/Jun/14 12:34;github-import;[Date: Wed Mar 19 19:58:49 CET 2014, Author: [alexff91|https://github.com/alexff91]]

I have added another test and refactored some classes(added {} in if cases).;;;","09/Jun/14 12:34;github-import;[Date: Sun Mar 23 13:17:03 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The tests are good. But, can you undo the code re-formatting? It blows up the diffs to a point where it becomes hard to spot where real changes are, and makes us overlook parts that have actually an implication.

Have a look at the general coding guidelines here:https://github.com/stratosphere/stratosphere/wiki/CodingGuidelines;;;","09/Jun/14 12:34;github-import;[Date: Sun Mar 23 15:53:53 CET 2014, Author: [alexff91|https://github.com/alexff91]]

Ok, I will undo reformatting. I've used cntrl+shift+f in eclipse, I suggest
it was a bad idea?
23.03.2014 16:17 ÐÏÌØÚÏ×ÁÔÅÌØ ""Stephan Ewen"" <notifications@github.com>
ÎÁÐÉÓÁÌ:

> The tests are good. But, can you undo the code re-formatting? It blows up
> the diffs to a point where it becomes hard to spot where real changes are,
> and makes us overlook parts that have actually an implication.
>
> Have a look at the general coding guidelines here:
> https://github.com/stratosphere/stratosphere/wiki/CodingGuidelines
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/593#issuecomment-38380816>
> .
>;;;","09/Jun/14 12:34;github-import;[Date: Sun Mar 23 21:14:22 CET 2014, Author: [alexff91|https://github.com/alexff91]]

Done, I leaved only tests.;;;","09/Jun/14 12:34;github-import;[Date: Tue Mar 25 21:45:30 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Nice, thanks. Testing and merging it...;;;","09/Jun/14 12:34;github-import;[Date: Wed Mar 26 00:53:20 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [9fb786da0ce2316a01363fb891afe6f386107797|https://github.com/stratosphere/stratosphere/commit/9fb786da0ce2316a01363fb891afe6f386107797];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[yarn] set the same user for the services running in the YARN containers,FLINK-591,12719765,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:34,09/Jun/14 12:34,14/Jul/23 05:57,09/Jun/14 12:34,,,,pre-apache,,,,,,,0,github-import,,"Fix ([#584|https://github.com/stratosphere/stratosphere/issues/584] | [FLINK-584|https://issues.apache.org/jira/browse/FLINK-584])
Fix ([#568|https://github.com/stratosphere/stratosphere/issues/568] | [FLINK-568|https://issues.apache.org/jira/browse/FLINK-568])

I'm happy for your comments!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/591
Created by: [rmetzger|https://github.com/rmetzger]
Labels: YARN, 
Milestone: Release 0.5
Created at: Sun Mar 16 11:03:32 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:34;github-import;pull-request-591-8884425146250918433.patch;https://issues.apache.org/jira/secure/attachment/12649220/pull-request-591-8884425146250918433.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397964,,,Mon Jun 09 12:34:41 UTC 2014,,,,,,,,,,"0|i1wijz:",398091,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:34;github-import;[Date: Sun Mar 16 14:47:21 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Looks good to me.;;;","09/Jun/14 12:34;github-import;[Date: Mon Mar 17 09:33:57 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged into master.
Rebasing `staging` ontop of the current master (including the YARN fix) leads to only one conflict that is super easy to fix (delete vs modified in swt-visualization).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop compat tests and parquet (proposed for GSoC),FLINK-590,12719764,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:34,09/Jun/14 12:34,14/Jul/23 05:57,09/Jun/14 12:34,,,,pre-apache,,,,,,,0,github-import,,"Some comments on this here: https://groups.google.com/d/msg/stratosphere-dev/qYvJRSoMYWQ/RfkVGBvP2zEJ

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/590
Created by: [atsikiridis|https://github.com/atsikiridis]
Labels: 
Created at: Sun Mar 16 05:14:58 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:34;github-import;pull-request-590-5763778864690790275.patch;https://issues.apache.org/jira/secure/attachment/12649219/pull-request-590-5763778864690790275.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397963,,,Mon Jun 09 12:34:36 UTC 2014,,,,,,,,,,"0|i1wijr:",398090,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:34;github-import;[Date: Tue Apr 08 12:44:28 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

@rmetzger Is this pull request good to merge?;;;","09/Jun/14 12:34;github-import;[Date: Tue May 06 12:52:40 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I will move this code to the stratosphere-experimental repository, which is the place for additional module contributions. The parquet code is there already.

For the discussion on the mailing list, see here: https://groups.google.com/forum/#!topic/stratosphere-dev/J89Jco164HQ;;;","09/Jun/14 12:34;github-import;[Date: Tue May 06 18:07:59 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Moved to stratosphere-examples in (https://github.com/stratosphere/stratosphere-experimental/commit/9ae1210235002ecb04ff0c24ca2cc3c9bc59da05)

I had to manually move files, so the commit diffs are a bit screwed.

Thanks for the patch!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Tests for the basic type utils,FLINK-589,12719763,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:34,09/Jun/14 12:34,14/Jul/23 05:57,09/Jun/14 12:34,,,,pre-apache,,,,,,,0,github-import,,"We need thorough test for the basic type utils, as they are a core part of the API type system.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/589
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: java api, testing, 
Created at: Sun Mar 16 01:28:27 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397962,,,Mon Jun 09 12:34:28 UTC 2014,,,,,,,,,,"0|i1wijj:",398089,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:34;github-import;[Date: Thu Apr 10 10:33:01 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

This issue refers to the untested serializers and comparators in the `stratosphere-core` module in package `eu.stratosphere.api.common.typeutils.base` for the basic data types:

* Boolean
* Byte
* Char
* Double
* Float
* Int
* Long
* Short
* String

The *ValueSerializer classes are for the data types of our old Java API which will be depreciated in the future. 

We need thorough unit-tests for all of these classes because they will be heavily used. The tests should be put in the test directory of the `stratosphere-core` module in the same package as the tested classes. 

I would start with tests for the *Serializer classes. and continue with the *Comparator classes.
;;;","09/Jun/14 12:34;github-import;[Date: Thu Apr 10 11:05:58 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think we should retain the value types of mutable versions of javas
primitives.;;;","09/Jun/14 12:34;github-import;[Date: Thu Apr 10 11:07:15 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Have a look at the string array serializer tests. It uses a serializer test
util that makes it easy to add serializer tests.;;;","09/Jun/14 12:34;github-import;[Date: Thu Apr 10 15:43:29 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

stratosphere-java/src/main/java/eu/stratosphere/api/java/typeutils/runtime/StringArraySerializer.java
and the associated test.;;;","09/Jun/14 12:34;github-import;[Date: Thu Apr 17 06:02:24 CEST 2014, Author: [aaronchlam|https://github.com/aaronchlam]]

I will be working on *Serializer tests in this issue and then move on to do the *Comparator if someone else does not get to it first.;;;","09/Jun/14 12:34;github-import;[Date: Fri May 02 15:00:53 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

With  ([#745|https://github.com/stratosphere/stratosphere/issues/745] | [FLINK-745|https://issues.apache.org/jira/browse/FLINK-745]), the serializers for all nine primitive types are covered with unit tests.
Thanks @aaronchlam !

Tests for comparators are addressed by issue ([#744|https://github.com/stratosphere/stratosphere/issues/744] | [FLINK-744|https://issues.apache.org/jira/browse/FLINK-744]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Descending group sorting does not work.,FLINK-588,12719762,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:34,09/Jun/14 12:34,14/Jul/23 05:57,09/Jun/14 12:34,,,,pre-apache,,,,,,,0,github-import,,"The flag that describes whether comparisons should result in an ascending or descending order is not used by any of the basic type comparators.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/588
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: bug, java api, 
Milestone: Release 0.5
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Sun Mar 16 01:27:40 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397961,,,Mon Jun 09 12:34:22 UTC 2014,,,,,,,,,,"0|i1wijb:",398088,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:34;github-import;[Date: Mon Mar 17 12:39:58 CET 2014, Author: [twalthr|https://github.com/twalthr]]

You mean something like that in every Comparator, right?

<code>
@Override
	public int compare(DataInputView firstSource, DataInputView secondSource) throws IOException {
		double l1 = firstSource.readDouble();
		double l2 = secondSource.readDouble();
		int comp = (l1 < l2 ? -1 : (l1 > l2 ? 1 : 0));
		return ascendingComparison ? comp : -comp;
	}
</code>;;;","09/Jun/14 12:34;github-import;[Date: Fri Mar 21 18:08:44 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Exactly;;;","09/Jun/14 12:34;github-import;[Date: Mon Apr 14 16:28:18 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

Is this fixed? I am working on testcases for groupReduce and the descending order seems to be not working. I think this is connected to this issue.;;;","09/Jun/14 12:34;github-import;[Date: Mon Apr 14 16:43:57 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

No, I don't think this is fixed. I have the same issue with a deactivated test in `ReduceITCase`.;;;","09/Jun/14 12:34;github-import;[Date: Mon Apr 14 16:59:31 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

I will do it, tomorrow. ok?
;;;","09/Jun/14 12:34;github-import;[Date: Mon Apr 14 21:35:35 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

:+1:;;;","09/Jun/14 12:34;github-import;[Date: Tue Apr 15 10:27:49 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I had to remove the GroupSorting test in the `ReduceITCase` because we do no longer support group sorting for the pair-wise `ReduceFunction`.
However, we should check if this solves the problem with GroupSorting (Ascending and Descending) in the `GroupReduceITCase`.;;;","09/Jun/14 12:34;github-import;[Date: Tue Apr 15 10:42:45 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

@twalthr and me are working on it. The simple change mentioned above seems not to work.;;;","09/Jun/14 12:34;github-import;[Date: Tue Apr 15 10:49:06 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Great! Thanks! :smile:;;;","09/Jun/14 12:34;github-import;[Date: Tue Apr 15 11:34:04 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

Ok, here are my investigations:
What I do is a ``groupBy(1).sortGroup(0,Order.DESCENDING)``. In the compiler post pass the localStrategyComparator of the channel to the reduce is correctly set to [0] LongComparator(asc) and [1] IntComparator(desc). Both comparators are instantiated, but the second one is never used. I set a breakpoint inside the compare() method and it is never called.

So I had a look at the ReduceDriver, more specifically to the KeyGroupedIterator and its ValueIterator and I don't see how there should be support for sorted values inside of a group.

Is there runtime support for sorted groups in the reduce?;;;","09/Jun/14 12:34;github-import;[Date: Tue Apr 15 11:48:11 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

The `BaseTypeComparator`s are bundled in a `TupleComparator` which uses them internally. Depending on the types that the `TupleComparator` needs to check, it might use normalized keys. Whether normalized keys can be used (for a prefix of the sorting key) is checked in the constructor of the `TupleComparator`. For this part, the comparisons are done on binary data and `compare()` will not be called. Since normalized keys do not work for descending sort (AFAIK), `compare()` should be called for attribute `0` if the comparison of attribute `1` returns 0 (equality).

I suspect that there is something not working correctly in the `TupleComparator`.;;;","09/Jun/14 12:34;github-import;[Date: Wed Apr 16 23:42:06 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

I merged ([#693|https://github.com/stratosphere/stratosphere/issues/693] | [FLINK-693|https://issues.apache.org/jira/browse/FLINK-693]) which fixes the `compare()` methods of the base type comparators. The comparators need to be checked by tests (([#589|https://github.com/stratosphere/stratosphere/issues/589] | [FLINK-589|https://issues.apache.org/jira/browse/FLINK-589])).

However, ([#693|https://github.com/stratosphere/stratosphere/issues/693] | [FLINK-693|https://issues.apache.org/jira/browse/FLINK-693]) does not (fully) solve this issue.
;;;","09/Jun/14 12:34;github-import;[Date: Mon Apr 28 15:57:29 CEST 2014, Author: [filiphaase|https://github.com/filiphaase]]

Hey I investigated this problem a little bit today. And as I see it the Ascending sort is also not working properly. If I change this:
```java
//data.add(new Tuple3<Integer, Long, String>(16,6l,""Comment([#10|https://github.com/stratosphere/stratosphere/issues/10] | [FLINK-10|https://issues.apache.org/jira/browse/FLINK-10])""));
data.add(new Tuple3<Integer, Long, String>(22,6l,""Comment([#10|https://github.com/stratosphere/stratosphere/issues/10] | [FLINK-10|https://issues.apache.org/jira/browse/FLINK-10])""));
```
in the CollectionDataSets, testcase 3 of  GroupReduceITCase(which is testing sort-ascending) also breaks, while the result should be independent from the first value of the tuple.

I also found the source of the bug: I think the Comparators are reading data from wrong positions in the memory. I looked into the LongComparator compare() methods and the long values which they read have wrong values. But if I add this code both cases work:

```java
	@Override
	public int compare(DataInputView firstSource, DataInputView secondSource) throws IOException {
		firstSource.skipBytes(4);
		secondSource.skipBytes(4);
		long l1 = firstSource.readLong();
		long l2 = secondSource.readLong();
		int comp = (l1 < l2 ? -1 : (l1 == l2 ? 0 : 1)); 
		return ascendingComparison ? comp : -comp;
	}
```
So I assume the offsets are not set properly, because the 4 bytes I skipped here are the 4 bytes of the first int value in the tuples.

But so far I don't know how to fix this.  I thought one could adjust that in the TupleComparator.compare() method, but I did not find a way to get memory-offsets for the different value positions.;;;","09/Jun/14 12:34;github-import;[Date: Thu May 15 23:47:27 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

@zentol @StephanEwen I believe this issue has been fix, right?;;;","09/Jun/14 12:34;github-import;[Date: Fri May 16 00:01:01 CEST 2014, Author: [zentol|https://github.com/zentol]]

i think so. The deactivated test in GroupReduceITCase (13 i think...) succeeded when i tried it. I also created two simple tests that check whether ascending/descending flag is passed from a plan to the comparators, both work.;;;","09/Jun/14 12:34;github-import;[Date: Fri May 16 00:11:51 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Great! @StephanEwen activated the last two tests in `GroupReduceITCase`.
I'm closing this issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a list of all dependencies to the NOTICE or LICENSE file,FLINK-587,12719761,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:34,29/Aug/14 06:52,14/Jul/23 05:57,29/Aug/14 06:52,,,,pre-apache,,,,,,,0,github-import,,"According to @sscdotopen , we need to add a list of all dependencies that we use. That includes other Apache licensed dependencies. 

We seem to be able to skip transitive dependencies, though.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/587
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Sat Mar 15 23:52:20 CET 2014
State: open
",,fhueske,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397960,,,Fri Aug 29 06:52:32 UTC 2014,,,,,,,,,,"0|i1wij3:",398087,,,,,,,,,,,,,,,,,,,,"29/Aug/14 06:52;fhueske;Has been fixed as part of the first Apache incubator release (v0.6);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Suggested Name change for the WriteMode,FLINK-586,12719759,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:34,09/Jun/14 12:34,14/Jul/23 05:57,09/Jun/14 12:34,,,,pre-apache,,,,,,,0,github-import,,"Got as feedback that CREATE as default write mode does not convey an intuitive meaning. Here is a suggested change of names.

CREATE --> NO_OVERWRITE
OVERWRITE FORCE_OVERWRITE

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/586
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: user satisfaction, 
Created at: Sat Mar 15 20:48:07 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:34;github-import;pull-request-586-5327756018379431124.patch;https://issues.apache.org/jira/secure/attachment/12649218/pull-request-586-5327756018379431124.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397958,,,Mon Jun 09 12:34:08 UTC 2014,,,,,,,,,,"0|i1wiin:",398085,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:34;github-import;[Date: Sun Mar 16 13:21:04 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I agree that CREATE is not an intuitive naming and should be changed to NO_OVERWRITE.

But why do you want to change OVERWRITE to FORCE_OVERWRITE?
I think OVERWRITE is shorter and has a smaller chance of misinterpretation (What does FORCE mean? Is there OVERWRITE without FORCE?).;;;","09/Jun/14 12:34;github-import;[Date: Sun Mar 23 12:36:17 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Yes, I see the point. The prefix force came from the unix tools, which often use `-f`or `--force` to overwrite. So I guess the options are 

I will change it to `NO_OVERWRITE` & `OVERWRITE`. ;;;","09/Jun/14 12:34;github-import;[Date: Sun Mar 23 15:17:41 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Adjusted and merged in [a65ee8f600d16596f6193af444265de515ec80c4|https://github.com/stratosphere/stratosphere/commit/a65ee8f600d16596f6193af444265de515ec80c4];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add CsvOutputFormat for new Java API,FLINK-585,12719758,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:33,09/Jun/14 12:34,14/Jul/23 05:57,09/Jun/14 12:34,,,,pre-apache,,,,,,,0,github-import,,"Also add some small fixes for TypeExtractor.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/585
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Sat Mar 15 18:23:22 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:34;github-import;pull-request-585-1269797806174659569.patch;https://issues.apache.org/jira/secure/attachment/12649217/pull-request-585-1269797806174659569.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397957,,,Mon Jun 09 12:34:02 UTC 2014,,,,,,,,,,"0|i1wiif:",398084,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:34;github-import;[Date: Sat Mar 15 21:11:23 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged into staging in [15bce4f6a52a86fa4507f8c441b6359a8add75d5|https://github.com/stratosphere/stratosphere/commit/15bce4f6a52a86fa4507f8c441b6359a8add75d5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HDFS access with YARN fails reproducible, but only on the first access",FLINK-584,12719757,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:33,09/Jun/14 12:33,14/Jul/23 05:57,09/Jun/14 12:33,,,,pre-apache,,,,,,,0,github-import,,"I'm sill working on https://github.com/stratosphere/stratosphere/issues/568 and there is a weird issue.

Once the YARN session came up, the system is unable to open the file input splits (dop=16, tm = 1, JM/TM on different machines).

The exception is thrown by the `InputSplitOpenThread`. The line in the FileSytem code is nothing special. I don't see any reason to timeout there.

```
03/15/2014 13:17:55:	Job execution switched to status FAILED
ERROR: The program execution failed: java.io.IOException: Error opening the Input Split hdfs://master.local:9000/input [26364,2197]: Input opening request timed out. Opener was  alive. Stack:
	at eu.stratosphere.core.fs.FileSystem.get(FileSystem.java:197)
	at eu.stratosphere.api.common.io.FileInputFormat$InputSplitOpenThread.run(FileInputFormat.java:687)

	at eu.stratosphere.api.common.io.FileInputFormat.open(FileInputFormat.java:550)
	at eu.stratosphere.api.common.io.DelimitedInputFormat.open(DelimitedInputFormat.java:415)
	at eu.stratosphere.api.common.io.DelimitedInputFormat.open(DelimitedInputFormat.java:44)
	at eu.stratosphere.pact.runtime.task.DataSourceTask.invoke(DataSourceTask.java:134)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:345)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.io.IOException: Input opening request timed out. Opener was  alive. Stack:
	at eu.stratosphere.core.fs.FileSystem.get(FileSystem.java:197)
	at eu.stratosphere.api.common.io.FileInputFormat$InputSplitOpenThread.run(FileInputFormat.java:687)

	at eu.stratosphere.api.common.io.FileInputFormat$InputSplitOpenThread.waitForCompletion(FileInputFormat.java:738)
	at eu.stratosphere.api.common.io.FileInputFormat.open(FileInputFormat.java:547)
	... 5 more
```

Any ideas? 



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/584
Created by: [rmetzger|https://github.com/rmetzger]
Labels: question, YARN, 
Created at: Sat Mar 15 14:38:21 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397956,,,Mon Jun 09 12:33:57 UTC 2014,,,,,,,,,,"0|i1wii7:",398083,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:33;github-import;[Date: Sat Mar 15 15:44:16 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Normally, the timeout is not set (or set to inifinite) in the stratosphere
configuration (see reference documentation on the homepage). Do you use a
configuration where you manually set that value?


On Sat, Mar 15, 2014 at 2:38 PM, Robert Metzger <notifications@github.com>wrote:

> I'm sill working on ([#568|https://github.com/stratosphere/stratosphere/issues/568] | [FLINK-568|https://issues.apache.org/jira/browse/FLINK-568])<https://github.com/stratosphere/stratosphere/issues/568>and there is a weird issue.
>
> Once the YARN session came up, the system is unable to open the file input
> splits (dop=16, tm = 1, JM/TM on different machines).
>
> The exception is thrown by the InputSplitOpenThread. The line in the
> FileSytem code is nothing special. I don't see any reason to timeout there.
>
> 03/15/2014 13:17:55:    Job execution switched to status FAILED
> ERROR: The program execution failed: java.io.IOException: Error opening the Input Split hdfs://master.local:9000/input [26364,2197]: Input opening request timed out. Opener was  alive. Stack:
>     at eu.stratosphere.core.fs.FileSystem.get(FileSystem.java:197)
>     at eu.stratosphere.api.common.io.FileInputFormat$InputSplitOpenThread.run(FileInputFormat.java:687)
>
>     at eu.stratosphere.api.common.io.FileInputFormat.open(FileInputFormat.java:550)
>     at eu.stratosphere.api.common.io.DelimitedInputFormat.open(DelimitedInputFormat.java:415)
>     at eu.stratosphere.api.common.io.DelimitedInputFormat.open(DelimitedInputFormat.java:44)
>     at eu.stratosphere.pact.runtime.task.DataSourceTask.invoke(DataSourceTask.java:134)
>     at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:345)
>     at java.lang.Thread.run(Thread.java:701)
> Caused by: java.io.IOException: Input opening request timed out. Opener was  alive. Stack:
>     at eu.stratosphere.core.fs.FileSystem.get(FileSystem.java:197)
>     at eu.stratosphere.api.common.io.FileInputFormat$InputSplitOpenThread.run(FileInputFormat.java:687)
>
>     at eu.stratosphere.api.common.io.FileInputFormat$InputSplitOpenThread.waitForCompletion(FileInputFormat.java:738)
>     at eu.stratosphere.api.common.io.FileInputFormat.open(FileInputFormat.java:547)
>     ... 5 more
>
> Any ideas?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/584>
> .
>;;;","09/Jun/14 12:33;github-import;[Date: Sat Mar 15 15:50:44 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

The timeout mechanism is not really working.
I have to do some more debugging, but the value is set to Long.MAX_VALUE.

The problem is that the timeout is evaluated in this statement
`remaining = this.timeout + start - System.currentTimeMillis()) > 0`
If this.timeout == Long.MAX_VALUE, we'll have a overflow here (start is always > 1).


I'm getting different exceptions: (and the error does not always occur ofter I started the YARN session. Only sometimes.)
I suspect one thing: I have a vagrant setup with 4 virtual machines. The hostname resolution works via zeroconf (avahi). If I access the virtual hosts from the physical host, the resolution sometimes takes more than 5 seconds. This might be the reason for the error.
But I'll certainly look into the InputSplitOpenThread class to check that the timeout is working correctly.

```
java.io.IOException: Error opening the Input Split hdfs://master.local:9000/input [10985,2197]: Input opening request timed out. Opener was  alive. Stack:
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:290)
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:286)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:286)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:763)
        at eu.stratosphere.runtime.fs.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:342)
        at eu.stratosphere.api.common.io.FileInputFormat$InputSplitOpenThread.run(FileInputFormat.java:688)

        at eu.stratosphere.api.common.io.FileInputFormat.open(FileInputFormat.java:550)
        at eu.stratosphere.api.common.io.DelimitedInputFormat.open(DelimitedInputFormat.java:415)
        at eu.stratosphere.api.common.io.DelimitedInputFormat.open(DelimitedInputFormat.java:44)
        at eu.stratosphere.pact.runtime.task.DataSourceTask.invoke(DataSourceTask.java:134)
        at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:345)
        at java.lang.Thread.run(Thread.java:701)
Caused by: java.io.IOException: Input opening request timed out. Opener was  alive. Stack:
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:290)
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:286)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:286)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:763)
        at eu.stratosphere.runtime.fs.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:342)
        at eu.stratosphere.api.common.io.FileInputFormat$InputSplitOpenThread.run(FileInputFormat.java:688)

        at eu.stratosphere.api.common.io.FileInputFormat$InputSplitOpenThread.waitForCompletion(FileInputFormat.java:738)
        at eu.stratosphere.api.common.io.FileInputFormat.open(FileInputFormat.java:547)
        ... 5 more
```;;;","09/Jun/14 12:33;github-import;[Date: Sat Mar 15 17:03:51 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Damn. The DelimitedInputFormat.getStatistics() method is setting the timeout to 10000 without setting it back to its ""old"" value.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Error,FLINK-583,12719756,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:33,09/Jun/14 12:33,14/Jul/23 05:57,09/Jun/14 12:33,,,,pre-apache,,,,,,,0,github-import,,"SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Your Job's Name 0.1
[INFO] ------------------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 0.363s
[INFO] Finished at: Sat Mar 15 07:23:05 WAT 2014
[INFO] Final Memory: 4M/111M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to parse plugin descriptor for org.apache.maven.plugins:maven-resources-plugin:2.5 (/home/theking/.m2/repository/org/apache/maven/plugins/maven-resources-plugin/2.5/maven-resources-plugin-2.5.jar): error in opening zip file -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginDescriptorParsingException


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/583
Created by: [Harvey-Theking|https://github.com/Harvey-Theking]
Labels: question, 
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Sat Mar 15 07:34:23 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397955,,,Mon Jun 09 12:33:52 UTC 2014,,,,,,,,,,"0|i1wihz:",398082,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:33;github-import;[Date: Sat Mar 15 09:18:20 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,
thank you for reporting the issue. I assume you were using the java quickstart.;;;","09/Jun/14 12:33;github-import;[Date: Sat Mar 15 09:21:25 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

The 0.4 java quickstart is working for me.
The `maven-resources-plugin-2.5.jar` file is probably corrupt (aborted download, transfer error etc.).
Delete the file using 
```
rm /home/theking/.m2/repository/org/apache/maven/plugins/maven-resources-plugin/2.5/maven-resources-plugin-2.5.jar
```
Maven will download the dependency again.
I hope this solves your problem.
```
robert@robert-tower /tmp % curl https://raw.github.com/stratosphere/stratosphere-quickstart/master/quickstart.sh | bash
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1279  100  1279    0     0   2536      0 --:--:-- --:--:-- --:--:--  2532
[INFO] Scanning for projects...
[INFO] 
[INFO] Using the builder org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder with a thread count of 1
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Maven Stub Project (No POM) 1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] >>> maven-archetype-plugin:2.2:generate (default-cli) @ standalone-pom >>>
[INFO] 
[INFO] <<< maven-archetype-plugin:2.2:generate (default-cli) @ standalone-pom <<<
[INFO] 
[INFO] --- maven-archetype-plugin:2.2:generate (default-cli) @ standalone-pom ---
[INFO] Generating project in Batch mode
[INFO] Archetype repository missing. Using the one from [eu.stratosphere:quickstart-java:0.4] found in catalog remote
[INFO] ----------------------------------------------------------------------------
[INFO] Using following parameters for creating project from Old (1.x) Archetype: quickstart-java:0.4
[INFO] ----------------------------------------------------------------------------
[INFO] Parameter: groupId, Value: eu.stratosphere
[INFO] Parameter: packageName, Value: eu.stratosphere.quickstart
[INFO] Parameter: package, Value: eu.stratosphere.quickstart
[INFO] Parameter: artifactId, Value: quickstart
[INFO] Parameter: basedir, Value: /tmp
[INFO] Parameter: version, Value: 0.1
[INFO] project created from Old (1.x) Archetype in dir: /tmp/quickstart
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3.559 s
[INFO] Finished at: 2014-03-15T09:18:50+01:00
[INFO] Final Memory: 13M/239M
[INFO] ------------------------------------------------------------------------



	A sample quickstart Stratosphere Job has been created.
	Switch into the directory using
		 cd quickstart
	Import the project there using your favorite IDE (Import it as a maven project)
	Build a jar inside the directory using
		 mvn clean package
	You will find the runnable jar in quickstart/target
	Consult our mailing list if you have any troubles: https://groups.google.com/forum/#!forum/stratosphere-dev



robert@robert-tower /tmp % cd quickstart
robert@robert-tower /tmp/quickstart % ls
pom.xml  src/
robert@robert-tower /tmp/quickstart % mvn clean package
[INFO] Scanning for projects...
[INFO] 
[INFO] Using the builder org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder with a thread count of 1
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Your Job's Name 0.1
[INFO] ------------------------------------------------------------------------
Downloading: https://oss.sonatype.org/content/repositories/snapshots/commons-logging/commons-logging/maven-metadata.xml
Downloading: http://repo.maven.apache.org/maven2/commons-logging/commons-logging/maven-metadata.xml
Downloaded: http://repo.maven.apache.org/maven2/commons-logging/commons-logging/maven-metadata.xml (602 B at 1.9 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpclient/maven-metadata.xml
Downloading: https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpclient/maven-metadata.xml
Downloaded: http://repo.maven.apache.org/maven2/org/apache/httpcomponents/httpclient/maven-metadata.xml (2 KB at 9.4 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/org/codehaus/jackson/jackson-core-asl/maven-metadata.xml
Downloading: https://oss.sonatype.org/content/repositories/snapshots/org/codehaus/jackson/jackson-core-asl/maven-metadata.xml
Downloaded: http://repo.maven.apache.org/maven2/org/codehaus/jackson/jackson-core-asl/maven-metadata.xml (3 KB at 18.4 KB/sec)
Downloading: http://repo.maven.apache.org/maven2/javax/mail/mail/maven-metadata.xml
Downloading: https://oss.sonatype.org/content/repositories/snapshots/javax/mail/mail/maven-metadata.xml
Downloaded: http://repo.maven.apache.org/maven2/javax/mail/mail/maven-metadata.xml (867 B at 5.9 KB/sec)
[INFO]      
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ quickstart ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ quickstart ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /tmp/quickstart/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ quickstart ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 2 source files to /tmp/quickstart/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ quickstart ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /tmp/quickstart/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ quickstart ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ quickstart ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ quickstart ---
[INFO] Building jar: /tmp/quickstart/target/quickstart-0.1.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 8.006 s
[INFO] Finished at: 2014-03-15T09:19:09+01:00
[INFO] Final Memory: 21M/239M
[INFO] ------------------------------------------------------------------------
mvn clean package  7.40s user 0.24s system 82% cpu 9.224 total
robert@robert-tower /tmp/quickstart %        
```;;;","09/Jun/14 12:33;github-import;[Date: Tue Mar 18 17:42:48 CET 2014, Author: [Harvey-Theking|https://github.com/Harvey-Theking]]

Another problem


SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Your Job's Name 0.1
[INFO] ------------------------------------------------------------------------
[WARNING] The POM for eu.stratosphere:stratosphere-java:jar:0.4 is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details
[WARNING] The POM for eu.stratosphere:stratosphere-clients:jar:0.4 is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ quickstart ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/theking/workspace/quickstart/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ quickstart ---
Downloading: http://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-incremental/1.1/maven-shared-incremental-1.1.jar
Downloading: http://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-compiler-api/2.2/plexus-compiler-api-2.2.jar
Downloading: http://repo.maven.apache.org/maven2/org/apache/xbean/xbean-reflect/3.4/xbean-reflect-3.4.jar
Downloading: http://repo.maven.apache.org/maven2/log4j/log4j/1.2.12/log4j-1.2.12.jar
Downloading: http://repo.maven.apache.org/maven2/commons-logging/commons-logging-api/1.1/commons-logging-api-1.1.jar
Downloaded: http://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-incremental/1.1/maven-shared-incremental-1.1.jar (14 KB at 0.5 KB/sec)
Downloaded: http://repo.maven.apache.org/maven2/commons-logging/commons-logging-api/1.1/commons-logging-api-1.1.jar (44 KB at 0.4 KB/sec)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2:24.986s
[INFO] Finished at: Tue Mar 18 17:41:00 WAT 2014
[INFO] Final Memory: 8M/161M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project quickstart: Execution default-compile of goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile failed: Plugin org.apache.maven.plugins:maven-compiler-plugin:3.1 or one of its dependencies could not be resolved: The following artifacts could not be resolved: org.codehaus.plexus:plexus-compiler-api:jar:2.2, org.apache.xbean:xbean-reflect:jar:3.4, log4j:log4j:jar:1.2.12: Could not transfer artifact org.codehaus.plexus:plexus-compiler-api:jar:2.2 from/to central (http://repo.maven.apache.org/maven2): No response received after 60000 -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginResolutionException
;;;","09/Jun/14 12:33;github-import;[Date: Tue Mar 18 18:39:54 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,
there seems to be a network issue:
```
Could not transfer artifact org.codehaus.plexus:plexus-compiler-api:jar:2.2 from/to central (http://repo.maven.apache.org/maven2): No response received after 60000 -> [Help 1]
```
I think you have to retry until your connection is properly working.;;;","09/Jun/14 12:33;github-import;[Date: Fri Mar 21 17:34:04 CET 2014, Author: [Harvey-Theking|https://github.com/Harvey-Theking]]

Think its ok for now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix issue about Windows line break in TextInputFormat,FLINK-582,12719755,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:33,09/Jun/14 12:33,14/Jul/23 05:57,09/Jun/14 12:33,,,,pre-apache,,,,,,,0,github-import,,"Window uses \r\n to split lines. so, for text files from Windows, when \n is used as delimiter, \r is still appear at the end of lines.

The code checks to see if \n is used as delimiter and end of a line is \r, then \r is removed from the line

The change is the same as in https://github.com/stratosphere/stratosphere/pull/577, but this time I move it into the TextInputFormat. https://github.com/stratosphere/stratosphere/pull/577 failed the test case for terasort because the input file uses DelimitedInputFormat but it is not a text file.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/582
Created by: [tuantrieu|https://github.com/tuantrieu]
Labels: 
Created at: Sat Mar 15 03:15:21 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:33;github-import;pull-request-582-2620365475031396048.patch;https://issues.apache.org/jira/secure/attachment/12649215/pull-request-582-2620365475031396048.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397954,,,Mon Jun 09 12:33:45 UTC 2014,,,,,,,,,,"0|i1wihr:",398081,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:33;github-import;[Date: Sat Mar 15 22:49:56 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks like good start :-)

We would need a test case, though, before we can merge the patch. The test case needs to cover cases where
  - `\n` is used
  - `\r\n` is used
  - a different line delimiter is used

;;;","09/Jun/14 12:33;github-import;[Date: Sat Mar 15 22:52:07 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looking at this specific functionality...

Does it make sense to decouple the TextInputFormat from the DelimitedInputFormat? The following arguments would speak for that
  - I think text inputs always have `n` (or`\r\n`) as delimiters.
  - We could be sure the delimiter is always one character (simplify logic)
  - Other cases would still work with a DelimitedInputFormat, or a 1-field CSV input format;;;","09/Jun/14 12:33;github-import;[Date: Sat Mar 15 22:54:17 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The TextValueInputFormat is a variant of the TextInputFormat made for mutable string objects. It has the same limitation as the TextInputFormat had and would also benefit from the here added functionality. This could be a separate issue, though,;;;","09/Jun/14 12:33;github-import;[Date: Mon Mar 17 17:14:03 CET 2014, Author: [tuantrieu|https://github.com/tuantrieu]]

Thank for your suggestion. I will make test cases for this change.

In your comment about decoupling TextInputFormat from the DelimitedInputFormat, do you mean that this change could be done in DelimitedInputFormat ?

I am not sure if I understand what you mean, but I was thinking that files that can use delimiters must be text files, so I made this change in DelimitedInputFormat. However, it failed the TeraSortITCase test case  where input file is a text file with lines containing 100 characters including '\r\n', the test case expects exactly 99 characters for lines ( after delimiting lines by '\n'), 10 for key + 89 characters (including '\r') for values. So I moved this change to TextInputFormat to pass this test case.;;;","09/Jun/14 12:33;github-import;[Date: Tue Mar 18 21:00:09 CET 2014, Author: [tuantrieu|https://github.com/tuantrieu]]

Could you take a look at my test cases ?;;;","09/Jun/14 12:33;github-import;[Date: Tue Mar 18 21:19:01 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Tests look good to me. Thank you very much!;;;","09/Jun/14 12:33;github-import;[Date: Sun Mar 23 15:18:01 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [4b6af0b11831b68aba4b50c69563ca2264ddd85e|https://github.com/stratosphere/stratosphere/commit/4b6af0b11831b68aba4b50c69563ca2264ddd85e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add UnitTests for TypeExtractors of new Java API,FLINK-579,12719752,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:33,09/Jun/14 12:33,14/Jul/23 05:57,09/Jun/14 12:33,,,,pre-apache,,,,,,,0,github-import,,"The generation of serializers and comparators for the new Java API depends a lot on automatic type extraction.
The type extraction needs thorough testing to avoid hard to debug serialization problems.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/579
Created by: [fhueske|https://github.com/fhueske]
Labels: java api, testing, 
Milestone: Release 0.5
Assignee: [twalthr|https://github.com/twalthr]
Created at: Fri Mar 14 22:39:35 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397951,,,Mon Jun 09 12:33:26 UTC 2014,,,,,,,,,,"0|i1wih3:",398078,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:33;github-import;[Date: Thu Mar 27 10:26:25 CET 2014, Author: [twalthr|https://github.com/twalthr]]

At the moment I tested the following types, some types are not handled correctly (no exception is thrown):

Supported types:
- basic types (String, Boolean, Integer etc.)
- tuple with basic types
- tuple with tuples
- subclass of tuple
- custom type
- value type
- tuple of values

Types not implemented (yet):
- array
- tuples with arrays
- custom types with generics
- usage of functions like ```class MapFunctionImpl<X, T> extends MapFunction<X, T>``` and
		```MapFunction<?, ?> function = new MapFunctionImpl<String, String>();``` -> TypeExtractor, returns null

Unintended usage of API:
- missing generics in operator function -> ""Generic function base class must be immediate super class."" Error message should be improved. Not user-friendly.
- missing generics in tuple -> No exception, but TypeExtractor returns null
- usage of Tuple in generics (```MapFunction<X, Tuple>```) -> No exception, but TypeExtractor returns null
- usage of Value in generics (```MapFunction<X, Value>```) -> TypeExtractor returns ```ValueType<eu.stratosphere.types.Value>```


Do you have other cases I could test?;;;","09/Jun/14 12:33;github-import;[Date: Thu Mar 27 13:37:53 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Please add also Types that implement Hadoop's `Writable` interface. They should have a special serializer that calls the `write()` and `readFields()` method.;;;","09/Jun/14 12:33;github-import;[Date: Thu Mar 27 13:45:00 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

How about the following cases (I tried to prioritize them)

  - Tuples of custom types, such as `Tuple2<Long, MyJavaClass>`
  - Cases where the generic parameters are not in the immediate superclasse:
```
public class LongKeyValue<V> extends Tuple2<Long, V> {
...
}

public class MyPair extends LonkKeyValue<String> {
...
}
```

  - Functions where the return type is dependent on the input type. The type extractor interface needs to be extended to take the input type in order to produce the return type:
```
public class IdentityMapper<T> extends MapFunction<T, T> {
...
}
```

  - Cases where parts of the return type depend on the input type:
```
public class OneAppender<T> extends MapFunction<T, Tuple2<T, Integer>> {
    public Tuple2<T, Integer> map(T value) {
        return new Tuple2<T, Integer>(value, 1);
    }
}
```;;;","09/Jun/14 12:33;github-import;[Date: Thu Mar 27 13:47:54 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

  - Input type may be in the output type multiple times
```
public class FieldDuplicator<T> extends Mapper<T, Tuple2<T, T>> {
    public Tuple2<T, T> map(T value) {
        return new Tuple2<T, T>(value, value);
    }
}
```;;;","09/Jun/14 12:33;github-import;[Date: Thu Mar 27 13:53:07 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

### For the cases that we do not support, please give specific error messages for the following cases:
  - If someone uses `Tuple`, rather than a specific type of tuple.
  - If the type is abstract or an interface
  - If the type cannot be found out and does not depend on the input type: `public class MyMapper<E> extends MapFunction<String, E> {}`;;;","09/Jun/14 12:33;github-import;[Date: Wed Apr 02 15:23:21 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

I have rewritten most of the TypeExtractor, but now I'm facing a problem. The TypeExtractor is now able to realize TypeVariables and can e.g. extract the return type of a IdentityMapper mentioned above.

This does work correctly:
```java
IdentityMapper<Boolean> function = new IdentityMapper<Boolean>(){ /* nothing in here */ };
TypeInformation<?> ti = TypeExtractor.getMapReturnTypes(function);
```

This doesn't work:
```java
IdentityMapper<Boolean> function = new IdentityMapper<Boolean>();
TypeInformation<?> ti = TypeExtractor.getMapReturnTypes(function);
```

I think this is due to type erasure, right? How should I handle this case exactly?;;;","09/Jun/14 12:33;github-import;[Date: Thu Apr 03 14:26:48 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

I implemented it the following way, I hope this is ok?

```java
IdentityMapper<String> function = new IdentityMapper<String>();

TypeInformation<?> ti = TypeExtractor.getMapReturnTypes(function, BasicTypeInfo.STRING_TYPE_INFO);

```;;;","09/Jun/14 12:33;github-import;[Date: Mon Apr 07 11:54:50 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

@StephanEwen In which package should I put a Writable Type? If it should be in the same package than the other types, I need to add a 'hadoop-common' dependency to the Java API...is this intended? If yes, which version?;;;","09/Jun/14 12:33;github-import;[Date: Tue Apr 08 10:33:18 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

Should ```GenericArrayType```s be supported? Theoretically it is possible to extract the TypeInformation from:

```java
MapFunction<?, ?> function = new MapFunction<String, Tuple2<String, String>[]>() {
	private static final long serialVersionUID = 1L;

	@Override
	public Tuple2<String, String>[] map(String value) throws Exception {
		return null;
	}				
};
			
TypeInformation<?> ti = TypeExtractor.getMapReturnTypes(function, null);
```

But the ```ArrayTypeInfo``` currently does not provide the API for that.;;;","09/Jun/14 12:33;github-import;[Date: Fri Apr 11 12:46:01 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

How should I treat custom objects with generics?

```java
public class MyObject<T> {
	public T myField;
}
	
@Test
public void testParamertizedCustomObject() {
	MapFunction<?, ?> function = new MapFunction<Boolean, MyObject<String>>() {
		private static final long serialVersionUID = 1L;

		@Override
		public MyObject<String> map(Boolean value) throws Exception {
			return null;
		}
	};
		
	TypeInformation<?> ti = TypeExtractor.getMapReturnTypes(function, null);
	System.out.println(ti);
}
```

At the moment I create a GenericTypeInfo with the raw object (without generics). Is this ok? Or should I throw an Exception that this is not suported?;;;","09/Jun/14 12:33;github-import;[Date: Fri Apr 11 22:14:14 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think that is fine from our side. The generic types go right now to Avro.
Can you check if Avro can handle them? If yes, we are good...;;;","09/Jun/14 12:33;github-import;[Date: Mon Apr 28 18:25:55 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Done in [b3e405f22cd0101e111c8ede8d04621d8ada33d3|https://github.com/stratosphere/stratosphere/commit/b3e405f22cd0101e111c8ede8d04621d8ada33d3];;;","09/Jun/14 12:33;github-import;[Date: Fri May 02 09:10:30 CEST 2014, Author: [twalthr|https://github.com/twalthr]]

Actually this issue is not done completely as 

""Please add also Types that implement Hadoop's Writable interface. They should have a special serializer that calls the write() and readFields() method.""

is still missing. I will open a separate issue for this.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix for ReduceOperator mutable object return bug.,FLINK-578,12719751,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:33,09/Jun/14 12:33,14/Jul/23 05:57,09/Jun/14 12:33,,,,pre-apache,,,,,,,0,github-import,,"This PR proposes a fix for ([#563|https://github.com/stratosphere/stratosphere/issues/563] | [FLINK-563|https://issues.apache.org/jira/browse/FLINK-563]).

Performance implications should be tolerable. For Sort-Group strategy only the first value of a group is copied. For Hash-Aggregate strategy (not supported yet), there will be no performance implications.

The fix assumes that the object of the first input value (```val1```) of a ```T reduce(T val1, T val2)``` may be returned and that the object of the second input value (```val2```) may NOT be returned.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/578
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, java api, 
Created at: Fri Mar 14 22:12:59 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:33;github-import;pull-request-578-324083296967891251.patch;https://issues.apache.org/jira/secure/attachment/12649213/pull-request-578-324083296967891251.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397950,,,Mon Jun 09 12:33:15 UTC 2014,,,,,,,,,,"0|i1wigv:",398077,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:33;github-import;[Date: Thu Mar 20 11:51:39 CET 2014, Author: [fhueske|https://github.com/fhueske]]

PR was rebased and updated as ([#614|https://github.com/stratosphere/stratosphere/issues/614] | [FLINK-614|https://issues.apache.org/jira/browse/FLINK-614]).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix issue #505 removing trailing \r from lines in Windows text files,FLINK-577,12719750,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:33,09/Jun/14 12:33,14/Jul/23 05:57,09/Jun/14 12:33,,,,pre-apache,,,,,,,0,github-import,,"Window uses \r\n to split lines. so, for text files from Windows, when
\n is used as delimiter, \r is still appear at the end of lines.

The code checks to see if \n is used as delimiter and end of a line is
\r, then \r is removed from the line

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/577
Created by: [tuantrieu|https://github.com/tuantrieu]
Labels: 
Created at: Fri Mar 14 18:22:03 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:33;github-import;pull-request-577-6141345236197247851.patch;https://issues.apache.org/jira/secure/attachment/12649212/pull-request-577-6141345236197247851.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397949,,,Mon Jun 09 12:33:10 UTC 2014,,,,,,,,,,"0|i1wign:",398076,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:33;github-import;[Date: Fri Mar 14 20:33:53 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,
thank you for your pull request.
your code is breaking some of our test cases. Have a look at the output of travis: https://travis-ci.org/stratosphere/stratosphere/builds/20784188
You can run the tests locally as well using `mvn verify`. You can also start individual JUnit Tests from Eclipse/IntelliJ;;;","09/Jun/14 12:33;github-import;[Date: Fri Mar 14 23:09:35 CET 2014, Author: [tuantrieu|https://github.com/tuantrieu]]

Thank, I didn't know there are unit test cases. I fixed it and running test cases.

I notice that there are 2TextInputFormat.java files in package eu.stratosphere.api.java.record.io and package eu.stratosphere.api.java.io.

Thus, I have to make 2 identical changes in the 2 files.;;;","09/Jun/14 12:33;github-import;[Date: Sat Mar 15 09:46:11 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Duplicate of https://github.com/stratosphere/stratosphere/pull/582;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes for ReduceFunction and some clean-up (new JavaAPI),FLINK-576,12719749,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:33,09/Jun/14 12:33,14/Jul/23 05:57,09/Jun/14 12:33,,,,pre-apache,,,,,,,0,github-import,,"- Fixes in ReduceFunction.
- Removed ReferenceWrappingReducer. 
- Cleaned imports in PlanOperators. 

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/576
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Fri Mar 14 13:09:13 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:33;github-import;pull-request-576-5983237724496559997.patch;https://issues.apache.org/jira/secure/attachment/12649211/pull-request-576-5983237724496559997.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397948,,,2014-06-09 12:33:02.0,,,,,,,,,,"0|i1wigf:",398075,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Current Stratosphere does not compile on Oracle JDK 6,FLINK-575,12719748,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:32,09/Jun/14 12:33,14/Jul/23 05:57,09/Jun/14 12:33,,,,pre-apache,,,,,,,0,github-import,,"When compiling Stratosphere with Oracle JDK 6 the following compiler bug occurs:

<code>
An exception has occurred in the compiler (1.6.0_26). Please file a bug at the Java Developer Connection (http://java.sun.com/webapps/bugreport)  after checking the Bug Parade for duplicates. Include your program and the following diagnostic in your report.  Thank you.
java.lang.NullPointerException
	at com.sun.tools.javac.comp.Check.checkCompatibleConcretes(Check.java:1215)
	at com.sun.tools.javac.comp.Check.checkCompatibleSupertypes(Check.java:1567)
	at com.sun.tools.javac.comp.Attr.attribClassBody(Attr.java:2674)
	at com.sun.tools.javac.comp.Attr.attribClass(Attr.java:2628)
	at com.sun.tools.javac.comp.Attr.attribClass(Attr.java:2584)
	at com.sun.tools.javac.comp.Attr.attribClass(Attr.java:2584)
	at com.sun.tools.javac.comp.Attr.attribClass(Attr.java:2564)
	at com.sun.tools.javac.main.JavaCompiler.attribute(JavaCompiler.java:1036)
	at com.sun.tools.javac.main.JavaCompiler.compile2(JavaCompiler.java:765)
	at com.sun.tools.javac.main.JavaCompiler.compile(JavaCompiler.java:730)
	at com.sun.tools.javac.main.Main.compile(Main.java:353)
	at com.sun.tools.javac.api.JavacTaskImpl.call(JavacTaskImpl.java:115)
	at org.codehaus.plexus.compiler.javac.JavaxToolsCompiler.compileInProcess(JavaxToolsCompiler.java:126)
	at org.codehaus.plexus.compiler.javac.JavacCompiler.performCompile(JavacCompiler.java:169)
	at org.apache.maven.plugin.compiler.AbstractCompilerMojo.execute(AbstractCompilerMojo.java:785)
</code>

the bug is reported here: http://bugs.java.com/view_bug.do?bug_id=6292765

OpenJDK 6 and Oracle/Open JDK 7 do not have this problem. 

Should this bug be prevented in the stratosphere code or should we don't support Oracle JDK 6 anymore?


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/575
Created by: [twalthr|https://github.com/twalthr]
Labels: bug, build system, simple-issue, 
Created at: Fri Mar 14 12:42:57 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397947,,,Mon Jun 09 12:33:00 UTC 2014,,,,,,,,,,"0|i1wig7:",398074,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:32;github-import;[Date: Fri Mar 14 12:45:31 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

As far as I know, Oracle stopped supporting the JDK 6 in 2012.
As long as the JDK 6 is able to execute Stratosphere, we are fine.;;;","09/Jun/14 12:32;github-import;[Date: Sat Mar 15 23:57:33 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I agree. I would not worry about this. as all newer versions work fine. It
affects only developers (who compile themselves), and those should be in a
position to upgrade a newer compiler version.;;;","09/Jun/14 12:32;github-import;[Date: Thu Apr 10 10:16:20 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

As found in ([#673|https://github.com/stratosphere/stratosphere/issues/673] | [FLINK-673|https://issues.apache.org/jira/browse/FLINK-673]), we need to update configuration of the maven-compile-plugin in the pom file to Java 1.7.;;;","09/Jun/14 12:33;github-import;[Date: Thu Apr 24 23:52:05 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Duplicate of ([#673|https://github.com/stratosphere/stratosphere/issues/673] | [FLINK-673|https://issues.apache.org/jira/browse/FLINK-673]) 

I am closing this for the same reason

This is clearly a java compiler bug in a no longer maintained Java version. We still support Java 6 runtimes, but we support not all versions of Java 6 compilers. That affects only developers that compile the system themselves: They need to upgrade their local JDK.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Implement writeAsCSV(Path filePath, String rowDelimiter, String fieldDelimiter) for new Java API",FLINK-570,12719743,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:32,09/Jun/14 12:32,14/Jul/23 05:57,09/Jun/14 12:32,,,,pre-apache,,,,,,,0,github-import,,"```DataSet.writeAsCSV(Path filePath, String rowDelimiter, String fieldDelimiter)``` is currently not implemented.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/570
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, java api, 
Created at: Fri Mar 14 00:07:36 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397942,,,Mon Jun 09 12:32:29 UTC 2014,,,,,,,,,,"0|i1wif3:",398069,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:32;github-import;[Date: Sat Mar 15 18:47:18 CET 2014, Author: [alexff91|https://github.com/alexff91]]

Hi, I think there is the realisation of this method:
https://github.com/aljoscha/stratosphere/commit/99660b21ccca6cb4ea2c7bd95aac3c6be0426c3e;;;","09/Jun/14 12:32;github-import;[Date: Sun Mar 16 11:34:10 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed in ([#585|https://github.com/stratosphere/stratosphere/issues/585] | [FLINK-585|https://issues.apache.org/jira/browse/FLINK-585]) and merged into staging.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix for termination criterion issues,FLINK-569,12719742,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:32,09/Jun/14 12:32,14/Jul/23 05:57,09/Jun/14 12:32,,,,pre-apache,,,,,,,0,github-import,,"I encountered some minor problems while testing the new termination criterion for bulk iterations and I tried to fix them.

1. NullPointerException in OptimizerNode.mergeLists because child2open is not checked for being not null
2. Faulty chaining of root of termination criterion and root of step function. This happens if one uses the next partial solution as the termination criterion
3. BulkIterationNode tries to generate alternative plans for the termination criterion even if there are no plans for the next partial solution. If the termination criterion depends on the next partial solution, then the algorithm won't find a plan fulfilling the requirements and thus throw an exception.
4. Some of the termination criterion nodes are instantiated as a RegularPactTasks instead of IntermediateIterationTasks. This is caused by not setting the dynamic paths of the termination criterion.

I hope that I haven't broken anything while trying to fix the issues.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/569
Created by: [tillrohrmann|https://github.com/tillrohrmann]
Labels: 
Created at: Thu Mar 13 23:12:34 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:32;github-import;pull-request-569-6079300019391394873.patch;https://issues.apache.org/jira/secure/attachment/12649208/pull-request-569-6079300019391394873.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397941,,,Mon Jun 09 12:32:25 UTC 2014,,,,,,,,,,"0|i1wiev:",398068,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:32;github-import;[Date: Thu Mar 20 14:37:27 CET 2014, Author: [markus-h|https://github.com/markus-h]]

Thanks for your testing! Your fixes look good for me. 
I don't fully understand the 3rd issue. Can you describe the case where there are no plans for next partial solution and candidates.size() == 0?;;;","09/Jun/14 12:32;github-import;[Date: Thu Mar 20 17:40:57 CET 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

The 3rd issue always arises if all generated plans for the next partial solution are discarded because they don't met the properties (instantiateCandidate method of BulkIterationNode). If all plans are discarded then there is no plan for the required properties and thus there is no need to look for a plan for the termination criterion. But since the termination criterion often depends on the next partial solution, it requires the next partial solution to have at least one plan. If not, then the system complains wrongly about the termination criterion plan which cannot be instantiated. But in the case where the next partial solution has no plans for given properties, we should never try to find a plan for the termination criterion.;;;","09/Jun/14 12:32;github-import;[Date: Thu Mar 20 23:51:47 CET 2014, Author: [markus-h|https://github.com/markus-h]]

Ok, that makes sense, thanks for your explanation!;;;","09/Jun/14 12:32;github-import;[Date: Sun Mar 23 15:18:30 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [525cb03668b6f6a6bdb4c5c91d350d2e1670c319|https://github.com/stratosphere/stratosphere/commit/525cb03668b6f6a6bdb4c5c91d350d2e1670c319];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for group sorting to new Java API,FLINK-565,12719737,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:32,09/Jun/14 12:32,14/Jul/23 05:57,09/Jun/14 12:32,,,,pre-apache,,,,,,,0,github-import,,"Adds initial support for group sorting. Currently limited to:
- Reduce operator
- Tuple data types with field selector key definition

Also includes a fix for sorting of Tuples (multi field sorting and sort order ([#543|https://github.com/stratosphere/stratosphere/issues/543] | [FLINK-543|https://issues.apache.org/jira/browse/FLINK-543])) in ```TupleComparator``` and ```BasicTypeComparator```.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/565
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, enhancement, java api, 
Created at: Thu Mar 13 09:47:20 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:32;github-import;pull-request-565-4067553265148208116.patch;https://issues.apache.org/jira/secure/attachment/12649207/pull-request-565-4067553265148208116.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397936,,,2014-06-09 12:32:05.0,,,,,,,,,,"0|i1widr:",398063,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add distributed cache to stratosphere,FLINK-564,12719736,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:31,18/Jun/14 16:07,14/Jul/23 05:57,09/Jun/14 12:32,,,,pre-apache,,,,,,,0,github-import,,"implement issue ([#506|https://github.com/stratosphere/stratosphere/issues/506] | [FLINK-506|https://issues.apache.org/jira/browse/FLINK-506])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/564
Created by: [qmlmoon|https://github.com/qmlmoon]
Labels: 
Created at: Wed Mar 12 23:33:30 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,FLINK-506,,,,,,,"09/Jun/14 12:31;github-import;pull-request-564-5340183388418221361.patch;https://issues.apache.org/jira/secure/attachment/12649206/pull-request-564-5340183388418221361.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397935,,,Mon Jun 09 12:32:03 UTC 2014,,,,,,,,,,"0|i1widj:",398062,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:31;github-import;[Date: Mon Mar 17 21:37:56 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Manually triggered a travis test build: https://travis-ci.org/rmetzger/stratosphere/builds/20968273;;;","09/Jun/14 12:32;github-import;[Date: Mon Mar 17 21:53:51 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

3 files are missing the license header.;;;","09/Jun/14 12:32;github-import;[Date: Mon Mar 17 21:54:24 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Please execute `mvn verify` before submitting a pull request.;;;","09/Jun/14 12:32;github-import;[Date: Tue Mar 18 11:43:52 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Have you tested the code on a cluster?
;;;","09/Jun/14 12:32;github-import;[Date: Thu Apr 03 21:30:27 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

@qmlmoon @rmetzger any news for this PR? Did you test it on a cluster? 
Will have a closer look later.;;;","09/Jun/14 12:32;github-import;[Date: Fri Apr 04 09:31:03 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi, Mingliang told me via email that he tested it on a cluster.;;;","09/Jun/14 12:32;github-import;[Date: Fri Apr 25 12:15:24 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I'm testing the DC on the cluster. @zentol depends on this for the python lang binding.;;;","09/Jun/14 12:32;github-import;[Date: Fri Apr 25 18:12:16 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Successfully tested on the cluster with this code https://github.com/stratosphere/testjob/commit/e148b743207ae6ef2993cbb1f4c015e63501877c

Committed to master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New Java API: reduce function does not allow to return input object.,FLINK-563,12719735,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:31,09/Jun/14 12:31,14/Jul/23 05:57,09/Jun/14 12:31,,,,pre-apache,,,,,,,0,github-import,,"The reduce function of the new Java API does not work if the an input object is returned, i.e.,

New example:
```
public T reduce(T value1, T value2) throws Exception {
  value1.setVal(value1.getVal() + value2.getVal());
  return value1;
}
```

This is due to the internal mutable object runtime.

Wrong example:
```
public T reduce(T value1, T value2) throws Exception {
  value1 = value1 + value2;
  return value1;
}
```



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/563
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, java api, 
Milestone: Release 0.5
Created at: Wed Mar 12 17:31:21 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397934,,,Mon Jun 09 12:31:53 UTC 2014,,,,,,,,,,"0|i1widb:",398061,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:31;github-import;[Date: Wed Mar 12 17:38:20 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you elaborate a bit more, what does happen in this case? In the
example, it looks like the value1 reference (or value) is reassigned to
value1+value2. That does not modify any internal object, or am I
overlooking something?;;;","09/Jun/14 12:31;github-import;[Date: Wed Mar 12 18:03:03 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I observed the problem with the ```ReferenceWrappingReducer``` in [PlanReduceOperator|https://github.com/stratosphere/stratosphere/blob/master/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/translation/PlanReduceOperator.java] but expect this to have the same effect in [ReduceFunction|https://github.com/stratosphere/stratosphere/blob/master/stratosphere-java/src/main/java/eu/stratosphere/api/java/functions/ReduceFunction.java).

When looping over the iterator, the final hasNext() call in the wrapping ```reduce(Iterator<Reference<T>> in, Collector<Reference<T>> out]``` overwrites the curr object.
I think the problem is that return values are not *safe* when returned from the user function as in the other function which serialize output values or immediately forward them to the next function.

I am currently working on a fix that copies the result of the function call into another mutable object using a Serializer.;;;","09/Jun/14 12:31;github-import;[Date: Wed Mar 12 18:05:45 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Damn, stupid example.
It should be something like:

```
public T reduce(T value1, T value2) throws Exception {
  value1.setVal(value1.getVal() + value2.getVal());
  return value1;
}
```;;;","09/Jun/14 12:31;github-import;[Date: Wed Mar 12 18:07:34 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Let's see if we can solve this without copying... Otherwise that kills
performance...;;;","09/Jun/14 12:31;github-import;[Date: Sun Mar 16 17:29:34 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you post a concrete example where it fails? Maybe add a unit test that fails due to this problem?;;;","09/Jun/14 12:31;github-import;[Date: Sun Mar 16 22:12:50 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I added a unit test for this to PR ([#578|https://github.com/stratosphere/stratosphere/issues/578] | [FLINK-578|https://issues.apache.org/jira/browse/FLINK-578]).;;;","09/Jun/14 12:31;github-import;[Date: Fri Apr 11 15:22:26 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

In commit [cf0eff3b1386486ed656aed1c1e8260c0cba5a09|https://github.com/stratosphere/stratosphere/commit/cf0eff3b1386486ed656aed1c1e8260c0cba5a09] I added JavaDocs to `T ReduceFunction.reduce(T value1, T value2)` function that tell the user that the second input object (`value2`) can be safely returned while the first input object (`value1`) must not be returned (checked by an integration test).
This is not the optimal solution, but I think the best we can do right now.
See also the discussion in ([#614|https://github.com/stratosphere/stratosphere/issues/614] | [FLINK-614|https://issues.apache.org/jira/browse/FLINK-614]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implementation of the remaining BasicTypes for new Java API,FLINK-562,12719734,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:31,09/Jun/14 12:31,14/Jul/23 05:57,09/Jun/14 12:31,,,,pre-apache,,,,,,,0,github-import,,"The following types are included:

- Byte
- Short
- Boolean
- Character
- Float

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/562
Created by: [twalthr|https://github.com/twalthr]
Labels: bug, java api, 
Created at: Wed Mar 12 15:21:58 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:31;github-import;pull-request-562-85207101712646668.patch;https://issues.apache.org/jira/secure/attachment/12649205/pull-request-562-85207101712646668.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397933,,,Mon Jun 09 12:31:46 UTC 2014,,,,,,,,,,"0|i1wid3:",398060,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:31;github-import;[Date: Wed Mar 12 15:32:29 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Nice, thank you! Looks good at a first glance. I will try and review it
soon.;;;","09/Jun/14 12:31;github-import;[Date: Sun Mar 16 01:26:36 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It is as correct as thee other utils.

Unfortunately, none yet respects the ""ascending order"" flag during comparisons. I'll create a separate issue for this.;;;","09/Jun/14 12:31;github-import;[Date: Tue Mar 18 23:08:08 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [90e0337c920cd4460a5d8c77deda2e123bd0dab7|https://github.com/stratosphere/stratosphere/commit/90e0337c920cd4460a5d8c77deda2e123bd0dab7];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes a bug when using Double types as sorting key,FLINK-560,12719732,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:31,09/Jun/14 12:31,14/Jul/23 05:57,09/Jun/14 12:31,,,,pre-apache,,,,,,,0,github-import,,"Fixes the `UnsupportedOperationException` mentioned in ([#551|https://github.com/stratosphere/stratosphere/issues/551] | [FLINK-551|https://issues.apache.org/jira/browse/FLINK-551]). And corrects the documentation of the `isNormalizedKeyPrefixOnly()` method in `TypeComparator`.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/560
Created by: [twalthr|https://github.com/twalthr]
Labels: java api, 
Created at: Wed Mar 12 11:32:09 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:31;github-import;pull-request-560-8065797352523147077.patch;https://issues.apache.org/jira/secure/attachment/12649204/pull-request-560-8065797352523147077.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397931,,,2014-06-09 12:31:34.0,,,,,,,,,,"0|i1wicn:",398058,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes a NullPointerException in PlanUnwrappingReduceGroupOperator,FLINK-556,12719728,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:31,09/Jun/14 12:31,14/Jul/23 05:57,09/Jun/14 12:31,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/556
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Tue Mar 11 12:26:58 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:31;github-import;pull-request-556-5503314397448328041.patch;https://issues.apache.org/jira/secure/attachment/12649203/pull-request-556-5503314397448328041.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397927,,,Mon Jun 09 12:31:23 UTC 2014,,,,,,,,,,"0|i1wibr:",398054,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:31;github-import;[Date: Wed Mar 12 21:41:58 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Merged;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Links in the website in Documentation page showing a 404 error,FLINK-554,12719726,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:31,09/Jun/14 12:31,14/Jul/23 05:57,09/Jun/14 12:31,,,,pre-apache,,,,,,,0,github-import,,"The following links in the website 
http://stratosphere.eu/docs/0.4/program_execution/local_executer.html
http://stratosphere.eu/docs/0.4/program_execution/remote_executer.html 
are showing a 404 error. Please fix it
![screenshot from 2014-03-11 10 42 58|https://f.cloud.github.com/assets/1849435/2382222/5869ddc8-a8dc-11e3-9ade-2227c79e2571.png]
![screenshot from 2014-03-11 10 42 46|https://f.cloud.github.com/assets/1849435/2382223/586a5122-a8dc-11e3-8a79-1363b6828d92.png]



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/554
Created by: [SarathSomana|https://github.com/SarathSomana]
Labels: 
Created at: Tue Mar 11 06:16:54 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397925,,,2014-06-09 12:31:14.0,,,,,,,,,,"0|i1wibb:",398052,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thread 'SortMerger Reading Thread' terminated due to an exception,FLINK-551,12719723,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:30,09/Jun/14 12:30,14/Jul/23 05:57,09/Jun/14 12:30,,,,pre-apache,,,,,,,0,github-import,,"By accident, I grouped by a tuple field with type `Double`. The following exception occurs when trying to `groupBy()` by a Double field and using a `reduceGroup()` afterwards:

<code>java.lang.Exception: The data preparation for task 'Reduce(eu.stratosphere.example.java.relational.TPCHQuery1$1)' , caused an error: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: null
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:481)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:374)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:345)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: null
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:635)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.getInput(RegularPactTask.java:1113)
	at eu.stratosphere.pact.runtime.task.ReduceDriver.prepare(ReduceDriver.java:86)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:476)
	... 3 more
Caused by: java.io.IOException: Thread 'SortMerger Reading Thread' terminated due to an exception: null
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:826)
Caused by: java.lang.UnsupportedOperationException
	at eu.stratosphere.api.common.typeutils.base.DoubleComparator.putNormalizedKey(DoubleComparator.java:58)
	at eu.stratosphere.api.common.typeutils.base.DoubleComparator.putNormalizedKey(DoubleComparator.java:1)
	at eu.stratosphere.api.java.typeutils.runtime.TupleSingleFieldComparator.putNormalizedKey(TupleSingleFieldComparator.java:101)
	at eu.stratosphere.api.java.typeutils.runtime.TupleSingleFieldComparator.putNormalizedKey(TupleSingleFieldComparator.java:1)
	at eu.stratosphere.api.java.typeutils.runtime.ReferenceWrappedComparator.putNormalizedKey(ReferenceWrappedComparator.java:93)
	at eu.stratosphere.api.java.typeutils.runtime.ReferenceWrappedComparator.putNormalizedKey(ReferenceWrappedComparator.java:1)
	at eu.stratosphere.pact.runtime.sort.NormalizedKeySorter.write(NormalizedKeySorter.java:269)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$ReadingThread.go(UnilateralSortMerger.java:1056)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:822)</code>

I don't know if grouping over a double key makes much sense but anyway the exception is not user friendly and should be improved.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/551
Created by: [twalthr|https://github.com/twalthr]
Labels: bug, java api, 
Created at: Mon Mar 10 14:34:23 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397922,,,Mon Jun 09 12:30:40 UTC 2014,,,,,,,,,,"0|i1wian:",398049,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:30;github-import;[Date: Mon Mar 10 16:29:20 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The system thinks it can use normalized keys, but double types do not
implement them. Can you check whether the double comparator returns true on
the method that declares whether normalized keys are enabled?;;;","09/Jun/14 12:30;github-import;[Date: Tue Mar 11 11:19:59 CET 2014, Author: [twalthr|https://github.com/twalthr]]

The `DoubleComparator` returns false. I think the problem lies in `NormalizedKeySorter` because the support is checked in the constructor but in `write(T record)` 

    this.comparator.putNormalizedKey(record, this.currentSortIndexSegment, this.currentSortIndexOffset + OFFSET_LEN, this.numKeyBytes);
		
is called anyway. Would it be ok to simply remove the thrown `UnsupportedException` in `putNormalizedKey` of the comparator?;;;","09/Jun/14 12:30;github-import;[Date: Tue Mar 11 14:29:21 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The exception is there for a reason. Simply doing nothing screws up the
behavior. The issue is to figure out why the system tries to use normalized
keys in the first place. Can you debug that?;;;","09/Jun/14 12:30;github-import;[Date: Wed Mar 12 13:56:36 CET 2014, Author: [fhueske|https://github.com/fhueske]]

We had a look at this.
The problem is that the NormalizedKeySorter always calls the putNormalizedKey() method, even if normalized keys are not supported. 
In this case, the NormalizedKeySorter sets the key-length to 0 and expects that the function call doesn't do anything. However, the DoubleComparator throws an exception and causes the failure.;;;","09/Jun/14 12:30;github-import;[Date: Wed Mar 12 15:25:38 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, so a quick fix is to do nothing on 0 length calls. We should fix the
behavior of the sorter, though, that it does not even try to put normalized
keys in the first place.;;;","09/Jun/14 12:30;github-import;[Date: Wed Mar 12 15:33:16 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Already done, see ([#560|https://github.com/stratosphere/stratosphere/issues/560] | [FLINK-560|https://issues.apache.org/jira/browse/FLINK-560]) :smile:;;;","09/Jun/14 12:30;github-import;[Date: Wed Mar 12 21:14:25 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed in ([#560|https://github.com/stratosphere/stratosphere/issues/560] | [FLINK-560|https://issues.apache.org/jira/browse/FLINK-560]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix tests failing with java 8 #457 ,FLINK-549,12719721,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:30,09/Jun/14 12:30,14/Jul/23 05:57,09/Jun/14 12:30,,,,pre-apache,,,,,,,0,github-import,,"Compatibility fixes for java 8, see ([#457|https://github.com/stratosphere/stratosphere/issues/457] | [FLINK-457|https://issues.apache.org/jira/browse/FLINK-457]).

Note: preliminary version, this is mostly to see whether i found all issuess

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/549
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Sun Mar 09 16:12:42 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:30;github-import;pull-request-549-6028680803089969231.patch;https://issues.apache.org/jira/secure/attachment/12649199/pull-request-549-6028680803089969231.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397920,,,Mon Jun 09 12:30:27 UTC 2014,,,,,,,,,,"0|i1wia7:",398047,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:30;github-import;[Date: Sun Mar 09 18:05:43 CET 2014, Author: [zentol|https://github.com/zentol]]

looks like compiling on java8 works fine.

only issue remaining is that using -Xdoclint:none breaks previous versions of java.
(it prevent errors from malformed javadoc comments);;;","09/Jun/14 12:30;github-import;[Date: Sun Mar 09 18:09:59 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Mh. Another user on Stackoverflow has the same issue we do: http://stackoverflow.com/a/16743137/568695

I have no idea what to do in this case.;;;","09/Jun/14 12:30;github-import;[Date: Tue Mar 11 18:29:58 CET 2014, Author: [zentol|https://github.com/zentol]]

looks like it works. :);;;","09/Jun/14 12:30;github-import;[Date: Tue Mar 11 18:32:12 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

How did you resolve it?;;;","09/Jun/14 12:30;github-import;[Date: Tue Mar 11 18:34:08 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

We have a regular javadoc build plugin now and one conditionally for java 8 that overrides the regular one?;;;","09/Jun/14 12:30;github-import;[Date: Tue Mar 11 18:36:00 CET 2014, Author: [zentol|https://github.com/zentol]]

@StephanEwen correct.;;;","09/Jun/14 12:30;github-import;[Date: Tue Mar 11 18:37:07 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

God job :-)

Out of curiosity, what is the javassist dependency for?;;;","09/Jun/14 12:30;github-import;[Date: Tue Mar 11 18:42:41 CET 2014, Author: [zentol|https://github.com/zentol]]

there's a transitive dependency in stratosphere-core for a javaassist version that is not compatible with java 8. guess i could encapsulate it as well in an extra profile.;;;","09/Jun/14 12:30;github-import;[Date: Tue Mar 11 18:48:54 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay. Can you solve it with the dependency management entries? I think that would be the ""clean"" way to do it.

Is that dependency actually used? Or is it a dependency that for example hadoop adds that is never used, because we only use hadoop's hdfs client?;;;","09/Jun/14 12:30;github-import;[Date: Tue Mar 11 19:15:43 CET 2014, Author: [zentol|https://github.com/zentol]]

Ill add it to the dependency management.

i checked the dependendies again; javaassist actually comes up in multiple parts.
(clients, runtime and addons aswell)

maven marks it as used in clients and runtime.;;;","09/Jun/14 12:30;github-import;[Date: Thu Mar 13 10:23:19 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I think the pull request is good to merge.;;;","09/Jun/14 12:30;github-import;[Date: Sat Mar 15 22:43:29 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Should we exclude one of the Java7 profiles? With Hadoop 1 and 2, we now have 8 builds to test. How about we test
  - oraclejdk8
  - oraclejdk7
  - openjdk6;;;","09/Jun/14 12:30;github-import;[Date: Sun Mar 16 13:25:27 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Agree :+1:;;;","09/Jun/14 12:30;github-import;[Date: Mon Mar 17 21:40:20 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

@zentol Can you update the `.travis.yml` and remove the `openjdk7` ?
Then I'll merge your PR.;;;","09/Jun/14 12:30;github-import;[Date: Mon Mar 17 23:48:06 CET 2014, Author: [zentol|https://github.com/zentol]]

@rmetzger Done.;;;","09/Jun/14 12:30;github-import;[Date: Tue Mar 18 18:35:19 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged in [6bf7d5a2451f9fd0b1773a51785b24f7aeb83304|https://github.com/stratosphere/stratosphere/commit/6bf7d5a2451f9fd0b1773a51785b24f7aeb83304];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests for the operators of the new Java API,FLINK-548,12719720,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:30,09/Jun/14 12:30,14/Jul/23 05:57,09/Jun/14 12:30,,,,pre-apache,,,,,,,0,github-import,,"The operators of the new Java API need to be tested for different data types (Tuples, Native Java Objects (Generic Avro Serialization), ...) and different key definitions (record fields, key extractors) in different combinations.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/548
Created by: [fhueske|https://github.com/fhueske]
Labels: java api, testing, 
Milestone: Release 0.5
Created at: Sat Mar 08 15:51:42 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397919,,,Mon Jun 09 12:30:14 UTC 2014,,,,,,,,,,"0|i1wi9z:",398046,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:30;github-import;[Date: Fri Mar 21 12:59:19 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I thought about the operator tests and made a [wiki page of tests that we should run|https://github.com/stratosphere/stratosphere/wiki/Operator-Tests-New-Java-API].
Please review, comment, extend.

Since this is a larger effort, it would be cool, if some of you guys would pick one or two operators and write tests for them. 
I started with the tests for [**Filter**|https://github.com/stratosphere/stratosphere/blob/a7d10395058f21f1c66375857c92544ef2a78ce9/stratosphere-tests/src/test/java/eu/stratosphere/test/javaApiOperators/FilterITCase.java] and will continue with tests for **Reduce**.

If you decide to test an operator, have a look at the [filter tests|https://github.com/stratosphere/stratosphere/blob/a7d10395058f21f1c66375857c92544ef2a78ce9/stratosphere-tests/src/test/java/eu/stratosphere/test/javaApiOperators/FilterITCase.java] and drop a line, so that it is clear what is currently being worked on.;;;","09/Jun/14 12:30;github-import;[Date: Tue Apr 08 14:52:02 CEST 2014, Author: [vasia|https://github.com/vasia]]

Hey! I can work on this, e.g. for Map and FlatMap to start with. Is that OK?;;;","09/Jun/14 12:30;github-import;[Date: Tue Apr 08 15:17:47 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Yes, definitely! That would be great! More tests a always very welcome! :smile:;;;","09/Jun/14 12:30;github-import;[Date: Wed Apr 09 22:36:53 CEST 2014, Author: [vasia|https://github.com/vasia]]

Hey @fhueske, I have pushed my Map operator test [here|https://github.com/vasia/ozone/blob/japi_operator_tests/stratosphere-tests/src/test/java/eu/stratosphere/test/javaApiOperators/MapITCase.java). 
Could you take a look and let me know if that's what you had in mind? If yes, I'll continue with the FlatMap operator :] Thanks!;;;","09/Jun/14 12:30;github-import;[Date: Thu Apr 10 11:34:22 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Hi @vasia,
looks very good to me! Thank you!

I'll be working on tests for Join.;;;","09/Jun/14 12:30;github-import;[Date: Thu Apr 10 12:43:57 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I started working on cogroup;;;","09/Jun/14 12:30;github-import;[Date: Thu Apr 10 13:16:48 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

@fhueske In your wiki page you wrote ""check correctness of cogroup a tuple input with key field selector and a custom type input with key extractor""
Currently there is no support for mixing key extractor and field position keys. Should this be implemented?;;;","09/Jun/14 12:30;github-import;[Date: Thu Apr 10 13:42:21 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Yes, I think we should support that. ;;;","09/Jun/14 12:30;github-import;[Date: Thu Apr 10 13:57:45 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I opened an issue for that: ([#675|https://github.com/stratosphere/stratosphere/issues/675] | [FLINK-675|https://issues.apache.org/jira/browse/FLINK-675]) 
;;;","09/Jun/14 12:30;github-import;[Date: Fri Apr 11 14:00:34 CEST 2014, Author: [vasia|https://github.com/vasia]]

Hey, you can find the test for the FlatMap [here] (https://github.com/vasia/ozone/blob/japi_operator_tests/stratosphere-tests/src/test/java/eu/stratosphere/test/javaApiOperators/FlatMapITCase.java). Let me know if you want me to add/change annything, otherwise I'll create a PR for both Map and FlatMap tests. Thanks!;;;","09/Jun/14 12:30;github-import;[Date: Fri Apr 11 15:35:47 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Hi @vasia. Looks very good. Please create a PR.
Thank you very much!;;;","09/Jun/14 12:30;github-import;[Date: Mon Apr 14 14:16:08 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I started to work on cross.;;;","09/Jun/14 12:30;github-import;[Date: Mon Apr 14 16:05:43 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

Last but not least: groupReduce ;;;","09/Jun/14 12:30;github-import;[Date: Wed Apr 16 22:30:34 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

First of all, @markus-h and @vasia thanks for helping with the tests!

The current state of this issue is as follows:

Only one operator is not covered by integration tests:
- `JoinITCase`: I will take care of it. **DONE**

Unit tests to check for valid parameters similar to `GroupingTest` are missing for:
- Join **DONE**
- CoGroup

A few integration tests need some extensions:
- `CoGroupITCase`: left, right combinable CoGroup functions **NOT SUPPORTED**
- `FilterITCase`: broadcast variable **DONE**

There are also some tests that need to be extended but which require some other issues to be solve first:
- `GroupReduceITCase`: descending group sort waits for a fix of ([#588|https://github.com/stratosphere/stratosphere/issues/588] | [FLINK-588|https://issues.apache.org/jira/browse/FLINK-588]) 
- `GroupReduceITCase`: combinable GroupReduce function due to ([#700|https://github.com/stratosphere/stratosphere/issues/700] | [FLINK-700|https://issues.apache.org/jira/browse/FLINK-700])
- `CrossITCase` (and `JoinITCase`): custom objects are not supported due to ([#691|https://github.com/stratosphere/stratosphere/issues/691] | [FLINK-691|https://issues.apache.org/jira/browse/FLINK-691]) 

;;;","09/Jun/14 12:30;github-import;[Date: Tue Apr 22 18:44:12 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I think we don't need tests for combinable cogroup, because it is not supported yet. See ([#712|https://github.com/stratosphere/stratosphere/issues/712] | [FLINK-712|https://issues.apache.org/jira/browse/FLINK-712]);;;","09/Jun/14 12:30;github-import;[Date: Fri Apr 25 09:48:59 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Alright, we made some good progress (thanks @markus-h !)
The current status of this issues is as follows:

Unit tests to check for valid parameters similar to `GroupingTest` are missing for:
- CoGroup

There are also some tests that need to be extended but which require some other issues to be solve first:
- `GroupReduceITCase`: descending group sort waits for a fix of ([#588|https://github.com/stratosphere/stratosphere/issues/588] | [FLINK-588|https://issues.apache.org/jira/browse/FLINK-588]) 
- `GroupReduceITCase`: combinable GroupReduce function due to ([#700|https://github.com/stratosphere/stratosphere/issues/700] | [FLINK-700|https://issues.apache.org/jira/browse/FLINK-700])
- `CrossITCase` (and `JoinITCase`): custom objects are not supported due to ([#691|https://github.com/stratosphere/stratosphere/issues/691] | [FLINK-691|https://issues.apache.org/jira/browse/FLINK-691]) ;;;","09/Jun/14 12:30;github-import;[Date: Thu May 01 12:32:22 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Just realized, we do not have any tests for the Aggregate operator...
We need unit tests to check for correct parameters and integration tests for result correctness.
;;;","09/Jun/14 12:30;github-import;[Date: Thu May 01 12:37:45 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

agree;;;","09/Jun/14 12:30;github-import;[Date: Mon May 12 14:31:23 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

If we did not miss anything the only things left to close this issue are to enable some tests:

- `GroupReduceITCase`: descending group sort waits for a fix of ([#588|https://github.com/stratosphere/stratosphere/issues/588] | [FLINK-588|https://issues.apache.org/jira/browse/FLINK-588]) (@zentol: this is the test for sorting that I mentioned previously)
- `GroupReduceITCase`: combinable GroupReduce function due to ([#700|https://github.com/stratosphere/stratosphere/issues/700] | [FLINK-700|https://issues.apache.org/jira/browse/FLINK-700]);;;","09/Jun/14 12:30;github-import;[Date: Wed May 14 13:22:05 CEST 2014, Author: [zentol|https://github.com/zentol]]

case 12 (combinable) in GroupReduceITCase fails for me.
```
arrays first differed at element [0]; expected:<231,[91,Hello World]> but was:<231,[I am fine.]>
```
i reran it and got
```
arrays first differed at element [0]; expected:<231,[91,Hello World]> but was:<231,[Comment([#7|https://github.com/stratosphere/stratosphere/issues/7] | [FLINK-7|https://issues.apache.org/jira/browse/FLINK-7])]>
```

;;;","09/Jun/14 12:30;github-import;[Date: Thu May 15 11:47:48 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

The input data sets in `CollectionDataSets` are randomly shuffled. So the input looks a bit different everytime the test is executed. 
However, the result should be identical (maybe in different order). 

So there's still a bug somewhere.;;;","09/Jun/14 12:30;github-import;[Date: Thu May 15 12:57:25 CEST 2014, Author: [zentol|https://github.com/zentol]]

For some reason the combine method is never called.;;;","09/Jun/14 12:30;github-import;[Date: Thu May 15 13:22:51 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I am looking into it...;;;","09/Jun/14 12:30;github-import;[Date: Thu May 15 13:29:39 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Makes sense that the combiner is not called:

Source (DOP1, because collection) -> AllGRoupReduce(DOP1)

Makes no sense to inject a combiner.;;;","09/Jun/14 12:30;github-import;[Date: Thu May 15 23:46:21 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Finally, we have tests for all operators in place. Thanks @markus-h, @vasia and @StephanEwen!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnitTests for new Java API Runtime,FLINK-547,12719719,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:29,09/Jun/14 12:29,14/Jul/23 05:57,09/Jun/14 12:29,,,,pre-apache,,,,,,,0,github-import,,"The runtime code of the Java API needs to be extensively tested. 
This includes the Serializers and Comparators for the various data types found in the
[eu.stratosphere.api.java.typeutils.runtime|https://github.com/stratosphere/stratosphere/tree/master/stratosphere-java/src/main/java/eu/stratosphere/api/java/typeutils/runtime] package.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/547
Created by: [fhueske|https://github.com/fhueske]
Labels: java api, runtime, testing, 
Milestone: Release 0.5
Created at: Sat Mar 08 15:48:48 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397918,,,Mon Jun 09 12:29:57 UTC 2014,,,,,,,,,,"0|i1wi9r:",398045,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;[Date: Sat Mar 08 16:39:15 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

+1
I would suggest to add a test for every bug we ever see. For example your sorting bug should covered by a test.;;;","09/Jun/14 12:29;github-import;[Date: Mon Mar 10 13:15:32 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

+1;;;","09/Jun/14 12:29;github-import;[Date: Wed Apr 30 11:58:41 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

Tests for comparators are missing.;;;","09/Jun/14 12:29;github-import;[Date: Thu May 15 14:40:26 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Comparator tests are done ([#772|https://github.com/stratosphere/stratosphere/issues/772] | [FLINK-772|https://issues.apache.org/jira/browse/FLINK-772]).;;;","09/Jun/14 12:29;github-import;[Date: Thu May 15 14:42:22 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Are we also done with the serializer tests?;;;","09/Jun/14 12:29;github-import;[Date: Thu May 15 15:36:25 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Yep, they are done.;;;","09/Jun/14 12:29;github-import;[Date: Thu May 15 16:16:37 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

OK, let's close it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add RAT plugin in the MAVEN build. Also add licenses where-ever needed.,FLINK-546,12719718,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:29,09/Jun/14 12:29,14/Jul/23 05:57,09/Jun/14 12:29,,,,pre-apache,,,,,,,0,github-import,,"Hi,

I have added licenses where ever required and RAT plugin for the MAVEN build.



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/546
Created by: [zerolevel|https://github.com/zerolevel]
Labels: 
Created at: Sat Mar 08 09:59:25 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;pull-request-546-4181825038515391098.patch;https://issues.apache.org/jira/secure/attachment/12649198/pull-request-546-4181825038515391098.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397917,,,Mon Jun 09 12:29:50 UTC 2014,,,,,,,,,,"0|i1wi9j:",398044,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;[Date: Sat Mar 08 16:38:03 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for your contribution. I added some minor comments.

if you are a GSoC student, please remember to add your GitHub username to your project proposal so that we know which contribution was from you.;;;","09/Jun/14 12:29;github-import;[Date: Sun Mar 09 12:59:55 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I'll have a look soon.;;;","09/Jun/14 12:29;github-import;[Date: Sun Mar 09 18:15:52 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for your pull request. I merged it in [3205c527bb5c971a3f949d6f5ee7d2fb052883ef|https://github.com/stratosphere/stratosphere/commit/3205c527bb5c971a3f949d6f5ee7d2fb052883ef]. (I added one exclude for the Eclipse configuration files).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change the MutableObjectOperator to allow immutable objects and remove the Reference<T> hack,FLINK-545,12719717,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:29,09/Jun/14 12:29,14/Jul/23 05:57,09/Jun/14 12:29,,,,pre-apache,,,,,,,0,github-import,,"This is a big change, so we need several pairs of eyes to look over this.

The basic idea of the change is that instead of always requiring calling code to hand in a record to be filled we now hand in a reuse object that can be reused but does not have to be. The signature of relevant methods is changed so that they now return a ""record"" instead of a boolean. Before this, the boolean would tell whether the record had been filled. Now we return null if there is no more data, so calling code has to take care of preserving reuse objects if it needs them later.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/545
Created by: [aljoscha|https://github.com/aljoscha]
Labels: java api, 
Created at: Fri Mar 07 20:50:02 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;pull-request-545-2210556431007154896.patch;https://issues.apache.org/jira/secure/attachment/12649197/pull-request-545-2210556431007154896.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397916,,,Mon Jun 09 12:29:43 UTC 2014,,,,,,,,,,"0|i1wi9b:",398043,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;[Date: Sat Mar 08 01:00:44 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I had a look at the full PR. Only had a few remarks / questions.
Looks good to me otherwise.;;;","09/Jun/14 12:29;github-import;[Date: Sat Mar 15 23:20:08 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged into staging in [90c2285c2d876add48df25fe70a1bc111650da82|https://github.com/stratosphere/stratosphere/commit/90c2285c2d876add48df25fe70a1bc111650da82];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bug in MapNode,FLINK-544,12719716,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:29,09/Jun/14 12:29,14/Jul/23 05:57,09/Jun/14 12:29,,,,pre-apache,,,,,,,0,github-import,,"MapNode is responsible for PlainMapOperatorBase and not for
MapOperatorBase, but this is a mess right now.

Maybe we should think about cleaning the situation a bit. CollectorMap is the same as FlatMap, only the function in the interface is called map instead of flatMap. I know that this is because of historic reasons but it is still a mess because we handle all three types through the layers. For example, some code in PactCompiler looks like this:

```java
else if (c instanceof MapOperatorBase) {
	n = new CollectorMapNode((MapOperatorBase<?>) c);
}
else if (c instanceof PlainMapOperatorBase) {
	n = new MapNode((PlainMapOperatorBase<?>) c);
}
else if (c instanceof FlatMapOperatorBase) {
	n = new FlatMapNode((FlatMapOperatorBase<?>) c);
}
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/544
Created by: [aljoscha|https://github.com/aljoscha]
Labels: java api, 
Created at: Fri Mar 07 20:48:10 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;pull-request-544-8461467729629880819.patch;https://issues.apache.org/jira/secure/attachment/12649196/pull-request-544-8461467729629880819.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397915,,,Mon Jun 09 12:29:32 UTC 2014,,,,,,,,,,"0|i1wi93:",398042,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;[Date: Fri Mar 07 20:50:33 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I had a fix for this in ([#542|https://github.com/stratosphere/stratosphere/issues/542] | [FLINK-542|https://issues.apache.org/jira/browse/FLINK-542]) as well ;-);;;","09/Jun/14 12:29;github-import;[Date: Fri Mar 07 20:56:15 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Damn. 
I suggest to have faster merge cycles for JAPI related (notice the new label) pull requests. As there are more and more developers working on it, we should sort out bugs as fast as possible.

Can we agree to a 
2 x +1 (from a committer or @markus-h)  AND no complaints ==> merge
voting scheme ?;;;","09/Jun/14 12:29;github-import;[Date: Fri Mar 07 21:03:06 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I agree if the two +1 do not include the PR author. ;;;","09/Jun/14 12:29;github-import;[Date: Fri Mar 07 21:04:28 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay. Makes sense.;;;","09/Jun/14 12:29;github-import;[Date: Fri Mar 07 21:13:20 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I agree with cleaning up the situation. It also took me quite some time to figure out how the different MapOperators relate to each other...;;;","09/Jun/14 12:29;github-import;[Date: Fri Mar 14 02:09:14 CET 2014, Author: [fhueske|https://github.com/fhueske]]

The bug was fixed by another commit.
I'll open a new issue for the clean-up discussion and close this PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New Java API. Inverted Sort Order,FLINK-543,12719715,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:29,09/Jun/14 12:29,14/Jul/23 05:57,09/Jun/14 12:29,,,,pre-apache,,,,,,,0,github-import,,"``TupleComparator`` causes inverted sort order, i.e., DESCENDING instead of ASCENDING (and vice versa).

To reproduce emit the key invocation order of a Reduce function with an Integer as key (should be ascending by default but is descending). 
Might be a problem related to inverted normalized keys (similar to the bug fixed in db68dd8).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/543
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, java api, runtime, 
Created at: Fri Mar 07 18:24:54 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397914,,,Mon Jun 09 12:29:24 UTC 2014,,,,,,,,,,"0|i1wi8v:",398041,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;[Date: Wed Mar 12 17:32:37 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed in ([#542|https://github.com/stratosphere/stratosphere/issues/542] | [FLINK-542|https://issues.apache.org/jira/browse/FLINK-542]) ;;;","09/Jun/14 12:29;github-import;[Date: Sun Mar 16 11:31:34 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed in staging branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug fixes for and extensions of new Java API,FLINK-542,12719714,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:29,09/Jun/14 12:29,14/Jul/23 05:57,09/Jun/14 12:29,,,,pre-apache,,,,,,,0,github-import,,"- Fix for Map operator translation (introduced by merge of 0038c9d0773e90676)
- Fix for ``TupleComparator`` (normalized key writing) @stephanewen please verify
- Fix for ``BasicTypeComparator`` (inverted sort order) @StephanEwen please verify
- initial support for Group Sorting (position keys and Reduce operator only)
- extended ``TypeExtractor`` and ``TupleTypeInfo`` to work with subclasses of Tuple classes.
- added join function for semi joins. Support in runtime required to enable this feature.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/542
Created by: [fhueske|https://github.com/fhueske]
Labels: java api, 
Created at: Fri Mar 07 18:12:33 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;pull-request-542-4847117911152583770.patch;https://issues.apache.org/jira/secure/attachment/12649195/pull-request-542-4847117911152583770.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397913,,,Mon Jun 09 12:29:20 UTC 2014,,,,,,,,,,"0|i1wi8n:",398040,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;[Date: Wed Mar 12 17:33:05 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Fixes ([#543|https://github.com/stratosphere/stratosphere/issues/543] | [FLINK-543|https://issues.apache.org/jira/browse/FLINK-543]) ;;;","09/Jun/14 12:29;github-import;[Date: Thu Mar 13 09:48:11 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Rebased to stratosphere:staging branch in ([#565|https://github.com/stratosphere/stratosphere/issues/565] | [FLINK-565|https://issues.apache.org/jira/browse/FLINK-565]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add RAT plugin to MAVEN build. Also adds licenses where-ever required.,FLINK-541,12719713,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:29,09/Jun/14 12:29,14/Jul/23 05:57,09/Jun/14 12:29,,,,pre-apache,,,,,,,0,github-import,,"build using maven.

and then 
mvn org.apache.rat:apache-rat-plugin:check
this checks all the licenses.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/541
Created by: [zerolevel|https://github.com/zerolevel]
Labels: 
Created at: Fri Mar 07 16:46:56 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;pull-request-541-6383296970160076590.patch;https://issues.apache.org/jira/secure/attachment/12649194/pull-request-541-6383296970160076590.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397912,,,Mon Jun 09 12:29:15 UTC 2014,,,,,,,,,,"0|i1wi8f:",398039,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;[Date: Fri Mar 07 16:51:39 CET 2014, Author: [zerolevel|https://github.com/zerolevel]]

There was some error related to the push. Please ignore this.;;;","09/Jun/14 12:29;github-import;[Date: Fri Mar 07 17:06:56 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hey, you can update pending pull requests (you can either add new commits to the PR or force push to replace existing commits);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reuse of already generated OptimizerNodes in the recursive descent step of GraphCreatingVisitor,FLINK-540,12719712,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:29,09/Jun/14 12:29,14/Jul/23 05:57,09/Jun/14 12:29,,,,pre-apache,,,,,,,0,github-import,,"I tried to tackle the problem of generating multiple OptimizerNodes for one and the same Operator during recursive descent steps of the GraphCreatingVisitor as they happen for the step function generation of iterations. This poses a problem, because flows with multiple sinks, where one of the inputs to the step function is outputted as well, will be wrongly recognised as an unconnected data flow. Moreover, in the case of an Operator which produces random results, the generated data of this Operator inside and outside of the iteration would be distinct and thus false.

My solution is to forward the generated OptimizerNodes to the recursive GraphCreatingVisitor. This implies, however, to consider the step function for the computation of the unclosed branch stack. Furthermore, the branch plans of the step function has to be taken account of as well when merging the branch plans of the iteration plan nodes. For the WorksetIterationPlanNode, I could only come up with a nasty hack to disable the mergeBranchPlanMaps of the DualInputPlanNode.

So far, all test cases succeed locally but I'm not sure whether I found all code positions requiring a change for this modification. Would be great if you could review it to make sure that I didn't forget anything.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/540
Created by: [tillrohrmann|https://github.com/tillrohrmann]
Labels: 
Created at: Fri Mar 07 10:56:41 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;pull-request-540-6544000532687122994.patch;https://issues.apache.org/jira/secure/attachment/12649193/pull-request-540-6544000532687122994.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397911,,,Mon Jun 09 12:29:08 UTC 2014,,,,,,,,,,"0|i1wi87:",398038,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:29;github-import;[Date: Sun Mar 09 16:12:11 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi @tillrohrmann,
thank you for your contribution.
We'll have a look at your pull request as soon as we find time.;;;","09/Jun/14 12:29;github-import;[Date: Tue Mar 11 10:25:42 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Thank you, the pull request looks good.

There are two things where I am wondering about implications:
  - The logic that deals with branching and rejoining data flows may be affected by this. To verify that, we would need as test with a data flow as shown further below.
  - The visualization of iteration plans might be broken for such plans. This issue is less critical, as we are going to replace the visualization library anyways in the near future.

```
                 +----Iteration------+
                 |                   |
        /--map--< >----\             |
       /         |      \         /-< >---sink
src-map          |     join------/   |
       \         |      /            |
        \        +-----/-------------+
         \            /
          \--reduce--/
```;;;","09/Jun/14 12:29;github-import;[Date: Tue Mar 11 12:19:09 CET 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

I added the test case for the bulk iteration and the delta iteration. They pass on my computer.;;;","09/Jun/14 12:29;github-import;[Date: Tue Mar 11 15:22:15 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

That is good news :-) It think it is good to merge then. Can you include
the test in your pull request? You only need to update the branch from
which you created the pull request.;;;","09/Jun/14 12:29;github-import;[Date: Tue Mar 11 15:48:39 CET 2014, Author: [tillrohrmann|https://github.com/tillrohrmann]]

They should be included. I appended them to the class BranchingPlansCompilerTest.;;;","09/Jun/14 12:29;github-import;[Date: Sat Mar 15 23:47:26 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Very nice work.

The only comment I have is to not use wildcard imports. We try to keep them out of the core code to prevent name clashes that may be overlooked.

Merging it now and testing...;;;","09/Jun/14 12:29;github-import;[Date: Sun Mar 23 13:46:27 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [f05d24ad61dcda29affe67bacd61926a657bb77c|https://github.com/stratosphere/stratosphere/commit/f05d24ad61dcda29affe67bacd61926a657bb77c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[yarn] Major YARN-Client improvements,FLINK-539,12719711,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:28,09/Jun/14 12:29,14/Jul/23 05:57,09/Jun/14 12:29,,,,pre-apache,,,,,,,0,github-import,,"- HDFS security token support (resolves https://github.com/stratosphere/stratosphere/issues/492)
- ""ship/"" directory to transfer files to all TaskManagers (user-files)
- Log4j-based logging (YARN now respects the logging configuration) (resolves https://github.com/stratosphere/stratosphere/issues/493)
- the YARN-client deletes all ""temp"" files from HDFS.
- The JVMs started by YARN now respect the configured JVM opts in the yaml-file (resolves https://github.com/stratosphere/stratosphere/issues/501)
- the JobManager webinterface shows the log file (e.g it is aware of the YARN-specific log-directory)
- The YARN-client now creates a hidden "".yarn-jobmanager"" with the address of the JobManager in YARN. users do not have to specify the -m argument anymore.
- Fix a little bug with the JobManager's ""cloud"" model for taskManager with less that 1GB memory.
- TM ports are now assigned based on the ports availability on the machine. (Users can still ""hardcode"" a port)
- Tested on Cloudera Hadoop 5 Beta 2
- Tested on Cloduera Hadoop 5 Beta 2 WITH CDH5-B2 Maven Dependencies.
- Tested on Hadoop 2.2.0
- Tested on Hadoop 2.3.0 (resolves https://github.com/stratosphere/stratosphere/issues/500)
- Tested on Amazon EMR

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/539
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Thu Mar 06 19:43:38 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:28;github-import;pull-request-539-7829054324230204956.patch;https://issues.apache.org/jira/secure/attachment/12649192/pull-request-539-7829054324230204956.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397910,,,Mon Jun 09 12:29:00 UTC 2014,,,,,,,,,,"0|i1wi7z:",398037,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:28;github-import;[Date: Thu Mar 06 19:49:41 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Wow, impressive!
Am 06.03.2014 19:43 schrieb ""Robert Metzger"" <notifications@github.com>:

>
>    - HDFS security token support (resolves ([#492|https://github.com/stratosphere/stratosphere/issues/492] | [FLINK-492|https://issues.apache.org/jira/browse/FLINK-492])<https://github.com/stratosphere/stratosphere/issues/492>
>    )
>    - ""ship/"" directory to transfer files to all TaskManagers (user-files)
>    - Log4j-based logging (YARN now respects the logging configuration)
>    (resolves ([#493|https://github.com/stratosphere/stratosphere/issues/493] | [FLINK-493|https://issues.apache.org/jira/browse/FLINK-493])<https://github.com/stratosphere/stratosphere/issues/493>
>    )
>    - the YARN-client deletes all ""temp"" files from HDFS.
>    - The JVMs started by YARN now respect the configured JVM opts in the
>    yaml-file (resolves ([#501|https://github.com/stratosphere/stratosphere/issues/501] | [FLINK-501|https://issues.apache.org/jira/browse/FLINK-501])<https://github.com/stratosphere/stratosphere/issues/501>
>    )
>    - the JobManager webinterface shows the log file (e.g it is aware of
>    the YARN-specific log-directory)
>    - The YARN-client now creates a hidden "".yarn-jobmanager"" with the
>    address of the JobManager in YARN. users do not have to specify the -m
>    argument anymore.
>    - Fix a little bug with the JobManager's ""cloud"" model for taskManager
>    with less that 1GB memory.
>    - TM ports are now assigned based on the ports availability on the
>    machine. (Users can still ""hardcode"" a port)
>    - Tested on Cloudera Hadoop 5 Beta 2
>    - Tested on Cloduera Hadoop 5 Beta 2 WITH CDH5-B2 Maven Dependencies.
>    - Tested on Hadoop 2.2.0
>    - Tested on Hadoop 2.3.0 (resolves ([#500|https://github.com/stratosphere/stratosphere/issues/500] | [FLINK-500|https://issues.apache.org/jira/browse/FLINK-500])<https://github.com/stratosphere/stratosphere/issues/500>
>    )
>    - Tested on Amazon EMR
>
> ------------------------------
> You can merge this Pull Request by running
>
>   git pull https://github.com/rmetzger/stratosphere yarn_security_pr
>
> Or view, comment on, or merge it at:
>
>   https://github.com/stratosphere/stratosphere/pull/539
> Commit Summary
>
>    - [yarn] Major YARN-Client improvements
>
> File Changes
>
>    - *M* stratosphere-addons/yarn/pom.xml<https://github.com/stratosphere/stratosphere/pull/539/files#diff-0>(6)
>    - *M*
>    stratosphere-addons/yarn/src/main/java/eu/stratosphere/yarn/ApplicationMaster.java<https://github.com/stratosphere/stratosphere/pull/539/files#diff-1>(94)
>    - *M*
>    stratosphere-addons/yarn/src/main/java/eu/stratosphere/yarn/Client.java<https://github.com/stratosphere/stratosphere/pull/539/files#diff-2>(174)
>    - *M*
>    stratosphere-addons/yarn/src/main/java/eu/stratosphere/yarn/Utils.java<https://github.com/stratosphere/stratosphere/pull/539/files#diff-3>(44)
>    - *M*
>    stratosphere-clients/src/main/java/eu/stratosphere/client/CliFrontend.java<https://github.com/stratosphere/stratosphere/pull/539/files#diff-4>(32)
>    - *M*
>    stratosphere-core/src/main/java/eu/stratosphere/configuration/ConfigConstants.java<https://github.com/stratosphere/stratosphere/pull/539/files#diff-5>(11)
>    - *M* stratosphere-dist/src/main/assemblies/yarn.xml<https://github.com/stratosphere/stratosphere/pull/539/files#diff-6>(11)
>    - *M*
>    stratosphere-dist/src/main/stratosphere-bin/conf/stratosphere-conf.yaml<https://github.com/stratosphere/stratosphere/pull/539/files#diff-7>(3)
>    - *M*
>    stratosphere-dist/src/main/stratosphere-bin/yarn-bin/yarn-session.sh<https://github.com/stratosphere/stratosphere/pull/539/files#diff-8>(2)
>    - *M*
>    stratosphere-runtime/src/main/java/eu/stratosphere/nephele/instance/InstanceConnectionInfo.java<https://github.com/stratosphere/stratosphere/pull/539/files#diff-9>(4)
>    - *M*
>    stratosphere-runtime/src/main/java/eu/stratosphere/nephele/jobmanager/web/LogfileInfoServlet.java<https://github.com/stratosphere/stratosphere/pull/539/files#diff-10>(7)
>    - *M*
>    stratosphere-runtime/src/main/java/eu/stratosphere/nephele/jobmanager/web/WebInfoServer.java<https://github.com/stratosphere/stratosphere/pull/539/files#diff-11>(4)
>    - *M*
>    stratosphere-runtime/src/main/java/eu/stratosphere/nephele/taskmanager/TaskManager.java<https://github.com/stratosphere/stratosphere/pull/539/files#diff-12>(51)
>
> Patch Links:
>
>    - https://github.com/stratosphere/stratosphere/pull/539.patch
>    - https://github.com/stratosphere/stratosphere/pull/539.diff
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/539>
> .
>;;;","09/Jun/14 12:28;github-import;[Date: Thu Mar 06 20:09:50 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think it is good to merge.;;;","09/Jun/14 12:29;github-import;[Date: Thu Mar 06 20:12:03 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay, I'll push it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up LICENSE and NOTICE files,FLINK-535,12719707,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:28,09/Jun/14 12:28,14/Jul/23 05:57,09/Jun/14 12:28,,,,pre-apache,,,,,,,0,github-import,,"This commit is from @uce.

1. Added LICENSE and NOTICE files to the source tree
=> we now have two files, one for the source, one for the dist

2. Renamed LICENSE.txt and NOTICE.txt without the .txt 
=> This seems to be the common naming.

3. Removed swt-visualization project, because it used LGPL 2.1 dependencies and nobody really uses it. (We do not support it)

5. Removed the notices about the usage of other Apache licensed SW, because I am not sure if we need it

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/535
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Thu Mar 06 11:47:49 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:28;github-import;pull-request-535-5423232166598263002.patch;https://issues.apache.org/jira/secure/attachment/12649191/pull-request-535-5423232166598263002.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397906,,,Mon Jun 09 12:28:36 UTC 2014,,,,,,,,,,"0|i1wi73:",398033,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:28;github-import;[Date: Thu Mar 06 11:48:19 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I think we should keep the contents of the old NOTICE file.;;;","09/Jun/14 12:28;github-import;[Date: Sat Mar 15 22:23:24 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The old Notice file had an incomplete list of license dependencies. Ufuk's new LICENSE file has a complete list.

I'll add a note in the NOTICE file that a list of dependencies with their licenses is in the LICENSE file;;;","09/Jun/14 12:28;github-import;[Date: Sat Mar 15 22:40:13 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [551e83f8b04be549d5750477da814370e2def541|https://github.com/stratosphere/stratosphere/commit/551e83f8b04be549d5750477da814370e2def541] and [2d0975b028dc457f9b1f36bce6cb538a7d484967|https://github.com/stratosphere/stratosphere/commit/2d0975b028dc457f9b1f36bce6cb538a7d484967];;;","09/Jun/14 12:28;github-import;[Date: Sun Mar 16 10:58:45 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Why did you merge it into `staging` ?
I though the branch is only for the new JavaAPI ?
Actually, `master` is kind of a staging area for releases. If we have the feeling that too many users are using `master` we should release a new version.;;;","09/Jun/14 12:28;github-import;[Date: Sun Mar 16 11:49:28 CET 2014, Author: [fhueske|https://github.com/fhueske]]

The idea of the staging branch was to preserve the stability of the master branch, mainly because of the complex PR ([#545|https://github.com/stratosphere/stratosphere/issues/545] | [FLINK-545|https://issues.apache.org/jira/browse/FLINK-545]). @StephanEwen asked to do all PRs and merges against the staging branch to avoid merge conflicts when staging goes into master.
Once staging is merged with master, we will again work directly on master.;;;","09/Jun/14 12:28;github-import;[Date: Sun Mar 16 11:54:24 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Oh, I got that wrong. I merged some PR's into `master` recently.

What's the problem with rebasing `staging` before merging it to master ?

I'm against having a `staging` branch. It makes our development process too complicated.
New features should be developed ""somewhere"" until they are stable enough, then, they should come into the mainline.
If the new Java API is not stable enough for `master` we should have kept it in @StephanEwen's repo  and make the pull requests there.

;;;","09/Jun/14 12:28;github-import;[Date: Sun Mar 16 12:04:30 CET 2014, Author: [fhueske|https://github.com/fhueske]]

The problem with rebasing staging is the hugh changelist by PR ([#545|https://github.com/stratosphere/stratosphere/issues/545] | [FLINK-545|https://issues.apache.org/jira/browse/FLINK-545]).
As I said, we will continue to work on master once staging is merged.

I think it's OK to have unstable new features in master but we should be careful with things that people are currently relying on (therefore the staging branch). 
What would be the difference of having a branch in @StephanEwen's repository (to which only he can commit) and a temp branch in the @stratosphere repository?;;;","09/Jun/14 12:28;github-import;[Date: Sun Mar 16 12:05:02 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It's only to avoid merge conflicts. We should merge staging tomorrow, if
Tobias' Tests work. Staging will then have existed for a week only.

Not all people were aware of the separate branch in my repo.;;;","09/Jun/14 12:28;github-import;[Date: Sun Mar 16 19:44:47 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

> What would be the difference of having a branch in @StephanEwen's repository (to which only he can commit)

That we follow a development process that most of the other open source projects follow. (Something like a design pattern)
The main repo only contains code that is considered stable and is reviewed.
Adding contributors to a repository is really easy ;) 
We could also create a separate repository in @stratosphere for the new Java API.

> We should merge staging tomorrow, if Tobias' Tests work.

Who is Tobias?

> Not all people were aware of the separate branch in my repo.

I understand. But there is an issue (https://github.com/stratosphere/stratosphere/issues/463) for planning the new Java API and you could have used that to promote your branch and find contributors. (Sorry, I don't want to annoy you. It is about reflecting our development process)
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bad nightly build for stratosphere-0.5-hadoop2,FLINK-533,12719705,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:27,09/Jun/14 12:27,14/Jul/23 05:57,09/Jun/14 12:27,,,,pre-apache,,,,,,,0,github-import,,"http://stratosphere-bin.s3-website-us-east-1.amazonaws.com/
Tonights build of stratosphere-0.5-hadoop2  was just 122 bytes of binaries.  Is it Travis bug, or something else?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/533
Created by: [MikhailErofeev|https://github.com/MikhailErofeev]
Labels: bug, 
Created at: Wed Mar 05 15:38:24 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397904,,,Mon Jun 09 12:27:38 UTC 2014,,,,,,,,,,"0|i1wi6n:",398031,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:27;github-import;[Date: Wed Mar 05 17:29:51 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hey,
thanks for reporting the issue!

According to the `tools/deploy_to_maven.sh` script
```
# Build Responsibilities
# 1. Deploy to sonatype (old hadoop)
# 2. Nothing
# 3. Deploy to s3 (old hadoop)
# 4. deploy to sonatype (yarn hadoop) (this build will also generate specific poms for yarn hadoop)
# 5. Nothing
# 6. deploy to s3 (yarn hadoop)
```

It was machine 6 (https://travis-ci.org/stratosphere/stratosphere/jobs/20121364) that was responsible to deploy the file.
It seems that https://s3.amazonaws.com/archive.travis-ci.org/jobs/20121364/log.txt copying the file did not work
```
cp: cannot stat `stratosphere-dist/target/stratosphere-dist-0.5-hadoop2-SNAPSHOT-bin/stratosphere-0.5-hadoop2-SNAPSHOT/*': No such file or directory
```

I'll push a fix in a minute.
;;;","09/Jun/14 12:27;github-import;[Date: Wed Mar 05 18:46:46 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thanks again. I pushed a fix in [f8a81732afaa3d48ace99fc3f5ed01b5ef334a2d|https://github.com/stratosphere/stratosphere/commit/f8a81732afaa3d48ace99fc3f5ed01b5ef334a2d].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopDataSink works both for hadoop 1.2.1 and 2.2.0,FLINK-531,12719703,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:27,09/Jun/14 12:27,14/Jul/23 05:57,09/Jun/14 12:27,,,,pre-apache,,,,,,,0,github-import,,"issue: ([#455|https://github.com/stratosphere/stratosphere/issues/455] | [FLINK-455|https://issues.apache.org/jira/browse/FLINK-455])
Need to delete temporary output folder when job finishes.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/531
Created by: [qmlmoon|https://github.com/qmlmoon]
Labels: 
Created at: Tue Mar 04 15:50:15 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:27;github-import;pull-request-531-6721469423059230673.patch;https://issues.apache.org/jira/secure/attachment/12649189/pull-request-531-6721469423059230673.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397902,,,Mon Jun 09 12:27:29 UTC 2014,,,,,,,,,,"0|i1wi67:",398029,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:27;github-import;[Date: Tue Mar 04 23:38:05 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for your pull request. I'll take a look as soon as I find time.

Could you add a test case that tests both the Hadoop Input / Output format? ;;;","09/Jun/14 12:27;github-import;[Date: Thu Mar 06 12:49:07 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Have you noticed the failed builds? https://travis-ci.org/stratosphere/stratosphere/builds/20147138
Log file: https://s3.amazonaws.com/archive.travis-ci.org/jobs/20147139/log.txt;;;","09/Jun/14 12:27;github-import;[Date: Thu Mar 06 12:55:25 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

You said `Need to delete temporary output folder when job finishes.`.

Isn't it possible to move the finished parts in the close() method to the final destination (HDFS) ? Once the job has finished, all files should be gone in the tmp dir?;;;","09/Jun/14 12:27;github-import;[Date: Thu Mar 06 13:19:56 CET 2014, Author: [qmlmoon|https://github.com/qmlmoon]]

The problem is the tmp dir itself cannot be deleted in close() if still
some tasks are running.


2014-03-06 12:55 GMT+01:00 Robert Metzger <notifications@github.com>:

> You said Need to delete temporary output folder when job finishes..
>
> Isn't it possible to move the finished parts in the close() method to the
> final destination (HDFS) ? Once the job has finished, all files should be
> gone in the tmp dir?
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/531#issuecomment-36849449>
> .
>;;;","09/Jun/14 12:27;github-import;[Date: Thu Mar 06 13:33:23 CET 2014, Author: [qmlmoon|https://github.com/qmlmoon]]

Ah. The failed builds is due to the dependency. That's also something i
want to ask:). There's no hadoop-compatibility dependency under
sratosphere-test package, should I put the test file also in
hadoop-compatibility package or?


2014-03-06 12:49 GMT+01:00 Robert Metzger <notifications@github.com>:

> Have you noticed the failed builds?
> https://travis-ci.org/stratosphere/stratosphere/builds/20147138
> Log file:
> https://s3.amazonaws.com/archive.travis-ci.org/jobs/20147139/log.txt
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/531#issuecomment-36847938>
> .
>;;;","09/Jun/14 12:27;github-import;[Date: Thu Mar 06 19:45:55 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,
yes, put the tests into the ""hadoop-compatibility"" package.
For spargel, we added the tests like this: (https://github.com/stratosphere/stratosphere/blob/master/stratosphere-addons/spargel/pom.xml)
```xml
		<dependency>
			<groupId>eu.stratosphere</groupId>
			<artifactId>stratosphere-tests</artifactId>
			<version>${project.version}</version>
			<scope>test</scope>
			<type>test-jar</type>
		</dependency>
```;;;","09/Jun/14 12:27;github-import;[Date: Sun Mar 23 13:17:40 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

@rmetzger Is this good to merge?;;;","09/Jun/14 12:27;github-import;[Date: Sun Mar 23 15:00:09 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I want to test it first on the cluster. I try to do that tomorrow.;;;","09/Jun/14 12:27;github-import;[Date: Sat Mar 29 13:17:15 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I started working on this. I have to fix a few things since the code does not check the user input and is causing null pointer exceptions at runtime (even in local mode).
I will work on this on Monday and then push the change.;;;","09/Jun/14 12:27;github-import;[Date: Mon Mar 31 17:07:26 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

I merged this pull request in [cc03f84450169bbcb36630569f4e70258e60bb51|https://github.com/stratosphere/stratosphere/commit/cc03f84450169bbcb36630569f4e70258e60bb51].

I also added code to check if the user has properly set the output directory ([d61d87f3df85eefe4848c3fd97ba9fb2495d748e|https://github.com/stratosphere/stratosphere/commit/d61d87f3df85eefe4848c3fd97ba9fb2495d748e]) so that we throw the exception at compile time (and not at runtime).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed Issue #504,FLINK-530,12719702,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:27,09/Jun/14 12:27,14/Jul/23 05:57,09/Jun/14 12:27,,,,pre-apache,,,,,,,0,github-import,,"Hi,

I managed to find a way to fix this issue my increase the size where the navbar starts to collapse. This is needed because we have to many items in our top-navigation bar ... . .
![bildschirmfoto 2014-03-03 um 19 01 56|https://f.cloud.github.com/assets/5738978/2312584/ff859d48-a2fd-11e3-9e57-76563a9b667c.png]


fixes ([#5|https://github.com/stratosphere/stratosphere/issues/5] | [FLINK-5|https://issues.apache.org/jira/browse/FLINK-5])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/530
Created by: [JonathanH5|https://github.com/JonathanH5]
Labels: 
Created at: Mon Mar 03 19:05:26 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:27;github-import;pull-request-530-5754134715106266715.patch;https://issues.apache.org/jira/secure/attachment/12649188/pull-request-530-5754134715106266715.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397901,,,Mon Jun 09 12:27:19 UTC 2014,,,,,,,,,,"0|i1wi5z:",398028,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:27;github-import;[Date: Mon Mar 03 19:31:39 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for your pull request.
There is a new issue with a width between 880 - 991 px (inclusive)

![bug|https://f.cloud.github.com/assets/89049/2312878/095552d8-a302-11e3-819e-5c464c913a20.png]
;;;","09/Jun/14 12:27;github-import;[Date: Tue Mar 04 13:08:43 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Please reopen the pull request against the new website repository (https://github.com/stratosphere/stratosphere.github.io) See also https://github.com/stratosphere/stratosphere/issues/487;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KMeans Quickstart Example not working,FLINK-529,12719701,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:27,09/Jun/14 12:27,14/Jul/23 05:57,09/Jun/14 12:27,,,,pre-apache,,,,,,,0,github-import,,"The Quickstart example depends on the SNAPSHOT Version.

A user reported that the example is currently not working
```
16:41:25,204 ERROR eu.stratosphere.pact.runtime.task.RegularPactTask             - Error in task code:  CHAIN DataSource(Centers) -> Map(Build cluster points) (2/2)
eu.stratosphere.api.common.io.ParseException: Row too short: 7|38.81|47.06|
        at eu.stratosphere.api.common.io.GenericCsvInputFormat.parseRecord(GenericCsvInputFormat.java:239)
 ```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/529
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, invalid, website, 
Created at: Mon Mar 03 18:18:12 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397900,,,2014-06-09 12:27:13.0,,,,,,,,,,"0|i1wi5r:",398027,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compiler deadlock detection,FLINK-527,12719699,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:27,09/Jun/14 12:27,14/Jul/23 05:57,09/Jun/14 12:27,,,,pre-apache,,,,,,,0,github-import,,"I implemented a deadlock detection inside the compiler. For details see ([#516|https://github.com/stratosphere/stratosphere/issues/516] | [FLINK-516|https://issues.apache.org/jira/browse/FLINK-516]). Basically I build a dependency graph and check for cycles. If there is a cycle I try to fix the deadlock by first trying to switch build and probe side of hash joins. If that does not work I insert pipeline breakers and force materialization.

I am not 100% sure that all cases of deadlocks in combinations with iterations are detected. Especially I couldn't take care of deadlocks that occure in combination with the termination criterion of the BulkIteration, since it is not yet part of the master.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/527
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon Mar 03 14:28:25 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:27;github-import;pull-request-527-7351174888970989499.patch;https://issues.apache.org/jira/secure/attachment/12649187/pull-request-527-7351174888970989499.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397898,,,Mon Jun 09 12:27:06 UTC 2014,,,,,,,,,,"0|i1wi5b:",398025,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:27;github-import;[Date: Mon Mar 03 15:06:13 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good to me! I have two minor suggestions:

  1. Can you rename the subpackage `deadlock` to `deadlockdetect` ?
  2. Can you make the deadlock detector its own class? I know the other steps are realized as internal classes, but I would suggest to go away from that pattern a bit. The compiler class is too huge already.;;;","09/Jun/14 12:27;github-import;[Date: Mon Mar 03 15:24:40 CET 2014, Author: [markus-h|https://github.com/markus-h]]

I integrated your proposed changes.
Moreover I tested the detection with our big testjob and it is working. The manual compiler hint that was needed before ist now superfluous.;;;","09/Jun/14 12:27;github-import;[Date: Mon Mar 03 18:13:57 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

That is amazing :-);;;","09/Jun/14 12:27;github-import;[Date: Tue Mar 04 01:24:03 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [32b218535f0bd4ebc99126a092bdea53ad26fee1|https://github.com/stratosphere/stratosphere/commit/32b218535f0bd4ebc99126a092bdea53ad26fee1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BC Vars Branching Test,FLINK-526,12719698,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:26,09/Jun/14 12:27,14/Jul/23 05:57,09/Jun/14 12:27,,,,pre-apache,,,,,,,0,github-import,,"Implements the test case for BC vars branching described in ([#491|https://github.com/stratosphere/stratosphere/issues/491] | [FLINK-491|https://issues.apache.org/jira/browse/FLINK-491]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/526
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Mon Mar 03 10:30:12 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:26;github-import;pull-request-526-4811449909977336034.patch;https://issues.apache.org/jira/secure/attachment/12649186/pull-request-526-4811449909977336034.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397897,,,Mon Jun 09 12:26:58 UTC 2014,,,,,,,,,,"0|i1wi53:",398024,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:26;github-import;[Date: Mon Mar 03 12:37:24 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

Hm, strage... I thought the test should fail at this point. Am I missing something?;;;","09/Jun/14 12:26;github-import;[Date: Mon Mar 03 13:02:24 CET 2014, Author: [twalthr|https://github.com/twalthr]]

Stephan already implemented branching support over the weekend.;;;","09/Jun/14 12:26;github-import;[Date: Tue Mar 04 01:24:19 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [2afb4941b3419805aaccf1d841639d54ad8c3f33|https://github.com/stratosphere/stratosphere/commit/2afb4941b3419805aaccf1d841639d54ad8c3f33];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
#519 Add setValue methods to Value types,FLINK-525,12719697,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:26,09/Jun/14 12:26,14/Jul/23 05:57,09/Jun/14 12:26,,,,pre-apache,,,,,,,0,github-import,,"add ResettableValue interface to base values and refactor setValue() in StringValue 

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/525
Created by: [MikhailErofeev|https://github.com/MikhailErofeev]
Labels: 
Created at: Sun Mar 02 20:30:42 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:26;github-import;pull-request-525-4721227430171742624.patch;https://issues.apache.org/jira/secure/attachment/12649185/pull-request-525-4721227430171742624.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397896,,,Mon Jun 09 12:26:52 UTC 2014,,,,,,,,,,"0|i1wi4v:",398023,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:26;github-import;[Date: Sun Mar 02 22:23:37 CET 2014, Author: [MikhailErofeev|https://github.com/MikhailErofeev]]

oops, Travis gone red at 'stratosphere-tests', with strange error. 

""eu.stratosphere.client.program.ProgramInvocationException: The program execution failed: java.lang.NoSuchMethodError: eu.stratosphere.types.IntValue.setValue(I)V
	at eu.stratosphere.examples.scala.datamining.KMeansForTest$UDTSerializerImpl$2.serialize(KMeansForTest.scala:79)
	at eu.stratosphere.examples.scala.datamining.KMeansForTest$UDTSerializerImpl$2.serialize(KMeansForTest.scala)
	at eu.stratosphere.examples.scala.datamining.KMeansForTest$$anon$8.readRecord(KMeansForTest.scala:79)
	at eu.stratosphere.examples.scala.datamining.KMeansForTest$$anon$8.readRecord(KMeansForTest.scala:79)
	at eu.stratosphere.api.common.io.DelimitedInputFormat.nextRecord(DelimitedInputFormat.java:449)
	at eu.stratosphere.pact.runtime.task.DataSourceTask.invoke(DataSourceTask.java:151)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:345)
	at java.lang.Thread.run(Thread.java:744)""

why KMeansForTest commented? what is the UDTSerializerImpl? where are in code invocation of  setValue() ?

;;;","09/Jun/14 12:26;github-import;[Date: Mon Mar 03 10:09:27 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Thanks a lot for your pull request and welcome!

The Scala API automatically generates data structures (List, Records, etc.) based on the basic data types (IntValue, StringValue, ...). The UDTSerializerImpl is auto-generated code to read and write one of these data structures if I am not mistaken.

I think we should keep the setValue methods for the primitive types (``setValue(int i)``, ``setValue(double d)``, ...) as well for performance reasons. Otherwise, one would need to create (or let autoboxing create) an object just to update a value. Since these methods tend to be called very often, this could increase the pressure on garbage collection which we try to avoid as much as possible. Adding the methods for the basic types should fix the issue with Scala data type deserializer.

Also, the issue ([#519|https://github.com/stratosphere/stratosphere/issues/519] | [FLINK-519|https://issues.apache.org/jira/browse/FLINK-519]) did mean to add methods like ``setValue(IntValue i)`` and ``setValue(DoubleVal)`` such that the value of an IntValue can be set using another IntValue.

Best, Fabian;;;","09/Jun/14 12:26;github-import;[Date: Mon Mar 03 10:41:44 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The error comes from a special test whose classes are in a jar file under
/src/test/resources. We have them not in the normal compile/test scope to
verify the behavior of classloading with user-defined classes; that is why
the test is commented. If you change fundamental signatures, you need to
rebuild that test.;;;","09/Jun/14 12:26;github-import;[Date: Mon Mar 03 14:50:50 CET 2014, Author: [MikhailErofeev|https://github.com/MikhailErofeev]]

oops, i misunderstand issue, and implement it to java primitives wrappers instead of Value. fix it ;;;","09/Jun/14 12:26;github-import;[Date: Mon Mar 03 15:07:55 CET 2014, Author: [fhueske|https://github.com/fhueske]]

OK, thanks a lot! :-);;;","09/Jun/14 12:26;github-import;[Date: Mon Mar 03 19:36:27 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good. One question though: You removed a few checks for null. The old checks gave a specific error message. When now someone submits a null value, the `value.value` statements will all throw an unspecific null pointer exception.;;;","09/Jun/14 12:26;github-import;[Date: Mon Mar 03 19:59:12 CET 2014, Author: [MikhailErofeev|https://github.com/MikhailErofeev]]

ok, I replace this with apache Validate, forgot about old checking, then thought that exception will occurs few lines below and remove checking. Should i add validators in other setters?
;;;","09/Jun/14 12:26;github-import;[Date: Wed Mar 05 00:38:52 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think it looks good. I will merge it.;;;","09/Jun/14 12:26;github-import;[Date: Wed Mar 05 11:13:28 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [2006980be27debb604b40dc8f5c0ea229a4ae0ac|https://github.com/stratosphere/stratosphere/commit/2006980be27debb604b40dc8f5c0ea229a4ae0ac]

Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix examples in GitHub readme page to 0.5 version usage ,FLINK-524,12719696,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:26,09/Jun/14 12:26,14/Jul/23 05:57,09/Jun/14 12:26,,,,pre-apache,,,,,,,0,github-import,,"looks like move to 0.5 not well documented still, fix main gh page

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/524
Created by: [MikhailErofeev|https://github.com/MikhailErofeev]
Labels: 
Created at: Sun Mar 02 16:22:06 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:26;github-import;pull-request-524-9059969760750537564.patch;https://issues.apache.org/jira/secure/attachment/12649184/pull-request-524-9059969760750537564.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397895,,,Mon Jun 09 12:26:43 UTC 2014,,,,,,,,,,"0|i1wi4n:",398022,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:26;github-import;[Date: Sun Mar 02 17:13:38 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Cool! Thank you for fixing this.;;;","09/Jun/14 12:26;github-import;[Date: Sun Mar 02 17:16:04 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I merged your changes in [706dec17c125b0ac93f299adc2b14c695640d78c|https://github.com/stratosphere/stratosphere/commit/706dec17c125b0ac93f299adc2b14c695640d78c] (note that I merged the two commits into one);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Website] Clarify FAQ and Downloads page regarding Hadoop dependency,FLINK-523,12719695,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:26,09/Jun/14 12:26,14/Jul/23 05:57,09/Jun/14 12:26,,,,pre-apache,,,,,,,0,github-import,,"As a first approach to reflect the results of the discussion here (https://github.com/stratosphere/stratosphere/issues/513) , I reworked the FAQ and Downloads page.

Preview Downloads page: http://robertmetzger.de/stratosphere/downloads/
Preview FAQ: http://robertmetzger.de/stratosphere/docs/0.4/general/faq.html

I'm happy to hear your feedback!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/523
Created by: [rmetzger|https://github.com/rmetzger]
Labels: documentation, website, 
Created at: Sun Mar 02 14:04:48 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:26;github-import;pull-request-523-962985018445092589.patch;https://issues.apache.org/jira/secure/attachment/12649183/pull-request-523-962985018445092589.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397894,,,Mon Jun 09 12:26:37 UTC 2014,,,,,,,,,,"0|i1wi4f:",398021,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:26;github-import;[Date: Sun Mar 02 14:14:41 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Ah .. note that the changes of the second commit are not reflected on the preview page. You'll find the all changes in https://github.com/stratosphere/stratosphere/pull/523/files;;;","09/Jun/14 12:26;github-import;[Date: Mon Mar 03 19:43:11 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It is much better, but I personally do not like the ""many users do this and that"" wording... I have a few comments in-line;;;","09/Jun/14 12:26;github-import;[Date: Tue Mar 04 12:52:59 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

merged in [9d783ce44cac87f94bc99854b236b203be004121|https://github.com/stratosphere/stratosphere/commit/9d783ce44cac87f94bc99854b236b203be004121];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
included double quotes around scalars to handle paths with spaces,FLINK-521,12719693,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:26,09/Jun/14 12:26,14/Jul/23 05:57,09/Jun/14 12:26,,,,pre-apache,,,,,,,0,github-import,,"Resolving the following issue: https://github.com/stratosphere/stratosphere/issues/466

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/521
Created by: [iasip|https://github.com/iasip]
Labels: 
Created at: Sun Mar 02 06:39:24 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:26;github-import;pull-request-521-8729894808490940498.patch;https://issues.apache.org/jira/secure/attachment/12649182/pull-request-521-8729894808490940498.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397892,,,Mon Jun 09 12:26:21 UTC 2014,,,,,,,,,,"0|i1wi3z:",398019,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:26;github-import;[Date: Sun Mar 02 09:01:50 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi, Thank you for your pull request! 
Can you have a look at the other scripts and see if they also need this fix? (and also fix them in that case).
You can update a pending pull request by pushing the new commit into the branch.
By the way branch: I recommend not to open a pull request against `master`. Ideally, you should use feature branches for a pull request. But it does not matter in your case. ;;;","09/Jun/14 12:26;github-import;[Date: Sun Mar 02 23:10:07 CET 2014, Author: [iasip|https://github.com/iasip]]

Thanks for the clarification, and I'll take a look at the other scripts right away.
-Billy;;;","09/Jun/14 12:26;github-import;[Date: Mon Mar 03 20:01:43 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Thanks for the bugfix. Looks good to me in.

I have a minor question though (inline comment), can you take a look please?;;;","09/Jun/14 12:26;github-import;[Date: Wed Mar 05 11:01:18 CET 2014, Author: [fhueske|https://github.com/fhueske]]

@iasip @rmetzger @StephanEwen Is this PR ready to merge?;;;","09/Jun/14 12:26;github-import;[Date: Wed Mar 05 11:06:17 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I think its not ready: The fix should also be applied to the other scripts.
I guess the additional `/bin/sh` call is not required (but I'm not 100% sure).;;;","09/Jun/14 12:26;github-import;[Date: Thu Mar 06 09:11:11 CET 2014, Author: [iasip|https://github.com/iasip]]

Hi everyone,

I took a look at the other scripts and made a few small corrections.
Notes:
1. I got rid of the /bin/sh I included at line 23 of start-local.sh and ended the double quote right after $STRATOSPHERE_BIN_DIR instead of after jobmanager.sh.  It still worked, meaning I probably made a mistake earlier.
2. I'd like to take a closer look at the shell scripts before they get merged.  The changes I've made work at a superficial level, but I just want to double check to make sure they're right.

Respectfully, Billy;;;","09/Jun/14 12:26;github-import;[Date: Thu Mar 06 09:13:34 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Cool, thanks.;;;","09/Jun/14 12:26;github-import;[Date: Thu Mar 06 15:49:08 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

It does not work.

I checked out the code and was running (in a path with a space obviously)
My path looks like this 
```
/home/robert/Projekte/ozone/ozone/stratosphere-dist/tar get/stratosphere-dist-0.5-SNAPSHOT-bin/stratosphere-0.5-SNAPSHOT
```
```bash
robert@robert-tp ...SNAPSHOT-bin/stratosphere-0.5-SNAPSHOT (git)-[iasip-master] % ./bin/start-local.sh
Starting job manager
robert@robert-tp ...SNAPSHOT-bin/stratosphere-0.5-SNAPSHOT (git)-[iasip-master] % ./bin/start-webclient.sh

Starting Stratosphere webclient
```
But
```
robert@robert-tp ...SHOT-bin/stratosphere-0.5-SNAPSHOT/log (git)-[iasip-master] % cat stratosphere-robert-jobmanager-robert-tp.out
Error: Could not find or load main class get.stratosphere-dist-0.5-SNAPSHOT-bin.stratosphere-0.5-SNAPSHOT.bin....log.stratosphere-robert-jobmanager-robert-tp.log
```
;;;","09/Jun/14 12:26;github-import;[Date: Fri Mar 07 09:21:14 CET 2014, Author: [iasip|https://github.com/iasip]]

Sorry this is taking longer than I expected, but I'm slowly chipping away at it.
Robert, I've resolved the issue you mentioned above.
As of now, this is what I get for start-local.sh and start-webclient.sh:
```
[ec2-user@ip-XXX-XX-XX-XXX stratosphere-0.5-SNAPSHOT]$ pwd
/home/ec2-user/Test Directory/stratosphere/stratosphere-dist/target/stratosphere-dist-0.5-SNAPSHOT-bin/stratosphere-0.5-SNAPSHOT

[ec2-user@ip-XXX-XX-XX-XXX stratosphere-0.5-SNAPSHOT]$ cat log/stratosphere-root-jobmanager-ip-XXX-XX-XX-XXX.out
07:57:53,203 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Starting Stratosphere JobManager (Version: 0.5-SNAPSHOT, Rev:d457f96)
07:57:53,292 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Starting job manager in LOCAL mode
07:57:53,311 INFO  eu.stratosphere.nephele.instance.local.LocalInstanceManager   - Default instance type is default
07:57:53,321 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Reading location of job manager from configuration
07:57:53,321 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Job manager address: localhost/127.0.0.1:6123
07:57:53,355 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Determined /127.0.0.1 as the TaskTracker's own IP address
07:57:53,355 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Announcing connection information localhost to job manager
07:57:53,526 INFO  eu.stratosphere.nephele.taskmanager.bufferprovider.GlobalBufferPool  - Initialized global buffer pool with 2048 buffers with a size 32768 bytes each
07:57:53,540 INFO  eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager  - Initialized byte buffered channel manager with sender-side spilling disabled and spilled buffer merging enabled
07:57:53,540 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Initializing memory manager with 428 megabytes of memory
07:57:54,085 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Trying to load eu.stratosphere.nephele.jobmanager.scheduler.local.LocalScheduler as scheduler
07:57:54,100 INFO  eu.stratosphere.nephele.jobmanager.web.WebInfoServer          - Setting up web info server, using web-root directory '/home/ec2-user/Test Directory/stratosphere/stratosphere-dist/target/stratosphere-dist-0.5-SNAPSHOT-bin/stratosphere-0.5-SNAPSHOT/bin/../conf/.././resources/web-docs-infoserver'.
07:57:54,100 INFO  eu.stratosphere.nephele.jobmanager.web.WebInfoServer          - Web info server will display information about nephele job-manager on localhost, port 8081.
07:57:54,210 INFO  eu.stratosphere.nephele.jobmanager.web.WebInfoServer          - Starting web info server for JobManager on port 8081
2014-03-07 07:57:54.211:INFO::jetty-8.0.0.M1
2014-03-07 07:57:54.256:INFO::Started SelectChannelConnector@0.0.0.0:8081

[ec2-user@ip-XXX-XX-XX-XXX stratosphere-0.5-SNAPSHOT]$ cat log/stratosphere-root-webclient-ip-XXX-XX-XX-XXX.out
14/03/07 08:18:54 INFO web.WebInterfaceServer: Setting up web frontend server, using web-root directory '/home/ec2-user/Test Directory/stratosphere/stratosphere-dist/target/stratosphere-dist-0.5-SNAPSHOT-bin/stratosphere-0.5-SNAPSHOT/bin/../conf/.././resources/web-docs'.
14/03/07 08:18:54 INFO web.WebInterfaceServer: Web frontend server will store temporary files in '/tmp', uploaded jobs in '/tmp/webclient-jobs', plan-json-dumps in '/tmp/webclient-plans'.
14/03/07 08:18:54 INFO web.WebInterfaceServer: Web-frontend will submit jobs to nephele job-manager on localhost, port 6123.
14/03/07 08:18:54 INFO client.WebFrontend: Starting web frontend server on port 8080.
2014-03-07 08:18:54.970:INFO::jetty-8.0.0.M1
```

Does that look correct?

;;;","09/Jun/14 12:26;github-import;[Date: Fri Mar 07 09:23:09 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,

yes. The JobManager and the webfrontend started correctly.

Regards,
Robert


On Fri, Mar 7, 2014 at 9:21 AM, Billy (Doo Hyun) Lee <
notifications@github.com> wrote:

> Sorry this is taking longer than I expected, but I'm slowly chipping away
> at it.
> Robert, I've resolved the issue you mentioned above.
> As of now, this is what I get for start-local.sh and start-webclient.sh:
>
> [ec2-user@ip-XXX-XX-XX-XXX stratosphere-0.5-SNAPSHOT]$ pwd
> /home/ec2-user/Test Directory/stratosphere/stratosphere-dist/target/stratosphere-dist-0.5-SNAPSHOT-bin/stratosphere-0.5-SNAPSHOT
>
> [ec2-user@ip-XXX-XX-XX-XXX stratosphere-0.5-SNAPSHOT]$ cat log/stratosphere-root-jobmanager-ip-XXX-XX-XX-XXX.out
> 07:57:53,203 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Starting Stratosphere JobManager (Version: 0.5-SNAPSHOT, Rev:d457f96)
> 07:57:53,292 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Starting job manager in LOCAL mode
> 07:57:53,311 INFO  eu.stratosphere.nephele.instance.local.LocalInstanceManager   - Default instance type is default
> 07:57:53,321 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Reading location of job manager from configuration
> 07:57:53,321 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Job manager address: localhost/127.0.0.1:6123
> 07:57:53,355 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Determined /127.0.0.1 as the TaskTracker's own IP address
> 07:57:53,355 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Announcing connection information localhost to job manager
> 07:57:53,526 INFO  eu.stratosphere.nephele.taskmanager.bufferprovider.GlobalBufferPool  - Initialized global buffer pool with 2048 buffers with a size 32768 bytes each
> 07:57:53,540 INFO  eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager  - Initialized byte buffered channel manager with sender-side spilling disabled and spilled buffer merging enabled
> 07:57:53,540 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Initializing memory manager with 428 megabytes of memory
> 07:57:54,085 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Trying to load eu.stratosphere.nephele.jobmanager.scheduler.local.LocalScheduler as scheduler
> 07:57:54,100 INFO  eu.stratosphere.nephele.jobmanager.web.WebInfoServer          - Setting up web info server, using web-root directory '/home/ec2-user/Test Directory/stratosphere/stratosphere-dist/target/stratosphere-dist-0.5-SNAPSHOT-bin/stratosphere-0.5-SNAPSHOT/bin/../conf/.././resources/web-docs-infoserver'.
> 07:57:54,100 INFO  eu.stratosphere.nephele.jobmanager.web.WebInfoServer          - Web info server will display information about nephele job-manager on localhost, port 8081.
> 07:57:54,210 INFO  eu.stratosphere.nephele.jobmanager.web.WebInfoServer          - Starting web info server for JobManager on port 8081
> 2014-03-07 07:57:54.211:INFO::jetty-8.0.0.M1
> 2014-03-07 07:57:54.256:INFO::Started SelectChannelConnector@0.0.0.0:8081
>
> [ec2-user@ip-XXX-XX-XX-XXX stratosphere-0.5-SNAPSHOT]$ cat log/stratosphere-root-webclient-ip-XXX-XX-XX-XXX.out
> 14/03/07 08:18:54 INFO web.WebInterfaceServer: Setting up web frontend server, using web-root directory '/home/ec2-user/Test Directory/stratosphere/stratosphere-dist/target/stratosphere-dist-0.5-SNAPSHOT-bin/stratosphere-0.5-SNAPSHOT/bin/../conf/.././resources/web-docs'.
> 14/03/07 08:18:54 INFO web.WebInterfaceServer: Web frontend server will store temporary files in '/tmp', uploaded jobs in '/tmp/webclient-jobs', plan-json-dumps in '/tmp/webclient-plans'.
> 14/03/07 08:18:54 INFO web.WebInterfaceServer: Web-frontend will submit jobs to nephele job-manager on localhost, port 6123.
> 14/03/07 08:18:54 INFO client.WebFrontend: Starting web frontend server on port 8080.
> 2014-03-07 08:18:54.970:INFO::jetty-8.0.0.M1
>
> Does that look correct?
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/521#issuecomment-36976332>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","09/Jun/14 12:26;github-import;[Date: Fri Mar 07 21:38:47 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Please let me know when the pull request is ready for review!;;;","09/Jun/14 12:26;github-import;[Date: Sat Mar 08 05:09:10 CET 2014, Author: [iasip|https://github.com/iasip]]

My pull request is ready for review.  Fingers crossed!
-Billy;;;","09/Jun/14 12:26;github-import;[Date: Sun Mar 09 12:34:19 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,
I had a look at your changes. Starting and stopping is now working.
There is one problem left: After your change, the start-local.sh script is only creating one logfile.
This is how it should look like
```
robert@robert-tower ...SNAPSHOT-bin/stratosphere-0.5-SNAPSHOT (git)-[0038c9d...] % ./bin/start-local.sh
Starting job manager
robert@robert-tower ...SNAPSHOT-bin/stratosphere-0.5-SNAPSHOT (git)-[0038c9d...] % cd log
robert@robert-tower ...SHOT-bin/stratosphere-0.5-SNAPSHOT/log (git)-[0038c9d...] % ll
total 8
-rw-r--r-- 1 robert robert 2280 Mar  9 12:30 stratosphere-robert-jobmanager-robert-tower.log
-rw-r--r-- 1 robert robert  267 Mar  9 12:30 stratosphere-robert-jobmanager-robert-tower.out
```

This is how it looks with your change:
```
robert@robert-tower ...SNAPSHOT-bin/stratosphere-0.5-SNAPSHOT (git)-[iasip-master] % cd log
robert@robert-tower ...SHOT-bin/stratosphere-0.5-SNAPSHOT/log (git)-[iasip-master] % ll
total 4
-rw-r--r-- 1 robert robert 1853 Mar  9 12:19 stratosphere-robert-jobmanager-robert-tower.out
```

So there seems to be something wrong. (Might be an issue with the log4j configuration).
;;;","09/Jun/14 12:26;github-import;[Date: Sun Mar 09 22:48:57 CET 2014, Author: [iasip|https://github.com/iasip]]

By changing log_setting from a string to an array of parameters, 
From:
```
log_setting=""-Dlog.file=""$log"" -Dlog4j.configuration=file:""$STRATOSPHERE_CONF_DIR""/log4j.properties""
```
```
$JAVA_RUN $JVM_ARGS ""$log_setting"" -classpath ""$STRATOSPHERE_WEBCLIENT_CLASSPATH"" eu.stratosphere.client.WebFrontend -configDir ""$STRATOSPHERE_CONF_DIR"" > ""$out"" 2>&1 < /dev/null &
```
To:
```
 log_setting=(-Dlog.file=""$log"" -Dlog4j.configuration=file:""$STRATOSPHERE_CONF_DIR""/log4j.properties)
```
```
$JAVA_RUN $JVM_ARGS ""${log_setting[@]}"" -classpath ""$STRATOSPHERE_WEBCLIENT_CLASSPATH"" eu.stratosphere.client.WebFrontend -configDir ""$STRATOSPHERE_CONF_DIR"" > ""$out"" 2>&1 < /dev/null &
```
both .log and .out files are created.
When log_setting was a string, everything was being written to the .out file.

I apologize for being so reactive instead of proactive when dealing with this bug, but if there's anything else you see wrong, or if you foresee any problems with $log_setting being an array, please let me know.;;;","09/Jun/14 12:26;github-import;[Date: Mon Mar 10 02:45:49 CET 2014, Author: [iasip|https://github.com/iasip]]

```
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] stratosphere ...................................... SUCCESS [30.621s]
[INFO] stratosphere-core ................................. SUCCESS [38.075s]
[INFO] stratosphere-java ................................. SUCCESS [21.355s]
[INFO] stratosphere-runtime .............................. SUCCESS [12:27.043s]
[INFO] stratosphere-compiler ............................. SUCCESS [17.833s]
[INFO] stratosphere-scala ................................ SUCCESS [1:32.656s]
[INFO] stratosphere-clients .............................. SUCCESS [10.929s]
[INFO] stratosphere-examples ............................. SUCCESS [0.289s]
[INFO] stratosphere-java-examples ........................ SUCCESS [14.515s]
[INFO] stratosphere-scala-examples ....................... SUCCESS [1:29.602s]
[INFO] stratosphere-tests ................................ SUCCESS [17:04.824s]
[INFO] stratosphere-addons ............................... SUCCESS [0.384s]
[INFO] array-datamodel ................................... SUCCESS [15.015s]
[INFO] avro .............................................. SUCCESS [22.451s]
[INFO] jdbc .............................................. SUCCESS [10.575s]
[INFO] spargel ........................................... SUCCESS [17.432s]
[INFO] swt-visualization ................................. SUCCESS [11.222s]
[INFO] hadoop-compatibility .............................. SUCCESS [23.641s]
[INFO] hbase ............................................. FAILURE [10:36.366s]
[INFO] yarn .............................................. SKIPPED
[INFO] stratosphere-dist ................................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 47:05.761s
[INFO] Finished at: Sun Mar 09 23:48:53 UTC 2014
[INFO] Final Memory: 44M/190M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hbase: Could not resolve dependencies for project eu.stratosphere:hbase:jar:0.5-SNAPSHOT: Failed to collect dependencies at org.apache.hbase:hbase:jar:0.94.2-cdh4.2.1: Failed to read artifact descriptor for org.apache.hbase:hbase:jar:0.94.2-cdh4.2.1: Could not transfer artifact org.apache.hbase:hbase:pom:0.94.2-cdh4.2.1 from/to cloudera-releases (https://repository.cloudera.com/artifactory/cloudera-repos): Connection to https://repository.cloudera.com refused: Connection timed out -> [Help 1]
```
I also get the message above when Travis CI is building the system with my latest commit (log_setting as an array).;;;","09/Jun/14 12:26;github-import;[Date: Mon Mar 10 11:14:44 CET 2014, Author: [zentol|https://github.com/zentol]]

i don't think that's related to your code. i get the same error message on an unrelated issue.

Connection to https://repository.cloudera.com refused: Connection timed out

the website is not reachable at the moment.;;;","09/Jun/14 12:26;github-import;[Date: Mon Mar 10 18:49:59 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi, yes. The server seems to be down.
The problem is that we are using a CDH version of HBase. 
We need to rework hbase to be compatible with an official hbase release.;;;","09/Jun/14 12:26;github-import;[Date: Wed Mar 12 04:30:22 CET 2014, Author: [iasip|https://github.com/iasip]]

I noticed that the trouble above with Travis CI seemed to be resolved.
Is there anyway I can rebuild without re-committing?;;;","09/Jun/14 12:26;github-import;[Date: Wed Mar 12 08:37:21 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Its okay that the travis build failed. Your pull request did not change any code and travis does not test  the scripts.;;;","09/Jun/14 12:26;github-import;[Date: Thu Mar 13 10:36:24 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay, the fix is working.
I will squash your commits together and the push it to master;;;","09/Jun/14 12:26;github-import;[Date: Thu Mar 13 10:54:40 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

This is how I created the commit:
```
robert@robert-tower ~/Projekte/ozone/ozone/stratosphere-dist (git)-[master] % git checkout -b space_fix iasip/master
1 robert@robert-tower ~/Projekte/ozone/ozone/stratosphere-dist (git)-[master] % git checkout -b space_fix origin/master
Branch space_fix set up to track remote branch master from origin.
Switched to a new branch 'space_fix'
robert@robert-tower ~/Projekte/ozone/ozone/stratosphere-dist (git)-[space_fix] % git cherry-pick 9eb25ce  d457f96
[space_fix af9cb96] included double quotes around scalars to handle paths with spaces
 Author: Billy Lee <dlee167@jhu.edu>
 2 files changed, 10 insertions(+), 10 deletions(-)
[space_fix 51fbd3a] enclosed all  in double quotes; re-edited start-local.sh to remove /bin/sh at line 23
 Author: Billy Lee <dlee167@jhu.edu>
 6 files changed, 8 insertions(+), 8 deletions(-)
robert@robert-tower ~/Projekte/ozone/ozone/stratosphere-dist (git)-[space_fix] % git cherry-pick 033771a f35c361  685dbd3 77d4cf1  7b54b0c
[space_fix bc92a0e] Enclosed scalars in double quotes to handle spaces in path
 Author: Billy Lee <dlee167@jhu.edu>
 2 files changed, 5 insertions(+), 4 deletions(-)
[space_fix 575e15c] Enclosed scalars in double quotes to handle spaces in path
 Author: Billy Lee <dlee167@jhu.edu>
 1 file changed, 1 deletion(-)
[space_fix 386c016] Enclosed scalars in double quotes to handle spaces in path
 Author: Billy Lee <dlee167@jhu.edu>
 1 file changed, 2 insertions(+), 2 deletions(-)
[space_fix 8ca6087] Enclosed scalars in double quotes to handle spaces in path
 Author: Billy Lee <dlee167@jhu.edu>
 4 files changed, 8 insertions(+), 8 deletions(-)
[space_fix ede1045] Changed log_setting from string to array
 Author: Billy Lee <dlee167@jhu.edu>
 4 files changed, 8 insertions(+), 8 deletions(-)

robert@robert-tower ~/Projekte/ozone/ozone/stratosphere-dist (git)-[space_fix] % git rebase -i HEAD~8
```

Pushed to master in [7fe59e9072e925fddb735d85110f6b85996b28ca|https://github.com/stratosphere/stratosphere/commit/7fe59e9072e925fddb735d85110f6b85996b28ca];;;","09/Jun/14 12:26;github-import;[Date: Thu Mar 13 10:54:57 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
include examples source code into assembly,FLINK-520,12719691,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:25,09/Jun/14 12:26,14/Jul/23 05:57,09/Jun/14 12:26,,,,pre-apache,,,,,,,0,github-import,,"The source code for the examples is now placed inside
`/examples/scala-src` and `/examples/java-src`.


```
robert@robert-tower ...bin/stratosphere-0.5-SNAPSHOT/examples (git)-[exsrc] % find .
.
./stratosphere-java-examples-0.5-SNAPSHOT-EnumTrianglesWithDegrees.jar
./stratosphere-java-examples-0.5-SNAPSHOT-WebLogAnalysis.jar
./stratosphere-java-examples-0.5-SNAPSHOT-EnumTrianglesOnEdgesWithDegrees.jar
./stratosphere-scala-examples-0.5-SNAPSHOT-ConnectedComponents.jar
./stratosphere-scala-examples-0.5-SNAPSHOT-TPCHQuery3.jar
./stratosphere-scala-examples-0.5-SNAPSHOT-WordCount.jar
./stratosphere-scala-examples-0.5-SNAPSHOT-javadoc.jar
./scala-src
./scala-src/datamining
./scala-src/datamining/BatchGradientDescent.scala
./scala-src/datamining/LanczosSO.scala
./scala-src/datamining/KMeansForTest.scala
./scala-src/datamining/KMeans.scala
./scala-src/graph
./scala-src/graph/PageRank.scala
./scala-src/graph/EnumTrianglesOnEdgesWithDegrees.scala
./scala-src/graph/LineRank.scala
./scala-src/graph/ComputeEdgeDegrees.scala
./scala-src/graph/ConnectedComponents.scala
./scala-src/graph/TransitiveClosureRD.scala
./scala-src/graph/TransitiveClosureNaive.scala
./scala-src/graph/PageRankWithWeight.scala
./scala-src/wordcount
./scala-src/wordcount/WordCount.scala
./scala-src/wordcount/WordCountWithUserDefinedType.scala
./scala-src/wordcount/WordCountWithCount.scala
./scala-src/grabbag
./scala-src/grabbag/Grabbag.scala
./scala-src/relational
./scala-src/relational/WebLogAnalysis.scala
./scala-src/relational/RelationalQuery.scala
./java-src
./java-src/incremental
./java-src/incremental/pagerank
./java-src/incremental/pagerank/DeltaPageRankWithInitialDeltas.java
./java-src/triangles
./java-src/triangles/EnumTrianglesRdfFoaf.java
./java-src/triangles/EnumTrianglesWithDegrees.java
./java-src/triangles/io
./java-src/triangles/io/EdgeWithDegreesInputFormat.java
./java-src/triangles/io/TriangleOutputFormat.java
./java-src/triangles/io/EdgeWithDegreesOutputFormat.java
./java-src/triangles/io/EdgeInputFormat.java
./java-src/triangles/ComputeEdgeDegrees.java
./java-src/triangles/EnumTrianglesOnEdgesWithDegrees.java
./java-src/shortestpaths
./java-src/shortestpaths/PairwiseSP.java
./java-src/sort
./java-src/sort/terasort
./java-src/sort/terasort/TeraKey.java
./java-src/sort/terasort/TeraValue.java
./java-src/sort/terasort/TeraOutputFormat.java
./java-src/sort/terasort/TeraInputFormat.java
./java-src/sort/terasort/TeraDistribution.java
./java-src/sort/ReduceGroupSort.java
./java-src/sort/TeraSort.java
./java-src/kmeans
./java-src/kmeans/udfs
./java-src/kmeans/udfs/PointOutFormat.java
./java-src/kmeans/udfs/RecomputeClusterCenter.java
./java-src/kmeans/udfs/ComputeDistance.java
./java-src/kmeans/udfs/PointInFormat.java
./java-src/kmeans/udfs/CoordVector.java
./java-src/kmeans/udfs/FindNearestCenter.java
./java-src/kmeans/udfs/ComputeDistanceParameterized.java
./java-src/kmeans/KMeans.java
./java-src/kmeans/KMeansIterative.java
./java-src/kmeans/KMeansSampleDataGenerator.java
./java-src/util
./java-src/util/ConfigUtils.java
./java-src/connectedcomponents
./java-src/connectedcomponents/WorksetConnectedComponents.java
./java-src/wordcount
./java-src/wordcount/WordCountOptimized.java
./java-src/wordcount/AnonymousWordCount.java
./java-src/wordcount/WordCountAccumulators.java
./java-src/wordcount/WordCount.java
./java-src/relational
./java-src/relational/generator
./java-src/relational/generator/WebLogGenerator.java
./java-src/relational/WebLogAnalysis.java
./java-src/relational/TPCHQuery3.java
./java-src/pagerank
./java-src/pagerank/DiffL1NormConvergenceCriterion.java
./java-src/pagerank/PageRankStats.java
./java-src/pagerank/DotProductMatch.java
./java-src/pagerank/DanglingPageRankInputFormat.java
./java-src/pagerank/DanglingPageRank.java
./java-src/pagerank/DotProductCoGroup.java
./java-src/pagerank/PageWithRankOutFormat.java
./java-src/pagerank/PageRankStatsAggregator.java
./java-src/pagerank/ImprovedAdjacencyListInputFormat.java
./java-src/pagerank/SimplePageRank.java
./java-src/pagerank/LongArrayView.java
./java-src/pagerank/AsciiLongArrayView.java
./stratosphere-scala-examples-0.5-SNAPSHOT-KMeansForTest.jar
./stratosphere-java-examples-0.5-SNAPSHOT-TPCHQuery3.jar
./stratosphere-scala-examples-0.5-SNAPSHOT-KMeans.jar
./stratosphere-java-examples-0.5-SNAPSHOT-DanglingPageRank.jar
./stratosphere-java-examples-0.5-SNAPSHOT-ConnectedComponents.jar
./stratosphere-java-examples-0.5-SNAPSHOT-KMeansIterative.jar
./stratosphere-java-examples-0.5-SNAPSHOT-TeraSort.jar
./stratosphere-scala-examples-0.5-SNAPSHOT-ComputeEdgeDegrees.jar
./stratosphere-java-examples-0.5-SNAPSHOT-PairwiseSP.jar
./stratosphere-java-examples-0.5-SNAPSHOT-EnumTrianglesRdfFoaf.jar
./stratosphere-java-examples-0.5-SNAPSHOT-javadoc.jar
./stratosphere-scala-examples-0.5-SNAPSHOT-EnumTrianglesOnEdgesWithDegrees.jar
./stratosphere-java-examples-0.5-SNAPSHOT-WordCount.jar
./stratosphere-java-examples-0.5-SNAPSHOT-KMeans.jar
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/520
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Sun Mar 02 00:09:53 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:25;github-import;pull-request-520-5385593952061097970.patch;https://issues.apache.org/jira/secure/attachment/12649181/pull-request-520-5385593952061097970.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397890,,,Mon Jun 09 12:26:04 UTC 2014,,,,,,,,,,"0|i1wi3j:",398017,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:25;github-import;[Date: Tue Mar 04 00:55:16 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Nice work!

Out of curiosity: You cannot directly load the directoty `./java-src`into an IDE, because the directory path `pagerank/DanglingPageRank.java` does not match the package (missing the prefix `eu.stratosphere.example...`). Would it make sense to retain the deep paths? On the pros, it would work easier to import it into an IDE, on the cons, it makes the file based browsing more tedious (deep paths).

What is your take on that?;;;","09/Jun/14 12:25;github-import;[Date: Tue Mar 04 01:47:02 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

If people want to inspect the code in an IDE, I would recommend to import the `stratosphere-examples` project as a maven project.
My idea for this was to give people a convenient way to access the source code, for example with a simple text editor.
The only feasible approach to allow this would be to change the package name. But I'm not aware of any maven-based solution for this.;;;","09/Jun/14 12:25;github-import;[Date: Tue Mar 04 13:45:14 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

As a potential user of Stratosphere, I'd rather it be easy to open the example source in an IDE. I can always ask the IDE to take me to the folder containing a source file (and that would be rare). 

In general, users' liking or disliking to a product is highly related to the effort they have to put on first contact. So, my view is that if users install the Ready to Run Package, they should be able to open the example source in Eclipse or other IDE, set the class path to Stratosphere ""bin"", and run the examples from the IDE without installing anything else, except for examples that illustrate using Stratosphere with Hadoop (which I think can be further addressed by separating the Hadoop examples from non-Hadoop examples).;;;","09/Jun/14 12:25;github-import;[Date: Wed Mar 05 10:37:07 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,
thank you for your feedback!

You are right. Exploring a complex software such as Stratosphre without IDE support is probably impossible!
I also agree with your second comment. We probably have 15-45 minutes to convince potential users of the greatness of our software.

I have however a different view on providing source code in the ready-to-run packages.

Lets say we have two different audiences:
a) Software Developers (Java, Scala)
b) System Administrators.

For a), the developers follow the [java quickstart guide|http://stratosphere.eu/quickstart/java.html) on the website. It will create a sample project skeleton with everything required to develop and debug a Stratosphere job on a computer. The only step to set this up is invoking one script and importing the files into eclipse! (they do not need the ready-to-run packages)
If a developer wants to access (or execute) the examples, they just have to open the class. (again, no ready-to-run binaries required! -- everything out of the IDE).
Once the developer has finished writing his Job and wants to execute it on the cluster, he has to download the ""ready to run package"" for the cluster. We provide the examples with the packages so that they can validate the correctness of the installation.

So to conclude the a)-case: Software developers will most likely have their first contact with Stratosphere using our quickstarts (the technology behind this are maven archetypes).


Users from category b) will download the ready to run packages, install Stratospehre on their local machine or cluster and run the examples. They can look at the webinterface or in the log files.

(We also have/had the Quickstart Virtual Machine for users that are not so familiar with Linux or are frighten by the installation]

It might be the case that I have the wrong assumption about our user's behavior.;;;","09/Jun/14 12:25;github-import;[Date: Wed Mar 05 21:28:09 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

I will be careful writing my observations (and ask for a wide berth), because everything I know about Stratosphere is limited to what I have read on its web site in the last week or so. You probably have been on the development project from its inception, so I will defer to you on the project's internals, goals, and priorities.

With that said, assuming the Ready to Run Package includes example source:

1. Is Maven is necessary to run the example source when users have only the Ready to Run Package? I can understand if Maven is recommended to view/build the complete Stratosphere source.

2. What are the impediments to users viewing/building/running the examples in an IDE *without* using Maven when they have only the Ready to Run Package?

Here are some notes from a quick trial I did: I created an Eclipse project (I don't have Maven) and added reference to jars in Stratosphere's lib and lib_clients directories. I then added the example WordCount.java (extracted from Stratosphere source) to the project and the project compiled successfully, except the compiler did not approve of using the 4-arg constructor of class CsvOutputFormat. The program did compile when I changed the offending line to use the default constructor. (I did not have the time to investigate why the original constructor was unacceptable.)

So, at least superficially, it seems I am able to view/build the example source in Eclipse without using Maven.

Unfortunately, running WordCount caused an exception, but in the limited time I spent investigating it, I don't immediately see why this exception should occur (WordCount.TokenizeLine's ancestor AbstractFunction implements Seralized). I assume the exception is caused by incorrect order of import or references (or some such programmer silliness), but I am left with the following questions:

1. Are there Stratosphere dependencies I am not meeting in this trial? I really hope not.

2. Would using Maven eliminate both the compile-time and run-time errors? I doubt it, and hope the answer isn't ""yes, it does, because it makes sure you have the latest everything."".

3. Is it necessary for the user to go through this process to get some instant gratification (which is key to user engagement)? I don't believe they should.

BTW, I spent close to 90 calendar minutes conducting this trial, which IMHO is rather high, especially if I don't yet have that instant gratification. :)

![image|https://f.cloud.github.com/assets/6477490/2337744/afae56b2-a4a0-11e3-8fb0-f65a6e852fcb.png]
;;;","09/Jun/14 12:25;github-import;[Date: Wed Mar 05 21:36:34 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,
I guess you've downloaded the 0.4 release (ready to run package) but used the current source code from the repository for your experiment ?
They are incompatible since the source code in the repo belongs to version 0.5-SNAPSHOT. ;;;","09/Jun/14 12:25;github-import;[Date: Wed Mar 05 21:51:42 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Now let me answer your questions

1. No, users do not need Maven to run the compiled examples inside the ready to run packages.

2. When you download Eclipse from eclipse.org, it already contains maven support. You do not have to install anything. (The same applies to IntelliJ). So most of the users already have or use maven, without knowing it.
Maven is the de-facto standard for all other ""big data"" projects such as Hadoop, HBase, Giraph etc. So our users should be used to the typical maven commands (most importantly `mvn clean package -DskipTests`).

What you did should be possible if you do not mix the versions (its just an assumption, you might have found a bug). So yes, you can run Stratosphere without Maven only with the jars in the ready to run packages. 


	except the compiler did not approve of using the 4-arg constructor of class CsvOutputFormat.

In the 0.5-SNAPSHOT version, we introduced an additional way to define the fields in the CSV input.

	WordCount.TokenizeLine's ancestor AbstractFunction implements Seralized

But only in the 0.5-SNAPSHOT version. @uce made a pull request recently to introduce that.

Second set of questions:

1. No, you should have all dependencies by adding all jars in lib/.

2. If you had imported the ""stratosphere-examples"" project as a maven project into Eclipse maven would have downloaded the 0.5-SNAPSHOT dependencies into your local Maven repository and created a correct CLASSPATH for you. 

3. No. The easiest way is to open a shell and enter
```bash
 curl https://raw.github.com/stratosphere/stratosphere-quickstart/master/quickstart.sh | bash
 ```
 and then import the contents of the quickstart/ directory into eclipse (or intellij) as a maven project.


;;;","09/Jun/14 12:25;github-import;[Date: Wed Mar 05 21:54:24 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

It might very well be I have version differences between the binaries and the source. FWIW, I followed the instructions at http://stratosphere.eu/downloads/ to get both the Ready to Run package and the source. I should have known better that I was getting the current snapshot.

Regardless, I am still confused by your apparent insistence on using/needing Maven to view/build/run the example source. Modulo the version problem, it looks like my trial establishes that Maven is not necessary. That is, including source code with the Ready to Run Package, and may be including some simple instructions, makes it extremely easy for users to taste test the examples

Am I right or am I missing something?;;;","09/Jun/14 12:25;github-import;[Date: Wed Mar 05 22:04:11 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

How exactly did you get the source? We do not have instructions on getting
the source for the release on the Downloads page.


The thing with Maven is not easy to understand, but let me try:

You (need) maven to*:
- build Stratosphere from the source code
- to build the code of a Stratosphere program you've written
- view the source code without stuff being marked as errors

*You can do everything manually (without Maven) if you want, but that would
be crazy. As I said before, you usually have Maven.

You do not need maven to:
- run Stratosphere from the ready to run packages
- to run the examples in the ready to run packages




On Wed, Mar 5, 2014 at 9:54 PM, nullusDefectus <notifications@github.com>wrote:

> It might very well be I have version differences between the binaries and
> the source. FWIW, I followed the instructions at
> http://stratosphere.eu/downloads/ to get both the Ready to Run package
> and the source. I should have known better that I was getting the current
> snapshot.
>
> Regardless, I am still confused by your apparent insistence on
> using/needing Maven to view/build/run the example source. Modulo the
> version problem, it looks like my trial establishes that Maven is not
> necessary. That is, including source code with the Ready to Run Package,
> and may be including some simple instructions, makes it extremely easy for
> users to taste test the examples
>
> Am I right or am I missing something?
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/520#issuecomment-36792155>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","09/Jun/14 12:25;github-import;[Date: Wed Mar 05 22:04:40 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

Looks like we were both commenting simultaneously.

It looks like you and I are in agreement for the most part, except I don't think Maven needs to be even mentioned in the context of the Ready to Run package: may be mention it as an aside; definitely not so prominently. ;;;","09/Jun/14 12:25;github-import;[Date: Wed Mar 05 23:04:05 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

http://stratosphere.eu/downloads/#source shows how to get the source code (without qualifying the version). The instructions there are simple and they work. It is just that I should have realized I might be getting code for a later build.

<pre>The thing with Maven is not easy to understand</pre> My point exactly.

<pre>You can do everything manually (without Maven) if you want, but that would be crazy. As I said before, you usually have Maven.</pre>

I disagree with both points. It is a lot easier to create an Eclipse project and add reference to lib than downloading and learning yet another tool. I wouldn't assume people *usually* have Maven.

Bottom line, I think it is great if Ready to Run Package includes example source and if users are able to view/build/run in their favorite IDE without installing anything else. It looks like that will be possible with the next stable release.
;;;","09/Jun/14 12:25;github-import;[Date: Thu Mar 06 00:09:11 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Hi @nullusDefectus,
First of all, thanks a lot for spending so much time with us, trying out the project, and giving lots of valuable feedback!

I also think, that the ""Ready to Run"" package should contain everything to run, view, and import  the examples into an IDE. We could build a example-source.jar (or zip) file that contains the source code of the examples and can be imported into an IDE *and* have the plain source code files in the examples folder for instant inspection with an editor.

To get the code in the IDE, one would follow the steps you did before:
1. create a new project
2. import code from examples-source.jar
3. set build path to ./lib and ./lib-clients

A side from the examples-source.jar, this should already work as @rmetzger said.
So, there is no Maven required for this. In fact Maven is not mentioned in the context of the ""Ready to Run"" section of the Downloads page.

What I get out of this conversation is, that the Downloads page is mainly targeted to the group of ""System Admins"", i.e., people who want to set-up the system in a cluster and less for users that want to inspect the system by digging into the examples and running them from IDE or shell. 
For these guys, we have the QuickStart section of our page (although the aspect of getting the code of the examples in your IDE is not really covered there either). 
I think that many people from the users or ""Developers"" group will have look at Downloads first where they might not find the information they are looking for.

In general, we should make sure that people find the information they are looking for on our website (even if they do not know how we intended the website to be used ;-) ).
So we should make sure that the interests of all users are met by the Downloads page, which is  the forth most visited page (after start, quickstart, and main-docs-page).

;;;","09/Jun/14 12:25;github-import;[Date: Thu Mar 06 11:28:35 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

We could have a prepared eclipse project that includes sources and required libs and is pre-configured. Then you can simply open that project in eclipse and start browsing. No additional tools required, no setup or cofiguration.;;;","09/Jun/14 12:25;github-import;[Date: Thu Mar 06 11:30:26 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The cleaner solution for any developer that wants to write serious applications is definitely Maven (or some other dependendy managemet tool), but if that is considered a blocker for getting started or browsing, the prepared eclipse project may be a solution?;;;","09/Jun/14 12:26;github-import;[Date: Thu Mar 06 11:50:31 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

It is possible to generate the Eclipse configuration file using Maven. However, I don't know if it is possible to create ""shipable"" maven projects.;;;","09/Jun/14 12:26;github-import;[Date: Thu Mar 06 12:10:27 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Not everybody is using Eclipse. 
Including an importable archive with the examples code in the binary distribution is the most accessible solution IMHO.;;;","09/Jun/14 12:26;github-import;[Date: Thu Mar 06 15:04:12 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

@fhueske: I think you have nailed the issue in your first comment here. BTW, the QuickStart guide requires Maven, and Maven makes quick and prominent appearance in this repository in pretty much any conversation on using Stratosphere. Both of these need to be addressed. Of course, improved documentation goes a long way.

@StephanEwen: I totally agree that any one developing complex projects benefits from using Maven, but trying out examples hardly qualifies. Your proposal of including a prepared Eclipse project might work (if that is feasible) assuming Eclipse is the more popular IDE.

Here is what I feel is necessary and sufficient for a beginner:

1. Include example source in the Ready to Run package. I believe this is already done.

2. No need for an examples jar. While the jar makes it easy to import all examples at once, it adds work if user wants to examine examples selectively/progressively.

3. Add a Quickstart secion such as ""Browse examples in IDE"" after the section ""Run Example"" (http://stratosphere.eu/quickstart/setup.html)
-- Use Eclipse as an example, or may be no need to call out any particular IDE.
-- Just three steps: Create an Eclipse project; Add Stratosphere libraries to project; Add WordCount.Java to project.
-- See below for thoughts on organizing example source

4. Link to Quickstart at the end of the Ready to Run Package section in the Downloads page.

Thoughts on organizing example source (Achtung: I have looked at only a few of the examples as of now, so these thoughts might not be ripe yet.)

It is unclear what ""record"" represents in the package hierarchy for examples. To my untrained eyes, it seems irrelevant at least to the wordcount examples. Perhaps this level can be removed.

I assume the examples illustrate using Stratosphere without Hadoop (WordCount), with Hadoop 1, with Hadoop 2, and YARN. It will be helpful to organize the examples along these lines. So, instead of ""record"", use intermediates such as ""nohadoop"", ""hadoop1"", ""hadoop2"", and ""yarn"". This proposed organization offers the following benefits:

1. Reflects the purpose of the examples.

2. Lines up with the Ready to Run Package user has downloaded. 

3. Helps include examples relevant only to the version downloaded, if you choose to.

4. Helps user work with increasing/varying complexity of examples. For example, user will know which examples need Hadoop installed.

Lastly, place each independent example in its own package/directory. In the current structure, I see  at least one package with multiple ""programs"", that is .java files with main method. For example, the wordcount package includes three such .java files. The file AnonymousWordCount.java has no main method and this class is not referenced in any of the other classes in the package. 

The pagerank package has 12 .java files, none of which have a main method. [Wipes glasses, checks again.]

My rule of thumb for examples: Unless the directory contains shared classes re-used across examples, each directory includes one and only .java file with a main method. I realize this approach increases the number of directories, but the user experience will be consistent. Alternatively, if a directory must contain multiple .java files with main method, use the suffix ""Program"" for the name of class/file that contains a main method and mention this pattern in QuickStart.;;;","09/Jun/14 12:26;github-import;[Date: Thu Mar 06 15:43:19 CET 2014, Author: [fhueske|https://github.com/fhueske]]

You are definitely right. Our example programs are in a messed up state right now. Over time, examples were added and modified and things became inconsistent.
Cleaning up the examples module is an issue, which I find very important. The structure / organization you suggested of having a single java file and putting all other classes (if necessary) in a sub-package sounds like a very good idea. I will open an issue with detailed information for this right now.

A few side notes:
- Stratosphere programs are totally independent on whether Hadoop is used or in which version Hadoop is used. So there is no need to differentiate there.
- The ``record`` package differentiates between the old (Record-based) and the new (more generic) Java API. The new API is currently under construction and will be promoted as the default API once it is ready.

I also support your suggestions for the website.

Thanks for the good input!;;;","09/Jun/14 12:26;github-import;[Date: Thu Mar 06 18:54:17 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

Clarifying my thumb rule for examples: It is ok if a package has multiple Java files, but one and only of the files should contain the main method. A trivial/simple example such as wordcount might have just one file in the package; a non-trivial example (pagerank?) might end up having multiple files, even sup-packages.

Please consider this clarification in the context of the discussion on Issue ([#538|https://github.com/stratosphere/stratosphere/issues/538] | [FLINK-538|https://issues.apache.org/jira/browse/FLINK-538]).

Here is my poor attempt to illustrate the package organization.

examples
-- common [files common to all examples]
-- example1
---- example1.java [has main method}
-- example2
---- example2.java [has main method]
---- e2support1.java [does not have main method; example2.java references something in this file]
-- example3
---- example3_1
-------- example3_1.java [has main method]
---- example3_2
-------- example3_2.java [has main method]
-------- e312.java [does not have main method; example3_2.java references something in this file]
---- common
-------- [files common to example3_1 and example3_2; none of these have main method];;;","09/Jun/14 12:26;github-import;[Date: Sun Mar 23 13:56:23 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

@rmetzger Is this pull request ready?;;;","09/Jun/14 12:26;github-import;[Date: Sun Mar 23 14:58:09 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

The pull request is as described in my initial issue. (So if you want that, you can merge it)
It does not reflect the results of the discussion (the source files are not ""import""-able into eclipse).;;;","09/Jun/14 12:26;github-import;[Date: Wed Mar 26 00:54:28 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged current version in [3b3260a6d7013fc8afb4fa9ac9744205aed50f81|https://github.com/stratosphere/stratosphere/commit/3b3260a6d7013fc8afb4fa9ac9744205aed50f81];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing source code for examples in the Ready to Run Package,FLINK-517,12719688,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:25,18/Jun/14 16:05,14/Jul/23 05:57,18/Jun/14 16:05,,,,pre-apache,,,,,,,0,github-import,,"Not sure if this is by design or by accident, but the Ready to Run Package includes jar files in the examples folder, but no source.

Presumably, the source code for the examples are included in the Stratosphere source (I have not looked at the source), but it will be instructive to include the source code for the examples even in the binaries-only package. 

BTW, I am curious if anyone has tried using the examples source (from the stratosphere-examples module) with just the Stratosphere binaries downloaded? Is it a question of just creating an eclipse project and referencing the jars in the ""lib"" and ""lib_clients"" folder?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/517
Created by: [nullusDefectus|https://github.com/nullusDefectus]
Labels: 
Created at: Fri Feb 28 20:48:42 CET 2014
State: open
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397887,,,Mon Jun 09 12:25:30 UTC 2014,,,,,,,,,,"0|i1wi2v:",398014,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:25;github-import;[Date: Sun Mar 02 00:17:44 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for the suggestion. I proposed this pull request to fix the issue: https://github.com/stratosphere/stratosphere/pull/520 I'll merge it once the other developers agree.

You actually do not have to manually reference the jar files in `lib` and `lib_clients` from your eclipse if you have imported the examples as Maven projects into Eclipse. That way, Eclipse will automatically download the required dependencies and add them to the classpath.
The same holds for the [quickstart|https://github.com/stratosphere/stratosphere-quickstart] scripts, which are based on the [maven archetype|https://maven.apache.org/guides/introduction/introduction-to-archetypes.html) plugin. Users that want to start using Stratosphere need only a Maven-enabled Eclipse to start writing Jobs. It is even possible to run the ""LocalExecutor"" (our integrated debugging facility] as it is also pulled from Maven central.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Detect Pipelined Dataflows that might lead to Deadlocks,FLINK-516,12719687,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:25,09/Jun/14 12:25,14/Jul/23 05:57,09/Jun/14 12:25,,,,pre-apache,,,,,,,0,github-import,,"Certain pipelined flows may lead to deadlocks, in which case we need to make sure the pipelines are broken or made elastic enough to prevent that.

This is only relevat to pipelined data flows where one operator has more than one consumers (successors in the flow).

Most cases are caught by the general logic that deals with branching/joining flows. The following cases need additional checks:

```
                    <build>
(source1) ------ (join)
          \    /    <probe>
           \  /
            \/
            /\
           /  \
          /    \    <probe>
(source2) ------(join)
                    <build>
```

Since both sources pipeline their data into a build and a probe side, they get stalled by  the back pressure from the probe side (which waits for the build side to complete) and never finish the build side.

We can model dependencies of pipelined / materialized connections and do a reguar cyclic dependencies check to detect such situations. Pipelined connections have a dependency from sender to receiver, non-pipelined (fully dammed) connections have a dependency from receiver to sender.

A [PlanNode|https://github.com/stratosphere/stratosphere/blob/master/stratosphere-compiler/src/main/java/eu/stratosphere/compiler/plan/PlanNode.java] and a [Channel|https://github.com/stratosphere/stratosphere/blob/master/stratosphere-compiler/src/main/java/eu/stratosphere/compiler/plan/Channel.java] are considered pipelined unless the following holds:

  - The channel has a [LocalStrategy|https://github.com/stratosphere/stratosphere/blob/master/stratosphere-runtime/src/main/java/eu/stratosphere/pact/runtime/task/util/LocalStrategy.java) where `LocalStrategy#dams(]`returns true.
  - The channel has a [TempMode|https://github.com/stratosphere/stratosphere/blob/master/stratosphere-compiler/src/main/java/eu/stratosphere/compiler/dag/TempMode.java) where `TempMode#breaksPipeline(]` is true.
  - The PlanNode has a [DriverStrategy] (https://github.com/stratosphere/stratosphere/blob/master/stratosphere-runtime/src/main/java/eu/stratosphere/pact/runtime/task/DriverStrategy.java) where the [DamBehavior] (https://github.com/stratosphere/stratosphere/blob/master/stratosphere-runtime/src/main/java/eu/stratosphere/pact/runtime/task/DamBehavior.java)  as returned by `firstDam()` or `secondDam()` for the respective input is a `FULL_DAM`.

Deadlocks can be resolved by adjusting the `DriverStrategy` such that the `DamBehavior` of the inputs changes to break the cyclic dependency. An example is switching the build- and probe- side of that hash join.

An alternative is introduce an arficial pipeline breaker at a channel, by calling `channel.setTempMode(channel.getTempMode().makePipelineBreaker())`.





---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/516
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: optimizer, 
Milestone: Release 0.5
Assignee: [markus-h|https://github.com/markus-h]
Created at: Fri Feb 28 19:18:48 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397886,,,Mon Jun 09 12:25:26 UTC 2014,,,,,,,,,,"0|i1wi2n:",398013,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:25;github-import;[Date: Thu Mar 06 11:31:15 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed via ([#527|https://github.com/stratosphere/stratosphere/issues/527] | [FLINK-527|https://issues.apache.org/jira/browse/FLINK-527]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offer buffer-oriented API for I/O (#25),FLINK-511,12719682,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:24,09/Jun/14 12:24,14/Jul/23 05:57,09/Jun/14 12:24,,,,pre-apache,,,,,,,0,github-import,,"The runtime offered a record-oriented API (([#25|https://github.com/stratosphere/stratosphere/issues/25] | [FLINK-25|https://issues.apache.org/jira/browse/FLINK-25])) for data transfers, which
* resulted in unnecessary data (de)serialization,
* complicated the upcoming fault tolerance implementation, and
* blocked more efficient implementations of higher-level operators.

With this commit, the runtime offers a buffer-oriented API for the output side (sending), which is oblivious to records. The buffer oriented input side (receiving) is still to be implemented.

---

* Previously, the I/O classes allowed different implementations of channels, gates etc. with the `ByteBuffered*` implementation being the default one. I've merged those classes to only have the default implementation (e.g. InputChannel, OutputChannel), which greatly simplifies the code.
* Every I/O class had an associated `*Context`, which was initiated at runtime for buffer allocation. All these classes could be removed, since the I/O stack does not allocate buffers anymore, but instead is fed buffers to send/receive.
* Record serialization has been moved from the channels (for the output side, input side is still pending) to the writers/readers. For example `RecordWriter` still offers the record-oriented API, but serializes records into buffers and dispatches full buffers via the new buffer-oriented API (`BufferWriter`). 
* The (previosuly singleton) `GlobalBufferPool` (for network buffers) is now instantiated by the `ChannelManager` for each TaskManager. The network buffers are used by `RecordWriter` for record serialization and by the input channels (will be changed) for deserialization. In the future, the buffers can directly come from the upper layers (formerly PACT).
* I've reorganized the touched classes into the package `eu.stratosphere.runtime.*` (previously `eu.stratosphere.nephele.*`) to start with the removal of the deprecated Nephele/PACT separation.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/511
Created by: [uce|https://github.com/uce]
Labels: runtime, 
Created at: Wed Feb 26 12:20:00 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:24;github-import;pull-request-511-316991910314196777.patch;https://issues.apache.org/jira/secure/attachment/12649179/pull-request-511-316991910314196777.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397881,,,Mon Jun 09 12:24:45 UTC 2014,,,,,,,,,,"0|i1wi1j:",398008,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:24;github-import;[Date: Tue Mar 25 21:50:54 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged into the development branch for network stack reorganization and fault tolerance at https://github.com/StephanEwen/stratosphere/tree/netstack_rework as of [4506d1f973121b6833aec0a9c52f4d7cfe69f6f9|https://github.com/stratosphere/stratosphere/commit/4506d1f973121b6833aec0a9c52f4d7cfe69f6f9];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve virtual machines and document them,FLINK-510,12719681,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:24,09/Jun/14 12:24,14/Jul/23 05:57,09/Jun/14 12:24,,,,pre-apache,,,,,,,0,github-import,,"The virtual machines are currently deployed without a proper Stratosphere installation (the source is present, but there is no binary build).
The also has to be a README file or so that explains the first steps.


I would suggest to install a minimal GUI, a web-browser with some bookmarks to the Stratosphere webinterface.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/510
Created by: [rmetzger|https://github.com/rmetzger]
Labels: build system, duplicate, website, 
Milestone: Release 0.5
Created at: Wed Feb 26 10:53:25 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397880,,,Mon Jun 09 12:24:21 UTC 2014,,,,,,,,,,"0|i1wi1b:",398007,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:24;github-import;[Date: Wed Feb 26 10:57:35 CET 2014, Author: [fhueske|https://github.com/fhueske]]

duplicate ([#265|https://github.com/stratosphere/stratosphere/issues/265] | [FLINK-265|https://issues.apache.org/jira/browse/FLINK-265]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly mark Scala UDF and InputFormat fields transient,FLINK-508,12719679,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:24,09/Jun/14 12:24,14/Jul/23 05:57,09/Jun/14 12:24,,,,pre-apache,,,,,,,0,github-import,,"Several fields of Scala UDFs contain values that need not be serialized, because they are initialized in configure() or open().

In some cases, these fields may contain non-serializable values, causing the programs to fail.

All basic function fields (serializers, etc...) should be marked as transient, if they should not be part of the serialized object.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/508
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Assignee: [aljoscha|https://github.com/aljoscha]
Created at: Wed Feb 26 02:24:30 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397878,,,Mon Jun 09 12:24:15 UTC 2014,,,,,,,,,,"0|i1wi0v:",398005,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:24;github-import;[Date: Thu Feb 27 17:55:39 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [6d4ec0edbf7d5a4ab9149f74012d69ffc4fbb511|https://github.com/stratosphere/stratosphere/commit/6d4ec0edbf7d5a4ab9149f74012d69ffc4fbb511];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Distributed Cache,FLINK-506,12719677,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:24,18/Jun/14 16:07,14/Jul/23 05:57,18/Jun/14 16:07,,,,pre-apache,,,,,,,0,github-import,,"Add a Hadoop-like Distributed Cache utility.

The distributed cache is needed for tasks that require files to be accessible with local file paths. Files are registered at the Program level with (file, name). The utility must take the files and write them to the local temp directory, assigning them a random file name and make the path available under the name.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/506
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Wed Feb 26 00:58:54 CET 2014
State: open
",,github-import,,,,,,,,,,,,,,,,FLINK-564,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397876,,,Mon Jun 09 12:24:10 UTC 2014,,,,,,,,,,"0|i1wi0f:",398003,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:24;github-import;[Date: Thu Mar 06 12:05:21 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The distributed cache would be usable as follows, from the perspective of the someone who writes a Stratosphere program:

  1. The files of interest must be stored in a place that can be accessed from all workers (most commonly HDFS)

  2. When submitting a job, you register the files that you need locally cached at the job. For the new java API, that would be the `ExecutionEnvironment`, for the old api the `Plan`. The method could be called `registerCachedFile(filePath, name)`.
 
  3. Within a task (let's say in the `open()` method) , you access the cache to give you the local path of that file `File f = getRuntimeContext().getDistributedCache().getFile(name)`

Under the hood, we need a the following (here is a suggestion, feel free to suggest improvements):
  - Registered cached files are written into the job configuration (the same way as the job-wide switch to activate/deactivate the accumulators works)
  - The TaskManager has a component called `FileCache`. When a task is deployed to the TaskManager, it checks that job configuration for registered (filePath, name) entries.
  - The FileCache is invoked with that path and name and the job id (to qualify the name by job). If that file is not locally cached, it copies the file to the temp directory (same temp directory as the IOManager uses)
  - When a task is unregistered, the cache should remove the tmp file. Given that another task from the same job may be registered shortly after, I suggest to have a 5 second delay (or so) before clearing files, to prevent unnecessary repeated downloads from HDFS to the local tmp directory.;;;","09/Jun/14 12:24;github-import;[Date: Thu Mar 06 12:15:56 CET 2014, Author: [fhueske|https://github.com/fhueske]]

That would introduce a hard dependency on an HDFS setup when ever you want to use the Distributed Cache feature.

We should at least add an exception for Stratosphere's local mode, and maybe later also for setups with no HDFS (although these should be rather rare), for example via shared dirs such as NFS.
;;;","09/Jun/14 12:24;github-import;[Date: Thu Mar 06 12:29:34 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I don't think it needs. If we add files with the normal URIs  (hdfs:///some/file/) or (file:///another/file), we can use the `FileSystem` api to copy files. That should transparently handle it, independent of HDFS, no?;;;","09/Jun/14 12:24;github-import;[Date: Thu Mar 06 12:34:04 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Sure, but somewhere the file system needs to be chosen.
All I am saying is, that we should not rely on HDFS for the local case and add support for other file systems in distributed environments later.;;;","09/Jun/14 12:24;github-import;[Date: Thu Mar 06 13:19:03 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Eh, OK. I got it wrong...

Proposal sounds good to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bleeding menu on Downloads and Documentation web pages,FLINK-504,12719675,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:23,09/Jun/14 12:23,14/Jul/23 05:57,09/Jun/14 12:23,,,,pre-apache,,,,,,,0,github-import,,"This is probably low priority, but I notice that the menu in the left sidebar on the Downloads and the Documentation page bleeds on to content when the page is scrolled down.

The page is responsive for the most part, except when I dock the page left/right on the screen. Notice that the menu isn't collapsed to the right-hand top corner of the page as it should.

My monitor is 24"" set to 1920x1080 (native) resolution. I am running Chrome ""33.0.1750.117 m"" on Windows 8.1. 

![image|https://f.cloud.github.com/assets/6477490/2264720/00640024-9e76-11e3-945d-3e0f2c1eb363.png]

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/504
Created by: [nullusDefectus|https://github.com/nullusDefectus]
Labels: bug, website, 
Created at: Wed Feb 26 00:46:56 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397874,,,Mon Jun 09 12:23:56 UTC 2014,,,,,,,,,,"0|i1whzz:",398001,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:23;github-import;[Date: Sat Mar 01 21:21:43 CET 2014, Author: [uce|https://github.com/uce]]

Thank you for posting this. I will look into it after I'm back from holidays, if nobody else finds the time.

I think the left sub navigation needs a proper fix, where it collapses earlier and for the top navigation we should think about moving some links to a dropdown, for example Events and Contact to the existing Project dropdown?;;;","09/Jun/14 12:23;github-import;[Date: Sun Mar 02 09:28:06 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thanks for reporting the issue. I fixed it in commit: [3cf916d3b1083177fbd22474de4263e76df3106e|https://github.com/stratosphere/stratosphere/commit/3cf916d3b1083177fbd22474de4263e76df3106e];;;","09/Jun/14 12:23;github-import;[Date: Sun Mar 02 13:21:31 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

Thanks @rmetzger.

I notice a minor problem with the fix. When the screen is narrow and the left menu is collapsed, everything looks fine, but not when the left menu is expanded, about a half of the first entry in the left menu is hidden. I believe the difference is the changing height of the top menu, illustrated below with four different screen (progressively smaller) widths.

Screen wide enough for full top-menu
------

![image|https://f.cloud.github.com/assets/6477490/2304108/6cd062a2-a204-11e3-8eb6-c048cd937acb.png]

Narrower screen, but top menu not yet collapsed
----
![image|https://f.cloud.github.com/assets/6477490/2304109/83dd3830-a204-11e3-8785-5076a3cb1186.png)

Even narrower screen (~20%?], top menu not yet collapsed
--
![image|https://f.cloud.github.com/assets/6477490/2304113/9bc2f0a2-a204-11e3-9188-fcba43d006b4.png]

Screen narrow enough for top menu to collapse
-------
![image|https://f.cloud.github.com/assets/6477490/2304114/bb56d9a6-a204-11e3-9db7-246f54dc63e4.png]

;;;","09/Jun/14 12:23;github-import;[Date: Sun Mar 02 14:00:21 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi, thanks for your feedback.
I'll ask somebody to fix the remaining issues with the website.
;;;","09/Jun/14 12:23;github-import;[Date: Sun Mar 02 14:06:11 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

I notice a couple of other issues, but I will wait for the currently known issues to be fixed. 

A quick question: What is the expectation for the web site? Is it expected to be responsive among desktops, tablets, and phones? For example, I see a couple of issues when viewed from my tablet, but I don't want to report them if that is not a priority. Also, I can conserve the time it takes me to understand and document the problems.;;;","09/Jun/14 12:23;github-import;[Date: Sun Mar 02 14:12:29 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Mh. Good question. I think we have not discussed that so far.

I personally want to have a very good user experience on desktops. The responsiveness is a nice bonus and should also work, but our project has a lot of other very important issues to take care about. Finding somebody who's able to do some HTML and CSS is rather simple compared to finding somebody who's able to develop a database engine ;)
Also, we get very positive feedback from many users for our website. Compare it to the Hadoop website, I think we have already a pretty good standing.
I'm very happy about your feedback and I think a step-by-step approach is the most efficient solution right now. Lets see how the fixing of the current issue works out, then, we'll address the next one.;;;","09/Jun/14 12:23;github-import;[Date: Sun Mar 02 14:24:25 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

:+1: 

Having managed products and businesses end to end, I completely understand your position on wanting to provide a good user experience and balancing it with making progress on other fronts.

Until I hear otherwise, I will limit my reports on web UI to just the desktop.;;;","09/Jun/14 12:23;github-import;[Date: Tue Mar 04 22:56:35 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

We resolved the issue (with PR https://github.com/stratosphere/stratosphere.github.io/pull/2);;;","09/Jun/14 12:23;github-import;[Date: Wed Mar 05 23:06:08 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

Excellent. I will look at the revision in the next few days.;;;","09/Jun/14 12:23;github-import;[Date: Wed Apr 16 15:19:47 CEST 2014, Author: [JonathanH5|https://github.com/JonathanH5]]

I think we can close this issue, because it was fixed around a month ago... . ;;;","09/Jun/14 12:23;github-import;[Date: Wed Apr 16 15:28:40 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Right, closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Download Virtual Machine"" from the Download page fails",FLINK-503,12719674,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:23,09/Jun/14 12:23,14/Jul/23 05:57,09/Jun/14 12:23,,,,pre-apache,,,,,,,0,github-import,,"I just learned about Stratoshpere from Volker Markl. I am very impressed by the little I have read so far about the system and am eager to try it out. 

I attempted to download the VM image from the Downloads page (http://stratosphere.eu/downloads/), but that process errs out after spinning for 20 seconds or so. I imagine there is a time out. Given that I just downloaded a 100MB VirtualBox installer package in under 30 seconds over my 50 Mbps connection, I am guessing the time out is on the server side.

Would someone please take a look at this issue? Perhaps there is a problem on my side?

![image|https://f.cloud.github.com/assets/6477490/2264458/78b27e10-9e72-11e3-816c-fee336ea106b.png]

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/503
Created by: [nullusDefectus|https://github.com/nullusDefectus]
Labels: 
Created at: Wed Feb 26 00:24:03 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397873,,,2014-06-09 12:23:46.0,,,,,,,,,,"0|i1whzr:",398000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Download Virtual Machine"" from the Download page fails",FLINK-502,12719673,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:23,09/Jun/14 12:23,14/Jul/23 05:57,09/Jun/14 12:23,,,,pre-apache,,,,,,,0,github-import,,"I just learned about Stratoshpere from Volker Markl. I am very impressed by the little I have read so far about the system and am eager to try it out. 

I attempted to download the VM image from the Downloads page (http://stratosphere.eu/downloads/), but that process errs out after spinning for 20 seconds or so. I imagine there is a time out. Given that I just downloaded a 100MB VirtualBox installer package in under 30 seconds over my 50 Mbps connection, I am guessing the time out is on the server side.

Would someone please take a look at this issue? Perhaps there is a problem on my side?

![image|https://f.cloud.github.com/assets/6477490/2264458/78b27e10-9e72-11e3-816c-fee336ea106b.png]

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/502
Created by: [nullusDefectus|https://github.com/nullusDefectus]
Labels: 
Created at: Wed Feb 26 00:21:57 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397872,,,Mon Jun 09 12:23:44 UTC 2014,,,,,,,,,,"0|i1whzj:",397999,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:23;github-import;[Date: Wed Feb 26 00:26:07 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I cannot access it either. Seems that the server `dev.stratosphere.eu` is down. It is a hosted VM at TU Berlin. I can ping the ops team only tomorrow morning (CET). Sorry for that.;;;","09/Jun/14 12:23;github-import;[Date: Wed Feb 26 00:26:09 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thanks for reporting the issue!
I was able to confirm it. I'll talk to our infrastructure people to fix the
issue asap.

Sorry for the inconvenience, we will look into a more reliable hosting
solution.


Sent from my iPhone

On 26.02.2014, at 00:21, ssmurthy <notifications@github.com> wrote:

I just learned about Stratoshpere from Volker Markl. I am very impressed by
the little I have read so far about the system and am eager to try it out.

I attempted to download the VM image from the Downloads page (
http://stratosphere.eu/downloads/), but that process errs out after
spinning for 20 seconds or so. I imagine there is a time out. Given that I
just downloaded a 100MB VirtualBox installer package in under 30 seconds
over my 50 Mbps connection, I am guessing the time out is on the server
side.

Would someone please take a look at this issue? Perhaps there is a problem
on my side?

[image: image]<https://f.cloud.github.com/assets/6477490/2264458/78b27e10-9e72-11e3-816c-fee336ea106b.png>

--
Reply to this email directly or view it on
GitHub<https://github.com/stratosphere/stratosphere/issues/502>
.;;;","09/Jun/14 12:23;github-import;[Date: Wed Feb 26 00:27:38 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

No problem. It seems I somehow managed to submit the same issue twice, but don't see a way to delete the dupe. 

Would someone with more power (or better eyesight) than me delete the issue that has no comments on it yet?

Thanks.;;;","09/Jun/14 12:23;github-import;[Date: Wed Feb 26 10:43:15 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

The server is up again.

Thanks again for reporting the issue. Your feedback is very valuable for us!

I have to tell you that the virtual machine is not really helpful at this point. I just tried it out and some things are not working and its not documented at all.
I will remove the virtual machine download link from our website for now until we have reworked it a bit.

We extended Stratosphere's Windows support, so it should be possible to start it using the ""start-local.bat"" file. Development from an IDE is also possible with Windows.

Sorry for the inconveniences.;;;","09/Jun/14 12:23;github-import;[Date: Wed Feb 26 10:54:37 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I filed to issues to track the virtual machine fixes: https://github.com/stratosphere/stratosphere/issues/509, https://github.com/stratosphere/stratosphere/issues/510.
I removed the virtual machine download from the website in [cbab020d630b9910753da4048ecc283970f3abb0|https://github.com/stratosphere/stratosphere/commit/cbab020d630b9910753da4048ecc283970f3abb0].;;;","09/Jun/14 12:23;github-import;[Date: Wed Feb 26 15:21:36 CET 2014, Author: [nullusDefectus|https://github.com/nullusDefectus]]

Thanks for the quick response. I am a bit sad the VM option is gone. Hopefully it will come back soon.

PS: I changed my github user name because I notice a new user has a very similar name to mine and I want to avoid confusion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pass ""env.java.opts"" configuration values to YARN JVMs",FLINK-501,12719672,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:23,09/Jun/14 12:23,14/Jul/23 05:57,09/Jun/14 12:23,,,,pre-apache,,,,,,,0,github-import,,"There is currently no way to set custom JVM arguments for the JVMs started by YARN.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/501
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue Feb 25 21:34:38 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397871,,,2014-06-09 12:23:38.0,,,,,,,,,,"0|i1whzb:",397998,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor fix on UserCodeClassLoader,FLINK-499,12719670,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:23,09/Jun/14 12:23,14/Jul/23 05:57,09/Jun/14 12:23,,,,pre-apache,,,,,,,0,github-import,,"Accept transient fields as they are not serialized.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/499
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Feb 24 14:09:14 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:23;github-import;pull-request-499-1328531170406298066.patch;https://issues.apache.org/jira/secure/attachment/12649178/pull-request-499-1328531170406298066.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397869,,,Mon Jun 09 12:23:33 UTC 2014,,,,,,,,,,"0|i1whyv:",397996,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:23;github-import;[Date: Wed Feb 26 10:14:04 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [77b88ac13faa2aac19b8d0c5a9d9273b14276214|https://github.com/stratosphere/stratosphere/commit/77b88ac13faa2aac19b8d0c5a9d9273b14276214];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroInputFormat for generic objects,FLINK-498,12719669,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:23,09/Jun/14 12:23,14/Jul/23 05:57,09/Jun/14 12:23,,,,pre-apache,,,,,,,0,github-import,,"An input format for avro that serializes avro object directly. It compplements the existing AvroRecordInputformat that works with the generic record from avro, and converts its fields individually, putting them into a startosphere record.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/498
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Mon Feb 24 14:08:28 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:23;github-import;pull-request-498-6475353120103969723.patch;https://issues.apache.org/jira/secure/attachment/12649177/pull-request-498-6475353120103969723.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397868,,,2014-06-09 12:23:24.0,,,,,,,,,,"0|i1whyn:",397995,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug In UserCodeObjectWrapper (serialization copy),FLINK-497,12719668,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:23,09/Jun/14 12:23,14/Jul/23 05:57,09/Jun/14 12:23,,,,pre-apache,,,,,,,0,github-import,,"The UserCodeObjectWrapper has a bug when creating a copy of the user code object. The copy is made through serialization/deserialization. During deserialization, classes are resolved with Class.forName(...), which requires a class loader. The deserialization phase has, however, no access to the UserCodeClassLoader, but uses the class loaded that was used to load the class of the to-be-cloned object.

While this works for most cases, it does not work if the object itself is part of the application (in the lib directory and hence loaded with the AppClassLoader), but has some parameters that are part of the user code. An example is the AvroInputFormat. The format itself is part of the Stratosphere Framework. It has as a parameter the usercode class to be deserialized with Avro. When cloning an instance of the AvroInputFormat, the format class itself can be resolved, but the parameter class cannot be resolved.

The object wrapper itself is deserialized from the task config using the user code class loader, hence everything works there. The problem is really just the additional serialization when creating a copy of the object.

A simple fix is not to clone in the input format at all. This should be fine in all cases we currently have. Even in input formats, which are used in various roles (create split, create statistics, create records), this should not be a problem: The statistics are created in the compiler (before sending the job graph to the job manager). The splits are created on the job manager (object is freshly deserialized from the job graph parameters). The records are created in the data source tasks (input format object is freshly deserialized from the task parameters).

Unless I overlook something, I would suggest to drop the copy step. Would also save sone cycles during execution.

@aljoscha : Please double check my assumptions, as you wrote the UserCodeWrapper initially.



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/497
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Assignee: [aljoscha|https://github.com/aljoscha]
Created at: Mon Feb 24 14:00:58 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397867,,,Mon Jun 09 12:23:22 UTC 2014,,,,,,,,,,"0|i1whyf:",397994,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:23;github-import;[Date: Mon Feb 24 14:09:39 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I have found another issue with the UserCodeWrapper: https://github.com/stratosphere/stratosphere/pull/499;;;","09/Jun/14 12:23;github-import;[Date: Thu Feb 27 17:59:58 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [93ecfc6406298b0e91e5298907e165fe391b3f6f|https://github.com/stratosphere/stratosphere/commit/93ecfc6406298b0e91e5298907e165fe391b3f6f] and [6cd4299723f13368cf23cff5831bf9404ef55b03|https://github.com/stratosphere/stratosphere/commit/6cd4299723f13368cf23cff5831bf9404ef55b03];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stratosphere Developer Overview Images,FLINK-496,12719667,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mholzemer,github-import,github-import,09/Jun/14 12:23,18/Jun/14 16:08,14/Jul/23 05:57,18/Jun/14 16:08,,,,pre-apache,,,,,,,0,github-import,,"I'm currently trying to create some overview images for developers to give a quick initial overview of how different components of Stratosphere are working together.
The aim is to make it easier for new developers to get a general understanding of the overall architecture of the whole project and of single components.

I found it quite hard to find a good way of presentation, that's why I would appreciate some comments and suggestions.
My initial try can be found here: https://github.com/stratosphere/stratosphere/wiki/DeveloperOverview

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/496
Created by: [markus-h|https://github.com/markus-h]
Labels: documentation, website, 
Created at: Mon Feb 24 13:52:51 CET 2014
State: open
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397866,,,Mon Jun 09 12:23:19 UTC 2014,,,,,,,,,,"0|i1why7:",397993,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:23;github-import;[Date: Mon Feb 24 14:30:23 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Nice start. There are two comments:

  1. The local executor is not a part of the compile/execute chain. It is
rather one every point. I would remove it from the picture.

  2. The union replacer occurs twice.

Looks good otherwise.
Am 24.02.2014 13:52 schrieb ""markus-h"" <notifications@github.com>:

> I'm currently trying to create some overview images for developers to give
> a quick initial overview of how different components of Stratosphere are
> working together.
> The aim is to make it easier for new developers to get a general
> understanding of the overall architecture of the whole project and of
> single components.
>
> I found it quite hard to find a good way of presentation, that's why I
> would appreciate some comments and suggestions.
> My initial try can be found here:
> https://github.com/stratosphere/stratosphere/wiki/DeveloperOverview
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/496>
> .
>;;;","09/Jun/14 12:23;github-import;[Date: Tue Feb 25 09:31:55 CET 2014, Author: [markus-h|https://github.com/markus-h]]

Thanks for your feedback! I adjusted the images accordingly.;;;","09/Jun/14 12:23;github-import;[Date: Wed Apr 16 22:17:21 CEST 2014, Author: [JonathanH5|https://github.com/JonathanH5]]

Hi, are you still working on it? I think this is a great idea because Stratosphere is a little bit overwhelming at the beginning. If not this issue can be closed ... .;;;","09/Jun/14 12:23;github-import;[Date: Thu Apr 17 17:00:03 CEST 2014, Author: [markus-h|https://github.com/markus-h]]

I had a discussion on this issue with some other developers a few days ago and we agreed that it would be really nice to have a more or less complete development overview of the system for new developers. There will be some efforts torwards documentation during the next weeks, so perhaps we should keep this issue open for now.;;;","09/Jun/14 12:23;github-import;[Date: Thu Apr 17 17:15:10 CEST 2014, Author: [JonathanH5|https://github.com/JonathanH5]]

I would appreciate something like this.;;;","09/Jun/14 12:23;github-import;[Date: Thu Apr 17 23:30:54 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Yes, I definitely agree that we need (better) documentation of the projects internals to help new contributors to get familiar with the codebase. 

I think we need a separate section on the website for this purpose.
What do you think?;;;","09/Jun/14 12:23;github-import;[Date: Fri Apr 18 11:51:34 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I would suggest to use the github wiki for that. Also easier to extend and
update. We can link to it from the website.;;;","09/Jun/14 12:23;github-import;[Date: Fri Apr 18 13:03:00 CEST 2014, Author: [fhueske|https://github.com/fhueske]]

Yes, I agree. The Github wiki is the right place to put this information.;;;","09/Jun/14 12:23;github-import;[Date: Fri Apr 18 13:06:18 CEST 2014, Author: [uce|https://github.com/uce]]

+1

It's also possible to generate the website docs from the GitHub wiki. I know that netty.io is doing something along those lines. I will look into it later.

Sent from my iPhone

> On 18 Apr 2014, at 13:03, Fabian Hueske <notifications@github.com> wrote:
> 
> Yes, I agree. The Github wiki is the right place to put this information.
> 
> —
> Reply to this email directly or view it on GitHub.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix NullPointException in web interface when inspecting a job immediately after it finished,FLINK-495,12719666,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:23,09/Jun/14 12:23,14/Jul/23 05:57,09/Jun/14 12:23,,,,pre-apache,,,,,,,0,github-import,,"issue ([#464|https://github.com/stratosphere/stratosphere/issues/464] | [FLINK-464|https://issues.apache.org/jira/browse/FLINK-464])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/495
Created by: [qmlmoon|https://github.com/qmlmoon]
Labels: 
Created at: Mon Feb 24 11:29:47 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:23;github-import;pull-request-495-8625949103585512933.patch;https://issues.apache.org/jira/secure/attachment/12649176/pull-request-495-8625949103585512933.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397865,,,Mon Jun 09 12:23:12 UTC 2014,,,,,,,,,,"0|i1whxz:",397992,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:23;github-import;[Date: Mon Feb 24 11:50:15 CET 2014, Author: [markus-h|https://github.com/markus-h]]

Where exactly did this NullPointerException occur? Is events null? Because we're iterating over it at two places.
If I understand your fix correctly you are iterating now fully over jobmanager.getOldJobs() and thereby writing out all state changes of all archived jobs. That produces a lot of network overhead because we are only intersted in the state changes of one specific job.;;;","09/Jun/14 12:23;github-import;[Date: Wed Feb 26 11:46:38 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

@markus-h Are your concerns addressed by the last commit? (So is the PR ready to merge?);;;","09/Jun/14 12:23;github-import;[Date: Wed Feb 26 13:56:34 CET 2014, Author: [markus-h|https://github.com/markus-h]]

Yes it is addressed and can be merged.
For the future I'm still not sure if the new solution is a good one. 8 secs wait time is really long. Perhaps we should think about a better solution at some time.;;;","09/Jun/14 12:23;github-import;[Date: Wed Feb 26 19:21:18 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

So the refresh-rate of the interface (also for running jobs?) has been set to 8 seconds with this pull request?;;;","09/Jun/14 12:23;github-import;[Date: Wed Feb 26 19:53:46 CET 2014, Author: [qmlmoon|https://github.com/qmlmoon]]

No. Only for the job that is finished, it will be shown in the job history
8 secs later after it finishes


2014-02-26 19:21 GMT+01:00 Robert Metzger <notifications@github.com>:

> So the refresh-rate of the interface (also for running jobs?) has been set
> to 8 seconds with this pull request?
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/495#issuecomment-36158099>
> .
>;;;","09/Jun/14 12:23;github-import;[Date: Tue Mar 04 01:24:36 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [7aacbf3994cca859d4097ebea117934899a7b684|https://github.com/stratosphere/stratosphere/commit/7aacbf3994cca859d4097ebea117934899a7b684];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Apache Rat-based license checking to maven,FLINK-494,12719665,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:22,09/Jun/14 12:23,14/Jul/23 05:57,09/Jun/14 12:23,,,,pre-apache,,,,,,,0,github-import,,"http://creadur.apache.org/rat/

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/494
Created by: [rmetzger|https://github.com/rmetzger]
Labels: build system, 
Milestone: Release 0.5
Created at: Sat Feb 22 14:34:01 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397864,,,Mon Jun 09 12:23:03 UTC 2014,,,,,,,,,,"0|i1whxr:",397991,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:22;github-import;[Date: Thu Mar 06 20:13:39 CET 2014, Author: [zerolevel|https://github.com/zerolevel]]

Hi rmetzger,
I have added the rat repository.
But it points to unavailable licenses in many files.
Should I create this push by adding licenses to them all ?  
Or just add the apache-rat(dependency and the plugin information) details in pom.xml ?;;;","09/Jun/14 12:23;github-import;[Date: Thu Mar 06 20:18:06 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

If you could create a pull request that adds the missing licenses, that'd
be great :-)
Thanks a lot!;;;","09/Jun/14 12:23;github-import;[Date: Thu Mar 06 20:18:15 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,
cool that you've started working on the issue.

Have a look how Apache Giraph is integrating it (https://github.com/apache/giraph/blob/release-1.0/pom.xml).
They include it into the `verify` phase, so that a build will fail if it does not have the appropriate license headers. As you can see there, they exclude some directories. You should do that too where it makes sense (for example `target/` directories, the readme etc.)

And yes, please add the headers to files where it is missing. I would like to have a pull request where the plugin is added + the checks are passing.;;;","09/Jun/14 12:23;github-import;[Date: Sat Mar 08 10:02:06 CET 2014, Author: [zerolevel|https://github.com/zerolevel]]

hi stephan and rmetzger,

Please find the PULL request for this issue.

https://github.com/stratosphere/stratosphere/pull/546

Waiting to hear from you regarding this.

Mohit.;;;","09/Jun/14 12:23;github-import;[Date: Sat Mar 08 10:24:13 CET 2014, Author: [zerolevel|https://github.com/zerolevel]]

To check if RAT plugin works use the following command

 mvn org.apache.rat:apache-rat-plugin:check
;;;","09/Jun/14 12:23;github-import;[Date: Sun Mar 09 12:44:53 CET 2014, Author: [zerolevel|https://github.com/zerolevel]]

Hii.
I have edited it as suggested by you.
I think this is ready to be merged now. ;);;;","09/Jun/14 12:23;github-import;[Date: Sun Mar 09 12:47:18 CET 2014, Author: [zerolevel|https://github.com/zerolevel]]

Hi robert,

Can you give me some tasks from the sql repository you are working on. I would be happy to do them and get some experience.

Thanks.
Mohit;;;","09/Jun/14 12:23;github-import;[Date: Sun Mar 09 12:50:44 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,

sure. Help is always very welcome. Have a look at this issue here: https://github.com/rmetzger/stratosphere-sql/issues/3
I outline three possible tasks.
Make sure you coordinate with @camelia-c (via the comments there) so that you do not work at the same stuff.

Please tell me if you want more suggested tasks;;;","09/Jun/14 12:23;github-import;[Date: Sun Mar 09 18:16:10 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Resolved in https://github.com/stratosphere/stratosphere/pull/546. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement BCVars with branching,FLINK-491,12719662,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:22,09/Jun/14 12:22,14/Jul/23 05:57,09/Jun/14 12:22,,,,pre-apache,,,,,,,0,github-import,,"Here is a simple test case for the BCVars branching functionality which need to be implemented. The test job evaluates polynomials:

```
           Sc1(id,a,b,c) --
                           \
Sc2(id,x) --------          Jn2(id) -- Mp2 -- Sk
                  \        /          /
                   Jn1(id) -- Mp1 ----
                  /
Sc3(id,y) --------
```

The operators in this dataflow graph implement the following functionality:

* `Sc1` generates M parameters `a,b,c` for second degree polynomials `P(x) = ax^2 + bx + c` identified by `id`.
* `Sc2` generates N `x` values to be evaluated with the polynomial identified by `id`.
* `Sc3` generates N `y` values to be evaluated with the polynomial identified by `id`.
* `Jn1` matches `x` and `y` values on `id` and emits `(id, x, y)` triples.
* `Jn2` matches polynomial and arguments by `id`, computes `p = min(P(x),P(y))` and emits `(id, p)` tuples.
* `Mp1` selects `(id, x, y)` triples where `x = y` and broadcasts `z` (=x=y) to `Mp2`.
* `Mp2` filters out all `p` values which can be divided by `z`.

With a suiting N and M values and selectivity for `Mp1` the optimizer prefer pick a re-partition strategy for `Jn1`.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/491
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Created at: Fri Feb 21 16:56:59 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397861,,,Mon Jun 09 12:22:50 UTC 2014,,,,,,,,,,"0|i1whx3:",397988,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:22;github-import;[Date: Thu Feb 27 17:52:17 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Here is another program that does not run without broadcast variable branching logic. It is a variant of K Means that additionally tags the points at the end.

```
/***********************************************************************************************************************
 * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
 *
 * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
 * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
 * specific language governing permissions and limitations under the License.
 **********************************************************************************************************************/

package eu.stratosphere.example.java.record.kmeans;


import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;

import eu.stratosphere.api.common.Plan;
import eu.stratosphere.api.common.Program;
import eu.stratosphere.api.common.ProgramDescription;
import eu.stratosphere.api.common.operators.BulkIteration;
import eu.stratosphere.api.common.operators.FileDataSink;
import eu.stratosphere.api.common.operators.FileDataSource;
import eu.stratosphere.api.java.record.functions.MapFunction;
import eu.stratosphere.api.java.record.functions.ReduceFunction;
import eu.stratosphere.api.java.record.io.CsvInputFormat;
import eu.stratosphere.api.java.record.io.FileOutputFormat;
import eu.stratosphere.api.java.record.operators.MapOperator;
import eu.stratosphere.api.java.record.operators.ReduceOperator;
import eu.stratosphere.api.java.record.operators.ReduceOperator.Combinable;
import eu.stratosphere.client.LocalExecutor;
import eu.stratosphere.configuration.Configuration;
import eu.stratosphere.types.DoubleValue;
import eu.stratosphere.types.IntValue;
import eu.stratosphere.types.Record;
import eu.stratosphere.types.Value;
import eu.stratosphere.util.Collector;


public class KMeansTutorialExample implements Program, ProgramDescription {
	
	private static final long serialVersionUID = 1L;

	@Override
	@SuppressWarnings(""unchecked"")
	public Plan getPlan(String... args) {
		
		// parse job parameters
		final int numSubTasks = (args.length > 0 ? Integer.parseInt(args[0]) : 1);
		final String dataPointInput = (args.length > 1 ? args[1] : """");
		final String clusterInput = (args.length > 2 ? args[2] : """");
		final String output = (args.length > 3 ? args[3] : """");
		final int numIterations = (args.length > 4 ? Integer.parseInt(args[4]) : 2);
		
		final String centersOutput = output + ""/centers"";
		final String taggedPointsOutput = output + ""/points"";

		// create DataSourceContract for data point input
		FileDataSource pointsSource = new FileDataSource(new CsvInputFormat('|', IntValue.class, DoubleValue.class, DoubleValue.class, DoubleValue.class), dataPointInput, ""Data Points"");

		// create DataSourceContract for cluster center input
		FileDataSource clustersSource = new FileDataSource(new CsvInputFormat('|', IntValue.class, DoubleValue.class, DoubleValue.class, DoubleValue.class), clusterInput, ""Centers"");
		
		MapOperator dataPoints = MapOperator.builder(new PointBuilder()).name(""Build data points"").input(pointsSource).build();
		
		MapOperator clusterPoints = MapOperator.builder(new PointBuilder()).name(""Build cluster points"").input(clustersSource).build();
		
		// ---------------------- Begin K-Means Loop ---------------------
		
		BulkIteration iter = new BulkIteration(""k-means loop"");
		iter.setInput(clusterPoints);
		iter.setMaximumNumberOfIterations(numIterations);

		// compute the distances and select the closest center
		MapOperator findNearestClusterCenters = MapOperator.builder(new SelectNearestCenter())
			.setBroadcastVariable(""centers"", iter.getPartialSolution())
			.input(dataPoints)
			.name(""Find Nearest Centers"")
			.build();

		//  computing the new cluster positions
		ReduceOperator recomputeClusterCenter = ReduceOperator.builder(new RecomputeClusterCenter(), IntValue.class, 0)
			.input(findNearestClusterCenters)
			.name(""Recompute Center Positions"")
			.build();
		
		iter.setNextPartialSolution(recomputeClusterCenter);
		
		// ---------------------- End K-Means Loop ---------------------
		
		
		// run one additional points-to-centers assignment, to have the points tagged with the final centers they belong to
		// create DataSourceContract for data point input
		
		FileDataSource finalPointsSource = new FileDataSource(new CsvInputFormat('|', IntValue.class, DoubleValue.class, DoubleValue.class, DoubleValue.class), dataPointInput, ""Data Points2"");
		
		MapOperator finalPoints = MapOperator.builder(new PointBuilder()).name(""Build data points"").input(finalPointsSource).build();
		
		MapOperator findNearestFinalCluster = MapOperator.builder(new SelectNearestCenter())
			.setBroadcastVariable(""centers"", iter)
			.input(finalPoints)
			.name(""Tag points with final centroid"")
			.build();

		// write the new cluster positions
		FileDataSink newClusterPoints = new FileDataSink(new PointOutFormat(), centersOutput, iter, ""Cluster Positions"");

		// write assigned clusters
		FileDataSink clusterAssignments = new FileDataSink(new PointOutFormat(), taggedPointsOutput, findNearestFinalCluster, ""Cluster Assignments"");

		// return the plan
		Plan plan = new Plan(newClusterPoints, ""KMeans Iteration"");
		plan.addDataSink(clusterAssignments);
		plan.setDefaultParallelism(numSubTasks);
		return plan;
	}

	@Override
	public String getDescription() {
		return ""Parameters: <numSubStasks> <dataPoints> <clusterCenters> <output> <numIterations>"";
	}
	
	// --------------------------------------------------------------------------------------------
	//  Data Types and UDFs
	// --------------------------------------------------------------------------------------------
	
	/**
	 * A simple two-dimensional point.
	 */
	public static final class Point implements Value {
		private static final long serialVersionUID = 1L;
		
		public double x, y;
		
		public Point() {}

		public Point(double x, double y) {
			this.x = x;
			this.y = y;
		}
		
		public void add(Point other) {
			x += other.x;
			y += other.y;
		}
		
		public Point div(long val) {
			x /= val;
			y /= val;
			return this;
		}
		
		public double euclideanDistance(Point other) {
			return Math.sqrt((x-other.x)*(x-other.x) + (y-other.y)*(y-other.y));
		}
		
		public void clear() {
			x = y = 0.0;
		}

		@Override
		public void write(DataOutput out) throws IOException {
			out.writeDouble(x);
			out.writeDouble(y);
		}

		@Override
		public void read(DataInput in) throws IOException {
			x = in.readDouble();
			y = in.readDouble();
		}
		
		@Override
		public String toString() {
			return ""("" + x + ""|"" + y + "")"";
		}
	}
	
	public static final class PointWithId {
		
		public int id;
		public Point point;
		
		public PointWithId(int id, Point p) {
			this.id = id;
			this.point = p;
		}
	}
	
	/**
	 * Determines the closest cluster center for a data point.
	 */
	public static final class SelectNearestCenter extends MapFunction {
		private static final long serialVersionUID = 1L;

		private final IntValue one = new IntValue(1);
		private final Record result = new Record(3);

		private List<PointWithId> centers = new ArrayList<PointWithId>();

		/**
		 * Reads all the center values from the broadcast variable into a collection.
		 */
		@Override
		public void open(Configuration parameters) throws Exception {
			Collection<Record> clusterCenters = this.getRuntimeContext().getBroadcastVariable(""centers"");
			
			centers.clear();
			for (Record r : clusterCenters) {
				centers.add(new PointWithId(r.getField(0, IntValue.class).getValue(), r.getField(1, Point.class)));
			}
		}

		/**
		 * Computes a minimum aggregation on the distance of a data point to cluster centers.
		 * 
		 * Output Format:
		 * 0: centerID
		 * 1: pointVector
		 * 2: constant(1) (to enable combinable average computation in the following reducer)
		 */
		@Override
		public void map(Record dataPointRecord, Collector<Record> out) {
			Point p = dataPointRecord.getField(1, Point.class);
			
			double nearestDistance = Double.MAX_VALUE;
			int centerId = -1;

			// check all cluster centers
			for (PointWithId center : centers) {
				// compute distance
				double distance = p.euclideanDistance(center.point);
				
				// update nearest cluster if necessary 
				if (distance < nearestDistance) {
					nearestDistance = distance;
					centerId = center.id;
				}
			}

			// emit a new record with the center id and the data point. add a one to ease the
			// implementation of the average function with a combiner
			result.setField(0, new IntValue(centerId));
			result.setField(1, p);
			result.setField(2, one);

			out.collect(result);
		}
	}
	
	@Combinable
	public static final class RecomputeClusterCenter extends ReduceFunction {
		private static final long serialVersionUID = 1L;
		
		private final Point p = new Point();
		
		
		/**
		 * Compute the new position (coordinate vector) of a cluster center.
		 */
		@Override
		public void reduce(Iterator<Record> points, Collector<Record> out) {
			Record sum = sumPointsAndCount(points);
			sum.setField(1, sum.getField(1, Point.class).div(sum.getField(2, IntValue.class).getValue()));
			out.collect(sum);
		}

		/**
		 * Computes a pre-aggregated average value of a coordinate vector.
		 */
		@Override
		public void combine(Iterator<Record> points, Collector<Record> out) {
			out.collect(sumPointsAndCount(points));
		}
		
		private final Record sumPointsAndCount(Iterator<Record> dataPoints) {
			Record next = null;
			p.clear();
			int count = 0;
			
			// compute coordinate vector sum and count
			while (dataPoints.hasNext()) {
				next = dataPoints.next();
				p.add(next.getField(1, Point.class));
				count += next.getField(2, IntValue.class).getValue();
			}
			
			next.setField(1, p);
			next.setField(2, new IntValue(count));
			return next;
		}
	}
	
	public static final class PointBuilder extends MapFunction {

		private static final long serialVersionUID = 1L;

		@Override
		public void map(Record record, Collector<Record> out) throws Exception {
			double x = record.getField(1, DoubleValue.class).getValue();
			double y = record.getField(2, DoubleValue.class).getValue();
			
			record.setField(1, new Point(x, y));
			out.collect(record);
		}
	}
	
	public static final class PointOutFormat extends FileOutputFormat {

		private static final long serialVersionUID = 1L;
		
		private static final String format = ""%d|%.2f|%.2f|\n"";

		@Override
		public void writeRecord(Record record) throws IOException {
			int id = record.getField(0, IntValue.class).getValue();
			Point p = record.getField(1, Point.class);
			
			byte[] bytes = String.format(format, id, p.x, p.y).getBytes();
			
			this.stream.write(bytes);
		}
	}
	
	public static void main(String[] args) throws Exception {
		LocalExecutor.execute(new KMeansTutorialExample(), ""4"", ""/dev/random"", ""/dev/random"", ""/tmp"", ""10"");
	}
}
```;;;","09/Jun/14 12:22;github-import;[Date: Thu Mar 06 12:16:42 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed and merged in [32563150385a33e5907d711d29ec079c797e61dc|https://github.com/stratosphere/stratosphere/commit/32563150385a33e5907d711d29ec079c797e61dc];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Initial BCVars documentation,FLINK-489,12719660,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:22,09/Jun/14 12:22,14/Jul/23 05:57,09/Jun/14 12:22,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/489
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Thu Feb 20 15:17:17 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:22;github-import;pull-request-489-7096357386714968430.patch;https://issues.apache.org/jira/secure/attachment/12649175/pull-request-489-7096357386714968430.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397859,,,Mon Jun 09 12:22:40 UTC 2014,,,,,,,,,,"0|i1whwn:",397986,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:22;github-import;[Date: Tue Mar 04 01:25:58 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

We can update that one and remove the part that branching is not supported;;;","09/Jun/14 12:22;github-import;[Date: Tue Mar 04 01:30:38 CET 2014, Author: [twalthr|https://github.com/twalthr]]

I will do it tomorrow.;;;","09/Jun/14 12:22;github-import;[Date: Tue Mar 04 13:09:11 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Please reopen the pull request against the new website repository (https://github.com/stratosphere/stratosphere.github.io) See also https://github.com/stratosphere/stratosphere/issues/487;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update YARN documentation ,FLINK-488,12719659,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:22,09/Jun/14 12:22,14/Jul/23 05:57,09/Jun/14 12:22,,,,pre-apache,,,,,,,0,github-import,,"The [Yarn documentation|http://stratosphere.eu/docs/0.4/setup/yarn.html] should be updated to the new Yarn package added by ([#485|https://github.com/stratosphere/stratosphere/issues/485] | [FLINK-485|https://issues.apache.org/jira/browse/FLINK-485]), ([#473|https://github.com/stratosphere/stratosphere/issues/473] | [FLINK-473|https://issues.apache.org/jira/browse/FLINK-473]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/488
Created by: [fhueske|https://github.com/fhueske]
Labels: documentation, website, YARN, 
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Wed Feb 19 14:29:21 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397858,,,Mon Jun 09 12:22:34 UTC 2014,,,,,,,,,,"0|i1whwf:",397985,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:22;github-import;[Date: Wed Feb 19 14:30:05 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Yes, I will write a new documentation for the 0.5 release.;;;","09/Jun/14 12:22;github-import;[Date: Mon Mar 31 18:16:57 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

https://github.com/stratosphere/stratosphere.github.io/pull/20;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move gh-pages branch to seperate repository,FLINK-487,12719658,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:22,09/Jun/14 12:22,14/Jul/23 05:57,09/Jun/14 12:22,,,,pre-apache,,,,,,,0,github-import,,"The gh-pages branch blows up our repo and initial checkout takes quite long. Also both things are seperate.

We might want to move gh-pages to a seperate repo.

Note: if we do this, we have to fix the ""Edit this page"" link of the documentation.



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/487
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Wed Feb 19 12:06:44 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397857,,,Mon Jun 09 12:22:30 UTC 2014,,,,,,,,,,"0|i1whw7:",397984,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:22;github-import;[Date: Wed Feb 19 12:12:53 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,
we should certainly do this! I will create a repository in the stratosphere organization.
I would like to do the migration on the next Sunday, thats the day where we have the least traffic.
I'm not sure how the migration will work. We can not just push the gh-pages into the master of the web-repo. GitHub will complain because the domain is already registered with another repository. 
Can you ask GitHub how to do the migration?;;;","09/Jun/14 12:22;github-import;[Date: Tue Mar 04 12:59:14 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay, I'm doing it now.

I pushed the branch to the new repository. Now I'll check if it is working from the new one.;;;","09/Jun/14 12:22;github-import;[Date: Tue Mar 04 13:07:53 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay, the move was successful. I'll delete the `gh-pages` branch here.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
extended the YARN documentation on the website,FLINK-486,12719657,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:22,09/Jun/14 12:22,14/Jul/23 05:57,09/Jun/14 12:22,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/486
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue Feb 18 18:13:22 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:22;github-import;pull-request-486-4345615453705970152.patch;https://issues.apache.org/jira/secure/attachment/12649174/pull-request-486-4345615453705970152.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397856,,,Mon Jun 09 12:22:25 UTC 2014,,,,,,,,,,"0|i1whvz:",397983,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:22;github-import;[Date: Wed Feb 19 14:25:34 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Looks good to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn improvements,FLINK-485,12719656,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:22,09/Jun/14 12:22,14/Jul/23 05:57,09/Jun/14 12:22,,,,pre-apache,,,,,,,0,github-import,,"Implementation for issue https://github.com/stratosphere/stratosphere/issues/473

The new tar.gz file has the following structure:
```
./examples
./examples/stratosphere-java-examples-0.5-SNAPSHOT-EnumTrianglesWithDegrees.jar
./examples/stratosphere-java-examples-0.5-SNAPSHOT-WebLogAnalysis.jar
./examples/stratosphere-java-examples-0.5-SNAPSHOT-EnumTrianglesOnEdgesWithDegrees.jar
./examples/stratosphere-scala-examples-0.5-SNAPSHOT-ConnectedComponents.jar
./examples/stratosphere-scala-examples-0.5-SNAPSHOT-TPCHQuery3.jar
./examples/stratosphere-scala-examples-0.5-SNAPSHOT-WordCount.jar
./examples/stratosphere-scala-examples-0.5-SNAPSHOT-KMeansForTest.jar
./examples/stratosphere-java-examples-0.5-SNAPSHOT-TPCHQuery3.jar
./examples/stratosphere-scala-examples-0.5-SNAPSHOT-KMeans.jar
./examples/stratosphere-java-examples-0.5-SNAPSHOT-DanglingPageRank.jar
./examples/stratosphere-java-examples-0.5-SNAPSHOT-ConnectedComponents.jar
./examples/stratosphere-java-examples-0.5-SNAPSHOT-KMeansIterative.jar
./examples/stratosphere-java-examples-0.5-SNAPSHOT-TeraSort.jar
./examples/stratosphere-scala-examples-0.5-SNAPSHOT-ComputeEdgeDegrees.jar
./examples/stratosphere-java-examples-0.5-SNAPSHOT-PairwiseSP.jar
./examples/stratosphere-java-examples-0.5-SNAPSHOT-EnumTrianglesRdfFoaf.jar
./examples/stratosphere-scala-examples-0.5-SNAPSHOT-EnumTrianglesOnEdgesWithDegrees.jar
./examples/stratosphere-java-examples-0.5-SNAPSHOT-WordCount.jar
./examples/stratosphere-java-examples-0.5-SNAPSHOT-KMeans.jar
./.version.properties
./tools
./tools/planVisualizer.html
./conf
./conf/log4j.properties
./conf/stratosphere-conf.yaml
./LICENSE.txt
./NOTICE.txt
./README.txt
./bin
./bin/config.sh
./bin/yarn-session.sh
./bin/stratosphere
./log
./lib
./lib/stratosphere-dist-0.5-SNAPSHOT-yarn-uberjar.jar
```

I will test this in the AWS Cloud and update the blog post.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/485
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue Feb 18 09:42:47 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:22;github-import;pull-request-485-1233653372364629217.patch;https://issues.apache.org/jira/secure/attachment/12649173/pull-request-485-1233653372364629217.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397855,,,Mon Jun 09 12:22:16 UTC 2014,,,,,,,,,,"0|i1whvr:",397982,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:22;github-import;[Date: Tue Feb 18 11:22:19 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Tested on amazon's servers.
I'll test the travis->s3 deployment. If it passes, I'll merge it to finally publish the blog post.;;;","09/Jun/14 12:22;github-import;[Date: Tue Feb 18 11:34:42 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Structure looks good to me. Nice!;;;","09/Jun/14 12:22;github-import;[Date: Tue Feb 18 17:00:46 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good. One minor comment, otherwise +1;;;","09/Jun/14 12:22;github-import;[Date: Tue Feb 18 17:03:54 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thanks.
I merged it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove ExecutionStateTransition-Exceptions for failed tasks,FLINK-484,12719655,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:22,09/Jun/14 12:22,14/Jul/23 05:57,09/Jun/14 12:22,,,,pre-apache,,,,,,,0,github-import,,"The JobManager logfile often contains exceptions like the one attached for failing tasks.

The exception is confusing users since they assume a bug in the system. It is actually not a real bug in the system, its the inability of the ExecutionGraph to properly handle asynchronous tasks.

```
21:59:03,350 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM
: ExecutionState set from CANCELING to FAILED for task Map(ItemVectorMapper) (6/9)
21:59:03,350 ERROR eu.stratosphere.nephele.execution.ExecutionStateTransition    - java.lang.IllegalStateException: Unexpected state change: CANCELING -> FAILED
        at eu.stratosphere.nephele.execution.ExecutionStateTransition.checkTransition(ExecutionStateTransition.java:164)
        at eu.stratosphere.nephele.executiongraph.ExecutionVertex.updateExecutionState(ExecutionVertex.java:381)
        at eu.stratosphere.nephele.executiongraph.ExecutionVertex$1.run(ExecutionVertex.java:316)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1170)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:640)
        at java.lang.Thread.run(Thread.java:853)
```

Has anybody a suggestion how to solve it?
Just remove the exception for this particular state transition?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/484
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, enhancement, user satisfaction, 
Created at: Mon Feb 17 17:59:15 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397854,,,Mon Jun 09 12:22:09 UTC 2014,,,,,,,,,,"0|i1whvj:",397981,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:22;github-import;[Date: Mon Feb 17 20:47:49 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think there is no such thing as an illegal state transition. Certainly
not when the target state is 'failed'. The patch should make sure no
transition to 'failed' is ever treated as unexpected. Let's conservatively
keep the other checks in place for now.
Am 17.02.2014 17:59 schrieb ""Robert Metzger"" <notifications@github.com>:

> The JobManager logfile often contains exceptions like the one attached for
> failing tasks.
>
> The exception is confusing users since they assume a bug in the system. It
> is actually not a real bug in the system, its the inability of the
> ExecutionGraph to properly handle asynchronous tasks.
>
> 21:59:03,350 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM
> : ExecutionState set from CANCELING to FAILED for task Map(ItemVectorMapper) (6/9)
> 21:59:03,350 ERROR eu.stratosphere.nephele.execution.ExecutionStateTransition    - java.lang.IllegalStateException: Unexpected state change: CANCELING -> FAILED
>         at eu.stratosphere.nephele.execution.ExecutionStateTransition.checkTransition(ExecutionStateTransition.java:164)
>         at eu.stratosphere.nephele.executiongraph.ExecutionVertex.updateExecutionState(ExecutionVertex.java:381)
>         at eu.stratosphere.nephele.executiongraph.ExecutionVertex$1.run(ExecutionVertex.java:316)
>         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1170)
>         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:640)
>         at java.lang.Thread.run(Thread.java:853)
>
> Has anybody a suggestion how to solve it?
> Just remove the exception for this particular state transition?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/484>
> .
>;;;","09/Jun/14 12:22;github-import;[Date: Thu Mar 06 12:17:36 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [cd07ed6fd9128655e8ad5b13bc1fe496f538e3ed|https://github.com/stratosphere/stratosphere/commit/cd07ed6fd9128655e8ad5b13bc1fe496f538e3ed];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Exceptions on TaskManagers (usercode, input formats etc) are not displayed in the JobManager log",FLINK-482,12719653,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:21,09/Jun/14 12:22,14/Jul/23 05:57,09/Jun/14 12:22,,,,pre-apache,,,,,,,0,github-import,,"The JobManager log file should contain the exceptions of a failing task. 

This is currently only possible by starting a job using the `-w` argument of the stratosphere client.
The exception should always be in the JobManager log file!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/482
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, runtime, 
Milestone: Release 0.5
Created at: Mon Feb 17 17:50:41 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397852,,,Mon Jun 09 12:22:00 UTC 2014,,,,,,,,,,"0|i1whv3:",397979,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:21;github-import;[Date: Fri Mar 07 06:34:56 CET 2014, Author: [danrk|https://github.com/danrk]]

hello, 

I would like to work on this and provide a patch.;;;","09/Jun/14 12:21;github-import;[Date: Fri Mar 07 09:26:06 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Cool.
I would suggest to take the wordcount example and throw a Runtime Exception in the mapper. Then, run the Job on a cluster with the `-w` option. You'll see your exception.
As a next step, start a *distributed* Stratosphere cluster (e.g. not  `start-local.sh`). Running the job again (without `-w`) will also fail the job, but this time, you won't see your Exception in the jobmanager log file, only in one of the taskmanager's log files (or in all TMs).

It would be great if the exception is also shown in the JobManager log file. As a awesome bonus, we would also like to show the Exception in the JobManager's webinterface (on port 8081). Once you've managed to fix the first issue, the webinterface thing will be easy.;;;","09/Jun/14 12:21;github-import;[Date: Fri Mar 07 09:37:19 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The exceptions are already reported from task manager to the job Manager,
which forwards them to the client (and in -w Mode, the client prints the
exception).

If you can find the point where the job manager stores the exception for
the jobclient to pull the status (and the exception), you probably have a
good point to start. Logging it is most likely a quick fix then, forwarding
the exception to the web Frontend would be a very nice addition!;;;","09/Jun/14 12:22;github-import;[Date: Tue May 20 19:56:59 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

This was fixed as part of ([#688|https://github.com/stratosphere/stratosphere/issues/688] | [FLINK-688|https://issues.apache.org/jira/browse/FLINK-688]) (https://github.com/stratosphere/stratosphere/pull/688/files#diff-0808a97b4666fab09f9746c60e4a9b36R669);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalDistributedExecutor startup fails with single TaskManager,FLINK-480,12719651,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:21,18/Jun/14 23:22,14/Jul/23 05:57,18/Jun/14 23:22,,,,pre-apache,,,,,,,0,github-import,,"When I start the `LocalDistributedExecutor` with just a single TaskManager, the startup fails, e.g. changing line 46 from `lde.start(2);` to `lde.start(1);` in [LocalDistributedExecutorTest|https://github.com/stratosphere/stratosphere/blob/master/stratosphere-tests/src/test/java/eu/stratosphere/test/localDistributed/LocalDistributedExecutorTest.java?source=cc] does not work.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/480
Created by: [uce|https://github.com/uce]
Labels: bug, runtime, 
Assignee: [uce|https://github.com/uce]
Created at: Mon Feb 17 15:08:27 CET 2014
State: open
",,github-import,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397850,,,Wed Jun 18 23:22:43 UTC 2014,,,,,,,,,,"0|i1whun:",397977,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:21;github-import;[Date: Tue Feb 18 17:54:57 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Recently, the LDE test failed on travis: https://travis-ci.org/stratosphere/stratosphere/jobs/19119983

```
Running eu.stratosphere.test.localDistributed.LocalDistributedExecutorTest
16:29:32,678 ERROR eu.stratosphere.nephele.jobmanager.JobManager                 - Cannot start RPC server: java.net.BindException: Problem binding to localhost/127.0.0.1:6498 : Address already in use
	at eu.stratosphere.nephele.ipc.Server.bind(Server.java:198)
	at eu.stratosphere.nephele.ipc.Server$Listener.<init>(Server.java:272)
	at eu.stratosphere.nephele.ipc.Server.<init>(Server.java:994)
	at eu.stratosphere.nephele.ipc.RPC$Server.<init>(RPC.java:402)
	at eu.stratosphere.nephele.ipc.RPC.getServer(RPC.java:355)
	at eu.stratosphere.nephele.jobmanager.JobManager.<init>(JobManager.java:236)
	at eu.stratosphere.client.localDistributed.LocalDistributedExecutor.start(LocalDistributedExecutor.java:93)
	at eu.stratosphere.test.localDistributed.LocalDistributedExecutorTest.testLocalDistributedExecutorWithWordCount(LocalDistributedExecutorTest.java:46)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:104)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:147)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:98)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.ProviderFactory$ClassLoaderProxy.invoke(ProviderFactory.java:101)
	at com.sun.proxy.$Proxy0.invoke(Unknown Source)
	at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:139)
	at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcess(SurefireStarter.java:82)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:81)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at eu.stratosphere.nephele.ipc.Server.bind(Server.java:196)
	... 35 more
```;;;","09/Jun/14 12:21;github-import;[Date: Tue Feb 18 18:18:58 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I have seen this also, but could not reproduce it. seems like multiple
jobmanager instances get started.

Do you run more than one test with the DistributedExecutor in the same JVM?
If so, you may see that the previous one has not fully shut down before the
next one starts. Particularly, the RPC threads linger...


On Tue, Feb 18, 2014 at 5:54 PM, Robert Metzger <notifications@github.com>wrote:

> Recently, the LDE test failed on travis:
> https://travis-ci.org/stratosphere/stratosphere/jobs/19119983
>
> Running eu.stratosphere.test.localDistributed.LocalDistributedExecutorTest
> 16:29:32,678 ERROR eu.stratosphere.nephele.jobmanager.JobManager                 - Cannot start RPC server: java.net.BindException: Problem binding to localhost/127.0.0.1:6498 : Address already in use
>     at eu.stratosphere.nephele.ipc.Server.bind(Server.java:198)
>     at eu.stratosphere.nephele.ipc.Server$Listener.<init>(Server.java:272)
>     at eu.stratosphere.nephele.ipc.Server.<init>(Server.java:994)
>     at eu.stratosphere.nephele.ipc.RPC$Server.<init>(RPC.java:402)
>     at eu.stratosphere.nephele.ipc.RPC.getServer(RPC.java:355)
>     at eu.stratosphere.nephele.jobmanager.JobManager.<init>(JobManager.java:236)
>     at eu.stratosphere.client.localDistributed.LocalDistributedExecutor.start(LocalDistributedExecutor.java:93)
>     at eu.stratosphere.test.localDistributed.LocalDistributedExecutorTest.testLocalDistributedExecutorWithWordCount(LocalDistributedExecutorTest.java:46)
>     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>     at java.lang.reflect.Method.invoke(Method.java:606)
>     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
>     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
>     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
>     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
>     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
>     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
>     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
>     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
>     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
>     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
>     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
>     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
>     at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:104)
>     at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:147)
>     at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:98)
>     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>     at java.lang.reflect.Method.invoke(Method.java:606)
>     at org.apache.maven.surefire.booter.ProviderFactory$ClassLoaderProxy.invoke(ProviderFactory.java:101)
>     at com.sun.proxy.$Proxy0.invoke(Unknown Source)
>     at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:139)
>     at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcess(SurefireStarter.java:82)
>     at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:81)
> Caused by: java.net.BindException: Address already in use
>     at sun.nio.ch.Net.bind0(Native Method)
>     at sun.nio.ch.Net.bind(Net.java:444)
>     at sun.nio.ch.Net.bind(Net.java:436)
>     at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
>     at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
>     at eu.stratosphere.nephele.ipc.Server.bind(Server.java:196)
>     ... 35 more
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/480#issuecomment-35405736>
> .
>;;;","18/Jun/14 23:22;uce;The issue description seems incomplete, but is fixed in [a70de7e3faef8afea0134c8fed1240d742cccb3d|https://github.com/apache/incubator-flink/commit/a70de7e3faef8afea0134c8fed1240d742cccb3d].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplified and fixed estimate computation.,FLINK-479,12719650,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:21,09/Jun/14 12:21,14/Jul/23 05:57,09/Jun/14 12:21,,,,pre-apache,,,,,,,0,github-import,,"Thsi patch greatly simplifies the estimate computation. To that end, it also simplifies the compiler hints.

The prior version was more expressive, but was never used because it required the programmer to ahve detailed knowledge about the program (like per column group cardinalities).

In the long run, we need a clear plan how to best expose hints. Until then, I think this makes it much simpler to use and it fixed a buch of bugs on the way.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/479
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Sun Feb 16 22:10:30 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:21;github-import;pull-request-479-2918054112858423676.patch;https://issues.apache.org/jira/secure/attachment/12649172/pull-request-479-2918054112858423676.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397849,,,Mon Jun 09 12:21:49 UTC 2014,,,,,,,,,,"0|i1whuf:",397976,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:21;github-import;[Date: Tue Feb 18 17:06:28 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Any objections to merging this?

@fhueske You have worked on that part before as well, do you see any complications with this?;;;","09/Jun/14 12:21;github-import;[Date: Wed Feb 19 00:06:11 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I'll have a look at it.;;;","09/Jun/14 12:21;github-import;[Date: Wed Feb 19 16:10:04 CET 2014, Author: [fhueske|https://github.com/fhueske]]

The PR reduces and eases the set of compiler hints.
It also significantly cleans up the estimates computation in the compiler.

We lose some expressiveness through the removed compiler hints, but these were hard to use in any case.
I have some cosmetic remarks, but nothing curcial.

After that it's good to merge, IMHO.
;;;","09/Jun/14 12:21;github-import;[Date: Fri Feb 21 01:04:06 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Just made a PR on branch of this PR to address my comments.;;;","09/Jun/14 12:21;github-import;[Date: Thu Feb 27 16:59:04 CET 2014, Author: [fhueske|https://github.com/fhueske]]

@StephanEwen Why didn't you merge it into master?;;;","09/Jun/14 12:21;github-import;[Date: Thu Feb 27 17:00:35 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Tests are still running... will merge as soon as they turn green.


On Thu, Feb 27, 2014 at 4:59 PM, Fabian Hueske <notifications@github.com>wrote:

> @StephanEwen <https://github.com/StephanEwen> Why didn't you merge it
> into master?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/479#issuecomment-36256908>
> .
>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adjusted file URIs in some tests for Windows compatibility,FLINK-477,12719648,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:21,09/Jun/14 12:21,14/Jul/23 05:57,09/Jun/14 12:21,,,,pre-apache,,,,,,,0,github-import,,"In some of the newer tests filesystem URIs were again hard coded. I did a quick adjustment for Windows to get the test running in Windows environments again.
All tests that are running external shell commands were excluded when running under windows for now.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/477
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Fri Feb 14 16:05:53 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:21;github-import;pull-request-477-1927726698578539115.patch;https://issues.apache.org/jira/secure/attachment/12649171/pull-request-477-1927726698578539115.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397847,,,Mon Jun 09 12:21:36 UTC 2014,,,,,,,,,,"0|i1whtz:",397974,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:21;github-import;[Date: Mon Feb 17 00:29:38 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

In other places, we used file.toURI().toString(), rather than hardcoding the ""file:/"" or ""file:///"" prefix. Can we do the same thing here?;;;","09/Jun/14 12:21;github-import;[Date: Mon Feb 24 10:58:31 CET 2014, Author: [markus-h|https://github.com/markus-h]]

You are right, that is the nicer solution. I adjusted my changes.;;;","09/Jun/14 12:21;github-import;[Date: Thu Feb 27 14:35:03 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I opened ([#505|https://github.com/stratosphere/stratosphere/issues/505] | [FLINK-505|https://issues.apache.org/jira/browse/FLINK-505]) to address the text input format on windows issue.;;;","09/Jun/14 12:21;github-import;[Date: Thu Feb 27 17:54:35 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed and merged in [d0ef7c48bba8c55a6d932ba7e2a19d2278b65a75|https://github.com/stratosphere/stratosphere/commit/d0ef7c48bba8c55a6d932ba7e2a19d2278b65a75];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add RuntimeContext Argument to open Method of AbstractFunction,FLINK-476,12719647,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:21,09/Jun/14 12:21,14/Jul/23 05:57,09/Jun/14 12:21,,,,pre-apache,,,,,,,0,github-import,,"I propose to add `RuntimeContext` as a second argument to the open method of `AbstractFunction`, e.g. `open(Configuration, RuntimeContext)` instead of `open(Configuration)`.

It came to me as I looked at the new Boratcast variables (([#471|https://github.com/stratosphere/stratosphere/issues/471] | [FLINK-471|https://issues.apache.org/jira/browse/FLINK-471])) [KMeans example|https://github.com/stratosphere/stratosphere/blob/master/stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/record/kmeans/KMeans.java].

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/476
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Fri Feb 14 13:06:17 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397846,,,2014-06-09 12:21:29.0,,,,,,,,,,"0|i1whtr:",397973,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adjust Webclient Plan display to handle Programs with Broadcast Variables,FLINK-475,12719646,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JonathanH5,github-import,github-import,09/Jun/14 12:21,18/Jun/14 12:20,14/Jul/23 05:57,18/Jun/14 12:20,,,,0.6-incubating,pre-apache,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/475
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Fri Feb 14 00:21:31 CET 2014
State: open
",,github-import,rmetzger,,,,,,,,,,,,,,,,FLINK-711,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397845,,,Wed Jun 18 12:20:13 UTC 2014,,,,,,,,,,"0|i1whtj:",397972,,,,,,,,,,,,,,,,,,,,"18/Jun/14 12:20;rmetzger;The issue has been resolved by https://github.com/apache/incubator-flink/pull/15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Branching Data Flow Logic For BC Variables,FLINK-474,12719645,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:21,09/Jun/14 12:21,14/Jul/23 05:57,09/Jun/14 12:21,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/474
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Fri Feb 14 00:20:56 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397844,,,Mon Jun 09 12:21:24 UTC 2014,,,,,,,,,,"0|i1whtb:",397971,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:21;github-import;[Date: Fri Mar 21 21:17:22 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Fixed in https://github.com/stratosphere/stratosphere/pull/526.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Termination Criterion for Bulk Iterations,FLINK-472,12719643,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:21,09/Jun/14 12:21,14/Jul/23 05:57,09/Jun/14 12:21,,,,pre-apache,,,,,,,0,github-import,,"I implemented the setTerminationCriterion method for BulkIterations. 

I did my best to rebase my changes to the current version. Unfortunately the current test base is again not Windows compatible, so I was not able to run the tests.

Since the changes are quite complex I would highly appreciate if some of you would test the new termination criterion with some of their currently working iterations. The adjustment to the new criterion should be not too hard.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/472
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Thu Feb 13 15:14:05 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:21;github-import;pull-request-472-614937931867929289.patch;https://issues.apache.org/jira/secure/attachment/12649170/pull-request-472-614937931867929289.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397842,,,Mon Jun 09 12:21:16 UTC 2014,,,,,,,,,,"0|i1whsv:",397969,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:21;github-import;[Date: Fri Feb 14 14:12:56 CET 2014, Author: [markus-h|https://github.com/markus-h]]

I had to adjust the ConnectedComponentsNepheleITCase testcase because in my opinion flags for the IterationTailPactTask were not set correctly,;;;","09/Jun/14 12:21;github-import;[Date: Fri Feb 14 14:21:11 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

You refer to the flags that mark a tail as updating the workset or the
solution set?


On Fri, Feb 14, 2014 at 2:12 PM, markus-h <notifications@github.com> wrote:

> I had to adjust the ConnectedComponentsNepheleITCase testcase because in
> my opinion flags for the IterationTailPactTask were not set correctly,
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/472#issuecomment-35081905>
> .
>;;;","09/Jun/14 12:21;github-import;[Date: Fri Feb 14 14:35:03 CET 2014, Author: [markus-h|https://github.com/markus-h]]

Yes. For the solution set only isSolutionSetUpdate was set but not isWorksetIteration. For the current implementation of the termination criterion to work it hast to be speciefied as workset iteration. Before my changes it didn't matter.;;;","09/Jun/14 12:21;github-import;[Date: Fri Feb 14 16:25:25 CET 2014, Author: [markus-h|https://github.com/markus-h]]

I do not understand why travis fails to compile. I compiled and run all testcases on a cluster and it worked.;;;","09/Jun/14 12:21;github-import;[Date: Wed Mar 05 11:14:04 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [2f1050f7e0505dc9bd23e417be9d6d208065eb2a|https://github.com/stratosphere/stratosphere/commit/2f1050f7e0505dc9bd23e417be9d6d208065eb2a]

Thanks for the feature!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broadcast Variables,FLINK-471,12719642,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:20,09/Jun/14 12:21,14/Jul/23 05:57,09/Jun/14 12:21,,,,pre-apache,,,,,,,0,github-import,,"The pull request merges @aalexandrov 's pull request ([#460|https://github.com/stratosphere/stratosphere/issues/460] | [FLINK-460|https://issues.apache.org/jira/browse/FLINK-460]) with the current master and adds support for iterations. Also includes a variety of cleanups.

Missing is the support for branching plans. I would suggest to merge this part now, such that newer patches can build on it.

Since the K-Means example is affected by this change, we need to add an additional variant that assigns the  center id to teh data point, to support the clustering tutorial.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/471
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Thu Feb 13 02:48:12 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:21;github-import;pull-request-471-4101197601115031815.patch;https://issues.apache.org/jira/secure/attachment/12649169/pull-request-471-4101197601115031815.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397841,,,Mon Jun 09 12:21:08 UTC 2014,,,,,,,,,,"0|i1whsn:",397968,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:21;github-import;[Date: Thu Feb 13 02:55:03 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

There is something goofy about this pull request. It includes commits [d1e7d06c1c9f30e9e0220d2ce6cafb1fac68a5ea|https://github.com/stratosphere/stratosphere/commit/d1e7d06c1c9f30e9e0220d2ce6cafb1fac68a5ea] and [2ee0255c8d9e070aa1f77f9ad485573aec695d0e|https://github.com/stratosphere/stratosphere/commit/2ee0255c8d9e070aa1f77f9ad485573aec695d0e], which are subsumed in the master already. However, when I merge the master into this PR, it says that everything is up to date. Rebasing this PR on the master fails.

Do not merge until this git problem is resolved.;;;","09/Jun/14 12:21;github-import;[Date: Thu Feb 13 08:29:31 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Sadly, it is not possible to apply the pr as a single patch to master.
```
$ wget https://github.com/stratosphere/stratosphere/pull/471.patch
$ git apply --check 471.patch
error: patch failed: stratosphere-compiler/src/main/java/eu/stratosphere/compiler/PactCompiler.java:952
error: stratosphere-compiler/src/main/java/eu/stratosphere/compiler/PactCompiler.java: patch does not apply
error: patch failed: stratosphere-addons/avro/src/test/java/eu/stratosphere/api/avro/EncoderDecoderTest.java:19
error: stratosphere-addons/avro/src/test/java/eu/stratosphere/api/avro/EncoderDecoderTest.java: patch does not apply
error: patch failed: stratosphere-addons/avro/pom.xml:41
error: stratosphere-addons/avro/pom.xml: patch does not apply
error: patch failed: stratosphere-tests/src/main/java/eu/stratosphere/test/util/TestBase2.java:66
error: stratosphere-tests/src/main/java/eu/stratosphere/test/util/TestBase2.java: patch does not apply
```

If you want to try the merge, use `git am --signoff 471.patch`;;;","09/Jun/14 12:21;github-import;[Date: Thu Feb 13 14:29:35 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

> [...] we need to add an additional variant that assigns the center id to the data point [...].

What do you mean with that? How does this differ from the test jobs added in a6f0603 and ba11e19?;;;","09/Jun/14 12:21;github-import;[Date: Thu Feb 13 16:34:25 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

We need that not for testing, but to support this quickstart example here:

http://stratosphere.eu/quickstart/example.html;;;","09/Jun/14 12:21;github-import;[Date: Thu Feb 13 17:18:16 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

Can't we just modify the adapted KMeansTest instead of adding a variant? It would still work as a test case, right?;;;","09/Jun/14 12:21;github-import;[Date: Thu Feb 13 17:35:58 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Yes, we could. I was a bit hesitant, because this would not show core
K-Means any more, rather a modified variant tailored to that demo case.

I vote for keeping the core examples simple and to think first in terms of
presentation of examples (as these are somewhat acting like documentation
as well) and then in terms of tests (these are internal and somewhat
invisible).

So from the view of the test, I guess it is fine. Another question is
whether we want two variants of KMeans in the examples.




On Thu, Feb 13, 2014 at 5:18 PM, Alexander Alexandrov <
notifications@github.com> wrote:

> Can't we just modify the adapted KMeansTest instead of adding a variant?
> It would still work as a test case, right?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/471#issuecomment-34994847>
> .
>;;;","09/Jun/14 12:21;github-import;[Date: Thu Feb 13 18:56:51 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

Alright, agreed. We should close this PR before we get another conflict.;;;","09/Jun/14 12:21;github-import;[Date: Thu Feb 13 19:37:41 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Commit history is fixed.

I will add the additional KMeans example (so we do not break the current demo) and then merge.;;;","09/Jun/14 12:21;github-import;[Date: Fri Feb 14 00:19:15 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [0c7fe86c398a5fff33f92d9970a7ebe8e517639c|https://github.com/stratosphere/stratosphere/commit/0c7fe86c398a5fff33f92d9970a7ebe8e517639c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Website: Blogpost: Use Stratosphere with Amazon Elastic MapReduce,FLINK-470,12719641,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:20,09/Jun/14 12:20,14/Jul/23 05:57,09/Jun/14 12:20,,,,pre-apache,,,,,,,0,github-import,,"Please review the blog post.

Preview: http://robertmetzger.de/stratosphere/tutorial/blog/2014/02/12/amazon-elastic-mapreduce-cloud-yarn.html

I will now go through the steps one more time to validate that everything is working. Then, I'll update everything once again, adding your feedback.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/470
Created by: [rmetzger|https://github.com/rmetzger]
Labels: website, 
Created at: Wed Feb 12 20:27:00 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:20;github-import;pull-request-470-1514234901283600826.patch;https://issues.apache.org/jira/secure/attachment/12649168/pull-request-470-1514234901283600826.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397840,,,Mon Jun 09 12:20:47 UTC 2014,,,,,,,,,,"0|i1whsf:",397967,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:20;github-import;[Date: Thu Feb 13 16:26:46 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Nice post!

A few comments:
- 3GB heap is not really necessary for the JM.
- I think, it is confusing to use the uber Jar a second time as the job Jar file. It would be better to use the dedicated Wordcount example jar file, even if it means to do another download, IMHO. 

On a side note:
Can't we create a ""proper"" YARN bundle, with the uber Jar, example job jars, (start) scripts, etc?
;;;","09/Jun/14 12:20;github-import;[Date: Tue Feb 18 11:34:08 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay. I addressed all your comments and updated the post accordingly.
Once the build for the yarn changes passed, I'll publish the post.;;;","09/Jun/14 12:20;github-import;[Date: Tue Feb 18 11:38:14 CET 2014, Author: [uce|https://github.com/uce]]

Nice. Thank you for the post.;;;","09/Jun/14 12:20;github-import;[Date: Tue Feb 18 17:08:57 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I published the post on the website;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalDistributedExecutor Deadlock with Low Buffer Count,FLINK-469,12719640,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:20,18/Jun/14 23:19,14/Jul/23 05:57,18/Jun/14 23:19,,,,,,,,,,,0,github-import,,"I'm currently working on ([#25|https://github.com/stratosphere/stratosphere/issues/25] | [FLINK-25|https://issues.apache.org/jira/browse/FLINK-25]) and discovered a possible deadlock in the network stack, because of the buffer management in combination with the `LocalDistributedExecutor` (LDE).

The LDE starts a JobManager and multiple TaskManagers on different network ports in a single VM. Every TaskManager has an associated `ByteBufferedChannelManager` (single instance) and `GlobalBufferPool` (singleton) for data transfers. When tasks get registered with a TaskManager (which is atomic per TaskManager), the ChannelManager ensures that there are enough network buffers available to execute the task -- this means that there has to be at least one buffer per task channel. If this condition does not hold, an exception is thrown and the task fails. This decision is made locally per task and not for the whole plan, e.g. for WordCount it is possible that all map tasks get enough buffers, but a following reduce throws an exception at runtime.

The problem occurs in combination with the LDE: we have multiple TMs with their ChannelManager instances, but only a singleton GlobalBufferPool. This results in a problem with the available buffer computation, because each TM justs considers its local channels (registered at the ChannelManager) and not the channels of others TMs (which is perfectly fine in a real distributed setup). Therefore, it is possible for tasks to deadlock, because of missing buffers (buffer requests are blocking).

You are likely to reproduce this problem by running `LocalDistributedExecutorTest` and setting the number of buffers to 20 and the buffer size to 4096 bytes (see `ConfigConstants`; make also sure to set `multicastEnabled` in ByteBufferedChannelManager to `false`, because it influences the computation -- multicast does not work anyways).

I will fix this with the upcoming PR for ([#25|https://github.com/stratosphere/stratosphere/issues/25] | [FLINK-25|https://issues.apache.org/jira/browse/FLINK-25]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/469
Created by: [uce|https://github.com/uce]
Labels: bug, runtime, 
Assignee: [uce|https://github.com/uce]
Created at: Wed Feb 12 13:58:36 CET 2014
State: open
",,github-import,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397839,,,Wed Jun 18 23:19:34 UTC 2014,,,,,,,,,,"0|i1whs7:",397966,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:20;github-import;[Date: Wed Feb 12 14:03:40 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It seems that the crux of the issue is that teh GlobalBufferPool is
modelled as a singleton. Is it possible to make it a member of the
TaskManager?


On Wed, Feb 12, 2014 at 1:58 PM, Ufuk Celebi <notifications@github.com>wrote:

> I'm currently working on ([#25|https://github.com/stratosphere/stratosphere/issues/25] | [FLINK-25|https://issues.apache.org/jira/browse/FLINK-25])<https://github.com/stratosphere/stratosphere/issues/25>and discovered a possible deadlock in the network stack, because of the
> buffer management in combination with the LocalDistributedExecutor (LDE).
>
> The LDE starts a JobManager and multiple TaskManagers on different network
> ports in a single VM. Every TaskManager has an associated
> ByteBufferedChannelManager (single instance) and GlobalBufferPool(singleton) for data transfers. When tasks get registered with a
> TaskManager (which is atomic per TaskManager), the ChannelManager ensures
> that there are enough network buffers available to execute the task -- this
> means that there has to be at least one buffer per task channel. If this
> condition does not hold, an exception is thrown and the task fails. This
> decision is made locally per task and not for the whole plan, e.g. for
> WordCount it is possible that all map tasks get enough buffers, but a
> following reduce throws an exception at runtime.
>
> The problem occurs in combination with the LDE: we have multiple TMs with
> their ChannelManager instances, but only a singleton GlobalBufferPool. This
> results in a problem with the available buffer computation, because each TM
> justs considers its local channels (registered at the ChannelManager) and
> not the channels of others TMs (which is perfectly fine in a real
> distributed setup). Therefore, it is possible for tasks to deadlock,
> because of missing buffers (buffer requests are blocking).
>
> You are likely to reproduce this problem by running
> LocalDistributedExecutorTest and setting the number of buffers to 20 and
> the buffer size to 4096 bytes (see ConfigConstants; make also sure to set
> multicastEnabled in ByteBufferedChannelManager to false, because it
> influences the computation -- multicast does not work anyways).
>
> I will fix this with the upcoming PR for ([#25|https://github.com/stratosphere/stratosphere/issues/25] | [FLINK-25|https://issues.apache.org/jira/browse/FLINK-25])<https://github.com/stratosphere/stratosphere/issues/25>
> .
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/469>
> .
>;;;","09/Jun/14 12:20;github-import;[Date: Wed Feb 12 14:13:34 CET 2014, Author: [uce|https://github.com/uce]]

Yes, exactly. I am looking into what would be the best way to pass the global buffer pool instance to the local pools with the suggested change.;;;","09/Jun/14 12:20;github-import;[Date: Thu Feb 27 17:57:55 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

@uce  Is this issue addressed in the pull request for the buffer oriented execution?;;;","18/Jun/14 23:19;uce;Fixed in [2db78a8dc1a4664f3e384005d7e07bea594b835b|https://github.com/apache/incubator-flink/commit/2db78a8dc1a4664f3e384005d7e07bea594b835b].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stratosphere Job to generate input data for wordcount,FLINK-468,12719639,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:20,09/Jun/14 12:20,14/Jul/23 05:57,09/Jun/14 12:20,,,,pre-apache,,,,,,,0,github-import,,"I hacked together a little utility Stratosphere job that generates input data for wordcount. I wanted to have this to demonstrate Stratosphere with a few TB of input data on an Amazon AWS cloud.

Its not sophisticated at all, uses the same seed for all data sources and the generated strings are equally random distributed, not zipf or so. (So its not very good for a Wordcount-benchmark).

I also changed the current word count to generate some (really small) input data. It generates data automatically if the user did not specify any arguments. BUT: Our Wordcount now runs out of the box!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/468
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue Feb 11 20:27:12 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:20;github-import;pull-request-468-2895243002053542719.patch;https://issues.apache.org/jira/secure/attachment/12649167/pull-request-468-2895243002053542719.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397838,,,Mon Jun 09 12:20:25 UTC 2014,,,,,,,,,,"0|i1whrz:",397965,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:20;github-import;[Date: Wed Feb 12 13:45:02 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

We have a [wordcount generator|https://github.com/TU-Berlin-DIMA/wordcount-gen] on top of the that [Myriad Toolkit|https://github.com/TU-Berlin-DIMA/myriad-toolkit] which follows the distribution of the top 100000 words in the English Gutenberg corpus.

Take a look at the actual [distribution specification here|https://github.com/TU-Berlin-DIMA/wordcount-gen/tree/master/src/config/distributions/lexicon] and at the [CombinedPrFunction|https://github.com/TU-Berlin-DIMA/myriad-toolkit/blob/master/src/cpp/math/probability/CombinedPrFunction.h] for an implementation of skewed sampling based on a uniform double sequence.

BTW all Myriad Generators now ship with a [map-only hadoop-job|https://github.com/TU-Berlin-DIMA/myriad-driver], so you can run them on the cluster without too much effort. I would welcome a PR with a similar implementation for Stratosphere.

PS. Data skew actually doesn't influence performance for wordcount.;;;","09/Jun/14 12:20;github-import;[Date: Wed Feb 12 13:53:23 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Cool! my suggested generator had a really poor performance on Amazon EC2. 
But their machines only have a disk write throughput of 20 MB/s.
I thought generating 3 TB of wordcount data would be a matter of an hour or so.

Does the Map only job take care of shipping the binary within the cluster? 
Do you think I'm able to get the myriad+MR tool to run within 10 minutes? If yes, I'll try it. Otherwise I'll stick with hamlet.;;;","09/Jun/14 12:20;github-import;[Date: Wed Feb 12 14:03:51 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

In my experience data generation is mostly bounded by the CPU. On my machine the Myriad version spills out ~ 11MB/s, so 20 seems pretty fast.

If you want to try it out on a Ubuntu image on EC2 I might be able to send you a binary which works out of the box. Otherwise you'll have to follow the instructions from the Readme and build it on site (takes ~ 10 mins time).

To start the Hadoop job you have to type the following command:

```bash
${HDP_MAPR_BIN}/hadoop jar \
    ${WC_DGEN_HOME}/bin/wordcount-gen-driver-jobs.jar \
    ${WC_DGEN_HOME} \
    -s${SCALING_FACTOR} \
    -N${NODE_COUNT} \
    -m${DATASET_ID} \
    -o${HDFS_PATH} -xtoken
```

This will start a Haoop job with DOP of `${NODE_COUNT}` and generate `${SCALING_FACTOR}` GB of data in the `${HDFS_PATH}/${DATASET_ID}` folder.;;;","09/Jun/14 12:20;github-import;[Date: Wed Feb 12 14:08:06 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

20 MB/s is the performance writing zeros to disk ;)

The build instructions are quite intimidating. It goes beyond a simple ""git clone and make"", right?
The EMR instances are Amazon AMI (which seems to be redhat-based). Not sure if it will work with your binary. Can you send it to me anyway?;;;","09/Jun/14 12:20;github-import;[Date: Wed Feb 12 14:10:55 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

They are intimidating cause I didn't now better at the time of writing the generic README.md which is shipped with all Myriad-based project.

Actually it boils down to the 4 commands [described here|https://github.com/TU-Berlin-DIMA/wordcount-gen#building-on-unix--linux-distributions].;;;","09/Jun/14 12:20;github-import;[Date: Tue Feb 18 17:09:28 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Lets discard this code for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add failed node list to web interface,FLINK-467,12719638,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:20,09/Jun/14 12:20,14/Jul/23 05:57,09/Jun/14 12:20,,,,pre-apache,,,,,,,0,github-import,,"implement ([#390|https://github.com/stratosphere/stratosphere/issues/390] | [FLINK-390|https://issues.apache.org/jira/browse/FLINK-390])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/467
Created by: [qmlmoon|https://github.com/qmlmoon]
Labels: 
Created at: Tue Feb 11 15:17:22 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:20;github-import;pull-request-467-4810825065308648213.patch;https://issues.apache.org/jira/secure/attachment/12649166/pull-request-467-4810825065308648213.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397837,,,Mon Jun 09 12:20:17 UTC 2014,,,,,,,,,,"0|i1whrr:",397964,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:20;github-import;[Date: Tue Feb 11 15:23:25 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I tested the code on a cluster with a failing job! It works!;;;","09/Jun/14 12:20;github-import;[Date: Tue Feb 11 15:58:05 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

@markus-h: Can you please review the PR ?;;;","09/Jun/14 12:20;github-import;[Date: Tue Feb 11 16:31:03 CET 2014, Author: [markus-h|https://github.com/markus-h]]

The code looks good for me, I just commented on two little coding style issues.;;;","09/Jun/14 12:20;github-import;[Date: Wed Feb 26 10:14:35 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [f050b708268b781fd86cadd96aeaf6469ebec7e2|https://github.com/stratosphere/stratosphere/commit/f050b708268b781fd86cadd96aeaf6469ebec7e2];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stratosphere scripts fail if the path contains spaces,FLINK-466,12719637,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:20,09/Jun/14 12:20,14/Jul/23 05:57,09/Jun/14 12:20,,,,pre-apache,,,,,,,0,github-import,,"Hi,

a user reported this issue via email:

```
ruenau4 akunkel 5 ( Full install ) $ ./stratosphere/bin/start-local.sh
sed: can't read /vol/fob-vol2/mi08/akunkel/StratosphereWBI/Full/conf/stratosphere-conf.yaml: No such file or directory
sed: can't read /vol/fob-vol2/mi08/akunkel/StratosphereWBI/Full/conf/stratosphere-conf.yaml: No such file or directory
sed: can't read /vol/fob-vol2/mi08/akunkel/StratosphereWBI/Full/conf/stratosphere-conf.yaml: No such file or directory
sed: can't read /vol/fob-vol2/mi08/akunkel/StratosphereWBI/Full/conf/stratosphere-conf.yaml: No such file or directory
sed: can't read /vol/fob-vol2/mi08/akunkel/StratosphereWBI/Full/conf/stratosphere-conf.yaml: No such file or directory
sed: can't read /vol/fob-vol2/mi08/akunkel/StratosphereWBI/Full/conf/stratosphere-conf.yaml: No such file or directory
sed: can't read /vol/fob-vol2/mi08/akunkel/StratosphereWBI/Full/conf/stratosphere-conf.yaml: No such file or directory
./stratosphere/bin/start-local.sh: line 23: /vol/fob-vol2/mi08/akunkel/StratosphereWBI/Full/bin/jobmanager.sh: No such file or directory
gruenau4 akunkel 6 ( Full install ) $ pwd
/vol/fob-vol2/mi08/akunkel/StratosphereWBI/Full install
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/466
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, user satisfaction, 
Milestone: Release 0.5
Created at: Tue Feb 11 12:49:27 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397836,,,Mon Jun 09 12:20:11 UTC 2014,,,,,,,,,,"0|i1whrj:",397963,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:20;github-import;[Date: Sat Mar 01 10:30:45 CET 2014, Author: [iasip|https://github.com/iasip]]

Just leaving a note for other members that I'm hoping to get this issue resolved.;;;","09/Jun/14 12:20;github-import;[Date: Sat Mar 01 11:45:52 CET 2014, Author: [iasip|https://github.com/iasip]]

I wanted to discuss my first steps at resolving this issue.

1. I replicated the bug by running start-local.sh under the path ""Desktop/Test Directory/"".

2. Shell scripts that have problems with whitespace are usually missing double quotes somewhere, so I looked for arguments and variables that might need double quotes around them.

3. I found a few places in config.sh that might have needed double quotes:

a. The first is the readFromConfig function at Line 44:
(local value=`sed -n ""s/^[ ]*${key}[ ]*: \([^#]*\).*$/\1/p"" ${configFile})
Because ${configFile} is not enclosed in double quotes, if you run sed on an example path like ""Desktop/Test Directory/stratosphere/conf/stratosphere-conf.yaml"", sed utility will take it as two arguments (""Desktop/Test"" and ""Directory/stratosphere/conf/stratosphere-conf.yaml"") and only run on the first (""Desktop/Test"").

b. The second is at Line 113: 
(STRATOSPHERE_ROOT_DIR_MANGLED=`manglePath $STRATOSPHERE_ROOT_DIR`). 
The manglePath function will take $STRATOSPHERE_ROOT_DIR as two arguments if it has a space in it. To avoid that, we would need double quotes around $STRATOSPHERE_ROOT_DIR.

c. The third is includes all the instances that readFromConfig is called:
(Lines 124, 156, 161, 165, 169, 173, 177)
If any of the three arguments (specifically the ${YAML_CONF}) had a space in it, without being enclosed in double quotes, one argument would be mistaken for multiple arguments.

4. To sum it up, enclosing variables/arguments in double quotes prevent functions from mistaking a single argument with whitespace as multiple arguments.  Adding the double quotes that I mentioned above got rid of the ""sed: can't find ..."" error messages.

5. Unfortunately, I'm now getting the error message: 
./stratosphere/bin/start-local.sh: line 23: /Desktop/Test: No such file or directory 
which is different from the original error message above:
./stratosphere/bin/start-local.sh: line 23: /vol/fob-vol2/mi08/akunkel/StratosphereWBI/Full/bin/jobmanager.sh: No such file or directory

Any help and criticism are welcome.;;;","09/Jun/14 12:20;github-import;[Date: Sat Mar 01 12:24:10 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,
thanks for taking care of the issue!

Your approach seems reasonable.
I think fixing the second problem ""./stratosphere/bin/start-local.sh: line 23: /Desktop/Test: No such file or directory "" should be easy.
Something like
```bash
/bin/sh ""$STRATOSPHERE_BIN_DIR/jobmanager.sh"" start local
```
should do it.

;;;","09/Jun/14 12:20;github-import;[Date: Sat Mar 01 19:01:47 CET 2014, Author: [iasip|https://github.com/iasip]]

Robert, your suggestion worked perfectly.  After correcting everything, I get the message ""Starting job manager"".
Could you direct me to where the shell scripts are located in the repo so that I may submit a pull request?
Thanks a lot.;;;","09/Jun/14 12:20;github-import;[Date: Sat Mar 01 19:09:35 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,
cool!

```bash
find . | grep ""jobmanager.sh""
```
Will point you to
```
./stratosphere-dist/src/main/stratosphere-bin/bin/jobmanager.sh
```
This is where the scripts are located.
;;;","09/Jun/14 12:20;github-import;[Date: Sun Mar 16 10:46:31 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Fixed in https://github.com/stratosphere/stratosphere/pull/521.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MacWarning is added,FLINK-465,12719636,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:20,09/Jun/14 12:20,14/Jul/23 05:57,09/Jun/14 12:20,,,,pre-apache,,,,,,,0,github-import,,"See issue ([#235|https://github.com/stratosphere/stratosphere/issues/235] | [FLINK-235|https://issues.apache.org/jira/browse/FLINK-235]) for more informations.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/465
Created by: [JonathanH5|https://github.com/JonathanH5]
Labels: 
Created at: Mon Feb 10 15:51:34 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:20;github-import;pull-request-465-5489474648026490097.patch;https://issues.apache.org/jira/secure/attachment/12649165/pull-request-465-5489474648026490097.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397835,,,2014-06-09 12:20:02.0,,,,,,,,,,"0|i1whrb:",397962,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accessing a job in the job history right after it finished leads to NPE,FLINK-464,12719635,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:19,09/Jun/14 12:20,14/Jul/23 05:57,09/Jun/14 12:20,,,,pre-apache,,,,,,,0,github-import,,"If you want to inspect a finished job right after it finished, the web gui will not load the JSON data.

In the jobmanager log file, you find the following NPE:
```
20:58:13,694 WARN  eu.stratosphere.nephele.jobmanager.web.JobmanagerInfoServlet  - java.lang.NullPointerException
	at eu.stratosphere.nephele.jobmanager.web.JobmanagerInfoServlet.writeJsonForArchivedJob(JobmanagerInfoServlet.java:229)
	at eu.stratosphere.nephele.jobmanager.web.JobmanagerInfoServlet.doGet(JobmanagerInfoServlet.java:81)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:532)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:453)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:965)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:388)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:187)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:901)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:113)
	at org.eclipse.jetty.server.Server.handle(Server.java:352)
	at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596)
	at org.eclipse.jetty.server.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:1048)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:549)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:211)
	at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:425)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:489)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:436)
	at java.lang.Thread.run(Thread.java:662)
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/464
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, gui, 
Milestone: Release 0.5
Assignee: [qmlmoon|https://github.com/qmlmoon]
Created at: Sun Feb 09 21:00:23 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397834,,,2014-06-09 12:19:59.0,,,,,,,,,,"0|i1whr3:",397961,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stratosphere debian package fixes,FLINK-462,12719633,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:19,09/Jun/14 12:19,14/Jul/23 05:57,09/Jun/14 12:19,,,,pre-apache,,,,,,,0,github-import,,"The debian package is working quite well actually. I just fixed some minor naming issues.
(build it with `mvn clean package -DskipTests -Pdebian-package`)
```
ubuntu@ubuntu-VirtualBox:~$ sudo dpkg -i stratosphere-dist_0.5-SNAPSHOT.deb 
[sudo] password for ubuntu: 
Selecting previously unselected package stratosphere-dist.
(Reading database ... 177788 files and directories currently installed.)
Unpacking stratosphere-dist (from stratosphere-dist_0.5-SNAPSHOT.deb) ...
Setting up stratosphere-dist (0.5~SNAPSHOT) ...
Adding group `stratosphere' (GID 125) ...
Done.
Processing triggers for ureadahead ...
ureadahead will be reprofiled on next reboot
ubuntu@ubuntu-VirtualBox:~$ sudo /etc/init.d/jobmanager start
Starting stratosphere jobmanager daemon: Starting job manager
stratosphere-jobmanager.
ubuntu@ubuntu-VirtualBox:~$ sudo /etc/init.d/taskmanager start
Starting stratosphere taskmanager daemon: Starting task manager on host ubuntu-VirtualBox
stratosphere-taskmanager.
ubuntu@ubuntu-VirtualBox:~$ /usr/share/stratosphere-dist/bin/stratosphere
ERROR: Please specify an action.
./stratosphere [ACTION] [GENERAL_OPTIONS] [ACTION_ARGUMENTS]
  general options:
     -h,--help      Show the help for the CLI Frontend.
     -v,--verbose   Print more detailed error messages.
```

This PR resolves issue https://github.com/stratosphere/stratosphere/issues/247

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/462
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Thu Feb 06 22:39:06 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:19;github-import;pull-request-462-5728054674216152060.patch;https://issues.apache.org/jira/secure/attachment/12649164/pull-request-462-5728054674216152060.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397832,,,Mon Jun 09 12:19:49 UTC 2014,,,,,,,,,,"0|i1whqn:",397959,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:19;github-import;[Date: Fri Feb 07 16:22:58 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged in [cfd0641037473d9abc8778917261e5ecee6b8f2d|https://github.com/stratosphere/stratosphere/commit/cfd0641037473d9abc8778917261e5ecee6b8f2d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed several bugs related to AllReduce and Iterations.,FLINK-461,12719632,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:19,09/Jun/14 12:19,14/Jul/23 05:57,09/Jun/14 12:19,,,,pre-apache,,,,,,,0,github-import,,"Fixed ([#450|https://github.com/stratosphere/stratosphere/issues/450] | [FLINK-450|https://issues.apache.org/jira/browse/FLINK-450]) and other bugs that occurred in conjunction with the large test job.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/461
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Thu Feb 06 20:45:05 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:19;github-import;pull-request-461-4870174664150520380.patch;https://issues.apache.org/jira/secure/attachment/12649163/pull-request-461-4870174664150520380.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397831,,,2014-06-09 12:19:42.0,,,,,,,,,,"0|i1whqf:",397958,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deploy spargel with dist,FLINK-459,12719630,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:19,09/Jun/14 12:19,14/Jul/23 05:57,09/Jun/14 12:19,,,,pre-apache,,,,,,,0,github-import,,"We currently do not ship `spargel` with the stratosphere downloadable binary.
The spargel out-of-the-box experience is bad since the quickstart does not use `jar-with-dependencies` as the default packaging method (for some good reasons). But, users can not simply add `spargel` as a dependency to their project and run it on a cluster.

I suggest to include this PR to the 0.4.1 release https://github.com/stratosphere/stratosphere/issues/395

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/459
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Wed Feb 05 21:19:49 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:19;github-import;pull-request-459-8897769086941176391.patch;https://issues.apache.org/jira/secure/attachment/12649161/pull-request-459-8897769086941176391.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397829,,,Mon Jun 09 12:19:29 UTC 2014,,,,,,,,,,"0|i1whpz:",397956,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:19;github-import;[Date: Fri Feb 07 16:24:46 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged in [9b717e4971f73633f8cd52f111f8af494507ae50|https://github.com/stratosphere/stratosphere/commit/9b717e4971f73633f8cd52f111f8af494507ae50];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add warning box to pre-0.4 documentation + update link to mailinglist,FLINK-458,12719629,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:19,09/Jun/14 12:19,14/Jul/23 05:57,09/Jun/14 12:19,,,,pre-apache,,,,,,,0,github-import,,"Hi,
two minor issues: 
1) Some documentation pages from the pre-0.4 documentation rank high on google, therefore, some users are confused about the mismatch. We should have a orange or red box on to of each pre-0.4 page that warns users and tells them that the documentation is for an old version.

2) I created a users-mailinglist. There are links to our current mailinglist on the front-page and on the contacts page.
They should now point to https://groups.google.com/forum/#!forum/stratosphere-users
(However, the stratosphere-dev list should be mentioned too)



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/458
Created by: [rmetzger|https://github.com/rmetzger]
Labels: website, 
Assignee: [JonathanH5|https://github.com/JonathanH5]
Created at: Wed Feb 05 15:15:18 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397828,,,Mon Jun 09 12:19:25 UTC 2014,,,,,,,,,,"0|i1whpr:",397955,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:19;github-import;[Date: Wed Feb 05 15:31:29 CET 2014, Author: [uce|https://github.com/uce]]

1) http://stratosphere.eu/docs/pre-0.4/;;;","09/Jun/14 12:19;github-import;[Date: Wed Feb 05 15:51:46 CET 2014, Author: [uce|https://github.com/uce]]

2) http://stratosphere.eu/index.html and http://stratosphere.eu/contact.;;;","09/Jun/14 12:19;github-import;[Date: Wed Feb 05 15:56:27 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Wow. I like the warning! Thanks for updating the links.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"hadoop-compat addons project: package renaming, compile-time type safety, usercode-classloader.",FLINK-453,12719624,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:19,09/Jun/14 12:19,14/Jul/23 05:57,09/Jun/14 12:19,,,,pre-apache,,,,,,,0,github-import,,"This pull request fixes the hadoop-compatibility project to use the usercode-classloader.

I also introduced the Hadoop MR `<K, V>` types for key and value.

The intention of this pull request was to allow the following syntax:
```
BSONWritable value = record.getField(1, WritableWrapper.class).value();
```
But I was not able to achieve this, even though the method signature of `value()` looks like this:
`public <X extends Writable> X value()`.
The error message is `Type mismatch: cannot convert from Writable to BSONWritable`.

I guess its because of the generics usage in the `Record.getField()` method because the whole thing works if used like this:
```
WritableWrapper<Writable> test = new WritableWrapper<Writable>();
BSONWritable aa = test.value();
```

Has anybody an idea how to solve this? (or a good explanation) 


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/453
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Sun Feb 02 18:52:16 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:19;github-import;pull-request-453-6100120220454784020.patch;https://issues.apache.org/jira/secure/attachment/12649160/pull-request-453-6100120220454784020.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397823,,,Mon Jun 09 12:19:07 UTC 2014,,,,,,,,,,"0|i1whon:",397950,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:19;github-import;[Date: Fri Feb 07 16:31:27 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

TODO: Test on cluster;;;","09/Jun/14 12:19;github-import;[Date: Sun Feb 09 21:22:47 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

merged in [169e456d3bea1252952ed1d706318901d004c540|https://github.com/stratosphere/stratosphere/commit/169e456d3bea1252952ed1d706318901d004c540];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Moving all annotations from the core to the Java API package,FLINK-452,12719623,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:18,09/Jun/14 12:19,14/Jul/23 05:57,09/Jun/14 12:19,,,,pre-apache,,,,,,,0,github-import,,"I created a ```SemanticProperties``` class (and children for SingleInput and DualInput operators) in the core package, so the properties inferred currently from Java annotations can be stored (this follows a discussion with @StephanEwen and @aljoscha).

With this commit and commit in ([#448|https://github.com/stratosphere/stratosphere/issues/448] | [FLINK-448|https://issues.apache.org/jira/browse/FLINK-448]), all the annotations have been moved from the core to the Java API package. 


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/452
Created by: [jcamachor|https://github.com/jcamachor]
Labels: 
Created at: Thu Jan 30 18:29:07 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:18;github-import;pull-request-452-6504656133582106916.patch;https://issues.apache.org/jira/secure/attachment/12649159/pull-request-452-6504656133582106916.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397822,,,Mon Jun 09 12:19:00 UTC 2014,,,,,,,,,,"0|i1whof:",397949,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:18;github-import;[Date: Thu Jan 30 18:43:10 CET 2014, Author: [jcamachor|https://github.com/jcamachor]]

I had messed up, I merge ([#448|https://github.com/stratosphere/stratosphere/issues/448] | [FLINK-448|https://issues.apache.org/jira/browse/FLINK-448]) and this in the same request... ;;;","09/Jun/14 12:19;github-import;[Date: Tue Mar 04 00:56:26 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

This is already merged as part of the new java api branch and will go in the main line together with the java api. Just letting you know that we have not forgotten this ;-);;;","09/Jun/14 12:19;github-import;[Date: Tue Mar 11 10:04:00 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged as part of [0038c9d0773e90676b1ee2a8a94dcf764f023e22|https://github.com/stratosphere/stratosphere/commit/0038c9d0773e90676b1ee2a8a94dcf764f023e22];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Iteration with keyless reducer fails on cluster,FLINK-450,12719621,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:18,09/Jun/14 12:18,14/Jul/23 05:57,09/Jun/14 12:18,,,,pre-apache,,,,,,,0,github-import,,"When running a job with a DeltaIteration that uses a keyless reducer on a cluster, the jobs fails with the following Exception:

    Exception in thread ""main"" eu.stratosphere.compiler.CompilerException: An error occurred while translating the optimized plan to a nephele JobGraph: Error: All functions that are part of an iteration must have the same degree-of-parallelism as that iteration.
	at eu.stratosphere.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:563)
	at eu.stratosphere.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:95)
	at eu.stratosphere.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:218)
	at eu.stratosphere.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:137)
	at eu.stratosphere.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:137)
	at eu.stratosphere.compiler.plan.NAryUnionPlanNode.accept(NAryUnionPlanNode.java:48)
	at eu.stratosphere.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:137)
	at eu.stratosphere.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:216)
	at eu.stratosphere.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:216)
	at eu.stratosphere.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:216)
	at eu.stratosphere.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:216)
	at eu.stratosphere.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:137)
	at eu.stratosphere.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:217)
	at eu.stratosphere.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:137)
	at eu.stratosphere.compiler.plan.OptimizedPlan.accept(OptimizedPlan.java:159)
	at eu.stratosphere.compiler.plantranslate.NepheleJobGraphGenerator.compileJobGraph(NepheleJobGraphGenerator.java:164)
	at eu.stratosphere.client.program.Client.getJobGraph(Client.java:202)
	at eu.stratosphere.client.program.Client.run(Client.java:281)
	at eu.stratosphere.client.program.Client.run(Client.java:250)
	at eu.stratosphere.client.CliFrontend.run(CliFrontend.java:304)
	at eu.stratosphere.client.CliFrontend.parseParameters(CliFrontend.java:773)
	at eu.stratosphere.client.CliFrontend.main(CliFrontend.java:796)
    Caused by: eu.stratosphere.compiler.CompilerException: Error: All functions that are part of an iteration must have the same degree-of-parallelism as that iteration.
	at eu.stratosphere.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:341)
	at eu.stratosphere.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:95)
	at eu.stratosphere.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:136)
	at eu.stratosphere.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:216)
	at eu.stratosphere.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:137)
	at eu.stratosphere.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:137)
	at eu.stratosphere.compiler.plan.WorksetIterationPlanNode.acceptForStepFunction(WorksetIterationPlanNode.java:177)
	at eu.stratosphere.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:388)
	... 21 more

A special DOP is not set.

There's already a testcase for this issue (([#280|https://github.com/stratosphere/stratosphere/issues/280] | [FLINK-280|https://issues.apache.org/jira/browse/FLINK-280])) but no fix available.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/450
Created by: [twalthr|https://github.com/twalthr]
Labels: bug, 
Created at: Thu Jan 30 14:12:35 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397820,,,2014-06-09 12:18:49.0,,,,,,,,,,"0|i1whnz:",397947,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Moving all annotations to Java API,FLINK-448,12719619,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:18,09/Jun/14 12:18,14/Jul/23 05:57,09/Jun/14 12:18,,,,pre-apache,,,,,,,0,github-import,,"I just moved the Combinable (Reduce), CombinableLeft and CombinableRight (CoGroup) annotations to the package corresponding to the Java API (as annotations are Java specific). Then I created corresponding variables in the base classes, which may be useful for APIs built on top.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/448
Created by: [jcamachor|https://github.com/jcamachor]
Labels: 
Created at: Wed Jan 29 23:50:16 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:18;github-import;pull-request-448-2774652171568009428.patch;https://issues.apache.org/jira/secure/attachment/12649158/pull-request-448-2774652171568009428.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397818,,,2014-06-09 12:18:41.0,,,,,,,,,,"0|i1whnj:",397945,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added materials section + upcoming events,FLINK-447,12719618,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:18,09/Jun/14 12:18,14/Jul/23 05:57,09/Jun/14 12:18,,,,pre-apache,,,,,,,0,github-import,,"Added materials section for logos, presentation, etc. and upcoming events... .

http://jonathanh5.github.io/stratosphere/events/

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/447
Created by: [JonathanH5|https://github.com/JonathanH5]
Labels: 
Created at: Wed Jan 29 15:37:39 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:18;github-import;pull-request-447-5883217575380670599.patch;https://issues.apache.org/jira/secure/attachment/12649157/pull-request-447-5883217575380670599.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397817,,,Mon Jun 09 12:18:39 UTC 2014,,,,,,,,,,"0|i1whnb:",397944,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:18;github-import;[Date: Wed Jan 29 16:36:37 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good, please merge;;;","09/Jun/14 12:18;github-import;[Date: Wed Jan 29 16:53:12 CET 2014, Author: [fhueske|https://github.com/fhueske]]

@rmetzger Do you want to add the Slides of your Bitkom Big Data Talk to the materials section?
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added K-Means Demo to Quickstart (Website),FLINK-446,12719617,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:17,09/Jun/14 12:18,14/Jul/23 05:57,09/Jun/14 12:18,,,,pre-apache,,,,,,,0,github-import,,"Preview: http://robertmetzger.de/stratosphere/quickstart/

Thanks to @fhueske for fixing the KMeans example and writing the python tool!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/446
Created by: [rmetzger|https://github.com/rmetzger]
Labels: website, 
Created at: Wed Jan 29 11:22:37 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:18;github-import;pull-request-446-8678374636523917300.patch;https://issues.apache.org/jira/secure/attachment/12649156/pull-request-446-8678374636523917300.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397816,,,Mon Jun 09 12:18:33 UTC 2014,,,,,,,,,,"0|i1whn3:",397943,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:18;github-import;[Date: Wed Jan 29 12:00:04 CET 2014, Author: [uce|https://github.com/uce]]

Great idea!

Some suggestions:
- What about the following ordering on the quickstart page: Setup, **Example**, Java, Scala
- For the lead paragraph, I would remove the linebreak before ""on the way""
- What about adding K-Means to the title (and other places) instead of just clustering
- The markdown markup (pun intended) is not working for `*relative standard deviation*`
;;;","09/Jun/14 12:18;github-import;[Date: Wed Jan 29 13:44:30 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Beautiful. Like Ufuk said, some markup stuff is not working, bot otherwise,
I like it...


On Wed, Jan 29, 2014 at 12:00 PM, Ufuk Celebi <notifications@github.com>wrote:

> Great idea!
>
> Some suggestions:
>
>    - What about the following ordering on the quickstart page: Setup,
>    *Example*, Java, Scala
>    - For the lead paragraph, I would remove the linebreak before ""on the
>    way""
>    - What about adding K-Means to the title (and other places) instead of
>    just clustering
>    - The markdown markup (pun intended) is not working for *relative
>    standard deviation*
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/446#issuecomment-33574599>
> .
>;;;","09/Jun/14 12:18;github-import;[Date: Wed Jan 29 13:59:13 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thanks. I fixed the issues (I did not add k-means into the box on the quickstart page, but its in the text below).;;;","09/Jun/14 12:18;github-import;[Date: Wed Jan 29 16:48:34 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Some remarks:
- Since I changed the KMeans Job (added a data source, two operators, and a sink) you should update the screenshots of the web frontend that show the plan and the progress monitoring.
- The plots of the raw data have not the same size.;;;","09/Jun/14 12:18;github-import;[Date: Fri Jan 31 12:05:35 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I updated the figures and merged it!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adapted KMeans example,FLINK-445,12719616,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:17,09/Jun/14 12:17,14/Jul/23 05:57,09/Jun/14 12:17,,,,pre-apache,,,,,,,0,github-import,,"Adapted KMeans example. Point-cluster assignments are also written as result in addition to the cluster centers.
Adapted KMeans TestCases.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/445
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Tue Jan 28 18:21:43 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;pull-request-445-4849428903792433577.patch;https://issues.apache.org/jira/secure/attachment/12649155/pull-request-445-4849428903792433577.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397815,,,Mon Jun 09 12:17:54 UTC 2014,,,,,,,,,,"0|i1whmv:",397942,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;[Date: Wed Jan 29 17:00:23 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good. Feel free to merge.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MatchNode UnsupportedOperationException fix,FLINK-444,12719615,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:17,09/Jun/14 12:17,14/Jul/23 05:57,09/Jun/14 12:17,,,,pre-apache,,,,,,,0,github-import,,"Fixes the UnsupportedOperationException in MatchNode due to immutable list. Mentioned in ([#109|https://github.com/stratosphere/stratosphere/issues/109] | [FLINK-109|https://issues.apache.org/jira/browse/FLINK-109]) comment.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/444
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Tue Jan 28 13:46:14 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;pull-request-444-5265634806246988432.patch;https://issues.apache.org/jira/secure/attachment/12649154/pull-request-444-5265634806246988432.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397814,,,Mon Jun 09 12:17:49 UTC 2014,,,,,,,,,,"0|i1whmn:",397941,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;[Date: Wed Jan 29 16:37:03 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Good to merge;;;","09/Jun/14 12:17;github-import;[Date: Fri Feb 07 16:34:33 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Commited in [9690abb2c382fbf6309676456cb867f8e76e63d8|https://github.com/stratosphere/stratosphere/commit/9690abb2c382fbf6309676456cb867f8e76e63d8];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speeded up startup time for Local Executor and Tests,FLINK-443,12719614,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:17,09/Jun/14 12:17,14/Jul/23 05:57,09/Jun/14 12:17,,,,pre-apache,,,,,,,0,github-import,,"Involves the following:
  - Faster registration of TM at JM
  - Optional lazy memory allocation in TM (used in local and test mode)
  - Some cleanup in TM startup

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/443
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Tue Jan 28 10:10:37 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;pull-request-443-8120859185428229505.patch;https://issues.apache.org/jira/secure/attachment/12649153/pull-request-443-8120859185428229505.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397813,,,Mon Jun 09 12:17:44 UTC 2014,,,,,,,,,,"0|i1whmf:",397940,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;[Date: Tue Jan 28 12:48:14 CET 2014, Author: [uce|https://github.com/uce]]

Nice! :-);;;","09/Jun/14 12:17;github-import;[Date: Wed Jan 29 17:02:12 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

That is fixed by now. Some of the legacy instance manager code is failing non-deterministically at this point. Seems that it somewhat relied on a delayed heartbeat.;;;","09/Jun/14 12:17;github-import;[Date: Fri Apr 25 18:54:35 CEST 2014, Author: [rmetzger|https://github.com/rmetzger]]

This pull request has not been merged, right?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added Reflective AVRO types.,FLINK-442,12719613,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:17,09/Jun/14 12:17,14/Jul/23 05:57,09/Jun/14 12:17,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/442
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Tue Jan 28 10:06:24 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;pull-request-442-7336181184802200787.patch;https://issues.apache.org/jira/secure/attachment/12649152/pull-request-442-7336181184802200787.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397812,,,Mon Jun 09 12:17:36 UTC 2014,,,,,,,,,,"0|i1whm7:",397939,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;[Date: Tue Jan 28 10:06:50 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

New pull request with cleaned history and updated naming per @uce's suggestion;;;","09/Jun/14 12:17;github-import;[Date: Wed Jan 29 16:55:52 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [930b064ecb1c6a73236eb1a7e39673fc64b6e5d4|https://github.com/stratosphere/stratosphere/commit/930b064ecb1c6a73236eb1a7e39673fc64b6e5d4];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reorganized .gitignore,FLINK-440,12719611,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:17,09/Jun/14 12:17,14/Jul/23 05:57,09/Jun/14 12:17,,,,pre-apache,,,,,,,0,github-import,,"The .gitignore file seemed to contain duplicates. Here is a cleaned up and sorted version of the file.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/440
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Created at: Mon Jan 27 11:55:22 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;pull-request-440-357665904734905345.patch;https://issues.apache.org/jira/secure/attachment/12649151/pull-request-440-357665904734905345.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397810,,,Mon Jun 09 12:17:28 UTC 2014,,,,,,,,,,"0|i1whlr:",397937,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;[Date: Mon Jan 27 11:56:59 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Looks mergeable.;;;","09/Jun/14 12:17;github-import;[Date: Tue Jan 28 09:25:43 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

@uce Can you add the entry and then merge?;;;","09/Jun/14 12:17;github-import;[Date: Tue Jan 28 12:31:26 CET 2014, Author: [uce|https://github.com/uce]]

Merged in [85020f84cfa42fd5c1bc9319cd644005295daff4|https://github.com/stratosphere/stratosphere/commit/85020f84cfa42fd5c1bc9319cd644005295daff4].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
new blog post on accessing mongodb from stratosphere,FLINK-439,12719610,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:17,09/Jun/14 12:17,14/Jul/23 05:57,09/Jun/14 12:17,,,,pre-apache,,,,,,,0,github-import,,"Preview: http://robertmetzger.de/stratosphere/news/blog/tutorial/2014/01/28/querying_mongodb.html

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/439
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Jan 27 09:20:46 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;pull-request-439-2789499331032638123.patch;https://issues.apache.org/jira/secure/attachment/12649150/pull-request-439-2789499331032638123.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397809,,,Mon Jun 09 12:17:22 UTC 2014,,,,,,,,,,"0|i1whlj:",397936,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;[Date: Mon Jan 27 11:16:56 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Sweet! Nice work!

You might want to add, what the example program is doing with the data in MongoDB.
How about putting the example code somewhere in the stratosphere repository instead of linking against your repo? This post might exists for quite some time.

There might also be some follow up issues:
- Is the format also integrated with the Scala API. If not, we should open another issue for that. 
- We should also add it to the documentation.;;;","09/Jun/14 12:17;github-import;[Date: Mon Jan 27 11:19:46 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good!

How did you implement the writable wrapper? Does it serialize the class of
the type every time, or is there some fancy different mechanism?


Am 27.01.2014 09:20 schrieb ""Robert Metzger"" <notifications@github.com>:

> Preview:
> http://robertmetzger.de/stratosphere/news/blog/tutorial/2014/01/28/querying_mongodb.html
> ------------------------------
> You can merge this Pull Request by running
>
>   git pull https://github.com/rmetzger/stratosphere mongodb
>
> Or view, comment on, or merge it at:
>
>   https://github.com/stratosphere/stratosphere/pull/439
> Commit Summary
>
>    - new blog post on accessing mongodb from stratosphere
>
> File Changes
>
>    - *A* _posts/2014-01-28-querying_mongodb.md<https://github.com/stratosphere/stratosphere/pull/439/files#diff-0>(108)
>    - *A* img/blog/robomongo.png<https://github.com/stratosphere/stratosphere/pull/439/files#diff-1>(0)
>
> Patch Links:
>
>    - https://github.com/stratosphere/stratosphere/pull/439.patch
>    - https://github.com/stratosphere/stratosphere/pull/439.diff
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/439>
> .
>;;;","09/Jun/14 12:17;github-import;[Date: Mon Jan 27 11:36:20 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

> You might want to add, what the example program is doing with the data in MongoDB.

Thats already in the text: ""The example program reads data from the enron dataset that contains about 500k internal e-mails. The data is stored in MongoDB and the Stratosphere program counts the number of e-mails per day.""

> How about putting the example code somewhere in the stratosphere repository instead of linking against your repo? This post might exists for quite some time.
Agree, will do.

You are right, documentation and Scala support is missing. I think we need to find a generic way to support Stratosphere Java InputFormats in Scala.

@StephanEwen: The wrapper is not fancy at all ;) It serializes the type as a string all the time. Feel free to suggest improvements (if the usability does not suffer).
;;;","09/Jun/14 12:17;github-import;[Date: Mon Jan 27 12:20:29 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The only way to avoid that is with static type info, similar to the way I
did it with the Avro value. Have a look at that.

I am a bit torn between the the PROs of writing such a blog post and the
CONs of making it depend on such a workaround that is really not feasible
for anything real where you expect only a little bit performance.

For the new type safe record API, we need to directly support:

  1. Out Value types
  2. Java's primitive types
  3. Hadoop's writable types
  4. Avro types

There are nice ways of doing this efficiently, without needing to serialize
types.


On Mon, Jan 27, 2014 at 2:36 AM, Robert Metzger <notifications@github.com>wrote:

> You might want to add, what the example program is doing with the data in
> MongoDB.
>
> Thats already in the text: ""The example program reads data from the enron
> dataset that contains about 500k internal e-mails. The data is stored in
> MongoDB and the Stratosphere program counts the number of e-mails per day.""
>
> How about putting the example code somewhere in the stratosphere
> repository instead of linking against your repo? This post might exists for
> quite some time.
> Agree, will do.
>
>  You are right, documentation and Scala support is missing. I think we
> need to find a generic way to support Stratosphere Java InputFormats in
> Scala.
>
> @StephanEwen <https://github.com/StephanEwen>: The wrapper is not fancy
> at all ;) It serializes the type as a string all the time. Feel free to
> suggest improvements (if the usability does not suffer).
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/439#issuecomment-33356722>
> .
>;;;","09/Jun/14 12:17;github-import;[Date: Mon Jan 27 13:01:16 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

While we're at MongoDB, there is an upcoming [Meetup in the MongoDB user group|http://www.meetup.com/MUGBerlin/events/161353802/] in Berlin. 

I know the guy who is presenting and I'm going this Thurday from 6PM, and I also have a +2 for the event if you are interested. 

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Discussion: Allow Anonymous Classes for Functions in Plan,FLINK-438,12719609,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:17,09/Jun/14 12:17,14/Jul/23 05:57,09/Jun/14 12:17,,,,pre-apache,,,,,,,0,github-import,,"**This PR is just for discussion.**

I've been thinking about our Java API and @StephanEwen is also prototyping a new API, which will bring the Java and Scala APIs closer together (among other things) by moving the Java API to the distributed DataSet abstraction instead of operator wiring (which is imho super important and will be a big plus).

With the upcoming support for Lambda expressions in Java 8, the upcoming API will be very concise. Since this will only be supported in Java 8, I assume that we will provide separate Java 6/7 and Java 8 APIs. For the Java 6/7 API, I thought that it might be worthwhile to support anonymous classes as function implementations.

`AnonymousWordCount` of this PR shows the default `WordCount` implementation with anonymous classes instead of static nested classes, which makes it imho more readable.

But there is a problem with serializability. With anonymous classes you can only implement a single interface (e.g. `MapFunction`), but we also need to implement `Serializable` (and make sure that everything is indeed serializable). For this discussion I've declared `AbstractFunction` as `Serializable` as a work around, but it has problems (see ([#147|https://github.com/stratosphere/stratosphere/issues/147] | [FLINK-147|https://issues.apache.org/jira/browse/FLINK-147])). One way around this would be to provide something like `SerializableMapFunction`. But I find it somehow ugly and inconvenient in terms of usability for newcomers.

So the questions for discussions are:
- What do you think about supporting this?
- What is the best solution if we want it?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/438
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Sat Jan 25 14:10:11 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;pull-request-438-6292829065504271929.patch;https://issues.apache.org/jira/secure/attachment/12649149/pull-request-438-6292829065504271929.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397808,,,Mon Jun 09 12:17:14 UTC 2014,,,,,,,,,,"0|i1whlb:",397935,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;[Date: Sat Jan 25 20:23:39 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I have stumbled over that issue as well. I think serializability on the
abstractstub level would be fine. I think we should also promote the object
passing instead of class passing as the default case.
Am 25.01.2014 05:10 schrieb ""Ufuk Celebi"" <notifications@github.com>:

> *This PR is just for discussion.*
>
> I've been thinking about our Java API and @StephanEwen<https://github.com/StephanEwen>is also prototyping a new API, which will bring the Java and Scala APIs
> closer together (among other things) by moving the Java API to the
> distributed DataSet abstraction instead of operator wiring (which is imho
> super important and will be a big plus).
>
> With the upcoming support for Lambda expressions in Java 8, the upcoming
> API will be very concise. Since this will only be supported in Java 8, I
> assume that we will provide separate Java 6/7 and Java 8 APIs. For the Java
> 6/7 API, I thought that it might be worthwhile to support anonymous classes
> as function implementations.
>
> AnonymousWordCount of this PR shows the default WordCount implementation
> with anonymous classes instead of static nested classes, which makes it
> imho more readable.
>
> But there is a problem with serializability. With anonymous classes you
> can only implement a single interface (e.g. MapFunction), but we also
> need to implement Serializable (and make sure that everything is indeed
> serializable). For this discussion I've declared AbstractFunction as
> Serializable as a work around, but it has problems (see ([#147|https://github.com/stratosphere/stratosphere/issues/147] | [FLINK-147|https://issues.apache.org/jira/browse/FLINK-147])<https://github.com/stratosphere/stratosphere/issues/147>).
> One way around this would be to provide something like
> SerializableMapFunction. But I find it somehow ugly and inconvenient in
> terms of usability for newcomers.
>
> So the questions for discussions are:
>
>    - What do you think about supporting this?
>    - What is the best solution if we want it?
>
> ------------------------------
> You can merge this Pull Request by running
>
>   git pull https://github.com/uce/stratosphere anon_classes
>
> Or view, comment on, or merge it at:
>
>   https://github.com/stratosphere/stratosphere/pull/438
> Commit Summary
>
>    - Add WordCount example with anonymous classes (for discussion)
>
> File Changes
>
>    - *M*
>    stratosphere-core/src/main/java/eu/stratosphere/api/common/Program.java<https://github.com/stratosphere/stratosphere/pull/438/files#diff-0>(4)
>    - *M*
>    stratosphere-core/src/main/java/eu/stratosphere/api/common/functions/AbstractFunction.java<https://github.com/stratosphere/stratosphere/pull/438/files#diff-1>(4)
>    - *A*
>    stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/record/wordcount/AnonymousWordCount.java<https://github.com/stratosphere/stratosphere/pull/438/files#diff-2>(99)
>    - *M*
>    stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/record/wordcount/WordCount.java<https://github.com/stratosphere/stratosphere/pull/438/files#diff-3>(4)
>    - *M*
>    stratosphere-tests/src/test/java/eu/stratosphere/test/exampleRecordPrograms/WordCountITCase.java<https://github.com/stratosphere/stratosphere/pull/438/files#diff-4>(3)
>
> Patch Links:
>
>    - https://github.com/stratosphere/stratosphere/pull/438.patch
>    - https://github.com/stratosphere/stratosphere/pull/438.diff
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/438>
> .
>;;;","09/Jun/14 12:17;github-import;[Date: Mon Jan 27 11:27:12 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I like to have anonymous classes as an option. This is much nicer to use for simple UDFs. 
Giving The user the choice between regular and anonymous classes is the right way to go, I think.;;;","09/Jun/14 12:17;github-import;[Date: Tue Jan 28 09:29:13 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I vote for making the AbstractFunction serializable.;;;","09/Jun/14 12:17;github-import;[Date: Thu Feb 06 08:37:18 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

This PR is mergable, right? AbstractFunction is serializable.;;;","09/Jun/14 12:17;github-import;[Date: Fri Feb 07 16:36:42 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged in [d74d4bf7e360de4c230314285725a54472e095d5|https://github.com/stratosphere/stratosphere/commit/d74d4bf7e360de4c230314285725a54472e095d5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hadoop-compat addons project with HadoopDataSource,FLINK-437,12719608,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:16,09/Jun/14 12:17,14/Jul/23 05:57,09/Jun/14 12:17,,,,pre-apache,,,,,,,0,github-import,,"I picked up the code of https://github.com/stratosphere/stratosphere/pull/424 and enhanced it with the following:

* make hadoop-compat compatible to hadoop yarn and remove code in package `org.apache.hadoop`.
* Extended usercode object wrapper (@aljoscha, please validate my changes), fixed InputFormat serialization
* introduce pluggable type converter
* Add a generic wrapper for Hadoop's `Writable` and `WritableComparable`.

The generic wrapper allows to do the following: (The example is from Mongodb's Hadoop InputFormat)
```java
public void map(Record record, Collector<Record> out) throws Exception {
	WritableWrapper wrap = record.getField(0, WritableWrapper.class);
	Writable wr = wrap.value();
	Writable valWr = record.getField(1, WritableWrapper.class).value();
	BSONWritable bson = (BSONWritable) wr;
	BSONWritable value = (BSONWritable) valWr;
	System.err.println(""bson value has ""+value.toString());
}
```

Lets discuss if we want the project being called ""hadoop-compat"" or if you prefer ""hadoop-compatability"" ?

Open issues:
* If the Hadoop IF is a FileInputFormat, we should make the split assignment file locality aware
* Add junit tests for various formats such as ORC, Parquet and Avro.
* Write website documentation for this code.
* Add HadoopDataSink
* See if it is possible to map hadoop counters to our accumulators

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/437
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Milestone: Release 0.5
Created at: Sat Jan 25 13:54:15 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;pull-request-437-6596101347621610826.patch;https://issues.apache.org/jira/secure/attachment/12649148/pull-request-437-6596101347621610826.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397807,,,Mon Jun 09 12:17:06 UTC 2014,,,,,,,,,,"0|i1whl3:",397934,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:17;github-import;[Date: Sat Jan 25 14:13:27 CET 2014, Author: [uce|https://github.com/uce]]

Nice. I vote for `hadoop-compatability` as `compat` is imho not a standard acronym.

What do you mean with ""Add HadoopDataSource""? Typo for **sink**? ;;;","09/Jun/14 12:17;github-import;[Date: Sat Jan 25 17:44:27 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I saw `compat` being used by some Linux developers ;)
And the source was a typo, I meant sink.

I added an additional converter that allows to use any Hadoop `Writable`!;;;","09/Jun/14 12:17;github-import;[Date: Mon Jan 27 11:32:41 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Not sure, if I would call it either. 
This feature gives ""only"" support for Hadoop InputFormats, but there is lots of other stuff in Hadoop which we do not support right now such as OutputFormats, Map, and Reduce functions.
If we plan to extend the support for these interfaces as well, I would go with `hadoop-compatibility`. 
Otherwise, `hadoop-input` would be a good name IMHO.;;;","09/Jun/14 12:17;github-import;[Date: Mon Jan 27 11:34:05 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

The plan is to put everything related to hadoop compatability into this package. 
I think the next step would be a `OutputFormat`.

I will change the name!;;;","09/Jun/14 12:17;github-import;[Date: Mon Jan 27 12:15:50 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

We are thinking about adding proper interface compatibility with Hadoop
later. That could all be in one project, called  hadoop-compatibility.



On Mon, Jan 27, 2014 at 2:34 AM, Robert Metzger <notifications@github.com>wrote:

> The plan is to put everything related to hadoop compatability into this
> package.
> I think the next step would be a OutputFormat.
>
> I will change the name!
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/437#issuecomment-33356572>
> .
>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typo in example for Configuration example,FLINK-436,12719607,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:16,09/Jun/14 12:16,14/Jul/23 05:57,09/Jun/14 12:16,,,,pre-apache,,,,,,,0,github-import,,"Small typo in example code.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/436
Created by: [pims|https://github.com/pims]
Labels: 
Created at: Sat Jan 25 08:47:39 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:16;github-import;pull-request-436-8834280184166696285.patch;https://issues.apache.org/jira/secure/attachment/12649147/pull-request-436-8834280184166696285.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397806,,,Mon Jun 09 12:16:56 UTC 2014,,,,,,,,,,"0|i1whkv:",397933,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:16;github-import;[Date: Sat Jan 25 09:18:19 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thanks @pims for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
First prototype of language-binding for python,FLINK-435,12719606,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:16,09/Jun/14 12:16,14/Jul/23 05:57,09/Jun/14 12:16,,,,pre-apache,,,,,,,0,github-import,,"The first prototype consists of a small python- and java-framework, which can currently work with an Reduce and Map-Operator and sent over sockets and stdio. Google Protocol Buffers are used to send records between the java and python processes. 

This commit also includes an running-wordcount example. The UDFs can be defined in python, while the configuration and Plan-setup still has to be made in java. 



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/435
Created by: [filiphaase|https://github.com/filiphaase]
Labels: 
Milestone: Release 0.5
Created at: Fri Jan 24 17:46:58 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:16;github-import;pull-request-435-7933146672845985349.patch;https://issues.apache.org/jira/secure/attachment/12649146/pull-request-435-7933146672845985349.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397805,,,Mon Jun 09 12:16:48 UTC 2014,,,,,,,,,,"0|i1whkn:",397932,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:16;github-import;[Date: Mon Jan 27 11:49:15 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Nice work!

I guess you have this already on your roadmap, but in any case: You can extend `MapFunction` and `ReduceFunction` to `PyMapFunction` and `PyReduceFunction` to hide much of the stream setup and make the example WordCount program more concise and attractive. Also, 8080 is a port commonly used for HTTP. Maybe using a less popular port would be better.
;;;","09/Jun/14 12:16;github-import;[Date: Mon Jan 27 12:24:14 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

8080 is also used for our own web client by default ;-)



On Mon, Jan 27, 2014 at 2:49 AM, Fabian Hueske <notifications@github.com>wrote:

> Nice work!
>
> I guess you have this already on your roadmap, but in any case: You can
> extend MapFunction and ReduceFunction to PyMapFunction and
> PyReduceFunction to hide much of the stream setup and make the example
> WordCount program more concise and attractive. Also, 8080 is a port
> commonly used for HTTP. Maybe using a less popular port would be better.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/435#issuecomment-33357503>
> .
>;;;","09/Jun/14 12:16;github-import;[Date: Tue Jan 28 10:00:37 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I thought about how to represent the data flow in external languages.
Couldn't we build a generic compiler in Java, that translates data flows defined in JSON into Stratosphere programs. Then, each language binding could offer native language primitives to build a data flow, compile it to JSON, and generate a Stratosphere program from that.

Any ideas?;;;","09/Jun/14 12:16;github-import;[Date: Tue Jan 28 12:16:07 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

We thought about something similar, but using Protobuf for that. So our approach would represent the Graph using a set of protobuf objects. At the end of the day, it does not matter which data serialisation we're using.

Here is a list of pro and cons:

JSON:
+ Human Readable
+ String-based, so easier to use from all languages.
- Verbose than a binary representation
- One more dependency since we need a JSON parser.

Protobuf:
+ We already use it

@filiphaase is going to work on this this week to figure out whats the best approach for this. We will discuss his results on Friday.;;;","09/Jun/14 12:16;github-import;[Date: Tue Jan 28 17:36:18 CET 2014, Author: [filiphaase|https://github.com/filiphaase]]

@fhueske Thanks :) Your definitively right about the PyMapFunction,... classes and they will absolutely be part of the next version of the prototype. But currently I still have hardcoded values in my classes(Path of python script, the classes of each value of the records, connectionType and if necessary the port), so I would still have to overwrite the open() function to include them in this code. Therefore I would like to include the extended classes in the next version. 

And for the JSON/Protobuf discussion: Like robert stated, I'm currently working on the protobuf-version to transfer the plan to java, and therefore enabling the user to use stratosphere by just writing python. Since I'm already using protobuf for the tuple streaming and know how to use it, it's not very hard to use protobuf for transfering the plan and hopefully it will work until friday :) 

Since I'm a stratosphere-newbie I also have two general questions:

- Can I update this pull-request by just pushing into filiphaase:langbinding-pr ? If not, how can I update my code?

- I need to transfer some values like the python-script-path and the classes for each value of the records to my Map/ReduceFunctions. As I see it using setParameter() and the Configuration-Object is the way to do this. But I can not call it with a list of classes. Is there any way I could give something else than string,int or boolean to my Functions? Otherwise my best idea is to give a String-Representation of the classes to my functions(f.e.""StringValue,IntValue"");;;","09/Jun/14 12:16;github-import;[Date: Tue Jan 28 17:43:07 CET 2014, Author: [fhueske|https://github.com/fhueske]]

@filiphaase Thanks for sharing your plans. Sounds good! 
Regarding you questions:
- You can update your PR by simply pushing into the same branch. If you changed some previous commits, you need to force the push (`git push origin abc --force`).
- We're moving a bit away from the configuration. You can also pass objects to the operators (instead of classes). The objects need to be serializable (implement the interface). Then Java serialization will take care of moving the object state which can be set by a custom constructor or settters.;;;","09/Jun/14 12:16;github-import;[Date: Tue Jan 28 18:20:28 CET 2014, Author: [filiphaase|https://github.com/filiphaase]]

@fhueske Thanks for the quick answer, I will try it that way :);;;","09/Jun/14 12:16;github-import;[Date: Fri Feb 07 16:38:33 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

We will not merge this pull request into the `master`-branch since the advanced python support is making good progress. (we are also planning to release 0.5 soon). 
Full python support (with python-based plan creation) goes into 0.6.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala reduce operator throws NPE for empty input,FLINK-434,12719605,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:16,09/Jun/14 12:16,14/Jul/23 05:57,09/Jun/14 12:16,,,,pre-apache,,,,,,,0,github-import,,"The following key-less reduce operator produces a NPE for empty input.

```scala
val agg = pl reduce { (l1, l2) =>  l1 + l2 }
```

```
14:47:03,860 ERROR eu.stratosphere.pact.runtime.task.RegularPactTask             - Error in PACT code: Reduce(<Unnamed Reducer>) (1/1)
14/01/24 14:47:03 ERROR task.RegularPactTask: java.lang.NullPointerException
java.lang.NullPointerException
	at eu.stratosphere.pact.common.type.PactRecord.copyFrom(PactRecord.java:750)
	at eu.stratosphere.tpch.query.TPCHQuery19$$anon$14.reduce(TPCHQuery19.scala:114)
	at eu.stratosphere.pact.runtime.task.AllReduceDriver.run(AllReduceDriver.java:96)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:405)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:296)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:354)
	at java.lang.Thread.run(Thread.java:744)
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/434
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, runtime, scala api, 
Assignee: [aljoscha|https://github.com/aljoscha]
Created at: Fri Jan 24 14:50:43 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397804,,,2014-06-09 12:16:36.0,,,,,,,,,,"0|i1whkf:",397931,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide Iterable instead of Iterator to updateVertex method of Spargel API ,FLINK-433,12719604,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:16,09/Jun/14 12:16,14/Jul/23 05:57,09/Jun/14 12:16,,,,pre-apache,,,,,,,0,github-import,,"This is issue ([#425|https://github.com/stratosphere/stratosphere/issues/425] | [FLINK-425|https://issues.apache.org/jira/browse/FLINK-425]).

I did this, before @StephanEwen and @sscdotopen discussed the `IterableIterator` in ([#425|https://github.com/stratosphere/stratosphere/issues/425] | [FLINK-425|https://issues.apache.org/jira/browse/FLINK-425]). Why do we want to expose the Iterator?



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/433
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Thu Jan 23 18:32:45 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:16;github-import;pull-request-433-2953217509240297908.patch;https://issues.apache.org/jira/secure/attachment/12649145/pull-request-433-2953217509240297908.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397803,,,Mon Jun 09 12:16:33 UTC 2014,,,,,,,,,,"0|i1whk7:",397930,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:16;github-import;[Date: Thu Jan 23 18:38:15 CET 2014, Author: [sscdotopen|https://github.com/sscdotopen]]

Exposing the iterator is part of the Iterable interface :)



On 01/23/2014 06:32 PM, Ufuk Celebi wrote:
> This is issue ([#425|https://github.com/stratosphere/stratosphere/issues/425] | [FLINK-425|https://issues.apache.org/jira/browse/FLINK-425]).
>
> I did this, before @StephanEwen and @sscdotopen discuessed the `IterableIterator` in ([#425|https://github.com/stratosphere/stratosphere/issues/425] | [FLINK-425|https://issues.apache.org/jira/browse/FLINK-425]). Why do we want to expose the Iterator?
>
>
> You can merge this Pull Request by running:
>
>    git pull https://github.com/uce/stratosphere spargel_iterable
>
> Or you can view, comment on it, or merge it online at:
>
>    https://github.com/stratosphere/stratosphere/pull/433
>
> -- Commit Summary --
>
>    * Provide Iterable instead of Iterator to updateVertex method of Spargel API (issue ([#425|https://github.com/stratosphere/stratosphere/issues/425] | [FLINK-425|https://issues.apache.org/jira/browse/FLINK-425]))
>    * Remove non-sensical comment from SpargelConnectedComponents (issue ([#428|https://github.com/stratosphere/stratosphere/issues/428] | [FLINK-428|https://issues.apache.org/jira/browse/FLINK-428]))
>
> -- File Changes --
>
>      M stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/SpargelIteration.java (10)
>      M stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/VertexUpdateFunction.java (2)
>      M stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/examples/connectedcomponents/SpargelConnectedComponents.java (7)
>      A stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/util/MessageIterable.java (56)
>
> -- Patch Links --
>
> https://github.com/stratosphere/stratosphere/pull/433.patch
> https://github.com/stratosphere/stratosphere/pull/433.diff
>
> ---
> Reply to this email directly or view it on GitHub:
> https://github.com/stratosphere/stratosphere/pull/433
>;;;","09/Jun/14 12:16;github-import;[Date: Thu Jan 23 18:40:46 CET 2014, Author: [uce|https://github.com/uce]]

Yes. I meant why not just provide Iterable to the user?

Sent from my iPhone

> On 23 Jan 2014, at 18:38, sscdotopen <notifications@github.com> wrote:
> 
> Exposing the iterator is part of the Iterable interface :)
> 
> 
> 
> On 01/23/2014 06:32 PM, Ufuk Celebi wrote:
> > This is issue ([#425|https://github.com/stratosphere/stratosphere/issues/425] | [FLINK-425|https://issues.apache.org/jira/browse/FLINK-425]).
> >
> > I did this, before @StephanEwen and @sscdotopen discuessed the `IterableIterator` in ([#425|https://github.com/stratosphere/stratosphere/issues/425] | [FLINK-425|https://issues.apache.org/jira/browse/FLINK-425]). Why do we want to expose the Iterator?
> >
> >
> > You can merge this Pull Request by running:
> >
> > git pull https://github.com/uce/stratosphere spargel_iterable
> >
> > Or you can view, comment on it, or merge it online at:
> >
> > https://github.com/stratosphere/stratosphere/pull/433
> >
> > -- Commit Summary --
> >
> > * Provide Iterable instead of Iterator to updateVertex method of Spargel API (issue ([#425|https://github.com/stratosphere/stratosphere/issues/425] | [FLINK-425|https://issues.apache.org/jira/browse/FLINK-425]))
> > * Remove non-sensical comment from SpargelConnectedComponents (issue ([#428|https://github.com/stratosphere/stratosphere/issues/428] | [FLINK-428|https://issues.apache.org/jira/browse/FLINK-428]))
> >
> > -- File Changes --
> >
> > M stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/SpargelIteration.java (10)
> > M stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/VertexUpdateFunction.java (2)
> > M stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/examples/connectedcomponents/SpargelConnectedComponents.java (7)
> > A stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/util/MessageIterable.java (56)
> >
> > -- Patch Links --
> >
> > https://github.com/stratosphere/stratosphere/pull/433.patch
> > https://github.com/stratosphere/stratosphere/pull/433.diff
> >
> > ---
> > Reply to this email directly or view it on GitHub:
> > https://github.com/stratosphere/stratosphere/pull/433
> >
> —
> Reply to this email directly or view it on GitHub.;;;","09/Jun/14 12:16;github-import;[Date: Thu Jan 23 18:47:01 CET 2014, Author: [sscdotopen|https://github.com/sscdotopen]]

We discuss the proposal to provide an Iterable to the user. This is more 
convenient and in sync with what other systems like Hadoop or Giraph do.

There is only one problem: You can call iterator() on an Iterable to get 
an Iterator for the underlying data.

Now, in general you could create two Iterators and advance them 
individually in different orders. This doesn't work in our case, as we 
don't want to materialize all the records of the Iterable in memory, 
instead Stratosphere deserializes them one by one according to next() 
calls on the Iterator (afaik).

I think always returning the same iterator instance should be fine as it 
is reasonable. We should simply document it.


On 01/23/2014 06:40 PM, Ufuk Celebi wrote:
> Yes. I meant why not just provide Iterable to the user?
>
> Sent from my iPhone
>
>> On 23 Jan 2014, at 18:38, sscdotopen <notifications@github.com> wrote:
>>
>> Exposing the iterator is part of the Iterable interface :)
>>
>>
>>
>> On 01/23/2014 06:32 PM, Ufuk Celebi wrote:
>>> This is issue ([#425|https://github.com/stratosphere/stratosphere/issues/425] | [FLINK-425|https://issues.apache.org/jira/browse/FLINK-425]).
>>>
>>> I did this, before @StephanEwen and @sscdotopen discuessed the `IterableIterator` in ([#425|https://github.com/stratosphere/stratosphere/issues/425] | [FLINK-425|https://issues.apache.org/jira/browse/FLINK-425]). Why do we want to expose the Iterator?
>>>
>>>
>>> You can merge this Pull Request by running:
>>>
>>> git pull https://github.com/uce/stratosphere spargel_iterable
>>>
>>> Or you can view, comment on it, or merge it online at:
>>>
>>> https://github.com/stratosphere/stratosphere/pull/433
>>>
>>> -- Commit Summary --
>>>
>>> * Provide Iterable instead of Iterator to updateVertex method of Spargel API (issue ([#425|https://github.com/stratosphere/stratosphere/issues/425] | [FLINK-425|https://issues.apache.org/jira/browse/FLINK-425]))
>>> * Remove non-sensical comment from SpargelConnectedComponents (issue ([#428|https://github.com/stratosphere/stratosphere/issues/428] | [FLINK-428|https://issues.apache.org/jira/browse/FLINK-428]))
>>>
>>> -- File Changes --
>>
>>> M stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/SpargelIteration.java (10)
>>> M stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/VertexUpdateFunction.java (2)
>>> M stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/examples/connectedcomponents/SpargelConnectedComponents.java (7)
>>> A stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/util/MessageIterable.java (56)
>>>
>>> -- Patch Links --
>>>
>>> https://github.com/stratosphere/stratosphere/pull/433.patch
>>> https://github.com/stratosphere/stratosphere/pull/433.diff
>>>
>>> ---
>>> Reply to this email directly or view it on GitHub:
>>> https://github.com/stratosphere/stratosphere/pull/433
>>>
>> —
>> Reply to this email directly or view it on GitHub.
>
> ---
> Reply to this email directly or view it on GitHub:
> https://github.com/stratosphere/stratosphere/pull/433#issuecomment-33148154
>;;;","09/Jun/14 12:16;github-import;[Date: Thu Jan 23 21:37:34 CET 2014, Author: [uce|https://github.com/uce]]

OK, if the others agree, I would add the missing documentation.

@rmetzger I wanted to add a test case based on `SpargelConnectedComponents` example, but had a problem with the dependencies. When I add `stratosphere-tests` as a (test-scoped) dependency to Spargel, I only have access to `TestBase2`, which is in the main branch of `stratosphere-tests`. But I also want to build on existing tests, which provide utility methods for connected components.

It worked in IntelliJ, when I provided the following two dependencies:

```XML
<!-- TestBase2 -->
<dependency>
    <groupId>eu.stratosphere</groupId>
    <artifactId>stratosphere-tests</artifactId>
    <version>${project.version}</version>
    <scope>test</scope>
</dependency>
<dependency>
    <groupId>eu.stratosphere</groupId>
    <artifactId>stratosphere-tests</artifactId>
    <version>${project.version}</version>
    <type>test-jar</type>
    <scope>test</scope>
</dependency>
```
This works in the IDE, but not in Maven on the CLI (which makes sense, because we are not packaging the test-jars, right?). Is there a way to do it ([a|http://stackoverflow.com/questions/7000812/how-can-i-make-the-test-jar-include-dependencies-in-maven], [b|http://nerdgerl.wordpress.com/2009/07/24/creating-a-test-jar-in-maven/)]?;;;","09/Jun/14 12:16;github-import;[Date: Fri Jan 24 15:51:39 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

You forgot to export the code of `stratosphere-tests` as a jar.
I pushed the test to a branch in my repository, see the changes here: https://github.com/rmetzger/stratosphere/compare/rtfm;;;","09/Jun/14 12:16;github-import;[Date: Fri Jan 24 15:55:19 CET 2014, Author: [uce|https://github.com/uce]]

OK, I didn't test the advice from [the link I provided|http://nerdgerl.wordpress.com/2009/07/24/creating-a-test-jar-in-maven/], because I thought that we had to somehow rely solely on the Maven assembly plugin for packaging.

I'll add it to this PR.;;;","09/Jun/14 12:16;github-import;[Date: Sat Jan 25 01:34:59 CET 2014, Author: [uce|https://github.com/uce]]

The `stratosphere-tests` test jars have to be deployed for the new Spargel test case to work.

I hope it will work with the addition of the `maven-jar-plugin` to `stratosphere-tests/pom.xml`.;;;","09/Jun/14 12:16;github-import;[Date: Sat Jan 25 20:18:22 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Do we need a test jar? Does it not work to add to spargel a dependency on
stratosphere-tests in scope test?
Am 24.01.2014 16:35 schrieb ""Ufuk Celebi"" <notifications@github.com>:

> The stratosphere-tests test jars have to be deployed for the new Spargel
> test case to work.
>
> I hope it will work with the addition of the maven-jar-plugin to
> stratosphere-tests/pom.xml.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/433#issuecomment-33275061>
> .
>;;;","09/Jun/14 12:16;github-import;[Date: Sun Jan 26 10:33:51 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

No, thats not enough.
The problem is that maven does not package the contents of `src/test/java` into the `jar` file.
My change builds now two jar-files for the `stratosphere-tests` project (one with the tests).
I dont see any disadvantages for my approach.;;;","09/Jun/14 12:16;github-import;[Date: Sun Jan 26 11:37:38 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

What classes from stratosphere-tests do you need? The test base is under
src/main/java, so it should be packaged and available by default. I am not
against a test jar, I am only wondering what I am missing...
Am 26.01.2014 01:33 schrieb ""Robert Metzger"" <notifications@github.com>:

> No, thats not enough.
> The problem is that maven does not package the contents of src/test/javainto the
> jar file.
> My change builds now two jar-files for the stratosphere-tests project
> (one with the tests).
> I dont see any disadvantages for my approach.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/433#issuecomment-33312922>
> .
>;;;","09/Jun/14 12:16;github-import;[Date: Sun Jan 26 12:06:16 CET 2014, Author: [uce|https://github.com/uce]]

The helper methods from test. In this case, NepheleConnectedComponentsITCase for the odd/even result checking and stuff.

Sent from my iPhone

> On 26 Jan 2014, at 11:37, Stephan Ewen <notifications@github.com> wrote:
> 
> What classes from stratosphere-tests do you need? The test base is under 
> src/main/java, so it should be packaged and available by default. I am not 
> against a test jar, I am only wondering what I am missing... 
> Am 26.01.2014 01:33 schrieb ""Robert Metzger"" <notifications@github.com>: 
> 
> > No, thats not enough. 
> > The problem is that maven does not package the contents of src/test/javainto the 
> > jar file. 
> > My change builds now two jar-files for the stratosphere-tests project 
> > (one with the tests). 
> > I dont see any disadvantages for my approach. 
> > 
> > — 
> > Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/433#issuecomment-33312922> 
> > . 
> >
> —
> Reply to this email directly or view it on GitHub.;;;","09/Jun/14 12:16;github-import;[Date: Tue Jan 28 19:01:51 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good to me.;;;","09/Jun/14 12:16;github-import;[Date: Fri Feb 07 16:58:53 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

merged in [71a721cabcb7df38054668aca97255e7b545b6ef|https://github.com/stratosphere/stratosphere/commit/71a721cabcb7df38054668aca97255e7b545b6ef] and subsequent commits.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve error message when no match is found in solution set join (issue #429),FLINK-432,12719603,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:16,09/Jun/14 12:16,14/Jul/23 05:57,09/Jun/14 12:16,,,,pre-apache,,,,,,,0,github-import,,"This is a quick fix as @StephanEwen is working on providing the missing functionality (see issue ([#429|https://github.com/stratosphere/stratosphere/issues/429] | [FLINK-429|https://issues.apache.org/jira/browse/FLINK-429])).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/432
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Thu Jan 23 18:28:41 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:16;github-import;pull-request-432-5679457195264512004.patch;https://issues.apache.org/jira/secure/attachment/12649144/pull-request-432-5679457195264512004.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397802,,,Mon Jun 09 12:16:22 UTC 2014,,,,,,,,,,"0|i1whjz:",397929,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:16;github-import;[Date: Fri Jan 24 13:56:19 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thanks. Merged in [0d43d8f8934be9fb6796d24c4029e5d89b4c3ff8|https://github.com/stratosphere/stratosphere/commit/0d43d8f8934be9fb6796d24c4029e5d89b4c3ff8];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add constructors with degree of parallelism argument to Plan (issue #426),FLINK-431,12719602,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:16,09/Jun/14 12:16,14/Jul/23 05:57,09/Jun/14 12:16,,,,pre-apache,,,,,,,0,github-import,,"I added constructors with the degree of parallelism argument to Plan (see issue ([#426|https://github.com/stratosphere/stratosphere/issues/426] | [FLINK-426|https://issues.apache.org/jira/browse/FLINK-426])).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/431
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Thu Jan 23 18:27:19 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:16;github-import;pull-request-431-6293302901344165097.patch;https://issues.apache.org/jira/secure/attachment/12649143/pull-request-431-6293302901344165097.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397801,,,2014-06-09 12:16:15.0,,,,,,,,,,"0|i1whjr:",397928,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cryptic error message when solutionset update fails,FLINK-429,12719600,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:16,09/Jun/14 12:16,14/Jul/23 05:57,09/Jun/14 12:16,,,,pre-apache,,,,,,,0,github-import,,"I got the following error:

    java.lang.RuntimeException: No Match found in solution set.
	at eu.stratosphere.pact.runtime.task.JoinWithSolutionSetCoGroupDriver.run(JoinWithSolutionSetCoGroupDriver.java:178)

It would be very handy for debugging to print the key that was not found in the solution.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/429
Created by: [sscdotopen|https://github.com/sscdotopen]
Labels: 
Created at: Thu Jan 23 15:22:25 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397799,,,2014-06-09 12:16:06.0,,,,,,,,,,"0|i1whjb:",397926,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectedComponents example in Spargel has a non-sensical comment,FLINK-428,12719599,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:16,09/Jun/14 12:16,14/Jul/23 05:57,09/Jun/14 12:16,,,,pre-apache,,,,,,,0,github-import,,"Remove this comment:

    // create DataSinkContract for writing the new cluster positions

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/428
Created by: [sscdotopen|https://github.com/sscdotopen]
Labels: 
Created at: Thu Jan 23 15:20:32 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397798,,,Mon Jun 09 12:16:04 UTC 2014,,,,,,,,,,"0|i1whj3:",397925,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:16;github-import;[Date: Fri Jan 24 01:42:59 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Agree.


On Thu, Jan 23, 2014 at 6:20 AM, sscdotopen <notifications@github.com>wrote:

> Remove this comment:
>
> // create DataSinkContract for writing the new cluster positions
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/428>
> .
>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DecimalTextIntParser has no static method that simply returns an int,FLINK-427,12719598,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:15,09/Jun/14 12:16,14/Jul/23 05:57,09/Jun/14 12:16,,,,pre-apache,,,,,,,0,github-import,,"DecimalTextIntParser has no static method that simply returns an int (in contrast to DecimalTextLongParser). That makes it hard to use.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/427
Created by: [sscdotopen|https://github.com/sscdotopen]
Labels: 
Assignee: [qmlmoon|https://github.com/qmlmoon]
Created at: Thu Jan 23 15:19:37 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397797,,,Mon Jun 09 12:16:00 UTC 2014,,,,,,,,,,"0|i1whiv:",397924,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;[Date: Fri Jan 24 01:38:49 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I agree, we should add this.


On Thu, Jan 23, 2014 at 6:19 AM, sscdotopen <notifications@github.com>wrote:

> DecimalTextIntParser has no static method that simply returns an int (in
> contrast to DecimalTextLongParser). That makes it hard to use.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/427>
> .
>;;;","09/Jun/14 12:16;github-import;[Date: Tue Mar 18 22:25:29 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Some of the other parsers are also lacking a static method.;;;","09/Jun/14 12:16;github-import;[Date: Wed Apr 16 18:46:59 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [a1555ce84ae47edb8cd696f672935cbeffd3481b|https://github.com/stratosphere/stratosphere/commit/a1555ce84ae47edb8cd696f672935cbeffd3481b];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add another constructor to Plan (cosmetic),FLINK-426,12719597,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:15,09/Jun/14 12:15,14/Jul/23 05:57,09/Jun/14 12:15,,,,pre-apache,,,,,,,0,github-import,,"every example has the following code:

    Plan plan = new Plan(result);
    plan.setDefaultParallelism(dop);
    return plan;

a nicer way would be this:

    return new Plan(result, dop);

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/426
Created by: [sscdotopen|https://github.com/sscdotopen]
Labels: 
Created at: Thu Jan 23 15:18:50 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397796,,,Mon Jun 09 12:15:55 UTC 2014,,,,,,,,,,"0|i1whin:",397923,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;[Date: Thu Jan 23 18:42:06 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Very good idea. Would you be able to make a patch?


On Thu, Jan 23, 2014 at 6:18 AM, sscdotopen <notifications@github.com>wrote:

> every example has the following code:
>
> Plan plan = new Plan(result);
> plan.setDefaultParallelism(dop);
> return plan;
>
> a nicer way would be this:
>
> return new Plan(result, dop);
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/426>
> .
>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Iterable instead of Iterator for the messages in Spargel (cosmetic),FLINK-425,12719596,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:15,09/Jun/14 12:15,14/Jul/23 05:57,09/Jun/14 12:15,,,,pre-apache,,,,,,,0,github-import,,"Spargel provides an Iterator over the messages. Iterables are more handy as they allow for loops.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/425
Created by: [sscdotopen|https://github.com/sscdotopen]
Labels: 
Created at: Thu Jan 23 15:17:08 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397795,,,Mon Jun 09 12:15:52 UTC 2014,,,,,,,,,,"0|i1whif:",397922,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;[Date: Thu Jan 23 18:22:18 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Can we do an IterableIterator? One that is both, returning 'this' on the
'getIterator()' call?

Spargel provides an Iterator over the messages. Iterables are more handy as
they allow for loops.

—
Reply to this email directly or view it on
GitHub<https://github.com/stratosphere/stratosphere/issues/425>
.;;;","09/Jun/14 12:15;github-import;[Date: Thu Jan 23 18:24:19 CET 2014, Author: [sscdotopen|https://github.com/sscdotopen]]

I think in our case this should be fine, people would not expect to be able to create two different iterators from the messages.;;;","09/Jun/14 12:15;github-import;[Date: Sat Feb 08 10:07:06 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Fixed with PR https://github.com/stratosphere/stratosphere/pull/433;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop compat,FLINK-424,12719595,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:15,09/Jun/14 12:15,14/Jul/23 05:57,09/Jun/14 12:15,,,,pre-apache,,,,,,,0,github-import,,"Implemented issue ([#382|https://github.com/stratosphere/stratosphere/issues/382] | [FLINK-382|https://issues.apache.org/jira/browse/FLINK-382]).

There is an org.apache package which was needed to extend Task class of hadoop which has package visibility. Any suggestions to solve this?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/424
Created by: [faisalmoeen|https://github.com/faisalmoeen]
Labels: 
Created at: Wed Jan 22 17:08:59 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;pull-request-424-6803255320257378541.patch;https://issues.apache.org/jira/secure/attachment/12649142/pull-request-424-6803255320257378541.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397794,,,Mon Jun 09 12:15:47 UTC 2014,,,,,,,,,,"0|i1whi7:",397921,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;[Date: Wed Jan 22 17:11:14 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Oh, I did not know that the comments will be shown here.;;;","09/Jun/14 12:15;github-import;[Date: Tue Jan 28 09:39:33 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged in [d8fb8706b24c09ea72311ed892b9852c8e85453b|https://github.com/stratosphere/stratosphere/commit/d8fb8706b24c09ea72311ed892b9852c8e85453b] with PR https://github.com/stratosphere/stratosphere/issues/437;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
removeField method in Record class,FLINK-423,12719594,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:15,09/Jun/14 12:15,14/Jul/23 05:57,09/Jun/14 12:15,,,,pre-apache,,,,,,,0,github-import,,"Is there a reason to not have a method that deletes (projects) a field from a Record?

The code of the method has been taken from an older version of Stratosphere, where it was commented out. I just needed to update it, and now it works with the current version.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/423
Created by: [jcamachor|https://github.com/jcamachor]
Labels: 
Created at: Wed Jan 22 15:58:32 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;pull-request-423-6056532832124799675.patch;https://issues.apache.org/jira/secure/attachment/12649141/pull-request-423-6056532832124799675.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397793,,,Mon Jun 09 12:15:41 UTC 2014,,,,,,,,,,"0|i1whhz:",397920,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;[Date: Wed Jan 22 16:25:11 CET 2014, Author: [fhueske|https://github.com/fhueske]]

_If I remember correctly_, the reason for removing the ``removeField()`` method was that is is hard to reason about.
When this method is called, all fields with a higher index are shifted by one which makes it hard to keep track of the field indexes. We mostly set fields that were no longer needed to null (via ``setNull(i)`` or ``setField(i, null)``) which preserves the field indexes.

However, this is just an assumption. There might also be some issues with the function itself.
The Record object is quite complex and I cannot tell if the function is correct right now.
@StephanEwen is the expert for this class ;-)

In the long run, we are moving away from the record-based Java API. It will still be offered and supported, but we are in the progress of drafting a new one which makes the whole thing easier to use.
;;;","09/Jun/14 12:15;github-import;[Date: Thu Jan 23 08:12:48 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think it is fine to re-add this functionality.

We are moving away from the Record, but until then, I see no reason not to
have this functionality.


On Wed, Jan 22, 2014 at 7:25 AM, Fabian Hueske <notifications@github.com>wrote:

> *If I remember correctly*, the reason for removing the removeField()method was that is is hard to reason about.
> When this method is called, all fields with a higher index are shifted by
> one which makes it hard to keep track of the field indexes. We mostly set
> fields that were no longer needed to null (via setNull(i) or setField(i,
> null)) which preserves the field indexes.
>
> However, this is just an assumption. There might also be some issues with
> the function itself.
> The Record object is quite complex and I cannot tell if the function is
> correct right now.
> @StephanEwen <https://github.com/StephanEwen> is the expert for this
> class ;-)
>
> In the long run, we are moving away from the record-based Java API. It
> will still be offered and supported, but we are in the progress of drafting
> a new one which makes the whole thing easier to use.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/423#issuecomment-33032823>
> .
>;;;","09/Jun/14 12:15;github-import;[Date: Fri Jan 24 11:11:23 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I think this is a question of whether the Record API will be supported in the future or whether we will drop it completely. 
If we keep it (some people are using it and would need to do a second port after the refactoring), I would vote to not add the method. I think, we had a good reason to remove it. The weird semantics of shifting indexes is very confusing to users. However, if there is a good reason to include it, I would be fine with adding it.

@jcamachor: Would setting the fields to null work for you as well? The overhead of having sparse records should be negligible, right @StephanEwen ? ;;;","09/Jun/14 12:15;github-import;[Date: Fri Jan 24 11:54:04 CET 2014, Author: [jcamachor|https://github.com/jcamachor]]

> We are moving away from the Record

What do you mean? Will you kill the Record API in future releases? Will you stick to the key-value pairs?

Independently, I think the method could be added to the Record class, so the user is given the **freedom** to use it. You are not enforcing him to use it, and if he does, it is up to him to work properly with it. The name ```removeField``` or ```projectField``` would be already self explanatory. Further, a mention *'Use carefully as this method actually removes a field and shift indexes in a record'* could be added to the method.


> @jcamachor: Would setting the fields to null work for you as well?

Sure, I could do that. But at the PAXQuery algebra level, that's weird semantics, as 'projecting a field' would not change indexes in a record (which is the expected behaviour).;;;","09/Jun/14 12:15;github-import;[Date: Fri Jan 24 14:10:35 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I think we will not kill the record API immediately.
We will develop a second, much improved Java API for Stratosphere. Our internals are flexible enough to provide different APIs at the same time.

> Will you stick to the key-value pairs?
We do not have key-value pairs currently and I think the new API won't have them either.

@jcamachor: Can you write a little comment to the method that warns the users? Can you also uncomment the test-case that belongs to the remove method? (in `eu.stratosphere.types.RecordTest`).

You can change code in a pending pull request by adding comments to the branch the PR is based on.

;;;","09/Jun/14 12:15;github-import;[Date: Fri Jan 24 17:46:45 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The Record API will stay for all the things that were built on top of it.

The rational for the new Java API is to add compile time safety for Java
Programs, less confusing play with positions in the records, and strong
schema for runtime performance. It will be added, it will not be a
replacement.

As Robert mentioned, the architecture has a generic API at the optimizer
level, and then specialized on top of it (Record, Scala, new Java)


On Fri, Jan 24, 2014 at 5:10 AM, Robert Metzger <notifications@github.com>wrote:

> I think we will not kill the record API immediately.
> We will develop a second, much improved Java API for Stratosphere. Our
> internals are flexible enough to provide different APIs at the same time.
>
> Will you stick to the key-value pairs?
> We do not have key-value pairs currently and I think the new API won't
> have them either.
>
>  @jcamachor <https://github.com/jcamachor>: Can you write a little
> comment to the method that warns the users? Can you also uncomment the
> test-case that belongs to the remove method? (in
> eu.stratosphere.types.RecordTest).
>
> You can change code in a pending pull request by adding comments to the
> branch the PR is based on.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/423#issuecomment-33221284>
> .
>;;;","09/Jun/14 12:15;github-import;[Date: Mon Jan 27 10:59:28 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for your contribution. I merged it in [b7217d35ce01a4c4fec4ace8f3155e1a389214c0|https://github.com/stratosphere/stratosphere/commit/b7217d35ce01a4c4fec4ace8f3155e1a389214c0].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting MaximumNumberOfIterations leads to Exception,FLINK-422,12719593,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:15,09/Jun/14 12:15,14/Jul/23 05:57,09/Jun/14 12:15,,,,pre-apache,,,,,,,0,github-import,,"I don't know the maximum number of iterations when creating the plan, so I tried to set the maximum to `Integer.MAX_VALUE` but this leads to 

    Exception in thread ""main"" java.lang.IllegalArgumentException: Cannot subtract more cost then there is.
	at eu.stratosphere.compiler.costs.Costs.subtractCosts(Costs.java:365)
	at eu.stratosphere.compiler.plan.DualInputPlanNode.setCosts(DualInputPlanNode.java:206)
	at eu.stratosphere.compiler.costs.CostEstimator.costOperator(CostEstimator.java:204)
	at eu.stratosphere.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:469)
	at eu.stratosphere.compiler.dag.WorksetIterationNode.instantiate(WorksetIterationNode.java:299)
	at eu.stratosphere.compiler.dag.TwoInputNode.addLocalCandidates(TwoInputNode.java:514)
	at eu.stratosphere.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:444)
	at eu.stratosphere.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:244)
	at eu.stratosphere.compiler.dag.DataSinkNode.getAlternativePlans(DataSinkNode.java:220)
	at eu.stratosphere.compiler.PactCompiler.compile(PactCompiler.java:708)
	at eu.stratosphere.compiler.PactCompiler.compile(PactCompiler.java:543)
	at eu.stratosphere.client.LocalExecutor.executePlan(LocalExecutor.java:94)
	at eu.stratosphere.client.LocalExecutor.execute(LocalExecutor.java:152)
	at eu.stratosphere.test.testPrograms.fullTest.FullTest.main(FullTest.java:71)

I'm using:

    DeltaIteration iteration = new DeltaIteration(0);
    iteration.setMaximumNumberOfIterations(Integer.MAX_VALUE);

Other high numbers such as `Integer.MAX_VALUE / 4` lead to the same result.

Is this a bug?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/422
Created by: [twalthr|https://github.com/twalthr]
Labels: bug, optimizer, 
Created at: Wed Jan 22 11:01:58 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397792,,,Mon Jun 09 12:15:32 UTC 2014,,,,,,,,,,"0|i1whhr:",397919,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;[Date: Wed Jan 22 16:08:41 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Yes, I think that's a bug.
The optimizer weights the cost of operators within iterations by the number of iterations. So, with a very high number of max iteration, that cost becomes huge and will cause the exception.;;;","09/Jun/14 12:15;github-import;[Date: Thu Jan 23 04:28:57 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It seems that the NUM_ITERATIONS * PER_ITERATION_COST leads to a number
overflow. I'll have a look at that.


On Wed, Jan 22, 2014 at 7:08 AM, Fabian Hueske <notifications@github.com>wrote:

> Yes, I think that's a bug.
> The optimizer weights the cost of operators within iterations by the
> number of iterations. So, with a very high number of max iteration, that
> cost becomes huge and will cause the exception.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/422#issuecomment-33031154>
> .
>;;;","09/Jun/14 12:15;github-import;[Date: Thu Feb 06 16:48:05 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

fixed in [5e40545ca23c06d9d1e8700e50ac6b881b4fb549|https://github.com/stratosphere/stratosphere/commit/5e40545ca23c06d9d1e8700e50ac6b881b4fb549];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduced CREATE and OVERWRITE file output write modes.,FLINK-421,12719592,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:15,09/Jun/14 12:15,14/Jul/23 05:57,09/Jun/14 12:15,,,,pre-apache,,,,,,,0,github-import,,"Introduced CREATE and OVERWRITE file output write modes. OVERWRITE is default (as before).
Introduced optional OUTPUT_DIRECTORY parameter to specify write location of output tasks with DOP = 1. 
Rewrote code for file output path preparation.
Added FileOutputFormatTest.
Fixed distributed writing to local filesystem ([#286|https://github.com/stratosphere/stratosphere/issues/286] | [FLINK-286|https://issues.apache.org/jira/browse/FLINK-286])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/421
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Tue Jan 21 23:48:12 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;pull-request-421-8997883249104221532.patch;https://issues.apache.org/jira/secure/attachment/12649140/pull-request-421-8997883249104221532.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397791,,,Mon Jun 09 12:15:27 UTC 2014,,,,,,,,,,"0|i1whhj:",397918,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;[Date: Wed Jan 22 09:28:35 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

@fhueske: we once had the discussion whether it is better to create one output file in the dop=1 case or always use a directory.
How hard would it be to make this configurable (in the FileOutputFormat) ?;;;","09/Jun/14 12:15;github-import;[Date: Wed Jan 22 09:31:25 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Shouldn't be a big deal.;;;","09/Jun/14 12:15;github-import;[Date: Wed Jan 22 15:44:53 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I've added an optional parameter to specify that always a new directory is created (even for DOP = 1).;;;","09/Jun/14 12:15;github-import;[Date: Thu Jan 30 00:55:46 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good. Configurable for overwrite and DOP 1 create file or directory. Nice!

The defaults are overwrite and DOP 1 = directory?;;;","09/Jun/14 12:15;github-import;[Date: Thu Jan 30 08:49:02 CET 2014, Author: [fhueske|https://github.com/fhueske]]

The defaults are as before, i.e., OVERWRITE and directories only for DOP > 1.

I think, CREATE write mode is more conservative and a better default. However, it changes the behavior of all existing jobs and we would need to adapt many tests. Therefore, I kept the defaults as they were before and left this as a future issue / discussion.

 So, are we good to merge?;;;","09/Jun/14 12:15;github-import;[Date: Thu Jan 30 09:37:27 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you add config parameters for the defaults? That would be good. After
that, I think we a re good to merge.


On Thu, Jan 30, 2014 at 8:49 AM, Fabian Hueske <notifications@github.com>wrote:

> The defaults are as before, i.e., OVERWRITE and directories only for DOP 1.
>
> I think, CREATE write mode is more conservative and a better default.
> However, it changes the behavior of all existing jobs and we would need to
> adapt many tests. Therefore, I kept the defaults as they were before and
> left this as a future issue / discussion.
>
> So, are we good to merge?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/421#issuecomment-33666497>
> .
>;;;","09/Jun/14 12:15;github-import;[Date: Thu Jan 30 10:44:59 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Merged PR and opened an issue to add system config parameters (([#449|https://github.com/stratosphere/stratosphere/issues/449] | [FLINK-449|https://issues.apache.org/jira/browse/FLINK-449])).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed ExecutionGraphTest. ,FLINK-420,12719591,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:15,09/Jun/14 12:15,14/Jul/23 05:57,09/Jun/14 12:15,,,,pre-apache,,,,,,,0,github-import,,"Output directories of tests are removed. 
Reduced log level to WARN.

Fixes ([#419|https://github.com/stratosphere/stratosphere/issues/419] | [FLINK-419|https://issues.apache.org/jira/browse/FLINK-419]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/420
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Tue Jan 21 20:12:04 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;pull-request-420-232578361919924290.patch;https://issues.apache.org/jira/secure/attachment/12649139/pull-request-420-232578361919924290.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397790,,,Mon Jun 09 12:15:20 UTC 2014,,,,,,,,,,"0|i1whhb:",397917,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;[Date: Wed Jan 22 08:16:12 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Good to merge.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added broadcast variables support to the runtime.,FLINK-418,12719589,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:15,09/Jun/14 12:15,14/Jul/23 05:57,09/Jun/14 12:15,,,,pre-apache,,,,,,,0,github-import,,"This is a first step towards realizing the functionality requested in issue ([#66|https://github.com/stratosphere/stratosphere/issues/66] | [FLINK-66|https://issues.apache.org/jira/browse/FLINK-66]) in the runtime, building upon the API proposed in PR ([#383|https://github.com/stratosphere/stratosphere/issues/383] | [FLINK-383|https://issues.apache.org/jira/browse/FLINK-383]).

Broadcast variables are realized as additional outputs attached to a `RegularPactTask`. Convenience methods for accessing the configuration information for broadcast input setup was added to the `TaskConfig`.

An integrated test case for with a simple hardwired JobGraph can be found in `BoradcastVarsNepheleITCase`.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/418
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Created at: Tue Jan 21 00:28:55 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;pull-request-418-884682649629480737.patch;https://issues.apache.org/jira/secure/attachment/12649138/pull-request-418-884682649629480737.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397788,,,Mon Jun 09 12:15:11 UTC 2014,,,,,,,,,,"0|i1whgv:",397915,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:15;github-import;[Date: Tue Jan 21 00:44:18 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

I would like to get feedback on the proposed changes to the runtime from @StephanEwen. 

In particular, I wasn't able to decouple the modifications in the `RegularPactTask` from concrete implementation of the record model (see the comment on line 412 in the modified `RegularPactTask`).

However, since `Record` is used as a concrete type parameter in other places of the `RegularPactTask`. I guess that the issue runs deeper and the decoupling could be done as a refactoring at once later.;;;","09/Jun/14 12:15;github-import;[Date: Tue Jan 28 18:45:06 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

OK ignore the last comment I was able to resolve the issue with the hard-coded `Record` after a chat with @StephanEwen. The fix is up in an ammended commit.;;;","09/Jun/14 12:15;github-import;[Date: Fri Feb 07 10:03:24 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Redundant with https://github.com/stratosphere/stratosphere/pull/460;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changed CsvOutputFormat constructor to support additional arguments,FLINK-417,12719588,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:14,09/Jun/14 12:15,14/Jul/23 05:57,09/Jun/14 12:15,,,,pre-apache,,,,,,,0,github-import,,"I changed the CsvOutputFormat constructor to support non-generic instantiation.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/417
Created by: [skunert|https://github.com/skunert]
Labels: bug, 
Created at: Mon Jan 20 17:27:51 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:14;github-import;pull-request-417-8588368571338075690.patch;https://issues.apache.org/jira/secure/attachment/12649137/pull-request-417-8588368571338075690.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397787,,,Mon Jun 09 12:15:03 UTC 2014,,,,,,,,,,"0|i1whgn:",397914,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:14;github-import;[Date: Mon Jan 20 17:33:13 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

This pull-request implements https://github.com/stratosphere/stratosphere/issues/414;;;","09/Jun/14 12:14;github-import;[Date: Wed Jan 29 19:21:45 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good in most parts, I'll give some comments inline.

Also, please reformat the code for a wider line-width ( >= 120 ). We are in the era of larger screens and higher resolutions ;-) Most classes assume 120 chard line width.

;;;","09/Jun/14 12:14;github-import;[Date: Wed Jan 29 19:32:32 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Please avoid reformatting source. It makes the diffs impossible to read. Right now, I cannot really figure out where you really changed something and where the formatter changed something.;;;","09/Jun/14 12:15;github-import;[Date: Wed Jan 29 19:35:50 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It would also be nice to add a setter for the types, as another way to set the types in addition to the constructor.;;;","09/Jun/14 12:15;github-import;[Date: Mon Feb 03 13:17:36 CET 2014, Author: [skunert|https://github.com/skunert]]

I think the issues you mentioned are fixed.;;;","09/Jun/14 12:15;github-import;[Date: Fri Feb 07 17:20:40 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I merged together the 7 commits into one and removed some empty lines.
Merged in [8344bbcce3bb1b524e75e22def90a4be30083a25|https://github.com/stratosphere/stratosphere/commit/8344bbcce3bb1b524e75e22def90a4be30083a25];;;","09/Jun/14 12:15;github-import;[Date: Sat Feb 08 10:08:19 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I merged this pull request. But I did not see that Travis did not test the most recent commits.
Sadly, the CSVOutputFormat tests are failing!

The problem is that the OF does not support multiple `configure()` calls.
But I don't really get why we want to throw an exception if users call configure after a ctor-based initialization.;;;","09/Jun/14 12:15;github-import;[Date: Sat Feb 08 11:30:56 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Fixed in [db40aa164112b2c4de5867dcb4adb8baca9561a8|https://github.com/stratosphere/stratosphere/commit/db40aa164112b2c4de5867dcb4adb8baca9561a8] @skunert and @StephanEwen please review.;;;","09/Jun/14 12:15;github-import;[Date: Tue Feb 11 18:00:40 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

It seems that this does not work in a cluster
```
ERROR: The job was not successfully submitted to the nephele job manager: eu.stratosphere.nephele.executiongraph.GraphConversionException: java.lang.RuntimeException: The user defined 'configure()' method in the Output Format caused an error: Invalid configuration for CsvOutputFormat: Need to specify number of fields > 0.
	at eu.stratosphere.pact.runtime.task.DataSinkTask.initOutputFormat(DataSinkTask.java:300)
```;;;","09/Jun/14 12:15;github-import;[Date: Tue Feb 11 18:04:44 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay. Its is working. I have some versions-mixup in my cluster.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing Testcases in 0.4 release,FLINK-416,12719587,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:14,09/Jun/14 12:14,14/Jul/23 05:57,09/Jun/14 12:14,,,,pre-apache,,,,,,,0,github-import,,"Hello,
a Stratosphere user send me an email regarding two failing test cases in the current 0.4 stable release.


https://gist.github.com/rmetzger/8521843
https://gist.github.com/rmetzger/8521862

It seems to be an issue with the Scala interface?


Some more information

 * openSuse 11.4
Tested on three different computers 
* Computer 1, User 1: works.
* Computer 1, User 2: fails.
* Computer 2, User 2: fails.
Seems to be an issue with the user-environment or so?

Its the release-0.4 branch and the error occurs with mvn clean verify.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/416
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, scala api, user satisfaction, 
Created at: Mon Jan 20 16:30:50 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397786,,,Mon Jun 09 12:14:51 UTC 2014,,,,,,,,,,"0|i1whgf:",397913,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:14;github-import;[Date: Mon Jan 20 20:31:30 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Hmmm... And this did not cause the build to fail? Did we configure maven
incorrectly?


On Mon, Jan 20, 2014 at 10:30 AM, Robert Metzger
<notifications@github.com>wrote:

> Hello,
> a Stratosphere user send me an email regarding two failing test cases in
> the current 0.4 stable release.
>
> https://gist.github.com/rmetzger/8521843
> https://gist.github.com/rmetzger/8521862
>
> It seems to be an issue with the Scala interface?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/416>
> .
>;;;","09/Jun/14 12:14;github-import;[Date: Mon Jan 20 20:38:20 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

It seems that there is an error with the input parsing or so. It looks like part of the exception is missing.;;;","09/Jun/14 12:14;github-import;[Date: Mon Jan 20 20:45:37 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay. Is anyone onto the bug already?


On Mon, Jan 20, 2014 at 2:38 PM, Robert Metzger <notifications@github.com>wrote:

> It seems that there is an error with the input parsing or so. It looks
> like part of the exception is missing.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/416#issuecomment-32790075>
> .
>;;;","09/Jun/14 12:14;github-import;[Date: Mon Jan 20 21:07:44 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I'm not sure if it is a local issue of the user. I asked for the full stack trace. (The user is from HU-Berlin);;;","09/Jun/14 12:14;github-import;[Date: Mon Jan 20 21:23:13 CET 2014, Author: [aljoscha|https://github.com/aljoscha]]

Sorry, was a bit busy today...
Did anyone notice that in the first gist the problem is with the decimal point:
```
expected:<5|0|147828[.]97> but was:<5|0|147828[,]97>
```
Seems to be that his system is on german locale...
;;;","09/Jun/14 12:14;github-import;[Date: Mon Jan 20 22:54:16 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Ah, ja, the java parsers are sensitive to that, good catch!

Can we make the locale configurable and always use en-us unless something
else is specified?


On Mon, Jan 20, 2014 at 3:23 PM, Aljoscha Krettek
<notifications@github.com>wrote:

> Sorry, was a bit busy today...
> Did anyone notice that in the first gist the problem is with the decimal
> point:
> expected: but was:
> Seems to be that his system is on german locale...
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/416#issuecomment-32793784>
> .
>;;;","09/Jun/14 12:14;github-import;[Date: Tue Jan 21 11:09:58 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay, the user confirmed the bug by changing the locale for user 2 from german to english.

We have to make sure that we control the locale in the settings.
Which parser exactly is parsing the strings there?;;;","09/Jun/14 12:14;github-import;[Date: Tue Jan 21 21:04:29 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

It is the DecimalTextDoubleParser in the stratosphere-core project. Same
issue will also hold for the DecimalTextFloatParser.


On Tue, Jan 21, 2014 at 2:09 AM, Robert Metzger <notifications@github.com>wrote:

> Okay, the user confirmed the bug by changing the locale for user 2 from
> german to english.
>
> We have to make sure that we control the locale in the settings.
> Which parser exactly is parsing the strings there?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/416#issuecomment-32835327>
> .
>;;;","09/Jun/14 12:14;github-import;[Date: Thu Jan 23 11:08:57 CET 2014, Author: [aljoscha|https://github.com/aljoscha]]

It is not because of the parsers. The problem is simply, that the scala job writes the output records using:
```scala
""%d|%d|%.2f"".format(item.orderId, item.shipPriority, item.revenue)
```
Where the format is the Java String.format() method that uses the default Locale on the system. In TPCHQuery3 we have harcoded values that have a dot, not a comma, as the ""decimal point"".

The java TPCHQuery3 uses CsvOutputFormat which simply does Float.toString() which, I'm guessing here, always uses a dot as a ""decimal point"".

The scala behaviour seems to be correct, but it's harder to test this....

What do you think? I could change the scala jobs to also use the CsvOutputFormat.;;;","09/Jun/14 12:14;github-import;[Date: Wed Feb 05 15:28:09 CET 2014, Author: [aljoscha|https://github.com/aljoscha]]

If more cases surface I will force a locale change there as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added reflective Avro type,FLINK-415,12719586,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:14,09/Jun/14 12:14,14/Jul/23 05:57,09/Jun/14 12:14,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/415
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Mon Jan 20 13:36:07 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:14;github-import;pull-request-415-7179845502666084181.patch;https://issues.apache.org/jira/secure/attachment/12649136/pull-request-415-7179845502666084181.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397785,,,Mon Jun 09 12:14:43 UTC 2014,,,,,,,,,,"0|i1whg7:",397912,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:14;github-import;[Date: Tue Jan 21 19:06:44 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Yes, I agree, SAvroValue is not too nice. How about

  - AvroValueBase
  - AvroValueHolder;;;","09/Jun/14 12:14;github-import;[Date: Tue Jan 21 20:40:01 CET 2014, Author: [uce|https://github.com/uce]]

AvroBaseValue to keep the Value suffix?

Sent from my iPhone

> On 21 Jan 2014, at 19:06, Stephan Ewen <notifications@github.com> wrote:
> 
> Yes, I agree, SAvroValue is not too nice. How about
> 
> AvroValueBase
> AvroValueHolder
> —
> Reply to this email directly or view it on GitHub.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set individual descriptions and keywords for the most important pages.,FLINK-413,12719584,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:14,09/Jun/14 12:14,14/Jul/23 05:57,09/Jun/14 12:14,,,,pre-apache,,,,,,,0,github-import,,"Extracted static meta description and keywords from layout.
Set individual descriptions and keywords for the most important pages.

Search engines (at least Google) penalize identical description and keyword meta data on multiple pages.
This should give us a better search result entries (title + description) and hopefully also better keyword coverage.

This PR updates PR ([#412|https://github.com/stratosphere/stratosphere/issues/412] | [FLINK-412|https://issues.apache.org/jira/browse/FLINK-412]) addresses issue ([#411|https://github.com/stratosphere/stratosphere/issues/411] | [FLINK-411|https://issues.apache.org/jira/browse/FLINK-411])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/413
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Sun Jan 19 14:45:04 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:14;github-import;pull-request-413-3541737086327195010.patch;https://issues.apache.org/jira/secure/attachment/12649135/pull-request-413-3541737086327195010.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397783,,,Mon Jun 09 12:14:26 UTC 2014,,,,,,,,,,"0|i1whfr:",397910,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:14;github-import;[Date: Sun Jan 19 14:46:25 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hey @fhueske, you can change pending pull requests by (force-)pushing into your branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some search engine optimization for the website.,FLINK-412,12719583,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:14,09/Jun/14 12:14,14/Jul/23 05:57,09/Jun/14 12:14,,,,pre-apache,,,,,,,0,github-import,,"Updated title of start page from ""Stratosphere > Overview"" to ""Stratosphere > Next-Generation Big Data Analytics"".

Extracted static meta description and keywords from layout. 
Set individual descriptions and keywords for the most important pages.

Search engines (at least Google) penalize identical description and keyword meta data on multiple pages.

This should give us a better search result entries (title + description) and hopefully also better keyword coverage.

This PR resolves issue ([#411|https://github.com/stratosphere/stratosphere/issues/411] | [FLINK-411|https://issues.apache.org/jira/browse/FLINK-411]) 

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/412
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Sun Jan 19 14:02:08 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:14;github-import;pull-request-412-5659331107306974373.patch;https://issues.apache.org/jira/secure/attachment/12649134/pull-request-412-5659331107306974373.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397782,,,Mon Jun 09 12:14:22 UTC 2014,,,,,,,,,,"0|i1whfj:",397909,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:14;github-import;[Date: Sun Jan 19 14:03:24 CET 2014, Author: [uce|https://github.com/uce]]

See ([#411|https://github.com/stratosphere/stratosphere/issues/411] | [FLINK-411|https://issues.apache.org/jira/browse/FLINK-411]).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create 0.5 documentation on website,FLINK-410,12719581,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:14,09/Jun/14 12:14,14/Jul/23 05:57,09/Jun/14 12:14,,,,pre-apache,,,,,,,0,github-import,,"We need to start documenting new features such as the JDBC Output format, the CollectionDataSource.

The 0.5 documentation should not be linked by default.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/410
Created by: [rmetzger|https://github.com/rmetzger]
Labels: documentation, website, 
Milestone: Release 0.5
Assignee: [uce|https://github.com/uce]
Created at: Sat Jan 18 12:41:47 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397780,,,Mon Jun 09 12:14:12 UTC 2014,,,,,,,,,,"0|i1whf3:",397907,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:14;github-import;[Date: Sun Jan 19 13:18:35 CET 2014, Author: [uce|https://github.com/uce]]

:+1: I think this is especially important in order to *not* end up with a stop-the-world-documentation-week right before the release.

I would propose to just list the 0.5-snapshort-specific new features and not do a copy-0.4-and-add-stuff thing, because this will be hard to keep in sync with our static documentation setup.;;;","09/Jun/14 12:14;github-import;[Date: Sun Jan 19 13:22:15 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I agree, maintaining two documentations at the same time will be horrible.
Okay, how about adding **one** page in the Documentation dropdown which says ""0.5-SNAPSHOT"".


The page contains ""snippets"" of documentation for features in development (such as JDBCOutputFormator CollectionDataSource).
Once 0.5 release is ready, we'll copy these snippets into the 0.4 docu and make it the new 0.5 documentation.
Agree?;;;","09/Jun/14 12:14;github-import;[Date: Sun Jan 19 13:22:49 CET 2014, Author: [uce|https://github.com/uce]]

Yes, this is exactly what I meant. :+1: :laughing: ;;;","09/Jun/14 12:14;github-import;[Date: Sun Jan 19 14:48:44 CET 2014, Author: [uce|https://github.com/uce]]

You can [check it out|http://stratosphere.eu/docs/0.5/] online. It's only accessible via the drop down and the default documentation link still goes to the 0.4 docs. I think this is OK for now. We should add the content asap though.;;;","09/Jun/14 12:14;github-import;[Date: Sun Jan 19 14:51:12 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Perfect! Thank you.
We'll instruct the authors of new features to write documentation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bug in scala cogroup.flatMap,FLINK-409,12719580,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:14,09/Jun/14 12:14,14/Jul/23 05:57,09/Jun/14 12:14,,,,pre-apache,,,,,,,0,github-import,,"Did fail with null pointer exception when one of the inputs is empty.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/409
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri Jan 17 15:36:02 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:14;github-import;pull-request-409-4705485794186523244.patch;https://issues.apache.org/jira/secure/attachment/12649133/pull-request-409-4705485794186523244.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397779,,,Mon Jun 09 12:14:06 UTC 2014,,,,,,,,,,"0|i1whev:",397906,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:14;github-import;[Date: Fri Jan 17 15:39:56 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thanks for the fix. Commited in [418fb21ff886cd4be782c2499e7548c0303baad6|https://github.com/stratosphere/stratosphere/commit/418fb21ff886cd4be782c2499e7548c0303baad6];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reenable ClosureCleaner and add new Test case for everything,FLINK-408,12719579,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:13,09/Jun/14 12:14,14/Jul/23 05:57,09/Jun/14 12:14,,,,pre-apache,,,,,,,0,github-import,,"Closure cleaner did not work before because the user code class loader
was not used when it was trying to load inner anonymous classes.

The new test checks whether the ClosureCleaner works together with the
user code class loader.

It also used the local distributed executor and remote executor.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/408
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri Jan 17 15:33:47 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:13;github-import;pull-request-408-7789703234518255898.patch;https://issues.apache.org/jira/secure/attachment/12649132/pull-request-408-7789703234518255898.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397778,,,Mon Jun 09 12:14:01 UTC 2014,,,,,,,,,,"0|i1when:",397905,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:13;github-import;[Date: Fri Jan 17 15:41:49 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Do we really want a test called `UserCodeClassLoaderRemoteExecutorLocalDistributedExecutorClosureCleanerTest` in our code ?;;;","09/Jun/14 12:13;github-import;[Date: Sun Jan 19 21:09:41 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good to me, besides the name ;-);;;","09/Jun/14 12:14;github-import;[Date: Sun Jan 19 21:42:47 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Travis indicates that there is something wrong


Sent from my iPhone

On 19.01.2014, at 21:09, Stephan Ewen <notifications@github.com> wrote:

Looks good to me, besides the name ;-)

—
Reply to this email directly or view it on
GitHub<https://github.com/stratosphere/stratosphere/pull/408#issuecomment-32718391>
.;;;","09/Jun/14 12:14;github-import;[Date: Sun Jan 19 21:48:46 CET 2014, Author: [uce|https://github.com/uce]]

We also have to make sure that it is not a problem with the `LocalDistributedExecutor`. Actually, I've waited to integrate `TestBase2` with the LDE in order to make sure that everything is fine with this test case.

@aljoscha Do you have any ideas why the test case fails? Is it because of LDE?;;;","09/Jun/14 12:14;github-import;[Date: Tue Jan 21 12:08:33 CET 2014, Author: [aljoscha|https://github.com/aljoscha]]

Stupid me, I forgot to add the jar file to the repo...
I also changed to name per @StephanEwen's suggestion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add shutdown method to LocalDistributedExecutor,FLINK-407,12719578,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:13,09/Jun/14 12:13,14/Jul/23 05:57,09/Jun/14 12:13,,,,pre-apache,,,,,,,0,github-import,,"This PR adds a shutdown method to the `LocalDistributedExecutor`.

While reworking the corresponding test case I noticed a problem with `CustomDataTypeTest` (see ([#404|https://github.com/stratosphere/stratosphere/issues/404] | [FLINK-404|https://issues.apache.org/jira/browse/FLINK-404])). As @rmetzger noted, a JAR file has been accidently deleted in [33cb2ca9898809d2fc90765996ea56bbea458e59|https://github.com/stratosphere/stratosphere/commit/33cb2ca9898809d2fc90765996ea56bbea458e59]. Because @aljoscha is going to implement a similar test case, we voted to remove `CustomDataTypeTest`.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/407
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Fri Jan 17 12:13:35 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:13;github-import;pull-request-407-4236740510723051898.patch;https://issues.apache.org/jira/secure/attachment/12649131/pull-request-407-4236740510723051898.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397777,,,Mon Jun 09 12:13:50 UTC 2014,,,,,,,,,,"0|i1whef:",397904,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:13;github-import;[Date: Fri Jan 17 12:14:32 CET 2014, Author: [uce|https://github.com/uce]]

BTW: I will add a switch to `TestBase2` to allow using `LocalDistributedExecutor` for test cases (will be off by default).;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 15:03:12 CET 2014, Author: [aljoscha|https://github.com/aljoscha]]

I like :thumbsup: ;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 15:14:39 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged in [bdd378c8b754d05a32ad260ada6843c104ffbf2a|https://github.com/stratosphere/stratosphere/commit/bdd378c8b754d05a32ad260ada6843c104ffbf2a]. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updated Examples,FLINK-406,12719577,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:13,09/Jun/14 12:13,14/Jul/23 05:57,09/Jun/14 12:13,,,,pre-apache,,,,,,,0,github-import,,"Here is a suggestion for the examples section. I think we can put that online soon and extend it bit by bit.

---
layout: inner_docs_v04
title:  ""Example Programs""
sublinks:
  - {anchor: ""wordcount_scala"", title: ""Word Count (Scala)""}
  - {anchor: ""page_rank_scala"", title: ""Page Rank (Scala)""}
  - {anchor: ""connected_components_scala"", title: ""Connected Components (Scala)""}
  - {anchor: ""relational_scala"", title: ""Relational Query (Scala)""}
  - {anchor: ""wordcount_java"", title: ""Word Count (Java)""}
  - {anchor: ""connected_components_java"", title: ""Connected Components (Java)""}
  - {anchor: ""relational_java"", title: ""Relational Query (Java)""}
  - {anchor: ""test_data"", title: ""Test Data""}
---

## Example Programs

<p class=""lead"">The following example programs showcase different applications of Stratosphere from simple word counting to graph algorithms. The code samples are simplified and intended to illustrate the principles of writing Stratosphere programs. The full source code of all examples is in the `stratosphere-java-examples` and `stratosphere-scala-examples` module
([https://github.com/stratosphere/stratosphere/tree/release-{{site.current_stable}}/stratosphere-examples/stratosphere-java-examples|https://github.com/stratosphere/stratosphere/tree/release-{{site.current_stable}}/stratosphere-examples/stratosphere-java-examples)] and ([https://github.com/stratosphere/stratosphere/tree/release-{{site.current_stable}}/stratosphere-examples/stratosphere-scala-examples|https://github.com/stratosphere/stratosphere/tree/release-{{site.current_stable}}/stratosphere-examples/stratosphere-scala-examples)).


## Scala API

<section id=""wordcount_scala"">
<div class=""page-header""><h2>Word Count (Scala)</h2></div>

Counting words in a collection of text documents is a simple two step algorithm: First, the texts are tokenized to individual words (optionally stemming/normalizing the words). Second, the collection of words is grouped and counted.

```scala
val input = TextFile(textInput)

val words = input.flatMap { _.split("" "") map { (_, 1) } }

val counts = words.groupBy { case (word, _) => word }
  .reduce { (w1, w2) => (w1._1, w1._2 + w2._2) }

val output = counts.write(wordsOutput, CsvOutputFormat()))
```

The [WordCount example (Scala]](https://github.com/stratosphere/stratosphere/blob/release-{{site.current_stable}}/stratosphere-examples/stratosphere-scala-examples/src/main/scala/eu/stratosphere/example/scala/wordcount/WordCount.scala) implements the above described algorithm with input parameters `degree-of-parallelism`, `inputPath`, and `outputPath`. As test data, any text file will do.

</section>


<section id=""page_rank_scala"">
<div class=""page-header""><h2>Page Rank (Scala)</h2></div>

PageRank is an iterative algorithm, which means that it applies the same computation repeatedly. Each time, all pages distribute their current rank over their neighbors, and compute a new rank as a taxed sum of those ranks they received from the neighbors.

```scala
// cases classes so we have named fields
case class PageWithRank(pageId: Long, rank: Double)
case class Edge(from: Long, to: Long, transitionProbability: Double)

// constants for the page rank formula
val dampening = 0.85
val randomJump = (1.0 - dampening) / NUM_VERTICES
val initialRank = 1.0 / NUM_VERTICES
  
// read inputs
val pages = DataSource(verticesPath, CsvInputFormat[Long|)]
val edges = DataSource(edgesPath, CsvInputFormat[Edge|))

// assign initial rank
val pagesWithRank = pages map { p => PageWithRank(p, initialRank] }

// the iterative compüutation
def computeRank(ranks: DataSet[PageWithRank]) = {

    // send rank to neighbors
    val ranksForNeighbors = ranks join edges where { _.pageId } isEqualTo { _.from } map { (p, e) => (e.to, p.rank * e.transitionProbability) }
    
    // gather ranks per vertex and apply page rank formula
    ranksForNeighbors .groupBy { case (node, rank) => node }
                      .reduce { (a, b) => (a._1, a._2 + b._2) }
                      .map {case (node, rank) => PageWithRank(node, rank * dampening + randomJump) }
}

// invoke iteratively
val finalRanks = pagesWithRank.iterate(numIterations, computeRank)
val output = finalRanks.write(outputPath, CsvOutputFormat())
```

Note that because programs are optimized by the Stratosphere compiler, no manual caching of invariant data sets is necessary; this happens automatically.
</section>


<section id=""connected_component_scala"">
<div class=""page-header""><h2>Connected Components (Scala)</h2></div>

This algorithm computes connected components in a graph by assigning all vertices in the same component the same label. In each step, the vertices propagate their current lable to their neighbors. A vertex accepts the laber from the neighbor, if it is smaller than its own label. The was originally suggested by (pegasus link).

This algorithm uses a delta iteration: Vertices that have not changed their component to not participate in the next step. This yields much better performance, because the later iterations typically deal only with a few outlier vertices.

```scala
case class VertexWithComponent(vertex: Long, componentId: Long)
case class Edge(from: Long, to: Long)
  
val vertices = DataSource(verticesPath, CsvInputFormat[Long|)]
val directedEdges = DataSource(edgesPath, CsvInputFormat[Edge|))

val initialComponents = vertices map { v => VertexWithComponent(v, v) }
val undirectedEdges = directedEdges flatMap { e => Seq(e, Edge(e.to, e.from)] }

def propagateComponent(s: DataSet[VertexWithComponent], ws: DataSet[VertexWithComponent]) = {

  val allNeighbors = ws join undirectedEdges where { _.vertex } isEqualTo { _.from } map { (v, e) => VertexWithComponent(e.to, v.componentId )}
    
    val minNeighbors = allNeighbors groupBy { _.vertex } reduceGroup { cs => cs minBy { _.componentId } }

    // updated solution elements == new workset
    val s1 = s join minNeighbors where { _.vertex } isEqualTo { _.vertex } flatMap { (curr, candidate) =>
      if (candidate.componentId < curr.componentId) Some(candidate) else None
    }

  (s1, s1)
}

  val components = initialComponents.iterateWithDelta(initialComponents, { _.vertex }, propagateComponent, maxIterations)
  val output = components.write(componentsOutput, CsvOutputFormat())
```

The [ConnectedComponents example (Scala)|https://github.com/stratosphere/stratosphere/blob/release-{{site.current_stable}}/stratosphere-examples/stratosphere-scala-examples/src/main/scala/eu/stratosphere/example/scala/grap/ConnectecComponents.scala) implements the above example.
</section>


<section id=""relational_scala"">
<div class=""page-header""><h2>Relational Query (Scala)</h2></div>

The examples below assume two tables, one with `orders`, one with the `lineitems` per order. The programs execute functionality resembling the following SQL statement, as inspired by the TPC-H decision support benchmark:
```
  SELECT l_orderkey, o_shippriority, sum(l_extendedprice) as revenue
    FROM orders, lineitem
   WHERE l_orderkey = o_orderkey
     AND o_orderstatus = ""F"" 
     AND YEAR(o_orderdate) > 1993
     AND o_orderpriority LIKE ""5%""
GROUP BY l_orderkey, o_shippriority;
```

```scala
// --- define some custom classes to address fields by name ---
case class Order(orderId: Int, status: Char, date: String, orderPriority: String, shipPriority: Int)
case class LineItem(orderId: Int, extendedPrice: Double)
case class PrioritizedOrder(orderId: Int, shipPriority: Int, revenue: Double)

val orders = DataSource(ordersInputPath, DelimitedInputFormat(parseOrder))
val lineItems = DataSource(lineItemsInput, DelimitedInputFormat(parseLineItem))

val filteredOrders = orders filter { o => o.status == ""F"" && o.date.substring(0, 4).toInt > 1993 && o.orderPriority.startsWith(""5"") }

val prioritizedItems = filteredOrders join lineItems where { _.orderId } isEqualTo { _.orderId } // join on the orderIds
                        map { (o, li) => PrioritizedOrder(o.orderId, o.shipPriority, li.extendedPrice) }

val prioritizedOrders = prioritizedItems groupBy { pi => (pi.orderId, pi.shipPriority) } 
                                          reduce { (po1, po2) => po1.copy(revenue = po1.revenue + po2.revenue) }

val output = prioritizedOrders.write(ordersOutput, CsvOutputFormat(formatOutput)]
```

The source code of this example can be found [here|https://github.com/stratosphere/stratosphere/blob/release-{{site.current_stable}}/stratosphere-examples/stratosphere-scala-examples/src/main/scala/eu/stratosphere/examples/scala/relational/TPCHQuery3.scala).


## Java API:

<section id=""wordcount_java"">
<div class=""page-header""><h2>Word Count (Java)</h2></div>

Counting words in a collection of text documents is a simple two step algorithm: First, the texts are tokenized to individual words (optionally stemming/normalizing the words). Second, the collection of words is grouped and counted.

```java
// --- tokenizer function ---
public class TokenizeLine extends MapFunction {
    public void map(Record record, Collector<Record> collector) {
        // get the first field (as type StringValue) from the record
        String line = record.getField(0, StringValue.class).getValue();

        // split and emit (word, 1) pairs
        for (String word : line.split("" "")) {
            collector.collect(new Record(new StringValue(word), new IntValue(1)));
        }
    }
}

// --- counting function ---
@Combinable
public static class CountWords extends ReduceFunction {

    public void reduce(Iterator<Record> records, Collector<Record> out) throws Exception {
        Record element = null;
        int sum = 0;
        while (records.hasNext()) {
            element = records.next();
            int cnt = element.getField(1, IntValue.class).getValue();
            sum += cnt;
        }

        element.setField(1, new IntValue(sum));
        out.collect(element);
    }
}

// --- program assembly ---

FileDataSource source = new FileDataSource(new TextInputFormat(), inputPath, ""Input Lines"");

MapOperator mapper = MapOperator.builder(TokenizeLine.class)
                    .input(source).build();

ReduceOperator reducer = ReduceOperator.builder(CountWords.class, 
                    StringValue.class, 0)    // group on field 0 which is a string
                    .input(mapper).build();

FileDataSink out = new FileDataSink(new CsvOutputFormat(), outputPath, reducer];
```

The [wordcount example|https://github.com/stratosphere/stratosphere/blob/release-{{site.current_stable}}/stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/record/wordcount/WordCount.java) implements the above described algorithm with input parameters `degree-of-parallelism`, `inputPath`, and `outputPath`. As test data, any text file will do.
</section>


<section id=""connected_component_java"">
<div class=""page-header""><h2>Connected Components (Java)</h2></div>

Please refer to the Scala version of the ConnectedComponents example for an introduction to the algorithm.
```java
// --- plan assembly ---
FileDataSource initialVertices = new FileDataSource(new CsvInputFormat(' ', LongValue.class), verticesInput, ""Vertices"");

// assign the initial id
MapOperator verticesWithId = MapOperator.builder(AssignInitialId.class).input(initialVertices).name(""Assign Vertex Ids"").build();

// the loop takes the vertices as the solution set and changed vertices as the workset.
// the vertices are identified (and replaced) by their vertex id (field 0)
// initially, all vertices are changed. 
DeltaIteration iteration = new DeltaIteration(0, ""Connected Components Iteration"");
iteration.setInitialSolutionSet(verticesWithId);
iteration.setInitialWorkset(verticesWithId);
iteration.setMaximumNumberOfIterations(MAX_NUM_ITERATIONS); // a guard

// data source for the edges
FileDataSource edges = new FileDataSource(new CsvInputFormat(' ', LongValue.class, LongValue.class), edgeInput, ""Edges"");

// join workset (changed vertices) with the edges to propagate changes to neighbors
JoinOperator joinWithNeighbors = JoinOperator.builder(NeighborWithComponentIDJoin.class, LongValue.class, 0, 0)
        .input1(iteration.getWorkset())
        .input2(edges)
        .name(""Join Candidate Id With Neighbor"").build();

// find for each neighbor the smallest of all candidates
ReduceOperator minCandidateId = ReduceOperator.builder(new MinimumComponentIDReduce(), LongValue.class, 0)
        .input(joinWithNeighbors)
        .name(""Find Minimum Candidate Id"").build();

// join candidates with the solution set and update if the candidate component-id is smaller
JoinOperator updateComponentId = JoinOperator.builder(UpdateComponentIdJoin.class, LongValue.class, 0, 0)
        .input1(minCandidateId)
        .input2(iteration.getSolutionSet())
        .name(""Update Component Id"").build();

// the result from the join (which checked whether a vertex really changed) is the delta
// and the driving data for the next round
iteration.setNextWorkset(updateComponentId);
iteration.setSolutionSetDelta(updateComponentId);

// sink is the iteration result
FileDataSink result = new FileDataSink(new CsvOutputFormat(), output, iteration, ""Result"");
CsvOutputFormat.configureRecordFormat(result)
        .fieldDelimiter(' ')
        .field(LongValue.class, 0)
        .field(LongValue.class, 1);

// --- the individual functions ---

public class AssignInitialId extends MapFunction {

    public void map(Record record, Collector<Record> out) throws Exception {
        record.setField(1, record.getField(0, LongValue.class));    // give vertex id as initial id
        out.collect(record);
    }
}

public class NeighborWithComponentIDJoin extends JoinFunction {

    public void join(Record vertexWithComponent, Record edge, Collector<Record> out) {
        this.result.setField(0, edge.getField(1, LongValue.class));
        this.result.setField(1, vertexWithComponent.getField(1, LongValue.class));
        out.collect(this.result);
    }
}

@Combinable
@ConstantFields(0)
public class MinimumComponentIDReduce extends ReduceFunction {

    public void reduce(Iterator<Record> records, Collector<Record> out) {
        Record rec = null;
        long minimumComponentID = Long.MAX_VALUE;

        while (records.hasNext()) {
            rec = records.next();
            long candidateComponentID = rec.getField(1, LongValue.class).getValue();
            if (candidateComponentID < minimumComponentID)
                minimumComponentID = candidateComponentID;

        }

        rec.setField(1, new LongValue(minimumComponentID));
        out.collect(rec);
    }
}

@ConstantFieldsFirst(0)
public class UpdateComponentIdJoin extends JoinFunction {

    public void join(Record newVertexWithComponent, Record currentVertexWithComponent, Collector<Record> out){
        long candidateComponentID = newVertexWithComponent.getField(1, LongValue.class).getValue();
        long currentComponentID = currentVertexWithComponent.getField(1, LongValue.class).getValue();

        if (candidateComponentID < currentComponentID) {
            out.collect(newVertexWithComponent);
        }
    }
}
```

The [ConnectedComponents example (Java]](https://github.com/stratosphere/stratosphere/blob/release-{{site.current_stable}}/stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/record/connectedcomponents/WorksetConnectedComponents.java) implements the above example.
</section>

<section id=""relational_java"">
<div class=""page-header""><h2>Relational Query (Java)</h2></div>

```java
// --- program assembly ---

// configure CSV parser for orders file
FileDataSource orders = new FileDataSource(new CsvInputFormat(), ordersPath, ""Orders"");
    CsvInputFormat.configureRecordFormat(orders).fieldDelimiter('|')
            .field(LongValue.class, 0)          // order id as Longh from position 0
            .field(IntValue.class, 7)           // ship prio as Int from position 7
            .field(StringValue.class, 2)        // order status as String from position 2
            .field(StringValue.class, 4)        // order date as String from position 4
            .field(StringValue.class, 5);       // order prio as String from position 5

FileDataSource lineitems = new FileDataSource(new CsvInputFormat(), lineitemsPath, ""LineItems"");
     CsvInputFormat.configureRecordFormat(lineitems).fieldDelimiter('|')
            .field(LongValue.class, 0)          // order id as Long from position 0
            .field(DoubleValue.class, 5);       // extended price as Double from position 5

MapOperator filterO = MapOperator.builder(new FilterO()).input(orders).name(""FilterO"").build();

// join both on field 0 which is a LongValue
JoinOperator joinLiO = JoinOperator.builder(new JoinLiO(), LongValue.class, 0, 0)
            .input1(filterO).input2(lineitems)
            .name(""JoinLiO"").build();

 // reduce on both field 0 (LongValue) and field 1 (StringValue) 
ReduceOperator aggLiO = ReduceOperator.builder(new AggLiO())
            .keyField(LongValue.class, 0)    
            .keyField(StringValue.class, 1)   
            .input(joinLiO).name(""AggLio"").build();

FileDataSink result = new FileDataSink(new CsvOutputFormat(), output, aggLiO, ""Output"");
    CsvOutputFormat.configureRecordFormat(result).fieldDelimiter('|')
            .field(LongValue.class, 0)
            .field(IntValue.class, 1)
            .field(DoubleValue.class, 2);
            
// --- Filter Function ---
public static class FilterO extends MapFunction {

    public void map(Record record, Collector<Record> out) {
        String orderStatus = record.getField(2, StringValue.class).getValue();
        String orderPrio   = record.getField(4, StringValue.class).getValue();
        String orderDate   = record.getField(3, StringValue.class).getValue();
        int year = Integer.parseInt(orderDate.substring(0, 4));

        if (orderStatus.equals(""F"") && orderPrio.startsWith(""5"") && year > 1993)
            out.collect(record);
    }
}

// --- Join Function ---
@ConstantFieldsFirst({0,1})
public static class JoinLiO extends JoinFunction {

    public void join(Record order, Record lineitem, Collector<Record> out) {
        order.setField(2, lineitem.getField(1, DoubleValue.class));
        out.collect(order);
    }
}

// --- Aggregation Function ---
public class AggLiO extends ReduceFunction {

    public void reduce(Iterator<Record> values, Collector<Record> out) {
        Record rec = null;
        double partExtendedPriceSum = 0;

        while (values.hasNext()) {
            rec = values.next();
            partExtendedPriceSum += rec.getField(2, DoubleValue.class).getValue();
        }
        rec.setField(2, new DoubleValue(partExtendedPriceSum));
        out.collect(rec);
    }
}
```

The source code of this example can be found [here|https://github.com/stratosphere/stratosphere/blob/release-{{site.current_stable}}/stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/record/relational/TPCHQuery3.java).

</section>

## Test Data

<section id=""test_data"">
The relational query example works with the TPC-H benchmark suite's data generator tool (DBGEN]. You can use it to generate arbitrarily large sample input data sets (see [http://www.tpc.org/tpch/|http://www.tpc.org/tpch/)).
To use it together with Stratosphere, take the following steps:

1.  Download and unpack DBGEN
2.  Make a copy of *makefile.suite* called *Makefile* and perform the following changes:

```bash
# The Stratosphere program was tested with DB2 data format
DATABASE = DB2
MACHINE  = LINUX
WORKLOAD = TPCH

# according to your compiler, mostly gcc
CC       = gcc
```

1.  Build DBGEN using *make*
2.  Generate lineitem and orders relations using dbgen. A scale factor
    (-s] of 1 results in a generated data set with about 1 GB size.

```
./dbgen -T o -s 1
```
</section>

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/406
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: documentation, 
Created at: Fri Jan 17 05:37:11 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397776,,,Mon Jun 09 12:13:43 UTC 2014,,,,,,,,,,"0|i1whe7:",397903,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:13;github-import;[Date: Fri Jan 17 09:54:43 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hey,

I'm really against using the `_` in our scala examples too much, especially in the word count example

```scala
val words = input.flatMap { _.split("" "") map { (_, 1) } }
```
One of the nice properties of Java's verbosity is that the code is automatically a documentation.;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 10:00:36 CET 2014, Author: [sscdotopen|https://github.com/sscdotopen]]

+1 very good point. The underscore is especially hard to read for people
not familiar with scala
Am 17.01.2014 09:54 schrieb ""Robert Metzger"" <notifications@github.com>:

> Hey,
>
> I'm really against using the _ in our scala examples too much, especially
> in the word count example
>
> val words = input.flatMap { _.split("" "") map { (_, 1) } }
>
> One of the nice properties of Java's verbosity is that the code is
> automatically a documentation.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/406#issuecomment-32588928>
> .
>;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 14:36:25 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

All right, feel free to adjust.

Do you want to put it online anyways and then improve from there?


On Fri, Jan 17, 2014 at 4:00 AM, sscdotopen <notifications@github.com>wrote:

> +1 very good point. The underscore is especially hard to read for people
> not familiar with scala
> Am 17.01.2014 09:54 schrieb ""Robert Metzger"" <notifications@github.com>:
>
> > Hey,
> >
> > I'm really against using the _ in our scala examples too much,
> especially
> > in the word count example
> >
> > val words = input.flatMap { _.split("" "") map { (_, 1) } }
> >
> > One of the nice properties of Java's verbosity is that the code is
> > automatically a documentation.
> >
> > —
> > Reply to this email directly or view it on GitHub<
> https://github.com/stratosphere/stratosphere/issues/406#issuecomment-32588928>
>
> > .
> >
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/406#issuecomment-32589266>
> .
>;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 14:39:54 CET 2014, Author: [uce|https://github.com/uce]]

Yes, I went over the text, fixed typos and links, but left the code as is. I will push it in a few minutes.

I think we should also do this for the ""real"" code examples.;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 14:42:12 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Yes, the examples code is in dire need for cleanup. I have started in some
cases and will push that soon and go on. Both the Java and Scala parts need
beautification.


On Fri, Jan 17, 2014 at 8:39 AM, Ufuk Celebi <notifications@github.com>wrote:

> Yes, I went over the text, fixed typos and links, but left the code as is.
> I will push it in a few minutes.
>
> I think we should also do this for the ""real"" code examples.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/406#issuecomment-32606171>
> .
>;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 15:29:24 CET 2014, Author: [uce|https://github.com/uce]]

OK, I've pushed this in [307cf0e4707caf4e495652c607e87e90227452bf|https://github.com/stratosphere/stratosphere/commit/307cf0e4707caf4e495652c607e87e90227452bf].;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 16:39:23 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Beautiful, thanks a lot!
Am 17.01.2014 09:29 schrieb ""Ufuk Celebi"" <notifications@github.com>:

> Closed ([#406|https://github.com/stratosphere/stratosphere/issues/406] | [FLINK-406|https://issues.apache.org/jira/browse/FLINK-406]) <https://github.com/stratosphere/stratosphere/issues/406>.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/406>
> .
>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Avro complex types (array, map, enum) implemented",FLINK-405,12719576,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:13,09/Jun/14 12:13,14/Jul/23 05:57,09/Jun/14 12:13,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/405
Created by: [twalthr|https://github.com/twalthr]
Labels: 
Created at: Fri Jan 17 00:00:44 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:13;github-import;pull-request-405-2944139714389583962.patch;https://issues.apache.org/jira/secure/attachment/12649130/pull-request-405-2944139714389583962.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397775,,,Mon Jun 09 12:13:36 UTC 2014,,,,,,,,,,"0|i1whdz:",397902,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:13;github-import;[Date: Fri Jan 17 10:46:05 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

The travis error is caused by a bug in java6 http://maven.40175.n5.nabble.com/mvn-javadoc-javadoc-failing-on-only-ONE-Module-project-td5093349.html (http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6709246)
;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 13:47:48 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I manually reverted the eclipse code formatter changes in [a96bf0d4357cc51032e142b1462e981f192ef257|https://github.com/stratosphere/stratosphere/commit/a96bf0d4357cc51032e142b1462e981f192ef257];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing CustomDataTypeTest,FLINK-404,12719575,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:13,09/Jun/14 12:13,14/Jul/23 05:57,09/Jun/14 12:13,,,,pre-apache,,,,,,,0,github-import,,"Can anybody confirm that this test is not working:

`mvn clean compile test -DfailIfNoTests=false -Dtest=CustomDataTypeTest`

The weird thing is that the test doesn't fail when the all tests are run, e.g. `mvn clean verify`.

I have an idea why this might happen. It would be nice though if someone could make sure that it is nothing local to my setup. After that I will try the fix. ;-)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/404
Created by: [uce|https://github.com/uce]
Labels: bug, build system, testing, 
Created at: Thu Jan 16 18:28:56 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397774,,,Mon Jun 09 12:13:30 UTC 2014,,,,,,,,,,"0|i1whdr:",397901,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:13;github-import;[Date: Thu Jan 16 18:32:56 CET 2014, Author: [aljoscha|https://github.com/aljoscha]]

Yes, I talked to @rmetzger about this today. It fails when I run it as a single test but not with mvn verify and so on.;;;","09/Jun/14 12:13;github-import;[Date: Thu Jan 16 23:50:43 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

`mvn clean compile test -DfailIfNoTests=false -Dtest=CustomDataTypeTest` gives me: 
```
Failed tests: 
  testJob[0|eu.stratosphere.test.operators.io.CustomDataTypeTest)
```
So I can confirm this.
`CustomDataTypeTest` also fails from Eclipse.


`mvn clean verify` does not fail. BUT it also shows the exception: https://s3.amazonaws.com/archive.travis-ci.org/jobs/17051218/log.txt

The maven output looks like this
```
	at com.sun.proxy.$Proxy0.invoke(Unknown Source)
	at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:139)
	at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcess(SurefireStarter.java:82)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:81]
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 4.299 sec <<< FAILURE!
```

We have to find out why `maven-surefire-plugin` is accepting failed tests.
So I would argue this particular issue is caused by the ""build system""

;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 11:29:47 CET 2014, Author: [uce|https://github.com/uce]]

It's not a problem with the build system and the test fails with my upcoming changes. Anyways, the test is flawed. I vote to remove it.

PS: It throws a `ClassNotFoundException`, because a jar has been deleted by accident before. @rmetzger is on it. ;-);;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 11:40:57 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

The jar file has been deleted by commit [33cb2ca9898809d2fc90765996ea56bbea458e59|https://github.com/stratosphere/stratosphere/commit/33cb2ca9898809d2fc90765996ea56bbea458e59];;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 14:38:16 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

We should have some test that tests the class loading, though. Will this be
checked now by the new big regression job that Robert is creating?


On Fri, Jan 17, 2014 at 6:14 AM, Ufuk Celebi <notifications@github.com>wrote:

> Closed ([#404|https://github.com/stratosphere/stratosphere/issues/404] | [FLINK-404|https://issues.apache.org/jira/browse/FLINK-404]) <https://github.com/stratosphere/stratosphere/issues/404>.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/404>
> .
>;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 14:40:33 CET 2014, Author: [uce|https://github.com/uce]]

Yes, @aljoscha's test case will involve class loading.;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 14:42:26 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Ah, bueno!


On Fri, Jan 17, 2014 at 8:40 AM, Ufuk Celebi <notifications@github.com>wrote:

> Yes, @aljoscha <https://github.com/aljoscha>'s test case will involve
> class loading.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/404#issuecomment-32606218>
> .
>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generated quickstart job fails with IndexOutOfBounceException,FLINK-401,12719572,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:13,09/Jun/14 12:13,14/Jul/23 05:57,09/Jun/14 12:13,,,,pre-apache,,,,,,,0,github-import,,"A full log can be found here:

http://pastebin.com/ZqiMwS0n

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/401
Created by: [RobertBuhren|https://github.com/RobertBuhren]
Labels: bug, question, testing, 
Created at: Wed Jan 15 14:13:04 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397771,,,Mon Jun 09 12:13:18 UTC 2014,,,,,,,,,,"0|i1whd3:",397898,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:13;github-import;[Date: Wed Jan 15 14:20:32 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Hmmm, looks like a problem in the ContextChecker. Could you please check whether:

  - It is actually a bug in the ContextChecker (it reports a problem where there is none)
  - Or whether the job really has a problem. In that case, please give it a better error message

;;;","09/Jun/14 12:13;github-import;[Date: Wed Jan 15 14:22:44 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I'm having a look at that right now.;;;","09/Jun/14 12:13;github-import;[Date: Wed Jan 15 14:47:29 CET 2014, Author: [fhueske|https://github.com/fhueske]]

OK, the quickstart contains two job classes. An empty template called Job.java and the Wordcount Example job (WordCountJob.java).

The MANIFEST file of the jar file points to the job that is executed by default, which in case of the Quickstart project is the incompletely implemented Job.java. Therefore, the execution fails.

Other jobs than the default job, can be executed using the `--class` option of the CLI client. 
```
 ./stratosphere run -j ~/quickstart/target/quickstart-0.1.jar 
                           -c eu.stratosphere.quickstart.WordCountJob 
                           -a file:///path/to/input file:///path/to/output 1 
                           -w
```

See the [CLI Interface Documentation|http://stratosphere.eu/docs/0.4/program_execution/cli_client.html] for details.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create FAQ section on website,FLINK-400,12719571,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:13,09/Jun/14 12:13,14/Jul/23 05:57,09/Jun/14 12:13,,,,pre-apache,,,,,,,0,github-import,,"The faq should answer the following question
* how can I do a outer join with Stratosphere


We also need to document the `open()` and `close()` method in the Java Programming API.
It is also necessary to explain how developers can pass arguments and variables to their udfs ( from the plan!)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/400
Created by: [rmetzger|https://github.com/rmetzger]
Labels: website, 
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Wed Jan 15 09:35:55 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397770,,,Mon Jun 09 12:13:13 UTC 2014,,,,,,,,,,"0|i1whcv:",397897,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:13;github-import;[Date: Fri Jan 17 05:13:44 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Here is a first version of the FAQs. Feel free to correct and augment. (You can copy the markdown code when you click ""edit"" in the issue)

--- 
layout: inner_faq
title: Frequently Asked Questions (FAQ)
questions: 
  - {section: ""true"", anchor: ""usage"", title: ""General""}
  - {anchor: ""stratosphere_hadoop_project"", title: ""Is Stratosphere a Hadoop Project?""}
  - {section: ""true"", anchor: ""usage"", title: ""Usage""}
  - {anchor: ""usage_progress"", title: ""How do I assess the progress of a Stratosphere program?""}
  - {anchor: ""usage_crash"", title: ""How can I figure out why a program failed?""}
  - {anchor: ""usage_debugging"", title: ""How do I debug Stratosphere programs?""}
  - {section: ""true"", anchor: ""errors"", title: ""Errors""}
  - {anchor: ""errors_buffers"", title: ""I get an error message saying that not enough buffers are available. How do I fix this?""}
  - {anchor: ""errors_hdfs"", title: ""My job fails early with a java.io.EOFException. What could be the cause?""}
  - {anchor: ""errors_keys"", title: ""My program does not compute the correct result. Why are my custom key types are not grouped/joined correctly?""}
  - {anchor: ""errors_instantiation"", title: ""I get a <em>java.lang.InstantiationException</em> for my data type, what is wrong?""}
  - {anchor: ""errors_visualization"", title: ""I get a <em>java.lang.UnsatisfiedLinkError</em> when starting the runtime visualization. How can I fix that?""}
  - {anchor: ""errors_stop_stratosphere"", title: ""I can't stop Stratosphere with the provided stop-scripts. What can I do?""}
  - {anchor: ""errors_out_of_memory"", title: ""I got an <em>OutOfMemoryException</em>. What can I do?""}
  - {anchor: ""errors_huge_tm_log"", title: ""Why do the TaskManager log files become so huge?""}
  - {section: ""true"", anchor: ""features"", title: ""Features""}
  - {anchor: ""features_fault_tolerance"", title: ""What kind of fault-tolerance does Stratosphere provide?""}
  - {anchor: ""features_hadoop"", title: ""Are Hadoop-like utilities, such as Counters and the DistributedCache supported?""}
---

<section id=""usage"">
<div class=""page-header""><h2>General</h2></div>

<section id=""stratosphere_hadoop_project"">
### Is Stratosphere a Hadoop Project?

Stratosphere is an alternative analysis engine to Hadoop's MapReduce and comes with its own runtime, rather than building on top of MapReduce. As such, it can work completely independently of Hadoop, but it can also run on top of Hadoop's distributed file system (HDFS) and Hadoop's next-generation resource manager (YARN).
</section>

</section>


<section id=""usage"">
<div class=""page-header""><h2>Usage</h2></div>

<section id=""usage_progress"">
### How do I assess the progress of a Stratosphere program?

There are a multiple of ways to track the progress of a Stratosphere program:

-   The JobManager (the master of the distributed system) starts a web interface to observe program execution. In runs on port 8081 by default (configured in `conf/stratosphere-config.yml`).
-   When you start a program from the command line, it will print the status changes of all operators as the program progresses through the operations.
-   The `swt-visualization` tool reports the states of all subtasks. If *profiling* is enabled (see [Configuration Reference|http://stratosphere.eu/docs/0.4/setup/config.html ""Configuration Reference"")), the load of all machines is displayed as well.
-   All status changes are also logged to the JobManager's log file.
</section>

<section id=""usage_crash"">
### How can I figure out why a program failed?

-   If you run the program from the command-line in blocking mode (with *wait* flag `--wait` or `-w`), task exceptions are printed to the standard error stream and shown on the console.
-   Both the command line and the web interface allow you to figure out which parallel task first failed and caused the other tasks to cancel the execution.
-   Failing tasks and the corresponding exceptions are reported in the log files of the master and the worker where the exception occurred (`log/stratosphere-<user>-jobmanager-<host>.log` and `log/stratosphere-<user>-taskmanager-<host>.log`].
</section>

<section id=""usage_debugging"">
### How do I debug Stratosphere programs?

-   When you start a program locally with the [LocalExecutor|http://stratosphere.eu/docs/0.4/program_execution/local_executor.html], you can place breakpoints in your functions and debug them like normal Java/Scala programs.
-   The [Accumulators|http://stratosphere.eu/docs/0.4/programming_guides/java.html#accumulators) are very helpful in tracking the behavior of the parallel execution. They allow you to gather information inside the program's operations and show them after the program execution.
</section>
</section>

<section id=""errors"">
<div class=""page-header""><h2>Errors</h2></div>

<section id=""errors_buffers"">

### I get an error message saying that not enough buffers are available. How do I fix this?

If you run Stratosphere in a massively parallel setting (100+ parallel threads], you need to adapt the number of network buffers via the config parameter `taskmanager.network.numberOfBuffers`.
As a rule-of-thumb, the number of buffers should be at least `4 * numberOfNodes * numberOfTasksPerNode^2` See [Configuration Reference|http://stratosphere.eu/docs/0.4/setup/config.html ""Configuration Reference"") for details.

</section>


<section id=""errors_hdfs"">
### My job fails early with a <em>java.io.EOFException</em>. What could be the cause?

Note: In version <em>0.4</em>, the delta iterations limit the solution set to records with fixed-length data types. We will  in the next version.

The most common case for these exception is when Stratosphere is set up with the wrong HDFS version. Because different HDFS versions are often not compatible with each other, the connection between the filesystem master and the client breaks. This is indicated by a stack trace similar to the one below:

```
Exceptions caused by this problem usually have a stack trace similar to the one below:
    Call to <host:port> failed on local exception: java.io.EOFException
        at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
        at org.apache.hadoop.ipc.Client.call(Client.java:743)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
        at $Proxy0.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82]
        at eu.stratosphere.runtime.fs.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:276
```

Please refer to the [download page|http://stratosphere.eu/downloads/#maven] and the [build instructions|https://github.com/stratosphere/stratosphere/blob/master/README.md) for details on how to set up Stratosphere for different Hadoop and HDFS versions.
</section>

<section id=""errors_keys"">
### My program does not compute the correct result. Why are my custom key types are not grouped/joined correctly?

Keys must correctly implement the methods `java.lang.Object#hashCode()`, `java.lang.Object#equals(Object o)`, and `java.util.Comparable#compareTo(...)`. These methods are always backed with default implementations which are usually inadequate. Therefore, all keys must override `hashCode()` and `equals(Object o)`.
</section>

<section id=""errors_instantiation"">
### I get a <em>java.lang.InstantiationException</em> for my data type, what is wrong?

All data type classes must be public and have a public nullary constructor (constructor with no arguments).
Further more, the classes must not be abstract or interfaces.
If the classes are internal classes, they must be public and static.
</section>

<section id=""errors_visualization"">
### I get a <em>java.lang.UnsatisfiedLinkError</em> when starting the swt-visualization. How can I fix this?

The swt-visualization is, as the name suggests, a SWT application. It requires the appropriate native library for the SWT (Standard Widget
Toolkit] gui library. That library must be specific to your platform. The one that is packaged with Stratosphere by default is for 64bit Linux GTK systems.
If you have a different operating system, architecture, or graphics library, you need a different SWT version.

To fix the problem, update the maven dependency in
[stratosphere-addons/swt-visualization/pom.xml|https://github.com/stratosphere/stratosphere/blob/master/stratosphere-addons/swt-visualization/pom.xml ""https://github.com/stratosphere/stratosphere/blob/master/stratosphere-addons/swt-visualization/pom.xml""]
to refer to your platform specific library. The relevant dependency entry is 
```
<dependency>
    <groupId>org.eclipse.swt.gtk.linux</groupId>
    <artifactId>x86_64</artifactId>
    <version>3.3.0-v3346</version>
</dependency>
```
You can find a the list of available library versions under
[http://repo1.maven.org/maven2/org/eclipse/swt/|http://repo1.maven.org/maven2/org/eclipse/swt/ ""http://repo1.maven.org/maven2/org/eclipse/swt/"").
</section>

<section id=""errors_stop_stratosphere"">
### I can't stop Stratosphere with the provided stop-scripts. What can I do?

Stopping the processes sometimes takes a few seconds, because the shutdown may do some cleanup work.

In some error cases it happens that the JobManager or TaskManager cannot be stopped with the provided stop-scripts (`bin/stop-local.sh` or `bin/stop-cluster.sh`).   
You can kill their processes on Linux/Mac as follows:

-   Determine the process id (pid) of the JobManager / TaskManager process. You can use the `jps` command on Linux(if you have OpenJDK installed] or command `ps -ef | grep java` to find all Java processes. 
-   Kill the process with `kill -9 <pid>`, where `pid` is the process id of the affected JobManager or TaskManager process.
    
On Windows, the TaskManager shows a table of all processes and allows you to destroy a process by right its entry.
</section>

<section id=""errors_out_of_memory"">
### I got an <em>OutOfMemoryException</em>. What can I do?

These exceptions occur usually when the functions in the program consume a lot of memory by collection large numbers of objects, for example in lists or maps. The OutOfMemoryExceptions in Java are kind of tricky. The exception is not necessarily thrown by the component that allocated most of the memory but by the component that tried to requested the latest bit of memory that could not be provided.

There are two ways to go about this:
1.  See whether you can use less memory inside the functions. For example, use arrays of primitive types instead of object types.
2.  Reduce the memory that Stratosphere reserves for its own processing. The TaskManager reserves a certain portion of the available memory for sorting, hashing, caching, network buffering, etc. That part of the memory is unavailable to the user-defined functions. By reserving it, the system can guarantee to not run out of memory on large inputs, but to plan with the available memory and destage operations to disk, if necessary. By default, the system reserves around 70% of the memory. If you frequently run applications that need more memory in the user-defined functions, you can reduce that value using the configuration entries `taskmanager.memory.fraction` or `taskmanager.memory.size`. See the [Configuration Reference|http://stratosphere.eu/docs/0.4/setup/config.html ""Configuration Reference"") for details. This will leave more memory to JVM heap, but may cause data processing tasks to go to disk more often. 
</section>

<section id=""errors_huge_tm_log"">
### Why do the TaskManager log files become so huge?

Check the logging behavior of your jobs. Emitting logging per or tuple may be helpful to debug jobs in small setups with tiny data sets, it becomes very inefficient and disk space consuming if used for large input data.
</section>

</section>


<section id=""features"">
<div class=""page-header""><h2>Features</h2></div>

<section id=""features_fault_tolerance"">
### What kind of fault-tolerance does Stratosphere provide?

Fault tolerance will go into the open source project in the next versions.
</section>

<section id=""features_hadoop"">
### Are Hadoop-like utilities, such as Counters and the DistributedCache supported?

Stratosphere's Accumulators (link] work very similar like Hadoop's counters, but are more powerful.

A distributed cache is not available right now. In many cases, operators like cross and the upcoming broadcast variables handle these situations more efficiently.
</section>

</section>;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 15:30:40 CET 2014, Author: [uce|https://github.com/uce]]

I've pushed @StephanEwen's proposal in [307cf0e4707caf4e495652c607e87e90227452bf|https://github.com/stratosphere/stratosphere/commit/307cf0e4707caf4e495652c607e87e90227452bf].

The stuff @rmetzger mentioned is still missing though.;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 16:17:57 CET 2014, Author: [uce|https://github.com/uce]]

I've added documentation on passing parameters (including open and close) to functions in [86c5771ea6d87e2db01cdd6f5f6883dd7a57bf00|https://github.com/stratosphere/stratosphere/commit/86c5771ea6d87e2db01cdd6f5f6883dd7a57bf00]. It's already [online|http://stratosphere.eu/docs/0.4/programming_guides/java.html#config].

Do we really want the outer join in the FAQ? I think we should introduce some kind of *Stratosphere cookbook* instead.

For the outer join it would be even better to have a special operator.;;;","09/Jun/14 12:13;github-import;[Date: Fri Jan 17 16:34:02 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Thanks, looks very good! The new java api has explicit outer- and semi-
joins. We could add this question to the faq for the time being.
Am 17.01.2014 09:30 schrieb ""Ufuk Celebi"" <notifications@github.com>:

> I've pushed @StephanEwen <https://github.com/StephanEwen>'s proposal in
> 307cf0e<https://github.com/stratosphere/stratosphere/commit/307cf0e4707caf4e495652c607e87e90227452bf>
> .
>
> The stuff @rmetzger <https://github.com/rmetzger> mentioned is still
> missing though.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/400#issuecomment-32609875>
> .
>;;;","09/Jun/14 12:13;github-import;[Date: Mon Jan 20 23:25:59 CET 2014, Author: [fhueske|https://github.com/fhueske]]

FAQ was pushed in: [307cf0e4707caf4e495652c607e87e90227452bf|https://github.com/stratosphere/stratosphere/commit/307cf0e4707caf4e495652c607e87e90227452bf]
UDF open(), close(), and configuration in: [86c5771ea6d87e2db01cdd6f5f6883dd7a57bf00|https://github.com/stratosphere/stratosphere/commit/86c5771ea6d87e2db01cdd6f5f6883dd7a57bf00]

Ready to close.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Meteor/Sopremo documentation added,FLINK-398,12719569,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:13,09/Jun/14 12:13,14/Jul/23 05:57,09/Jun/14 12:13,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/398
Created by: [AHeise|https://github.com/AHeise]
Labels: 
Created at: Mon Jan 13 18:39:37 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:13;github-import;pull-request-398-4724824572341411093.patch;https://issues.apache.org/jira/secure/attachment/12649129/pull-request-398-4724824572341411093.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397768,,,Mon Jun 09 12:13:05 UTC 2014,,,,,,,,,,"0|i1whcf:",397895,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:13;github-import;[Date: Mon Jan 13 18:47:41 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi
thanks for the pull request. looks good!

One minor comment (but I think it is okay to to use the text even with that) is that you're mentioning the ""Nephele JobManager"". We don't describe nephele and pact in the documentation anymore, so these are unknown concepts to the average user.
;;;","09/Jun/14 12:13;github-import;[Date: Mon Jan 13 19:53:05 CET 2014, Author: [AHeise|https://github.com/AHeise]]

Fixed in  	e02260a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
localExecutor documentation,FLINK-396,12719567,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:12,09/Jun/14 12:12,14/Jul/23 05:57,09/Jun/14 12:12,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/396
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Jan 13 11:43:04 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;pull-request-396-5474368774445382571.patch;https://issues.apache.org/jira/secure/attachment/12649128/pull-request-396-5474368774445382571.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397766,,,2014-06-09 12:12:50.0,,,,,,,,,,"0|i1whbz:",397893,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release 0.4.1 Planning,FLINK-395,12719566,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:12,09/Jun/14 12:12,14/Jul/23 05:57,09/Jun/14 12:12,,,,pre-apache,,,,,,,0,github-import,,"With this issue, I want to plan the first bugfix release in the 0.4 line.


List of pull requests and commits for 0.4.1.

* `start-local.bat` fix for paths with spaces https://github.com/stratosphere/stratosphere/pull/394
* NPE in Scala interface https://github.com/stratosphere/stratosphere/pull/409
* improved error message in spargel/iterations https://github.com/stratosphere/stratosphere/pull/432
* fix in execution graph test https://github.com/stratosphere/stratosphere/pull/420
* NPE in Scala interface (commit: [eedfe1e0e5598630dd446e89db4c68d122f3d32a|https://github.com/stratosphere/stratosphere/commit/eedfe1e0e5598630dd446e89db4c68d122f3d32a]) reporting issue: https://github.com/stratosphere/stratosphere/issues/434
* Tests failing with non-us locale https://github.com/stratosphere/stratosphere/issues/416 [f17ccfb52eac786497dd2a7d150f5e361cfca5b1|https://github.com/stratosphere/stratosphere/commit/f17ccfb52eac786497dd2a7d150f5e361cfca5b1]
* YARN URL fix: [5300f4b4b4cfdb00edbd5b4e87721640b0c7dc0c|https://github.com/stratosphere/stratosphere/commit/5300f4b4b4cfdb00edbd5b4e87721640b0c7dc0c]
* dist-binary fix: [00676c56a875a987ce23371c1665e7d28c1f46ee|https://github.com/stratosphere/stratosphere/commit/00676c56a875a987ce23371c1665e7d28c1f46ee], travis fix: [82542d68edd5999cd7cd8330bb7a3b19852b8eb6|https://github.com/stratosphere/stratosphere/commit/82542d68edd5999cd7cd8330bb7a3b19852b8eb6]


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/395
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Milestone: Release 0.4
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Mon Jan 13 09:49:50 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397765,,,Mon Jun 09 12:12:48 UTC 2014,,,,,,,,,,"0|i1whbr:",397892,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;[Date: Fri Feb 07 10:24:45 CET 2014, Author: [fhueske|https://github.com/fhueske]]

- Configurable write modes for FileDataSink: ([#421|https://github.com/stratosphere/stratosphere/issues/421] | [FLINK-421|https://issues.apache.org/jira/browse/FLINK-421])
- fix for distributed writing to local FS: ([#421|https://github.com/stratosphere/stratosphere/issues/421] | [FLINK-421|https://issues.apache.org/jira/browse/FLINK-421])
- improved YARN package ([#485|https://github.com/stratosphere/stratosphere/issues/485] | [FLINK-485|https://issues.apache.org/jira/browse/FLINK-485]) ;;;","09/Jun/14 12:12;github-import;[Date: Wed Apr 16 18:50:31 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Subsumed by 0.5 release planning.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paths in windows scripts enclosed in double quotes,FLINK-394,12719565,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:12,09/Jun/14 12:12,14/Jul/23 05:57,09/Jun/14 12:12,,,,pre-apache,,,,,,,0,github-import,,"Small fix for Issue ([#326|https://github.com/stratosphere/stratosphere/issues/326] | [FLINK-326|https://issues.apache.org/jira/browse/FLINK-326]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/394
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon Jan 13 08:53:57 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;pull-request-394-2472892338287649174.patch;https://issues.apache.org/jira/secure/attachment/12649127/pull-request-394-2472892338287649174.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397764,,,Mon Jun 09 12:12:43 UTC 2014,,,,,,,,,,"0|i1whbj:",397891,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;[Date: Mon Jan 20 12:26:02 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [0596773fbe4b22d08ae4a8b4d4ef4a6d74656ef0|https://github.com/stratosphere/stratosphere/commit/0596773fbe4b22d08ae4a8b4d4ef4a6d74656ef0];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add 0.4 migration guide blog post,FLINK-392,12719563,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:12,09/Jun/14 12:12,14/Jul/23 05:57,09/Jun/14 12:12,,,,pre-apache,,,,,,,0,github-import,,"I also changed the width of the blog.

Preview: 
http://robertmetzger.de/stratosphere/blog/

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/392
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Sun Jan 12 18:42:29 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;pull-request-392-9065760592124182932.patch;https://issues.apache.org/jira/secure/attachment/12649126/pull-request-392-9065760592124182932.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397762,,,2014-06-09 12:12:31.0,,,,,,,,,,"0|i1whb3:",397889,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change versions on website to 0.4 and 0.4-hadoop2,FLINK-391,12719562,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:12,09/Jun/14 12:12,14/Jul/23 05:57,09/Jun/14 12:12,,,,pre-apache,,,,,,,0,github-import,,"add rss feed
add latest blogposts to start page
add version information to front page.

Preview http://robertmetzger.de/stratosphere/ (the quickstarts on my preview are broken .. this won't be the case when merging this pr))

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/391
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Fri Jan 10 23:38:31 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;pull-request-391-1827337782929753990.patch;https://issues.apache.org/jira/secure/attachment/12649125/pull-request-391-1827337782929753990.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397761,,,Mon Jun 09 12:12:29 UTC 2014,,,,,,,,,,"0|i1whav:",397888,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;[Date: Sat Jan 11 11:23:56 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

@StephanEwen: I fixed everything I commented on your commits from last night.
If you want, I can remove the ""latest blog posts"" section from the PR again so that we can merge it.;;;","09/Jun/14 12:12;github-import;[Date: Sat Jan 11 11:45:32 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Yes, please merge it without the blog posts on the start page.

Thanks!

Am 11.01.2014 11:23 schrieb ""Robert Metzger"" <notifications@github.com>:
>
> @StephanEwen: I fixed everything I commented on your commits from last
night.
> If you want, I can remove the ""latest blog posts"" section from the PR
again so that we can merge it.
>
> —
> Reply to this email directly or view it on GitHub.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
blogpost for hadoop summit,FLINK-389,12719560,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:12,09/Jun/14 12:12,14/Jul/23 05:57,09/Jun/14 12:12,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/389
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Fri Jan 10 13:58:37 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;pull-request-389-1196511854503515584.patch;https://issues.apache.org/jira/secure/attachment/12649124/pull-request-389-1196511854503515584.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397759,,,Mon Jun 09 12:12:20 UTC 2014,,,,,,,,,,"0|i1whaf:",397886,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;[Date: Fri Jan 10 14:03:19 CET 2014, Author: [uce|https://github.com/uce]]

Brief and to the point. I like it!;;;","09/Jun/14 12:12;github-import;[Date: Fri Jan 10 16:44:34 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Nice!

Please address Ufuk's comment as ""Future of Hadoop"" track and merge.


On Fri, Jan 10, 2014 at 2:03 PM, Ufuk Celebi <notifications@github.com>wrote:

> Brief and to the point. I like it!
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/389#issuecomment-32025450>
> .
>;;;","09/Jun/14 12:12;github-import;[Date: Fri Jan 10 16:47:47 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Version with a few corrections and shorter sentences.

The Stratosphere team is proud to announce that it is going to present at the [Hadoop Summit 2014 in Amsterdam|http://hadoopsummit.org/amsterdam/] on April 2-3. Our talk ""Big Data looks tiny from Stratosphere"" is part of the ""Future of Hadoop"" track. The talk abstract already made it into the top 5 in the [Community Vote|https://hadoopsummit.uservoice.com/forums/196822-future-of-apache-hadoop/filters/top] that took place by the end of last year.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
create parallel collections and add java value to stratosphere value interface,FLINK-388,12719559,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:12,09/Jun/14 12:12,14/Jul/23 05:57,09/Jun/14 12:12,,,,pre-apache,,,,,,,0,github-import,,"This is the implementation for issue https://github.com/stratosphere/stratosphere/issues/376.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/388
Created by: [qmlmoon|https://github.com/qmlmoon]
Labels: 
Created at: Fri Jan 10 12:02:47 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;pull-request-388-2420294475242151504.patch;https://issues.apache.org/jira/secure/attachment/12649123/pull-request-388-2420294475242151504.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397758,,,Mon Jun 09 12:12:14 UTC 2014,,,,,,,,,,"0|i1wha7:",397885,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;[Date: Fri Jan 10 21:41:08 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

@qmlmoon: I updated the issue description (https://github.com/stratosphere/stratosphere/issues/376) with an iterator example. ;;;","09/Jun/14 12:12;github-import;[Date: Fri Jan 10 23:05:48 CET 2014, Author: [qmlmoon|https://github.com/qmlmoon]]

Yeah. It should be a collection. I was not familiar with this collection data format. I’ll change it.

On 10 Jan 2014, at 21:41, Robert Metzger <notifications@github.com> wrote:

> tor exam;;;","09/Jun/14 12:12;github-import;[Date: Sat Jan 18 22:14:58 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hey @qmlmoon,

you're code is really good! I tested it today and everything worked as expected.
Once you resolved the issues I commented on, it is good to merge.;;;","09/Jun/14 12:12;github-import;[Date: Sat Jan 18 22:16:40 CET 2014, Author: [qmlmoon|https://github.com/qmlmoon]]

Thank you:D. I’ll work on your comments at Monday.

On 18 Jan 2014, at 22:15, Robert Metzger <notifications@github.com> wrote:

> Hey @qmlmoon,
> 
> you're code is really good! I tested it today and everything worked as expected.
> Once you resolved the issues I commented on, it is good to merge.
> 
> —
> Reply to this email directly or view it on GitHub.
> ;;;","09/Jun/14 12:12;github-import;[Date: Wed Jan 29 19:21:04 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Mingliang, I wrote some comments to different parts of the code. Please have a look at them and let me know if you have questions, or if you do not agree with the comment.;;;","09/Jun/14 12:12;github-import;[Date: Fri Feb 07 18:28:46 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Merged in [8dd2470162167a6bb051ca3383aa0c364a680985|https://github.com/stratosphere/stratosphere/commit/8dd2470162167a6bb051ca3383aa0c364a680985];;;","09/Jun/14 12:12;github-import;[Date: Thu May 08 22:09:47 CEST 2014, Author: [qmlmoon|https://github.com/qmlmoon]]

The newest update of tuple up to 25 seems not update the MAX_ARITY in abstract class Tuple, which leads to the IllegalArgumentException in TupleTypeInfo;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Set LocalExecutor logging to INFO; print runtime. reduce logging verbosity of IPC server",FLINK-387,12719558,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:12,09/Jun/14 12:12,14/Jul/23 05:57,09/Jun/14 12:12,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/387
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Thu Jan 09 21:34:47 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;pull-request-387-2583125570864780341.patch;https://issues.apache.org/jira/secure/attachment/12649122/pull-request-387-2583125570864780341.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397757,,,Mon Jun 09 12:12:04 UTC 2014,,,,,,,,,,"0|i1wh9z:",397884,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:12;github-import;[Date: Fri Jan 10 13:49:01 CET 2014, Author: [uce|https://github.com/uce]]

I think this is good to merge.;;;","09/Jun/14 12:12;github-import;[Date: Fri Jan 17 18:36:02 CET 2014, Author: [uce|https://github.com/uce]]

Merged in [3b682a98e949a052a907e501be067190d4f0a1dd|https://github.com/stratosphere/stratosphere/commit/3b682a98e949a052a907e501be067190d4f0a1dd].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala quickstart problem,FLINK-384,12719555,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:11,09/Jun/14 12:11,14/Jul/23 05:57,09/Jun/14 12:11,,,,pre-apache,,,,,,,0,github-import,,"Hi,

I have a problem on running the jar file in scala quickstart. The jar file was built without any problem, but when I want to run it, I get the following error:

.../stratosphere/bin$ ./stratosphere run -j ../stratosphere-project-0.1-SNAPSHOT.jar -a 1 file:///<path>/hamlet.txt file:///<path>/output

ERROR: No plan assembler class defined in manifest
eu.stratosphere.client.program.ProgramInvocationException: No plan assembler class defined in manifest
	at eu.stratosphere.client.program.PackagedProgram.getPactAssemblerFromJar(PackagedProgram.java:260)
	at eu.stratosphere.client.program.PackagedProgram.<init>(PackagedProgram.java:87)
	at eu.stratosphere.client.CliFrontend.run(CliFrontend.java:280)
	at eu.stratosphere.client.CliFrontend.parseParameters(CliFrontend.java:763)
	at eu.stratosphere.client.CliFrontend.main(CliFrontend.java:786)

Do you have any idea about this problem?

Thanks,
-- Amir

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/384
Created by: [payberah|https://github.com/payberah]
Labels: 
Created at: Thu Jan 09 08:17:33 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397754,,,Mon Jun 09 12:11:52 UTC 2014,,,,,,,,,,"0|i1wh9b:",397881,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:11;github-import;[Date: Thu Jan 09 08:24:21 CET 2014, Author: [aljoscha|https://github.com/aljoscha]]

Could you check whether the class name of you assembler class matches the namen given in pom.xml? It looks like this:
```xml
<Pact-Assembler-Class>eu.stratosphere.quickstart.Job</Pact-Assembler-Class>
```

If you changed the package name or anything from the default it will not find it anymore.;;;","09/Jun/14 12:11;github-import;[Date: Thu Jan 09 08:28:26 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

@aljoscha I recently changed something on the quickstarts.
The old quickstarts had a hardcoded artifactId (so we ignored the user input, if the user entered a custom artifactId).
It is now `<groupId>${groupId}</groupId>` and `<artifactId>${artifactId}</artifactId>`.
I think we have to use the `${package}` for the Pact-Assembler-Class manifest entry.;;;","09/Jun/14 12:11;github-import;[Date: Thu Jan 09 08:31:38 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

@payberah: a quick fix for you would be to define the `-c` (or `--class`) argument for the `./stratosphere` client.
Something like this:

```
.../stratosphere/bin$ ./stratosphere run -j ../stratosphere-project-0.1-SNAPSHOT.jar -c eu.stratosphere.quickstart.Job -a 1 file:////hamlet.txt file:////output
```

Did you use the `curl .../quickstart-scala.sh | bash` thing to create the project?
;;;","09/Jun/14 12:11;github-import;[Date: Thu Jan 09 08:39:46 CET 2014, Author: [payberah|https://github.com/payberah]]

Thanks for your prompt replies. I tried your solution, but non of them worked. 

Yes, I used the ""curl ..."" to create the project.

Moreover, when I used ""-c eu.stratosphere.quickstart.Job"" in the command, I got the following error message instead:

ERROR: An unknown problem ocurred during the instantiation of the program assembler: java.lang.NoClassDefFoundError: eu/stratosphere/pact/common/plan/PlanAssemblerDescription
eu.stratosphere.client.program.ProgramInvocationException: An unknown problem ocurred during the instantiation of the program assembler: java.lang.NoClassDefFoundError: eu/stratosphere/pact/common/plan/PlanAssemblerDescription
	at eu.stratosphere.client.program.PackagedProgram.getPactAssemblerFromJar(PackagedProgram.java:287)
	at eu.stratosphere.client.program.PackagedProgram.<init>(PackagedProgram.java:117)
	at eu.stratosphere.client.CliFrontend.run(CliFrontend.java:282)
	at eu.stratosphere.client.CliFrontend.parseParameters(CliFrontend.java:763)
	at eu.stratosphere.client.CliFrontend.main(CliFrontend.java:786)
Caused by: java.lang.NoClassDefFoundError: eu/stratosphere/pact/common/plan/PlanAssemblerDescription
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
	at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at eu.stratosphere.client.program.PackagedProgram.getPactAssemblerFromJar(PackagedProgram.java:276)
	... 4 more
Caused by: java.lang.ClassNotFoundException: eu.stratosphere.pact.common.plan.PlanAssemblerDescription
	at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
	... 17 more
;;;","09/Jun/14 12:11;github-import;[Date: Thu Jan 09 09:07:43 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay, sorry for the inconveniences. We recently renamed many parts of the project.
It looks like the version you got from the quickstart script is not compatible with the Stratosphere version you installed.
More precisely, you probably installed a 0.4-rc1 or the 0.4 version and the archetype probably expects 0.4-SNAPSHOT.

Old api: 0.4-SNAPSHOT, 0.2
new api. 0.4-rc1, 0.4

You should certainly use the new api, we hope to release the stable 0.4 this week.
I'm going to rework the Scala quickstart now. I'll post you here once I done (the only problem is that my car is broken and I'm currently waiting for some guy who's coming to fix it .. this could cause some delay.);;;","09/Jun/14 12:11;github-import;[Date: Thu Jan 09 09:12:25 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, there are some versions mixed here  (a 0.4 code version with a
pre-0.4 quickstart). Seems the quickstart is out of sync with the pending
release version. We will fix that asap and ping you back.

As a manual workaround, you could directly reference the version 0.4-rc1 or
0.5-SNAPSHOT in your project.
Am 09.01.2014 08:39 schrieb ""payberah"" <notifications@github.com>:

> Thanks for your prompt replies. I tried your solution, but non of them
> worked.
>
> Yes, I used the ""curl ..."" to create the project.
>
> Moreover, when I used ""-c eu.stratosphere.quickstart.Job"" in the command,
> I got the following error message instead:
>
> ERROR: An unknown problem ocurred during the instantiation of the program
> assembler: java.lang.NoClassDefFoundError:
> eu/stratosphere/pact/common/plan/PlanAssemblerDescription
> eu.stratosphere.client.program.ProgramInvocationException: An unknown
> problem ocurred during the instantiation of the program assembler:
> java.lang.NoClassDefFoundError:
> eu/stratosphere/pact/common/plan/PlanAssemblerDescription
> at
> eu.stratosphere.client.program.PackagedProgram.getPactAssemblerFromJar(PackagedProgram.java:287)
> at
> eu.stratosphere.client.program.PackagedProgram.(PackagedProgram.java:117)
> at eu.stratosphere.client.CliFrontend.run(CliFrontend.java:282)
> at eu.stratosphere.client.CliFrontend.parseParameters(CliFrontend.java:763)
> at eu.stratosphere.client.CliFrontend.main(CliFrontend.java:786)
> Caused by: java.lang.NoClassDefFoundError:
> eu/stratosphere/pact/common/plan/PlanAssemblerDescription
> at java.lang.ClassLoader.defineClass1(Native Method)
> at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
> at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
> at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
> at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
> at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
> at java.security.AccessController.doPrivileged(Native Method)
> at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
> at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
> at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
> at java.lang.Class.forName0(Native Method)
> at java.lang.Class.forName(Class.java:270)
> at
> eu.stratosphere.client.program.PackagedProgram.getPactAssemblerFromJar(PackagedProgram.java:276)
> ... 4 more
> Caused by: java.lang.ClassNotFoundException:
> eu.stratosphere.pact.common.plan.PlanAssemblerDescription
> at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
> at java.security.AccessController.doPrivileged(Native Method)
> at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
> at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
> at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
> ... 17 more
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/384#issuecomment-31908690>
> .
>;;;","09/Jun/14 12:11;github-import;[Date: Thu Jan 09 09:44:06 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay, things got clearer for me.
The `quickstart-scala.sh` version was pointing to the wrong version (0.4-SNAPSHOT). I fixed this now.
So the exception that @payberah currently has should be fixed.
But there is another issue with the scala quickstart that will prevent you from running it: https://github.com/stratosphere/stratosphere-quickstart/issues/6

Again, I'm really sorry that all this happened to you. 
Once Stratosphere and your projects are set up, things run smoothly. 
We will create scripts to automatically verify the quickstarts to avoid those problems in the future.

I will write again here if the last issue with the scala quickstart has been fixed. Should be within the next 4 hours.;;;","09/Jun/14 12:11;github-import;[Date: Thu Jan 09 11:51:15 CET 2014, Author: [payberah|https://github.com/payberah]]

Thanks again, and looking forward to seeing the stable version.
;;;","09/Jun/14 12:11;github-import;[Date: Thu Jan 09 15:58:34 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay, here's a personalized getting started for you ;)
The problem is that we don't have any ""up to date"" versions of the 0.4 release in maven central for you.
But since you want to develop an application for stratosphere, you need to have a stable code basis.

The following instructions help you to set up the 0.5-SNAPSHOT version. It contains everything you need and is in sync with maven central.
once the 0.4 release is out, the move back is quite easy for you (you only have to change the version in the `pom.xml`).

```bash
# download latest 0.5 version:
wget http://stratosphere-bin.s3-website-us-east-1.amazonaws.com/stratosphere-0.5-SNAPSHOT.tgz
tar xzf stratosphere-0.5-SNAPSHOT.tgz
cd stratosphere
./bin/start-local.sh
# test everything with wordcount
wget -O hamlet.txt http://www.gutenberg.org/cache/epub/1787/pg1787.txt
bin/stratosphere run  --jarfile ./examples/stratosphere-java-examples-0.5-SNAPSHOT-WordCount.jar  --arguments 1 file://`pwd`/hamlet.txt file://`pwd`/wordcount-result.txt
# now we create your project
cd ..
curl https://raw2.github.com/stratosphere/stratosphere-quickstart/master/quickstart-scala-SNAPSHOT.sh | bash
cd quickstart
# you can import the contents of this directory into eclipse or intellj (as a maven project!)
# build jar file
mvn package
cd ..
cd stratosphere
# finally execute it
bin/stratosphere run  --jarfile ../quickstart/target/quickstart-0.1.jar  --arguments 1 file://`pwd`/hamlet.txt file://`pwd`/wordcount-result.txt 
```

All these issues will be resolved once the 0.4 release is out.
The problem is that the 0.4-rc1 release that is available in maven central is kind of broken

Please keep posting issues if you have any troubles using our system. We are very happy about every comment and feedback!;;;","09/Jun/14 12:11;github-import;[Date: Thu Jan 09 16:09:36 CET 2014, Author: [payberah|https://github.com/payberah]]

Perfect! Everything worked well.;;;","09/Jun/14 12:11;github-import;[Date: Tue Jan 14 13:41:16 CET 2014, Author: [aljoscha|https://github.com/aljoscha]]

Can we close this issue?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added API for broadcast variables.,FLINK-383,12719554,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:11,09/Jun/14 12:11,14/Jul/23 05:57,09/Jun/14 12:11,,,,pre-apache,,,,,,,0,github-import,,"This is a first step towards realizing the functionality requested in issue ([#66|https://github.com/stratosphere/stratosphere/issues/66] | [FLINK-66|https://issues.apache.org/jira/browse/FLINK-66]). 

The broadcast inputs API consists of two aspects:

* Setters: ~~setParameterInput()~~ `setBroadcastVariable()` / `getBroadcastVariable()` functions in `AbstractUdfOperator` and all Operator Builders. Used to broadcast (bind) subplan results to UDF-local variables.
* Getters: ~~getParameterInput()~~ `setBroadcastVariable()` / `getBroadcastVariable()` in `RuntimeContext`. Used within an UDF open() method for parametrization of the UDF with values provided by the plans rooted at the configured parameter inputs.

A sample intended usage of the API can be seen in the modified `KMeansIterativeWithBroadcastVariables` Java example.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/383
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Created at: Wed Jan 08 15:39:43 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:11;github-import;pull-request-383-8153748046614393321.patch;https://issues.apache.org/jira/secure/attachment/12649121/pull-request-383-8153748046614393321.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397753,,,Mon Jun 09 12:11:43 UTC 2014,,,,,,,,,,"0|i1wh93:",397880,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:11;github-import;[Date: Thu Jan 09 15:09:05 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Do you have any ideas for alternative Names (for `setParameterInput()`)
What do you say to `setBroadcastVariable()` ? and `getBroadcastVariables()` ?;;;","09/Jun/14 12:11;github-import;[Date: Fri Jan 10 14:00:29 CET 2014, Author: [uce|https://github.com/uce]]

Thanks for sharing early. I like @rmetzger's proposal. I think *broadcast* should definitely be in the method names.

I would even propose a simpler variation like (`broadcast(String, Value)` and (`getBroadcastValue(String)` xor just `broadcastValue(String)`)).;;;","09/Jun/14 12:11;github-import;[Date: Fri Jan 10 14:05:35 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

Ok I renamed the methods as follows

* `broadcast(String name, Operator root)` and 
* `Collection<Record> getBroadcastVariable(String name)`.;;;","09/Jun/14 12:11;github-import;[Date: Fri Jan 10 17:23:55 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I think naming the function in `AbstractUDFOperator` `broadcast()` could be misleading.
I would expect that the operator on which `broadcast()`is called, actually broadcasts its result. Instead it would read the broadcasted variable...

How about `addBroadcastVariable()` or `addBroadcastInput()`?
;;;","09/Jun/14 12:11;github-import;[Date: Wed Jan 22 16:46:06 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Isn't the K-Means example a bit too complex to start with?
In this case, the broadcast variable becomes the data that is fed back into the iteration. I am not very familiar with the iterations compiler and runtime code, but I would guess that this will cause some non-trivial changes there.

Wouldn't a simpler example job without iterations be easier to handle for a first draft?;;;","09/Jun/14 12:11;github-import;[Date: Wed Jan 22 18:42:51 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

I was just using for an example job that lends itself for rewriting. I think K-Means should work with a naive implementation because the broadcasted intermediate result and the consumer are within the same iterative path.

A more general case which will cause problems will be a program with the following structure:

```scala
{ 
  var A = ... // broadcast variable

  iterate {
    var B = ... // broadcast variable

    iterate {
      var C = // broadcast variable
      // depends on A, B, and C
      var D = map( t => { ... } )
    } // inner iteration
  } // outer iteration
} // program
```

Here, the parameterized mapper function depends on the broadcast variables A, B, and C. And while C is re-computed and has to be re-drained from the RegularPactTask in each iteration, B and A have to be drained only once per outer iteration and program execution respectively (in K-Means the ""centers"" variable is computed within the same iteration just like C). 

In oder to implement this logic we need to annotate tasks with some sort of scope information and keep track of changes in the scope paths on task re-deployment:

```scala
{ 
  var A = ... // broadcast variable

  iterate {
    var B = ... // broadcast variable

    iterate {
      var C = // broadcast variable
      // depends on:
      // * A (drained on change in s1),
      // * B (drained on change in s2)
      // * C (drained on change in s3)
      var D = map( t => { ... } )
    } // inner iteration (scope s1.s2.s3)
  } // outer iteration (scope s1.s2)
} // program (scope s1)
```

I'm not quite sure whether and how the cost estimation logic for plan enumeration and pruning is affected by this addition. @StephanEwen: your thoughts on this?;;;","09/Jun/14 12:11;github-import;[Date: Wed Jan 22 19:07:45 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I thought of an example without iteration.
Something like:
```
X -> Reduce(avg) --broadcast-----\
                             Y --> Map(y - avg(X)) --> out
```
which can currently be emulated in Stratosphere with a Cross (instead of broadcast Map) or the distributed cache in Hadoop.
;;;","09/Jun/14 12:11;github-import;[Date: Wed Jan 22 19:10:24 CET 2014, Author: [sscdotopen|https://github.com/sscdotopen]]

I really like to comments from your example: boradcast variable, I think BoradCast is a much cooler name than broadcast;;;","09/Jun/14 12:11;github-import;[Date: Thu Jan 23 08:23:27 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I agree with Fabian. It is a good idea to use K-Means to model the API and
validate its ease of use. However, for a first test of the runtime code, a
non-iterative program would be easier.

I will try and comment on the code soon.


On Wed, Jan 22, 2014 at 10:10 AM, sscdotopen <notifications@github.com>wrote:

> I really like to comments from your example: boradcast variable, I think
> BoradCast is a much cooler name than broadcast
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/383#issuecomment-33050214>
> .
>;;;","09/Jun/14 12:11;github-import;[Date: Thu Jan 23 09:05:20 CET 2014, Author: [sscdotopen|https://github.com/sscdotopen]]

You could try to rewrite this program: https://github.com/stratosphere/gagarin/blob/master/src/main/scala/de/tuberlin/dima/gagarin/recommendation/CooccurrenceAnalysis.scala

If numInteractionsPerItem and numInteractionsPerUser are made broadcast variables, the code should be much easier to read.;;;","09/Jun/14 12:11;github-import;[Date: Fri Jan 24 22:46:44 CET 2014, Author: [fhueske|https://github.com/fhueske]]

TPC-H Query 11 gives another good example, how a broadcast variable could replace a cross: 
[TPC-H 11|https://github.com/stratosphere/stratosphere-tpch/blob/master/src/main/scala/eu/stratosphere/tpch/query/TPCHQuery11.scala];;;","09/Jun/14 12:11;github-import;[Date: Fri Feb 07 01:09:59 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

Ooops, just noted @sscdotopen comment. Some BoradCasted variables should be flying around in the integration test for PR ([#460|https://github.com/stratosphere/stratosphere/issues/460] | [FLINK-460|https://issues.apache.org/jira/browse/FLINK-460]).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Polling for jobmanager webinterface,FLINK-381,12719552,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:11,09/Jun/14 12:11,14/Jul/23 05:57,09/Jun/14 12:11,,,,pre-apache,,,,,,,0,github-import,,"I added polling functionality to the webinterface of the jobmanager, so no more pressing of F5 is neccessary and bandwidth is saved. The script is polling for changes every 2secs.
I tested it on FF26, Chrome 31 and IE 11 (on windows).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/381
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Wed Jan 08 11:30:37 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:11;github-import;pull-request-381-6339765310641502620.patch;https://issues.apache.org/jira/secure/attachment/12649119/pull-request-381-6339765310641502620.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397751,,,Mon Jun 09 12:11:23 UTC 2014,,,,,,,,,,"0|i1wh8n:",397878,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:11;github-import;[Date: Fri Jan 10 09:40:40 CET 2014, Author: [markus-h|https://github.com/markus-h]]

I fixed the YARN proxy compartibility and a small bug concerning duplicates in the job history. Since there were some problems testing the new polling functionality a small reminder: Very often the browser cache has to be cleared to test new javascript files.;;;","09/Jun/14 12:11;github-import;[Date: Mon Jan 20 12:25:42 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [e8194b0d350dc9344c7ac07d4ee218a3dedf4364|https://github.com/stratosphere/stratosphere/commit/e8194b0d350dc9344c7ac07d4ee218a3dedf4364];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update stratosphere stack image +spargel,FLINK-380,12719551,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:11,09/Jun/14 12:11,14/Jul/23 05:57,09/Jun/14 12:11,,,,pre-apache,,,,,,,0,github-import,,"![stratosphere_stack|https://f.cloud.github.com/assets/89049/1863145/6a14d94a-77dd-11e3-98b3-110862c215ae.png]


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/380
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue Jan 07 21:51:13 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:11;github-import;pull-request-380-8717590308548507473.patch;https://issues.apache.org/jira/secure/attachment/12649118/pull-request-380-8717590308548507473.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397750,,,2014-06-09 12:11:12.0,,,,,,,,,,"0|i1wh8f:",397877,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bugfix for webinterface: Failed jobs don't disappear,FLINK-375,12719546,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:10,09/Jun/14 12:10,14/Jul/23 05:57,09/Jun/14 12:10,,,,pre-apache,,,,,,,0,github-import,,"Because of a small typo inside of the EventCollector failed jobs were not removed from the recentJobs list. See issue ([#276|https://github.com/stratosphere/stratosphere/issues/276] | [FLINK-276|https://issues.apache.org/jira/browse/FLINK-276]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/375
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Tue Jan 07 10:35:20 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;pull-request-375-4836892253052728733.patch;https://issues.apache.org/jira/secure/attachment/12649117/pull-request-375-4836892253052728733.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397745,,,2014-06-09 12:10:52.0,,,,,,,,,,"0|i1wh7b:",397872,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add scala programming guide,FLINK-374,12719545,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:10,09/Jun/14 12:10,14/Jul/23 05:57,09/Jun/14 12:10,,,,pre-apache,,,,,,,0,github-import,,"This is not yet complete, but almost. The sections about rich stubs and execution are missing.

You can have a look at it here: http://aljoscha.github.io/stratosphere/docs/0.4/programming_guides/scala.html

Sorry for the delay, it turned out to be quite a bit of writing. :sweat_smile: 

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/374
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Mon Jan 06 18:04:50 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;pull-request-374-7368693125648404103.patch;https://issues.apache.org/jira/secure/attachment/12649116/pull-request-374-7368693125648404103.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397744,,,Mon Jun 09 12:10:50 UTC 2014,,,,,,,,,,"0|i1wh73:",397871,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;[Date: Mon Jan 06 18:18:49 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good for a start.

A few quick comments at a first glance:

1) Please rename ""Usable Data Types"" to ""Data Types"". Sounds simpler.
2) In that same Section, Key and Value are confused.
3) Please reorder the input formats such that Text, Csv, and Delimited come
first (I expect those to be the most interesting ones).




On Mon, Jan 6, 2014 at 6:04 PM, Aljoscha Krettek
<notifications@github.com>wrote:

> This is not yet complete, but almost. The sections about rich stubs and
> execution are missing.
>
> You can have a look at it here:
> http://aljoscha.github.io/stratosphere/docs/0.4/programming_guides/scala.html
>
> Sorry for the delay, it turned out to be quite a bit of writing. [image:
> :sweat_smile:]
> ------------------------------
> You can merge this Pull Request by running
>
>   git pull https://github.com/aljoscha/stratosphere add-scala-doc
>
> Or view, comment on, or merge it at:
>
>   https://github.com/stratosphere/stratosphere/pull/374
> Commit Summary
>
>    - Add scala programming guide
>
> File Changes
>
>    - *M* docs/0.4/programming_guides/scala.markdown<https://github.com/stratosphere/stratosphere/pull/374/files#diff-0>(803)
>
> Patch Links:
>
>    - https://github.com/stratosphere/stratosphere/pull/374.patch
>    - https://github.com/stratosphere/stratosphere/pull/374.diff
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/374>
> .
>;;;","09/Jun/14 12:10;github-import;[Date: Tue Jan 07 17:35:39 CET 2014, Author: [uce|https://github.com/uce]]

I will merge this to get an overview of the current state of things on the site.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
improve page button on documentation,FLINK-373,12719544,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:10,09/Jun/14 12:10,14/Jul/23 05:57,09/Jun/14 12:10,,,,pre-apache,,,,,,,0,github-import,,"This should help engaging users to edit our documentation.

![imrp|https://f.cloud.github.com/assets/89049/1832835/94e7a5b2-73be-11e3-9e18-92e2c4ecab4f.png]
GitHub tells the users to create a fork!
![wikidoc|https://f.cloud.github.com/assets/89049/1832834/92bb898e-73be-11e3-8aa7-c630dfcd350e.png]


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/373
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Thu Jan 02 16:01:42 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;pull-request-373-1076144058430191583.patch;https://issues.apache.org/jira/secure/attachment/12649115/pull-request-373-1076144058430191583.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397743,,,Mon Jun 09 12:10:44 UTC 2014,,,,,,,,,,"0|i1wh6v:",397870,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;[Date: Mon Jan 06 16:20:31 CET 2014, Author: [uce|https://github.com/uce]]

Merged in [84533b855aad3df10d74b2cfe7a98d3868a45b21|https://github.com/stratosphere/stratosphere/commit/84533b855aad3df10d74b2cfe7a98d3868a45b21].

I renamed *improve this page* to *edit this page*.;;;","09/Jun/14 12:10;github-import;[Date: Mon Jan 06 16:23:07 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Does not work for me. When I am not logged in, I get a 404 error when I
clock teh button.


On Mon, Jan 6, 2014 at 4:20 PM, Ufuk Celebi <notifications@github.com>wrote:

> Closed ([#373|https://github.com/stratosphere/stratosphere/issues/373] | [FLINK-373|https://issues.apache.org/jira/browse/FLINK-373]) <https://github.com/stratosphere/stratosphere/pull/373>.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/373>
> .
>;;;","09/Jun/14 12:10;github-import;[Date: Mon Jan 06 16:34:40 CET 2014, Author: [uce|https://github.com/uce]]

OK, sorry. I didn't notice this.

I changed the link to *blob* instead of the *edit* page in [381e504088311a5b496bb5417f2011a09fec98dc|https://github.com/stratosphere/stratosphere/commit/381e504088311a5b496bb5417f2011a09fec98dc]. If not logged in, this just shows the code. If logged in, the user can click the *edit button* on GitHub etc.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
implemented jdbcoutputformat,FLINK-372,12719543,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:10,09/Jun/14 12:10,14/Jul/23 05:57,09/Jun/14 12:10,,,,pre-apache,,,,,,,0,github-import,,"Here's the implementation of the JDBC outputformat ( issue https://github.com/stratosphere/stratosphere/issues/261 ).

Arguments that have to be passed via the configuration:
* database url
* username and password (if not contained in the url)
* jdbc driver
* insert query
* the types of the columns in order. 

The configuration is constructed using a configbuilder. The target table is assumed to exist already.

The test file covers the functionality of the format by copying the contents of a table to another.
i added an example file showing both jdbc formats.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/372
Created by: [zentol|https://github.com/zentol]
Labels: 
Milestone: Release 0.5
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Thu Jan 02 00:07:25 CET 2014
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;pull-request-372-3527540380252369916.patch;https://issues.apache.org/jira/secure/attachment/12649114/pull-request-372-3527540380252369916.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397742,,,Mon Jun 09 12:10:38 UTC 2014,,,,,,,,,,"0|i1wh6n:",397869,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;[Date: Wed Jan 08 14:46:09 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for your contribution.
We are still working on the 0.4 release that's why nobody had time to look over your code.
We will merge your code into 0.5-SNAPSHOT once the 0.4 release is out (should be within this week);;;","09/Jun/14 12:10;github-import;[Date: Wed Jan 08 16:41:29 CET 2014, Author: [zentol|https://github.com/zentol]]

done.

i took the project structure that already exists in `master` and adjusted the poms.

tests and example run successfully with the newest code.;;;","09/Jun/14 12:10;github-import;[Date: Sat Jan 18 16:08:59 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hey, again sorry for the delay.
I checked the code out locally to test it. it looks good .. I was just wondering if you did a performance test with it?

I tried to insert 1.000.000 records into a MySQL database and I though this should be done within a few seconds ... I aborted it after a minute or so. It had inserted 30.000 records after that time.
Does anybody has an idea how to speed this up?

EDIT: [Solution|http://stackoverflow.com/a/10617768/568695]: `?useServerPrepStmts=false&rewriteBatchedStatements=true`
The code is here: https://github.com/rmetzger/scratch/tree/collectionAndJDBCTest;;;","09/Jun/14 12:10;github-import;[Date: Sat Jan 18 18:34:58 CET 2014, Author: [zentol|https://github.com/zentol]]

no i haven*t done a performance test, i dont have any data to compare to. Ill look into it, maybe there some big slowdown somewhere.
the solution you linked, does that work only for mysql?;;;","09/Jun/14 12:10;github-import;[Date: Sat Jan 18 18:40:25 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think that simple tuple-by-tuple insertions are not very fast in MySQL.
Typically, speedup comes through bulk-commit (if the storage manager does
transactions) or by bulk insertions/bulk imports.

I would say that the speed is okay as it is for now. Meaning that right now
we cannot push gigabytes fast into the data base. That might not be an
issue for many use cases that read data from the data bases, do the fancy
processing in Stratosphere, and write an aggregated result back.


On Sat, Jan 18, 2014 at 10:09 AM, Robert Metzger
<notifications@github.com>wrote:

> Hey, again sorry for the delay.
> I checked the code out locally to test it. it looks good .. I was just
> wondering if you did a performance test with it?
>
> I tried to insert 1.000.000 records into a MySQL database and I though
> this should be done within a few seconds ... I aborted it after a minute or
> so. It had inserted 30.000 records after that time.
> Does anybody has an idea how to speed this up?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/372#issuecomment-32683956>
> .
>;;;","09/Jun/14 12:10;github-import;[Date: Sat Jan 18 19:02:14 CET 2014, Author: [zentol|https://github.com/zentol]]

@rmetzger could you try the test again, but rearrange the switch statement? like moving the IntValue/StringValue case upwards, I'd be curious of the result.

aside from outside effects, the switch (and the enum) are the only potential slowdowns i can think of.;;;","09/Jun/14 12:10;github-import;[Date: Sat Jan 18 19:04:32 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,

As i said, I solved the problem. I edited the original post on github.

Sent  from my iPhone

On 18.01.2014, at 19:02, zentol <notifications@github.com> wrote:

@rmetzger <https://github.com/rmetzger> could you try the test again, but
rearrange the switch statement? like moving the IntValue/StringValue case
upwards, I'd be curious of the result.

aside from outside effects, the switch (and the enum) are the only
potential slowdowns i can think of.

—
Reply to this email directly or view it on
GitHub<https://github.com/stratosphere/stratosphere/pull/372#issuecomment-32688195>
.;;;","09/Jun/14 12:10;github-import;[Date: Sat Jan 18 19:12:37 CET 2014, Author: [fhueske|https://github.com/fhueske]]

I noticed another thing. The batch is only executed in the close() call.
I don't know how the JDBC driver works internally, but in worst-case everything is cached in memory until the OF is closed. This might lead to high memory consumption.
It might be a better idea to call executeBatch() periodically. Not sure what a good threshold value would be, but an optional user parameter would be good, I think.

~~@rmetzger Is that a generic JDBC solution or specific to MYSQL? In that case, passing optional user config parameters to the JDBC connection string might be a good idea.~~;;;","09/Jun/14 12:10;github-import;[Date: Sat Jan 18 19:18:05 CET 2014, Author: [sscdotopen|https://github.com/sscdotopen]]

My 2 cents: executeBatch will do the trick, pick a reasonable default 
for the interval, don't burden this onto the user.

--sebastian


On 01/18/2014 07:12 PM, Fabian Hueske wrote:
> I noticed another thing. The batch is only executed in the close() call.
> I don't know how the JDBC driver works internally, but in worst-case everything is cached in memory until the OF is closed. This might lead to high memory consumption.
> It might be a better idea to call executeBatch() periodically. Not sure what a good threshold value would be, but an optional user parameter would be good, I think.
>
> @rmetzger Is that a generic JDBC solution or specific to MYSQL? In that case, passing optional user config parameters to the JDBC connection string might be a good idea.
>
> ---
> Reply to this email directly or view it on GitHub:
> https://github.com/stratosphere/stratosphere/pull/372#issuecomment-32688484
>;;;","09/Jun/14 12:10;github-import;[Date: Sat Jan 18 19:21:22 CET 2014, Author: [fhueske|https://github.com/fhueske]]

The inserts are already batched, but the batch is only executed once at the end.
That's why I said **optional** parameter ;-);;;","09/Jun/14 12:10;github-import;[Date: Sat Jan 18 21:53:18 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

@zentol: If you're going to change something, use this branch as a basis: https://github.com/rmetzger/stratosphere/tree/jdbc I did some minor changes to the package structure (and one little thing in the pom).

I'll look into the code to see if we need to change something on the batching. I guess everything is being cached in memory because my test job is dying from a OOM exception if I reduce the heapsize. 
I'll read the JDBC driver documentation.

Without these two parameters, I had an insert rate of 20k/minute, with the parameters, the thing finished in less than 5 secs. (for 1.000.000 records (100 mb or so)) without an index.
;;;","09/Jun/14 12:10;github-import;[Date: Sat Jan 18 23:55:21 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I set the default ""batch interval"" to 5000, but users can configure a custom value for it. (My test with `-Xmx250m` worked, so I assume memory is fine);;;","09/Jun/14 12:10;github-import;[Date: Sun Jan 19 13:34:08 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Okay, I commited the JDBCOutpuFormat in [d1b47f6dd430f1bca5dd0ac5747393767f669a16|https://github.com/stratosphere/stratosphere/commit/d1b47f6dd430f1bca5dd0ac5747393767f669a16] and surrounding commits.
Really good work by @zentol and @emrehasan! The JDBC-stuff is already on most of our presentations ;) Thank you!

Please get in touch if you are interested more contributions to the project.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rework download page & versions as variables,FLINK-371,12719542,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:10,09/Jun/14 12:10,14/Jul/23 05:57,09/Jun/14 12:10,,,,pre-apache,,,,,,,0,github-import,,"We download page now points to the most recent stable release (0.4-rc1)

I also introduced variables for the current stable and snapshot versions.
(I guess we will have some 0.4.1, ... releases soon).


Btw: I promised to work on Christmas!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/371
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue Dec 24 15:35:54 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;pull-request-371-2143524916305613775.patch;https://issues.apache.org/jira/secure/attachment/12649113/pull-request-371-2143524916305613775.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397741,,,Mon Jun 09 12:10:25 UTC 2014,,,,,,,,,,"0|i1wh6f:",397868,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;[Date: Mon Jan 06 17:10:16 CET 2014, Author: [uce|https://github.com/uce]]

Merged in [4b21ac50f638da80d9fd6728f75bdc9ca5274be5|https://github.com/stratosphere/stratosphere/commit/4b21ac50f638da80d9fd6728f75bdc9ca5274be5].

All relevant version-specific  download links etc. are in `_config.yml` now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updated web interface documentation,FLINK-370,12719541,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:10,09/Jun/14 12:10,14/Jul/23 05:57,09/Jun/14 12:10,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/370
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Mon Dec 23 22:59:07 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;pull-request-370-2824437338057035936.patch;https://issues.apache.org/jira/secure/attachment/12649112/pull-request-370-2824437338057035936.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397740,,,2014-06-09 12:10:18.0,,,,,,,,,,"0|i1wh67:",397867,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adapt web interface to new renaming changes,FLINK-369,12719540,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:10,08/Sep/14 15:14,14/Jul/23 05:57,08/Sep/14 15:14,,,,pre-apache,,,,,,,0,github-import,,"The web interface still uses the the term ""Pact program"" and probably other deprecated terms as well.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/369
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, gui, user satisfaction, 
Created at: Mon Dec 23 22:36:25 CET 2013
State: open
",,fhueske,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397739,,,Mon Sep 08 15:14:08 UTC 2014,,,,,,,,,,"0|i1wh5z:",397866,,,,,,,,,,,,,,,,,,,,"08/Sep/14 15:14;fhueske;Has been fixed somewhere on the way of improving the web submission client.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken link,FLINK-368,12719539,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:10,09/Jun/14 12:10,14/Jul/23 05:57,09/Jun/14 12:10,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/368
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Sun Dec 22 19:18:50 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;pull-request-368-7151168062814005075.patch;https://issues.apache.org/jira/secure/attachment/12649111/pull-request-368-7151168062814005075.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397738,,,Mon Jun 09 12:10:13 UTC 2014,,,,,,,,,,"0|i1wh5r:",397865,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;[Date: Sun Dec 22 21:22:36 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I think it is okay that you push such quick fixes directly into `gh-pages`. (Saves us one ... merge pull request .. commit);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed typos,FLINK-367,12719538,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:10,09/Jun/14 12:10,14/Jul/23 05:57,09/Jun/14 12:10,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/367
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Sun Dec 22 00:04:22 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;pull-request-367-6961784232115206621.patch;https://issues.apache.org/jira/secure/attachment/12649110/pull-request-367-6961784232115206621.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397737,,,2014-06-09 12:10:07.0,,,,,,,,,,"0|i1wh5j:",397864,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
updated CLI client options,FLINK-366,12719537,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:10,09/Jun/14 12:10,14/Jul/23 05:57,09/Jun/14 12:10,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/366
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Sat Dec 21 23:57:55 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;pull-request-366-7984437239457868618.patch;https://issues.apache.org/jira/secure/attachment/12649109/pull-request-366-7984437239457868618.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397736,,,2014-06-09 12:10:03.0,,,,,,,,,,"0|i1wh5b:",397863,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
#360: updated CLI client help text after renaming,FLINK-365,12719536,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:09,09/Jun/14 12:10,14/Jul/23 05:57,09/Jun/14 12:10,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/365
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Sat Dec 21 23:44:42 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:10;github-import;pull-request-365-7693301860215123432.patch;https://issues.apache.org/jira/secure/attachment/12649108/pull-request-365-7693301860215123432.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397735,,,2014-06-09 12:09:59.0,,,,,,,,,,"0|i1wh53:",397862,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update setup documentation after refactoring,FLINK-364,12719535,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:09,09/Jun/14 12:09,14/Jul/23 05:57,09/Jun/14 12:09,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/364
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Sat Dec 21 23:11:47 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;pull-request-364-5294669858503936995.patch;https://issues.apache.org/jira/secure/attachment/12649107/pull-request-364-5294669858503936995.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397734,,,2014-06-09 12:09:54.0,,,,,,,,,,"0|i1wh4v:",397861,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
#286: Fixed distributed writing to local filesystem,FLINK-363,12719534,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:09,09/Jun/14 12:09,14/Jul/23 05:57,09/Jun/14 12:09,,,,pre-apache,,,,,,,0,github-import,,"This fix enables local filesystem writes in a cluster setup.
Output directories for local FS writes are created on each TM but not on the JM (except if it hosts a TM as well).

The currently implemented override policy for local filesystem writes is:
- existing files is not deleted to create output directories (to avoid deleting OS files)
- existing files in existing output directories are overwritten

Let me know, if you want to enable deleting local FS files to create output directories. 


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/363
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Sat Dec 21 00:02:04 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;pull-request-363-5800033496426544306.patch;https://issues.apache.org/jira/secure/attachment/12649106/pull-request-363-5800033496426544306.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397733,,,Mon Jun 09 12:09:52 UTC 2014,,,,,,,,,,"0|i1wh4n:",397860,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;[Date: Wed Jan 22 15:46:19 CET 2014, Author: [fhueske|https://github.com/fhueske]]

New PR for this: ([#421|https://github.com/stratosphere/stratosphere/issues/421] | [FLINK-421|https://issues.apache.org/jira/browse/FLINK-421]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
updated CLI client documentation,FLINK-362,12719533,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:09,09/Jun/14 12:09,14/Jul/23 05:57,09/Jun/14 12:09,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/362
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Fri Dec 20 17:58:37 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;pull-request-362-4600190240070752823.patch;https://issues.apache.org/jira/secure/attachment/12649105/pull-request-362-4600190240070752823.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397732,,,2014-06-09 12:09:36.0,,,,,,,,,,"0|i1wh4f:",397859,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Help text of CLI Client needs to be updated,FLINK-360,12719531,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:09,09/Jun/14 12:09,14/Jul/23 05:57,09/Jun/14 12:09,,,,pre-apache,,,,,,,0,github-import,,"The help text contains outdated terminology (Pact, Program Assembler, etc.)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/360
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, documentation, user satisfaction, 
Milestone: Release 0.4
Created at: Fri Dec 20 17:33:36 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397730,,,Mon Jun 09 12:09:33 UTC 2014,,,,,,,,,,"0|i1wh3z:",397857,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;[Date: Sat Dec 21 23:46:19 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Fixed in ([#365|https://github.com/stratosphere/stratosphere/issues/365] | [FLINK-365|https://issues.apache.org/jira/browse/FLINK-365]).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extracted client documentation from Java API description,FLINK-359,12719530,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:09,09/Jun/14 12:09,14/Jul/23 05:57,09/Jun/14 12:09,,,,pre-apache,,,,,,,0,github-import,,"Extracted the client documentation from java api and separated into individual pages for easier management.
Added a new section in the documentation ""Program Execution"".

For now, it is just content copy&paste.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/359
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Fri Dec 20 16:49:39 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;pull-request-359-4697101835793486529.patch;https://issues.apache.org/jira/secure/attachment/12649104/pull-request-359-4697101835793486529.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397729,,,2014-06-09 12:09:27.0,,,,,,,,,,"0|i1wh3r:",397856,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update contribs,FLINK-358,12719529,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:09,09/Jun/14 12:09,14/Jul/23 05:57,09/Jun/14 12:09,,,,pre-apache,,,,,,,0,github-import,,"Replaced Ozone by Stratosphere and fixed spelling of name.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/358
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Fri Dec 20 14:32:26 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;pull-request-358-7979002023870022561.patch;https://issues.apache.org/jira/secure/attachment/12649103/pull-request-358-7979002023870022561.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397728,,,2014-06-09 12:09:23.0,,,,,,,,,,"0|i1wh3j:",397855,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
#339: JDBC tests do not create derby.log file,FLINK-357,12719528,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:09,09/Jun/14 12:09,14/Jul/23 05:57,09/Jun/14 12:09,,,,pre-apache,,,,,,,0,github-import,,"Removes derby log from repository and avoids generation during JDBC tests.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/357
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Fri Dec 20 14:31:04 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;pull-request-357-1122529647291826610.patch;https://issues.apache.org/jira/secure/attachment/12649102/pull-request-357-1122529647291826610.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397727,,,2014-06-09 12:09:18.0,,,,,,,,,,"0|i1wh3b:",397854,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stratosphere 0.4 cannot read from HDFS 0.20.2,FLINK-356,12719527,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:09,09/Jun/14 12:09,14/Jul/23 05:57,09/Jun/14 12:09,,,,pre-apache,,,,,,,0,github-import,,"When trying to read data from HDFS 0.20.2 with Stratosphere 0.4 the following exception is thrown:

```
ERROR: The program execution failed: eu.stratosphere.nephele.executiongraph.GraphConversionException: Cannot compute input splits for CHAIN DataSource(Input Lines) -> Map(Tokenize Lines) -> Combine(Count Words): java.io.IOException: The given file URI (hdfs://host:port/my/input) described the host and port of an HDFS Namenode, but the File System could not be initialized with that address.
	at eu.stratosphere.runtime.fs.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:194)
	at eu.stratosphere.core.fs.FileSystem.get(FileSystem.java:220)
	at eu.stratosphere.core.fs.Path.getFileSystem(Path.java:293)
	at eu.stratosphere.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:393)
	at eu.stratosphere.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:68)
	at eu.stratosphere.pact.runtime.task.DataSourceTask.computeInputSplits(DataSourceTask.java:325)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:552)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:273)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:174)
	at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:551)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:416)
	at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:944)
Caused by: java.lang.NullPointerException
	at eu.stratosphere.runtime.fs.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:191)
	... 15 more

	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:555)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:273)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:174)
	at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:551)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:416)
	at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:944)
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/356
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, 
Milestone: Release 0.4
Created at: Fri Dec 20 14:01:51 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397726,,,Mon Jun 09 12:09:16 UTC 2014,,,,,,,,,,"0|i1wh33:",397853,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;[Date: Fri Dec 20 14:02:32 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

HDFS 0.20.2 is NOT compatible to Hadoop 1.x
HDFS 0.20.203 is.


On Fri, Dec 20, 2013 at 2:01 PM, Fabian Hueske <notifications@github.com>wrote:

> When trying to read data from HDFS 0.20.2 (compatible with HDFS 1.xxx)
> with Stratosphere 0.4 the following exception is thrown:
>
> ERROR: The program execution failed: eu.stratosphere.nephele.executiongraph.GraphConversionException: Cannot compute input splits for CHAIN DataSource(Input Lines) -> Map(Tokenize Lines) -> Combine(Count Words): java.io.IOException: The given file URI (hdfs://host:port/my/input) described the host and port of an HDFS Namenode, but the File System could not be initialized with that address.
>     at eu.stratosphere.runtime.fs.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:194)
>     at eu.stratosphere.core.fs.FileSystem.get(FileSystem.java:220)
>     at eu.stratosphere.core.fs.Path.getFileSystem(Path.java:293)
>     at eu.stratosphere.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:393)
>     at eu.stratosphere.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:68)
>     at eu.stratosphere.pact.runtime.task.DataSourceTask.computeInputSplits(DataSourceTask.java:325)
>     at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:552)
>     at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:273)
>     at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:174)
>     at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:551)
>     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
>     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
>     at java.lang.reflect.Method.invoke(Method.java:597)
>     at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:416)
>     at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:944)
> Caused by: java.lang.NullPointerException
>     at eu.stratosphere.runtime.fs.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:191)
>     ... 15 more
>
>     at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:555)
>     at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:273)
>     at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:174)
>     at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:551)
>     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
>     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
>     at java.lang.reflect.Method.invoke(Method.java:597)
>     at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:416)
>     at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:944)
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/356>
> .
>;;;","09/Jun/14 12:09;github-import;[Date: Fri Dec 20 20:55:30 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Validated @StephanEwen's fix on Hadoop 2.2 and Hadoop 1.0.4

current 0.4-rc1 is working on yarn.;;;","09/Jun/14 12:09;github-import;[Date: Sat Dec 21 22:24:09 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Also working with HDFS 0.20.203 and 0.20.2.
Issue fixed in relaese-0.4-rc1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replaced ozone by stratosphere in contributers file,FLINK-355,12719526,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:09,09/Jun/14 12:09,14/Jul/23 05:57,09/Jun/14 12:09,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/355
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Thu Dec 19 10:16:00 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;pull-request-355-1710274065100773593.patch;https://issues.apache.org/jira/secure/attachment/12649101/pull-request-355-1710274065100773593.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397725,,,Mon Jun 09 12:09:12 UTC 2014,,,,,,,,,,"0|i1wh2v:",397852,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;[Date: Fri Dec 20 14:30:05 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Reopened against 0.4 release candidate;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update documetation on configuration parameters.,FLINK-354,12719525,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:09,09/Jun/14 12:09,14/Jul/23 05:57,09/Jun/14 12:09,,,,pre-apache,,,,,,,0,github-import,,"
@StephanEwen has unified and cleaned up the configuration parameters: https://github.com/StephanEwen/stratosphere/blob/rename/stratosphere-core/src/main/java/eu/stratosphere/configuration/ConfigConstants.java 

These parameters should be listed in the configuration reference. 
Some of the parameters on the current configuration page are obsolete (either not supported any more, or should not be exposed). 
However, not all relevant parameters are also in ConfigConstants.java (Java home, JM + TM heap space, ...).

Let @StephanEwen know if the ConfigConstants are missing something.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/354
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, documentation, 
Created at: Wed Dec 18 23:52:17 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397724,,,Mon Jun 09 12:09:07 UTC 2014,,,,,,,,,,"0|i1wh2n:",397851,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;[Date: Sat Dec 21 23:52:03 CET 2013, Author: [fhueske|https://github.com/fhueske]]

@StephanEwen I believe you fixed this in [d42cd0bf364e948ec73a84e7d90db41f490b5fbd|https://github.com/stratosphere/stratosphere/commit/d42cd0bf364e948ec73a84e7d90db41f490b5fbd], right?;;;","09/Jun/14 12:09;github-import;[Date: Mon Jan 20 23:40:32 CET 2014, Author: [fhueske|https://github.com/fhueske]]

This issue has been resolved in [d42cd0bf364e948ec73a84e7d90db41f490b5fbd|https://github.com/stratosphere/stratosphere/commit/d42cd0bf364e948ec73a84e7d90db41f490b5fbd].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
added configuration of temp IO directory to cluster setup,FLINK-353,12719524,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:09,09/Jun/14 12:09,14/Jul/23 05:57,09/Jun/14 12:09,,,,pre-apache,,,,,,,0,github-import,,"Added configuration of temp IO directory to cluster setup and fixed broken link.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/353
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Wed Dec 18 23:44:22 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:09;github-import;pull-request-353-8084908281343073128.patch;https://issues.apache.org/jira/secure/attachment/12649100/pull-request-353-8084908281343073128.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397723,,,2014-06-09 12:09:02.0,,,,,,,,,,"0|i1wh2f:",397850,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added tab layout for the alternatives in the 'Quickstart » Create Project' sections.,FLINK-352,12719523,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:08,09/Jun/14 12:09,14/Jul/23 05:57,09/Jun/14 12:09,,,,pre-apache,,,,,,,0,github-import,,"Closes issue ([#351|https://github.com/stratosphere/stratosphere/issues/351] | [FLINK-351|https://issues.apache.org/jira/browse/FLINK-351]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/352
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Created at: Wed Dec 18 21:10:07 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:08;github-import;pull-request-352-3377406659964743214.patch;https://issues.apache.org/jira/secure/attachment/12649099/pull-request-352-3377406659964743214.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397722,,,Mon Jun 09 12:09:00 UTC 2014,,,,,,,,,,"0|i1wh27:",397849,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:08;github-import;[Date: Thu Dec 19 15:58:12 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Looks great. I think we can merge it.;;;","09/Jun/14 12:09;github-import;[Date: Mon Jan 06 10:54:19 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

Do I have to do something else on that?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"java quickstart ""one of the following"" is misleading",FLINK-351,12719522,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:08,09/Jun/14 12:08,14/Jul/23 05:57,09/Jun/14 12:08,,,,pre-apache,,,,,,,0,github-import,,"http://stratosphere.eu/quickstart/java.html

""Use one of the following commands to create a project:""

and then there is 1. and 2.
but people don't read the sentence.
Two of the new stratosphere commiters ran into the problem.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/351
Created by: [rmetzger|https://github.com/rmetzger]
Labels: documentation, 
Created at: Wed Dec 18 17:04:53 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397721,,,Mon Jun 09 12:08:55 UTC 2014,,,,,,,,,,"0|i1wh1z:",397848,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:08;github-import;[Date: Wed Dec 18 20:56:51 CET 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

I have a fix for this. Should be pushed on my fork in ~ 5 mins if the asymmetric DSL gods are with us.;;;","09/Jun/14 12:08;github-import;[Date: Tue Jan 07 19:16:19 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

I just noticed that now all the description on the quickstart is withing a p.lead. Do we want to keep it like that - my understanding is that the .lead class should be used for first sentences only.;;;","09/Jun/14 12:08;github-import;[Date: Tue Jan 07 19:21:20 CET 2014, Author: [uce|https://github.com/uce]]

Your understanding is right. It was just a matter of ""style"" to make the text bigger for this special page, because (imho) the font size doesn't work well for short texts.;;;","09/Jun/14 12:08;github-import;[Date: Tue Jan 07 19:25:48 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

I agree that the normal text size feels too small compared to the headers and leads, but if we use the .lead class things shift too much in the other directions (at least on my screen) - it looks as if you're shouting the instructions with large letters (especially as the text in the other section is smaller).

If we all agree that this the normal text size is a bit too small I think the appropriate way to fix this is to redefine the text-size css attribute in bootstrap.css.

Should I open an issue for that?;;;","09/Jun/14 12:08;github-import;[Date: Tue Jan 07 19:30:28 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

@aalexandrov: I agree with you that the font size in the quickstart is a bit too large.
The http://stratosphere.eu/downloads/ page uses the `lead`-class appropriately (probably still a little bit too much).

On the other hand, we should make sure that the pages look friendly (not like large text blocks you've to work through). I think this was Ufuk's intention and he did a good job!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not all JobClient ctors initialize accumulatorProtocolProxy,FLINK-349,12719520,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:08,09/Jun/14 12:08,14/Jul/23 05:57,09/Jun/14 12:08,,,,pre-apache,,,,,,,0,github-import,,"Leads to NPE during job execution.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/349
Created by: [AHeise|https://github.com/AHeise]
Labels: bug, 
Assignee: [andrehacker|https://github.com/andrehacker]
Created at: Wed Dec 18 14:14:26 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397719,,,Mon Jun 09 12:08:47 UTC 2014,,,,,,,,,,"0|i1wh1j:",397846,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:08;github-import;[Date: Wed Dec 18 14:20:14 CET 2013, Author: [andrehacker|https://github.com/andrehacker]]

Thanks @AHeise for reporting, I will look into this now and I already see the constructor which is missing the initialization.
Not sure when this is called (would like to cover via testcase) - do you know how to reproduce?;;;","09/Jun/14 12:08;github-import;[Date: Wed Dec 18 14:22:20 CET 2013, Author: [AHeise|https://github.com/AHeise]]

I use it in sopremo-testserver SopremoServerIT#testSuccessfulExecution. 

But it should be a copy and paste fix. You could also direct the second ctor to the third ctor and build the last param from the config.;;;","09/Jun/14 12:08;github-import;[Date: Wed Dec 18 14:29:53 CET 2013, Author: [andrehacker|https://github.com/andrehacker]]

Yes, the fix is one line, I will do it as proposed.;;;","09/Jun/14 12:08;github-import;[Date: Wed Dec 18 14:31:23 CET 2013, Author: [AHeise|https://github.com/AHeise]]

Also you could make the proxy final, than the error would be reported by the compiler. (good advertisement for using finals wherever possible ;));;;","09/Jun/14 12:08;github-import;[Date: Wed Jan 15 14:46:05 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in PR4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation for accumulators,FLINK-346,12719517,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:08,09/Jun/14 12:08,14/Jul/23 05:57,09/Jun/14 12:08,,,,pre-apache,,,,,,,0,github-import,,"First version of accumulator documentation. Click on View file below to get a preview.
I also removed the Best Practice note, because the link was broken and I think it was redundant (a few lines below the mentioned section begins). There are many links broken actually.
Feel free to comment.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/346
Created by: [andrehacker|https://github.com/andrehacker]
Labels: 
Created at: Tue Dec 17 19:53:39 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:08;github-import;pull-request-346-8276880203866674475.patch;https://issues.apache.org/jira/secure/attachment/12649098/pull-request-346-8276880203866674475.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397716,,,2014-06-09 12:08:32.0,,,,,,,,,,"0|i1wh0v:",397843,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log rotation for start-local.bat,FLINK-345,12719516,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:08,09/Jun/14 12:08,14/Jul/23 05:57,09/Jun/14 12:08,,,,pre-apache,,,,,,,0,github-import,,"A simple log rotation for the windows script, same as for linux in start-local.sh

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/345
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Tue Dec 17 15:23:53 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:08;github-import;pull-request-345-2027642871971642520.patch;https://issues.apache.org/jira/secure/attachment/12649097/pull-request-345-2027642871971642520.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397715,,,Mon Jun 09 12:08:31 UTC 2014,,,,,,,,,,"0|i1wh0n:",397842,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:08;github-import;[Date: Mon Jan 06 17:21:24 CET 2014, Author: [uce|https://github.com/uce]]

Can someone on Windows confirm this?;;;","09/Jun/14 12:08;github-import;[Date: Wed Jan 29 16:20:21 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Markus, can you merge/rebase this to the latest version and confirm that it still works?;;;","09/Jun/14 12:08;github-import;[Date: Mon Feb 24 14:27:03 CET 2014, Author: [markus-h|https://github.com/markus-h]]

I rebased and tested and for me (Windows 8) it is still working.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local Setup docu for Windows,FLINK-344,12719515,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:08,09/Jun/14 12:08,14/Jul/23 05:57,09/Jun/14 12:08,,,,pre-apache,,,,,,,0,github-import,,"I adjusted the local setup documentation to include a manual for running in windows, using either cygwin or the new batch scripts.
For a preview see http://markus-h.github.io/stratosphere/docs/setup/local.html

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/344
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Tue Dec 17 15:09:05 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:08;github-import;pull-request-344-6779416731917900143.patch;https://issues.apache.org/jira/secure/attachment/12649096/pull-request-344-6779416731917900143.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397714,,,Mon Jun 09 12:08:25 UTC 2014,,,,,,,,,,"0|i1wh0f:",397841,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:08;github-import;[Date: Tue Dec 17 17:50:22 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Thanks!
I fixed some typos and send you a pull request (https://github.com/markus-h/stratosphere/pull/1);;;","09/Jun/14 12:08;github-import;[Date: Mon Jan 06 17:20:50 CET 2014, Author: [uce|https://github.com/uce]]

Merged in [5b468b75caccae27e7513238b0bd11f6a3d17d40|https://github.com/stratosphere/stratosphere/commit/5b468b75caccae27e7513238b0bd11f6a3d17d40].

I changed `pact-client.bat` to `stratosphere.bat`.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Draft for renaming members of stratosphere-core,FLINK-343,12719514,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:08,09/Jun/14 12:08,14/Jul/23 05:57,09/Jun/14 12:08,,,,pre-apache,,,,,,,0,github-import,,"Until now, only ```stratosphere-core``` was renamed.

Comments welcome.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/343
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Sat Dec 14 03:21:38 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:08;github-import;pull-request-343-6082980707344827481.patch;https://issues.apache.org/jira/secure/attachment/12649095/pull-request-343-6082980707344827481.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397713,,,Mon Jun 09 12:08:20 UTC 2014,,,,,,,,,,"0|i1wh07:",397840,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:08;github-import;[Date: Sat Dec 14 19:15:38 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Preliminary refactoring protocol:
```
package names:

 eu.stratosphere.nephele.configuration      -->  eu.stratosphere.configuration
 eu.stratosphere.nephele.util               -->  eu.stratosphere.util
 eu.stratosphere.nephele.fs                 -->  eu.stratosphere.core.fs
 eu.stratosphere.nephele.fs.file            -->  eu.stratosphere.core.fs.local
 eu.stratosphere.nephele.protocols          -->  eu.stratosphere.core.protocols
 eu.stratosphere.nephele.io                 --> eu.stratosphere.core.io
 eu.stratosphere.nephele.services.memorymanager  -->  eu.stratosphere.core.memory
 
 
 eu.stratosphere.pact.common.util           -->  eu.stratosphere.util
 eu.stratosphere.pact.generic.stub          -->  eu.stratosphere.api.functions
 eu.stratosphere.pact.common.plan           -->  eu.stratosphere.api.plan
 eu.stratosphere.pact.common.io             -->  eu.stratosphere.api.io 
 eu.stratosphere.pact.common.io.statistics  --> eu.stratosphere.api.io.statistics 
 eu.stratosphere.pact.generic.io            -->  eu.stratosphere.api.io
 eu.stratosphere.pact.generic.types         -->  eu.stratosphere.api.typeutils
 eu.stratosphere.pact.common.distributions  --> eu.stratosphere.api.distributions
 eu.stratosphere.pact.common.type.base.parser  -->  eu.stratosphere.types.parser
 eu.stratosphere.pact.common.type              -->  eu.stratosphere.types
 eu.stratosphere.pact.common.type.base         -->  eu.stratosphere.types
 eu.stratosphere.pact.common.stubs             --> eu.stratosphere.api.record.functions
 eu.stratosphere.pact.common.io                --> eu.stratosphere.api.record.io
 eu.stratosphere.pact.common.contract          --> eu.stratosphere.api.record.operators 
 
 eu.stratosphere.pact.client.nephele.api       --> eu.stratosphere.client.program
 
 eu.stratosphere.pact.compiler.*      -->  eu.stratosphere.compiler.*
             compiler.plan            -->  compiler.dag
             ompiler.plan.candidate   -->  compiler.plan

 class names:
 
 eu.stratosphere.pact.common.io.RecordOutputFormat -->   eu.stratosphere.pact.common.io.CsvOutputFormat
 eu.stratosphere.pact.common.io.RecordInputFormat  -->   eu.stratosphere.pact.common.io.CsvInputFormat


 PactProgram  --> PackagedProgram
 *Contract    --> *Operator
 Match*       --> Join*
 
 eu.stratosphere.nephele.io.Record       -->   Removed and Replaced by eu.stratosphere.core.io.IOReadableWritable  (later to be replaced by Hadoop Writable, if we want easy dropin of hadoop types)
 
 
Moved classes:
 
 FieldSet
 FieldList
 PactConfigConstants           --> eu.stratosphere.configuration
 AsciiUtils                    --> eu.stratosphere.util
 IllegalConfigurationException --> eu.stratosphere.configuration
 StringRecord                  --> eu.stratosphere.core.io
 ```
;;;","09/Jun/14 12:08;github-import;[Date: Sat Dec 14 20:05:52 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Cool!

I vote for renaming `stratosphere-scala-api` to `stratosphere-scala`. (same for `java-api`)
only `stratosphere-record-api` should end with `-api`.
`stratosphere-runtime` still contains a `nephele` in its package names?


;;;","09/Jun/14 12:08;github-import;[Date: Sat Dec 14 20:09:10 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The runtime packages should also be renamed, but I thought that less
critical (and postponed it). They are not part of a public api after all.
Am 14.12.2013 20:05 schrieb ""Robert Metzger"" <notifications@github.com>:

> Cool!
>
> I vote for renaming stratosphere-scala-api to stratosphere-scala. (same
> for java-api)
> only stratosphere-record-api should end with -api.
> stratosphere-runtime still contains a nephele in its package names?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/343#issuecomment-30583525>
> .
>;;;","09/Jun/14 12:08;github-import;[Date: Mon Dec 16 13:26:45 CET 2013, Author: [AHeise|https://github.com/AHeise]]

Sound good to me.

But why 
compiler.plan            -->  compiler.dag? Don't you have more than DAGs now.

I'm also not completely convinced by
 PactProgram  --> PackagedProgram
 *Contract    --> *Operator
 Match*       --> Join*

Why PackagedProgram? Sounds a bit like a jar ;)
Why Operator? It's not bad at all, but I still like to know the reasoning.
Match should be EquiJoin.;;;","09/Jun/14 12:08;github-import;[Date: Tue Dec 17 10:56:34 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Commented in wrong PR, sorry...

I had a look at the PR and here are some comments:

- The overall structure looks very good and much more organized than before! :thumbsup:
- We should put a README file into the stratosphere-java-api module which tells the user that this will be a new Java API which is not done yet.
- stratosphere-runtime has a lot of ""legacy""-terminology but that's not exposed to the user and can be changed later.
- The package structure of stratosphere-client looks a bit messy (same as before). Do we want to clean that up a bit as well? Shouldn't be too much effort since its only a few classes.
- The project is compiling with and without tests for me, but I did not check if the build artifact is working.
- I had a closer look at stratosphere-core module and propose the following changes:

  - eu.stratosphere.accumulators -> eu.stratosphere.api.accumulators
  - eu.str.api.function.GenericMatcher -> eu.str.func.GenericJoiner
  - eu.str.api.function.Stub -> eu.str.func.Function (or FunctionStub if Function is a key word in Scala?)
  - eu.str.api.function.StubAnnotation -> eu.str.func.FunctionAnnotation
  - eu.str.api.function.GenericStub -> eu.str.func.GenericFunction(Stub)
  - eu.str.api.operators.AbstractPact -> eu.str.op.AbstractOperator
  - eu.str.api.operator.Contract -> eu.str.api.op.Operator (??)
  - eu.str.api.operator.(SingleInputContract, DualInputContract, IterationContract) -> eu.str.api.op.(SingleInputOperator, ...)
  - eu.str.api.operator.base.(XyzContract) -> eu.str.api.operator.base.(XyzOperator)
  - eu.str.api.operator.util.StubAnnotationConfigurable -> eu.str.api.op.util.FunctionAnnotationConfigurable
  - eu.str.configuration.PactConfigConstants -> merge with ConfigConstants (??)
  - eu.str.types.(PactXyz) -> some other prefix. I prefer (SXyz: SInteger, SDouble, ...) but maybe someone has a better idea

- Proposed changes for stratosphere-record-api:
  - eu.stratosphere.api.record.function.MatchStub -> eu.str.api.rec.func.JoinFunction
  - eu.stratosphere.api.record.function.(XyzStub) -> eu.str.api.rec.func.(XyzFunction)
  - Shouldn't PactRecord be part of stratosphere-record-api instead of stratosphere-core? Or do you want to avoid the dependency of str-runtime on str-record-api?;;;","09/Jun/14 12:08;github-import;[Date: Tue Dec 17 12:42:14 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

@AHeise 

Here are the rationals behind the renamings. Please try to evaluate the names from the perspective of someone how does not know the code and concepts, yet, and needs to find an intuitive api.

compiler.dag is the optimizer's representation of the DAG. The iterations are a meta operator, they do not make the data flow cyclic. So I think dag should be accurate. optimizer.plan is the actually created optimizer plan.

In general, we tried to make names more intuitive , it at the cost of being less technically precise at times. The decision to move away from Contract came from the idea to move away from the terminology of pact, which were always a bit confusing. A new user has no idea what a contract is supposed to be, while operator is an approachable term.

The same is actually the rational for Match --> Join. Because match is an inner equi-join with a udf, just calling it join makes it more graspable, I think. It is true that EquiJoin is technically more correct, but since equi-join is the default, we kept it simple here.

PackagedProgram is really a self contained program jar, with optional parameters. In contrast to that is the PlanWithJars which describes a Plan (program) with optional jar attachments for the UDFs. We use them in different circumstances: The PackagedProgram is the jar that you can upload and simply invoke from the web client or command line client. The PlanWithJars is intended to be used with the Local/Remote Executors (and later with the shell-like interface), where you do not need to package the program to send it to the cluster for execution. You only need to attach libraries, if you used them.;;;","09/Jun/14 12:08;github-import;[Date: Tue Dec 17 18:48:20 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

FullAck Stephan!
I had a long meeting today at the Telekom to discuss the Stratosphere Code I've written for them (a non trivial amount of code) and they totally agreed that ""Contract"" is very contra-intuitive, the same applies to all the Pact-prefixed stuff.
Someone of them also told me that he quickly looses interest in reading texts about Stratosphere because we introduce so many new names for our stuff (nephele, pact, Sky?, contract, ...).

;;;","09/Jun/14 12:08;github-import;[Date: Wed Dec 18 20:00:15 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Could we please rename SequentialInputFormat (and the output format). I know what it does, but I would never have guessed that from the name.

For the java record api RecordInputFormat would be a good name since it does exactly that, read records from the binary stream using their read() method. I do realize, however, that the name is a bit tainted because CsvInputFormat used to be called that (is still). It't still a good name for what it does, though.

For scala I propose ObjectInputFormat, because that is exactly how it behaves. If you had a sink with `ObjectOutputFormat[(Int, String)]` you could read the output files with a source of `ObjectInputFormat[(Int, String)]`.;;;","09/Jun/14 12:08;github-import;[Date: Fri Jan 03 13:07:02 CET 2014, Author: [AHeise|https://github.com/AHeise]]

eu.stratosphere.api.common.operators.util.ContractUtil should be renamed to OperatorUtil
what about renaming JoinFunction#match to join?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code Projects Reorganization,FLINK-342,12719513,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:07,09/Jun/14 12:08,14/Jul/23 05:57,09/Jun/14 12:08,,,,pre-apache,,,,,,,0,github-import,,"Current Status:
  - Added stubs for reorganized projects.
  - Packages reorganized

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/342
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Fri Dec 13 18:13:31 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397712,,,Mon Jun 09 12:08:10 UTC 2014,,,,,,,,,,"0|i1wgzz:",397839,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:08;github-import;Unable to add patch as an attachment, since its larger than 10 MB;;;","09/Jun/14 12:08;github-import;[Date: Fri Dec 13 21:25:33 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Current project structure with dependencies:

![picture1|https://f.cloud.github.com/assets/1727146/1745838/abd6799a-6434-11e3-92db-cd457f4062e0.png]
;;;","09/Jun/14 12:08;github-import;[Date: Fri Dec 13 21:33:15 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Still a bug with the dependencies when using hadoop 2.x. I think it is a conflicting version of the ASM library.;;;","09/Jun/14 12:08;github-import;[Date: Fri Dec 13 21:39:20 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fix: Added ASM exclusion to hadoop dependency in hadoop v2 profile.;;;","09/Jun/14 12:08;github-import;[Date: Fri Dec 13 22:42:23 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

The webinterface does not start properly
```
22:40:56,590 ERROR eu.stratosphere.nephele.jobmanager.JobManager                 - Cannot start jobmanager web info server. The directory containing the web documents does not exist: /home/robert/Projekte/ozone/ozone/stratosphere-dist/target/stratosphere-dist-0.4-SNAPSHOT-bin/stratosphere-0.4-SNAPSHOT/bin/../conf/.././resources/web-docs-infoserver
```

In the `bin.xml`:
```
		<fileSet>
			<!-- copy files for Jobmanager web frontend -->
			<directory>../nephele/nephele-server/resources</directory>
			<outputDirectory>resources</outputDirectory>
			<fileMode>0644</fileMode>
			<excludes>
				<exclude>*etc/users</exclude>
			</excludes>
		</fileSet>
```

This does not look so nice
```
eu.stratosphere:stratosphere-java-examples
```
What do you think about removing the `stratosphere-` prefix from all projects?


pact-array-datamodel still depends on pact-client, this causes maven to download those dependencies from sonatype ;)
```
<		dependency>
			<groupId>eu.stratosphere</groupId>
			<artifactId>pact-clients</artifactId>
			<version>${project.version}</version>
		</dependency>
```
(use `mvn dependency:tree` to find those problems);;;","09/Jun/14 12:08;github-import;[Date: Fri Dec 13 22:43:55 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you base a pull request on my one and submit a fix?
Am 13.12.2013 22:42 schrieb ""Robert Metzger"" <notifications@github.com>:

> The webinterface does not start properly
>
> 22:40:56,590 ERROR eu.stratosphere.nephele.jobmanager.JobManager                 - Cannot start jobmanager web info server. The directory containing the web documents does not exist: /home/robert/Projekte/ozone/ozone/stratosphere-dist/target/stratosphere-dist-0.4-SNAPSHOT-bin/stratosphere-0.4-SNAPSHOT/bin/../conf/.././resources/web-docs-infoserver
>
> I guess the maven assembly settings are wrong?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/342#issuecomment-30545976>
> .
>;;;","09/Jun/14 12:08;github-import;[Date: Fri Dec 13 22:45:19 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I'll open a PR tomorrow, ok. I'll update the previous comment.;;;","09/Jun/14 12:08;github-import;[Date: Fri Dec 13 23:17:23 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I updated the assembly. It was sill pointing to pact-clients and nephele-server.;;;","09/Jun/14 12:08;github-import;[Date: Fri Dec 13 23:19:44 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I am okay with removing the ```stratosphere-``` prefix from all artifact names.;;;","09/Jun/14 12:08;github-import;[Date: Fri Dec 13 23:41:45 CET 2013, Author: [uce|https://github.com/uce]]

Cool stuff! You really accomplished a lot! :)

The only name I am unsure about is the current Java API. Maybe let's add `java` to it? Like `java-record-api`?

> On 13 Dec 2013, at 23:19, Stephan Ewen <notifications@github.com> wrote:
> 
> I am okay with removing the stratosphere- prefix from all artifact names.
> 
> —
> Reply to this email directly or view it on GitHub.;;;","09/Jun/14 12:08;github-import;[Date: Sat Dec 14 03:22:19 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

For renaming, see ([#343|https://github.com/stratosphere/stratosphere/issues/343] | [FLINK-343|https://issues.apache.org/jira/browse/FLINK-343]) ;;;","09/Jun/14 12:08;github-import;[Date: Sat Dec 14 10:16:05 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I'm against adding empty projects into a version that is going to be released soon! (java api)
;;;","09/Jun/14 12:08;github-import;[Date: Sat Dec 14 13:28:02 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

This is only to give a look at what it will be like. We can remove the stub project before we merge it into the main line.;;;","09/Jun/14 12:08;github-import;[Date: Sat Dec 14 14:05:17 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

This PR is subsumed by ([#343|https://github.com/stratosphere/stratosphere/issues/343] | [FLINK-343|https://issues.apache.org/jira/browse/FLINK-343]) ;;;","09/Jun/14 12:08;github-import;[Date: Tue Dec 17 11:36:22 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Very well, the suggestions look good to me. I would actually remove the
java api for now.

One question I have been playing around in my head with its whether we
should use 'udf' instead of 'function'.
Am 17.12.2013 10:54 schrieb ""Fabian Hueske"" <notifications@github.com>:

> I had a look at the PR and here are some comments:
>
>    - The overall structure looks very good and much more organized than
>    before! [image: :thumbsup:]
>    - We should put a README file into the stratosphere-java-api module
>    which tells the user that this will be a new Java API which is not done yet.
>    - stratosphere-runtime has a lot of ""legacy""-terminology but that's
>    not exposed to the user and can be changed later.
>    - The package structure of stratosphere-client looks a bit messy (same
>    as before). Do we want to clean that up a bit as well? Shouldn't be too
>    much effort since its only a few classes.
>    - The project is compiling with and without tests for me, but I did
>    not check if the build artifact is working.
>    -
>
>    I had a closer look at stratosphere-core module and propose the
>    following changes:
>     - eu.stratosphere.accumulators -> eu.stratosphere.api.accumulators
>       - eu.str.api.function.GenericMatcher -> eu.str.func.GenericJoiner
>       - eu.str.api.function.Stub -> eu.str.func.Function (or FunctionStub
>       if Function is a key word in Scala?)
>       - eu.str.api.function.StubAnnotation ->
>       eu.str.func.FunctionAnnotation
>       - eu.str.api.function.GenericStub ->
>       eu.str.func.GenericFunction(Stub)
>       - eu.str.api.operators.AbstractPact -> eu.str.op.AbstractOperator
>       - eu.str.api.operator.Contract -> eu.str.api.op.Operator (??)
>       - eu.str.api.operator.(SingleInputContract, DualInputContract,
>       IterationContract) -> eu.str.api.op.(SingleInputOperator, ...)
>       - eu.str.api.operator.base.(*Contract) ->
>       eu.str.api.operator.base.(*Operator)
>       - eu.str.api.operator.util.StubAnnotationConfigurable ->
>       eu.str.api.op.util.FunctionAnnotationConfigurable
>       - eu.str.configuration.PactConfigConstants -> merge with
>       ConfigConstants (??)
>       - eu.str.types.(Pact*) -> some other prefix. I prefer (S*:
>       SInteger, SDouble, ...) but maybe someone has a better idea
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/342#issuecomment-30738268>
> .
>;;;","09/Jun/14 12:08;github-import;[Date: Tue Dec 17 11:53:36 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I had copied and extended my comments to the updated PR ([#343|https://github.com/stratosphere/stratosphere/issues/343] | [FLINK-343|https://issues.apache.org/jira/browse/FLINK-343]) (this PR is the subsumed one).
I suggest you do the same to have everything in one place. Please check also my extended comments.

I don't have a clear preference whether to use function or udf. UDF is more specific but also DB-terminology and not every user might be able to make sense of that. If I have to choose, I'd pick function because its easier to understand. But as I said, UDF would also be fine...

I am closing this PR to avoid further confusion between the PRs, OK?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debian package creator has references to old config files,FLINK-341,12719512,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:07,09/Jun/14 12:07,14/Jul/23 05:57,09/Jun/14 12:07,,,,pre-apache,,,,,,,0,github-import,,"The ""postinst"" file in ""stratosphere-dist/src/deb/control"" still references ""pact-user.xml""

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/341
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Fri Dec 13 16:09:00 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397711,,,Mon Jun 09 12:07:37 UTC 2014,,,,,,,,,,"0|i1wgzr:",397838,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:07;github-import;[Date: Fri Jan 17 15:48:14 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

```
root@precise64:/etc/apt/sources.list.d# apt-get install stratosphere-dist
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following NEW packages will be installed:
  stratosphere-dist
0 upgraded, 1 newly installed, 0 to remove and 152 not upgraded.
Need to get 55.5 MB of archives.
After this operation, 62.2 MB of additional disk space will be used.
Get:1 http://dev.stratosphere.eu/apt/ ./ stratosphere-dist 0.2 [55.5 MB]
Fetched 55.5 MB in 12s (4,363 kB/s)                                                                                                                                                                               
Selecting previously unselected package stratosphere-dist.
(Reading database ... 54604 files and directories currently installed.)
Unpacking stratosphere-dist (from .../stratosphere-dist_0.2_all.deb) ...
Processing triggers for ureadahead ...
Setting up stratosphere-dist (0.2) ...
sed: can't read /usr/share/stratosphere-dist/conf/pact-user.xml: No such file or directory
dpkg: error processing stratosphere-dist (--configure):
 subprocess installed post-installation script returned error exit status 2
Errors were encountered while processing:
 stratosphere-dist
E: Sub-process /usr/bin/dpkg returned an error code (1)
```

Is there a Debian/Ubuntu user who's willing to fix this?;;;","09/Jun/14 12:07;github-import;[Date: Thu Feb 06 08:55:35 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

duplicate of https://github.com/stratosphere/stratosphere/issues/247;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implemented accumulators,FLINK-340,12719511,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:07,09/Jun/14 12:07,14/Jul/23 05:57,09/Jun/14 12:07,,,,pre-apache,,,,,,,0,github-import,,"Now everything is in a single commit and merged with the current master.

Old pull request with discussion: https://github.com/stratosphere/stratosphere/pull/320

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/340
Created by: [andrehacker|https://github.com/andrehacker]
Labels: 
Created at: Thu Dec 12 15:11:26 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:07;github-import;pull-request-340-6014136265919934119.patch;https://issues.apache.org/jira/secure/attachment/12649094/pull-request-340-6014136265919934119.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397710,,,Mon Jun 09 12:07:32 UTC 2014,,,,,,,,,,"0|i1wgzj:",397837,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:07;github-import;[Date: Thu Dec 12 15:56:59 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I tested the code on the Telekom cluster (2 nodes) and I got the same result as the keyless reducer (and Hive).
I needed to add a open() method and one line inside my reducer() .. so the whole thing is very usable.

I will open a pull request to andre that shows the accumulator results after the pact-client has finished (only if started with `--wait`)
It will look like this
```
rmetzger@hadoop01:~$ ./runPersons.sh 
Job Runtime: 85300
Accumulator Results: 
- numGroups (java.lang.Integer): 39486796
```;;;","09/Jun/14 12:07;github-import;[Date: Thu Dec 12 20:35:20 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Do you think it would be a cool addition to have a way to explicitly disable the accumulators?
I mainly use them for debugging, people probably want to disable them for performance reasons in some point?
(On the other hand,  it is nice to see them in operation to get an idea of your data)

Another nice thing (for debugging) would be something like Standard accumulators that count how many records go in and out each Operator. I currently add them manually, but ideally I would like to add something like
`implement ProfileOperator` to my operators.
`implement StandardAccumulator`  or as a property to the `MapOperator.enableAccumulatorProfiling(true);`
What do you think?


I don't know if it is a good idea that you can call getAccumulator() on non-existing accumulators.
I just had the case that I copy pasted my accumulators from some mappers to a match (for counting incoming and outgoing tuples) and I accidentally forgot to change the name. Having a ""createAccumulator"" method could throw a exception ""accu already exists""
I now use this construct to avoid the mentioned problem:
```
cnt = getRuntimeContext().getIntCounter(getRuntimeContext().getTaskName()+"": counter"");
```;;;","09/Jun/14 12:07;github-import;[Date: Fri Dec 13 11:46:47 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Should we have the WordCount with Accumulators packed into an example JAR? Or is the code mainly for illustrative purposes?;;;","09/Jun/14 12:07;github-import;[Date: Fri Dec 13 11:49:56 CET 2013, Author: [andrehacker|https://github.com/andrehacker]]

First my reply to Stephan, since he is merging:
Currently I have a separate jar, but I think we don't need it. You can remove the section from the pom if you like.;;;","09/Jun/14 12:07;github-import;[Date: Fri Dec 13 11:54:42 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I think we don't need this as a packaged example job. 
We should definitely keep the code and link it from the documentation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectedComponentsNepheleITCase fails on Travis,FLINK-338,12719509,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:07,09/Jun/14 12:07,14/Jul/23 05:57,09/Jun/14 12:07,,,,pre-apache,,,,,,,0,github-import,,"```
Running eu.stratosphere.pact.test.iterative.nephele.ConnectedComponentsNepheleITCase Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.124 sec <<< FAILURE!
```

also eu.stratosphere.pact.test.pactPrograms.WordCountUnionReduceITCase
see https://travis-ci.org/stratosphere/stratosphere/jobs/15221965
https://travis-ci.org/stratosphere/stratosphere/jobs/15066210
https://travis-ci.org/stratosphere/stratosphere/jobs/15298533
https://travis-ci.org/stratosphere/stratosphere/jobs/15298534

I'm already looking into maven to see why it accepts errored tests (I though this was already fixed)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/338
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, runtime, 
Created at: Thu Dec 12 09:22:37 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397708,,,Mon Jun 09 12:07:19 UTC 2014,,,,,,,,,,"0|i1wgz3:",397835,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:07;github-import;[Date: Thu Dec 12 12:55:04 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I noticed that as well and restructured the test. The Problem is not that the tests themselves fail, but the class has many static subclasses with tests, which is not properly handled.;;;","09/Jun/14 12:07;github-import;[Date: Thu Dec 12 13:02:30 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [cfad3eb34595ee7efaf8f818f4bd8561a50838fd|https://github.com/stratosphere/stratosphere/commit/cfad3eb34595ee7efaf8f818f4bd8561a50838fd];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed all JavaDoc warnings in pact-common.,FLINK-337,12719508,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:07,14/Nov/14 14:45,14/Jul/23 05:57,09/Jun/14 12:07,,,,pre-apache,,,,,,,0,github-import,,"Fixed all JavaDoc warnings in pact-common. 
I focused on fixing the build warnings but also added and updated some JavaDoc on the way.

Adresses issue ([#330|https://github.com/stratosphere/stratosphere/issues/330] | [FLINK-330|https://issues.apache.org/jira/browse/FLINK-330]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/337
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Wed Dec 11 17:50:03 CET 2013
State: closed
",,githubbot,github-import,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:07;github-import;pull-request-337-8997712741697485992.patch;https://issues.apache.org/jira/secure/attachment/12649093/pull-request-337-8997712741697485992.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397707,,,Fri Nov 14 14:45:58 UTC 2014,,,,,,,,,,"0|i1wgyv:",397834,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:07;github-import;[Date: Thu Dec 12 19:51:44 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [cfd0ac3d20ef78cab8d0b191a6a383650bad625d|https://github.com/stratosphere/stratosphere/commit/cfd0ac3d20ef78cab8d0b191a6a383650bad625d];;;","14/Nov/14 14:34;githubbot;GitHub user zentol opened a pull request:

    https://github.com/apache/incubator-flink/pull/202

    [FLINK-337] [FLINK-671] Generic Interface / PAPI

    This PR contains the new Generic Language Interface and the Python API built on top of it.
    
    This version hasn't been tested yet on a cluster, this will be done over the weekend. I'm making the PR already so that the reviewing portion starts earlier. (since only minor changes will be necessary to make it work)
    
    I will mark several parts where i specifically would like some input on.
    
    Relevant issues:
    Ideally, [FLINK-1040] will be merged before this is one, as it removes roughly 600 lines of very much hated code in the PlanBinder.
    
    A while ago the distributed cache was acting up, not maintaining files across subsequent operations. I will verify whether this issue still exists while testing. That would not strictly be a blocking issue, as it stands i could work around that (with the caveat that a few files will remain in the tmp folder).

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/zentol/incubator-flink papipr

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-flink/pull/202.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #202
    
----
commit 1db16983e2fa784c7fd3ab3e29e32edcf271de7b
Author: zentol <s.motsu@web.de>
Date:   2014-11-14T12:37:24Z

    [FLINK-337] Generic Language Interface

commit f5b10f4fe5c0d78c89ff177808aec3a0d0487489
Author: zentol <s.motsu@web.de>
Date:   2014-11-14T12:39:00Z

    [FLINK-671] Python API

----
;;;","14/Nov/14 14:37;githubbot;Github user zentol commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/202#discussion_r20361477
  
    --- Diff: flink-addons/flink-language-binding/src/main/java/org/apache/flink/languagebinding/api/java/python/PythonPlanBinder.java ---
    @@ -0,0 +1,376 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE
    + * file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the
    + * License. You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + */
    +package org.apache.flink.languagebinding.api.java.python;
    +
    +import java.io.IOException;
    +import java.net.URI;
    +import java.net.URISyntaxException;
    +import java.util.Arrays;
    +import java.util.HashMap;
    +import org.apache.flink.api.java.DataSet;
    +import org.apache.flink.api.java.ExecutionEnvironment;
    +import org.apache.flink.api.java.functions.KeySelector;
    +import org.apache.flink.api.java.operators.GroupReduceOperator;
    +import org.apache.flink.api.java.operators.SortedGrouping;
    +import org.apache.flink.api.java.operators.UnsortedGrouping;
    +import org.apache.flink.api.java.tuple.Tuple;
    +import org.apache.flink.core.fs.FileSystem;
    +import org.apache.flink.core.fs.Path;
    +import org.apache.flink.languagebinding.api.java.common.PlanBinder;
    +import org.apache.flink.languagebinding.api.java.common.OperationInfo;
    +import org.apache.flink.languagebinding.api.java.python.PythonPlanBinder.PythonOperationInfo;
    +//CHECKSTYLE.OFF: AvoidStarImport - enum/function import
    +import static org.apache.flink.languagebinding.api.java.python.PythonPlanBinder.PythonOperationInfo.*;
    +import org.apache.flink.languagebinding.api.java.python.functions.*;
    +//CHECKSTYLE.ON: AvoidStarImport
    +import org.apache.flink.languagebinding.api.java.common.streaming.Receiver;
    +import org.apache.flink.languagebinding.api.java.common.streaming.StreamPrinter;
    +import org.apache.flink.runtime.filecache.FileCache;
    +
    +/**
    + * This class allows the execution of a Flink plan written in python.
    + */
    +public class PythonPlanBinder extends PlanBinder<PythonOperationInfo> {
    +	public static final String FLINK_PYTHON_ID = ""flink"";
    +	public static final String FLINK_PYTHON_PLAN_NAME = ""/plan.py"";
    +	public static final String FLINK_PYTHON_EXECUTOR_NAME = ""/executor.py"";
    +
    +	private static final String FLINK_PYTHON_FILE_PATH = System.getProperty(""java.io.tmpdir"") + ""/flink_plan"";
    +	private static final String FLINK_PYTHON_REL_LOCAL_PATH = ""/resources/python"";
    +	private static final String FLINK_DIR = System.getenv(""FLINK_ROOT_DIR"");
    +
    +	private Process process;
    +
    +	/**
    +	 * Entry point for the execution of a python plan.
    +	 *
    +	 * @param args planPath [package1[packageX[|parameter1[parameterX]]]]
    +	 * @throws Exception
    +	 */
    +	public static void main(String[] args) throws Exception {
    +		PythonPlanBinder binder = new PythonPlanBinder();
    +		binder.go(args);
    +	}
    +
    +	private void go(String[] args) throws Exception {
    +		env = ExecutionEnvironment.getExecutionEnvironment();
    +
    +		int split = 0;
    +		for (int x = 0; x < args.length; x++) {
    +			if (args[x].compareTo(""|"") == 0) {
    +				split = x;
    +			}
    +		}
    +
    +		prepareFiles(Arrays.copyOfRange(args, 0, split == 0 ? 1 : split));
    +		startPython(Arrays.copyOfRange(args, split == 0 ? args.length : split + 1, args.length));
    +		receivePlan();
    +		distributeFiles(env);
    +
    +		env.execute();
    +		close();
    +	}
    +
    +	//=====Setup========================================================================================================
    +	private void prepareFiles(String... filePaths) throws IOException, URISyntaxException {
    +		prepareFlinkPythonPackage();
    +
    +		String planPath = filePaths[0];
    +		if (planPath.endsWith(""/"")) {
    +			planPath = planPath.substring(0, planPath.length() - 1);
    +		}
    +		String tmpPlanPath = FLINK_PYTHON_FILE_PATH + FLINK_PYTHON_PLAN_NAME;
    +		clearPath(tmpPlanPath);
    +		FileCache.copy(new Path(planPath), new Path(tmpPlanPath), false);
    +
    +		for (int x = 1; x < filePaths.length; x++) {
    +			copyFile(filePaths[x]);
    +		}
    +	}
    +
    +	private void startPython(String[] args) throws IOException {
    +		sets = new HashMap();
    +		StringBuilder argsBuilder = new StringBuilder();
    +		for (String arg : args) {
    +			argsBuilder.append("" "").append(arg);
    +		}
    +		receiver = new Receiver(null);
    +		receiver.open(null);
    +		process = Runtime.getRuntime().exec(""python -B "" + FLINK_PYTHON_FILE_PATH + FLINK_PYTHON_PLAN_NAME + argsBuilder.toString());
    +
    +		new StreamPrinter(process.getInputStream()).start();
    +		new StreamPrinter(process.getErrorStream()).start();
    +	}
    +
    +	private void close() throws IOException, URISyntaxException {
    +		FileSystem hdfs = FileSystem.get(new URI(FLINK_HDFS_PATH));
    +		hdfs.delete(new Path(FLINK_HDFS_PATH), true);
    +
    +		FileSystem local = FileSystem.getLocalFileSystem();
    +		local.delete(new Path(FLINK_PYTHON_FILE_PATH), true);
    +		local.delete(new Path(FLINK_TMP_DATA_DIR), true);
    +
    +		try {
    +			receiver.close();
    +		} catch (NullPointerException npe) {
    +		}
    +		process.destroy();
    +	}
    +
    +	public static void prepareFlinkPythonPackage() throws IOException, URISyntaxException {
    +		String originalFilePath = FLINK_DIR.substring(0, FLINK_DIR.length() - 7) + FLINK_PYTHON_REL_LOCAL_PATH;
    +		String tempFilePath = FLINK_PYTHON_FILE_PATH;
    +		clearPath(tempFilePath);
    +		FileCache.copy(new Path(originalFilePath), new Path(tempFilePath), false);
    +	}
    +
    +	public static void prepareFlinkPythonPackage(String path) throws IOException {
    +		FileCache.copy(new Path(path), new Path(FLINK_PYTHON_FILE_PATH), true);
    +	}
    +
    +	public static void distributeFiles(ExecutionEnvironment env) throws IOException, URISyntaxException {
    +		clearPath(FLINK_HDFS_PATH);
    +		FileCache.copy(new Path(FLINK_PYTHON_FILE_PATH), new Path(FLINK_HDFS_PATH), true);
    +		env.registerCachedFile(FLINK_HDFS_PATH, FLINK_PYTHON_ID);
    +		clearPath(FLINK_PYTHON_FILE_PATH);
    +	}
    +
    +	private static void clearPath(String path) throws IOException, URISyntaxException {
    +		FileSystem fs = FileSystem.get(new URI(path));
    +		if (fs.exists(new Path(path))) {
    +			fs.delete(new Path(path), true);
    +		}
    +	}
    +
    +	public static String copyFile(String path) throws IOException, URISyntaxException {
    +		if (path.endsWith(""/"")) {
    +			path = path.substring(0, path.length() - 1);
    +		}
    +		String identifier = path.substring(path.lastIndexOf(""/""));
    +		String tmpFilePath = FLINK_PYTHON_FILE_PATH + ""/"" + identifier;
    +		clearPath(tmpFilePath);
    +		FileCache.copy(new Path(path), new Path(tmpFilePath), true);
    +		return identifier;
    +	}
    +
    +	//=====Plan Binding=================================================================================================
    +	protected class PythonOperationInfo extends OperationInfo {
    +		protected static final int INFO_MODE_UDF_DOUBLE_KEYED_PROJECTION_TYPED = -1;
    +		protected static final int INFO_MODE_UDF_DOUBLE_KEYED_TYPED = 0;
    +		protected static final int INFO_MODE_UDF_DOUBLE_TYPED = 1;
    +		protected static final int INFO_MODE_UDF_SINGLE_TYPED = 2;
    +		protected static final int INFO_MODE_UDF_SINGLE_TYPED_COMBINE = 9;
    +		protected static final int INFO_MODE_UDF = 3;
    +		protected static final int INFO_MODE_GROUP = 4;
    +		protected static final int INFO_MODE_SORT = 5;
    +		protected static final int INFO_MODE_UNION = 6;
    +		protected static final int INFO_MODE_PROJECT = 7;
    +		protected static final int INFO_MODE_UDF_DOUBLE_PROJECTION_TYPED = 8;
    +		protected String operator;
    +		protected String meta;
    +		protected boolean combine;
    +
    +		protected PythonOperationInfo(int mode) throws IOException {
    +			parentID = (Integer) receiver.getRecord();
    +			childID = (Integer) receiver.getRecord();
    +			switch (mode) {
    +				case INFO_MODE_UDF_DOUBLE_KEYED_PROJECTION_TYPED:
    +					keys1 = (Tuple) receiver.getRecord();
    +					keys2 = (Tuple) receiver.getRecord();
    +					otherID = (Integer) receiver.getRecord();
    +					types = receiver.getRecord();
    +					operator = (String) receiver.getRecord();
    +					meta = (String) receiver.getRecord();
    +					projectionKeys1 = (Tuple) receiver.getRecord();
    +					projectionKeys2 = (Tuple) receiver.getRecord();
    +					break;
    +				case INFO_MODE_UDF_DOUBLE_PROJECTION_TYPED:
    +					otherID = (Integer) receiver.getRecord();
    +					types = receiver.getRecord();
    +					operator = (String) receiver.getRecord();
    +					meta = (String) receiver.getRecord();
    +					projectionKeys1 = (Tuple) receiver.getRecord();
    +					projectionKeys2 = (Tuple) receiver.getRecord();
    +					break;
    +				case INFO_MODE_UDF_DOUBLE_KEYED_TYPED:
    +					keys1 = (Tuple) receiver.getRecord();
    +					keys2 = (Tuple) receiver.getRecord();
    +					otherID = (Integer) receiver.getRecord();
    +					types = receiver.getRecord();
    +					operator = (String) receiver.getRecord();
    +					meta = (String) receiver.getRecord();
    +					break;
    +				case INFO_MODE_UDF_DOUBLE_TYPED:
    +					otherID = (Integer) receiver.getRecord();
    +					types = receiver.getRecord();
    +					operator = (String) receiver.getRecord();
    +					meta = (String) receiver.getRecord();
    +					break;
    +				case INFO_MODE_UDF_SINGLE_TYPED:
    +					types = receiver.getRecord();
    +					operator = (String) receiver.getRecord();
    +					meta = (String) receiver.getRecord();
    +					break;
    +				case INFO_MODE_UDF_SINGLE_TYPED_COMBINE:
    +					types = receiver.getRecord();
    +					operator = (String) receiver.getRecord();
    +					meta = (String) receiver.getRecord();
    +					combine = (Boolean) receiver.getRecord();
    +					break;
    +				case INFO_MODE_UDF:
    +					operator = (String) receiver.getRecord();
    +					meta = (String) receiver.getRecord();
    +					break;
    +				case INFO_MODE_GROUP:
    +					keys1 = (Tuple) receiver.getRecord();
    +					break;
    +				case INFO_MODE_SORT:
    +					field = (Integer) receiver.getRecord();
    +					order = (Integer) receiver.getRecord();
    +					break;
    +				case INFO_MODE_UNION:
    +					otherID = (Integer) receiver.getRecord();
    +					break;
    +				case INFO_MODE_PROJECT:
    +					keys1 = (Tuple) receiver.getRecord();
    +					types = receiver.getRecord();
    +					break;
    +			}
    +		}
    +	}
    +
    +	@Override
    +	protected PythonOperationInfo createOperationInfo(String identifier) throws IOException {
    +		switch (Operations.valueOf(identifier.toUpperCase())) {
    +			case COGROUP:
    +				return new PythonOperationInfo(INFO_MODE_UDF_DOUBLE_KEYED_TYPED);
    +			case CROSS:
    +				return new PythonOperationInfo(INFO_MODE_UDF_DOUBLE_PROJECTION_TYPED);
    +			case CROSS_H:
    +				return new PythonOperationInfo(INFO_MODE_UDF_DOUBLE_PROJECTION_TYPED);
    +			case CROSS_T:
    +				return new PythonOperationInfo(INFO_MODE_UDF_DOUBLE_PROJECTION_TYPED);
    +			case FILTER:
    +				return new PythonOperationInfo(INFO_MODE_UDF);
    +			case FLATMAP:
    +				return new PythonOperationInfo(INFO_MODE_UDF_SINGLE_TYPED);
    +			case GROUPREDUCE:
    +				return new PythonOperationInfo(INFO_MODE_UDF_SINGLE_TYPED_COMBINE);
    +			case JOIN:
    +				return new PythonOperationInfo(INFO_MODE_UDF_DOUBLE_KEYED_PROJECTION_TYPED);
    +			case JOIN_H:
    +				return new PythonOperationInfo(INFO_MODE_UDF_DOUBLE_KEYED_PROJECTION_TYPED);
    +			case JOIN_T:
    +				return new PythonOperationInfo(INFO_MODE_UDF_DOUBLE_KEYED_PROJECTION_TYPED);
    +			case MAP:
    +				return new PythonOperationInfo(INFO_MODE_UDF_SINGLE_TYPED);
    +			case PROJECTION:
    +				return new PythonOperationInfo(INFO_MODE_PROJECT);
    +			case REDUCE:
    +				return new PythonOperationInfo(INFO_MODE_UDF);
    +			case GROUPBY:
    +				return new PythonOperationInfo(INFO_MODE_GROUP);
    +			case SORT:
    +				return new PythonOperationInfo(INFO_MODE_SORT);
    +			case UNION:
    +				return new PythonOperationInfo(INFO_MODE_UNION);
    +		}
    +		return new PythonOperationInfo(INFO_MODE_UDF_DOUBLE_KEYED_TYPED);
    +	}
    +
    +	@Override
    +	protected DataSet applyCoGroupOperation(DataSet op1, DataSet op2, int[] firstKeys, int[] secondKeys, PythonOperationInfo info) {
    +		return op1.coGroup(op2).where(firstKeys).equalTo(secondKeys).with(new PythonCoGroup(info.operator, info.types, info.meta));
    +	}
    +
    +	public static class PseudoKeySelector<X> implements KeySelector<X, Integer> {
    +		@Override
    +		public Integer getKey(X value) throws Exception {
    +			return 0;
    +		}
    +	}
    +
    +	@Override
    +	protected DataSet applyCrossOperation(DataSet op1, DataSet op2, int mode, PythonOperationInfo info) {
    +		switch (mode) {
    +			case 0:
    +				return op1.join(op2).where(new PseudoKeySelector()).equalTo(new PseudoKeySelector()).with(new PythonCross(info.operator, info.types, info.meta));
    --- End diff --
    
    A Cross is implemented as a join where every pair matches. I don't know the implications of doing it in such a hacky way. (The comparison overhead should be negligible considering the current performance.)
;;;","14/Nov/14 14:41;githubbot;Github user zentol commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/202#discussion_r20361691
  
    --- Diff: flink-dist/src/main/flink-bin/bin/flink ---
    @@ -48,6 +48,7 @@ CC_CLASSPATH=`manglePathList $(constructCLIClientClassPath)`
     log=$FLINK_LOG_DIR/flink-$FLINK_IDENT_STRING-flink-client-$HOSTNAME.log
     log_setting=""-Dlog.file=""$log"" -Dlog4j.configuration=file:""$FLINK_CONF_DIR""/log4j.properties -Dlogback.configurationFile=file:""$FLINK_CONF_DIR""/logback.xml""
     
    +export FLINK_ROOT_DIR
    --- End diff --
    
    I've been unsure about this change for a long time, wondering whether i should instead export only a specific directory. The python package currently resides in /resource/python, is there a more appropriate place to put it?
;;;","14/Nov/14 14:45;githubbot;Github user zentol commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/202#discussion_r20361957
  
    --- Diff: docs/python_api_guide.md ---
    @@ -0,0 +1,666 @@
    +--- 
    +title: Python Programming Guide
    --- End diff --
    
    The documentation is not integrated into the current one for Java&Scala. Should i (try to...) do this know or postpone it?
;;;","14/Nov/14 14:45;githubbot;Github user zentol commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/202#discussion_r20361989
  
    --- Diff: flink-addons/flink-language-binding/src/main/java/org/apache/flink/languagebinding/examples/java/python/wordcount/WordCount.java ---
    @@ -0,0 +1,47 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE
    + * file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the
    + * License. You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + */
    +package org.apache.flink.languagebinding.examples.java.python.wordcount;
    +
    +import org.apache.flink.api.java.DataSet;
    +import org.apache.flink.api.java.ExecutionEnvironment;
    +import org.apache.flink.api.java.operators.GroupReduceOperator;
    +import org.apache.flink.api.java.tuple.Tuple2;
    +import static org.apache.flink.languagebinding.api.java.common.PlanBinder.INT;
    +import static org.apache.flink.languagebinding.api.java.common.PlanBinder.STRING;
    +import org.apache.flink.languagebinding.api.java.python.PythonPlanBinder;
    +import org.apache.flink.languagebinding.api.java.python.functions.PythonFlatMap;
    +import org.apache.flink.languagebinding.api.java.python.functions.PythonGroupReduce;
    +
    +public class WordCount {
    +	public static void main(String[] args) throws Exception {
    --- End diff --
    
    Will be changed to be inline with other examples.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow empty paths in FileInputFormat to fix a problem with the job-preview,FLINK-335,12719506,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:07,09/Jun/14 12:07,14/Jul/23 05:57,09/Jun/14 12:07,,,,pre-apache,,,,,,,0,github-import,,"The commit [CsvFormat configurable via regular parameters|https://github.com/stratosphere/stratosphere/commit/9dd4635ad69bf107ab9f81c8e7ffd495475471e0) caused the job-preview of the job-submission web interface to fail. The preview currently creates a plan with empty String args. This is no longer possible due to this commit (see my changes to understand).

I added a quick-fix (see code, very simple], and updated a test case. Please merge for Ufuk.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/335
Created by: [andrehacker|https://github.com/andrehacker]
Labels: 
Created at: Wed Dec 11 15:48:33 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:07;github-import;pull-request-335-5227375675132353945.patch;https://issues.apache.org/jira/secure/attachment/12649092/pull-request-335-5227375675132353945.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397705,,,2014-06-09 12:07:03.0,,,,,,,,,,"0|i1wgyf:",397832,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added documentation for Stratosphere's programming model,FLINK-334,12719505,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 12:06,09/Jun/14 12:07,14/Jul/23 05:57,09/Jun/14 12:07,,,,pre-apache,,,,,,,0,github-import,,"Added documentation for Stratosphere's programming model.
The section about iterative data flows still needs to be written.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/334
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Wed Dec 11 14:15:32 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:06;github-import;pull-request-334-2781256216024047951.patch;https://issues.apache.org/jira/secure/attachment/12649091/pull-request-334-2781256216024047951.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397704,,,Mon Jun 09 12:07:01 UTC 2014,,,,,,,,,,"0|i1wgy7:",397831,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:07;github-import;[Date: Wed Dec 11 14:18:16 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Preview: http://robertmetzger.de/stratosphere/docs/programming_guides/pmodel.html;;;","09/Jun/14 12:07;github-import;[Date: Tue Dec 17 08:51:20 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

This is not merged?;;;","09/Jun/14 12:07;github-import;[Date: Tue Dec 17 09:00:16 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Not yet. 
Adapted it to the structure of the versioned documentation (([#332|https://github.com/stratosphere/stratosphere/issues/332] | [FLINK-332|https://issues.apache.org/jira/browse/FLINK-332])) and pushed an updated commit.
Can be merged now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Respect CrossWithSmall and CrossWithLarge hints,FLINK-333,12719504,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,fhueske,github-import,09/Jun/14 12:06,28/Feb/19 14:30,14/Jul/23 05:57,08/Jan/15 18:12,0.6.1-incubating,0.7.0-incubating,,0.9,,,API / DataSet,API / Scala,,,0,,,"The Java and the Scala API offer {{crossWithHuge()}} and {{crossWithTiny()}} API methods that allow the user to give hints to the optimizer about the size of the inputs a cross.
However, these hints are not considered and all {{cross}} transformations are handled the same by the optimizer. 

Similiar hints for Join exist and are implemented correctly.
",,aljoscha,fhueske,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397703,,,Thu Jan 08 18:12:27 UTC 2015,,,,,,,,,,"0|i1wgxz:",397830,,,,,,,,,,,,,,,,,,,,"30/Sep/14 19:01;fhueske;The new Scala API supports {{crossWithTiny()}} and {{crossWithHuge()}}.
Looking at the implementation, I get the impression that all cross functions ({{cross}}, {{crossWithHuge}}, {{crossWithTiny}}) are execute the same way.
[~aljoscha] can you check if this issue can be closed?;;;","30/Sep/14 19:14;hsaputra;Shall I assign it to [~aljoscha] as part of the new Scala API work?;;;","01/Oct/14 05:49;aljoscha;Yes, they are all executed the same way. Exactly how it is done in the Java API.;;;","01/Oct/14 08:19;fhueske;I updated this JIRA to fix the {{crossWith}} hints in the Java and Scala API.;;;","08/Jan/15 18:12;sewen;Fixed via 0954c4e521fafb0f1ecb7ae409bfc9ffbdfae51d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow version-specific documentation starting with v0.4,FLINK-332,12719502,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:59,09/Jun/14 12:03,14/Jul/23 05:57,09/Jun/14 12:03,,,,pre-apache,,,,,,,0,github-import,,"With the upcoming release we will need to have documentation for different versions of Stratosphere. @rmetzger already suggested to have this, when I did the initial overhaul of the documentation.

With this PR, the documentation is under `/docs/0.4` instead of just `/docs`. The docs root and `/docs/latest` redirect to the latest version, i.e. the upcoming 0.4.

Preview under http://uce.github.io.

I suggest to start with 0.4 as the initial open source version for which we provide documentation, as porting the old documentation will be too much work right and we will probably suggest to anybody who uses something older to move to 0.4 anyways.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/332
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Mon Dec 09 16:04:06 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397701,,,Mon Jun 09 12:03:44 UTC 2014,,,,,,,,,,"0|i1wgxj:",397828,,,,,,,,,,,,,,,,,,,,"09/Jun/14 12:03;github-import;Import aborted because patch was larger than 10 MB (JIRA limit).

The comments are missing for this issue!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Nephele task names,FLINK-331,12719501,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:59,09/Jun/14 11:59,14/Jul/23 05:57,09/Jun/14 11:59,,,,pre-apache,,,,,,,0,github-import,,"- Names have prefix of Pact contract (Map, Red, Cmb, Mat, Crs, CGp, ...)
- All chained tasks are added to name of chain parent

Task names are only proposals. Let me know, if you prefer a different naming.

This pull request addresses ([#300|https://github.com/stratosphere/stratosphere/issues/300] | [FLINK-300|https://issues.apache.org/jira/browse/FLINK-300]) 

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/331
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Mon Dec 09 16:03:44 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:59;github-import;pull-request-331-2247725896967619255.patch;https://issues.apache.org/jira/secure/attachment/12649090/pull-request-331-2247725896967619255.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397700,,,Mon Jun 09 11:59:10 UTC 2014,,,,,,,,,,"0|i1wgxb:",397827,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:59;github-import;[Date: Mon Dec 09 19:38:22 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I'm not sure but I guess the full names are probably nicer: Map, Reduce, Compare, Cross.
Horizontal space is not a problem in the interface.;;;","09/Jun/14 11:59;github-import;[Date: Mon Dec 09 20:25:52 CET 2013, Author: [fhueske|https://github.com/fhueske]]

You're probably right. Might be a bit unintuitive... 
Cmb was meant to be an abbreviation for Combine and not Compare ;-)

Other opinions?;;;","09/Jun/14 11:59;github-import;[Date: Mon Dec 09 20:26:45 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Seems like I'm a bit tired. there is no compare operator ;);;;","09/Jun/14 11:59;github-import;[Date: Tue Dec 10 14:33:46 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Alright, I'll extend the names.;;;","09/Jun/14 11:59;github-import;[Date: Thu Dec 12 15:18:26 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I am merging this for now, because of the pending code reorganization.

Asdding the names inside the operators that define properties (required/produced) is actually not the right place, as they have not a 1:1 correspondance to UDF operators. We should move that in the near future to a different place.;;;","09/Jun/14 11:59;github-import;[Date: Thu Dec 12 19:19:32 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I know, but this is the place where combiners are generated. Before that Reduce is just Reduce. 

We can extend the naming code with a check for the Contract type, but I thought right now there is a 1:1 relationship between strategies and contracts and omitted the check. Which contracts share strategies at the moment?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simple batch scripts to run stratosphere under windows,FLINK-329,12719499,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:58,09/Jun/14 11:59,14/Jul/23 05:57,09/Jun/14 11:59,,,,pre-apache,,,,,,,0,github-import,,"See Issue ([#326|https://github.com/stratosphere/stratosphere/issues/326] | [FLINK-326|https://issues.apache.org/jira/browse/FLINK-326]). Two simple batch scripts for running stratosphere in local mode and to submit jobs.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/329
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Mon Dec 09 15:17:24 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;pull-request-329-6564381195452222160.patch;https://issues.apache.org/jira/secure/attachment/12649088/pull-request-329-6564381195452222160.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397698,,,Mon Jun 09 11:58:59 UTC 2014,,,,,,,,,,"0|i1wgwv:",397825,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;[Date: Mon Dec 09 19:44:10 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Could you also add this to the documentation in http://stratosphere.eu/docs/setup/local.html
(You need to make a separate pull request for that);;;","09/Jun/14 11:58;github-import;[Date: Tue Dec 10 16:25:16 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

A question: Should we create a separate directory for the windows files? (bin/windows) or anything? Or leave all in the same dir? The later screws my shell command completion, but that's okay...;;;","09/Jun/14 11:58;github-import;[Date: Thu Dec 12 19:51:59 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [5efd24167a5d249cf4942464818e7a029aba38a8|https://github.com/stratosphere/stratosphere/commit/5efd24167a5d249cf4942464818e7a029aba38a8];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scheduling algorithms,FLINK-328,12719498,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:58,09/Jun/14 11:58,14/Jul/23 05:57,09/Jun/14 11:58,,,,pre-apache,,,,,,,0,github-import,,"Hi

i am a master student. my thesis is about stratosphere project . i have a question from all friends, my question is about scheduling at stratosphere project. i want to know more about scheduling algorithms. i know that the default scheduling at stratosphere is FIFO scheduler. Can be added  another scheduling algorithms for example real time scheduling or fair sharing scheduling to stratosphere project? my thesis topic is to add the new job scheduler to stratosphere project according to PACT programming model (map, reduce, match,cogroup,...)  

in your opinion, Whether it is implementable?

Best Regards,
Fahime

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/328
Created by: [fhmt|https://github.com/fhmt]
Labels: question, 
Created at: Mon Dec 09 12:13:32 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397697,,,Mon Jun 09 11:58:54 UTC 2014,,,,,,,,,,"0|i1wgwn:",397824,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;[Date: Mon Dec 09 12:14:57 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Hi Fahime,

lets discuss this on the mailing list. I'm already working on an answer there ;).

Best,
Robert;;;","09/Jun/14 11:58;github-import;[Date: Mon Dec 09 12:19:03 CET 2013, Author: [fhmt|https://github.com/fhmt]]

ok , thanks Robert;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"YARN + pact-client enhancements, fix stalling travis builds",FLINK-327,12719497,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:58,09/Jun/14 11:58,14/Jul/23 05:57,09/Jun/14 11:58,,,,pre-apache,,,,,,,0,github-import,,"* YARN now also starts the web interface
* extended pact-client to nicely support remote execution using new option ""remote""


```
Action ""run"" compiles and submits a PACT program.
  ""run"" action arguments:
     -a,--arguments <programArgs>   Pact program arguments
     -c,--class <classname>         Pact program assembler class
     -j,--jarfile <jarfile>         Pact program JAR file
     -w,--wait                      Wait until program finishes

Action ""remote"" is similar to ""run"" but allows to specify the JobManager connection
  ""remote"" action arguments:
     -a,--arguments <programArgs>   Pact program arguments
     -c,--class <classname>         Pact program assembler class
     -j,--jarfile <jarfile>         Pact program JAR file
     -r,--address <arg>             Hostname:port of JobManager
     -w,--wait                      Wait until program finishes
```

The webinterface starts but is not really usable from the YARN webinterfaces due to https://github.com/stratosphere/stratosphere/issues/325.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/327
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Dec 09 11:25:55 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;pull-request-327-4431635296633622665.patch;https://issues.apache.org/jira/secure/attachment/12649087/pull-request-327-4431635296633622665.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397696,,,Mon Jun 09 11:58:50 UTC 2014,,,,,,,,,,"0|i1wgwf:",397823,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;[Date: Mon Dec 09 11:36:22 CET 2013, Author: [fhueske|https://github.com/fhueske]]

How about merging both actions (run and remote) into one (run) with an optional paramter -r (I would vote to call it -h for host or -m for master, though).
If -r is specified, the job is submitted to the remote JM. If not, the client tries to submit the job to the JM specified in the local configuration.

What do you think?;;;","09/Jun/14 11:58;github-import;[Date: Mon Dec 09 11:39:54 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Yeah... you are right. Sometimes, I don't see the obvious. (But I have to say the implementation is ""historically grown"" ;)
I'll fix it.
;;;","09/Jun/14 11:58;github-import;[Date: Mon Dec 09 11:59:34 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay. Changed according to @fhueske's suggestion.
Updated the documentation on the website in [5cf6e23bb31b7d1c2f4cd302a4bd5bf8c393345f|https://github.com/stratosphere/stratosphere/commit/5cf6e23bb31b7d1c2f4cd302a4bd5bf8c393345f];;;","09/Jun/14 11:58;github-import;[Date: Mon Dec 09 12:15:54 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

While you are at changing the cli, can we remove the -w option and make it always wait? I think that is intuitive, I don't see cases where you want that asynchronously running and could not use the OS way of moving a process to the background.

One less option is one step easier to start...;;;","09/Jun/14 11:58;github-import;[Date: Mon Dec 09 12:33:42 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I'm against this Stephan.
We should write the address to the JobManager webinterface there .. but a blocking submission client probably not the best user experience.;;;","09/Jun/14 11:58;github-import;[Date: Mon Dec 09 12:53:00 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, I changed it as discussed with @fhueske. (also on the website).
I now print this information after job submission:
```
Job successfully submitted. Use -w (or --wait) option to track the progress here.
JobManager web interface: http://127.0.0.1:8081
```;;;","09/Jun/14 11:58;github-import;[Date: Mon Dec 09 13:47:45 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

@rmetzger Allright, if it is more prominent how to track status, that is fine with me. I was just comparing to the way it works in Hadoop. It always blocks there. ;;;","09/Jun/14 11:58;github-import;[Date: Tue Dec 10 13:12:45 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [06168008ad8d2523d9a951ef499eb1c22bcf2939|https://github.com/stratosphere/stratosphere/commit/06168008ad8d2523d9a951ef499eb1c22bcf2939];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManager webinterface does not work with YARN Tracking UI Proxy,FLINK-325,12719495,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:58,09/Jun/14 11:58,14/Jul/23 05:57,09/Jun/14 11:58,,,,pre-apache,,,,,,,0,github-import,,"YARN's web interface allows to track the application master using a special tracking url.
It is a proxy that routes the requests to the right container (that of the application master).
Since the request urls of the web interface are somewhat hard coded. Currently the ajax requests look like `/jobsInfo` but the proxy urls look like `http://localhost:8088/proxy/application_1386532236961_0001/` 

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/325
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, gui, 
Created at: Sun Dec 08 20:57:55 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397694,,,Mon Jun 09 11:58:35 UTC 2014,,,,,,,,,,"0|i1wgvz:",397821,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;[Date: Mon Dec 09 10:03:34 CET 2013, Author: [markus-h|https://github.com/markus-h]]

What do you mean by application master? So if I understand it right, the webinterface should be accessible when using a proxy, for example by using http://localhost:8088/proxy/application_1386532236961_0001/jobsInfo? So we'd need a new configuration value on the client side to specify a proxy url.;;;","09/Jun/14 11:58;github-import;[Date: Mon Dec 09 10:09:21 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

This picture explains what the application master is in the yarn context:
https://f.cloud.github.com/assets/5525371/1298189/1ac85602-30f0-11e3-9fd5-cd3993b38ce3.png

So its basically a Jvm that runs the JobManager with some yarn management stuff.

> So if I understand it right, the webinterface should be accessible when using a proxy, for example by using http://localhost:8088/proxy/application_1386532236961_0001/jobsInfo? 

Exactly!

Do you think it is possible to do this without an additional configuration value? I mean I'm sure that it is possible to find out the current url?
There should be a standard solution to that problem? http://stackoverflow.com/questions/4765740/relative-urls-in-ajax-requests ?

Probably changing `/jobsInfo` to `jobsInfo` should do the job?
;;;","09/Jun/14 11:58;github-import;[Date: Tue Jan 21 00:05:44 CET 2014, Author: [fhueske|https://github.com/fhueske]]

@rmetzger did you resolve this issue in [02f6328c09723afa0ce9a3e9bec6b72080aaa74a|https://github.com/stratosphere/stratosphere/commit/02f6328c09723afa0ce9a3e9bec6b72080aaa74a]?;;;","09/Jun/14 11:58;github-import;[Date: Tue Feb 11 21:36:52 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

This issue is not resolved.
The JobHistory link to the analzye page is still hardcoded and not compatible to yarn's proxy!;;;","09/Jun/14 11:58;github-import;[Date: Thu Feb 13 11:10:59 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I fixed the issue with commit: [5300f4b4b4cfdb00edbd5b4e87721640b0c7dc0c|https://github.com/stratosphere/stratosphere/commit/5300f4b4b4cfdb00edbd5b4e87721640b0c7dc0c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
localhost:8081 ,FLINK-324,12719494,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:58,09/Jun/14 11:58,14/Jul/23 05:57,09/Jun/14 11:58,,,,pre-apache,,,,,,,0,github-import,,"hello friends, 

i am a beginner at stratosphere project, but i like to progress. i tested stratosphere project on local system but i have some problems: 

1- when i enter  localhost:8080 address, web interface  opened for me and i uploaded wordcount.jar example and click runjob , then the message is "" successfully job submitted"" and i get output. but after that i do not see anythings! i can not  see jabmanager interface , i enter address: localhost:8081 but no page loaded. how can i see job processing stage? 

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/324
Created by: [fhmt|https://github.com/fhmt]
Labels: question, 
Created at: Sun Dec 08 11:52:26 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397693,,,Mon Jun 09 11:58:29 UTC 2014,,,,,,,,,,"0|i1wgvr:",397820,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;[Date: Sun Dec 08 11:59:51 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Hey,

cool that you are interested in the project!

Can you show me the first few lines in your JobManager log file?
The file is located under `logs/nephele-<username>-jobmanager-<hostname>.log`
The progress monitoring interface sometimes does not start if a) it can not find its files or b) the port is already in use.
But I'm sure the logfile will show us the exact reason.

Best,
Robert
;;;","09/Jun/14 11:58;github-import;[Date: Sun Dec 08 13:35:02 CET 2013, Author: [fhmt|https://github.com/fhmt]]

thank you very much because your quick reply....

21:51:56,995 INFO  eu.stratosphere.nephele.discovery.DiscoveryService            - Discovery service socket is bound to /127.0.0.1:7001
21:51:57,032 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server Responder: starting
21:51:57,033 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server listener on 6123: starting
21:51:57,033 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server handler 0 on 6123: starting
21:51:57,034 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server handler 2 on 6123: starting
21:51:57,034 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server handler 3 on 6123: starting
21:51:57,034 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server handler 1 on 6123: starting
21:51:57,034 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server handler 4 on 6123: starting
21:51:57,034 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Starting job manager in LOCAL mode
21:51:57,034 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server handler 6 on 6123: starting
21:51:57,034 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server handler 5 on 6123: starting
21:51:57,034 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server handler 7 on 6123: starting
21:51:57,387 INFO  eu.stratosphere.nephele.instance.HardwareDescriptionFactory   - Found Tenured Gen pool (max: 536870912, used: 0)
21:51:57,390 INFO  eu.stratosphere.nephele.instance.local.LocalInstanceManager   - Default instance type is default
21:51:57,397 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Reading location of job manager from configuration
21:51:57,398 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Determined address of job manager to be localhost/127.0.0.1:6123
21:51:57,400 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Announcing connection information 127.0.0.1 to job manager
21:51:57,416 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server Responder: starting
21:51:57,416 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server listener on 6122: starting
21:51:57,416 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server handler 0 on 6122: starting
21:51:57,448 INFO  eu.stratosphere.nephele.taskmanager.bufferprovider.GlobalBufferPool  - Initialized global buffer pool with 2048 buffers with a size 32768 bytes each
21:51:57,457 INFO  eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager  - Initialized byte buffered channel manager with sender-side spilling disabled and spilled buffer merging enabled
21:51:57,586 INFO  eu.stratosphere.nephele.instance.HardwareDescriptionFactory   - Found Tenured Gen pool (max: 536870912, used: 0)
21:51:57,586 INFO  eu.stratosphere.nephele.taskmanager.TaskManager               - Initializing memory manager with 409 megabytes of memory
21:51:57,871 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Trying to load eu.stratosphere.nephele.jobmanager.scheduler.local.LocalScheduler as scheduler
21:53:20,003 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Creating initial execution graph from job graph WordCount Example
21:53:20,260 INFO  eu.stratosphere.nephele.executiongraph.ExecutionGraph         - Job input vertex Input Lines generated 2 input splits
21:53:20,295 INFO  eu.stratosphere.nephele.jobmanager.splitassigner.InputSplitManager  - Trying to load input split assigner for type FileInputSplit
21:53:20,297 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Scheduling job WordCount Example
21:53:20,300 INFO  eu.stratosphere.nephele.jobmanager.scheduler.AbstractScheduler  - Requesting the following instances for job 1048a12f8f841c004e49c0ee9eec0c00
21:53:20,301 INFO  eu.stratosphere.nephele.jobmanager.scheduler.AbstractScheduler  -  default (8,8,4096,1,0) [1, 1]
21:53:20,303 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from CREATED to SCHEDULED for task Input Lines (1/2)
21:53:20,315 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Status of job WordCount Example(1048a12f8f841c004e49c0ee9eec0c00) changed to SCHEDULED
21:53:20,315 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from CREATED to SCHEDULED for task Count Words (1/2)
21:53:20,315 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from CREATED to SCHEDULED for task Word Counts (1/2)
21:53:20,315 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from CREATED to SCHEDULED for task Count Words (2/2)
21:53:20,316 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from CREATED to SCHEDULED for task Word Counts (2/2)
21:53:20,316 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from CREATED to SCHEDULED for task Input Lines (2/2)
21:53:20,316 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from SCHEDULED to ASSIGNED for task Input Lines (1/2)
21:53:20,317 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from SCHEDULED to ASSIGNED for task Word Counts (2/2)
21:53:20,317 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from SCHEDULED to ASSIGNED for task Word Counts (1/2)
21:53:20,317 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from SCHEDULED to ASSIGNED for task Input Lines (2/2)
21:53:20,317 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from SCHEDULED to ASSIGNED for task Count Words (2/2)
21:53:20,317 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from SCHEDULED to ASSIGNED for task Count Words (1/2)
21:53:20,317 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from ASSIGNED to READY for task Input Lines (1/2)
21:53:20,318 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from ASSIGNED to READY for task Input Lines (2/2)
21:53:20,318 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from READY to STARTING for task Input Lines (1/2)
21:53:20,318 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from READY to STARTING for task Input Lines (2/2)
21:53:20,332 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Starting task Input Lines (1/2) on 127.0.0.1
21:53:20,332 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Starting task Input Lines (2/2) on 127.0.0.1
21:53:20,364 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from STARTING to RUNNING for task Input Lines (1/2)
21:53:20,364 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from STARTING to RUNNING for task Input Lines (2/2)
21:53:20,366 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from STARTING to RUNNING for task Input Lines (1/2)
21:53:20,366 INFO  eu.stratosphere.pact.runtime.task.DataSourceTask              - Start PACT code: Input Lines (1/2)
21:53:20,366 INFO  eu.stratosphere.pact.runtime.task.RegularPactTask             - Start PACT code: Tokenize Lines (1/2)
21:53:20,366 INFO  eu.stratosphere.pact.runtime.task.RegularPactTask             - Start PACT code: Count Words (1/2)
21:53:20,367 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Status of job WordCount Example(1048a12f8f841c004e49c0ee9eec0c00) changed to RUNNING
21:53:20,367 INFO  eu.stratosphere.pact.runtime.task.DataSourceTask              - Start PACT code: Input Lines (2/2)
21:53:20,367 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from STARTING to RUNNING for task Input Lines (2/2)
21:53:20,367 INFO  eu.stratosphere.pact.runtime.task.RegularPactTask             - Start PACT code: Tokenize Lines (2/2)
21:53:20,368 INFO  eu.stratosphere.pact.runtime.task.RegularPactTask             - Start PACT code: Count Words (2/2)
21:53:20,381 INFO  eu.stratosphere.nephele.jobmanager.splitassigner.file.FileInputSplitList  - 127.0.0.1 receives remote file input split (distance 2147483647)
21:53:20,382 INFO  eu.stratosphere.nephele.jobmanager.splitassigner.InputSplitManager  - Input Lines (1/2) receives input split 1
21:53:20,383 INFO  eu.stratosphere.nephele.jobmanager.splitassigner.file.FileInputSplitList  - 127.0.0.1 receives remote file input split (distance 2147483647)
21:53:20,383 INFO  eu.stratosphere.nephele.jobmanager.splitassigner.InputSplitManager  - Input Lines (2/2) receives input split 0
21:53:20,502 INFO  eu.stratosphere.pact.runtime.task.RegularPactTask             - Finished PACT code: Tokenize Lines (1/2)
21:53:20,503 INFO  eu.stratosphere.pact.runtime.task.RegularPactTask             - Finished PACT code: Tokenize Lines (2/2)
21:53:20,507 INFO  eu.stratosphere.pact.runtime.task.RegularPactTask             - Finished PACT code: Count Words (1/2)
21:53:20,507 INFO  eu.stratosphere.pact.runtime.task.DataSourceTask              - Finished PACT code: Input Lines (1/2)
21:53:20,507 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from RUNNING to FINISHING for task Input Lines (1/2)
21:53:20,508 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from RUNNING to FINISHING for task Input Lines (1/2)
21:53:20,510 INFO  eu.stratosphere.pact.runtime.task.RegularPactTask             - Finished PACT code: Count Words (2/2)
21:53:20,510 INFO  eu.stratosphere.pact.runtime.task.DataSourceTask              - Finished PACT code: Input Lines (2/2)
21:53:20,510 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from RUNNING to FINISHING for task Input Lines (2/2)
21:53:20,511 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from ASSIGNED to READY for task Count Words (1/2)
21:53:20,511 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from READY to STARTING for task Count Words (1/2)
21:53:20,511 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from RUNNING to FINISHING for task Input Lines (2/2)
21:53:20,512 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Starting task Count Words (1/2) on 127.0.0.1
21:53:20,520 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from STARTING to RUNNING for task Count Words (1/2)
21:53:20,521 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from STARTING to RUNNING for task Count Words (1/2)
21:53:20,521 INFO  eu.stratosphere.pact.runtime.task.RegularPactTask             - Start PACT code.: Count Words (1/2)
21:53:21,018 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from ASSIGNED to READY for task Count Words (2/2)
21:53:21,019 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from READY to STARTING for task Count Words (2/2)
21:53:21,021 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Starting task Count Words (2/2) on 127.0.0.1
21:53:21,026 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from STARTING to RUNNING for task Count Words (2/2)
21:53:21,028 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from STARTING to RUNNING for task Count Words (2/2)
21:53:21,028 INFO  eu.stratosphere.pact.runtime.task.RegularPactTask             - Start PACT code.: Count Words (2/2)
21:53:21,055 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from ASSIGNED to READY for task Word Counts (1/2)
21:53:21,055 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from READY to STARTING for task Word Counts (1/2)
21:53:21,056 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Starting task Word Counts (1/2) on 127.0.0.1
21:53:21,059 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from STARTING to RUNNING for task Word Counts (1/2)
21:53:21,060 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from STARTING to RUNNING for task Word Counts (1/2)
21:53:21,060 INFO  eu.stratosphere.pact.runtime.task.DataSinkTask                - Start PACT code: Word Counts (1/2)
21:53:21,545 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from ASSIGNED to READY for task Word Counts (2/2)
21:53:21,545 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from READY to STARTING for task Word Counts (2/2)
21:53:21,547 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Starting task Word Counts (2/2) on 127.0.0.1
21:53:21,553 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from STARTING to RUNNING for task Word Counts (2/2)
21:53:21,555 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from STARTING to RUNNING for task Word Counts (2/2)
21:53:21,555 INFO  eu.stratosphere.pact.runtime.task.DataSinkTask                - Start PACT code: Word Counts (2/2)
21:53:21,560 INFO  eu.stratosphere.pact.runtime.task.RegularPactTask             - Finished PACT code.: Count Words (1/2)
21:53:21,560 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from RUNNING to FINISHING for task Count Words (1/2)
21:53:21,562 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from RUNNING to FINISHING for task Count Words (1/2)
21:53:21,588 INFO  eu.stratosphere.pact.runtime.task.DataSinkTask                - Finished PACT code: Word Counts (1/2)
21:53:21,589 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from RUNNING to FINISHING for task Word Counts (1/2)
21:53:21,590 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from RUNNING to FINISHING for task Word Counts (1/2)
21:53:21,590 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from FINISHING to FINISHED for task Word Counts (1/2)
21:53:21,592 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from FINISHING to FINISHED for task Word Counts (1/2)
21:53:21,662 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from FINISHING to FINISHED for task Count Words (1/2)
21:53:21,663 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from FINISHING to FINISHED for task Count Words (1/2)
21:53:22,050 INFO  eu.stratosphere.pact.runtime.task.RegularPactTask             - Finished PACT code.: Count Words (2/2)
21:53:22,050 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from RUNNING to FINISHING for task Count Words (2/2)
21:53:22,051 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from RUNNING to FINISHING for task Count Words (2/2)
21:53:22,065 INFO  eu.stratosphere.pact.runtime.task.DataSinkTask                - Finished PACT code: Word Counts (2/2)
21:53:22,065 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from RUNNING to FINISHING for task Word Counts (2/2)
21:53:22,066 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from RUNNING to FINISHING for task Word Counts (2/2)
21:53:22,066 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from FINISHING to FINISHED for task Word Counts (2/2)
21:53:22,067 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from FINISHING to FINISHED for task Word Counts (2/2)
21:53:22,122 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from FINISHING to FINISHED for task Input Lines (1/2)
21:53:22,123 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from FINISHING to FINISHED for task Input Lines (2/2)
21:53:22,124 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from FINISHING to FINISHED for task Input Lines (1/2)
21:53:22,126 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from FINISHING to FINISHED for task Input Lines (2/2)
21:53:22,152 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from FINISHING to FINISHED for task Count Words (2/2)
21:53:22,153 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from FINISHING to FINISHED for task Count Words (2/2)
21:53:22,153 INFO  eu.stratosphere.nephele.jobmanager.scheduler.AbstractScheduler  - Releasing instance 127.0.0.1
21:53:22,156 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Status of job WordCount Example(1048a12f8f841c004e49c0ee9eec0c00) changed to FINISHED


Best Regards,
Fahimeh;;;","09/Jun/14 11:58;github-import;[Date: Sun Dec 08 13:43:18 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Hey,

which version of Stratosphere are you using? It looks to me like this version is a bit older.
The version you have does probably not include the web interface!
But I can tell you from the log files that the wordcount example did run successfully.

Robert;;;","09/Jun/14 11:58;github-import;[Date: Sun Dec 08 14:55:33 CET 2013, Author: [fhmt|https://github.com/fhmt]]

first of all i have downloaded ozone-master version and after that downloaded stratosphere version at address : stratosphere.eu but none of them did not work correctly, and job could not submitted. so i download stratosphere.box and i worked with vagrant command. 
how to can access to job manager web interface? What should I do??;;;","09/Jun/14 11:58;github-import;[Date: Sun Dec 08 15:07:55 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

What operating system are you using?

If you have Linux or Mac, the version at Stratosphere.eu should work.
For Windows, I'm not sure.

The virtual machine images are not updated so often, sorry.

You can also try to update the version inside the virtual machine


Sent from my iPhone

On 08.12.2013, at 14:55, fhmt <notifications@github.com> wrote:

first of all i have downloaded ozone-master version and after that
downloaded stratosphere version at address : stratosphere.eu but none of
them did not work correctly, and job could not submitted. so i download
stratosphere.box and i worked with vagrant command.
how to can access to job manager web interface? What should I do??

—
Reply to this email directly or view it on
GitHub<https://github.com/stratosphere/stratosphere/issues/324#issuecomment-30082036>
.;;;","09/Jun/14 11:58;github-import;[Date: Sun Dec 08 15:09:39 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I agree with Robert, it seems you have an old version. The 'ozone-master' version was a branch that went into the mainline months ago, so something seems to be off there. Can you tell me where you downloaded that version?

The version from the web-site (http://dopa.dima.tu-berlin.de/bin/stratosphere-0.4-SNAPSHOT.tgz) should have a rather recent build. Which one did you use?

;;;","09/Jun/14 11:58;github-import;[Date: Sun Dec 08 15:26:23 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I just downloaded the version from the web-site (http://dopa.dima.tu-berlin.de/bin/stratosphere-0.4-SNAPSHOT.tgz) and started it locally. On the Ubuntu Linux x64, the webinterface was at localhost:8081. 

Which OS are you using?;;;","09/Jun/14 11:58;github-import;[Date: Sun Dec 08 15:37:42 CET 2013, Author: [fhmt|https://github.com/fhmt]]

i am using windows 7 and cygwin terminal.  ;;;","09/Jun/14 11:58;github-import;[Date: Sun Dec 08 17:13:20 CET 2013, Author: [fhmt|https://github.com/fhmt]]

my problem is solved. 

thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn Documentation for Website,FLINK-323,12719493,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:58,09/Jun/14 11:58,14/Jul/23 05:57,09/Jun/14 11:58,,,,pre-apache,,,,,,,0,github-import,,"* Yarn documentation
* yarn is not ""in development"" anymore on the front page
* comments on all documentation pages
* made yarn uberjar more prominent on download page

Preview: http://robertmetzger.de/stratosphere/docs/setup/yarn.html

Our documentation is in a horrible state.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/323
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Thu Dec 05 20:59:54 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;pull-request-323-4998579244565296189.patch;https://issues.apache.org/jira/secure/attachment/12649086/pull-request-323-4998579244565296189.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397692,,,Mon Jun 09 11:58:21 UTC 2014,,,,,,,,,,"0|i1wgvj:",397819,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;[Date: Sat Dec 07 00:11:26 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Very nice, please merge.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speed up travis builds by distributing the deploys among the workers,FLINK-322,12719492,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:58,09/Jun/14 11:58,14/Jul/23 05:57,09/Jun/14 11:58,,,,pre-apache,,,,,,,0,github-import,,"The 6 workers now have the following responsibilities:

```
# 1. Deploy to sonatype (old hadoop)
# 2. Deploy to dopa (old hadoop)
# 3. Nothing
# 4. deploy to sonatype (yarn hadoop)
# 5. deploy to dopa (yarn hadoop)
# 6. Nothing
```
Especially worker 1. (which is always running out of time) does not need to download, build and upload the yarn stratosphere version.
This is done by worker 5 which has to download the yarn dependencies anyways.

I also changed the `ReOpenableHashTableITCase` to re-open the HashJoin from 5 to 3 times (the test case takes the most time)





---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/322
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Thu Dec 05 18:33:48 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;pull-request-322-2271627807912678227.patch;https://issues.apache.org/jira/secure/attachment/12649085/pull-request-322-2271627807912678227.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397691,,,Mon Jun 09 11:58:15 UTC 2014,,,,,,,,,,"0|i1wgvb:",397818,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;[Date: Sat Dec 07 19:38:41 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

This pull request does not fix these slow builds here (yarn commit): https://travis-ci.org/stratosphere/stratosphere/builds/15089732 because the tests fail even before the deployment script is called.
On my home machine, the yarn stratosphere needs ""Total time: 18:48.837s"" .. ;;;","09/Jun/14 11:58;github-import;[Date: Sat Dec 07 20:08:40 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [6a1869292b4ac99419fb8e54438d2b099cd3bcaf|https://github.com/stratosphere/stratosphere/commit/6a1869292b4ac99419fb8e54438d2b099cd3bcaf];;;","09/Jun/14 11:58;github-import;[Date: Sat Dec 07 20:11:40 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Guys, it's saturday night. Time to partey. :dancer: ;;;","09/Jun/14 11:58;github-import;[Date: Sat Dec 07 20:12:58 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Will be on the way shortly ;-)


On Sat, Dec 7, 2013 at 8:11 PM, Aljoscha Krettek
<notifications@github.com>wrote:

> Guys, it's saturday night. Time to partey. [image: :dancer:]
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/322#issuecomment-30062323>
> .
>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename Pregelsphere to Spargel (issue #310),FLINK-321,12719491,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:58,09/Jun/14 11:58,14/Jul/23 05:57,09/Jun/14 11:58,,,,pre-apache,,,,,,,0,github-import,,"This is issue ([#310|https://github.com/stratosphere/stratosphere/issues/310] | [FLINK-310|https://issues.apache.org/jira/browse/FLINK-310]).

Renamed the BSP API of @StephanEwen (maven Module `pregelsphere` and packages `vertexcentric`) to Spargel (both Maven module and packages). Spargel is German for asparagus.



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/321
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Thu Dec 05 18:00:38 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;pull-request-321-277118267900036173.patch;https://issues.apache.org/jira/secure/attachment/12649084/pull-request-321-277118267900036173.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397690,,,Mon Jun 09 11:58:09 UTC 2014,,,,,,,,,,"0|i1wgv3:",397817,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:58;github-import;[Date: Thu Dec 05 18:03:08 CET 2013, Author: [uce|https://github.com/uce]]

I also wanted to add a test case, but had problems when adding `pact-tests` as a test-scoped dependency to the pom.

The test case is just an adoption of the existing ConnectedComponentsITCase to use `SpargelConnectedComponents` instead of `WorksetConnectedComponents`. If anyone has an idea what I am missing, I could add the extra commit here.;;;","09/Jun/14 11:58;github-import;[Date: Fri Dec 06 15:02:48 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Do we always want to rename the packages to ""spargel""? We can keep the name Spargel for the project/feature, but call the packages ""vertexcentric"".;;;","09/Jun/14 11:58;github-import;[Date: Fri Dec 06 15:11:08 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Why? Will there also be a ""edgecentric"" implementation?
I vote for renaming the packages as well.;;;","09/Jun/14 11:58;github-import;[Date: Fri Dec 06 15:13:27 CET 2013, Author: [uce|https://github.com/uce]]

I think spargel as a package name is better from a user's perspective (aka I agree with @rmetzger).;;;","09/Jun/14 11:58;github-import;[Date: Fri Dec 06 15:18:08 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

There is actually an ""edgecentric"" view, Vasia is implementing something
similar to that. There are also other possible views of graph processing,
such as matrix views, ...


On Fri, Dec 6, 2013 at 3:13 PM, Ufuk Celebi <notifications@github.com>wrote:

> I think spargel as a package name is better from a user's perspective (aka
> I agree with @rmetzger <https://github.com/rmetzger>).
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/321#issuecomment-29996838>
> .
>;;;","09/Jun/14 11:58;github-import;[Date: Fri Dec 06 15:42:18 CET 2013, Author: [uce|https://github.com/uce]]

The last commit is a look into our bright feature without the name pact :sun_with_face:  @aljoscha will add the Scala API in `*.spargel.scala`.;;;","09/Jun/14 11:58;github-import;[Date: Sat Dec 07 20:08:55 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [696c8fe49dd644476014e9f1eea7c8832104efbb|https://github.com/stratosphere/stratosphere/commit/696c8fe49dd644476014e9f1eea7c8832104efbb];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accumulators,FLINK-320,12719490,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:57,09/Jun/14 11:58,14/Jul/23 05:57,09/Jun/14 11:58,,,,pre-apache,,,,,,,0,github-import,,"DON'T MERGE !!!

This pull request has the purpose to discuss the API for Accumulators (this is how I call it right now). I want to talk with Stephan tomorrow about implementation so good to have your feedback.

Please note: I mean Hadoop-style accumulators here: Write in the UDFs and ONLY read at the end of the job in the driver. Goal is also to make results available in next iteration.

I implemented the test case [```AccumulatorITCase```|https://github.com/andrehacker/ozone/blob/counters/pact/pact-tests/src/test/java/eu/stratosphere/pact/test/counters/AccumulatorITCase.java) which shows how you can use accumulators from the UDFs (see the Map UDF]. I implemented 4 accumulators which I think should be shipped (see in the [Accumulators package|https://github.com/andrehacker/ozone/tree/counters/pact/pact-common/src/main/java/eu/stratosphere/pact/common/stubs/accumulators)). I think we should support convenience methods for these because it should be as simple as possible from UDF. The user can implement it's own accumulator type.

About the status: The counters are working locally in the UDFs, but are not merged and sent to the JobManager and Client (for clarity I don't write about the plans how to do this here). And we will probably have to move many classes around (e.g. Accumulator needs to be available in Nephele). But we should discuss about the API first.

@StephanEwen: I could not/didn't want to use the existing Aggregator classes for some reasons. Anyway, I think it's better to think of a good interface first and then see how we can merge these two.

#### Some design decisions
- Introduced ExecutionResult for local and remote executor to get counter results (not working yet, only partially implemented)
- Accumulators are identified by name (initially not by Enum as in Hadoop)
- All accumulators share a single namespace (i.e. stored in a single HashTable]
- No need to register accumulators, they are created on the fly in the UDFs. This means that type conflicts are possible, e.g. different UDFs use accumulators with same name but different type. I already check for this.

So far, please let me know what you think, e.g. about naming, functionality. We must be able to integrate this in Scala. Our interfaces Accumulator and SimpleAccumulator are comparable to Sparks [Accumulable|https://spark.incubator.apache.org/docs/0.6.2/api/core/spark/Accumulable.html] and [Accumulator|https://spark.incubator.apache.org/docs/0.6.2/api/core/spark/Accumulator.html] classes. They have a [test suite for accumulators|https://github.com/apache/incubator-spark/blob/2ce200bf7f7a38afbcacf3303ca2418e49bdbe2a/core/src/test/scala/org/apache/spark/AccumulatorSuite.scala] showing their functionality and API.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/320
Created by: [andrehacker|https://github.com/andrehacker]
Labels: 
Created at: Wed Dec 04 18:57:31 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:57;github-import;pull-request-320-7202093536108220896.patch;https://issues.apache.org/jira/secure/attachment/12649083/pull-request-320-7202093536108220896.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397689,,,Mon Jun 09 11:57:59 UTC 2014,,,,,,,,,,"0|i1wguv:",397816,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:57;github-import;[Date: Wed Dec 04 19:38:11 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Cool. I like the quality and speed of your work!

I think you can move everything from the `common` packages to `generic` since they are not related to `PactRecord` (which is nice!)
I also like the accumulators you ship by default
Am I right that your example does not contain a ""usercode"" accumulator?
I'm not sure if it is a good idea to hard-wire the standard accumulators into the `RuntimeContext`
```
 IntCounter getIntCounter(String name);
 LongCounter getLongCounter(String name);
 DoubleCounter getDoubleCounter(String name);
```
maybe at most the longCounter() (because that's probably the most frequent use-case)
;;;","09/Jun/14 11:57;github-import;[Date: Thu Dec 05 11:28:59 CET 2013, Author: [andrehacker|https://github.com/andrehacker]]

Thanks robert

> I think you can move everything from the common packages to generic...

I moved it, but we have to move it again to some Nephele layer anyways (e.g. to do the merging in JobManager). Otherwise we would need a dependency from nephele to pact-common. We will see this when discussing about the implementation...

> Am I right that your example does not contain a ""usercode"" accumulator?

Very good point. Now I added a custom accumulator ```SetAccumulator<T>``` to the test case and realized two problems:
- The code of the custom accumulator will not be available in the JobManager since the jar-classes are not available there (afaik). So custom accumulators would need to be added as jar to the lib-dir of the server. But this would add security, because we would not run arbitrary user code on the JobManager (only admins can add jar-files to the JobManager).
- I couldn't use the generic ```getAccumulator(String name, Class<? extends Accumulator<V, A>> accumulatorClass)``` method to create a parametrized accumulator object (e.g. ```SetAccumulator<String>```). So I added a new function ```void addAccumulator(String name, Accumulator<V, A> accumulator)``` where the user creates an instance of it's custom accumulator previously. I commented-out the previous interface (to have only a single interface). Disadvantage is that the user can forget to add the counter.
- There is another API option: Accumulators themselves could care for being added to the RuntimeContext: ```IntCounter numLines = new IntCounter(this)``` (Constructor argument has type ```Stub``` and thus the accumulator can access the runtime context). This way we force the user to add the counter, but it's less transparent. This involves some abstract class and additional logic, because the runtime is not initialized when the UDF members are being initialized.

> I'm not sure if it is a good idea to hard-wire the standard accumulators into the RuntimeContext
> maybe at most the longCounter() (because that's probably the most frequent use-case)

Ok, we can leave this open for now, easy to change...;;;","09/Jun/14 11:57;github-import;[Date: Wed Dec 11 21:50:57 CET 2013, Author: [andrehacker|https://github.com/andrehacker]]

Update: Accumulators are working now, including custom accumulators.
Will talk with Stephan this week how to finalize. If anyone is interested to look at the code - don't hesitate.
The test case ```AccumulatorITCase``` and the new example ```WordCountAccumulators``` are good entry points (we can also remove the example later) .

@markus-h You could already have a look at ´´´AccumulatorManager``` to see how you can visualize the accumulators in web interface. There is a accumulator-history of the same size as the MemoryArchivist.

So far as an update...;;;","09/Jun/14 11:57;github-import;[Date: Thu Dec 12 13:38:43 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I had a look at the WordCountAccumulators example. 
Looks really good!;;;","09/Jun/14 11:57;github-import;[Date: Thu Dec 12 15:06:18 CET 2013, Author: [andrehacker|https://github.com/andrehacker]]

I will open a new pull request where everything is in a single commit;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rich scala stubs,FLINK-319,12719489,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:57,09/Jun/14 11:57,14/Jul/23 05:57,09/Jun/14 11:57,,,,pre-apache,,,,,,,0,github-import,,"Change Operators to accept stubs with open/close etc. You might call them rich stubs, but in the code it's just MapStub, FlatMapStub, FilterStub, ReduceStub, etc. They derive from MapStub etc. (the java one), so you can do things like getting the iteration context or what you might have...

This allows something like:
```scala
.map( new MapStub[(String, Int), (String, Int)] {
  override def open(config: Configuration) = {
    println(""Opening up this badboy."")
  }
  override def map(in: (String, Int)) = {
    println(""IN RICH MAPPER: "" + in)
    val (w, c) = in
    (w, c + 1)
  }
})
```
and likewise for flatMap and filter, for example:
```scala
.map( new FlatMapStub[(String, Int), (String, Int)] {
  override def open(config: Configuration) = {
    println(""Opening up this badboy."")
  }
  override def flatMap(in: (String, Int)) = {
    println(""IN RICH MAPPER: "" + in)
    in
  }
})
```

If people like this I will also make this available for all the other operators.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/319
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Wed Dec 04 16:01:11 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:57;github-import;pull-request-319-4737522734328591872.patch;https://issues.apache.org/jira/secure/attachment/12649082/pull-request-319-4737522734328591872.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397688,,,Mon Jun 09 11:57:45 UTC 2014,,,,,,,,,,"0|i1wgun:",397815,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:57;github-import;[Date: Wed Dec 04 16:11:01 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I think we need these rich interfaces. Otherwise, the Scala API is less expressive than the Java API which should not be the case.;;;","09/Jun/14 11:57;github-import;[Date: Sun Dec 08 16:40:38 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

This can now be merged, all operators have rich stubs.;;;","09/Jun/14 11:57;github-import;[Date: Tue Dec 10 13:13:03 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [c7febc1ff1c7d54313da29a07d2eaaf469aab252|https://github.com/stratosphere/stratosphere/commit/c7febc1ff1c7d54313da29a07d2eaaf469aab252];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
added jprofiler link to download page,FLINK-318,12719488,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:57,09/Jun/14 11:57,14/Jul/23 05:57,09/Jun/14 11:57,,,,pre-apache,,,,,,,0,github-import,,"link to jprofiler in the download page in order to get open source licences

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/318
Created by: [akunft|https://github.com/akunft]
Labels: 
Created at: Wed Dec 04 12:33:22 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:57;github-import;pull-request-318-7006830945766831358.patch;https://issues.apache.org/jira/secure/attachment/12649081/pull-request-318-7006830945766831358.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397687,,,2014-06-09 11:57:35.0,,,,,,,,,,"0|i1wguf:",397814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unserializable fields in UserCodeObjectWrapper.java,FLINK-317,12719487,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:57,09/Jun/14 11:57,14/Jul/23 05:57,09/Jun/14 11:57,,,,pre-apache,,,,,,,0,github-import,,"Completely change this. Add new ClosureCleaner that is based on the spark ClosureCleaner but does things a bit differently.

This one now checks whether fields it removes are actually accessed. In UserCodeObjectWrapper now throw an exception if the user code object contains non-serializable fields.

Does someone know what to do about the code-copying? Because I took this from spark.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/317
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Tue Dec 03 19:22:21 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:57;github-import;pull-request-317-3318278040852182948.patch;https://issues.apache.org/jira/secure/attachment/12649080/pull-request-317-3318278040852182948.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397686,,,Mon Jun 09 11:57:33 UTC 2014,,,,,,,,,,"0|i1wgu7:",397813,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:57;github-import;[Date: Wed Dec 04 16:39:01 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

What do we actually gain here? Unnecessary serializable fields stay. Necessary non-serializable fields still crash the code and in a rather intransparent way, because users assume they have set the variable.

A clean solution would be to to an check about what fields are actually referenced and clean all others. If non-serializable fields remain, report an error.;;;","09/Jun/14 11:57;github-import;[Date: Wed Dec 04 16:56:25 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Yes but I have bigger fish to fry right now, so I wanted this as a quick fix for some cases.;;;","09/Jun/14 11:57;github-import;[Date: Wed Dec 04 19:33:14 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I am a little worried that this makes problems hard to debug. If someone writes a program with a non-serializable data type in the closure, he does not get a nonserializable exception, but simply a null pointer at runtime. That is hard to trace back.

What do you think?
;;;","09/Jun/14 11:57;github-import;[Date: Mon Dec 09 16:19:51 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Completely new, read updated description.;;;","09/Jun/14 11:57;github-import;[Date: Tue Dec 10 14:27:28 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

That is fine, if we handle the license correctly. Please make sure that:
  - You write in the source file that it is in parts based on code from the spark project.
  - You add to the NOTICE.txt file an entry for spark, similar to the ones that are in there.;;;","09/Jun/14 11:57;github-import;[Date: Wed Dec 11 18:25:09 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Please remember to update the NOTICE file.;;;","09/Jun/14 11:57;github-import;[Date: Wed Dec 11 23:59:48 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Yes, yes, and also a notice in the code.;;;","09/Jun/14 11:57;github-import;[Date: Fri Dec 13 00:49:27 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [039c0829cfd0b7741feb881e865b91958032d9dc|https://github.com/stratosphere/stratosphere/commit/039c0829cfd0b7741feb881e865b91958032d9dc];;;","09/Jun/14 11:57;github-import;[Date: Fri Dec 13 09:00:29 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I don't see a update in the NOTICE file: https://github.com/stratosphere/stratosphere/blob/master/stratosphere-dist/src/main/stratosphere-bin/NOTICE.txt;;;","09/Jun/14 11:57;github-import;[Date: Sun Dec 15 11:09:11 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Why did you close the PR without any comment?
I still think my comment regarding the missing entry in the NOTICE file is valid.;;;","09/Jun/14 11:57;github-import;[Date: Sun Dec 15 12:44:38 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

I don't think I closed it on purpose. @StephanEwen said he wanted to merge anyways because of the restructuring. He could have just added the notice, though...

I didn't come around to adding it myself yet.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add JProfiler logo to website for free licences,FLINK-316,12719486,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:57,09/Jun/14 11:57,14/Jul/23 05:57,09/Jun/14 11:57,,,,pre-apache,,,,,,,0,github-import,,"We can get free open source licences for JProfiler if we put a text link / logo on our website. The question is where to put it. They prefer the frontpage or download page. I would prefer to put it in the footer cause it will destroy the look & feel of the site if we put it random in the download page in my opinion.

So, are there other opinions where to put the link / dislikes of the idea in general?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/316
Created by: [akunft|https://github.com/akunft]
Labels: website, 
Created at: Tue Dec 03 15:38:40 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397685,,,Mon Jun 09 11:57:22 UTC 2014,,,,,,,,,,"0|i1wgtz:",397812,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:57;github-import;[Date: Tue Dec 03 15:42:46 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Hi,

Thanks for taking care of this!
I would also like to have JProfiler licenses because it seems to be the best profiling tool available.
I'm against putting the link in the footer because it is too prominent. The only reason why they want a link on our page is to improve their page rank. This goal can also be achieved by putting the link on our download page.

The last line on the DL page is ""Feel free to contact us if you have any problems or suggestions."".
We could add a sentence like. ""This project uses JProfiler to achieve its outstanding performance. Thanks for the open source license from ej-technologies.""
;;;","09/Jun/14 11:57;github-import;[Date: Tue Dec 03 17:15:00 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Do we know that the conditions are from the JProfiler side? If it is okay to place it under downloads, than I would vote for that option.;;;","09/Jun/14 11:57;github-import;[Date: Tue Dec 03 18:33:45 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Their website states:

> We prefer the front page or the download page. 

http://www.ej-technologies.com/buy/jprofiler/openSource;;;","09/Jun/14 11:57;github-import;[Date: Tue Dec 03 18:35:06 CET 2013, Author: [uce|https://github.com/uce]]

OK, perfect. Let's do it as @rmetzger proposed on the Download page then.;;;","09/Jun/14 11:57;github-import;[Date: Tue Dec 03 21:27:53 CET 2013, Author: [akunft|https://github.com/akunft]]

Alright. I'll do it tomorrow.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add deferred updates to workset iterations (issue #21) and fix chaining of iteration state updates (issue #123),FLINK-315,12719485,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:57,09/Jun/14 11:57,14/Jul/23 05:57,09/Jun/14 11:57,,,,pre-apache,,,,,,,0,github-import,,"This pull request enables deferred updates for workset iterations (([#21|https://github.com/stratosphere/stratosphere/issues/21] | [FLINK-21|https://issues.apache.org/jira/browse/FLINK-21])) and iteration state updates by chained tasks (([#123|https://github.com/stratosphere/stratosphere/issues/123] | [FLINK-123|https://issues.apache.org/jira/browse/FLINK-123])).

---

**Deferred updates**: Before, workset iterations updated both the workset and solution set in the tail task. For connected components, this meant that the iteration tail (update components join) created both the workset for the next iteration *and* updated the solution set.

  With this pull request, it will be possible to do the following:

  - Intermediate workset update
  - Intermediate solution set update
  - (Separate) solution set tail update
  - (Separate) workset tail update
  - Unified solution set/workset tail update (as before)

  I extended `ConnectedComponentsNepheleITCase` with all possible variants. @vasia has a use case, where she further wants to test this (sorry, for letting you wait this long :bowtie:).

@StephanEwen: does it also make sense to have an intermediate task, which updates both sets and then has a tail later? It didn't think so and intermediate updates are exclusively workset or solution set updates. While posting this, I'm actually not so sure anymore. ;-)

@StephanEwen: I think it is correct that with the reprobing solution set update, both the build and probe side have to have the same type. Is this true?

---

**Chained iteration state updates**: If an iteration task has chained tasks, the last operator in the chain will do the state update.

  `IterationsWithChainingNepheleITCase` is a test case for this.

@StephanEwen: does it also make sense to have an intermediate chained driver to do the update and not just the last?

@StephanEwen: is it correct to assume that the last ChainedDriver has to have the same output type as the RegularPactTask it is used in?

---

When creating a Nephele JobGraph, you have to explicitly configure the type of update for the tail (before this the tail was always doing a workset update).

The relevant options are:
- setIsWorksetIteration()
- setIsWorksetUpdate()
- setIsSolutionSetUpdate()
- setIsSolutionSetUpdateWithoutReprobe()

Also the head has to wait for the solution set tail, if there is a separate one:
- setWaitForSolutionSetUpdate()

---

At some point, the overengineering bug bit me and I actually spent quite some time not doing anything productive with the code (aka this was basically finished a couple of weeks ago). There are quite some overlappings between the tail and intermediate task, which could be refactored. But the resulting code might be harder to understand. **Feedback wanted as I feel like not seeing the forest for the trees.**

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/315
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Tue Dec 03 14:44:53 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:57;github-import;pull-request-315-2385919635330800777.patch;https://issues.apache.org/jira/secure/attachment/12649079/pull-request-315-2385919635330800777.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397684,,,Mon Jun 09 11:57:15 UTC 2014,,,,,,,,,,"0|i1wgtr:",397811,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:57;github-import;[Date: Sun Dec 08 19:12:06 CET 2013, Author: [uce|https://github.com/uce]]

@StephanEwen, I've addressed the issues we discussed last week:

- Tab formatting of `AbstractIterativePactTask`
- Removed IntelliJ's `.*` import optimization, which I didn't notice
- Call `config.getWaitForSolutionSetUpdate()` only once in the head
- Undo my renaming from `INSTANCE` to `SINGLETON` in the `*Broker` classes
- The answer to whether it makes sense to have a delegate for `FastSolutionSetUpdateCollector` is imho yes: an intermediate solution set join, which updates the solution set, but then has a separate workset tail

I've also merged your changes from [53bb5bda61b55b1a38806f0a60dd3ce0590c3fd7|https://github.com/stratosphere/stratosphere/commit/53bb5bda61b55b1a38806f0a60dd3ce0590c3fd7] (superstep abstraction in Nephele readers).

PS: I've force pushed the changes to my branch, so there are no new commits for the changes.
;;;","09/Jun/14 11:57;github-import;[Date: Wed Dec 11 13:53:59 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Does the PR also fix https://github.com/stratosphere/stratosphere/issues/275 ?;;;","09/Jun/14 11:57;github-import;[Date: Wed Dec 11 14:10:08 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

It does not. The PR is purely about workset iterations...;;;","09/Jun/14 11:57;github-import;[Date: Thu Dec 12 13:02:03 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [3ec7cae5561f12096fb23e1d6f54c41af5cfdaff|https://github.com/stratosphere/stratosphere/commit/3ec7cae5561f12096fb23e1d6f54c41af5cfdaff];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Open() and close() not called equally often (combine related),FLINK-314,12719484,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:57,09/Jun/14 11:57,14/Jul/23 05:57,09/Jun/14 11:57,,,,pre-apache,,,,,,,0,github-import,,"Problem: The system sometimes calls open() and close() for reduce different amount of times. Given DOP1 and our wordcount example, open() is called 3-times and close() only twice. Both should be called twice, first for combine and then for reduce. See logs below.

First analysis: The first open() is called by DataSourceTask for combine (probably because InputFormat->Map->Combine are chained all together and thus executed from DataSourceTask). The second and third calls are coming from RegularPactTask (which executes the reduce). See the stack traces below.

How to reproduce:
Add open() and close() method to Reduce in WordCount example, add System.out.println(...); in the first line, set DOP to 1 and run using LocalExecutor.

__Log with Combine:__
Map: open
__Reduce: open__ (this is probably the call for combine)
Map: map
Map: map
Map: map
Reduce: combine
Reduce: reduce
Reduce: combine
Reduce: reduce
Reduce: combine
Reduce: reduce
Reduce: combine
Reduce: reduce
Reduce: combine
Reduce: reduce
Reduce: combine
Reduce: reduce
Reduce: combine
Reduce: reduce
Map: close
__Reduce: close__
__Reduce: open__
__Reduce: open__ (this is one call too much here)
Reduce: reduce
Reduce: reduce
Reduce: reduce
Reduce: reduce
Reduce: reduce
Reduce: reduce
Reduce: reduce
__Reduce: close__

__Log without combine__
Map: open
Map: map
Map: map
Map: map
Map: close
__Reduce: open__
Reduce: reduce
Reduce: reduce
Reduce: reduce
Reduce: reduce
Reduce: reduce
Reduce: reduce
Reduce: reduce
__Reduce: close__

#### Stack Traces

__First open()__
```
WordCount$CountWords.open(Configuration) line: 116  
RegularPactTask<S,OT>.openUserCode(Stub, Configuration) line: 1212  
SynchronousChainedCombineDriver<T>.openTask() line: 104 
RegularPactTask<S,OT>.openChainedTasks(List<ChainedDriver<?,?>>, AbstractInvokable) line: 1253  
DataSourceTask<OT>.invoke() line: 119   
RuntimeEnvironment.run() line: 344  
Thread.run() line: 744  
```

__Second open()__
```
WordCount$CountWords.open(Configuration) line: 116  
RegularPactTask<S,OT>.initInputLocalStrategy(int) line: 730 
RegularPactTask<S,OT>.initLocalStrategies(int) line: 578    
RegularPactTask<S,OT>.invoke() line: 275    
RuntimeEnvironment.run() line: 344  
Thread.run() line: 744
```

__Third open()__
```
WordCount$CountWords.open(Configuration) line: 116  
RegularPactTask<S,OT>.run() line: 361   
RegularPactTask<S,OT>.invoke() line: 291    
RuntimeEnvironment.run() line: 344  
Thread.run() line: 744  
```



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/314
Created by: [andrehacker|https://github.com/andrehacker]
Labels: bug, 
Created at: Tue Dec 03 12:00:57 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397683,,,Mon Jun 09 11:57:05 UTC 2014,,,,,,,,,,"0|i1wgtj:",397810,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:57;github-import;[Date: Tue Dec 03 18:43:06 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I think I know what is happening there:

The combiner is instantiated twice. Once as the combiner before the network data transfer. Second as the combiner on the Reduce side that preaggregates data in case the sort spills to disk. If the sort never spills, then the second combiner never gets called.

I suspect that it is the combiner inside the sorter that does not get closed.
;;;","09/Jun/14 11:57;github-import;[Date: Wed Dec 04 18:15:15 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [cf9fad89c2cdc3532237308257ecb45c2dbf7a65|https://github.com/stratosphere/stratosphere/commit/cf9fad89c2cdc3532237308257ecb45c2dbf7a65];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove discoveryManager,FLINK-313,12719483,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:56,09/Jun/14 11:57,14/Jul/23 05:57,09/Jun/14 11:57,,,,pre-apache,,,,,,,0,github-import,,"(do not merge immediately -- not tested with yarn. (should not make any difference) )

Fix for issue: https://github.com/stratosphere/stratosphere/issues/18

I removed the DiscoveryManager, as it is not really used.

The detection of the TaskManager's own IP address does not need a RPC service at the JobManager. It is determined based on the network adapter that successfully connects to the JobManager.

Please review my implementation.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/313
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Dec 02 21:33:37 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;pull-request-313-2749273961965754456.patch;https://issues.apache.org/jira/secure/attachment/12649078/pull-request-313-2749273961965754456.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397682,,,Mon Jun 09 11:57:01 UTC 2014,,,,,,,,,,"0|i1wgtb:",397809,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:57;github-import;[Date: Wed Dec 04 16:39:22 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Ready to be merged?;;;","09/Jun/14 11:57;github-import;[Date: Wed Dec 04 16:41:27 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

No. I'll take a quick look on the start-cluster.sh script.


On Wed, Dec 4, 2013 at 4:39 PM, Stephan Ewen <notifications@github.com>wrote:

> Ready to be merged?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/313#issuecomment-29815206>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","09/Jun/14 11:57;github-import;[Date: Sun Dec 08 19:03:44 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I removed the start-cluster.sh script changes, so ready to merge.;;;","09/Jun/14 11:57;github-import;[Date: Tue Dec 10 13:13:17 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [a6cbd9408ee7c5859d7ba0599c2554551bceca80|https://github.com/stratosphere/stratosphere/commit/a6cbd9408ee7c5859d7ba0599c2554551bceca80];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Experimental YARN support for Stratosphere,FLINK-312,12719482,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:56,09/Jun/14 11:56,14/Jul/23 05:57,09/Jun/14 11:56,,,,pre-apache,,,,,,,0,github-import,,"This pull request contains the code required to submit Stratosphere to a Hadoop (Yarn|2.2|NextGen) cluster.

How to:
Build the Stratosphere-yarn-uberjar:
```
mvn  package -Dhadoop.profile=2 -DskipTests
```
Run it
```
cd stratosphere-dist/target
java -jar stratosphere-dist-0.4-SNAPSHOT-yarn-uberjar.jar 
Missing required option: [-n Number of Yarn container to allocate (=Number of TaskTrackers)]
Usage:
   Required
     -n,--container <arg>   Number of Yarn container to allocate (=Number of TaskTrackers)
   Optional
     -c,--conf <arg>                 Path to Stratosphere configuration file
     -j,--jar <arg>                  Path to Stratosphere jar file
     -jm,--jobManagerMemory <arg>    Memory for JobManager Container [in MB]
     -tm,--taskManagerMemory <arg>   Memory per TaskManager Container [in MB]
     -tmc,--taskManagerCores <arg>   Virtual CPU cores per TaskManager
     -v,--verbose                    Verbose debug mode

```

The jar will automatically create a local `stratosphere-config.yml` in the local directory and use this.
The only requirement for this to run is that the `HADOOP_HOME` environment variable is set. My client is able to read the hadoop configuration from there and talk to the yarn resource manager.
Yarn must be set up with a HDFS, since I'm using HDFS to distribute the Stratosphere-jar and the modified configuration file (the config for the TaskManagers must me changed to have the container of the JobManager contained)
The only required argument is the number of TaskManagers (`-n`).
The JobManager runs as a separate thread in the ApplicationMaster. So the number of containers in yarn is always `n+1`.

Submit a job to the yarn stratosphere: You can just use the regular mechanisms to submit jobs. I also extended the pact-client.sh to support the `RemoteExecutor` written by @aljoscha.

```
java -cp stratosphere-dist-0.4-SNAPSHOT-yarn-uberjar.jar eu.stratosphere.pact.client.CliFrontend remote cloud-13:6123 /home/rmetzger/stratosphere-tutorial-reference-0.1-SNAPSHOT.jar \
  eu.stratosphere.tutorial.task4.WeightVectorPlan \
  ""hdfs:///user/robert/bigdataclass-wikipedia hdfs:///user/robert/bigdataclass-wikipedia-result $DOP""
```
The pact-client.sh with remote submission is undocumented and only hacked into the client. It is used like this:
```
Usage: [host:port] [jar] [class] [args]
```

What's missing
 * Extensive testing (I tested it locally, and on the Dima cluster with 4 nodes (and the tf-idf job (10 GB))
 * Proper tear-down (currently, you have to use yarn to kill the application)
 * More robustness towards failure: The whole thing breaks if one container is killed or has an error (yarn is very strict with memory limits) 
 * The code does not check if the requested resources (memory) actually exist. The whole thing will die with an exception if the user requests too much.
 * The web interface is not started, since I currently do not extract the web-interface files on the application master. This easy to fix.
 * The user can not send custom files with our jar. (But the user-jar can contain whatever the user wants).


Overall, this code is very user friendly: Basically one jar file to download. It should run with only one command line argument, everything else is extracted from the system

This implementation is similar to those of Spark and Tez: We allocate long-running containers. So you can submit multiple jobs and de-allocate the containers once everything is done.

I added this as ""stratosphere-yarn"" to stratosphere-addons. The work is based on https://github.com/hortonworks/simple-yarn-app.

Some maven stuff has been changed:
 * the cloudera repos have been moved from the main pom to the `pact-hbase` package, since everything else is independent from cloudera versions
 * the debian package has moved to a separate `debian-package` build profile, so the build should be a bit faster.
 * The uberjar should be deployed automatically to `dopa.dima.tu-berlin.de`.




---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/312
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Dec 02 19:58:59 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;pull-request-312-5095790516980664432.patch;https://issues.apache.org/jira/secure/attachment/12649077/pull-request-312-5095790516980664432.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397681,,,2014-06-09 11:56:51.0,,,,,,,,,,"0|i1wgt3:",397808,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update CONTRIBUTORS,FLINK-311,12719481,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:56,09/Jun/14 11:56,14/Jul/23 05:57,09/Jun/14 11:56,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/311
Created by: [emrehasan|https://github.com/emrehasan]
Labels: 
Created at: Mon Dec 02 15:36:32 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;pull-request-311-3147509831339104706.patch;https://issues.apache.org/jira/secure/attachment/12649076/pull-request-311-3147509831339104706.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397680,,,Mon Jun 09 11:56:49 UTC 2014,,,,,,,,,,"0|i1wgsv:",397807,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;[Date: Mon Dec 02 15:40:56 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

We will merge this together with the JDBC input format.;;;","09/Jun/14 11:56;github-import;[Date: Wed Dec 04 18:15:35 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [404f65648f305cf341c4ce6463fc2496adc99f04|https://github.com/stratosphere/stratosphere/commit/404f65648f305cf341c4ce6463fc2496adc99f04];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileInputFormat builder pattern cannot be used for GenericDataSources,FLINK-309,12719479,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:56,09/Jun/14 11:56,14/Jul/23 05:57,09/Jun/14 11:56,,,,pre-apache,,,,,,,0,github-import,,"And since FILE_PARAMETER_KEY is now private, we cannot set the file path without hacks.

``` 
public static ConfigBuilder configureFileFormat(FileDataSource target) {
                return new ConfigBuilder(target.getParameters());
}
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/309
Created by: [AHeise|https://github.com/AHeise]
Labels: bug, 
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Sun Dec 01 21:30:07 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397678,,,Mon Jun 09 11:56:34 UTC 2014,,,,,,,,,,"0|i1wgsf:",397805,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;[Date: Mon Dec 02 08:55:53 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Hi @AHeise,

You have to use the `setFilePath()` method now to set the file path. But yes, the builder pattern looks broken.

The bug was introduced by https://github.com/stratosphere/stratosphere/pull/284.
I'm assigning this to @StephanEwen.

I'm also a bit unhappy with this change: https://github.com/stratosphere/stratosphere/commit/9dd4635ad69bf107ab9f81c8e7ffd495475471e0#commitcomment-4705847


;;;","09/Jun/14 11:56;github-import;[Date: Tue Dec 03 19:42:41 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Does it help if the file input format builder accepts GenericDataSource?

In the long run, we want to refrain from the config based parameterization and simply pass parameterized objects. Not exposing the config keys are a step into that direction.;;;","09/Jun/14 11:56;github-import;[Date: Tue Dec 03 22:22:28 CET 2013, Author: [AHeise|https://github.com/AHeise]]

 Yes, that would be a valid option and probably the easiest fix.
________________________________
Von: Stephan Ewen [notifications@github.com]
Gesendet: Dienstag, 3. Dezember 2013 19:42
An: stratosphere/stratosphere
Cc: Heise, Arvid
Betreff: Re: [stratosphere] FileInputFormat builder pattern cannot be used for GenericDataSources (([#309|https://github.com/stratosphere/stratosphere/issues/309] | [FLINK-309|https://issues.apache.org/jira/browse/FLINK-309]))


Does it help if the file input format builder accepts GenericDataSource?

In the long run, we want to refrain from the config based parameterization and simply pass parameterized objects. Not exposing the config keys are a step into that direction.

—
Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/309#issuecomment-29738300>.;;;","09/Jun/14 11:56;github-import;[Date: Thu Dec 05 00:37:53 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

FileInputFormat builder accepts GenericDataSource in [162652326ef0d75f68ff35bc966676eedd91a89d|https://github.com/stratosphere/stratosphere/commit/162652326ef0d75f68ff35bc966676eedd91a89d]

Does that work for you? If yes, please close the issue, otherwise let me know where it still breaks.;;;","09/Jun/14 11:56;github-import;[Date: Tue Dec 10 16:30:01 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I assume this problem is solved?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added cobertura Maven plugin for test coverage report generation,FLINK-308,12719478,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:56,09/Jun/14 11:56,14/Jul/23 05:57,09/Jun/14 11:56,,,,pre-apache,,,,,,,0,github-import,,"Addresses issue ([#305|https://github.com/stratosphere/stratosphere/issues/305] | [FLINK-305|https://issues.apache.org/jira/browse/FLINK-305]).
The regular build process is not affect by this change.

Test coverage reports can be generated by calling 
```
mvn clean cobertura:cobertura
```

The HTML reports are generated for each Maven module individually and are put into ./target/site/cobertura.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/308
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Fri Nov 29 12:22:52 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;pull-request-308-661937602385150721.patch;https://issues.apache.org/jira/secure/attachment/12649075/pull-request-308-661937602385150721.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397677,,,Mon Jun 09 11:56:28 UTC 2014,,,,,,,,,,"0|i1wgs7:",397804,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;[Date: Wed Dec 04 18:16:15 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [39ddc96d85e998838f191ff409dc968430c10e1a|https://github.com/stratosphere/stratosphere/commit/39ddc96d85e998838f191ff409dc968430c10e1a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataSinkTask processes union input,FLINK-307,12719477,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:56,09/Jun/14 11:56,14/Jul/23 05:57,09/Jun/14 11:56,,,,pre-apache,,,,,,,0,github-import,,"Extended DataSinkTask to handle union input. Fixes issue ([#112|https://github.com/stratosphere/stratosphere/issues/112] | [FLINK-112|https://issues.apache.org/jira/browse/FLINK-112]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/307
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Milestone: Release 0.4
Created at: Thu Nov 28 16:34:52 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;pull-request-307-5487818062230143451.patch;https://issues.apache.org/jira/secure/attachment/12649074/pull-request-307-5487818062230143451.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397676,,,Mon Jun 09 11:56:24 UTC 2014,,,,,,,,,,"0|i1wgrz:",397803,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;[Date: Thu Dec 05 00:36:35 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [1a527afaf3bcda8099f96285884452f87186b1ca|https://github.com/stratosphere/stratosphere/commit/1a527afaf3bcda8099f96285884452f87186b1ca];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor scala input output,FLINK-306,12719476,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:56,09/Jun/14 11:56,14/Jul/23 05:57,09/Jun/14 11:56,,,,pre-apache,,,,,,,0,github-import,,"This sits on ([#301|https://github.com/stratosphere/stratosphere/issues/301] | [FLINK-301|https://issues.apache.org/jira/browse/FLINK-301]), so only merge after that one's in.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/306
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Thu Nov 28 14:24:50 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;pull-request-306-1480792549611467527.patch;https://issues.apache.org/jira/secure/attachment/12649073/pull-request-306-1480792549611467527.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397675,,,Mon Jun 09 11:56:19 UTC 2014,,,,,,,,,,"0|i1wgrr:",397802,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;[Date: Thu Nov 28 20:08:06 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [cad8b4e717927b86f7b3499cc6cda8a8d28f2804|https://github.com/stratosphere/stratosphere/commit/cad8b4e717927b86f7b3499cc6cda8a8d28f2804];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typo on startpage and make sub headlines bold,FLINK-304,12719474,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:56,09/Jun/14 11:56,14/Jul/23 05:57,09/Jun/14 11:56,,,,pre-apache,,,,,,,0,github-import,,"1. @cboden noticed a typo on the front page: **constrainT MapReduce interface** should be **constrainED MapReduce interface**.

2. I've also put more emphasis on the headlines, which makes it imho easier to navigate the page.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/304
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Wed Nov 27 18:50:08 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;pull-request-304-8597563921121153522.patch;https://issues.apache.org/jira/secure/attachment/12649072/pull-request-304-8597563921121153522.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397673,,,Mon Jun 09 11:56:07 UTC 2014,,,,,,,,,,"0|i1wgrb:",397800,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:56;github-import;[Date: Wed Nov 27 18:58:13 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Preview http://robertmetzger.de/stratosphere/ (with https://github.com/stratosphere/stratosphere/pull/290)

I don't like the `<hr>` (the line) below the headlines.?
You are separating the headline from its content? That does not make much sense to me.;;;","09/Jun/14 11:56;github-import;[Date: Wed Nov 27 19:00:48 CET 2013, Author: [uce|https://github.com/uce]]

Ah, sorry. I also had a preview. Forgot to post it.

This is the default CSS element for page headers, e.g. css class `.page-header`.

http://getbootstrap.com/components/#page-header

PS: putting an element like `hr` *by hand* (to indicatae separate content) wouldn't make sense, but the line now actually is just layout and has no semantics (like separating content).;;;","09/Jun/14 11:56;github-import;[Date: Wed Nov 27 20:29:37 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Yes, but this is for the page header. Here, we have only a subheading.

And still, I don't like it ;)


On Wed, Nov 27, 2013 at 7:00 PM, Ufuk Celebi <notifications@github.com>wrote:

> Ah, sorry. I also had a preview. Forgot to post it.
>
> This is the default CSS element for page headers, e.g. css class
> .page-header.
>
> http://getbootstrap.com/components/#page-header
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/304#issuecomment-29406001>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","09/Jun/14 11:56;github-import;[Date: Wed Nov 27 23:39:57 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I agree with Robert here.
I would have hr above the headline to have the individual sections clearly separated.
Otherwise a section's content and the headline of the next section form a visual block.;;;","09/Jun/14 11:56;github-import;[Date: Wed Nov 27 23:52:10 CET 2013, Author: [uce|https://github.com/uce]]

I agree (in this case :bowtie:). The preview at http://uce.github.io/ should be up to date any minute.;;;","09/Jun/14 11:56;github-import;[Date: Thu Nov 28 18:38:32 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Are you merging this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fixed TableInputFormat returns no records when re-opened,FLINK-302,12719472,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:55,09/Jun/14 11:55,14/Jul/23 05:57,09/Jun/14 11:55,,,,pre-apache,,,,,,,0,github-import,,"Currently TableInputFormat can only scan a single split.

The endReached flag is not reset when a new split is opened, so once the end of any split is reached, an instance cannot be used to scan another split (which is what Stratosphere tries) to do.

This sophisticated fix should resolve the issue.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/302
Created by: [mleich|https://github.com/mleich]
Labels: 
Created at: Wed Nov 27 15:45:55 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:55;github-import;pull-request-302-2652420759127177537.patch;https://issues.apache.org/jira/secure/attachment/12649071/pull-request-302-2652420759127177537.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397671,,,Mon Jun 09 11:55:55 UTC 2014,,,,,,,,,,"0|i1wgqv:",397798,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:55;github-import;[Date: Thu Nov 28 20:08:19 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [b44349e8de1363077d4be2b96f558eccb26169e1|https://github.com/stratosphere/stratosphere/commit/b44349e8de1363077d4be2b96f558eccb26169e1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix logging,FLINK-301,12719471,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:55,09/Jun/14 11:55,14/Jul/23 05:57,09/Jun/14 11:55,,,,pre-apache,,,,,,,0,github-import,,"Make the scala schema printers use log4j

Fix the additional syserr appender in LocalExecutor: add a threshold so
that it does only output error messages

Make JobClient use log4j for the event output that was annoying me

This sits on ([#298|https://github.com/stratosphere/stratosphere/issues/298] | [FLINK-298|https://issues.apache.org/jira/browse/FLINK-298]), so only merge after that one's in.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/301
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Wed Nov 27 12:20:26 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:55;github-import;pull-request-301-6506555809265351506.patch;https://issues.apache.org/jira/secure/attachment/12649070/pull-request-301-6506555809265351506.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397670,,,Mon Jun 09 11:55:50 UTC 2014,,,,,,,,,,"0|i1wgqn:",397797,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:55;github-import;[Date: Thu Nov 28 20:08:35 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [14a6cb93296ce1ae03425adafe918ed8d4ee473a|https://github.com/stratosphere/stratosphere/commit/14a6cb93296ce1ae03425adafe918ed8d4ee473a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add SingleInstanceDataSource for inputs that are not splittable,FLINK-299,12719469,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:55,09/Jun/14 11:55,14/Jul/23 05:57,09/Jun/14 11:55,,,,pre-apache,,,,,,,0,github-import,,"This DataSource is required for https://github.com/stratosphere/stratosphere/pull/279 and the upcoming MongoDB connector.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/299
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue Nov 26 20:08:31 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:55;github-import;pull-request-299-9034394787823822995.patch;https://issues.apache.org/jira/secure/attachment/12649069/pull-request-299-9034394787823822995.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397668,,,Mon Jun 09 11:55:37 UTC 2014,,,,,,,,,,"0|i1wgq7:",397795,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:55;github-import;[Date: Wed Nov 27 08:26:20 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I had a little chat about this with @StephanEwen. We decided to add a method to the InputFormat `getMaxDOP()` to tell the framework the max dop.

I had an additional idea for this: We could also add a method to tell the framework a locality preference! it would really make sense to locate the DataSource on the same machine where the database etc. runs.
I guess the `SingleInstanceDataSource` is more suited for this since locality information should not be hardcoded into the input format.;;;","09/Jun/14 11:55;github-import;[Date: Fri Nov 29 01:15:42 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

c61bc993f9dd4825199c922086c5ed6c63e4c791 introduces a marker for input formats to declare their input as unsplittable (such as for database queries in most cases).

The long term alternative is to use the number of input splits as an upper bound for the degree of parallelism (we need to move the optimizer to the master server process before that though). Since a database query input format would typically create only a single input split, it would thereby declare that its input is unsplittable.

Should we still have the dedicated data source node suggested by this pull request?;;;","09/Jun/14 11:55;github-import;[Date: Tue Dec 03 09:28:48 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Not working;;;","09/Jun/14 11:55;github-import;[Date: Wed Dec 04 18:16:51 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [3e0d97416f761516f67e487ef997f4ec7a1d8fb5|https://github.com/stratosphere/stratosphere/commit/3e0d97416f761516f67e487ef997f4ec7a1d8fb5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extend csv format ctors,FLINK-298,12719468,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:55,09/Jun/14 11:55,14/Jul/23 05:57,09/Jun/14 11:55,,,,pre-apache,,,,,,,0,github-import,,"This builds on ([#293|https://github.com/stratosphere/stratosphere/issues/293] | [FLINK-293|https://issues.apache.org/jira/browse/FLINK-293]), so only merge after that one is merged.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/298
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Tue Nov 26 17:00:03 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:55;github-import;pull-request-298-7051308138949523063.patch;https://issues.apache.org/jira/secure/attachment/12649068/pull-request-298-7051308138949523063.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397667,,,Mon Jun 09 11:55:31 UTC 2014,,,,,,,,,,"0|i1wgpz:",397794,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:55;github-import;[Date: Thu Nov 28 20:09:00 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [175df978d13a62a7d6c71c93e91b75bff39e0cdc|https://github.com/stratosphere/stratosphere/commit/175df978d13a62a7d6c71c93e91b75bff39e0cdc];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Writing long UTF encoded Strings into PactRecords,FLINK-294,12719464,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:55,09/Jun/14 11:55,14/Jul/23 05:57,09/Jun/14 11:55,,,,pre-apache,,,,,,,0,github-import,,"I've implemented my own data type that represents a web document (URL, text content, some metadata).
When serializing this document into PactRecord I use writeUTF to write the text content.

I often encounter documents that are longer than 65535 bytes in UTF encoding.

https://github.com/stratosphere/stratosphere/blob/master/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java#L1564

What can I do to fix that?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/294
Created by: [mleich|https://github.com/mleich]
Labels: 
Created at: Tue Nov 26 13:35:03 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397663,,,Mon Jun 09 11:55:14 UTC 2014,,,,,,,,,,"0|i1wgp3:",397790,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:55;github-import;[Date: Tue Nov 26 14:38:50 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The writeUTF method is specified in the jaava DataOutput interface to
encode size in 2 bytes. PactString uses a different way of representing its
data, for that exact reason.

I do not think we can change writeUTF without breaking some functionality
(especially the aspired Hadoop compatibility). Can you use the PactString
Logic? Maybe refactor the PactString such that you have a static utility
method that lets you write char sequence or so to the stream?


On Tue, Nov 26, 2013 at 1:35 PM, mleich <notifications@github.com> wrote:

> I've implemented my own data type that represents a web document (URL,
> text content, some metadata).
> When serializing this document into PactRecord I use writeUTF to write the
> text content.
>
> I often encounter documents that are longer than 65535 bytes in UTF
> encoding.
>
>
> https://github.com/stratosphere/stratosphere/blob/master/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java#L1564
>
> What can I do to fix that?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/294>
> .
>;;;","09/Jun/14 11:55;github-import;[Date: Thu Nov 28 20:07:01 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Since we cannot change the writeUTF method (its functionality is specified by the java interface), I have added static utility serialization methods to PactString, that support long string serialization.

```
public class MyLongStringType {
	private String theString;

	public void read(DataInput in) {
		theString = PactString.readString(in);
	}

	public void write(DataOutput out) {
		PactString.writeString(theString, out);
	}
}

Fixed in [38725c715924a38432b3c16d0e781d630b7b0500|https://github.com/stratosphere/stratosphere/commit/38725c715924a38432b3c16d0e781d630b7b0500];;;","09/Jun/14 11:55;github-import;[Date: Thu Nov 28 20:07:43 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

If this solution does not cover your case, please re-open this issue!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala fixings,FLINK-293,12719463,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:55,09/Jun/14 11:55,14/Jul/23 05:57,09/Jun/14 11:55,,,,pre-apache,,,,,,,0,github-import,,"Fix a lot of smaller things.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/293
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Tue Nov 26 13:25:28 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:55;github-import;pull-request-293-4715358195030393928.patch;https://issues.apache.org/jira/secure/attachment/12649067/pull-request-293-4715358195030393928.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397662,,,Mon Jun 09 11:55:10 UTC 2014,,,,,,,,,,"0|i1wgov:",397789,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:55;github-import;[Date: Thu Nov 28 20:09:33 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [817b4ecfb9e146bbeb06c0210ca0c76d3b88671b|https://github.com/stratosphere/stratosphere/commit/817b4ecfb9e146bbeb06c0210ca0c76d3b88671b];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Write documentation on how to create a new InputFormat/Connector for Stratosphere,FLINK-292,12719462,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:55,09/Jun/14 11:55,14/Jul/23 05:57,09/Jun/14 11:55,,,,pre-apache,,,,,,,0,github-import,,"As a follow up to this (https://groups.google.com/forum/#!topic/stratosphere-dev/0yTcx4ljlOE) we should add a little document helping new users and contributors to write connectors to other systems.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/292
Created by: [rmetzger|https://github.com/rmetzger]
Labels: documentation, website, 
Created at: Tue Nov 26 13:08:17 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397661,,,Mon Jun 09 11:55:04 UTC 2014,,,,,,,,,,"0|i1wgon:",397788,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:55;github-import;[Date: Tue Nov 26 14:10:49 CET 2013, Author: [uce|https://github.com/uce]]

http://stratosphere.eu/docs/programming_guides/java.html#io_formats;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManager webfrontend redesign and extension,FLINK-291,12719461,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:54,09/Jun/14 11:55,14/Jul/23 05:57,09/Jun/14 11:55,,,,pre-apache,,,,,,,0,github-import,,"New Features:
* New Design for webfrontend
* Dependency graph colored
* Cancel Jobs
* Show number of registered taskmanagers
* History of old jobs
* Timeline analysis of old jobs

Impressions:
![bildschirmfoto vom 2013-11-26 12 10 04|https://f.cloud.github.com/assets/724330/1621439/1bd8002e-568e-11e3-8c11-80fe950092c7.png]
![bildschirmfoto vom 2013-11-26 12 01 58|https://f.cloud.github.com/assets/724330/1621441/26ba7b98-568e-11e3-8620-3a717cddb77b.png]


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/291
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Tue Nov 26 12:31:01 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:54;github-import;pull-request-291-2397245501887631935.patch;https://issues.apache.org/jira/secure/attachment/12649066/pull-request-291-2397245501887631935.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397660,,,Mon Jun 09 11:55:00 UTC 2014,,,,,,,,,,"0|i1wgof:",397787,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:54;github-import;[Date: Wed Nov 27 08:39:31 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

The merge conflict is in `JobmanagerInfoServlet.java`. I guess it's because Stephan changed some details there too.;;;","09/Jun/14 11:54;github-import;[Date: Wed Nov 27 21:13:25 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

We should add a text in the lower ""Tasks"" box if no task is selected, that explains that one can select task from the upper box!;;;","09/Jun/14 11:54;github-import;[Date: Thu Dec 05 00:38:47 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [29e67aed62a437f07744285bd3fdfe9fca675c51|https://github.com/stratosphere/stratosphere/commit/29e67aed62a437f07744285bd3fdfe9fca675c51];;;","09/Jun/14 11:54;github-import;[Date: Fri Dec 06 13:16:33 CET 2013, Author: [markus-h|https://github.com/markus-h]]

Sorry for that, I added the missing file.;;;","09/Jun/14 11:55;github-import;[Date: Fri Dec 06 15:36:18 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Thank you @markus-h. I pushed the fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unified frontpage figures,FLINK-290,12719460,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:54,09/Jun/14 11:54,14/Jul/23 05:57,09/Jun/14 11:54,,,,pre-apache,,,,,,,0,github-import,,"All figures adapted to width of 800px. 
Uniform size of font (approx.) and data flow nodes.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/290
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Tue Nov 26 11:58:27 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:54;github-import;pull-request-290-2284272433052176474.patch;https://issues.apache.org/jira/secure/attachment/12649065/pull-request-290-2284272433052176474.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397659,,,Mon Jun 09 11:54:45 UTC 2014,,,,,,,,,,"0|i1wgo7:",397786,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:54;github-import;[Date: Tue Nov 26 12:04:15 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I merged your changes into my preview: http://robertmetzger.de/stratosphere/;;;","09/Jun/14 11:54;github-import;[Date: Wed Nov 27 18:58:51 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

The corners of the figures are cut-off.;;;","09/Jun/14 11:54;github-import;[Date: Wed Nov 27 19:15:59 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Will take care of this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"New maven project ""stratosphere-addons"".",FLINK-289,12719459,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:54,09/Jun/14 11:54,14/Jul/23 05:57,09/Jun/14 11:54,,,,pre-apache,,,,,,,0,github-import,,"This pull request introduces the new sub-project structure suggested in https://github.com/stratosphere/stratosphere/issues/199#issuecomment-29057242

This PR moves pact-array-datamodel and pact-hbase to stratosphere-addons


I will also add the avro input format and the simple hbase output format soon.
Pull Request https://github.com/stratosphere/stratosphere/pull/279 depends on this, because I would like to add the jdbc input format as an addon.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/289
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue Nov 26 08:32:49 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:54;github-import;pull-request-289-570038242612419486.patch;https://issues.apache.org/jira/secure/attachment/12649064/pull-request-289-570038242612419486.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397658,,,2014-06-09 11:54:26.0,,,,,,,,,,"0|i1wgnz:",397785,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster Setup: Shared Directory is not required,FLINK-287,12719457,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:54,09/Jun/14 11:54,14/Jul/23 05:57,09/Jun/14 11:54,,,,pre-apache,,,,,,,0,github-import,,"The Cluster Setup instructions in the Quickstart states that a shared folder is required for a cluster setup. This is not true. Although this is convenient for small setups, it might even cause problems for large setups (e.g., in case of extensive logfile writing...) and it prohibits custom configurations for individual nodes, e.g., in case of heterogeneous hardware / software setups...

The start and stop scripts only require that the Stratosphere folder is located at the same path on each node (which might not even be given with a shared folder as each node can mount it to any path).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/287
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, documentation, website, 
Milestone: Release 0.4
Created at: Mon Nov 25 22:06:26 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397656,,,Mon Jun 09 11:54:21 UTC 2014,,,,,,,,,,"0|i1wgnj:",397783,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:54;github-import;[Date: Mon Nov 25 22:08:42 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Thank you. I'll include the fix into the pull request for the new front page. https://github.com/stratosphere/stratosphere/pull/283;;;","09/Jun/14 11:54;github-import;[Date: Mon Nov 25 22:22:21 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Preview: http://robertmetzger.de/stratosphere/quickstart/setup.html;;;","09/Jun/14 11:54;github-import;[Date: Mon Nov 25 22:44:18 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Can you change the first bullet point to:

""1. Copy Stratosphere to the same file system path on each node of your setup,""

or similar?;;;","09/Jun/14 11:54;github-import;[Date: Mon Nov 25 22:50:59 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Sure.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Writing to local file system in distributed operation does not work properly,FLINK-286,12719456,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:54,09/Jun/14 11:54,14/Jul/23 05:57,09/Jun/14 11:54,,,,pre-apache,,,,,,,0,github-import,,"I have a task that runs on two cluster nodes.
The output is set to ""file:///home/rmetzger/cmpDiscount/"", the `FileDataSink` has a DOP of 64.

After the job has finished, on host1, there is a directory in `/home/rmetzger/` called `cmpDiscount/` with 32 files inside.
On host2 on the other hand, there is only one file in my home directory, called `cmpDiscount`. So only one of the 32 threads on host2 writes into the file system?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/286
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, core, 
Assignee: [fhueske|https://github.com/fhueske]
Created at: Mon Nov 25 11:21:20 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397655,,,Mon Jun 09 11:54:15 UTC 2014,,,,,,,,,,"0|i1wgnb:",397782,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:54;github-import;[Date: Mon Nov 25 12:14:49 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Did the job produce any errors? Did the log mention failed operations?
Am 25.11.2013 11:21 schrieb ""Robert Metzger"" <notifications@github.com>:

> I have a task that runs on two cluster nodes.
> The output is set to ""file:///home/rmetzger/cmpDiscount/"", the
> FileDataSink has a DOP of 64.
>
> After the job has finished, on host1, there is a directory in
> /home/rmetzger/ called cmpDiscount/ with 32 files inside.
> On host2 on the other hand, there is only one file in my home directory,
> called cmpDiscount. So only one of the 32 threads on host2 writes into
> the file system?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/286>
> .
>;;;","09/Jun/14 11:54;github-import;[Date: Mon Nov 25 12:27:02 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

With INFO logging level, there are no warnings, nothing fails.;;;","09/Jun/14 11:54;github-import;[Date: Tue Nov 26 21:05:41 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Even though the file exists, I got this exception (I guess its the same bug)
 (Just for the one who is going to fix this)
```
21:05:43,844 ERROR eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask       - java.lang.RuntimeException: Stream to output file could not be opened: Opening the file output stream failed: File file:/home/rmetzger/cmpAge does not exis
t.
        at eu.stratosphere.pact.generic.io.FileOutputFormat.open(FileOutputFormat.java:115)
        at eu.stratosphere.pact.common.io.RecordOutputFormat.open(RecordOutputFormat.java:167)
        at eu.stratosphere.pact.runtime.task.DataSinkTask.invoke(DataSinkTask.java:163)
        at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
        at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.IOException: Opening the file output stream failed: File file:/home/rmetzger/cmpAge does not exist.
        at eu.stratosphere.pact.generic.io.FileOutputFormat$OutputPathOpenThread.waitForCompletion(FileOutputFormat.java:202)
        at eu.stratosphere.pact.generic.io.FileOutputFormat.open(FileOutputFormat.java:112)
        ... 4 more
Caused by: java.io.FileNotFoundException: File file:/home/rmetzger/cmpAge does not exist.
        at eu.stratosphere.nephele.fs.file.LocalFileSystem.getFileStatus(LocalFileSystem.java:106)
        at eu.stratosphere.pact.generic.io.FileOutputFormat$OutputPathOpenThread.run(FileOutputFormat.java:162)
```;;;","09/Jun/14 11:54;github-import;[Date: Tue Nov 26 21:29:19 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Does the file (or directory) really not exist, or is the message wrong?


On Tue, Nov 26, 2013 at 9:05 PM, Robert Metzger <notifications@github.com>wrote:

> Even though the file exists, I got this exception (I guess its the same
> bug)
> (Just for the one who is going to fix this)
>
> 21:05:43,844 ERROR eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask       - java.lang.RuntimeException: Stream to output file could not be opened: Opening the file output stream failed: File file:/home/rmetzger/cmpAge does not exis
> t.
>         at eu.stratosphere.pact.generic.io.FileOutputFormat.open(FileOutputFormat.java:115)
>         at eu.stratosphere.pact.common.io.RecordOutputFormat.open(RecordOutputFormat.java:167)
>         at eu.stratosphere.pact.runtime.task.DataSinkTask.invoke(DataSinkTask.java:163)
>         at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
>         at java.lang.Thread.run(Thread.java:679)
> Caused by: java.io.IOException: Opening the file output stream failed: File file:/home/rmetzger/cmpAge does not exist.
>         at eu.stratosphere.pact.generic.io.FileOutputFormat$OutputPathOpenThread.waitForCompletion(FileOutputFormat.java:202)
>         at eu.stratosphere.pact.generic.io.FileOutputFormat.open(FileOutputFormat.java:112)
>         ... 4 more
> Caused by: java.io.FileNotFoundException: File file:/home/rmetzger/cmpAge does not exist.
>         at eu.stratosphere.nephele.fs.file.LocalFileSystem.getFileStatus(LocalFileSystem.java:106)
>         at eu.stratosphere.pact.generic.io.FileOutputFormat$OutputPathOpenThread.run(FileOutputFormat.java:162)
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/286#issuecomment-29328834>
> .
>;;;","09/Jun/14 11:54;github-import;[Date: Tue Nov 26 21:31:26 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

There is a file with exactly that name but since the dop on that node is higher than 1, there should be a directory.
So the exception message is wrong.
But I assume that the system creates the directory if it does not exist. (it does on the host where I start the job);;;","09/Jun/14 11:54;github-import;[Date: Tue Nov 26 21:33:36 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Ah, there is your solution: The Directory is created by the master when the
outputs are prepared. They are not created by each workers.

We should change that ;-)


On Tue, Nov 26, 2013 at 9:31 PM, Robert Metzger <notifications@github.com>wrote:

> There is a file with exactly that name but since the dop on that node is
> higher than 1, there should be a directory.
> So the exception message is wrong.
> But I assume that the system creates the directory if it does not exist.
> (it does on the host where I start the job)
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/286#issuecomment-29331052>
> .
>;;;","09/Jun/14 11:54;github-import;[Date: Tue Nov 26 21:34:31 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Preparing the output happens during the JobGraph preparation and checking
phase


On Tue, Nov 26, 2013 at 9:33 PM, Stephan Ewen <ewenstephan@gmail.com> wrote:

> Ah, there is your solution: The Directory is created by the master when
> the outputs are prepared. They are not created by each workers.
>
> We should change that ;-)
>
>
> On Tue, Nov 26, 2013 at 9:31 PM, Robert Metzger <notifications@github.com>wrote:
>
>> There is a file with exactly that name but since the dop on that node is
>> higher than 1, there should be a directory.
>> So the exception message is wrong.
>> But I assume that the system creates the directory if it does not exist.
>> (it does on the host where I start the job)
>>
>> —
>> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/286#issuecomment-29331052>
>> .
>>
>
>;;;","09/Jun/14 11:54;github-import;[Date: Tue Dec 17 23:04:03 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I started with this one.;;;","09/Jun/14 11:54;github-import;[Date: Wed Dec 18 23:58:50 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Fixed, waiting for ([#257|https://github.com/stratosphere/stratosphere/issues/257] | [FLINK-257|https://issues.apache.org/jira/browse/FLINK-257]) to be merged.;;;","09/Jun/14 11:54;github-import;[Date: Thu Jan 30 10:45:35 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Fixed in ([#421|https://github.com/stratosphere/stratosphere/issues/421] | [FLINK-421|https://issues.apache.org/jira/browse/FLINK-421]).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stratosphere TaskManager dies in PactRecordComparator.<init>,FLINK-285,12719455,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:54,09/Jun/14 11:54,14/Jul/23 05:57,09/Jun/14 11:54,,,,pre-apache,,,,,,,0,github-import,,"Hi,

while working with Stratosphere, the TaskManager on one node died.

I got the following from stdout:
```
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f6e794bbc81, pid=1847, tid=140048604563200
#
# JRE version: 6.0_27-b27
# Java VM: OpenJDK 64-Bit Server VM (20.0-b12 mixed mode linux-amd64 )
# Derivative: IcedTea6 1.12.6
# Distribution: Ubuntu 12.04 LTS, package 6b27-1.12.6-1ubuntu0.12.04.2
# Problematic frame:
# J  eu.stratosphere.pact.runtime.plugable.pactrecord.PactRecordComparator.<init>([I[Ljava/lang/Class;[Z)V
#
# An error report file with more information is saved as:
# /home/rmetzger/hs_err_pid1847.log
#
# If you would like to submit a bug report, please include
# instructions how to reproduce the bug and visit:
#   https://bugs.launchpad.net/ubuntu/+source/openjdk-6/
#
```

This is the dmesg output:
```
[1703175.518529] INFO: task java:1847 blocked for more than 120 seconds.
[1703175.519002] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[1703175.519593] java            D 0000000000000011     0  1847      1 0x00000000
[1703175.519608]  ffff8810872d9cd8 0000000000000086 0000000000000000 ffffffffffffffe0
[1703175.519613]  ffff8810872d9fd8 ffff8810872d9fd8 ffff8810872d9fd8 00000000000139c0
[1703175.519616]  ffff88202413dc00 ffff881088ee2e00 ffff881088ee2e00 ffff88102506e900
[1703175.519619] Call Trace:
[1703175.519627]  [<ffffffff8169d8b9>] schedule+0x29/0x70
[1703175.519631]  [<ffffffff81058f75>] exit_mm+0x85/0x130
[1703175.519633]  [<ffffffff81059191>] do_exit+0x171/0x480
[1703175.519637]  [<ffffffff81066bea>] ? __dequeue_signal+0x6a/0xb0
[1703175.519640]  [<ffffffff81059644>] do_group_exit+0x44/0xa0
[1703175.519643]  [<ffffffff810696bb>] get_signal_to_deliver+0x22b/0x440
[1703175.519647]  [<ffffffff810147f9>] do_signal+0x29/0x130
[1703175.519650]  [<ffffffff8105206e>] ? do_fork+0x14e/0x290
[1703175.519653]  [<ffffffff810afd0c>] ? do_futex+0x7c/0x1b0
[1703175.519656]  [<ffffffff810aff87>] ? sys_futex+0x147/0x1a0
[1703175.519658]  [<ffffffff810149b0>] do_notify_resume+0x90/0xd0
[1703175.519662]  [<ffffffff816a72e2>] int_signal+0x12/0x17
[1703175.519664] INFO: task java:1848 blocked for more than 120 seconds.
[1703175.520144] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[1703175.520768] java            D 0000000000000002     0  1848      1 0x00000000
[1703175.520770]  ffff8807aded5cd8 0000000000000086 0000000000000000 ffffffffffffffe0
[1703175.520773]  ffff8807aded5fd8 ffff8807aded5fd8 ffff8807aded5fd8 00000000000139c0
[1703175.520776]  ffff881089d62e00 ffff880a1f0d1700 ffff880a1f0d1700 ffff88102506e900
[1703175.520779] Call Trace:
[1703175.520782]  [<ffffffff8169d8b9>] schedule+0x29/0x70
[1703175.520784]  [<ffffffff81058f75>] exit_mm+0x85/0x130
[1703175.520786]  [<ffffffff81059191>] do_exit+0x171/0x480
[1703175.520788]  [<ffffffff81066bea>] ? __dequeue_signal+0x6a/0xb0
[1703175.520790]  [<ffffffff81059644>] do_group_exit+0x44/0xa0
[1703175.520793]  [<ffffffff810696bb>] get_signal_to_deliver+0x22b/0x440
[1703175.520795]  [<ffffffff810147f9>] do_signal+0x29/0x130
[1703175.520797]  [<ffffffff810afd0c>] ? do_futex+0x7c/0x1b0
[1703175.520799]  [<ffffffff810aff87>] ? sys_futex+0x147/0x1a0
[1703175.520802]  [<ffffffff810149b0>] do_notify_resume+0x90/0xd0
[1703175.520804]  [<ffffffff816a72e2>] int_signal+0x12/0x17
```

Looking at the JobManager log, it seems that the system took quite a while to notice the failed TaskManager:
```
08:37:08,817 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from ASSIGNED to READY for task Verify reducer (38/64)
08:37:08,817 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from READY to STARTING for task Verify reducer (38/64)
08:59:52,732 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from STARTING to FAILED for task Match on string key (57/64)
08:59:52,733 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Status of job Generic Comparator(037fefc0a7738c003b6834cb0b132800) changed to FAILING
```

I'll report here again if the error is reproducible.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/285
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, core, 
Created at: Mon Nov 25 10:43:22 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397654,,,Mon Jun 09 11:54:06 UTC 2014,,,,,,,,,,"0|i1wgn3:",397781,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:54;github-import;[Date: Mon Nov 25 19:33:58 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Any luck reproducing?

The only part where we could trigger a segmentation violation is in the use of UNSAFE for memory access. PactRecord uses that, but not in the constructor, so I would be curious where it came from.;;;","09/Jun/14 11:54;github-import;[Date: Mon Nov 25 19:36:25 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

No, I tried the exact same job afterwards, nothing happened.
But I'm going to execute the job a few times more, hopefully I see it again. But nevertheless, the error detection and handling should be improved! We need to do some experiments with killing TaskTrackers etc.
;;;","09/Jun/14 11:54;github-import;[Date: Fri Nov 29 01:22:00 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I experienced a similar JVM crash when it run the maven java build process, ie, totally unrelated code from another party

I think this is not a bug in our code, as producing segfaults in a JVM should only be possible when using JNI or unofficial extensions (like the unsafe class). Neither are used in the part of the code where the JVM crashed.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactored Csv Input Format to be configurable via instance parameters,FLINK-284,12719454,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:53,09/Jun/14 11:54,14/Jul/23 05:57,09/Jun/14 11:54,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/284
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Mon Nov 25 10:01:39 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:53;github-import;pull-request-284-2050732997080838256.patch;https://issues.apache.org/jira/secure/attachment/12649063/pull-request-284-2050732997080838256.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397653,,,Mon Jun 09 11:54:02 UTC 2014,,,,,,,,,,"0|i1wgmv:",397780,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:53;github-import;[Date: Mon Nov 25 10:06:11 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

This pull request refactors the CsvInputFormats to:
 -  Allow passing types through parameters, rather than the config
 -  You pass types instead of parsers
 -  It is generified such that the functionality is available to models other than pact record
 
 Please have a look and comment if you like the way it is used:
 
 https://github.com/stratosphere/stratosphere/pull/284/files#diff-c5813e5503590883623a27e0a1dc4b40R73 (MergeOnlyJoin.java, line 73);
 
 @aljoscha I had to adjust the Scala DataSource Macros. Can you double check that I this has no undesired side effects? It actually looks a lot nicer now ;-)
 
 @aljoscha The generation of the types filters by isUsed. If that prunes away fields, will the other fields not be in the wrong CSV column position? There is an alternate version (commented out) that replaces unused fields by null. It seems that one would do properly, as null type fields are skipped in the parser (keeping the other csv columns in place);;;","09/Jun/14 11:54;github-import;[Date: Mon Nov 25 10:17:13 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

I don't like that the field delimiter and the record delimiter parameters have differing types. The rest seems nicer though.;;;","09/Jun/14 11:54;github-import;[Date: Mon Nov 25 10:24:52 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Also, we should have a way to skip fields arbitrarily, i.e.
```scala
CsvInputFormat[(Int, String, Int)|Seq(1,17,42)]
```
to only read csv fields 1, 17, and 42.

In the java front end we need something similar for the constructor-style. Using the old style this was possible, right?;;;","09/Jun/14 11:54;github-import;[Date: Mon Nov 25 10:37:31 CET 2013, Author: [uce|https://github.com/uce]]

@aljoscha :+1:

I think (have to check) I've noticed a problem yesterday: in ""real-world"" CVS files you will usually find escaped commas, which don't act as delimiters, e.g. `field1,field2,""field,3"",field4`. We should provide a simple solution for this by looking for opening escape symbols like "" or ' and don't count the comma as a delimiter if the escape is still open.;;;","09/Jun/14 11:54;github-import;[Date: Mon Nov 25 10:38:16 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

And also, I think the second way, you mentioned in the Scala CsvInputFormat is correct, since otherwise fields would wrongly be shifted around.;;;","09/Jun/14 11:54;github-import;[Date: Mon Nov 25 11:45:06 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The parsers were written such that the field delimiter may only be a char.
If you pass a String with more than one char, it complains. So I changed
the type to prevent those situations. If we change the parsers to introduce
multi-char delimiters, we can change that...


On Mon, Nov 25, 2013 at 10:17 AM, Aljoscha Krettek <notifications@github.com
> wrote:

> I don't like that the field delimiter and the record delimiter parameters
> have differing types. The rest seems nicer though.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/284#issuecomment-29186804>
> .
>;;;","09/Jun/14 11:54;github-import;[Date: Mon Nov 25 11:48:11 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

We can skip fields currently by passing nulls, e.g. (PactInteger, null,
PactString, null, null, PactDouble).

The old config style still works.

@ufuk; Strings in quotes work in order to skip delimiters. If you still
have cases where it did not work, please check the code in the var-length
string parser and adjust it.


On Mon, Nov 25, 2013 at 11:45 AM, Stephan Ewen <ewenstephan@gmail.com>wrote:

> The parsers were written such that the field delimiter may only be a char.
> If you pass a String with more than one char, it complains. So I changed
> the type to prevent those situations. If we change the parsers to introduce
> multi-char delimiters, we can change that...
>
>
> On Mon, Nov 25, 2013 at 10:17 AM, Aljoscha Krettek <
> notifications@github.com> wrote:
>
>> I don't like that the field delimiter and the record delimiter parameters
>> have differing types. The rest seems nicer though.
>>
>> —
>> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/284#issuecomment-29186804>
>> .
>>
>
>;;;","09/Jun/14 11:54;github-import;[Date: Mon Nov 25 19:17:46 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [9dd4635ad69bf107ab9f81c8e7ffd495475471e0|https://github.com/stratosphere/stratosphere/commit/9dd4635ad69bf107ab9f81c8e7ffd495475471e0];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extend website front page,FLINK-283,12719453,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:53,09/Jun/14 11:53,14/Jul/23 05:57,09/Jun/14 11:53,,,,pre-apache,,,,,,,0,github-import,,"Hi,

This is the preview of the website changes I suggest here: http://robertmetzger.de/stratosphere/


Please do not merge this pull request! I think we need to rework some texts. 

I'm also thinking about adding some more pictures  and linking to some website subsections (quickstart)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/283
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Fri Nov 22 23:25:14 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:53;github-import;pull-request-283-3112579704002390491.patch;https://issues.apache.org/jira/secure/attachment/12649062/pull-request-283-3112579704002390491.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397652,,,Mon Jun 09 11:53:49 UTC 2014,,,,,,,,,,"0|i1wgmn:",397779,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:53;github-import;[Date: Sat Nov 23 03:15:47 CET 2013, Author: [uce|https://github.com/uce]]

Perfect!

PS: I also think that we have to change some texts (and maybe adapt some graphics from the Big Data Beers slides)

PPS: Lets incorparate the changes by Sunday;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 23 11:49:28 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Thanks. (But honestly: You created all the nice pictures, so most of the credits are for you)

I added a new commit (the preview also changed)
I also added more graphics from the slides (they are only visible if the user explicitly opens them ;) ).
Remember that I won't be available tomorrow (=sunday ;) )

We certainly need to rework some texts (but we should keep them as simple as possible).

Once we have new performance results, we need to add a performance section on the front page.;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 23 13:06:23 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I like the Features section a lot. All important aspects are covered. Nice work!

I also have a few comments:
- The layout could be improved in my opinion. The two column layout made me first read the first column and then the second. However, the content is separated by rows not by columns. But maybe that's just me...
- The font size should be unified (Features in Optimizer section).
- I would skip the sentence ""Its functional nature allows to parallelize the execution"". In the context of the Scala Pact interface,  the parallelization comes from the programming model not Scala's functional nature (Java is parallelized as well without being functional...).
- In the optimizer section, I would refer to relational query optimizers. Also highlight, that the optimizer relieves the programmer from thinking about the efficient execution of the data flow. The features should cover: 
  - Cost-based choice of operators and shipping strategies. 
  - Reduction of shipped and written data volume. 
  - Pipelining of operations. 
- Instead of ""Known from Hadoop"" and ""New in Stratosphere"", I would simply make it ""Hadoop"" and ""Stratosphere"" and give all operators (incl. Map and Reduce) in the ""More Operators"" section. That would make the difference more clear.;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 23 15:45:56 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for your feedback! I Incorporated all your suggestions (please tell me if you're still unhappy with the Optimizer section!);;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 23 16:14:05 CET 2013, Author: [uce|https://github.com/uce]]

- **More Operators**: I would vote for only presenting our operators with the graphics and keep the comparison text-only. In the text I would say *the known MapReduce programming model* instead of paradigm.

- **Advanced Data Flow Graphs**: ""directed acyclic graphs (DAGs)"" is a direct contradiction to the figure, which includes an iteration.
    - Capitalization of ""Complex data flows in Hadoop""
    - The more detailed explanation says ""stored to HDFS after each job"". Isn't it stored *in*?
    - Also ""around 30 seconds just for setup tasks"". Shouldn't it be ""just for setup""?

- **Powerful programming interfaces**: Capitalization and I would skip *Scalable Language* and remove the link to scala-lang.org.

- **Support for Iterative Algorithms**: ""distributed database"" is confusing. Let's say ""inside the system""?
    - Iterations with Hadoop: the only in ""Data is only"" is wrong placed?

- **Build-In Optimizer**: ""For example the ""Join"" operator has two strategies available: Sort-merge-join and hybrid hash join."" is suddenly in-depth compared to all the Hadoop stuff. Let's just say that we choose join strategies?

- **Open Source Community and Support**: The link ""Open an Issue on GitHub"" should link to the issues page.

- No new line before ""We also have binaries that are ready to run on your server.""

Then I would add some emphasis to the text by making keywords bold.

Other than that: P E R F E C T! :+1:

PS: The detailed explanations are a really good idea. :8ball: ;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 23 16:54:03 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Operators: what do you say about this approach?
Advanced Data Flow: What is the ""official"" answer to that contradiction? I also noticed it. We can state that an an iteration is just another iterator, that internally has cycles? Or we say ""DAG (except for the explicit definition of iterations"" ?
Scala. Removed Scalable Language. Why removing the link? I made the link open on a new site, so that people hopefully find our tab still open after learning Scala ;)
Optimizer: the example is specific but that's why I called it an example. I just wanted to have the two join strategies somewhere on the front page. If you have a better idea: go for it.

I made some keywords bold. I like it for the lower part, in the upper part, I did not find highlight-able keywords. 

Do you think I should add the JobManager/TaskManager image as an additional (toggleable) explanation to the stack?
I asked @aalexandrov for performance results.;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 23 17:04:59 CET 2013, Author: [uce|https://github.com/uce]]

Operators: Really nice!

Data Flow: I think it would be best to just use *advanced data flow graphs* as a replacements for DAGs.

Scala: Because of the sending people away issue, which might be OK with the new window. For the future: it will be nice to send them to a Scala guide on our page.

Optmizer: OK.

JM/TM: Yes, sure.

Emphasis: OK, I see. Maybe keyword is too specific. We could also try whole (first) sentences. Bold headlines would also be nice I think.

GitHub issue link: let's just link to the issues list and not directly to the ""new issue"" form. So people will see the current issues and open one if they want to.;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 23 17:40:09 CET 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

I want to suggest some modifications directly as a push request. @rmetzger: Is the latest version pushed to your repository?;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 23 21:02:29 CET 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

I'm pushing my changes right now, should be visible in 10-15 minutes. I did some minor improvements on the presentation and the HTML and went through the text up until System Stack.

My only remaining concern is that towards the end the descriptions become a bit heavy. I'll try to see if something can be shortened tomorrow. 

From the presentation perspective, I suggest to 

1. use a pop-up for the detailed explanations and 
2. re-format the feature descriptions either as either
  * a rolling slider with fixed structure (title + 3 line explanation + image) or 
  * text flow with h3 titles, paragraph-ed text and ""float: left"" pictures (similar to what we have now).

Other then that everything looks and reads nice, good job Robert.;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 23 21:08:59 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Hey! Can we have a look together over the page on Monday before we push
anything?
Am 23.11.2013 21:02 schrieb ""Alexander Alexandrov"" <notifications@github.com
>:

> I'm pushing my changes right now, should be visible in 10-15 minutes. I
> did some minor improvements on the presentation and the HTML and went
> through the text up until System Stack.
>
> My only remaining concern is that towards the end the descriptions become
> a bit heavy. I'll try to see if something can be shortened tomorrow.
>
> From the presentation perspective, I suggest to
>
>    1. use a pop-up for the detailed explanations and
>    2. re-format the feature descriptions either as either
>       - a rolling slider with fixed structure (title + 3 line explanation
>       + image) or
>       - text flow with h3 titles, paragraph-ed text and ""float: left""
>       pictures (similar to what we have now).
>
> Other then that everything looks and reads nice, good job Robert.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/283#issuecomment-29140375>
> .
>;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 23 21:11:01 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Do you see major points?
I would like this to be pushed asap ;)
(Alex did not push to our website, he pushed to
http://aalexandrov.github.io/stratosphere/ )


On Sat, Nov 23, 2013 at 9:09 PM, Stephan Ewen <notifications@github.com>wrote:

> Hey! Can we have a look together over the page on Monday before we push
> anything?
> Am 23.11.2013 21:02 schrieb ""Alexander Alexandrov"" <
> notifications@github.com
> >:
>
> > I'm pushing my changes right now, should be visible in 10-15 minutes. I
> > did some minor improvements on the presentation and the HTML and went
> > through the text up until System Stack.
> >
> > My only remaining concern is that towards the end the descriptions
> become
> > a bit heavy. I'll try to see if something can be shortened tomorrow.
> >
> > From the presentation perspective, I suggest to
> >
> > 1. use a pop-up for the detailed explanations and
> > 2. re-format the feature descriptions either as either
> > - a rolling slider with fixed structure (title + 3 line explanation
> > + image) or
> > - text flow with h3 titles, paragraph-ed text and ""float: left""
> > pictures (similar to what we have now).
> >
> > Other then that everything looks and reads nice, good job Robert.
> >
> > —
> > Reply to this email directly or view it on GitHub<
> https://github.com/stratosphere/stratosphere/pull/283#issuecomment-29140375>
>
> > .
> >
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/283#issuecomment-29140500>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 23 21:22:44 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, your version is online.
Here are my points: (sorry for the long list)

I'm also a bit unhappy with the current texts of the 6 overview features .. but there are some politics involved with that. 

 * Beyond MR: People don't know what higher-order operators are. I want to keep it simple. (Especially in the first sentence)
 *  Efficient Complex Data Flows: We don't want to be complex, we want to be advanced! (Note that I always write advanced when it comes to Stratosphere and complex when Hadoop ). I removed ""arbitrary directed acyclic graphs"" because the respective image is a cyclic graph! (Concerning arbitrary: I was told that ""arbitrary"" is not a nice word)
 * I really liked the horizontal lines and the vertical spaces (including the headline (even though we do not only list features) ) (See also Fabian's concerns)

I like the idea of directly linking the hadoop solution in the text.
You are right about the text heaviness in the end. We need to change that.

What do you mean by ""text flow with h3 titles, paragraph-ed text and ""float: left"" pictures (similar to what we have now)."" ? 
I don't want to have a compact text on the website. People should not be scared by long texts. (That's why I used these blocks, switched the sides and the large spacings.)

I'm against using a slider because users have to click (and to understand the interface). I'm also against a pop up because it won't work on mobiles? Can google read popup's contents?. My approach only hides the advanced explanation if Javascript is enabled. So google's spider and users without JS will always see everything.

We work really hard to get people on the website, so we must do everything possible that they stay there and understand it!


;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 23 22:26:45 CET 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

""Can google read popup's contents?"" Depends whether they are loaded from a hidden area of the HTML code or via AJAX.

""My approach only hides the advanced explanation if Javascript is enabled."" Mine will do the same, its just a question of presentation to agents with JS support. 

Overall, do we really want to have all the detailed descriptions on the homepage? I would prefer a dedicated page with corresponding subsections explaining why Hadoop is bad and Stratosphere better with respect to dataflow graphs, iterations, etc., and brief feature descriptions linking to the corresponding technical subsection with an anchor.

Last, regarding sliding features vs text flow. The problem with a linear presentation is that it implicitly makes features placed higher more visible. A [carousel|http://getbootstrap.com/2.3.2/javascript.html#carousel) which changes the contents every 30 seconds time-share the visible space in the bottom half of page between all features (but can still degrade gracefully for non JS-users and phones].

Either way, we should try to have a predefined ""template"" with dedicated area for a picture, a title, and text, and have feature description text with similar weight across all features.;;;","09/Jun/14 11:53;github-import;[Date: Sun Nov 24 12:23:36 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, lets meet on Monday to discuss the new front page.;;;","09/Jun/14 11:53;github-import;[Date: Mon Nov 25 17:01:05 CET 2013, Author: [ktzoumas|https://github.com/ktzoumas]]

Some changes in the text:

Very few use cases and real world application fit that model. --> There are several applications that do not naturally fit in this programming model.

Stratosphere is the only runtime that natively supports iterative algorithms inside the system. --> Stratosphere natively supports iterative algorithms in the runtime and optimizer.

Have the 6 grid elements be clickable and point to the corresponding explanations. 

As far as I am concerned, you can merge this, and then we can make a separate pull request for text changes. ;;;","09/Jun/14 11:53;github-import;[Date: Mon Nov 25 21:27:45 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I added your changes.

Some of the 6 features now link to different sections below or somewhere else (I updated the demo page: http://robertmetzger.de/stratosphere/);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GraphConversionException: Job and execution vertex * have different number of inputs,FLINK-282,12719452,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:53,09/Jun/14 11:53,14/Jul/23 05:57,09/Jun/14 11:53,,,,pre-apache,,,,,,,0,github-import,,"This is a job I want to execute: https://github.com/rmetzger/scratch/blob/eda1e1432f104d81e6ae3fcb7657a24b163e5d75/src/main/java/eu/stratosphere/FlexCmp.java

Either the error message is not helpful or there is a bug with the pact-compiler?

```
13/11/22 19:49:43 ERROR client.JobClient: ERROR: eu.stratosphere.nephele.executiongraph.GraphConversionException: Job and execution vertex Write stats have different number of inputs
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createInitialGroupEdges(ExecutionGraph.java:409)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:282)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:176)
	at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:534)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:417)
	at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:946)
```

The error only occurs if the FileDataSink statsOut has more than one input files.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/282
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, duplicate, optimizer, runtime, 
Created at: Fri Nov 22 20:05:52 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397651,,,Mon Jun 09 11:53:19 UTC 2014,,,,,,,,,,"0|i1wgmf:",397778,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:53;github-import;[Date: Fri Nov 22 20:35:56 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Ah, okay, the issue has already been reported here: https://github.com/stratosphere/stratosphere/issues/112;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in MutableHashTable on cancelling Task,FLINK-281,12719451,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:53,09/Jun/14 11:53,14/Jul/23 05:57,09/Jun/14 11:53,,,,pre-apache,,,,,,,0,github-import,,"Today, I got this NPE.
The job has a Match and a Reduce. The Match failed due to a (usercode) runtime exception. During the cancellation of the whole job, the following NPE occurred:

```
14:19:08,130 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from CANCELING to CANCELED for task Match on string key (6/64)
14:19:08,134 ERROR eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask       - java.lang.NullPointerException
        at eu.stratosphere.nephele.services.memorymanager.spi.DefaultMemoryManager.release(DefaultMemoryManager.java:337)
        at eu.stratosphere.pact.runtime.hash.BuildFirstHashMatchIterator.abort(BuildFirstHashMatchIterator.java:171)
        at eu.stratosphere.pact.runtime.task.MatchDriver.cancel(MatchDriver.java:185)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.cancel(RegularPactTask.java:416)
        at eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask.cancelOrKillExecution(RuntimeTask.java:227)
        at eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask.cancelExecution(RuntimeTask.java:186)
        at eu.stratosphere.nephele.taskmanager.TaskManager$1.run(TaskManager.java:399)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:679)

14:19:08,136 WARN  eu.stratosphere.pact.runtime.task.RegularPactTask             - PACT code cancelled.: Match on string key (13/64)
```

The `availableMemory` of the HashMap seem to contain `null` memory segments.

(See also https://github.com/stratosphere/stratosphere/issues/154#issuecomment-29071859)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/281
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, core, 
Created at: Fri Nov 22 14:48:12 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397650,,,Mon Jun 09 11:53:16 UTC 2014,,,,,,,,,,"0|i1wgm7:",397777,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:53;github-import;[Date: Fri Dec 06 23:46:37 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The problem was that the cancel call on the hash join started releasing memory. The regular cleanup code also releases the memory. Segments cannot be double released, but obtaining the reference to them may fail in race conditions.

Logic is not that canceling only breaks the operating loops and memory release happens only in the operator cleanup parts.

[7b0c53b4c501690866523468f6c3e7f06438658c|https://github.com/stratosphere/stratosphere/commit/7b0c53b4c501690866523468f6c3e7f06438658c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added integration test case for AllReducer and iterations,FLINK-280,12719450,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:53,09/Jun/14 11:53,14/Jul/23 05:57,09/Jun/14 11:53,,,,pre-apache,,,,,,,0,github-import,,"This is the test case for issue [107|https://github.com/stratosphere/stratosphere/pull/107)
It is a simple scenario with iterations, dop>1 and with an keyless reducer. Should I check for anything else?

This test case will fail until (DOP < Iteration DOP] is allowed.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/280
Created by: [andrehacker|https://github.com/andrehacker]
Labels: 
Created at: Thu Nov 21 15:14:36 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:53;github-import;pull-request-280-1506728499471781011.patch;https://issues.apache.org/jira/secure/attachment/12649061/pull-request-280-1506728499471781011.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397649,,,Mon Jun 09 11:53:12 UTC 2014,,,,,,,,,,"0|i1wglz:",397776,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:53;github-import;[Date: Thu Dec 12 19:59:46 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [a4b1dbdb17c1df3348db362cafd185c326652158|https://github.com/stratosphere/stratosphere/commit/a4b1dbdb17c1df3348db362cafd185c326652158];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Implemented JDBCInputFormat for MariaDB, MySQL, Postgres and Derby. Code is covered by unit-test",FLINK-279,12719449,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:52,09/Jun/14 11:53,14/Jul/23 05:57,09/Jun/14 11:53,,,,pre-apache,,,,,,,0,github-import,,"The first part (input-format) of issue ([#261|https://github.com/stratosphere/stratosphere/issues/261] | [FLINK-261|https://issues.apache.org/jira/browse/FLINK-261]) is implemented now and is also covered by a junit-test.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/279
Created by: [emrehasan|https://github.com/emrehasan]
Labels: 
Created at: Thu Nov 21 10:14:13 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:52;github-import;pull-request-279-4387754590591605071.patch;https://issues.apache.org/jira/secure/attachment/12649060/pull-request-279-4387754590591605071.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397648,,,Mon Jun 09 11:53:07 UTC 2014,,,,,,,,,,"0|i1wglr:",397775,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:52;github-import;[Date: Thu Nov 21 15:09:24 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, some more general feedback:
Can you make the inputFormat more generic?
It has the driver classes for some databases hardcoded. It would be nicer to allow the InputFormat to work with ALL JDBC drivers/databases.
I think the user has to supply the JDBC driver name as a string (remember the input format runs in a distributed fashion, so you need to make sure that the JDBC driver is loaded on the node that is running the input format. (So we need to test your InputFormat on a cluster) (We can do that here at the TU).

I'm not sure if it is neccessary to store all these information as class properties:
```
 +    private String dbTypeStr;
 +    private String host;
 +    private Integer port;
 +    private String dbName;
 +    private String username;
 +    private String password;
 +    private String derbyDBPath;
```
They should all be encoded in the connection string. I think you can still keep the constructor that gets a Configuration object, but it is probably better to construct the connection string there.


I'm not sure if you really need the `need_configure` variable. You should be able to determine if configuration is required based on the availability of a valid connection string (the variable name is not conform to Java variable naming!)

We don't want to have these comments
```
 /*
 +     * (non-Javadoc)
 +     * @see eu.stratosphere.pact.generic.io.InputFormat
 +     */
```
because they don't add any value (any smart IDE will point you to the right documentation).


I think it is okay to have only one example file. Write about the different variants in a comment!


Please ask me if you have any questions regarding my comments.
It is not my intention to discourage you ;) Your overall work is quite good!;;;","09/Jun/14 11:52;github-import;[Date: Thu Nov 21 16:29:05 CET 2013, Author: [zentol|https://github.com/zentol]]

""Can you make the inputFormat more generic?""

if we let the user supply the driver url it should work. we could make it even more generic if we always require the user to pass the database url aswell, since that is hardcoded to the db type right now aswell.
;;;","09/Jun/14 11:52;github-import;[Date: Thu Nov 21 18:55:16 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, I would suggest to do it this way. Hardcoding database names is not a good idea, since JDBC is designed to be database independent.

You can, if you want, add the code you've already written to a static method that allows to build the database url.

Like this
```
GenericDataSource source = new GenericDataSource(new JDBCInputFormat(JDBCInputFormat.getURL(""mysql"", ""username"", ""secret"", ""table""), ""SELECT blabl""), ""Data Source""); 
```;;;","09/Jun/14 11:52;github-import;[Date: Sat Nov 23 21:56:15 CET 2013, Author: [emrehasan|https://github.com/emrehasan]]

I think we removed comments with (non-Javadoc) prefix, but i assume my partner and me got this from your classes like you can see in GenericDataSource[85];;;","09/Jun/14 11:53;github-import;[Date: Sun Nov 24 11:53:27 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Thanks for removing the comments.
You are right, our code is full of those comments, but we want to get rid of them: https://github.com/stratosphere/stratosphere/issues/122;;;","09/Jun/14 11:53;github-import;[Date: Mon Nov 25 17:26:00 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Nice work!

I added a few comments, but otherwise it looks good to me!;;;","09/Jun/14 11:53;github-import;[Date: Tue Nov 26 08:39:09 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Hi guys,

once https://github.com/stratosphere/stratosphere/issues/289 is merged, can you create a new maven project in `stratosphere-addons` for the jdbc input (/output) format? Call it `stratosphere-jdbc`.

@StephanEwen: I guess this implementation of the  `JDBCInputFormat` will run with multiple input splits. Is there a way to set the DOP of the `GenericDataSource` from inside the InputFormat?
Or is it better to overwrite `createInputSplits()` ignoring the minSplits argument?
Or is a `SingleInstanceDataSource` or so required?;;;","09/Jun/14 11:53;github-import;[Date: Thu Nov 28 20:10:48 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Hey guys,

we now merged the pull request for ""stratosphere-addons"" (https://github.com/stratosphere/stratosphere/tree/master/stratosphere-addons). 
Could you create a new maven project under `stratosphere-addons` called `stratosphere-jdbc`.  Move the example and JDBC Input format into the new maven project.

;;;","09/Jun/14 11:53;github-import;[Date: Thu Nov 28 21:25:09 CET 2013, Author: [zentol|https://github.com/zentol]]

we will create the maven project before the end of this week.;;;","09/Jun/14 11:53;github-import;[Date: Fri Nov 29 01:25:19 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I assume that the data base input is a task that can not be parallelized, i.e., there is a single instance only of the data source that issues the query, and which then distributes the data among all parallel tasks of the next stage.

In order to tell the system to not parallelize an input task, you can mark the input format as unsplittable by implementing the marker interface https://github.com/stratosphere/stratosphere/blob/master/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/io/UnsplittableInput.java

If that is the case for your input format, please add the UnsplittableInput interface to the list of implemented interfaces.;;;","09/Jun/14 11:53;github-import;[Date: Fri Nov 29 06:25:09 CET 2013, Author: [ankurcha|https://github.com/ankurcha]]

Pardon my ignorance, but I think it should be possible to parallize data load on a jdbc input. The idea would be to define the correct shard key(s), which may be left for the user to specify(abstract method). 

That being said, by default we could make it a single partition or maybe based off the primary key (if any). But allowing the user to specify the column to use to split the dataset may be very powerful that can have real scalability and concurrency advantages.

The UnsplittableInput route is definitely more simple but this would also mean that if for some reason any part of the dataload fails, we would end up needing to reload the full dataset, whereas in a parallelizable case, only that partition /split would need to be retried.;;;","09/Jun/14 11:53;github-import;[Date: Fri Nov 29 10:35:57 CET 2013, Author: [fhueske|https://github.com/fhueske]]

You are right. That would work for parallel database systems, that can efficiently run queries on individual data partitions without having a central master involved. 
If however: 
- The database system is not running in parallel, the same machine is asked many queries from different clients. A single query that asks for all data is probably more efficient than multiple queries asking for subsets. (Seq. IO vs. random IO, query compilation, locking, etc. overhead).
- The database system is parallel but with a master coordinator, this master needs to serve all clients the results (even though it does not need to compute the result). So the master might become the bottleneck here.
Please correct me if you think that's wrong ;-)

I think, this really depends on the database setup and the feature of the DBMS. Furthermore, the programmer needs to know the setup quite well (partition keys and ranges).
But I agree, for parallel DBMS, it would be good to have the option to shard queries over data partitions.;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 30 00:36:25 CET 2013, Author: [zentol|https://github.com/zentol]]

Would it be possible to let the user supply multiple queries, and use each of them in a separate split?

Given a user who knows about shard keys, i feel like it would be really inconvenient for him to pass the query and keys separately, und hope that the correct query is generated.;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 30 02:06:23 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Regarding your question: Yes. You could overwrite the getInputSplits() method (that would for example return two splits, one for each query).
I think we should first merge the current JDBC input format into the main line. Its nicely written and covers most use-cases.

If one wants to do more advanced stuff, there is also the way to create multiple DataSources with JDBCInputFormats. Like this.
```
+---------------+
| MySQL:Shard1  |
+---------------+
				  \
				   (Union (Stratosphere))--(Further processing)--Data Sink
				  /
+---------------+
| MySQL:Shard2  |
+---------------+
```
;;;","09/Jun/14 11:53;github-import;[Date: Sat Nov 30 02:59:24 CET 2013, Author: [ankurcha|https://github.com/ankurcha]]

That seems for to me. 

> On Nov 29, 2013, at 17:06, Robert Metzger <notifications@github.com> wrote:
> 
> Regarding your question: Yes. You could overwrite the getInputSplits() method (that would for example return two splits, one for each query).
> I think we should first merge the current JDBC input format into the main line. Its nicely written and covers most use-cases.
> 
> If one wants to do more advanced stuff, there is also the way to create multiple DataSources with JDBCInputFormats. Like this.
> 
> +---------------+
> | MySQL:Shard1  |
> +---------------+
>                   \
>                    (Union (Stratosphere))--(Further processing)--Data Sink
>                   /
> +---------------+
> | MySQL:Shard2  |
> +---------------+
> —
> Reply to this email directly or view it on GitHub.;;;","09/Jun/14 11:53;github-import;[Date: Sun Dec 01 16:32:42 CET 2013, Author: [zentol|https://github.com/zentol]]

We ran into a problem when using the UnsplittableInput interface: the example stops working.
""Cannot compute input splits for Data Source: java.lang.NegativeArraySizeException"" was the error message we got.

I was able to resolve it by overriding the createInputSplits method, with one that returns a list of a single GenericInputSplit().;;;","09/Jun/14 11:53;github-import;[Date: Sun Dec 01 18:28:09 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Hi @zentol;
I'm quite sure that this is a bug inside the system, so you don't have to do any workarounds. @StephanEwen can probably fix it within seconds ;)
I'll ask him tomorrow to fix it.;;;","09/Jun/14 11:53;github-import;[Date: Wed Dec 04 18:18:05 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged (because of conflicts and renaming) in [4d73ef004199fb933bf4669da63e4254ad115c3f|https://github.com/stratosphere/stratosphere/commit/4d73ef004199fb933bf4669da63e4254ad115c3f]

Thanks guys, that was good work!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Travis Build Status from Start Page,FLINK-278,12719448,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:52,09/Jun/14 11:52,14/Jul/23 05:57,09/Jun/14 11:52,,,,pre-apache,,,,,,,0,github-import,,"Until Travis manages to reliably run our builds (without more than half of the build timing out), we should remove the build status from the front page. Makes us look bad when it says ""build error"" all the time.

Today, I restarted an individual build four times without getting having a run where it was not canceled after 50 minutes.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/278
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Tue Nov 19 18:31:54 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397647,,,Mon Jun 09 11:52:37 UTC 2014,,,,,,,,,,"0|i1wglj:",397774,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:52;github-import;[Date: Tue Nov 19 18:32:56 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Of course, that does not change the policy that we use Travis and that we only accept commits that pass all tests! That goes without saying ;-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify Program Template and Readme Stub,FLINK-277,12719447,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:52,09/Jun/14 11:52,14/Jul/23 05:57,09/Jun/14 11:52,,,,pre-apache,,,,,,,0,github-import,,"I think we can simplify the Program Template (which is also shown in the Readme on the github start page) a bit. Would make it easier to parse for beginners.

 -  We don't need the PlanAssemblerDescriptor. It is barely ever read (I know only of explicit calls in the web frontend and the command line frontend)
 - Why have the local executor functionality split between main() and execute()? Make it one and use the static call that starts and stops the executor automatically.

The result would be much more consice and quite a bit nicer in my opinion:

```java
public class Tutorial implements PlanAssembler {

    @Override
    public Plan getPlan(String... args) {
        // your parallel program goes here
    }

    public static void main(String[] args) throws Exception {
        Tutorial tut = new Tutorial();
        Plan toExecute = tut.getPlan(args);
        long runtime = LocalExecutor.execute(toExecute);
        System.out.println(""Runime: "" + runtime);
    }
}

```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/277
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Tue Nov 19 18:28:47 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397646,,,Mon Jun 09 11:52:33 UTC 2014,,,,,,,,,,"0|i1wglb:",397773,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:52;github-import;[Date: Wed Nov 20 02:06:23 CET 2013, Author: [uce|https://github.com/uce]]

:+1:;;;","09/Jun/14 11:52;github-import;[Date: Mon Nov 25 19:34:21 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Done;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in webinterface: Failed jobs don't disappear,FLINK-276,12719446,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:52,09/Jun/14 11:52,14/Jul/23 05:57,09/Jun/14 11:52,,,,pre-apache,,,,,,,0,github-import,,"I was running a job that failed. It seems that the nephele does not properly cleans all internal data structures if a job fails:

![failed|https://f.cloud.github.com/assets/89049/1564975/753d1f4a-5077-11e3-91d6-c58caac4b986.png]


This status does not change even if I successfully submit the next job: I can not see the progress of this job, because the ""dead"" job still remains there.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/276
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, runtime, 
Created at: Mon Nov 18 18:34:42 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397645,,,Mon Jun 09 11:52:29 UTC 2014,,,,,,,,,,"0|i1wgl3:",397772,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:52;github-import;[Date: Mon Nov 18 19:51:01 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Just to get a bit more info on this issue:

- Can you consistently reproduce this bug with your job? 
- Do you know if that happens with any failing job?;;;","09/Jun/14 11:52;github-import;[Date: Tue Jan 07 10:33:46 CET 2014, Author: [markus-h|https://github.com/markus-h]]

I did some investigation on this issue and the solution was very easy. Because of a small typo in the EventCollector failed jobs were not correctly removed from the recentJobs list.;;;","09/Jun/14 11:52;github-import;[Date: Fri Feb 07 10:28:11 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Can be closed, right?;;;","09/Jun/14 11:52;github-import;[Date: Fri Feb 07 11:48:48 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think so.
Am 07.02.2014 10:28 schrieb ""Fabian Hueske"" <notifications@github.com>:

> Can be closed, right?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/276#issuecomment-34420693>
> .
>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkIteration member function is not implemented,FLINK-275,12719445,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:52,09/Jun/14 11:52,14/Jul/23 05:57,09/Jun/14 11:52,,,,pre-apache,,,,,,,0,github-import,,"The implementation of the ""public void setTerminationCriterion(Contract criterion)"" member function in BulkIteration class is missing.

<code>
public void setTerminationCriterion(Contract criterion) {
                throw new UnsupportedOperationException(""Termination criterion support is currently not implemented."");
        }
</code>

https://github.com/stratosphere/stratosphere/blob/master/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/contract/BulkIteration.java

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/275
Created by: [lukacsg|https://github.com/lukacsg]
Labels: 
Assignee: [markus-h|https://github.com/markus-h]
Created at: Mon Nov 18 15:35:36 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397644,,,Mon Jun 09 11:52:24 UTC 2014,,,,,,,,,,"0|i1wgkv:",397771,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:52;github-import;[Date: Mon Nov 18 15:46:01 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

It is on the list for the coming release ;-) Until then, you can use
 -  An absolute number of iterations, OR
 -  The iteration aggregators to define a convergence criterion.

For the second option, have a look at the PageRank Example on how to use it:
 -  An example aggregator (that collects many statistics about evolution of the solution): https://github.com/stratosphere/stratosphere/blob/master/pact/pact-examples/src/main/java/eu/stratosphere/pact/example/pagerank/PageRankStatsAggregator.java
 -  An example convergence criterion: https://github.com/stratosphere/stratosphere/blob/master/pact/pact-examples/src/main/java/eu/stratosphere/pact/example/pagerank/DiffL1NormConvergenceCriterion.java
 -  The creation of such an aggregator can be seen in the page rank plan construction (line 78) https://github.com/stratosphere/stratosphere/blob/master/pact/pact-examples/src/main/java/eu/stratosphere/pact/example/pagerank/DanglingPageRank.java#L78
 -  The usage of the aggregator can be seen in the CoGroup that computes the new rank (lines 64 and lines 97-100)
https://github.com/stratosphere/stratosphere/blob/master/pact/pact-examples/src/main/java/eu/stratosphere/pact/example/pagerank/DotProductCoGroup.java#L64

;;;","09/Jun/14 11:52;github-import;[Date: Mon Nov 18 16:28:01 CET 2013, Author: [lukacsg|https://github.com/lukacsg]]

Thanks! We will wait for the next release, while we can't define a convergence criterion or give an absolute number of iterations this time implementing our entity resolution algorithms. Should we leave the issue open?;;;","09/Jun/14 11:52;github-import;[Date: Mon Nov 18 16:48:20 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Lets keep the issue.

I assume using the aggregators for convergence checking is not applicable to your algorithm?;;;","09/Jun/14 11:52;github-import;[Date: Mon Nov 18 17:13:37 CET 2013, Author: [sidlo|https://github.com/sidlo]]

We will see; at first we thought that our loop invariant cannot be implemented as a convergence criterion, but checking it again, it might work!

(We first thought that as a ConvergenceCriterion we must provide a measure that decreases in every step, and in our case, there is no such a measure. However, we have a simple condition of termination.);;;","09/Jun/14 11:52;github-import;[Date: Mon Feb 24 14:18:21 CET 2014, Author: [markus-h|https://github.com/markus-h]]

The method is implemented in https://github.com/stratosphere/stratosphere/pull/472.;;;","09/Jun/14 11:52;github-import;[Date: Mon Feb 24 14:28:09 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I'll close this issue once the PR is merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add new way of specifying scala jobs: query {},FLINK-274,12719444,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:52,09/Jun/14 11:52,14/Jul/23 05:57,09/Jun/14 11:52,,,,pre-apache,,,,,,,0,github-import,,"This is just to let you know what I'm working on. Do not merge yet.

I want to get rid of ScalaPlan and let people specify queries like this:
```scala
def getScalaPlan(textInput: String, wordsOutput: String) = {
  query {
    val input = TextFile(textInput)

    val words = input flatMap { _.toLowerCase().split("" "") }
    val counts = words groupBy { case (word, _) => word } count()

    val output = counts.write(wordsOutput, CsvOutputFormat)

    Seq(output)
  }
}
```

I want to introduce this now before the new release so that people get to know it. In the future this will allow looking at the AST of the whole query, not just at single anonymous functions. Also, we could have something like this instead of the code mentioned above:
```scala

val ex = ExecutionContext(""local"") // ExecutionContext(""localhost:666"", <jars>)
ex.query {
  val input = TextFile(textInput)
  val words = input flatMap { _.toLowerCase().split("" "") }
  val counts = words groupBy { case (word, _) => word } count()
  val output = counts.write(wordsOutput, CsvOutputFormat)
  Seq(output)
}
```

Where the query is immediately executed on the executor.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/274
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri Nov 15 18:01:46 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:52;github-import;pull-request-274-2507114054717144407.patch;https://issues.apache.org/jira/secure/attachment/12649059/pull-request-274-2507114054717144407.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397643,,,Mon Jun 09 11:52:17 UTC 2014,,,,,,,,,,"0|i1wgkn:",397770,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:52;github-import;[Date: Sat Nov 16 16:24:25 CET 2013, Author: [uce|https://github.com/uce]]

Big :+1: on the single `ExecutionContext`.

I think we should support both things you proposed.

---

Why do you have the `query { ... }` block in the `getScalaPlan`? Can't you just look at the AST of the function or do you need the block for this?;;;","09/Jun/14 11:52;github-import;[Date: Sat Nov 16 17:10:12 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

I cannot just look at any ASTs. I need some kind of hook, so in some future version query could become a macro which would then allow me to look at the ast of the passed-in function.;;;","09/Jun/14 11:52;github-import;[Date: Wed Nov 20 02:32:28 CET 2013, Author: [uce|https://github.com/uce]]

I would also be OK with your proposals w/o the ability to analyze the complete query (i.e. skipping the query block in proposal 1). What do we expect to gain from analyzing the AST in Scala at this level of ""query"" granularty?;;;","09/Jun/14 11:52;github-import;[Date: Wed Feb 05 18:12:04 CET 2014, Author: [aalexandrov|https://github.com/aalexandrov]]

I'm drooling...;;;","09/Jun/14 11:52;github-import;[Date: Sun Mar 23 13:08:52 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

This is not a real pull request, so I would like to close this for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
throw exception instead of return max dop of 1 on IOException (from file system/hdfs),FLINK-273,12719443,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:52,09/Jun/14 11:52,14/Jul/23 05:57,09/Jun/14 11:52,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/273
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue Nov 12 19:38:10 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:52;github-import;pull-request-273-4708606161336262778.patch;https://issues.apache.org/jira/secure/attachment/12649058/pull-request-273-4708606161336262778.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397642,,,Mon Jun 09 11:52:10 UTC 2014,,,,,,,,,,"0|i1wgkf:",397769,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:52;github-import;[Date: Wed Nov 13 10:49:06 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

merged in https://github.com/stratosphere/stratosphere/commit/3fff141844347ebea413d4223f4e42d4d46924a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix workset iterations in scala frontend,FLINK-272,12719442,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:52,09/Jun/14 11:52,14/Jul/23 05:57,09/Jun/14 11:52,,,,pre-apache,,,,,,,0,github-import,,"We used the globalPos to create WorksetIteration, these are not
available before the post pass has run, therefore use the localPos for
now.

Also add maxIterations parameter to iterateWithWorkset

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/272
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Tue Nov 12 18:04:02 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:52;github-import;pull-request-272-8891445730590043455.patch;https://issues.apache.org/jira/secure/attachment/12649057/pull-request-272-8891445730590043455.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397641,,,Mon Jun 09 11:52:05 UTC 2014,,,,,,,,,,"0|i1wgk7:",397768,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:52;github-import;[Date: Thu Nov 21 16:22:39 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Do not merge until I give the go ahead.;;;","09/Jun/14 11:52;github-import;[Date: Fri Nov 22 17:08:53 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Alright, I think this badboy is ready for merging.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Name escaping for jobmanager webfrontend,FLINK-271,12719441,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:51,09/Jun/14 11:51,14/Jul/23 05:57,09/Jun/14 11:51,,,,pre-apache,,,,,,,0,github-import,,"HTML escaping for names in the webinterface of the jobmanager. Workaround for issue ([#171|https://github.com/stratosphere/stratosphere/issues/171] | [FLINK-171|https://issues.apache.org/jira/browse/FLINK-171]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/271
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Tue Nov 12 15:32:43 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:51;github-import;pull-request-271-5283614893629295255.patch;https://issues.apache.org/jira/secure/attachment/12649056/pull-request-271-5283614893629295255.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397640,,,2014-06-09 11:51:56.0,,,,,,,,,,"0|i1wgjz:",397767,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename Record*Format to Csv*Format in scala frontend,FLINK-270,12719440,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:51,09/Jun/14 11:51,14/Jul/23 05:57,09/Jun/14 11:51,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/270
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Tue Nov 12 09:06:21 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:51;github-import;pull-request-270-4476165853979920687.patch;https://issues.apache.org/jira/secure/attachment/12649055/pull-request-270-4476165853979920687.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397639,,,Mon Jun 09 11:51:53 UTC 2014,,,,,,,,,,"0|i1wgjr:",397766,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:51;github-import;[Date: Tue Nov 12 13:39:19 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [6cdc38b58a4ef41851487c25a1cd4b31a6188475|https://github.com/stratosphere/stratosphere/commit/6cdc38b58a4ef41851487c25a1cd4b31a6188475];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added count() function on grouped data,FLINK-269,12719439,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:51,09/Jun/14 11:51,14/Jul/23 05:57,09/Jun/14 11:51,,,,pre-apache,,,,,,,0,github-import,,"Added a function count() to count elements in a group and return a two tuple with original group value and count.

Allows to write code like:
```
val words = input flatMap { _.toLowerCase().split(""""""\W+"""""") filter { _ != """" } }
val counts = words groupBy { x => x } count()
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/269
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Tue Nov 12 00:07:16 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:51;github-import;pull-request-269-4157016039905255594.patch;https://issues.apache.org/jira/secure/attachment/12649054/pull-request-269-4157016039905255594.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397638,,,Mon Jun 09 11:51:49 UTC 2014,,,,,,,,,,"0|i1wgjj:",397765,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:51;github-import;[Date: Tue Nov 12 08:28:00 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Looks good to me but could you please also add an IT Test for the example.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor changes in RemoteExecutor and LocalExecutor. ,FLINK-268,12719438,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:51,09/Jun/14 11:51,14/Jul/23 05:57,09/Jun/14 11:51,,,,pre-apache,,,,,,,0,github-import,,"* Removed obsolete null-value check in the finally clauses in LocalExecutor
* Returning proper exit code in RemoteExecutor#executePlan()

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/268
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Created at: Mon Nov 11 18:30:54 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:51;github-import;pull-request-268-7220846520212985640.patch;https://issues.apache.org/jira/secure/attachment/12649053/pull-request-268-7220846520212985640.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397637,,,Mon Jun 09 11:51:45 UTC 2014,,,,,,,,,,"0|i1wgjb:",397764,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:51;github-import;[Date: Tue Nov 12 13:39:34 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [91f7d1604f5c76edb1e05ac4b8eba19cf9d5b702|https://github.com/stratosphere/stratosphere/commit/91f7d1604f5c76edb1e05ac4b8eba19cf9d5b702];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compile errors in the stratosphere-quickstart archetype,FLINK-267,12719437,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:51,09/Jun/14 11:51,14/Jul/23 05:57,09/Jun/14 11:51,,,,pre-apache,,,,,,,0,github-import,,"I get the following two errors on a freshly created scala quickstart archetype. 

First error occurs at line 2

```
import scala.Array.canBuildFrom
```

and says ""object Array is not a member of package eu.stratosphere.scala"".

Second error is at line 91

```
val words = input flatMap { _.toLowerCase().split(""""""\W+"""""") filter { _ != """" } map { (_, 1) } }
```

and says 

```
scala.reflect.macros.TypecheckException: object Array is not a member of package eu.stratosphere.scala
	at scala.reflect.macros.runtime.Typers$$anonfun$typeCheck$2.apply(Typers.scala:31)
	at scala.reflect.macros.runtime.Typers$$anonfun$typeCheck$2.apply(Typers.scala:25)
	at scala.tools.nsc.typechecker.Contexts$Context.withMacrosEnabled(Contexts.scala:242)
	at scala.reflect.macros.runtime.Typers$$anonfun$3.apply(Typers.scala:18)
	at scala.reflect.macros.runtime.Typers$$anonfun$3.apply(Typers.scala:18)
	at scala.reflect.macros.runtime.Typers$$anonfun$wrapper$1$1.apply(Typers.scala:19)
	at scala.reflect.macros.runtime.Typers$$anonfun$wrapper$1$1.apply(Typers.scala:19)
	at scala.tools.nsc.typechecker.Contexts$Context.withImplicitsEnabled(Contexts.scala:211)
	at scala.reflect.macros.runtime.Typers$$anonfun$1.apply(Typers.scala:17)
	at scala.reflect.macros.runtime.Typers$$anonfun$1.apply(Typers.scala:17)
	at scala.reflect.macros.runtime.Typers$class.wrapper$1(Typers.scala:19)
	at scala.reflect.macros.runtime.Typers$class.typeCheck(Typers.scala:25)
	at scala.reflect.macros.runtime.Context.typeCheck(Context.scala:6)
	at scala.reflect.macros.runtime.Context.typeCheck(Context.scala:6)
	at eu.stratosphere.scala.codegen.TreeGen$class.typeCheck(TreeGen.scala:184)
	at eu.stratosphere.scala.codegen.MacroContextHolder$$anon$1.typeCheck(MacroContextHolder.scala:21)
	at eu.stratosphere.scala.codegen.UDTGen$class.mkUdtClass(UDTGen.scala:37)
	at eu.stratosphere.scala.codegen.MacroContextHolder$$anon$1.mkUdtClass(MacroContextHolder.scala:21)
	at eu.stratosphere.scala.operators.MapMacros$.flatMap(MapOperator.scala:105)
```
Seems like the `scala.Array.canBuildFrom` import is not properly resolved.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/267
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Created at: Mon Nov 11 18:25:49 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397636,,,Mon Jun 09 11:51:40 UTC 2014,,,,,,,,,,"0|i1wgj3:",397763,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:51;github-import;[Date: Mon Nov 11 18:46:00 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

I updated the quickstart to make it work with the latest stratosphere version.;;;","09/Jun/14 11:51;github-import;[Date: Mon Nov 11 20:19:57 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Did you test it?

I'm still getting
```
$ mvn package
[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Your Job's Name 0.1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- build-helper-maven-plugin:1.7:add-source (add-source) @ stratosphere-project ---
[INFO] Source directory: /home/robert/Projekte/t-labs/bdaw.usecases/risk/tuberlin/verify-results/src/main/scala added.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ stratosphere-project ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/robert/Projekte/t-labs/bdaw.usecases/risk/tuberlin/verify-results/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ stratosphere-project ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- scala-maven-plugin:3.1.4:compile (default) @ stratosphere-project ---
[INFO] /home/robert/Projekte/t-labs/bdaw.usecases/risk/tuberlin/verify-results/src/main/scala:-1: info: compiling
[INFO] Compiling 1 source files to /home/robert/Projekte/t-labs/bdaw.usecases/risk/tuberlin/verify-results/target/classes at 1384197548509
[ERROR] /home/robert/Projekte/t-labs/bdaw.usecases/risk/tuberlin/verify-results/src/main/scala/eu/stratosphere/quickstart/Job.scala:83: error: object eu.stratosphere.pact.common.io.DelimitedOutputFormat is not a value
[ERROR]     val output = counts.write(wordsOutput, DelimitedOutputFormat(formatOutput.tupled))
[ERROR]                                            ^
[ERROR] one error found
```

BTW: I actually wanted to test if its possible to write a little scala job within 5 minutes. Seems like the test already failed.;;;","09/Jun/14 11:51;github-import;[Date: Mon Nov 11 20:40:55 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

I just did a:
```bash
curl https://raw.github.com/stratosphere/stratosphere-quickstart/master/quickstart-scala.sh | bash
cd quickstart
mvn package
mvn exec:exec -Dexec.executable=""java"" -Dexec.args=""-cp %classpath eu.stratosphere.quickstart.RunJobLocal 2 file:///home/aljoscha/unison.log file:///home/aljoscha/wc-out""
```

It worked just like that.;;;","09/Jun/14 11:51;github-import;[Date: Mon Nov 11 20:47:54 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, sorry. Seems that there is something wrong with my eclipse here? You're right, its working: http://showterm.io/a354d7e71755184bdd406;;;","09/Jun/14 11:51;github-import;[Date: Mon Nov 11 20:49:44 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

IntelliJ for the win... :+1: ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix stalling JobManagerITCase.testBroadcastChannels() (on travis),FLINK-263,12719433,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:51,09/Jun/14 11:51,14/Jul/23 05:57,09/Jun/14 11:51,,,,pre-apache,,,,,,,0,github-import,,"The test is currently disabled
https://github.com/stratosphere/stratosphere/blob/master/nephele/nephele-server/src/test/java/eu/stratosphere/nephele/jobmanager/JobManagerITCase.java?source=cc#L530

It does not fail on my own computers, only on travis but not always on travis:
If it fails, it just hangs.
Logs: https://s3.amazonaws.com/archive.travis-ci.org/jobs/13760959/log.txt (https://travis-ci.org/rmetzger/stratosphere/jobs/13760959)
https://travis-ci.org/rmetzger/stratosphere/jobs/13744608
https://travis-ci.org/rmetzger/stratosphere/jobs/13760618
(The logs are from some test builds with the local distributed executor https://github.com/stratosphere/stratosphere/pull/262, with this test enabled.)

The fails are random, I don't see a correlation between jvm versions or build profiles.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/263
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, runtime, 
Created at: Sun Nov 10 19:59:23 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397632,,,Mon Jun 09 11:51:18 UTC 2014,,,,,,,,,,"0|i1wgi7:",397759,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:51;github-import;[Date: Sun Nov 10 20:03:29 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I would ignore (disable) that test for now, because the code it tests is
known to be broken and is not used right now.


On Sun, Nov 10, 2013 at 7:59 PM, Robert Metzger <notifications@github.com>wrote:

> The test is currently disabled
>
> https://github.com/stratosphere/stratosphere/blob/master/nephele/nephele-server/src/test/java/eu/stratosphere/nephele/jobmanager/JobManagerITCase.java?source=cc#L530
>
> It does not fail on my own computers, only on travis but not always on
> travis:
> If it fails, it just hangs.
> Logs: https://s3.amazonaws.com/archive.travis-ci.org/jobs/13760959/log.txt(
> https://travis-ci.org/rmetzger/stratosphere/jobs/13760959)
> https://travis-ci.org/rmetzger/stratosphere/jobs/13744608
> https://travis-ci.org/rmetzger/stratosphere/jobs/13760618
> (The logs are from some test builds with the local distributed executor
> ([#262|https://github.com/stratosphere/stratosphere/issues/262] | [FLINK-262|https://issues.apache.org/jira/browse/FLINK-262]) <https://github.com/stratosphere/stratosphere/pull/262>, with this
> test enabled.)
>
> The fails are random, I don't see a correlation between jvm versions or
> build profiles.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/263>
> .
>;;;","09/Jun/14 11:51;github-import;[Date: Thu Mar 06 12:24:34 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The Broadcast/Multicast logic has been removed, as it was inherently unstable and unpredictable. It was not used anyways.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalDistributedExecutor: Execute multiple TaskManagers in one JVM,FLINK-262,12719432,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:51,09/Jun/14 11:51,14/Jul/23 05:57,09/Jun/14 11:51,,,,pre-apache,,,,,,,0,github-import,,"Fix for this issue: https://github.com/dimalabs/ozone/issues/86

The LocalDistributedExecutor simulates a Nephele-Cluster with multiple TaskManagers in one JVM. Data between the TMs is transferred using the OS network stack, not through memory. This is important to find bugs in the network stack.

It would be great to integrate the LDE with `TestBase` and `TestBase2`. I already started with this, but I had some errors.

(this is my second LDE PR, the first one (https://github.com/stratosphere/stratosphere/pull/130) had some issues)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/262
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Sun Nov 10 16:48:55 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:51;github-import;pull-request-262-7961650207519729429.patch;https://issues.apache.org/jira/secure/attachment/12649052/pull-request-262-7961650207519729429.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397631,,,2014-06-09 11:51:10.0,,,,,,,,,,"0|i1wghz:",397758,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerrit Code Review or Github’s fork,FLINK-260,12719430,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:50,09/Jun/14 11:50,14/Jul/23 05:57,09/Jun/14 11:50,,,,pre-apache,,,,,,,0,github-import,,"Just given the open source community a chance to comment on that:
I propose to enable this project for
http://gitenterprise.me/

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/260
Created by: [physikerwelt|https://github.com/physikerwelt]
Labels: 
Created at: Sat Nov 09 14:03:37 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397629,,,Mon Jun 09 11:50:53 UTC 2014,,,,,,,,,,"0|i1wghj:",397756,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:50;github-import;[Date: Sat Nov 09 15:02:05 CET 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

Do we have any problems with github and the pull requests? If not, we should live like: ""Don't fix it if it ain't broken"".;;;","09/Jun/14 11:50;github-import;[Date: Sat Nov 09 16:38:23 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

https://upload.wikimedia.org/wikipedia/commons/9/98/Ericsson_Taxen_%282%29.jpg;;;","09/Jun/14 11:50;github-import;[Date: Sun Nov 10 19:23:58 CET 2013, Author: [ktzoumas|https://github.com/ktzoumas]]

Nice phone. I am closing this for now.

@physikerwelt I propose that you open a new issue with an informative text on what problems this can solve for the project, or what processes it can improve on.;;;","09/Jun/14 11:50;github-import;[Date: Sun Nov 10 19:55:57 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

@ktzoumas gerrithubt simply adds another (maybe simpler) way to contribute to the project (see the link http://gitenterprise.me/ for a detailed comparison). It can be set up within 5 minutes does create only little management overhead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala renames and cleanups,FLINK-258,12719428,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:50,14/Mar/19 12:52,14/Jul/23 05:57,09/Jun/14 11:50,,,,pre-apache,,,API / Scala,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/258
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri Nov 08 17:25:42 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:50;github-import;pull-request-258-8375824509847558190.patch;https://issues.apache.org/jira/secure/attachment/12649051/pull-request-258-8375824509847558190.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397627,,,Mon Jun 09 11:50:44 UTC 2014,,,,,,,,,,"0|i1wgh3:",397754,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:50;github-import;[Date: Sun Nov 10 12:11:24 CET 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

Can we use DataCollection instead of DataSet? Since the system operators do not eliminate duplicates, DataCollection seems seems to be the more appropriate than DataSet. If we add duplicate elimination and duplicate eliminating operators to the runtime in the future, we can use DataSet as a specialisation of DataCollection that implicitly enforces this behaviour.;;;","09/Jun/14 11:50;github-import;[Date: Sun Nov 10 12:38:09 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

But in my mind DataSet is not associated with a mathematical Set. Other frameworks also use some variation of DataSet (see spark; Resilient Distributed DataSet).;;;","09/Jun/14 11:50;github-import;[Date: Sun Nov 10 12:40:36 CET 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

I agree with Aljoscha, I think dataset is the common term used by machine learners and data scientists;;;","09/Jun/14 11:50;github-import;[Date: Mon Nov 11 16:56:09 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [fd9520a571a5722774782ffdebf79237b6cf87c2|https://github.com/stratosphere/stratosphere/commit/fd9520a571a5722774782ffdebf79237b6cf87c2];;;","09/Jun/14 11:50;github-import;[Date: Mon Nov 11 20:33:59 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Does the change break bigdataclass ?;;;","09/Jun/14 11:50;github-import;[Date: Mon Nov 11 20:41:42 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

I changed bigdataclass to work with the new version.;;;","09/Jun/14 11:50;github-import;[Date: Mon Nov 25 21:59:39 CET 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

The following Scala classes still need to be renamed:

* CoGroupDataStream -> CoGroupDataSet
* JoinDataStream -> JoinDataSet
* CrossDataStream -> CrossDataSet

If I see more I'll add them here.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Renaming,FLINK-257,12719427,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:50,09/Jun/14 11:50,14/Jul/23 05:57,09/Jun/14 11:50,,,,pre-apache,,,,,,,0,github-import,,"Here is a comprehensive list of all renamings that we want to make before the release. Since renaming will break user code, we should do that once and coordinated, if we do it at all.


 -  Match -> Join

 -  Contract -> Operation
  -  MapContract -> MapOperator
  -  ReduceContract -> ReduceOperator
  -  MatchContract -> JoinOperator
  -  CoGroupContract -> CoGroupOperator
  -  CrossContract -> CrossOperator

 -  RecordInputFormat -> CsvInputFormat
 -  RecordOutputFormat -> CsvOutputFormat

 -  PlanAssembler -> Job (or StratosphereJob)

 -  Plan -> remains Plan (no change)
 -  ScalaPlan -> Plan

The next version of Pact program construction works without a Plan object and will be called ""Program""

---

We also want to reorganize the code project structure (aka the maven dependency structure) to make it simpler for developers and users. As this also breaks programs (in terms of configured dependencies), we want to do this in the same refactoring.

![projects|https://f.cloud.github.com/assets/1727146/1501858/5a9d9184-4892-11e3-91ed-987ae2ea487e.png]

---

This issue subsumes issues ([#222|https://github.com/stratosphere/stratosphere/issues/222] | [FLINK-222|https://issues.apache.org/jira/browse/FLINK-222]) ([#246|https://github.com/stratosphere/stratosphere/issues/246] | [FLINK-246|https://issues.apache.org/jira/browse/FLINK-246])

---

Please comment and raise concerns early!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/257
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Milestone: Release 0.4
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Fri Nov 08 16:51:04 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397626,,,Mon Jun 09 11:50:33 UTC 2014,,,,,,,,,,"0|i1wggv:",397753,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:50;github-import;[Date: Fri Nov 08 18:55:54 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

We have some submodules which are only conditionally included into the build. nephele-yarn and pact-hbase.

Affected by this change
 * Users have to change their dependencies + code
 * pending pull request
 * other unmerged work 
 * Quickstarts
 * BigDataClass.org
 * stratosphere2 should also do this change to be able to pull our changes
 * Documentation, slides, everything out in the web 
 * Source code documentation


But I totally agree with all changes!
We should do these changes as soon as possible!

What does ""bleibt"" and ""oder"" mean?;;;","09/Jun/14 11:50;github-import;[Date: Sat Nov 09 18:52:12 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

When doing the renaming, we can also fix this issue https://github.com/stratosphere/stratosphere/issues/122
;;;","09/Jun/14 11:50;github-import;[Date: Sun Nov 10 15:29:32 CET 2013, Author: [andrehacker|https://github.com/andrehacker]]

Just one comment: For me, CoGroup was always like a Reduce for multiple inputs.
Therefore I would propose to either change them to ""Reduce and CoReduce"" or ""Group and CoGroup"".
Or simply call them both ""Reduce"" or ""Group"", with two Builder classes: one for a single and one for multiple inputs.
(All other contracts have a very different semantic, but CoGroup and Reduce are almost the same in terms of semantic)

I guess you already discussed a lot about renaming, so if my proposal is not new - just ignore it ;-)
André;;;","09/Jun/14 11:50;github-import;[Date: Mon Nov 11 10:21:54 CET 2013, Author: [uce|https://github.com/uce]]

If we go with @andrehacker's proposal, we could also think about having just a single builder which decides to use CoGroup or Reduce depending on the inputs.

---

We should also rename the scripts. Replace the `pact-` prefix with `stratosphere-` and for the visualization script remove the `nephele-` prefix.;;;","09/Jun/14 11:50;github-import;[Date: Mon Nov 11 11:00:37 CET 2013, Author: [uce|https://github.com/uce]]

And JobManager and TaskManager are also very Hadoop inspired. Why not rename them to **master** and **worker**?

I know that @rmetzger is against this, because the comments will get out of sync, but they are out of sync already and need fixing anyways.;;;","09/Jun/14 11:50;github-import;[Date: Mon Nov 11 13:19:32 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

So is this package restructuring still happening? I'm all for it.;;;","09/Jun/14 11:50;github-import;[Date: Mon Nov 11 13:56:32 CET 2013, Author: [uce|https://github.com/uce]]

Just noticed: We also have to rename the config keys (like `pact.parallelization.degree`) if we want to get away from the `pact` name.

---

Btw: I'm also all for it.;;;","09/Jun/14 11:50;github-import;[Date: Mon Nov 11 14:33:31 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I agree with most changes but I am not sure if it is a good idea to replace Pact by Stratosphere and keep Nephele as it is. Stratosphere is the name of the whole thing and naming only a part of the system like that might confuse people. Pact might not be the best name, but Stratosphere is worse in my opinion. 
If Pact should go, I think we should use a new name for that...;;;","09/Jun/14 11:50;github-import;[Date: Mon Nov 11 14:33:52 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Wrong button, sorry...;;;","09/Jun/14 11:50;github-import;[Date: Fri Nov 22 09:43:16 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

We should also rename the `PactRecord` and all the other types with `Pact*`;;;","09/Jun/14 11:50;github-import;[Date: Sat Nov 23 18:34:33 CET 2013, Author: [uce|https://github.com/uce]]

Please also make sure that we rename all packages containing `type` like `eu.stratosphere.pact.common.type.Value` for example to `types`, becasue `type` is a Scala keyword.;;;","09/Jun/14 11:50;github-import;[Date: Fri Dec 13 16:15:52 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I am currently reorganizing the maven projects according to the components in the sketch above. I am now struggling where to put the clients and the compiler web-frontend. Any preferences?

Right now I see these options:
  - Make an own project for clients
  - Make it part of the compiler project, as the clients need the compiler/runtime anyways. At least the LocalExecutor needs to runtime and the remote executor needs the compiler until we moved the compiler to the master.
;;;","09/Jun/14 11:50;github-import;[Date: Thu Jan 09 09:27:26 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

`Pact-Assembler-Class` has been renamed to `program-class`.;;;","09/Jun/14 11:50;github-import;[Date: Mon Jan 20 23:36:01 CET 2014, Author: [fhueske|https://github.com/fhueske]]

@StephanEwen, @rmetzger: Is this issue resolved and can it be closed?;;;","09/Jun/14 11:50;github-import;[Date: Wed Jan 22 22:05:29 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

This issue is resolved.


On Mon, Jan 20, 2014 at 2:36 PM, Fabian Hueske <notifications@github.com>wrote:

> @StephanEwen <https://github.com/StephanEwen>, @rmetzger<https://github.com/rmetzger>:
> Is this issue resolved and can it be closed?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/257#issuecomment-32803797>
> .
>;;;","09/Jun/14 11:50;github-import;[Date: Fri Jan 24 10:59:12 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Alright, I close it then.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nephele ""loses"" data packets",FLINK-256,12719426,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,uce,github-import,github-import,09/Jun/14 11:50,13/Jun/14 00:06,14/Jul/23 05:57,13/Jun/14 00:06,,,,pre-apache,,,,,,,0,github-import,,"A pact job dies with the following message: ""An error occurred in the channel: Expected data packet 25 but received 27""

The error occurred both in version 0.21 and with the current 0.4-snapshot. Apparently only with bigger data sizes. 15GB Input data worked fine, 22GB crashed. I sent the job, that caused the error to Stephan Ewen. 

Full stack trace: 
15:32:11,639 ERROR eu.stratosphere.pact.runtime.task.RegularPactTask             - Error in PACT code: Join tweets and dates (2/4)
15:32:11,640 ERROR eu.stratosphere.pact.runtime.task.RegularPactTask             - java.io.IOException: An error occurred in the channel: Expected data packet 25 but received 27
java.io.IOException: An error occurred in the channel: Expected data packet 25 but received 27
        at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedInputChannel.isClosed(AbstractByteBufferedInputChannel.java:144)
        at eu.stratosphere.nephele.io.RuntimeInputGate.isClosed(RuntimeInputGate.java:261)
        at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:182)
        at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:80)
        at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:62)
        at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:27)
        at eu.stratosphere.pact.runtime.hash.MutableHashTable$ProbeIterator.next(MutableHashTable.java:1525)
        at eu.stratosphere.pact.runtime.hash.MutableHashTable.processProbeIter(MutableHashTable.java:450)
        at eu.stratosphere.pact.runtime.hash.MutableHashTable.nextRecord(MutableHashTable.java:536)
        at eu.stratosphere.pact.runtime.hash.BuildFirstHashMatchIterator.callWithNextKey(BuildFirstHashMatchIterator.java:116)
        at eu.stratosphere.pact.runtime.task.MatchDriver.run(MatchDriver.java:164)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:372)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
        at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Expected data packet 25 but received 27
        at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputChannelContext.queueTransferEnvelope(RuntimeInputChannelContext.java:148)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeWithBuffer(ByteBufferedChannelManager.java:365)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelope(ByteBufferedChannelManager.java:331)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeFromNetwork(ByteBufferedChannelManager.java:644)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnection.read(IncomingConnection.java:100)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.doRead(IncomingConnectionThread.java:187)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.run(IncomingConnectionThread.java:126)
15:32:12,362 WARN eu.stratosphere.pact.runtime.task.RegularPactTask             - Cancelling PACT code: Join tweets and dates (2/4)
15:32:12,362 INFO eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from RUNNING to FAILED for task Join tweets and dates (2/4)
15:32:12,362 ERROR eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask       - java.io.IOException: An error occurred in the channel: Expected data packet 25 but received 27
        at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedInputChannel.isClosed(AbstractByteBufferedInputChannel.java:144)
        at eu.stratosphere.nephele.io.RuntimeInputGate.isClosed(RuntimeInputGate.java:261)
        at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:182)
        at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:80)
        at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:62)
        at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:27)
        at eu.stratosphere.pact.runtime.hash.MutableHashTable$ProbeIterator.next(MutableHashTable.java:1525)
        at eu.stratosphere.pact.runtime.hash.MutableHashTable.processProbeIter(MutableHashTable.java:450)
        at eu.stratosphere.pact.runtime.hash.MutableHashTable.nextRecord(MutableHashTable.java:536)
        at eu.stratosphere.pact.runtime.hash.BuildFirstHashMatchIterator.callWithNextKey(BuildFirstHashMatchIterator.java:116)
        at eu.stratosphere.pact.runtime.task.MatchDriver.run(MatchDriver.java:164)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:372)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
        at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Expected data packet 25 but received 27
        at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputChannelContext.queueTransferEnvelope(RuntimeInputChannelContext.java:148)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeWithBuffer(ByteBufferedChannelManager.java:365)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelope(ByteBufferedChannelManager.java:331)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeFromNetwork(ByteBufferedChannelManager.java:644)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnection.read(IncomingConnection.java:100)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.doRead(IncomingConnectionThread.java:187)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.run(IncomingConnectionThread.java:126)

15:32:12,394 INFO eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask       - Canceling Hashtag Polarity Match (2/4)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/256
Created by: [matpeters|https://github.com/matpeters]
Labels: bug, runtime, user satisfaction, 
Created at: Fri Nov 08 13:57:57 CET 2013
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397625,,,Fri Jun 13 00:06:32 UTC 2014,,,,,,,,,,"0|i1wggn:",397752,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:50;github-import;[Date: Fri Nov 08 14:03:24 CET 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

I might have seen a similar error message some time ago, could it be the erroneous multicast in Nephele?;;;","09/Jun/14 11:50;github-import;[Date: Fri Nov 08 14:04:42 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Nephele multicast is disabled by default in the current version.

I would expect it is faulty logic at channel tear-down (closing).


On Fri, Nov 8, 2013 at 2:03 PM, sscdotopen <notifications@github.com> wrote:

> I might have seen a similar error message some time ago, could it be the
> erroneous multicast in Nephele?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/256#issuecomment-28061009>
> .
>;;;","09/Jun/14 11:50;github-import;[Date: Wed Nov 20 11:16:46 CET 2013, Author: [AHeise|https://github.com/AHeise]]

I have a similar issue when increasing the input size of a self-cross. With 50k input records it runs fine, but pretty much everything above crashes (reproducible!, same packet numbers on the same input sizes). 

```
The job was not successfully executed: eu.stratosphere.nephele.client.JobExecutionException: java.io.IOException: An error occurred in the channel: Expected data packet 7 but received 8
        at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedInputChannel.isClosed(AbstractByteBufferedInputChannel.java:144)
        at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedInputChannel.readRecord(AbstractByteBufferedInputChannel.java:94)
        at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:193)
        at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:80)
        at eu.stratosphere.pact.runtime.task.util.NepheleReaderIterator.next(NepheleReaderIterator.java:72)
        at eu.stratosphere.pact.runtime.resettable.SpillingResettableMutableObjectIterator.next(SpillingResettableMutableObjectIterator.java:152)
        at eu.stratosphere.pact.runtime.task.CrossDriver.runBlockedOuterFirst(CrossDriver.java:235)
        at eu.stratosphere.pact.runtime.task.CrossDriver.run(CrossDriver.java:165)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:370)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
        at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: Expected data packet 7 but received 8
        at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputChannelContext.queueTransferEnvelope(RuntimeInputChannelContext.java:148)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeWithBuffer(ByteBufferedChannelManager.java:365)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelope(ByteBufferedChannelManager.java:331)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeFromNetwork(ByteBufferedChannelManager.java:644)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnection.read(IncomingConnection.java:100)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.doRead(IncomingConnectionThread.java:187)
        at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.run(IncomingConnectionThread.java:126)

        at eu.stratosphere.nephele.client.JobClient.submitJobAndWait(JobClient.java:353)
        at eu.stratosphere.sopremo.server.SopremoExecutionThread.executePlan(SopremoExecutionThread.java:152)
        at eu.stratosphere.sopremo.server.SopremoExecutionThread.processPlan(SopremoExecutionThread.java:71)
        at eu.stratosphere.sopremo.server.SopremoExecutionThread.run(SopremoExecutionThread.java:64)
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Unknown Source)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
```
;;;","09/Jun/14 11:50;github-import;[Date: Wed Nov 20 11:28:03 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Good that it is reproducable. Can you send me the job and the data and the other parameters (DOP, etc) ? I will make this a priority issue...;;;","09/Jun/14 11:50;github-import;[Date: Wed Mar 05 14:31:58 CET 2014, Author: [mathiaspet|https://github.com/mathiaspet]]

Hi,

is there a fix for this issue? I just saw the bug on a small data set with the current release. 
 ;;;","09/Jun/14 11:50;github-import;[Date: Wed Mar 05 15:47:49 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

We are in the process of reworking the network stack for fault tolerance. I
think it will be fixed along those lines.

Can you provide a test that sort of reliably reproduces the problem?;;;","09/Jun/14 11:50;github-import;[Date: Thu Mar 06 13:07:29 CET 2014, Author: [mathiaspet|https://github.com/mathiaspet]]

Hi,

increasing the number of nw buffers helped. The job finished. I have just a test program from one of the PhD students from Trento, Matteo Lissandrini. It is located here: https://github.com/kuzeko/Tweets-Analyser. 

I can package and link the data set that he used. In general, this seems to happen if a lot of messages about task state changes get sent.  ;;;","13/Jun/14 00:05;sewen;Fixed in 4cd4a13415d609a2979c8fa3cf4b797c990ee8c2;;;","13/Jun/14 00:06;sewen;Fixed with the introduction of the Netty library to replace custom NIO code.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CustomDataTypeTest fails in compiler,FLINK-255,12719425,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:50,09/Jun/14 11:50,14/Jul/23 05:57,09/Jun/14 11:50,,,,pre-apache,,,,,,,0,github-import,,"For example:
https://s3.amazonaws.com/archive.travis-ci.org/jobs/13633118/log.txt

```
Running eu.stratosphere.pact.test.contracts.io.CustomDataTypeTest
14:34:28,651 WARN  eu.stratosphere.pact.compiler.PactCompiler                    - Could not instantiate input format to obtain statistics. Limited statistics will be available.
java.lang.IllegalStateException: java.lang.ClassNotFoundException: eu.stratosphere.pact.test.external.TestClass
	at eu.stratosphere.nephele.configuration.Configuration.getClass(Configuration.java:115)
	at eu.stratosphere.pact.test.contracts.io.CustomDataTypeTest$EmptyInputFormat.configure(CustomDataTypeTest.java:95)
	at eu.stratosphere.pact.compiler.plan.DataSourceNode.computeOutputEstimates(DataSourceNode.java:123)
	at eu.stratosphere.pact.compiler.PactCompiler$GraphCreatingVisitor.postVisit(PactCompiler.java:975)
	at eu.stratosphere.pact.compiler.PactCompiler$GraphCreatingVisitor.postVisit(PactCompiler.java:771)
	at eu.stratosphere.pact.common.contract.GenericDataSource.accept(GenericDataSource.java:165)
	at eu.stratosphere.pact.common.contract.GenericDataSink.accept(GenericDataSink.java:376)
	at eu.stratosphere.pact.common.plan.Plan.accept(Plan.java:203)
	at eu.stratosphere.pact.compiler.PactCompiler.compile(PactCompiler.java:665)
	at eu.stratosphere.pact.compiler.PactCompiler.compile(PactCompiler.java:547)
	at eu.stratosphere.pact.test.contracts.io.CustomDataTypeTest.getJobGraph(CustomDataTypeTest.java:75)
	at eu.stratosphere.pact.test.util.TestBase.testJob(TestBase.java:105)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:104)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:147)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:98)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.ProviderFactory$ClassLoaderProxy.invoke(ProviderFactory.java:101)
	at com.sun.proxy.$Proxy0.invoke(Unknown Source)
	at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:139)
	at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcess(SurefireStarter.java:82)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:81)
Caused by: java.lang.ClassNotFoundException: eu.stratosphere.pact.test.external.TestClass
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at eu.stratosphere.nephele.configuration.Configuration.getClass(Configuration.java:113)
	... 49 more
14:34:28,875 WARN  eu.stratosphere.nephele.jobmanager.splitassigner.InputSplitManager  - Unable to find specific input split provider for type eu.stratosphere.nephele.template.GenericInputSplit, using default assigner
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.793 sec
```

I will look into the maven configuration to properly trigger a failed test.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/255
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, core, optimizer, 
Created at: Fri Nov 08 13:19:19 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397624,,,Mon Jun 09 11:50:15 UTC 2014,,,,,,,,,,"0|i1wggf:",397751,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:50;github-import;[Date: Fri Nov 08 13:53:40 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

The exception is being catched in the `DataSourceNode` class (Line 125)
;;;","09/Jun/14 11:50;github-import;[Date: Fri Nov 08 14:02:08 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, the problem is that the compiler has currently no user code class loader. I'll take a quick stab at that...;;;","09/Jun/14 11:50;github-import;[Date: Mon Nov 11 19:28:59 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

First part is fixed in [80660c6e9dbc65802c6c53ba2f6275d20d98dbe4|https://github.com/stratosphere/stratosphere/commit/80660c6e9dbc65802c6c53ba2f6275d20d98dbe4].

Next step is to provide the compiler with a PlanWithJars instead of only a Plan.;;;","09/Jun/14 11:50;github-import;[Date: Mon Feb 03 20:08:55 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Test has been removed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for using arbitrary Pact Value classes in scala frontend,FLINK-254,12719424,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:50,09/Jun/14 11:50,14/Jul/23 05:57,09/Jun/14 11:50,,,,pre-apache,,,,,,,0,github-import,,"Also add checks in key selector extraction to check whether the selected
fields can actually be part of a key.

@StephanEwen, btw, PactDouble implements Key.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/254
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri Nov 08 11:44:20 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:50;github-import;pull-request-254-7450811428982579950.patch;https://issues.apache.org/jira/secure/attachment/12649050/pull-request-254-7450811428982579950.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397623,,,Mon Jun 09 11:50:10 UTC 2014,,,,,,,,,,"0|i1wgg7:",397750,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:50;github-import;[Date: Fri Nov 08 15:09:42 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Ok, I removed the Grabbag2 think, added WordCountPactValue, which is the WordCount example but uses PactString and PactInteger. There is also an integration test for that.;;;","09/Jun/14 11:50;github-import;[Date: Fri Nov 08 16:04:47 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [cbfc5c2097f2286f6e1b27beb695d3d2292c9570|https://github.com/stratosphere/stratosphere/commit/cbfc5c2097f2286f6e1b27beb695d3d2292c9570];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Manually configured JAVA_HOME is preferred over system configuration. ,FLINK-253,12719423,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:49,09/Jun/14 11:50,14/Jul/23 05:57,09/Jun/14 11:50,,,,pre-apache,,,,,,,0,github-import,,"Manually configured JAVA_HOME is preferred over system configuration. 
No default JAVA_HOME is used. Instead an error message is shown if neither config nor system JAVA_HOME is defined. Addresses ([#207|https://github.com/stratosphere/stratosphere/issues/207] | [FLINK-207|https://issues.apache.org/jira/browse/FLINK-207])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/253
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Fri Nov 08 09:38:17 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:49;github-import;pull-request-253-2851636796014855437.patch;https://issues.apache.org/jira/secure/attachment/12649049/pull-request-253-2851636796014855437.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397622,,,Mon Jun 09 11:50:04 UTC 2014,,,,,,,,,,"0|i1wgfz:",397749,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:50;github-import;[Date: Fri Nov 08 10:46:39 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

With this pull request, Stratosphere does not work on our cluster out of the box.
;;;","09/Jun/14 11:50;github-import;[Date: Fri Nov 08 11:01:19 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I don't think that should be an issue since our specific setup should not influence the default settings.
We should either properly set JAVA_HOME or remove one character ('#') in stratosphere-conf.yaml everytime we do a new setup.

I think having a default JVM specified is a bad idea since it is distribution specific. If JAVA_HOME is not set and the default configuration does not work, no process is started and the only error message is hidden in the ./log/jobmanager.out file.
Furthermore, manual configuration should override system configuration and not the other way round.;;;","09/Jun/14 11:50;github-import;[Date: Fri Nov 08 11:03:06 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

If I see it right, we need the `JAVA_HOME` **only** for 
`JAVA_RUN=$JAVA_HOME/bin/java`

Your pull request disables our default strategy to just use ""java""  (resolved by the PATH variable) if JAVA_HOME is not set.
In almost all cases invoking ""java"" will suffice ;;;","09/Jun/14 11:50;github-import;[Date: Fri Nov 08 11:11:34 CET 2013, Author: [fhueske|https://github.com/fhueske]]

OK, I see. So we need another check if ""java"" is available.

The right order is IMHO:
1) stratosphere-conf.yaml
2) JAVA_HOME
3) java
4) error message.

;;;","09/Jun/14 11:50;github-import;[Date: Fri Nov 08 11:13:16 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Yes, I think this is the best order.;;;","09/Jun/14 11:50;github-import;[Date: Fri Nov 08 11:33:04 CET 2013, Author: [fhueske|https://github.com/fhueske]]

OK, I adapted the script to use java as third option before showing an error message.;;;","09/Jun/14 11:50;github-import;[Date: Fri Nov 08 11:40:28 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, works on the cluster. 

I hereby approve this pull request.;;;","09/Jun/14 11:50;github-import;[Date: Mon Nov 11 16:56:38 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [1edf660e85a19772771f4aa50407f7e5dd860385|https://github.com/stratosphere/stratosphere/commit/1edf660e85a19772771f4aa50407f7e5dd860385];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ignore git-commit-id when in eclipse,FLINK-252,12719422,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:49,09/Jun/14 11:49,14/Jul/23 05:57,09/Jun/14 11:49,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/252
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Thu Nov 07 13:34:27 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:49;github-import;pull-request-252-4044772108185859488.patch;https://issues.apache.org/jira/secure/attachment/12649048/pull-request-252-4044772108185859488.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397621,,,2014-06-09 11:49:54.0,,,,,,,,,,"0|i1wgfr:",397748,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Website shows ""Build Status: build error""",FLINK-251,12719421,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:49,09/Jun/14 11:49,14/Jul/23 05:57,09/Jun/14 11:49,,,,pre-apache,,,,,,,0,github-import,,"The Travis build status icon on the bottom of the [Stratosphere Github start page|https://github.com/stratosphere/stratosphere] shows ""build error"".

However, when clicking on the icon, the Travis website says ""Status: passed"".


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/251
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, website, 
Created at: Thu Nov 07 09:29:12 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397620,,,Mon Jun 09 11:49:52 UTC 2014,,,,,,,,,,"0|i1wgfj:",397747,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:49;github-import;[Date: Thu Nov 07 10:08:01 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Mh, so currently, both badges show ""build:error"" which is correct because the last build failed.

I hope the current build is going through. Lets see if the badge on our readme updates accordingly.;;;","09/Jun/14 11:49;github-import;[Date: Thu Nov 07 18:41:29 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Hmm, seems to be fine again.... 
Its probably a Travis issue anyways. I'm closing this issue for know. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes #242. Excludes example artefacts from distribution lib folder,FLINK-250,12719420,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:49,09/Jun/14 11:49,14/Jul/23 05:57,09/Jun/14 11:49,,,,pre-apache,,,,,,,0,github-import,,"Excludes nephele-examples, pact-examples, and pact-scala-examples from distribution lib folder.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/250
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Wed Nov 06 23:58:09 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:49;github-import;pull-request-250-981099901631070007.patch;https://issues.apache.org/jira/secure/attachment/12649047/pull-request-250-981099901631070007.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397619,,,Mon Jun 09 11:49:49 UTC 2014,,,,,,,,,,"0|i1wgfb:",397746,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:49;github-import;[Date: Fri Nov 08 12:54:49 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [cfb20f9362d832fd2f813e9a8f8575aa25f1ca70|https://github.com/stratosphere/stratosphere/commit/cfb20f9362d832fd2f813e9a8f8575aa25f1ca70];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rework Quickstart so that people use the binary instead of building stratosphere,FLINK-248,12719418,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:49,09/Jun/14 11:49,14/Jul/23 05:57,09/Jun/14 11:49,,,,pre-apache,,,,,,,0,github-import,,"This page needs to be updated: http://stratosphere.eu/quickstart/build.html

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/248
Created by: [rmetzger|https://github.com/rmetzger]
Labels: documentation, website, 
Milestone: Release 0.4
Created at: Wed Nov 06 23:04:43 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397617,,,Mon Jun 09 11:49:38 UTC 2014,,,,,,,,,,"0|i1wgev:",397744,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:49;github-import;[Date: Wed Nov 20 02:37:22 CET 2013, Author: [uce|https://github.com/uce]]

This has been fixed already. The new page is http://stratosphere.eu/quickstart/setup.html (the old build.html still exists).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Stratosphere Debian Package,FLINK-247,12719417,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:49,09/Jun/14 11:49,14/Jul/23 05:57,09/Jun/14 11:49,,,,pre-apache,,,,,,,0,github-import,,"Derived from https://github.com/stratosphere/stratosphere/issues/220

The Stratosphere debian package has not been tested!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/247
Created by: [rmetzger|https://github.com/rmetzger]
Labels: build system, 
Created at: Wed Nov 06 22:59:55 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397616,,,Mon Jun 09 11:49:33 UTC 2014,,,,,,,,,,"0|i1wgen:",397743,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:49;github-import;[Date: Sat Nov 16 13:42:42 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

The Instructions on the website are wrong. These are correct:

```
# vim /etc/apt/sources.list.d/stratosphere.list
deb http://dev.stratosphere.eu/repo binary/
```

The setup does not work
```
stratosphere-dist (0.2) wird eingerichtet ...
Lege Gruppe »ozone« (GID 138) an ...
Fertig.
sed: kann /usr/share/stratosphere-dist/conf/pact-user.xml nicht lesen: Datei oder Verzeichnis nicht gefunden
dpkg: Fehler beim Bearbeiten von stratosphere-dist (--configure):
```
;;;","09/Jun/14 11:49;github-import;[Date: Thu Feb 06 10:55:03 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

I'm preparing a pull request to fix the last issues with the debian package.

The problem is that the debian repository is not build correctly (I guess wrong mvn invocation). I already asked the responsible person to fix this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename Input and Output Formates,FLINK-246,12719416,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:49,09/Jun/14 11:49,14/Jul/23 05:57,09/Jun/14 11:49,,,,pre-apache,,,,,,,0,github-import,,"I think we should rename the input and output formats because the names are misleading. Delimited*Format sounds like a CSV style input format, but it is a line input format. Record Input Format has no intuitive meaning to most people.

How about we name them
 -  RecordInputFormat to CSVInputFormat (even though delimiters are not necessarily commas)
 -  DelimitedInputFormat to LineInputFormat

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/246
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: core, user satisfaction, 
Created at: Wed Nov 06 20:40:30 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397615,,,Mon Jun 09 11:49:29 UTC 2014,,,,,,,,,,"0|i1wgef:",397742,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:49;github-import;[Date: Wed Nov 06 20:47:03 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Agreed on CSVInputFormat, but LineInputFormat would make people think that they get individual lines in the records. But they don't, that's what TextFile() (and TextDataSourceFormat) is for.;;;","09/Jun/14 11:49;github-import;[Date: Wed Nov 06 20:55:03 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay. What could we call the DelimitedInputFormat? Should we just keep it? Does the term 'delimited' raise confusion with delimited fields?;;;","09/Jun/14 11:49;github-import;[Date: Wed Nov 06 21:08:00 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Maybe BlockInputFormat.;;;","09/Jun/14 11:49;github-import;[Date: Wed Nov 06 21:40:02 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I agree that the formats should be renamed. 
CSVIF is fine.
How about ByteRecordIF for DelimitedIF? It splits the data into records of bytes which must be manually parsed, since the IF is an abstract class.;;;","09/Jun/14 11:49;github-import;[Date: Fri Nov 08 16:51:59 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Subsumed by ([#257|https://github.com/stratosphere/stratosphere/issues/257] | [FLINK-257|https://issues.apache.org/jira/browse/FLINK-257]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No exception when the input format does not match the prescribed type,FLINK-245,12719415,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:49,09/Jun/14 11:49,14/Jul/23 05:57,09/Jun/14 11:49,,,,pre-apache,,,,,,,0,github-import,,"I have played with Scala today. Here is what I have written:

    case class Department(id: String, name:Int)
    ...
    val departments  = DataSource(""departments.txt"", RecordDataSourceFormat[Department|""\n"", "" ""))
    ...

The departments.txt file looks like this

    5 Marketing
    2 Enginnering
    3 Sales

Note the error in my Department class: the id is defined as a String and the department name is an Integer. What I would expect from the ""parser"" is to tell me that the file does not match what I prescribed. But if you run this program, there will be no exception and the departments input will be just empty. 

I think that the right thing to do is raise an exception saying that ""an error was found while parsing the output. Please check your data types."" According to @aljoscha , the problem is in the Java side: `RecordInputFormat`.

So, is this a bug or a feature (maybe, ignoring lines that do not match the data source definition]?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/245
Created by: [asteriosk|https://github.com/asteriosk]
Labels: bug, core, scala api, 
Milestone: Release 0.4
Created at: Wed Nov 06 15:26:31 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397614,,,Mon Jun 09 11:49:23 UTC 2014,,,,,,,,,,"0|i1wge7:",397741,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:49;github-import;[Date: Wed Nov 06 19:09:41 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The reason is that the input format skips invalid line. In the above case, all lines are invalid and skipped.

We can
 -  Have a lenient and non-lenient mode (lenient filtering erroneous records, non-lenient throwing errors)
 -  Have a way to access to rejected rows...;;;","09/Jun/14 11:49;github-import;[Date: Wed Nov 06 19:23:46 CET 2013, Author: [asteriosk|https://github.com/asteriosk]]

Why was this done this way? Did you have a specific use case in mind?

I think we should throw exceptions in the case of bad input: this can really make you spend days debugging if you have a slight problem in your data (or program). 

So, ""strict parsing"" for me should be the default and we could also provide a constructor that will allow skipping of input lines that are not valid.

What do you think?;;;","09/Jun/14 11:49;github-import;[Date: Wed Nov 06 20:02:00 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Sounds reasonable.

I cannot tell you why exactly it is like that, but we should change it definitely.;;;","09/Jun/14 11:49;github-import;[Date: Wed Nov 06 21:13:52 CET 2013, Author: [uce|https://github.com/uce]]

Lets also rename lenient to ""non-strict"" (or to strict and change the code logic).;;;","09/Jun/14 11:49;github-import;[Date: Fri Nov 08 12:23:40 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Default behavior is now that parsers fail on invalid lines ([710b12ab97ddf8f8f996959afedb1663da3c1252|https://github.com/stratosphere/stratosphere/commit/710b12ab97ddf8f8f996959afedb1663da3c1252]);;;","09/Jun/14 11:49;github-import;[Date: Fri Dec 06 23:51:51 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

This issue is fixed in [9dd4635ad69bf107ab9f81c8e7ffd495475471e0|https://github.com/stratosphere/stratosphere/commit/9dd4635ad69bf107ab9f81c8e7ffd495475471e0]

Default behavior is to fail on invalid input. Input format can be set to 'lenient' mode to ignore invalid lines.

@uce I named it 'lenient' for now. I thought that was a rather common term for such behavior. If you think non-strict should be used as a name, feel free to rename it. Leaving the issue open until this is resolved.;;;","09/Jun/14 11:49;github-import;[Date: Thu Feb 06 20:41:33 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Resolved;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot access RuntimeContext in open(),FLINK-244,12719414,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:49,09/Jun/14 11:49,14/Jul/23 05:57,09/Jun/14 11:49,,,,pre-apache,,,,,,,0,github-import,,"Invoking getRuntimeContext() in open() inside a RegularPactTask always causes an IllegalStateException. 

In RegularPactTask initLocalStrategies() and thus open() of the stub is called before the RuntimeContext is set with initialize().

Please note that inside a ChainedMapTask it works as expected

Here is the corresponding fragment of RegularPactTask:

```java
// initialize the remaining data structures on the input and trigger the local processing
// the local processing includes building the dams / caches

try {
	int numInputs = driver.getNumberOfInputs();

	initInputsSerializersAndComparators(numInputs);
	initLocalStrategies(numInputs);

} catch (Exception e) {
	throw new RuntimeException(""Initializing the input processing failed"" +

		e.getMessage() == null ? ""."" : "": "" + e.getMessage(), e);

}

if (!this.running) {

	if (LOG.isDebugEnabled())
		LOG.info(formatLogString(""Task cancelled before PACT code was started.""));

	return;
}

// pre main-function initialization

initialize();
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/244
Created by: [AHeise|https://github.com/AHeise]
Labels: bug, core, user satisfaction, 
Created at: Wed Nov 06 13:52:14 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397613,,,Mon Jun 09 11:49:15 UTC 2014,,,,,,,,,,"0|i1wgdz:",397740,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:49;github-import;[Date: Wed Nov 06 14:24:17 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Are you using a combiner that is executed as part of a local strategy (combining sort) ? That is the only point I can think of where the UDFs have cross-dependencies with the local strategies...;;;","09/Jun/14 11:49;github-import;[Date: Wed Nov 06 14:26:10 CET 2013, Author: [AHeise|https://github.com/AHeise]]

Yes it definitively breaks in a combiner.


See https://s3.amazonaws.com/archive.travis-ci.org/jobs/13543000/log.txt (search for “runtime context has not been initialized”)



From: Stephan Ewen [mailto:notifications@github.com]
Sent: Mittwoch, 6. November 2013 14:24
To: stratosphere/stratosphere
Cc: Heise, Arvid
Subject: Re: [stratosphere] Cannot access RuntimeContext in open() (([#244|https://github.com/stratosphere/stratosphere/issues/244] | [FLINK-244|https://issues.apache.org/jira/browse/FLINK-244]))


Are you using a combiner that is executed as part of a local strategy (combining sort) ? That is the only point I can think of where the UDFs have cross-dependencies with the local strategies...

—
Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/244#issuecomment-27872597>.
;;;","09/Jun/14 11:49;github-import;[Date: Wed Nov 06 14:27:12 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, should be a quick fix...;;;","09/Jun/14 11:49;github-import;[Date: Wed Nov 06 19:11:02 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [36aabd4e9472659ed3320817e6f2ced4404397b8|https://github.com/stratosphere/stratosphere/commit/36aabd4e9472659ed3320817e6f2ced4404397b8];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude the pact-examples and pact-scala-examples from being copied to lib,FLINK-242,12719412,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:49,09/Jun/14 11:49,14/Jul/23 05:57,09/Jun/14 11:49,,,,pre-apache,,,,,,,0,github-import,,"Curretly, the maven project jars are copied to lib. That means the classes are part of the bootstrap classpath. When one wants to run a modified example, it fails becasuse the classes are already in the class path (in an older version).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/242
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: build system, 
Assignee: [fhueske|https://github.com/fhueske]
Created at: Wed Nov 06 12:06:26 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397611,,,2014-06-09 11:49:06.0,,,,,,,,,,"0|i1wgdj:",397738,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create FAQ section on the Website,FLINK-241,12719411,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:49,09/Jun/14 11:49,14/Jul/23 05:57,09/Jun/14 11:49,,,,pre-apache,,,,,,,0,github-import,,"Create an FAQ Section in the Wiki and link it from the Website (next to Documentation?)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/241
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Assignee: [uce|https://github.com/uce]
Created at: Wed Nov 06 11:47:40 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397610,,,Mon Jun 09 11:49:04 UTC 2014,,,,,,,,,,"0|i1wgdb:",397737,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:49;github-import;[Date: Wed Nov 06 17:17:49 CET 2013, Author: [uce|https://github.com/uce]]

Why do we want this as a Wiki?

I would like to have it consistent with the documentation (therefore on the webpage) even though it would be easier to edit with a Wiki.;;;","09/Jun/14 11:49;github-import;[Date: Wed Nov 06 17:33:10 CET 2013, Author: [uce|https://github.com/uce]]

Our Documentation is terribly outdated. :mask: Will also look into it. :cake: ;;;","09/Jun/14 11:49;github-import;[Date: Wed Nov 06 18:26:34 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

This issue is related https://github.com/stratosphere/stratosphere/issues/77
;;;","09/Jun/14 11:49;github-import;[Date: Wed Nov 20 03:08:33 CET 2013, Author: [uce|https://github.com/uce]]

This has been fixed, the [FAQ|http://stratosphere.eu/faq] still needs some updating though.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Builds on Travis-CI currently not working,FLINK-240,12719410,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:48,09/Jun/14 11:49,14/Jul/23 05:57,09/Jun/14 11:49,,,,pre-apache,,,,,,,0,github-import,,"There seems to be an issue with Travis and or sonatype.

I've already asked here: https://groups.google.com/forum/#!topic/travis-ci/ZQWQUc6cjKA

Ideas to fix this are very welcome.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/240
Created by: [rmetzger|https://github.com/rmetzger]
Labels: build system, testing, 
Created at: Tue Nov 05 21:02:24 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397609,,,Mon Jun 09 11:48:59 UTC 2014,,,,,,,,,,"0|i1wgd3:",397736,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;[Date: Thu Dec 12 13:08:02 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I think this one is resolved? Afaik, we just have long and intensive tests right now, other things work, no?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change how DataSourceFormats are handled internally.,FLINK-239,12719409,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:48,09/Jun/14 11:48,14/Jul/23 05:57,09/Jun/14 11:48,,,,pre-apache,,,,,,,0,github-import,,"This and the addition of implicit UDT generation makes it easy to implement custom DataSourceFormats. I noticed the need for this from @rmetzger's avro input format.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/239
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Tue Nov 05 16:32:06 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;pull-request-239-4610924163832295033.patch;https://issues.apache.org/jira/secure/attachment/12649046/pull-request-239-4610924163832295033.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397608,,,Mon Jun 09 11:48:56 UTC 2014,,,,,,,,,,"0|i1wgcv:",397735,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;[Date: Tue Nov 05 21:39:51 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

So should we merge this without the OK from travis? It don't think this could break anything. @rmetzger, are you still planning to run scala with the avro input format for the telko guys?;;;","09/Jun/14 11:48;github-import;[Date: Tue Nov 05 22:24:24 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Sure, I need this PR to be merged. But I will continue to work on this tomorrow evening, so there is some time left.;;;","09/Jun/14 11:48;github-import;[Date: Wed Nov 06 14:27:39 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

This contains some other commits that were also necessary for this. I ran mvn verify on my machine, so this should be good to merge.;;;","09/Jun/14 11:48;github-import;[Date: Wed Nov 06 14:31:52 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I'll take a quick look and then merge it...;;;","09/Jun/14 11:48;github-import;[Date: Wed Nov 06 20:42:10 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Travis seems to be working again ...;;;","09/Jun/14 11:48;github-import;[Date: Wed Nov 06 21:06:42 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [a02f070305e768a7976ad4cb503f091113a4a1bc|https://github.com/stratosphere/stratosphere/commit/a02f070305e768a7976ad4cb503f091113a4a1bc];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed CVS Parsers,FLINK-238,12719408,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:48,09/Jun/14 11:48,14/Jul/23 05:57,09/Jun/14 11:48,,,,pre-apache,,,,,,,0,github-import,,"This pull request fixes several issues with the CVS parsers:

- Handling of quoted strings
- Precision when parsing floating point values (([#6|https://github.com/stratosphere/stratosphere/issues/6] | [FLINK-6|https://issues.apache.org/jira/browse/FLINK-6]))

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/238
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Mon Nov 04 21:49:55 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;pull-request-238-8154434095076830162.patch;https://issues.apache.org/jira/secure/attachment/12649045/pull-request-238-8154434095076830162.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397607,,,Mon Jun 09 11:48:50 UTC 2014,,,,,,,,,,"0|i1wgcn:",397734,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;[Date: Wed Nov 06 18:37:31 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

Could this be the cause for ([#245|https://github.com/stratosphere/stratosphere/issues/245] | [FLINK-245|https://issues.apache.org/jira/browse/FLINK-245]) ? When the input does not match the fields specified for a RecordInputFormat not records are generated but there is no warning or exception or anything.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Downloading sources not possible from Eclipse,FLINK-237,12719407,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:48,09/Jun/14 11:48,14/Jul/23 05:57,09/Jun/14 11:48,,,,pre-apache,,,,,,,0,github-import,,"We generate source attachments, but eclipse is unable to download them.
I think I'm going to fix this.

https://github.com/stratosphere/stratosphere/pull/126

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/237
Created by: [rmetzger|https://github.com/rmetzger]
Labels: build system, user satisfaction, 
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Mon Nov 04 13:34:07 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397606,,,Mon Jun 09 11:48:45 UTC 2014,,,,,,,,,,"0|i1wgcf:",397733,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;[Date: Fri Dec 06 23:53:02 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Did work for me by now. Is this issue resolved?;;;","09/Jun/14 11:48;github-import;[Date: Sat Dec 07 00:12:39 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, I tried it on a fresh Ubuntu (I have a virtual machine for such tests). It does actually work.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Downloading Sources not possible from within Eclipse,FLINK-236,12719406,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:48,09/Jun/14 11:48,14/Jul/23 05:57,09/Jun/14 11:48,,,,pre-apache,,,,,,,0,github-import,,"We generate source attachments, but eclipse is unable to download them.
I think I'm going to fix this.

https://github.com/stratosphere/stratosphere/pull/126

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/236
Created by: [rmetzger|https://github.com/rmetzger]
Labels: build system, user satisfaction, 
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Mon Nov 04 13:26:53 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397605,,,Mon Jun 09 11:48:40 UTC 2014,,,,,,,,,,"0|i1wgc7:",397732,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;[Date: Fri Dec 06 23:53:26 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Duplicate of ([#237|https://github.com/stratosphere/stratosphere/issues/237] | [FLINK-237|https://issues.apache.org/jira/browse/FLINK-237]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stratosphere does not work out of the box on Mac OS X,FLINK-235,12719405,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:48,09/Jun/14 11:48,14/Jul/23 05:57,09/Jun/14 11:48,,,,pre-apache,,,,,,,0,github-import,,"The TaskManager fails to allocate memory.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/235
Created by: [rmetzger|https://github.com/rmetzger]
Labels: MacOSX, 
Created at: Mon Nov 04 10:49:32 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397604,,,Mon Jun 09 11:48:37 UTC 2014,,,,,,,,,,"0|i1wgbz:",397731,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;[Date: Wed Nov 06 17:01:23 CET 2013, Author: [uce|https://github.com/uce]]

Can you please be more specific? How can I ""reproduce""/see this?;;;","09/Jun/14 11:48;github-import;[Date: Wed Nov 06 18:05:13 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Sure,
I was in a hurry during the IMPRO lecture.

If you check out the BigDataClass exercise on MacOS X into eclipse and try
to run the main() method, it will fail with an OutOfMemory exception.

Could be that we can not do anything about this because you can probably
not change the run configuration in eclipse.



On Wed, Nov 6, 2013 at 5:01 PM, Ufuk Celebi <notifications@github.com>wrote:

> Can you please be more specific? How can I ""reproduce""/see this?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/235#issuecomment-27885777>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","09/Jun/14 11:48;github-import;[Date: Wed Nov 06 18:08:15 CET 2013, Author: [asteriosk|https://github.com/asteriosk]]

I had the same problem. But I think there is nothing that could could do about it (apart from mentioning it in the tutorial) as the VM heap size has to be set by the ""Run Configuration""...;;;","09/Jun/14 11:48;github-import;[Date: Wed Nov 06 18:41:02 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

There is a maven-eclipse plugin which can for example add folders to the source path. Someone should dig deeper into that to see if there is a way.;;;","09/Jun/14 11:48;github-import;[Date: Tue Nov 19 14:15:32 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

What is the status of this issue?

Is the problem that when starting examples out of eclipse, the default heap size is too small?  If yes, what is the size? Stratosphere should scale down with head sizes.

Or is there anything else?;;;","09/Jun/14 11:48;github-import;[Date: Fri Feb 07 10:38:08 CET 2014, Author: [fhueske|https://github.com/fhueske]]

@uce , @asteriosk : Any updates / comments on this issue?;;;","09/Jun/14 11:48;github-import;[Date: Fri Feb 07 10:39:48 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

The issue still persists.;;;","09/Jun/14 11:48;github-import;[Date: Fri Feb 07 12:02:24 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

What is the renaining problem?
Am 07.02.2014 10:39 schrieb ""Robert Metzger"" <notifications@github.com>:

> The issue still persists.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/235#issuecomment-34421440>
> .
>;;;","09/Jun/14 11:48;github-import;[Date: Fri Feb 07 12:08:44 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

The problem is that the default heapsize on Mac OS X is too small for Stratosphere to start successfully (with the LocalExecutor).
So if a new (OSX) user wants to run the WC example from eclipse, it will fail with a OOM Exception. We either have to reduce our memory footprint or put a clear notice onto all quickstart/documentations.

@JonathanH5: Can you add a notice box to http://stratosphere.eu/quickstart/java.html  above ""Build Project"" that says """"""
A note to Mac OS X users: The default JVM heapsize for Java is too small for Stratosphere. You have to manually increase it. Choose ""Run Configurations -> Arguments and write into the ""VM Arguments"" box: ""-Xmx800m"" in Eclipse.
"""""";;;","09/Jun/14 11:48;github-import;[Date: Fri Feb 07 12:11:25 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Hmmm... it runs for me with -Xmx100m on Linux. Does the Mac jvm eat vastly
more memory, or does it by default stay with a tiny amount?
Am 07.02.2014 12:08 schrieb ""Robert Metzger"" <notifications@github.com>:

> The problem is that the default heapsize on Mac OS X is too small for
> Stratosphere to start successfully (with the LocalExecutor).
> So if a new (OSX) user wants to run the WC example from eclipse, it will
> fail with a OOM Exception. We either have to reduce our memory footprint or
> put a clear notice onto all quickstart/documentations.
>
> @JonathanH5 <https://github.com/JonathanH5>: Can you add a notice box to
> http://stratosphere.eu/quickstart/java.html above ""Build Project"" that
> says """"""
> A note to Mac OS X users: The default JVM heapsize for Java is too small
> for Stratosphere. You have to manually increase it. Choose ""Run
> Configurations -> Arguments and write into the ""VM Arguments"" box:
> ""-Xmx800m"" in Eclipse.
> """"""
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/235#issuecomment-34427173>
> .
>;;;","09/Jun/14 11:48;github-import;[Date: Mon Apr 28 18:26:45 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in ([#465|https://github.com/stratosphere/stratosphere/issues/465] | [FLINK-465|https://issues.apache.org/jira/browse/FLINK-465]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude jruby-complete from HBase.,FLINK-234,12719404,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:48,09/Jun/14 11:48,14/Jul/23 05:57,09/Jun/14 11:48,,,,pre-apache,,,,,,,0,github-import,,"We don't need jruby-complete with hbase. It is used for the interactive HBase shell and causes troubles for Stratosphere users.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/234
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Nov 04 07:42:38 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;pull-request-234-6857790349965967784.patch;https://issues.apache.org/jira/secure/attachment/12649044/pull-request-234-6857790349965967784.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397603,,,Mon Jun 09 11:48:28 UTC 2014,,,,,,,,,,"0|i1wgbr:",397730,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;[Date: Wed Nov 06 11:40:58 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [c92f7900abc7ca6b38b4c562cacca26e6320ad4a|https://github.com/stratosphere/stratosphere/commit/c92f7900abc7ca6b38b4c562cacca26e6320ad4a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show cluster status info in JobManager Web Frontend (such as number of registered machines),FLINK-233,12719403,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:48,09/Jun/14 11:48,14/Jul/23 05:57,09/Jun/14 11:48,,,,pre-apache,,,,,,,0,github-import,,"It would be good to have additional information in the JobManager Info Frontend, such as the number of worker machines that are registered, etc. Saves the check in the log if all machines went up successfully...

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/233
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Sun Nov 03 15:47:37 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397602,,,Mon Jun 09 11:48:23 UTC 2014,,,,,,,,,,"0|i1wgbj:",397729,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;[Date: Sun Nov 03 15:56:46 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

+1 I suggested exactly the same.
There is code for that in my LocalDistributedExecutor
(JobManager.getNumberOfTaskTracker()) pull request (which is still not
ready :( )


On Sun, Nov 3, 2013 at 3:47 PM, Stephan Ewen <notifications@github.com>wrote:

> It would be good to have additional information in the JobManager Info
> Frontend, such as the number of worker machines that are registered, etc.
> Saves the check in the log if all machines went up successfully...
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/233>
> .
>;;;","09/Jun/14 11:48;github-import;[Date: Thu Feb 27 18:06:29 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

This is implemented for a while by now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Dump PACT plan as json, using the local executor",FLINK-232,12719402,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:48,09/Jun/14 11:48,14/Jul/23 05:57,09/Jun/14 11:48,,,,pre-apache,,,,,,,0,github-import,,"I needed this feature for generating graphs with graphviz

This python script converts a JSON Graph into a dot file: https://github.com/rmetzger/pact-plan-to-dot

Example: (I can't show it in a higher resolution, due to legal restrictions)
![plan|https://f.cloud.github.com/assets/89049/1458940/8d5352fa-43ae-11e3-8587-6c2b6316474c.png]


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/232
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Sat Nov 02 12:05:19 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;pull-request-232-2237238031065445741.patch;https://issues.apache.org/jira/secure/attachment/12649043/pull-request-232-2237238031065445741.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397601,,,Mon Jun 09 11:48:19 UTC 2014,,,,,,,,,,"0|i1wgbb:",397728,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;[Date: Sat Nov 02 19:23:35 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Converting the Json to Dot for plotting the plan DAG is a cool idea.

I think we might even push that further and use Dot for the web interface ([#185|https://github.com/stratosphere/stratosphere/issues/185] | [FLINK-185|https://issues.apache.org/jira/browse/FLINK-185]), ([#143|https://github.com/stratosphere/stratosphere/issues/143] | [FLINK-143|https://issues.apache.org/jira/browse/FLINK-143]).
I found a JavaScript based GraphViz (Dot) compiler. Canviz: http://code.google.com/p/canviz/
The online demo looks quite promising IMHO: http://www.ryandesign.com/canviz/

Canviz is published under MIT license though. Not sure about the legal aspects here, but from a first glimpse it might be OK.

What do you think?


;;;","09/Jun/14 11:48;github-import;[Date: Sat Nov 02 20:51:56 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Arbor.js https://github.com/samizdatco/arbor might be another candidate... Again MIT license.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show version and git revision on JobManager startup (in log),FLINK-231,12719401,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:48,09/Jun/14 11:48,14/Jul/23 05:57,09/Jun/14 11:48,,,,pre-apache,,,,,,,0,github-import,,"Hi, 

as a follow up of this discussion https://github.com/stratosphere/stratosphere/issues/208 I propose to include the version and git revision into the JobManager log file like this:
```
00:07:27,186 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Starting Stratosphere JobManager (Version: 0.4-SNAPSHOT, Rev:55bfa06)
00:07:27,195 INFO  eu.stratosphere.nephele.discovery.DiscoveryService            - Discovery service socket is bound to /127.0.0.1:7001
00:07:27,209 INFO  eu.stratosphere.nephele.ipc.Server                            - IPC Server Responder: starting
```

There will be also a file in the root directory of the build called `.version.properties` with the following contents:
```
#Generated by Git-Commit-Id-Plugin
#Sat Nov 02 00:06:43 CET 2013
git.commit.id.abbrev=55bfa06
git.commit.user.email=metzgerr@web.de
git.commit.message.full=Hotfixing version detection\nAdd renaming utility to master branch\n
git.commit.id=[55bfa06e8971957b3c2896855069b121c7c650df|https://github.com/stratosphere/stratosphere/commit/55bfa06e8971957b3c2896855069b121c7c650df]
git.commit.message.short=Hotfixing version detection Add renaming utility to master branch
git.commit.user.name=Robert Metzger
git.build.user.name=Robert Metzger
git.commit.id.describe=55bfa06
git.build.user.email=metzgerr@web.de
git.branch=version_information
git.commit.time=30.10.2013 @ 11\:38\:27 CET
git.build.time=02.11.2013 @ 00\:06\:43 CET
git.remote.origin.url=https\://github.com/stratosphere/stratosphere.git
```
If you want, we can also add the `.version.properties` file into each jar file we deploy. Currently, its only included into `nephele-server`.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/231
Created by: [rmetzger|https://github.com/rmetzger]
Labels: build system, 
Created at: Sat Nov 02 00:12:31 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;pull-request-231-3038023334726305489.patch;https://issues.apache.org/jira/secure/attachment/12649042/pull-request-231-3038023334726305489.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397600,,,Mon Jun 09 11:48:13 UTC 2014,,,,,,,,,,"0|i1wgb3:",397727,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;[Date: Sun Nov 03 14:46:55 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Quick question: Why do we need two version property files? One in the nephele-server jar and one in the distribution?;;;","09/Jun/14 11:48;github-import;[Date: Sun Nov 03 23:11:18 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I'm moving it to the main method.

The version file for the distribution is the one we certainly need.
The one in the nephele-server.jar file is probably not necessary if we know the path to the root-directory of the stratosphere installation at runtime. (Which should be the case if stratosphere has been started with our scripts)
The nephele-server.jar method is a bit more reliable because we read the file directly from the same jar as the JobManager itself (using the classloader).
There is currently no case in which users start stratosphere without our scripts. But I'm trying to change our Yarn implementation to be independent of the scripts, and then, we probably need to read it from the jar. (Currently, Yarn uses the nephele-jobmanager.sh script to start the JM. So the script and everything must be available on each machine (e.g. HDFS). I hope that there is way with Yarn to start an application from a jar-file. I have not really looked into the issue, so might be totally different in the end);;;","09/Jun/14 11:48;github-import;[Date: Tue Nov 05 18:41:41 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I moved the method into main(). 
I'm also in contact with travis to analyze the build issues we're experiencing right now.;;;","09/Jun/14 11:48;github-import;[Date: Wed Nov 06 18:36:00 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

The f*ck, this makes my eclipse complain about some plugin that it cannot find when I try to import. Average Joe is not gonna like this. ;;;","09/Jun/14 11:48;github-import;[Date: Wed Nov 06 18:43:14 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, I have to admit its my fault. I did not test this with eclipse.
Is there a way to ignore plugins in eclipse out of maven (profiles, etc.)

Seems like there is a solution to the problem: https://github.com/ktoso/maven-git-commit-id-plugin/issues/22;;;","09/Jun/14 11:48;github-import;[Date: Wed Nov 06 18:48:58 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

You can add this in the pom on the same level as <plugins> and it will ignore it in eclipse:
```xml
<pluginManagement>
    <plugins>
        <!--This plugin's configuration is used to store Eclipse m2e settings 
            only. It has no influence on the Maven build itself. -->
        <plugin>
            <groupId>org.eclipse.m2e</groupId>
            <artifactId>lifecycle-mapping</artifactId>
            <version>1.0.0</version>
            <configuration>
                <lifecycleMappingMetadata>
                    <pluginExecutions>
                        <pluginExecution>
                            <pluginExecutionFilter>
                                <groupId>
                                    pl.project13.maven
                                </groupId>
                                <artifactId>
                                    git-commit-id-plugin
                                </artifactId>
                                <versionRange>
                                    [2.1.5,)
                                </versionRange>
                                <goals>
                                    <goal>revision</goal>
                                </goals>
                            </pluginExecutionFilter>
                            <action>
                                <ignore></ignore>
                            </action>
                        </pluginExecution>
                    </pluginExecutions>
                </lifecycleMappingMetadata>
            </configuration>
        </plugin>
    </plugins>
</pluginManagement>
```;;;","09/Jun/14 11:48;github-import;[Date: Wed Nov 06 20:08:18 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you prepare a merge request for that?;;;","09/Jun/14 11:48;github-import;[Date: Thu Nov 07 09:13:04 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Fixed in https://github.com/stratosphere/stratosphere/commit/24f658177eb2f3272dfe0e9791b774bd86d52861;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Job Cancellation does not work properly: ""Cannot find execution graph to job ID""",FLINK-230,12719400,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,github-import,github-import,09/Jun/14 11:48,21/Sep/14 02:20,14/Jul/23 05:57,21/Sep/14 02:20,,,,0.7.0-incubating,,,,,,,0,github-import,,"Hi,

I noticed this error message on a failing Job.
```
12:37:10,697 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from CANCELING to CANCELED for task Invoices file (7/8)
12:37:10,697 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from CANCELING to CANCELED for task ([#2|https://github.com/stratosphere/stratosphere/issues/2] | [FLINK-2|https://issues.apache.org/jira/browse/FLINK-2]) filter invoices: month <= 12 (8/8)
12:37:10,697 INFO  eu.stratosphere.nephele.jobmanager.scheduler.AbstractScheduler  - Releasing instance hadoop02
12:37:10,699 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Status of job XX 0b7407b5ad73a40043c36c16baacf400) changed to FAILED
12:37:10,706 ERROR eu.stratosphere.nephele.jobmanager.JobManager                 - Cannot find execution graph to job ID 0b7407b5ad73a40043c36c16baacf400
12:37:10,706 ERROR eu.stratosphere.nephele.jobmanager.JobManager                 - Cannot find execution graph to job ID 0b7407b5ad73a40043c36c16baacf400
12:37:10,709 ERROR eu.stratosphere.nephele.jobmanager.JobManager                 - Cannot find execution graph to job ID 0b7407b5ad73a40043c36c16baacf400
```
The errors occurs quite often:
```
rmetzger@hadoop01:~/log$ cat nephele-rmetzger-jobmanager-hadoop01.log | grep ""Cannot find""  | wc -l
21262
```

The TaskManager also reports errors:

```
12:37:14,951 ERROR eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager  - Cannot find task(s) waiting for data from source channel with ID 43930c029c759c003792e4dfd4411800
12:37:14,952 ERROR eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager  - Cannot find task(s) waiting for data from source channel with ID 0937e32b635954000efb7f68c0c80c00
12:37:14,953 ERROR eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager  - Cannot find task(s) waiting for data from source channel with ID 6922a9feb031540009dbe583b3fbe800
```

```
rmetzger@hadoop01:~/log$ cat nephele-rmetzger-taskmanager-hadoop01.log | grep ""for data from source channel"" | wc -l 6612
```

I also saw this
```
12:00:00,221 ERROR eu.stratosphere.nephele.execution.ExecutionStateTransition    - java.lang.IllegalStateException: Unexpected state change: CANCELING -> FAILED
        at eu.stratosphere.nephele.execution.ExecutionStateTransition.checkTransition(ExecutionStateTransition.java:167)
        at eu.stratosphere.nephele.executiongraph.ExecutionVertex.updateExecutionState(ExecutionVertex.java:384)
        at eu.stratosphere.nephele.executiongraph.ExecutionVertex$1.run(ExecutionVertex.java:319)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
```


I did not see this behavior before, so it could be new (I did not do any major changes on the job)



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/230
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, runtime, 
Created at: Fri Nov 01 15:02:46 CET 2013
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397599,,,Sun Sep 21 02:20:25 UTC 2014,,,,,,,,,,"0|i1wgav:",397726,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;[Date: Fri Nov 01 18:15:21 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

This is part of a faulty logic in the ExecutionGraph state machine, as
reported in issue ([#15|https://github.com/stratosphere/stratosphere/issues/15] | [FLINK-15|https://issues.apache.org/jira/browse/FLINK-15])

The state machine is not able to handle asynchronous events properly, as
they occur in the presence of failures and canceling.


On Fri, Nov 1, 2013 at 3:02 PM, Robert Metzger <notifications@github.com>wrote:

> Hi,
>
> I noticed this error message on a failing Job.
>
> 12:37:10,697 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from CANCELING to CANCELED for task Invoices file (7/8)
> 12:37:10,697 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - JM: ExecutionState set from CANCELING to CANCELED for task ([#2|https://github.com/stratosphere/stratosphere/issues/2] | [FLINK-2|https://issues.apache.org/jira/browse/FLINK-2]) filter invoices: month <= 12 (8/8)
> 12:37:10,697 INFO  eu.stratosphere.nephele.jobmanager.scheduler.AbstractScheduler  - Releasing instance hadoop02
> 12:37:10,699 INFO  eu.stratosphere.nephele.jobmanager.JobManager                 - Status of job XX 0b7407b5ad73a40043c36c16baacf400) changed to FAILED
> 12:37:10,706 ERROR eu.stratosphere.nephele.jobmanager.JobManager                 - Cannot find execution graph to job ID 0b7407b5ad73a40043c36c16baacf400
> 12:37:10,706 ERROR eu.stratosphere.nephele.jobmanager.JobManager                 - Cannot find execution graph to job ID 0b7407b5ad73a40043c36c16baacf400
> 12:37:10,709 ERROR eu.stratosphere.nephele.jobmanager.JobManager                 - Cannot find execution graph to job ID 0b7407b5ad73a40043c36c16baacf400
>
> The errors occurs quite often:
>
> rmetzger@hadoop01:~/log$ cat nephele-rmetzger-jobmanager-hadoop01.log | grep ""Cannot find""  | wc -l
> 21262
>
> The TaskManager also reports errors:
>
> 12:37:14,951 ERROR eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager  - Cannot find task(s) waiting for data from source channel with ID 43930c029c759c003792e4dfd4411800
> 12:37:14,952 ERROR eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager  - Cannot find task(s) waiting for data from source channel with ID 0937e32b635954000efb7f68c0c80c00
> 12:37:14,953 ERROR eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager  - Cannot find task(s) waiting for data from source channel with ID 6922a9feb031540009dbe583b3fbe800
>
> rmetzger@hadoop01:~/log$ cat nephele-rmetzger-taskmanager-hadoop01.log | grep ""for data from source channel"" | wc -l 6612
>
> I did not see this behavior before, so it could be new (I did not do any
> major changes on the job)
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/230>
> .
>;;;","21/Sep/14 02:20;sewen;Fixed in ae139f5ae2199a52e8d7f561f94db51631107d00;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resolved issue with hardcoded ""file://"" URIs under windows",FLINK-229,12719399,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:47,09/Jun/14 11:48,14/Jul/23 05:57,09/Jun/14 11:48,,,,pre-apache,,,,,,,0,github-import,,"I implemented the proposed solution for Issue ([#217|https://github.com/stratosphere/stratosphere/issues/217] | [FLINK-217|https://issues.apache.org/jira/browse/FLINK-217]). I exchanged every hardcoded file:// path to a platform independent expression.
I tested it with mvn verify on Windows and on my Ubuntu VM in local mode. More tests on different Linux systems and on a cluster would probably be nice.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/229
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Thu Oct 31 21:26:10 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:47;github-import;pull-request-229-3392639077513186188.patch;https://issues.apache.org/jira/secure/attachment/12649041/pull-request-229-3392639077513186188.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397598,,,Mon Jun 09 11:48:01 UTC 2014,,,,,,,,,,"0|i1wgan:",397725,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:48;github-import;[Date: Fri Nov 01 08:55:07 CET 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

It's not a bug that this code breaks on windows, its a feature :D;;;","09/Jun/14 11:48;github-import;[Date: Fri Nov 08 15:27:00 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Sorry for all the critical comments, I'm currently working on `TestBase` so I see some things.

This line won't produce a correct ""file://"" path: https://github.com/markus-h/stratosphere/blob/Windows_Testbase2-pr/pact/pact-tests/src/test/java/eu/stratosphere/pact/test/util/filesystem/LocalFSProvider.java#L105;;;","09/Jun/14 11:48;github-import;[Date: Mon Nov 11 16:57:01 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [385c0be988eef4fe93602e0d13e526c22179c37e|https://github.com/stratosphere/stratosphere/commit/385c0be988eef4fe93602e0d13e526c22179c37e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Additional CrossWithSmall and CrossWithLarge operations,FLINK-227,12719397,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:47,09/Jun/14 11:47,14/Jul/23 05:57,09/Jun/14 11:47,,,,pre-apache,,,,,,,0,github-import,,"Fixes ([#218|https://github.com/stratosphere/stratosphere/issues/218] | [FLINK-218|https://issues.apache.org/jira/browse/FLINK-218])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/227
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Wed Oct 30 14:45:33 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:47;github-import;pull-request-227-7211841283198049477.patch;https://issues.apache.org/jira/secure/attachment/12649040/pull-request-227-7211841283198049477.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397596,,,Mon Jun 09 11:47:52 UTC 2014,,,,,,,,,,"0|i1wga7:",397723,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:47;github-import;[Date: Sun Nov 03 15:38:26 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [4285102eb5cd9024c07a841c66240f8ade310d50|https://github.com/stratosphere/stratosphere/commit/4285102eb5cd9024c07a841c66240f8ade310d50];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cleanup of main pom; generate javadocs",FLINK-226,12719396,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:47,09/Jun/14 11:47,14/Jul/23 05:57,09/Jun/14 11:47,,,,pre-apache,,,,,,,0,github-import,,"Step 1 for https://github.com/stratosphere/stratosphere/issues/225



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/226
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Wed Oct 30 12:45:20 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:47;github-import;pull-request-226-2929007888620322762.patch;https://issues.apache.org/jira/secure/attachment/12649039/pull-request-226-2929007888620322762.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397595,,,Mon Jun 09 11:47:47 UTC 2014,,,,,,,,,,"0|i1wg9z:",397722,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:47;github-import;[Date: Wed Oct 30 15:55:01 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

+1;;;","09/Jun/14 11:47;github-import;[Date: Sun Nov 03 14:17:06 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I agree that a test coverage check is good. But when we reconfigure this, we would probably move to a newer version anyways and check the configuration.

I would vote to remove the old garbage from the pom and setup test coverage checks properly when we will re-introduce them.;;;","09/Jun/14 11:47;github-import;[Date: Mon Nov 04 09:34:21 CET 2013, Author: [fhueske|https://github.com/fhueske]]

fair enough;;;","09/Jun/14 11:47;github-import;[Date: Wed Nov 06 11:41:15 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [a8228ceb96bfba2b4b9d63ace8c77077a0689a7b|https://github.com/stratosphere/stratosphere/commit/a8228ceb96bfba2b4b9d63ace8c77077a0689a7b];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reenabled range partitioning with manually specified data distributions.,FLINK-224,12719394,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:47,09/Jun/14 11:47,14/Jul/23 05:57,09/Jun/14 11:47,,,,pre-apache,,,,,,,0,github-import,,"- Extended DataDistribution with generics for data model independence
- Added range partitioning to output emitter using TypeComparator and DataDistribution
- Adapted and extended output emitter tests
- Added PactRecordDataDistribution and corresponding tests
- Reactivated pact tests with range distribution.
- Reincluded TeraSort Pact Java example into example jobs.
- minor fixes (on-the-way-bug, unused imports, typos)

Adresses issues ([#7|https://github.com/stratosphere/stratosphere/issues/7] | [FLINK-7|https://issues.apache.org/jira/browse/FLINK-7]), ([#138|https://github.com/stratosphere/stratosphere/issues/138] | [FLINK-138|https://issues.apache.org/jira/browse/FLINK-138]), ([#146|https://github.com/stratosphere/stratosphere/issues/146] | [FLINK-146|https://issues.apache.org/jira/browse/FLINK-146])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/224
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Wed Oct 30 09:13:09 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:47;github-import;pull-request-224-5306637250997024018.patch;https://issues.apache.org/jira/secure/attachment/12649038/pull-request-224-5306637250997024018.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397593,,,Mon Jun 09 11:47:38 UTC 2014,,,,,,,,,,"0|i1wg9j:",397720,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:47;github-import;[Date: Wed Oct 30 10:25:43 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I am not happy yet with the PactRecordDataDistribution. Will do another pass there.

Please, have a look at the changes anyways.;;;","09/Jun/14 11:47;github-import;[Date: Wed Oct 30 17:32:49 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I think this PR is ready to merge once it passed Travis.
@StephanEwen Please check and merge if you agree.;;;","09/Jun/14 11:47;github-import;[Date: Thu Dec 12 19:51:24 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [7922332797406d982a19288f22ea55fcf746f8e5|https://github.com/stratosphere/stratosphere/commit/7922332797406d982a19288f22ea55fcf746f8e5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
change deploy tool to detect current stratosphere version,FLINK-223,12719393,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:47,09/Jun/14 11:47,14/Jul/23 05:57,09/Jun/14 11:47,,,,pre-apache,,,,,,,0,github-import,," and dynamically generate the yarn build.

This should be merged before https://github.com/stratosphere/stratosphere/pull/219

This is how the tools builds the versions:
```
robert@robert-tower ~/Projekte/ozone/ozone (git)-[fix_deploy_tool] % ./tools/deploy_to_maven.sh
detected current version as: 0.4-SNAPSHOT ; yarn: 0.4-hadoop2-SNAPSHOT 
robert@robert-tower ~/Projekte/ozone/ozone (git)-[fix_deploy_tool] % ./tools/deploy_to_maven.sh
detected current version as: 0.4-alpta.2 ; yarn: 0.4-alpta.2-hadoop2 
```


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/223
Created by: [rmetzger|https://github.com/rmetzger]
Labels: build system, 
Created at: Tue Oct 29 21:28:22 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:47;github-import;pull-request-223-8943011544264009056.patch;https://issues.apache.org/jira/secure/attachment/12649037/pull-request-223-8943011544264009056.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397592,,,2014-06-09 11:47:27.0,,,,,,,,,,"0|i1wg9b:",397719,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix for Issue #93: slaves file requires blank line at end (otherwise las...,FLINK-221,12719391,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:47,09/Jun/14 11:47,14/Jul/23 05:57,09/Jun/14 11:47,,,,pre-apache,,,,,,,0,github-import,,"I added the proposed workaround for issue ([#93|https://github.com/stratosphere/stratosphere/issues/93] | [FLINK-93|https://issues.apache.org/jira/browse/FLINK-93]). Moreover I added a check for empty lines in the slave file to NetworkTopology.java to prevent it from throwing an exception.



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/221
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Created at: Tue Oct 29 10:34:34 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:47;github-import;pull-request-221-9055729811204635972.patch;https://issues.apache.org/jira/secure/attachment/12649036/pull-request-221-9055729811204635972.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397590,,,Mon Jun 09 11:47:16 UTC 2014,,,,,,,,,,"0|i1wg8v:",397717,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:47;github-import;[Date: Tue Oct 29 12:48:33 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Incorporated @rmetzger comment, tested it on a small 4-node setup. works.

Merged in [bb5d7708d8ddd7791d61b7a3f145cdbfbefa6fc3|https://github.com/stratosphere/stratosphere/commit/bb5d7708d8ddd7791d61b7a3f145cdbfbefa6fc3];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pre-release test,FLINK-219,12719389,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:46,09/Jun/14 11:46,14/Jul/23 05:57,09/Jun/14 11:46,,,,pre-apache,,,,,,,0,github-import,,"this is not an actual release
it's just a test how releases can be realized technically

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/219
Created by: [physikerwelt|https://github.com/physikerwelt]
Labels: build system, 
Created at: Tue Oct 29 01:24:36 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:46;github-import;pull-request-219-6746453463692122720.patch;https://issues.apache.org/jira/secure/attachment/12649035/pull-request-219-6746453463692122720.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397588,,,Mon Jun 09 11:46:57 UTC 2014,,,,,,,,,,"0|i1wg8f:",397715,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:46;github-import;[Date: Tue Oct 29 08:14:01 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

Caution: Do not merge to master.;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 08:29:58 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Can you move the change version script into `/tools` ?

@StephanEwen: What I told you yesterday with automatic sonatype deployment is only partially true.
It will deploy the ""normal"" Stratosphere with Hadoop 1.2.1 as a dependency. But the script that prepares the Hadoop Yarn build will fail. The first line of the script `deploy_to_maven.sh` contains two variables with the version numbers. 
I think it is an easy fix to extract the version number from the main pom, for the second variable, I would just append `-hadoop02` 
Note: the hadoop yarn version must contain the string `-hadoop02`.;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 18:54:12 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

@rmetzger @StephanEwen do I have to do something for this?;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 21:13:06 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I'm going to do it. Its only 5 mins for me;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 21:30:02 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, 16 Minutes.
@StephanEwen can you wait until https://github.com/stratosphere/stratosphere/pull/223 has been successfully build on Travis. Make sure that it correctly detects the version of Stratosphere (should be one of the last lines in the travis log). If this is the case, merge https://github.com/stratosphere/stratosphere/pull/223 into the master, rebase this pull request (so that the new script is also applied to the branch of this version. Then you can push this PR into a new branch.;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 21:41:43 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

local or jenkins testing takes 3 minutes ... compared to that Travis is somehow slow... I asked at the travis channel at freenode... a local travis installation is not recommended. I think we should provide some best practice for developers how to deal with the slow build time;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 21:43:27 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

It is impossible to build and verify in 3 minutes. My Core i7 (3.6 ghz or so on one core) machine here with 16 gb main memory needs around 15 mins (I will test that now to see the exact time).

Remember also that Travis has to download all dependencies from maven.
We use travis because we don't want to spend more than 15 seconds per pull request and push with building and verification. If we would run our own server, we would almost certainly spend much more time with the setup. 
Our core competency is developing software, not building good infrastructure.;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 21:57:51 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

ok. I was running package only (as stated in the readme)
dopa-vm/stratosphere-dev$ mvn package

[INFO] Reactor Summary:
[INFO]
[INFO] stratosphere ...................................... SUCCESS [0.512s]
[INFO] nephele ........................................... SUCCESS [0.006s]
[INFO] nephele-common .................................... SUCCESS [5.059s]
[INFO] nephele-management ................................ SUCCESS [0.284s]
[INFO] nephele-server .................................... SUCCESS [1.927s]
[INFO] nephele-profiling ................................. SUCCESS [1.583s]
[INFO] nephele-queuescheduler ............................ SUCCESS [0.261s]
[INFO] nephele-clustermanager ............................ SUCCESS [6.383s]
[INFO] nephele-hdfs ...................................... SUCCESS [0.078s]
[INFO] nephele-s3 ........................................ SUCCESS [0.347s]
[INFO] nephele-visualization ............................. SUCCESS [0.074s]
[INFO] nephele-examples .................................. SUCCESS [0.176s]
[INFO] pact .............................................. SUCCESS [0.010s]
[INFO] pact-common ....................................... SUCCESS [2.674s]
[INFO] pact-array-datamodel .............................. SUCCESS [0.033s]
[INFO] pact-runtime ...................................... SUCCESS [1:15.631s]
[INFO] pact-compiler ..................................... SUCCESS [0.613s]
[INFO] pact-clients ...................................... SUCCESS [1.095s]
[INFO] pact-examples ..................................... SUCCESS [0.444s]
[INFO] pact-scala ........................................ SUCCESS [0.208s]
[INFO] pact-scala-core ................................... SUCCESS [0.641s]
[INFO] pact-scala-examples ............................... SUCCESS [0.858s]
[INFO] pact-tests ........................................ SUCCESS [10.873s]
[INFO] stratosphere-dist ................................. SUCCESS [4.735s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:54.875s
[INFO] Finished at: Tue Oct 29 21:55:22 CET 2013
[INFO] Final Memory: 27M/236M
... I'll try clean verify as comparison;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 22:02:59 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

18 Minutes. So travis is actually quite okay, given that they have to download more than 80 MB first, and some workers also upload the artifacts to different servers!

```
[INFO] Reactor Summary:
[INFO] 
[INFO] stratosphere ...................................... SUCCESS [0.841s]
[INFO] nephele ........................................... SUCCESS [0.020s]
[INFO] nephele-common .................................... SUCCESS [8.864s]
[INFO] nephele-management ................................ SUCCESS [0.738s]
[INFO] nephele-server .................................... SUCCESS [34.086s]
[INFO] nephele-profiling ................................. SUCCESS [2.280s]
[INFO] nephele-queuescheduler ............................ SUCCESS [0.512s]
[INFO] nephele-clustermanager ............................ SUCCESS [6.646s]
[INFO] nephele-hdfs ...................................... SUCCESS [0.302s]
[INFO] nephele-s3 ........................................ SUCCESS [4.660s]
[INFO] nephele-visualization ............................. SUCCESS [0.435s]
[INFO] nephele-examples .................................. SUCCESS [0.296s]
[INFO] pact .............................................. SUCCESS [0.016s]
[INFO] pact-common ....................................... SUCCESS [10.776s]
[INFO] pact-array-datamodel .............................. SUCCESS [0.100s]
[INFO] pact-runtime ...................................... SUCCESS [4:43.413s]
[INFO] pact-compiler ..................................... SUCCESS [1.396s]
[INFO] pact-clients ...................................... SUCCESS [1.564s]
[INFO] pact-examples ..................................... SUCCESS [0.495s]
[INFO] pact-scala ........................................ SUCCESS [0.350s]
[INFO] pact-scala-core ................................... SUCCESS [42.165s]
[INFO] pact-scala-examples ............................... SUCCESS [34.659s]
[INFO] pact-tests ........................................ SUCCESS [10:40.035s]
[INFO] stratosphere-dist ................................. SUCCESS [9.582s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 18:04.589s
[INFO] Finished at: Tue Oct 29 22:01:43 CET 2013
[INFO] Final Memory: 42M/230M
[INFO] ------------------------------------------------------------------------
mvn clean verify  1039.18s user 125.37s system 107% cpu 18:05.89 total
```
;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 23:45:10 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

18 minutes seems quite a lot. considering an average of 3 attempts (maybe others need less attempts) leads to 40 minutes of waiting time before submitting something. This does not motivate people to commit atomic changes.
I played with maven configurations like `<parallel>classes</parallel>` and  `<useUnlimitedThreads>true</useUnlimitedThreads>` but it seems that are a lot of dependencies between the tests... If there is no best practice, I'll create an issue for that.
;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 00:25:00 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

You should use eclipse to see errors immediately.
I'm not sure if running tests in parallel does help us because it could actually introduce more errors (its a distributed system that starts servers on ports etc. you can't do that in parallel)

I'm against running the tests in parallel.;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 08:39:45 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

There are some IDEs like eclipse that help you to verify your code. However, for code validation they help little.
I think for the average JAVA programmer it's unlikely to write a program that does what he want's without testing it. (Even if a IDE like eclipse is used.)
Unit tests aim to design the smallest testable unit of the a software. I claim that one can start servers in parallel:-)
I recommend to run the tests locally, before submitting pull requests. (I used package rather than verify before.) However, running tests that take 18 Minutes is clearly not acceptable for local testing.
Not running all tests locally, makes it harder to determine why tests have failed.
E.g. Travis could not download jars from Sonatype on monday (HTTP 502). Restarting the build solved the problem.;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 09:01:05 CET 2013, Author: [fhueske|https://github.com/fhueske]]

@physikerwelt 
I agree that 18 mins build time on current hardware (and 30 mins on my machine) are quite a lot and that it would be nice to reduce that.
However, Stratosphere is a complex system which requires extensive testing. It was a significant effort to get all these tests into place and I think that we rather need to extend testing (esp. for the optimizer and distributed settings) than stripping it down. 

One thing that will not happen in any case is to reduce test coverage! However, reducing tests while keeping the coverage at the same level, requires a lot of effort and more time than we have right row. We have more pressing issues and I do not see this as an urgent or even important problem that needs to be addressed at the moment.

Although, if you find an easy way to reduce build and test time for example by running tests in parallel, go for it and many people including myself will appreciate it a lot.

 ;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:07:43 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

You merged the pull request into the wrong branch

And you forgot to apply my pull request first;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:10:37 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I did not mean to merge. Not sure what happened. I was trying to reply. Fat
fingers?
Am 30.10.2013 11:07 schrieb ""Robert Metzger"" <notifications@github.com>:

> You merged the pull request into the wrong branch
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/219#issuecomment-27377189>
> .
>;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:11:10 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you undo?
Am 30.10.2013 11:10 schrieb ""Stephan Ewen"" <ewenstephan@gmail.com>:

> I did not mean to merge. Not sure what happened. I was trying to reply.
> Fat fingers?
> Am 30.10.2013 11:07 schrieb ""Robert Metzger"" <notifications@github.com>:
>
>> You merged the pull request into the wrong branch
>>
>> —
>> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/pull/219#issuecomment-27377189>
>> .
>>
>;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:12:08 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I'm not sure if undoing history is a good idea here?
I think another commit reverting the change is the cleaner solution.;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:12:46 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

https://github.com/blog/1642-merge-pull-requests-from-your-phone;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:14:13 CET 2013, Author: [fhueske|https://github.com/fhueske]]

If you're fast enough, nobody will notice... Hurry up! :-D 
*justJoking*;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:17:16 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

No joking. I nuked our history!;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:17:32 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

@rmetzger yes please undo... the pull request was not designed to be merged.. can you create a new branch so that I can make a propper pull request;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:18:07 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Yes, I'm going to push it correctly now.;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:18:22 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

thx;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:27:08 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay. History has been changed.
I merged https://github.com/stratosphere/stratosphere/pull/223 (no rebase, stupid webinterface merge)
Created new branch with release version. https://github.com/stratosphere/stratosphere/tree/release-0.4-alpha.0

Now, we just have to wait for Travis.;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:34:52 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

The drama is not yet over. I'm having a minor issue with the deploy tool. Should be fixed by now;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:39:23 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I will backport the hotfix to master once I verified it. I will also add the renaming tool to the master.;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 11:43:11 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Thanks, Robert, for fixing this!;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 12:20:18 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

""mvn deploy"" pushed the artifact into the release staging area of Maven Central.
I'll look up if there is an easy way to release to maven central (please interrupt me, if this is not intended) ;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 12:25:54 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

I had a discussion about that with Kostas in June. Please ask him before submitting to maven central.;;;","09/Jun/14 11:46;github-import;[Date: Wed Oct 30 14:21:39 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

I appended `-SNAPSHOT`  in the release branch to deploy it to the snapshots repo.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestBase2 is not platform independent (WINDOWS),FLINK-217,12719387,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:46,09/Jun/14 11:46,14/Jul/23 05:57,09/Jun/14 11:46,,,,pre-apache,,,,,,,0,github-import,,"A student from IMPRO-3 who is using Windows 7 reported a bug with Testbase2.

The methods ""getTemp*Path()"" have the prefix ""file://"" hardcoded.


I assigned @markus-h to this issue, since he is a Windows user.
I think there is a way in java to find the platform's way to separate directories.

use ""mvn verify"" to trigger the bug.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/217
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, windows, 
Milestone: Release 0.4
Assignee: [markus-h|https://github.com/markus-h]
Created at: Mon Oct 28 17:44:42 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397586,,,Mon Jun 09 11:46:29 UTC 2014,,,,,,,,,,"0|i1wg7z:",397713,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:46;github-import;[Date: Tue Oct 29 12:06:30 CET 2013, Author: [markus-h|https://github.com/markus-h]]

I started working on the issue. Looks like not only file paths need to be adjusted in the testings but also most exec commands.
Moreover I tried to use the shell scripts to start the server (eg start-local.sh), but they seem not to work under cygwin for me. Would perhaps be a nice enhancement?;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 12:09:57 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

We had once a PR that claimed to make Stratosphere Cygwin compatible.

Where do we have exec commands? Do you mean the scripts in /bin ?;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 12:10:29 CET 2013, Author: [uce|https://github.com/uce]]

What is the problem with the scripts?

I remember @aljoscha adding a function to change the filepaths on cygwin installations. The changes we introduced afterwards only affected the call to `uname` to figure out on what kind of system we are running. Strange.

;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 12:27:17 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I think the only problem we are facing here is that the file URI prefix is hard coded.

It should do to replace 
```
file:// + f.getAbsolutePath()
```
with
```
f.toURI().toString()
```

Se we need to externalize the path prefix which contains the scheme;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 12:29:54 CET 2013, Author: [uce|https://github.com/uce]]

Yes, sure. @rmetzger checked it yesterday on a Windows machine and indeed `file:\\` does the trick.

But @markus-h's problem with the scripts sounds independent of this to me. Afaik there are no hardcoded paths in the scripts.;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 19:06:25 CET 2013, Author: [markus-h|https://github.com/markus-h]]

The issue for the hard coded URI prefixes is quite easy to resolve, like StephanEwen mentioned. 

But there are multiple exec calls out of Java in the testcases (Runtime.getRuntime().exec(cmd)). For example in ExternalProcessFixedLengthInputFormatTest, ExternalProcessInputFormatTest and even in the ExternalProcessInputFormat class. These seem to be a little bit harder to resolve in windows.;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 19:53:38 CET 2013, Author: [fhueske|https://github.com/fhueske]]

As the name suggests, an ExternalProcessInputFormat starts an external process and parses its output into records.
The process is of course not OS independent. Not sure if the tests can be adapted in a way that the call to Runtime.exec() call or Process that represents the external process is mocked.

Is there a way in Maven to provide platform dependent tests or disable certain tests for different platforms? ;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 20:01:33 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I think it would be file to do a OS detection in the test. If the OS is
windows, the tests are skipped (the test method just returns directly). We
still get the test coverage on the Linux systems, which is where we build
anyways, and I would not want to invest too much time in having this very
specific test platform independent.


On Tue, Oct 29, 2013 at 7:53 PM, Fabian Hueske <notifications@github.com>wrote:

> As the name suggests, an ExternalProcessInputFormat starts an external
> process and parses its output into records.
> The process is of course not OS independent. Not sure if the tests can be
> adapted in a way that the call to Runtime.exec() call or Process that
> represents the external process is mocked.
>
> Is there a way in Maven to provide platform dependent tests or disable
> certain tests for different platforms?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/217#issuecomment-27332188>
> .
>;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 21:05:35 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I agree, sounds like a plan.;;;","09/Jun/14 11:46;github-import;[Date: Tue Oct 29 21:09:14 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

+1;;;","09/Jun/14 11:46;github-import;[Date: Thu Oct 31 09:03:26 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

@StephanEwen: `f.toURI().toString()` will result in paths like `file:/tmp/input` (if `f instanceof java.io.File`). For some reason I don't understand, we usually use `file:///tmp/input` for local paths. 
I think it will work with `file:/tmp/input` too, because `eu.stratosphere.nephele.fs.Path.toUri().getScheme()` is able to extract ""file"" as the scheme.

I know the URI's for HDFS must contain `hdfs:///` to specify the Namenode host.;;;","09/Jun/14 11:46;github-import;[Date: Tue Nov 12 12:06:43 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Solved by ([#299|https://github.com/stratosphere/stratosphere/issues/299] | [FLINK-299|https://issues.apache.org/jira/browse/FLINK-299]) in [385c0be988eef4fe93602e0d13e526c22179c37e|https://github.com/stratosphere/stratosphere/commit/385c0be988eef4fe93602e0d13e526c22179c37e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automatic hardware extraction unstable,FLINK-216,12719386,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:46,09/Jun/14 11:46,14/Jul/23 05:57,09/Jun/14 11:46,,,,pre-apache,,,,,,,0,github-import,,"Occasionally, the hardware inference of Nephele does not work as expected. As can be seen in the following excerpt, sometimes too few free memory is assumed and thus the memory manager fails to initialize.
This error cannot be reproduced by running a single test, so that I suspect that prior tests fills the memory with short-lived objects, but not full enough for the garbage collector to run. This is indicated by the first line in the log (max is actually the difference). In general, free memory in Java seems to be a bad indicator on how much memory is actually available.

<pre>
13/10/24 16:14:14 INFO instance.HardwareDescriptionFactory: Found Tenured Gen pool (max: 16576, used: 357941056)
13/10/24 16:14:14 INFO taskmanager.TaskManager: Initializing memory manager with 0 megabytes of memory
13/10/24 16:14:14 FATAL taskmanager.TaskManager: Unable to initialize memory manager with 0 megabytes of memory
java.lang.IllegalArgumentException: The given amount of memory amounted to less than one page.
	at eu.stratosphere.nephele.services.memorymanager.spi.DefaultMemoryManager.<init>(DefaultMemoryManager.java:133)
	at eu.stratosphere.nephele.services.memorymanager.spi.DefaultMemoryManager.<init>(DefaultMemoryManager.java:100)
	at eu.stratosphere.nephele.taskmanager.TaskManager.<init>(TaskManager.java:289)
	at eu.stratosphere.nephele.instance.local.LocalTaskManagerThread.<init>(LocalTaskManagerThread.java:40)
	at eu.stratosphere.nephele.instance.local.LocalInstanceManager.<init>(LocalInstanceManager.java:143)
	at eu.stratosphere.nephele.jobmanager.JobManager.<init>(JobManager.java:226)
	at eu.stratosphere.pact.client.minicluster.NepheleMiniCluster.start(NepheleMiniCluster.java:164)
	at eu.stratosphere.sopremo.server.SopremoTestServer.<init>(SopremoTestServer.java:75)
	at eu.stratosphere.sopremo.server.SopremoServerIT.setup(SopremoServerIT.java:58)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:120)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:103)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
13/10/24 16:14:14 FATAL jobmanager.JobManager: Cannot instantiate local instance manager: java.lang.RuntimeException: java.lang.IllegalArgumentException: The given amount of memory amounted to less than one page.
	at eu.stratosphere.nephele.instance.local.LocalTaskManagerThread.<init>(LocalTaskManagerThread.java:42)
	at eu.stratosphere.nephele.instance.local.LocalInstanceManager.<init>(LocalInstanceManager.java:143)
	at eu.stratosphere.nephele.jobmanager.JobManager.<init>(JobManager.java:226)
	at eu.stratosphere.pact.client.minicluster.NepheleMiniCluster.start(NepheleMiniCluster.java:164)
	at eu.stratosphere.sopremo.server.SopremoTestServer.<init>(SopremoTestServer.java:75)
	at eu.stratosphere.sopremo.server.SopremoServerIT.setup(SopremoServerIT.java:58)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:120)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:103)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
Caused by: java.lang.IllegalArgumentException: The given amount of memory amounted to less than one page.
	at eu.stratosphere.nephele.services.memorymanager.spi.DefaultMemoryManager.<init>(DefaultMemoryManager.java:133)
	at eu.stratosphere.nephele.services.memorymanager.spi.DefaultMemoryManager.<init>(DefaultMemoryManager.java:100)
	at eu.stratosphere.nephele.taskmanager.TaskManager.<init>(TaskManager.java:289)
	at eu.stratosphere.nephele.instance.local.LocalTaskManagerThread.<init>(LocalTaskManagerThread.java:40)
	... 33 more
</pre>
(full log at  https://s3.amazonaws.com/archive.travis-ci.org/jobs/12992423/log.txt )

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/216
Created by: [AHeise|https://github.com/AHeise]
Labels: bug, runtime, 
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Mon Oct 28 11:02:42 CET 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397585,,,Mon Jun 09 11:46:20 UTC 2014,,,,,,,,,,"0|i1wg7r:",397712,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:46;github-import;[Date: Mon Oct 28 13:52:00 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Does it make sense to call `System.gc()` before determining the free memory?

We should also fix the TODO there in the code.;;;","09/Jun/14 11:46;github-import;[Date: Mon Oct 28 13:56:45 CET 2013, Author: [AHeise|https://github.com/AHeise]]

No, System.gc() is only a hint. Also with Java 7 and its concurrent GC, I doubt that this method would ever block.

http://docs.oracle.com/javase/7/docs/api/java/lang/System.html#gc%28%29

There are a couple of suggestions (aka Hacks) around, but basically you can never really estimate sufficiently enough how much main memory is left. I would rather suggest to take a percentage of overall memory instead of free memory. You could also make that percentage configurable. (I know that you can already configure the absolute number, however these errors occur mainly on CI VMs and I have not figured out how much RAM they have and even if they have the same amount at all).;;;","09/Jun/14 11:46;github-import;[Date: Mon Oct 28 14:38:31 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Assigned to @markus-h ;;;","09/Jun/14 11:46;github-import;[Date: Tue Nov 19 14:17:09 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [945a3c6e0a9533f2f3f0f115c2ac52f4e8c0d9c0|https://github.com/stratosphere/stratosphere/commit/945a3c6e0a9533f2f3f0f115c2ac52f4e8c0d9c0];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wordcount example broken,FLINK-215,12719385,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:46,09/Jun/14 11:46,14/Jul/23 05:57,09/Jun/14 11:46,,,,pre-apache,,,,,,,0,github-import,,"I guess introduced by https://github.com/stratosphere/stratosphere/pull/195

Calling Wordcount without any arguments causes a NPE.

Calling it with the correct arguments (`2  hdfs:///user/robert/datasets/hamlet-wc hdfs:///tmp/stratosphere-dev/wc`)

Causes this 
```
Could not find or load main class 2
```
No stacktrace.



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/215
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, user satisfaction, 
Milestone: Release 0.4
Assignee: [aljoscha|https://github.com/aljoscha]
Created at: Sat Oct 26 15:33:24 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397584,,,Mon Jun 09 11:46:14 UTC 2014,,,,,,,,,,"0|i1wg7j:",397711,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:46;github-import;[Date: Sat Oct 26 15:34:50 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I think we should set the LocalExecutor as the default execution method for WordCount.

The current variant requires the user to setup stratosphere and start it in Local mode.;;;","09/Jun/14 11:46;github-import;[Date: Sat Oct 26 15:38:26 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

The `LocalExecutor` does not work as well:

```
Plan plan = wc.getPlan(""1"", ""hdfs:///user/robert/datasets/hamlet-wc"", ""hdfs:///tmp/stratosphere-dev/wc"");
LocalExecutor.execute(plan);
```


Executing the wordcount example does not work on the cluster:
```
/stratosphere/bin/pact-client.sh run\
 --jarfile ./stratosphere/examples/pact/pact-examples-0.4-SNAPSHOT-WordCount.jar \
 --arguments $DOP hdfs:///user/robert/datasets/hamlet-wc hdfs:///tmp/stratosphere-dev/wc
```

results in
```
ERROR: The job was not successfully submitted to the nephele job manager: eu.stratosphere.nephele.executiongraph.GraphConversionException: Cannot compute input splits for Input Lines: java.io.IOException: Incomplete HDFS URI, no host: file:///
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:96)
	at eu.stratosphere.nephele.fs.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:131)
	at eu.stratosphere.nephele.fs.FileSystem.get(FileSystem.java:224)
	at eu.stratosphere.nephele.fs.Path.getFileSystem(Path.java:298)
	at eu.stratosphere.pact.generic.io.FileInputFormat.createInputSplits(FileInputFormat.java:322)
	at eu.stratosphere.pact.generic.io.FileInputFormat.createInputSplits(FileInputFormat.java:71)
	at eu.stratosphere.pact.runtime.task.DataSourceTask.computeInputSplits(DataSourceTask.java:326)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:554)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:275)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:176)
	at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:511)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:417)
	at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:946)

	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:557)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:275)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:176)
	at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:511)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:417)
	at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:946)
```

I think I've done nothing wrong.;;;","09/Jun/14 11:46;github-import;[Date: Sat Oct 26 17:12:20 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Part one fixed in [b2af1d9a1bd3d1af6ae166925c069d5febdf0eba|https://github.com/stratosphere/stratosphere/commit/b2af1d9a1bd3d1af6ae166925c069d5febdf0eba];;;","09/Jun/14 11:46;github-import;[Date: Sat Oct 26 17:15:38 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The problem that the hdfs path do not work is the following:
You need either a compete hdfs uri that specifies host and port of the namenode. Or you need a default hdfs configuration registered with stratosphere, in the config file.

We need a much better error message, though!;;;","09/Jun/14 11:46;github-import;[Date: Sat Oct 26 17:47:44 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The local executor cannot run with HDFS, because the distributed file system is in the nephele-hdfs project, which is not in the dependency path. I will add it there...;;;","09/Jun/14 11:46;github-import;[Date: Sat Oct 26 18:28:25 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Just as a documentation:
You need to set 
`fs.hdfs.hdfsdefault: /share/hadoop/hadoop/conf/hdfs-site.xml` to your Stratosphere config.

And this file must contain the value
```
<property><name>fs.default.name</name><value>hdfs://namenodeHost:40010/</value></property>
```

Or you specify the namenodeHost with your file URI:
`hdfs://namenodeHost:40010/fancyFile.seq`;;;","09/Jun/14 11:46;github-import;[Date: Mon Oct 28 10:54:06 CET 2013, Author: [aljoscha|https://github.com/aljoscha]]

@StephanEwen fixed this, right?;;;","09/Jun/14 11:46;github-import;[Date: Mon Oct 28 12:39:56 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Should be fixed in [159088dc0abf0e217a0441badea0e899a488b31b|https://github.com/stratosphere/stratosphere/commit/159088dc0abf0e217a0441badea0e899a488b31b] and [b2af1d9a1bd3d1af6ae166925c069d5febdf0eba|https://github.com/stratosphere/stratosphere/commit/b2af1d9a1bd3d1af6ae166925c069d5febdf0eba];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Java 1.6 compatibility?,FLINK-214,12719384,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:45,09/Jun/14 11:46,14/Jul/23 05:57,09/Jun/14 11:46,,,,pre-apache,,,,,,,0,github-import,,"In stratosphere-sopremo we consider removing JDK 1.6 compatibility. One thing less to worry about.

While stratopshere-sopremo could throw out support for 1.6 it does not make sense to do so, if Stratosphere retains support for 1.6.
It would be incoherent and in the worst case, people run Stratosphere with 1.6 on one system, Sopremo with 1.7 on another and don't understand why things stop working.

So, Sopremo can only stop supporting 1.6 if Stratosphere does the same, hence this issue.

I created my pro/con list, feel free to comment:

Pro:
+ one JDK version means easier maintanance
+ we can finally use all the Java 7 features
+ Oracle doesn't provide patches anymore for JDK 1.6 [citation needed] (apparently it does for paying customers)

Con:
- raised barrier of entry, there may be people who just didn't bother to update to 1.7 even though they could have
- use of Stratosphere may be even impossible for some users because the can't switch to 1.7 for whatever reason

[Disclaimer] I just moved this discussion to an issue, so everyone can participate, don't kill the messenger :-P

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/214
Created by: [mleich|https://github.com/mleich]
Labels: 
Created at: Fri Oct 25 17:06:16 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397583,,,Mon Jun 09 11:46:06 UTC 2014,,,,,,,,,,"0|i1wg7b:",397710,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:46;github-import;[Date: Fri Oct 25 17:18:50 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

I think we could remove jdk 1.6 support without long discussion, if the intersection of the jdk 1.6 users and the stratosphere 0.4-users is an empty set. ;;;","09/Jun/14 11:46;github-import;[Date: Fri Oct 25 17:26:16 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Is it really bad if Meteor/Sopremo requires 1.7 and the lower layer does not? You can state that as a Meteor requirement.

If someone starts with Stratosphere without Meteor and then adds meteor, he might have to upgrade Java. Is an impractical requirement? I am not sure...;;;","09/Jun/14 11:46;github-import;[Date: Fri Oct 25 17:41:37 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

Who said that it's bad? mleich pointed out that that it would be incoherent.
I'm not 100% sure how coherent software is defined, but I guess it's correct so state that the coherence decays  if the requirements diverge.;;;","09/Jun/14 11:46;github-import;[Date: Fri Oct 25 17:50:27 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I am a bit uncomfortable with any restriction that is not needed.

So, I wonder, does it really make a difference for Sopremo users if the lower level has fewer requirements?

Concerning coherency: Componentization of software is exactly to allow for divergence in development cycles and requirements. Higher layers may pose more requirements than lower ones.;;;","09/Jun/14 11:46;github-import;[Date: Fri Oct 25 18:03:11 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Which issues do you have with 1.6 in Sopremo?
Did anybody prove that Sopremo with 1.7 and Stratosphere with 1.6 don't work together?
We use apache commons-io in version 2.2 which is compiled for java 1.5. So backward compatibility should be given. 

Why should we drop support for something that is working perfectly fine.
Even if we run on java 7 jvms, our code is compiled for java 1.6.

I can not remember maintenance issues with Java 6.

The JVM on our cluster (cloud-11) at dima is a Java 6 one. Its an  Ubuntu 10.04.4 LTS which probably does not even have Java 7 in its repos.


So I vote for keeping Java 6 support!


I would personally recommend everybody to use Java 7, because there are no security patches for Java 6 anymore.;;;","09/Jun/14 11:46;github-import;[Date: Fri Oct 25 18:09:14 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

@rmetzger So you say cloud-11 is the user that requires java 1.6?;;;","09/Jun/14 11:46;github-import;[Date: Fri Oct 25 18:14:35 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

We are currently doing a research cooperation with the Deutsche Telekom,
evaluating Stratosphere.
Java 6 is on their server as well.


On Fri, Oct 25, 2013 at 6:09 PM, physikerwelt <notifications@github.com>wrote:

> @rmetzger <https://github.com/rmetzger> So you say could-11 is the user
> that requires java 1.6?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/214#issuecomment-27104962>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","09/Jun/14 11:46;github-import;[Date: Fri Oct 25 18:17:24 CEST 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

I don't understand the problem. Sopremo can move to Java 7 and without any problems use a Java 6-based Stratosphere.;;;","09/Jun/14 11:46;github-import;[Date: Fri Oct 25 18:19:38 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

There is no problem, if less coherence is not is a problem.;;;","09/Jun/14 11:46;github-import;[Date: Fri Oct 25 18:21:15 CEST 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

You trade two things off here: Easier development through a more modern version of the language vs the size of a possible user base

There are no problems with coherency. Java is so successful, because you can always use your old code with new versions of the language.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NepheleJobGraphGenerator bug with iterations,FLINK-213,12719383,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:45,09/Jun/14 11:45,14/Jul/23 05:57,09/Jun/14 11:45,,,,pre-apache,,,,,,,0,github-import,,"When traversing it is not checked whether iterations where already
visited, this led to a bug when you used the output of an iteration
twice.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/213
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri Oct 25 16:49:46 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:45;github-import;pull-request-213-6099067396670582723.patch;https://issues.apache.org/jira/secure/attachment/12649034/pull-request-213-6099067396670582723.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397582,,,Mon Jun 09 11:45:57 UTC 2014,,,,,,,,,,"0|i1wg73:",397709,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:45;github-import;[Date: Fri Oct 25 16:51:18 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@StephanEwen Is this how it is supposed to be now? This is unrelated to ([#212|https://github.com/stratosphere/stratosphere/issues/212] | [FLINK-212|https://issues.apache.org/jira/browse/FLINK-212]) ;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 17:08:54 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

That one looks good to me!;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 17:22:19 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [b08240d13a4208eda702539d0c47a068c67515b2|https://github.com/stratosphere/stratosphere/commit/b08240d13a4208eda702539d0c47a068c67515b2];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Map after BulkIteration does not work,FLINK-212,12719382,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:45,09/Jun/14 11:45,14/Jul/23 05:57,09/Jun/14 11:45,,,,pre-apache,,,,,,,0,github-import,,"This is the exception I get:
eu.stratosphere.pact.compiler.CompilerException: An error occurred while translating the optimized plan to a nephele JobGraph: null
	at eu.stratosphere.pact.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:556)
	at eu.stratosphere.pact.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:1)
	at eu.stratosphere.pact.compiler.plan.candidate.SingleInputPlanNode.accept(SingleInputPlanNode.java:142)
	at eu.stratosphere.pact.compiler.plan.candidate.SingleInputPlanNode.accept(SingleInputPlanNode.java:141)
	at eu.stratosphere.pact.compiler.plan.candidate.OptimizedPlan.accept(OptimizedPlan.java:161)
	at eu.stratosphere.pact.compiler.plantranslate.NepheleJobGraphGenerator.compileJobGraph(NepheleJobGraphGenerator.java:165)
	at eu.stratosphere.pact.test.util.TestBase2.getJobGraph(TestBase2.java:282)

@StephanEwen this could also have to to with the mapper chaining I suppose?

Also, when I use the output of an iteration in a file sink and also as one input of a match I get the same exception. Maybe thats related, maybe not.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/212
Created by: [aljoscha|https://github.com/aljoscha]
Labels: bug, optimizer, user satisfaction, 
Created at: Fri Oct 25 15:21:59 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397581,,,Mon Jun 09 11:45:51 UTC 2014,,,,,,,,,,"0|i1wg6v:",397708,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:45;github-import;[Date: Fri Oct 25 17:13:37 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

We cannot chain to iterations nodes. This is a one line fix, I'll get onto it...;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 17:23:06 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

31dbfe65e46d8419d0da8d670b5958407adbdbf2 fixes the mapper chaining issue.

I'll look into the case where the iteration result has two consumers...;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 17:23:30 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

@aljoscha Do you have a minimal test case for that?;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 18:20:05 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

I think we can close this, right? And the other thing is also fixed.;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 19:01:06 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, close it if your problem is solved. I am adding a test case for the case of an iteration followed by a multiple consumers.;;;","09/Jun/14 11:45;github-import;[Date: Mon Oct 28 18:36:18 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [d1498502482d4e5045880c21a8e5b4fab2caa1b4|https://github.com/stratosphere/stratosphere/commit/d1498502482d4e5045880c21a8e5b4fab2caa1b4];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use JAVA_RUN in all start scripts,FLINK-211,12719381,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:45,09/Jun/14 11:45,14/Jul/23 05:57,09/Jun/14 11:45,,,,pre-apache,,,,,,,0,github-import,,"See issue ([#207|https://github.com/stratosphere/stratosphere/issues/207] | [FLINK-207|https://issues.apache.org/jira/browse/FLINK-207]) for details.

We would still need to change the last `/` in `DEFAULT_ENV_JAVA_HOME`, but I didn't want to have a conflict with the pull request in ([#207|https://github.com/stratosphere/stratosphere/issues/207] | [FLINK-207|https://issues.apache.org/jira/browse/FLINK-207]). 

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/211
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Fri Oct 25 13:33:55 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:45;github-import;pull-request-211-4031658808964187800.patch;https://issues.apache.org/jira/secure/attachment/12649033/pull-request-211-4031658808964187800.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397580,,,Mon Jun 09 11:45:45 UTC 2014,,,,,,,,,,"0|i1wg6n:",397707,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:45;github-import;[Date: Fri Oct 25 17:32:02 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [9f227b245d6194faf32cf114e7aa20736e0e3d5c|https://github.com/stratosphere/stratosphere/commit/9f227b245d6194faf32cf114e7aa20736e0e3d5c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pass Objects instead of Classes for DataSource and DataSink,FLINK-210,12719380,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:45,09/Jun/14 11:45,14/Jul/23 05:57,09/Jun/14 11:45,,,,pre-apache,,,,,,,0,github-import,,"The same object passing mechanism should be available on data sources and data sinks as well. Especially for formats that require a lot of configuration (record input and -output formats), this would greatly simplify the API.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/210
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Assignee: [aljoscha|https://github.com/aljoscha]
Created at: Fri Oct 25 12:30:21 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397579,,,Mon Jun 09 11:45:41 UTC 2014,,,,,,,,,,"0|i1wg6f:",397706,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:45;github-import;[Date: Fri Oct 25 12:37:53 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

It is available, just check any example.;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 17:27:59 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, I see the data flow nodes support it. It is however not exploited with the RecordInputFormat, there is no way to pass parameters as variables. only through config parameters.;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 18:20:52 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Oh yeah, it is not exploited anywhere so far.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkIteration does not work with Array Datamodel,FLINK-209,12719379,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:45,09/Jun/14 11:45,14/Jul/23 05:57,09/Jun/14 11:45,,,,pre-apache,,,,,,,0,github-import,,"Things go bad starting here:
https://github.com/stratosphere/stratosphere/blob/master/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/postpass/GenericRecordPostPass.java#L150

It seems that the schema is not correctly propagated from the nodes inside the iteration.

@StephanEwen Any clue what could be the problem?

This is the execption I get:

Exception in thread ""main"" java.lang.IllegalArgumentException: Bug: Attempt to create serializer for -1 fields.
	at eu.stratosphere.pact.compiler.postpass.GenericArrayRecordPostPass.createSerializer(GenericArrayRecordPostPass.java:153)
	at eu.stratosphere.pact.compiler.postpass.GenericArrayRecordPostPass.createSerializer(GenericArrayRecordPostPass.java:1)
	at eu.stratosphere.pact.compiler.postpass.GenericRecordPostPass.createSerializer(GenericRecordPostPass.java:503)
	at eu.stratosphere.pact.compiler.postpass.GenericRecordPostPass.traverse(GenericRecordPostPass.java:150)
	at eu.stratosphere.pact.compiler.postpass.GenericRecordPostPass.propagateToChannel(GenericRecordPostPass.java:460)
	at eu.stratosphere.pact.compiler.postpass.GenericRecordPostPass.traverse(GenericRecordPostPass.java:93)
	at eu.stratosphere.pact.compiler.postpass.GenericRecordPostPass.postPass(GenericRecordPostPass.java:68)
	at eu.stratosphere.pact.compiler.PactCompiler.compile(PactCompiler.java:738)
	at eu.stratosphere.pact.compiler.PactCompiler.compile(PactCompiler.java:547)




---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/209
Created by: [aljoscha|https://github.com/aljoscha]
Labels: bug, 
Created at: Fri Oct 25 11:52:19 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397578,,,Mon Jun 09 11:45:36 UTC 2014,,,,,,,,,,"0|i1wg67:",397705,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:45;github-import;[Date: Fri Oct 25 11:54:57 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The array data model is not really tested well. I thought it a proof-of-concept of how you can implement different datamodels (such as a specific one for scala, a nicel tuple-based one for java, or so).

Should we rather remove it from the project? Or do you think it is promising and should be improved further?;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 11:56:59 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

I find it quite nice to work with actually. Except for the bug I have now, of course. I have an implementation of ALS using the PactRecord model and now I ported it to the Array model to see whether this makes a difference in performance in this case.;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 11:59:44 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, let me try to look into the problem...;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 12:01:45 CEST 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

Off-topic: A good way to increase the performance for this problem is to use JBlas for solving the least squares problems occurring. For larger ranks this makes a huge difference. I have a JBlas-based solver, let me know if you want to use it.;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 12:02:54 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Here is a thought I have been carrying for a bit: We create a similar data model, but not on arrays, but on tuples. We either use the scala tuple classes, or we generate ourselves 22 tuple classes in a similar way. We would then have type safety. The cost is that the generics in java are always verbose (no type inference).;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 12:04:01 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

A big advantage of the array/tuple data model is that the schema is fully known. So the runtime could (I don't think it array model utilities support that, yet) use fix-length algorithms for many things.;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 12:08:41 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@sscdotopen Yeah, that would be cool. Right now the stratosphere version is slower than the numpy/scipy based python version. (On a single machine, which I somewhat expected given the overhead. But still ...);;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 12:10:28 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@StephanEwen I'm also looking into it, the code seems to be taking the schema of the PartialSolution, and this seems to be empty. Could it not take the schema of the NextPartialSolution, which would in this case be an ArrayModelStub. I'm not sure how the traversal in the PostPass works, so it's just an idea...;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 12:27:17 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I think the schema you get from a stub is always its input schema. You need the output schema of the last operator in the step function.

It would be nice to have access to both (input schema of the step function, output schema of the step function), then we could verify at that point that the types match.;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 12:48:14 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Ok, but this seems to work for the PactRecord datamodel, why not for the arrays?;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 12:50:40 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

In PactRecord, we make the assumption that the programmer knows what he is doing, because we cannot check the schema ;-)

In array model, we could in theory check the schema. If we do not have it and infer it from the tail of the iteration, we make again the assumption that the programmer worked correctly, instead of checking it. Which is fine for now to make.;;;","09/Jun/14 11:45;github-import;[Date: Fri Mar 21 21:10:52 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

The Array Datamodel has been removed in commit [4bbc6a4c980eee767e6d98c3427ee690e676a88a|https://github.com/stratosphere/stratosphere/commit/4bbc6a4c980eee767e6d98c3427ee690e676a88a].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enhancement: Expand test coverage,FLINK-208,12719378,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:45,09/Jun/14 11:45,14/Jul/23 05:57,09/Jun/14 11:45,,,,pre-apache,,,,,,,0,github-import,,"I propose to configure travis in a way that changes in stratosphere/stratosphere trigger re-running the tests in in other project that depend on stratosphere i.e. stratosphere-sopremo and other essential projects.
By that we could expand the ""test coverage"" without much effort and would have a chance to discover errors that are hard to detect manually, especially if there are dependencies to the -SNAPSHOT version.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/208
Created by: [physikerwelt|https://github.com/physikerwelt]
Labels: build system, testing, 
Created at: Fri Oct 25 00:32:09 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397577,,,Mon Jun 09 11:45:27 UTC 2014,,,,,,,,,,"0|i1wg5z:",397704,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:45;github-import;[Date: Fri Oct 25 01:11:18 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you change that independently in your build server setup? 

I think the crux of the issue is actually that meteor and other projects depend on a snapshot version. These version are bound to change on ways that cause problems for dependent projects.

My feeling is that a correct software engineering solution would be to push a version, say 0.3.0 or so, that represents the current snapshot and will not change. You could use that as a stable dependency. At some point you could move forward to 0.4, which is then stable, while we work on the snapshot version of 0.4.1

What do you think? ;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 01:51:28 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

Yes. I completely agree with you about the version numbers... and I think I'm not the only one.
However, I think a retrigger all tests would be nice for fixes and pre release checks.
I think we have to expect hotfixes for released versions and should think about a process how to deal with them. (Maybe in terms of version numbers?);;;","09/Jun/14 11:45;github-import;[Date: Sun Oct 27 20:42:18 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

I found that with a random work approach http://mojo.codehaus.org/buildnumber-maven-plugin/usage.html
I'm not yet quite sure if it's helpful or distracting but it would at least allow to set unique references.
;;;","09/Jun/14 11:45;github-import;[Date: Mon Oct 28 00:11:19 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Hey,

so I think in general, its a good idea to somehow store the version (e.g. the git revision id) in the snapshots, so that we can see which version a user has.
The best solution in my opinion is to write the id into the README.txt file.
I'm really against changing the name of the build again. (We would have to update all the documentation and examples etc.)
I don't see that the buildnumber-maven-plugin does support this (it can only write into .property file).

I think this plugin does everything we need (we need to try if it also can write into non .property files, but it looks like (since the're also writing into xml): https://github.com/ktoso/maven-git-commit-id-plugin
;;;","09/Jun/14 11:45;github-import;[Date: Mon Oct 28 10:12:31 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I agree with @rmetzger. 
Having a VERSION file with an ID that can be linked to the last commit of the build is a very good idea and will help to identify bugs in user settings.
A new version number for each commit sounds a bit heavy weight to me.;;;","09/Jun/14 11:45;github-import;[Date: Mon Oct 28 11:16:07 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

OK. We are moving a little bit into the ""How to do releases?"" Question.

In a former thread Robert explained, that the master branch moves from a stable state to the next stable state in terms of pull requests.
Stable state was defined as ""all stratosphere tests are passed"".
I propose to change the definition of stable state in the following:
""All stratosphere tests and all tests of important dependent projects are passed.""

I think stratosphere-sopremo is an important dependent project. Furthermore I see the word-count sample and maybe some more basic examples as important dependent projects as well.
If you think the set of important dependent projects is empty. The definitions are equivalent and the issue should be closed.
If you think the set is not empty, we should discuss how, when and by whom the issue is going to be fixed.

I think the details of version numbering are a different issue... I feel sorry that I contributed to let the issue become ""offtopic"".

;;;","09/Jun/14 11:45;github-import;[Date: Mon Oct 28 11:43:29 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I agree that sopremo is an important dependent project, but I think the definition of stable state should only depend on the project itself, as @StephanEwen suggested.
Dependent projects that request a stable version should refer to a stable version and not a development version.

Otherwise, updates such as changing the programming interface, which might still happen since we are not in a version 1.0 mode, would require to update all dependent projects. I do not think this is reasonable.
Updates that introduce bugs should be identified by the project's own test infrastructure. Relying on the tests of a dependent project does not seem to be the right way, IMHO. Sure, there will be cases where bugs are not detected, but these should be handled by adding test cases to the project itself and finally fixing the bug. 

Btw., the Stratosphere example jobs are part of the project itself and checked by integration tests.
;;;","09/Jun/14 11:45;github-import;[Date: Mon Oct 28 11:59:57 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

@fhueske I think that's a little bit problematic. The example jobs and scala are tested and sopremo is not tested.
To which stable version dependent projects should refer?  To 0.2? or 0.4 that is not yet released, and which probably won't be stable if it is released soon?;;;","09/Jun/14 11:45;github-import;[Date: Mon Oct 28 18:38:10 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

We decided to solve this problem with periodic publications of minor releases that higher level layer projects like Metor/Sopremo can reference.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update default jdk to oracle java 7,FLINK-207,12719377,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:45,09/Jun/14 11:45,14/Jul/23 05:57,09/Jun/14 11:45,,,,pre-apache,,,,,,,0,github-import,,"w.r.t. l 130 I think to trailing / is needed

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/207
Created by: [physikerwelt|https://github.com/physikerwelt]
Labels: 
Created at: Thu Oct 24 19:41:44 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:45;github-import;pull-request-207-273298812973982948.patch;https://issues.apache.org/jira/secure/attachment/12649032/pull-request-207-273298812973982948.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397576,,,Mon Jun 09 11:45:19 UTC 2014,,,,,,,,,,"0|i1wg5r:",397703,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:45;github-import;[Date: Fri Oct 25 01:00:50 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Thanks for suggesting that. The java path is normally determined from the system set environment variable JAVA_HOME. The configuration parameter is only relevant if that value is not set, which normally indicates that there is no proper Java installation.

The default value was a help for developers. We might want to set out to something that would match a system workout explicit Java installation. I guess that would actually be rather open-jdk-7.

Any thoughts? ;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 01:05:39 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

After changing that variable I could start nephele-visualisation. I'd prefere open-jdk too if it's verified to work.;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 01:13:44 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

So the visualization Scripts do not properly read the java home variable?;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 01:31:13 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

That could be the case. Independent of that, the java-sun default should disappear in my opinion. ;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 12:59:14 CEST 2013, Author: [uce|https://github.com/uce]]

OK, I see the problem with the viz script.

I noticed it when I was changing stuff on the scripts before, but didn't think it was problematic.

The problem is the following.

- In `nephele-config.sh` JAVA_HOME is set to the default value if it is not set in the environment.
- JAVA_HOME is then used to set JAVA_RUN. If JAVA_HOME is not set (which will not be the case), it would fall back to setting it to `java` directly (in default path).
- In `nephele-visualization.sh` the JVM is started by `$JAVA_HOME/bin/java` instead of `$JAVA_RUN` like in the task mangager etc. Therefore, if JAVA_HOME is not set or the default doesn't make sense for the system, nepehle-visualization will not start.

I think @physikerwelt couldn't start nephele-viz, because the default path didn't make sense on his system.

I will change the nephele-viz start script to use JAVA_RUN. In general, I think a clean-up of the scripts would be worthwhile.;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 13:28:01 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

@uce Yes:
```
/b/bin$ ./nephele-visualization.sh 
./nephele-visualization.sh: line 46: /usr/lib/jvm/java-6-sun//bin/java: No such file or directory
```
;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 19:09:57 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

what?;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 19:50:06 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

OK I understand... but why didn't you modify my pull request?;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 20:36:41 CEST 2013, Author: [uce|https://github.com/uce]]

Because your pull requests title suggests something totally different than what I did in my pull request.

We still need to remove the trailing `/` though.

Since this issue and ([#214|https://github.com/stratosphere/stratosphere/issues/214] | [FLINK-214|https://issues.apache.org/jira/browse/FLINK-214]) are basically the same question, I assume that this here can also be closed?;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 20:38:50 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

It is impossible to modify other people's pull requests (except you have access to their repositories). See also: https://help.github.com/articles/using-pull-requests

@uce did not add the the change you suggested because it does not solve the problem, but hides the symptoms.;;;","09/Jun/14 11:45;github-import;[Date: Sat Oct 26 11:40:11 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

What I don't understand is why did ([#211|https://github.com/stratosphere/stratosphere/issues/211] | [FLINK-211|https://issues.apache.org/jira/browse/FLINK-211]) did not add the slash. At least I could have rebased this pull request...even though I created it using the webui only.
(Beside that is's just a minor style issue: Try /bin//echo test)
Whatever JAVA version you want to use, please try to use one of the travis config:
jdk:
  - oraclejdk7
  - openjdk7
  - openjdk6
java6sun is not part of the list. So I'd strongly recommend to change or remove the default parameter.
In order to avoid further conflicts that this pull request causes, I close it.

;;;","09/Jun/14 11:45;github-import;[Date: Thu Nov 07 18:38:25 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

by the was it's still set to sun... and so fails to run if JAVA_HOME is unset and java-sun is not installed;;;","09/Jun/14 11:45;github-import;[Date: Thu Nov 07 23:52:07 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I'm not sure if we should even set a default JAVA_HOME. This is very distribution specific...

Why not using the config as 1st choice, the system configured JAVA_HOME as 2nd choice, and if this does not give a valid home, simply show an error message?
In any case we should add it to the yaml config file (deactivated behind a comment).;;;","09/Jun/14 11:45;github-import;[Date: Fri Nov 08 08:24:10 CET 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

Yes. That would be better. Otherwise the sun-error message is the first thinh the user sees if he tries Stratosphere. That could be misinterpreted.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary Sort for Reduce and CoGroup is not working,FLINK-206,12719376,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:45,09/Jun/14 11:45,14/Jul/23 05:57,09/Jun/14 11:45,,,,pre-apache,,,,,,,0,github-import,,"Please reenable the secondary ordering as it is essential for data cleansing algorithms.

To reproduce, given input:
(1, x, b)
(1, y, c)
(1, z, a)
And reducer on first field with secondary sort on last, I would expect to get the ordering
(1, z, a)
(1, x, b)
(1, y, c)
But I receive the original order.

I could provide a TestPlan test case.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/206
Created by: [AHeise|https://github.com/AHeise]
Labels: bug, optimizer, 
Milestone: Release 0.4
Created at: Thu Oct 24 14:25:14 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397575,,,Mon Jun 09 11:45:07 UTC 2014,,,,,,,,,,"0|i1wg5j:",397702,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:45;github-import;[Date: Thu Oct 24 19:41:08 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I will take a look at that in a bit.;;;","09/Jun/14 11:45;github-import;[Date: Fri Oct 25 19:33:16 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [633e3e6a7ed8bc903a749b7f21902c5b4ec5099b|https://github.com/stratosphere/stratosphere/commit/633e3e6a7ed8bc903a749b7f21902c5b4ec5099b];;;","09/Jun/14 11:45;github-import;[Date: Wed Nov 06 17:45:34 CET 2013, Author: [AHeise|https://github.com/AHeise]]

Secondary Sort works only for PactRecord-based programs. 

It boils down to the basic issue that GenericReduceContract do not have Orderings. Hence, only ReduceContracts have a groupOrder that is passed as a GroupProperty by ReduceNode to the optimizer.

The following line in ReduceNode needs to be fixed.
```java
                // check if we can work with a grouping (simple reducer), or if we need ordering because of a group order
                Ordering groupOrder = null;
                if (getPactContract() instanceof ReduceContract) {
                        groupOrder = ((ReduceContract) getPactContract()).getGroupOrder();
                        if (groupOrder != null && groupOrder.getNumberOfFields() == 0) {
                                groupOrder = null;
                        }
                }
```

I either propose to let GenericReduceContract have group orders or provide an interface for implementing classes that they have such an order. I vote for former, because such Orderings do not assume a tuple model but only assume that the keys can be enumerated and are thus general enough for arbitrary data models.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added weblog analysis Scala example and corresponding integration test.,FLINK-204,12719374,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:44,09/Jun/14 11:44,14/Jul/23 05:57,09/Jun/14 11:44,,,,pre-apache,,,,,,,0,github-import,,"Added weblog analysis Scala example and corresponding integration test.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/204
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Thu Oct 24 00:14:56 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:44;github-import;pull-request-204-2693383065896768363.patch;https://issues.apache.org/jira/secure/attachment/12649031/pull-request-204-2693383065896768363.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397573,,,Mon Jun 09 11:44:58 UTC 2014,,,,,,,,,,"0|i1wg53:",397700,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:44;github-import;[Date: Thu Oct 24 07:42:29 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Looks very good to me. You seem to be growing on the scala frontend. :smiley_cat: ;;;","09/Jun/14 11:44;github-import;[Date: Thu Oct 24 09:09:09 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

Yes, sure. Will fix this and merge it with main.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove IPv4 Stack Hint (issue #189),FLINK-203,12719373,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:44,09/Jun/14 11:44,14/Jul/23 05:57,09/Jun/14 11:44,,,,pre-apache,,,,,,,0,github-import,,"See @StephanEwen's comments in ([#189|https://github.com/stratosphere/stratosphere/issues/189] | [FLINK-189|https://issues.apache.org/jira/browse/FLINK-189]).

These were all references to the IPv4 stack I found.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/203
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Wed Oct 23 23:54:46 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:44;github-import;pull-request-203-6698085585093277757.patch;https://issues.apache.org/jira/secure/attachment/12649030/pull-request-203-6698085585093277757.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397572,,,Mon Jun 09 11:44:52 UTC 2014,,,,,,,,,,"0|i1wg4v:",397699,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:44;github-import;[Date: Thu Oct 24 00:43:16 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Before we can merge this, we need to test whether cluster setups are affected. The discovery manager is important for task managers to find their job-manager-facing ip address. We should also let the os or jvm determine the network stack (ip v4 or ip v6) and not force anything, because that may  prohibit the system to work in certain settings. If I recall correctly, the discovery manager needed ip v4 for broadcast announcements of the job managers address in cloud settings where no slave file was available, but we don't want to go that route. ;;;","09/Jun/14 11:44;github-import;[Date: Thu Oct 24 00:52:17 CEST 2013, Author: [uce|https://github.com/uce]]

OK, sure. Then we should keep the system property check in `DiscoveryService`, no? Both

- `java.net.preferIPv4Stack`, and
- `java.net.preferIPv6Addresses`

are set by the OS, right?

The [docs|http://docs.oracle.com/javase/7/docs/api/java/net/doc-files/net-properties.html) say:

> java.net.preferIPv4Stack (default: false)
> If IPv6 is available on the operating system the underlying native socket will be, by default, an IPv6 socket which lets applications connect to, and accept connections from, both IPv4 and IPv6 hosts. However, in the case an application would rather use IPv4 only sockets, then this property can be set to true. The implication is that it will not be possible for the application to communicate with IPv6 only hosts.
>
> java.net.preferIPv6Addresses (default: false)
> When dealing with a host which has both IPv4 and IPv6 addresses, and if IPv6 is available on the operating system, the default behavior is to prefer using IPv4 addresses over IPv6 ones. This is to ensure backward compatibility, for example applications that depend on the representation of an IPv4 address (e.g. 192.168.1.1]. This property can be set to true to change that preference and use IPv6 addresses over IPv4 ones where possible.

I think I would go with the `java.net.preferIPv6Addresses` check.

Or is there another way? ;;;","09/Jun/14 11:44;github-import;[Date: Thu Oct 24 00:56:18 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Do we need such flags at all? If we do not use that one specific feature of the discovery service, is there anything left that is IPv4 or IPv6 aware or dependent? I think we should be able to write the ""get my public IP"" functionality such that it works with both v4 and v6 transparently.;;;","09/Jun/14 11:44;github-import;[Date: Thu Oct 24 01:07:58 CEST 2013, Author: [uce|https://github.com/uce]]

OK, now I see what you're getting at. I didn't have a close look :eyes: at the `DiscoveryService`, but what you describe (transparent ""get my public IP"") makes sense.;;;","09/Jun/14 11:44;github-import;[Date: Fri Oct 25 22:25:53 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Since the code paths in the discovery service that require the flag are inactive anyways, I will merge this now.
We will follow up on this with issue ([#18|https://github.com/stratosphere/stratosphere/issues/18] | [FLINK-18|https://issues.apache.org/jira/browse/FLINK-18]) ;;;","09/Jun/14 11:44;github-import;[Date: Fri Oct 25 22:29:19 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [361e469bb9ac742bd3c2ddaf930cdebe160300f1|https://github.com/stratosphere/stratosphere/commit/361e469bb9ac742bd3c2ddaf930cdebe160300f1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Combiner is cannot be chained if Reducer DOP differs from the DOP of its input.,FLINK-201,12719371,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:44,09/Jun/14 11:44,14/Jul/23 05:57,09/Jun/14 11:44,,,,pre-apache,,,,,,,0,github-import,,"Currently, the DOP of the combiner is set to the DOP of the reduce task. In case, that the Reduce task has a different DOP than its input, an inserted combiner is not chainable since this requires an identical DOP with the sender task.

This bug was detected and described in more detail in this issue ([#138|https://github.com/stratosphere/stratosphere/issues/138] | [FLINK-138|https://issues.apache.org/jira/browse/FLINK-138]).
Since, ([#138|https://github.com/stratosphere/stratosphere/issues/138] | [FLINK-138|https://issues.apache.org/jira/browse/FLINK-138]) also includes another bug, I created a new issue.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/201
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, optimizer, 
Created at: Wed Oct 23 21:16:01 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397570,,,Mon Jun 09 11:44:42 UTC 2014,,,,,,,,,,"0|i1wg4f:",397697,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:44;github-import;[Date: Wed Oct 23 21:30:03 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

I have implemented a fix for this bug, but for some reason it causes another test to fail. 
The fix is in a branch of my repository [combinerBugFixes|https://github.com/fhueske/ozone/tree/combinerBugFixes]. 

The failing test is the compiler test [IterativeKMeansTest|https://github.com/fhueske/ozone/blob/combinerBugFixes/pact/pact-tests/src/test/java/eu/stratosphere/pact/compiler/iterations/IterativeKMeansTest.java] and the the assertion error is
     <code>java.lang.AssertionError: expected:&lt;PARTITION_HASH&gt; but was:&lt;FORWARD&gt;</code>

I checked that the modification of [GroupWithPartialPreGroupProperties|https://github.com/fhueske/ozone/blob/59b81b64e3d6bff28c64b260e21ca4cd96a3191c/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/operators/GroupWithPartialPreGroupProperties.java] triggers the test to fail.
There seems to be some other bug in the Pact compiler.

@StephanEwen Can you check what causes the test to fail and fix it so we can merge my fix into the main branch?
;;;","09/Jun/14 11:44;github-import;[Date: Mon Oct 28 18:35:43 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed as of [80551cb783a5e3bf47c20f6c1b4059dd8c691220|https://github.com/stratosphere/stratosphere/commit/80551cb783a5e3bf47c20f6c1b4059dd8c691220];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updated CONTRIBUTORS file,FLINK-200,12719370,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:44,09/Jun/14 11:44,14/Jul/23 05:57,09/Jun/14 11:44,,,,pre-apache,,,,,,,0,github-import,,"Added myself and sorted by lastname

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/200
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Wed Oct 23 20:25:10 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:44;github-import;pull-request-200-453150242146219348.patch;https://issues.apache.org/jira/secure/attachment/12649029/pull-request-200-453150242146219348.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397569,,,2014-06-09 11:44:35.0,,,,,,,,,,"0|i1wg47:",397696,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add union input test (issue #192),FLINK-197,12719367,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:44,09/Jun/14 11:44,14/Jul/23 05:57,09/Jun/14 11:44,,,,pre-apache,,,,,,,0,github-import,,"This test is an adaption of issue ([#192|https://github.com/stratosphere/stratosphere/issues/192] | [FLINK-192|https://issues.apache.org/jira/browse/FLINK-192]).

I checked that it deadlocks without @StephanEwen's fix and succeeds with it.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/197
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Wed Oct 23 17:30:33 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:44;github-import;pull-request-197-7649958608713481140.patch;https://issues.apache.org/jira/secure/attachment/12649028/pull-request-197-7649958608713481140.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397566,,,Mon Jun 09 11:44:18 UTC 2014,,,,,,,,,,"0|i1wg3j:",397693,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:44;github-import;[Date: Thu Oct 24 19:40:37 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalExecutor does not properly stop.,FLINK-196,12719366,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:44,09/Jun/14 11:44,14/Jul/23 05:57,09/Jun/14 11:44,,,,pre-apache,,,,,,,0,github-import,,"Very often when executing a plan using the LocalExecutor and calling stop() it happens that there are lingering threads and the program won't stop. It does not seem to happen when simply starting the LocalExecutor and then stopping.

Does anyone have any idea why this is happening. It is really annoying and I want to use this in the upcoming ExecutionContext.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/196
Created by: [aljoscha|https://github.com/aljoscha]
Labels: bug, 
Created at: Wed Oct 23 10:39:32 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397565,,,Mon Jun 09 11:44:14 UTC 2014,,,,,,,,,,"0|i1wg3b:",397692,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:44;github-import;[Date: Wed Oct 23 10:55:02 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The RPC Service has some lingering Threads, that is a known problem. Do you see any other ones?;;;","09/Jun/14 11:44;github-import;[Date: Wed Oct 23 12:02:38 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

No, only the RPC threads. I tried manually closing them somehow in the NepeheMiniCluster but had no luck.;;;","09/Jun/14 11:44;github-import;[Date: Wed Oct 23 12:35:39 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The rpc service is pretty crappy, it needs to be changed as part of the JobManager reworking. Does the service mark its threads as daemons, or do they keep the vm alive?;;;","09/Jun/14 11:44;github-import;[Date: Wed Oct 23 12:41:13 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

If have no idea, and no time to look into it right now.;;;","09/Jun/14 11:44;github-import;[Date: Thu Dec 05 16:38:58 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [9306beb181fcb218f44d7a9a50e4bbb3258c1b79|https://github.com/stratosphere/stratosphere/commit/9306beb181fcb218f44d7a9a50e4bbb3258c1b79];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remote executor feature,FLINK-195,12719365,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:44,09/Jun/14 11:44,14/Jul/23 05:57,09/Jun/14 11:44,,,,pre-apache,,,,,,,0,github-import,,"This pull request will in the end have a RemoteExecutor which will be used like:
val exec = new RemoteExecutor(""localhost:666"", ""some.jar"")
exec.execute(myPlan)

(Also usable from Java and written in Java)

For now I just have some refactoring of how Client.run() works and of PactProgram, what do you think of these?

Client.run() now accepts a PlanWithJars which is a wrapper of a Plan and a list of jars required for the job. I removed the jar extraction/deletion logic from Client.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/195
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Wed Oct 23 00:44:58 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:44;github-import;pull-request-195-8195464935878615001.patch;https://issues.apache.org/jira/secure/attachment/12649027/pull-request-195-8195464935878615001.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397564,,,Mon Jun 09 11:44:09 UTC 2014,,,,,,,,,,"0|i1wg33:",397691,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:44;github-import;[Date: Wed Oct 23 01:11:18 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

This pull request contains the unrelated commit to rename groupReduce. Can you resubmit this?;;;","09/Jun/14 11:44;github-import;[Date: Wed Oct 23 02:07:08 CEST 2013, Author: [uce|https://github.com/uce]]

I really really like this. :+1: 

We could also think about merging this with the LocalExecutor. Something like Spark where we run locally if the JM address is ""local"" or so and otherwise remotly.

In any way it would be great to finish this before the next milestone. :dancers: ;;;","09/Jun/14 11:44;github-import;[Date: Wed Oct 23 08:34:20 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@uce Yes, this is where I am going with this. :smile: I will have ExecutionContext(address, jars*) and this will internally either create a LocalExecutor or a RemoteExecutor.

@StephanEwen I will not have to remove the commit. I'm working on top of the rename-reduce-... branch since I hope that this will be merged soon. When I then rebase this work on top of master it will disappear here. Concerning the lists: I know, but I didn't care because the code is nowhere near performance critical. I will change the code, though.;;;","09/Jun/14 11:44;github-import;[Date: Wed Oct 23 08:41:07 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Also, don't merge this until it's done, please.;;;","09/Jun/14 11:44;github-import;[Date: Wed Oct 23 12:45:35 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Ok, I think this is ready to merge once travis gives the okay.

What do you think?

I have some plans regarding what @uce said but that will have to wait. At least this gives the basic infrastructure.;;;","09/Jun/14 11:44;github-import;[Date: Thu Oct 24 17:11:23 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Did anyone actually check whether the web frontend and the command line client still work? There is no automated testing for that. Would be good to have that verified ;-);;;","09/Jun/14 11:44;github-import;[Date: Thu Oct 24 23:17:00 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

I ran the web-frontend today. It still worked :-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo and Redesign Funding Page,FLINK-194,12719364,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:43,09/Jun/14 11:44,14/Jul/23 05:57,09/Jun/14 11:44,,,,pre-apache,,,,,,,0,github-import,,"The current funding page of the website looks quite unstructured and can be improved in my opinion.

There is also a small typo in the first sentence
-->   ""Deustche Forschungsgemeinschaft""

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/194
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, enhancement, website, 
Created at: Tue Oct 22 22:34:45 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397563,,,Mon Jun 09 11:44:01 UTC 2014,,,,,,,,,,"0|i1wg2v:",397690,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:44;github-import;[Date: Fri Oct 25 19:11:51 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Already merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PactRecord allocating too few memory when writing UTF encoded strings,FLINK-193,12719363,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:43,09/Jun/14 11:43,14/Jul/23 05:57,09/Jun/14 11:43,,,,pre-apache,,,,,,,0,github-import,,"PactRecord serializes UTF strings by writing the size of the encoded string (2 bytes) and the actual data.
The current code correctly checks whether size+data actually fit in memory, but it only allocates the memory for the data, not the 2 size bytes.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/193
Created by: [mleich|https://github.com/mleich]
Labels: 
Created at: Tue Oct 22 16:17:04 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;pull-request-193-7431493431217265565.patch;https://issues.apache.org/jira/secure/attachment/12649026/pull-request-193-7431493431217265565.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397562,,,Mon Jun 09 11:43:58 UTC 2014,,,,,,,,,,"0|i1wg2n:",397689,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;[Date: Wed Oct 23 00:10:19 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [08a9e76b9fb3272fb228746a8106040c84e9047c|https://github.com/stratosphere/stratosphere/commit/08a9e76b9fb3272fb228746a8106040c84e9047c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SingleInputContract deadlock,FLINK-192,12719362,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:43,09/Jun/14 11:43,14/Jul/23 05:57,09/Jun/14 11:43,,,,pre-apache,,,,,,,0,github-import,,"The problem usually occurs when I'm running a PACT plan on large input file and I use a single input contract (extending SingleInputContract<T>, eg. Map, Reduce) with two inputs (according to the documentation, public Builder input(Contract ...inputs) lets us to use the union of the multiple input parameters).

In many cases, but not always, stubs are running, but nothing happens - they reach a deadlock waiting for each other. No errors are reported, CPU is not used, no I/O activity. There is no output of the Reduce stub. We can observe output of the Map - Reduce must wait for all of the input data, but Map is able to produce output without collecting all of the input.

I set the DOP to 1, Stratosphere was running in local mode.
The word counter example code reproduces the problematic behavior on a multiplied ""hamlet"" input file:
for i in {1..100}; do  cat ./hamlet.txt>>hamlet100.txt ; done

 1) The original example code works well on the original hamlet.txt file both with Cogroup and Reduce.
 2) The code runs using Cogroup, no apparent problems.
 3) We usually reach the deadlock using Reduce with 2 inputs.
 4) We also tested Map with two inputs with a slightly different test code, and reached the deadlock.

We experienced the same behaviour with both Stratosphere 2.0 and 4.0 Ozone standalone installs, and also on many different machines (Ubuntu, Fedora; Sun Java 7, OpenJDK 7).

The source code is the variation of WordCounter example to demonstration the bug:
(In the comment: Cogroup and Map test.)

<pre><code>
package hu.sztaki.ilab.er.stratosphere;

import eu.stratosphere.pact.common.contract.CoGroupContract;
import java.util.Iterator;

import eu.stratosphere.pact.common.contract.FileDataSink;
import eu.stratosphere.pact.common.contract.FileDataSource;
import eu.stratosphere.pact.common.contract.GenericDataSink;
import eu.stratosphere.pact.common.contract.MapContract;
import eu.stratosphere.pact.common.contract.ReduceContract;
import eu.stratosphere.pact.common.io.RecordOutputFormat;
import eu.stratosphere.pact.common.io.TextInputFormat;
import eu.stratosphere.pact.common.plan.Plan;
import eu.stratosphere.pact.common.plan.PlanAssembler;
import eu.stratosphere.pact.common.plan.PlanAssemblerDescription;
import eu.stratosphere.pact.common.stubs.CoGroupStub;
import eu.stratosphere.pact.common.stubs.Collector;
import eu.stratosphere.pact.common.stubs.MapStub;
import eu.stratosphere.pact.common.stubs.ReduceStub;
//import eu.stratosphere.pact.common.stubs.StubAnnotation.ConstantFields;
//import eu.stratosphere.pact.common.stubs.StubAnnotation.OutCardBounds;
import eu.stratosphere.pact.common.type.PactRecord;
import eu.stratosphere.pact.common.type.base.PactInteger;
import eu.stratosphere.pact.common.type.base.PactString;
import java.io.Serializable;
import java.util.Arrays;
import java.util.List;
import org.apache.log4j.Logger;


/**
 *
 * @author lukacsg
 */
public class TestDoubleInput4 implements PlanAssembler, PlanAssemblerDescription {
     private static Logger log = Logger.getLogger(TestDoubleInput.class);

//	@ConstantFields(fields={})
//	@OutCardBounds(lowerBound=0, upperBound=OutCardBounds.UNBOUNDED)
    public static class Map1 extends MapStub implements Serializable {
        private static final long serialVersionUID = 1L;
        // initialize reusable mutable objects
		private final PactRecord outputRecord = new PactRecord();
		private final PactString word = new PactString();
		private final PactInteger one = new PactInteger(1);
		
		private final AsciiUtils.WhitespaceTokenizer tokenizer =
				new AsciiUtils.WhitespaceTokenizer();
		
		@Override
		public void map(PactRecord record, Collector<PactRecord> collector) {
			// get the first field (as type PactString) from the record
			PactString line = record.getField(0, PactString.class);
			
			// normalize the line
			AsciiUtils.replaceNonWordChars(line, ' ');
			AsciiUtils.toLowerCase(line);
			
			// tokenize the line
			this.tokenizer.setStringToTokenize(line);
			while (tokenizer.next(this.word))
			{
				// we emit a (word, 1) pair 
				this.outputRecord.setField(0, this.word);
				this.outputRecord.setField(1, this.one);
				collector.collect(this.outputRecord);
			}
		}
    }

//	@ConstantFields(fields={})
//	@OutCardBounds(lowerBound=0, upperBound=OutCardBounds.UNBOUNDED)
    public static class Map2 extends MapStub implements Serializable {
        private static final long serialVersionUID = 1L;
        // initialize reusable mutable objects
		private final PactRecord outputRecord = new PactRecord();
		private final PactString word = new PactString();
		private final PactInteger one = new PactInteger(1);
		
		private final AsciiUtils.WhitespaceTokenizer tokenizer =
				new AsciiUtils.WhitespaceTokenizer();
		
		@Override
		public void map(PactRecord record, Collector<PactRecord> collector) {
			// get the first field (as type PactString) from the record
			PactString line = record.getField(0, PactString.class);
			
			// normalize the line
			AsciiUtils.replaceNonWordChars(line, ' ');
			AsciiUtils.toLowerCase(line);
			
			// tokenize the line
			this.tokenizer.setStringToTokenize(line);
			while (tokenizer.next(this.word))
			{
				// we emit a (word, 1) pair 
				this.outputRecord.setField(0, this.word);
				this.outputRecord.setField(1, this.one);
				collector.collect(this.outputRecord);
			}
		}
    }

//	@ConstantFields(fields={0})
//	@OutCardBounds(lowerBound=1, upperBound=1)
//	@Combinable
    public static class CogroupSolution extends CoGroupStub implements Serializable {
        private static final long serialVersionUID = 1L;
        private final PactInteger cnt = new PactInteger();
        
        @Override
        public void coGroup(Iterator<PactRecord> records1, Iterator<PactRecord> records2, Collector<PactRecord> out) throws Exception {
            PactRecord element = null;
			int sum = 0;
			while (records1.hasNext()) {
				element = records1.next();
				PactInteger i = element.getField(1, PactInteger.class);
				sum += i.getValue();
			}
			while (records2.hasNext()) {
				element = records2.next();
				PactInteger i = element.getField(1, PactInteger.class);
				sum += i.getValue();
			}

			this.cnt.setValue(sum);
			element.setField(1, this.cnt);
			out.collect(element);
        }
        
        
    }

 //	@ConstantFields(fields={0})
//	@OutCardBounds(lowerBound=1, upperBound=1)
//	@Combinable
    public static class ReduceSolution extends ReduceStub implements Serializable {
        private static final long serialVersionUID = 1L;
        private final PactInteger cnt = new PactInteger();
		
		@Override
		public void reduce(Iterator<PactRecord> records, Collector<PactRecord> out) throws Exception {
			PactRecord element = null;
			int sum = 0;
			while (records.hasNext()) {
				element = records.next();
				PactInteger i = element.getField(1, PactInteger.class);
				sum += i.getValue();
			}

			this.cnt.setValue(sum);
			element.setField(1, this.cnt);
			out.collect(element);
		}
    }
    
    //	@ConstantFields(fields={})
//	@OutCardBounds(lowerBound=0, upperBound=OutCardBounds.UNBOUNDED)
    public static class MapTest extends MapStub  implements Serializable {
        private static final long serialVersionUID = 1L;
        @Override
        public void map(PactRecord record, Collector<PactRecord> collector) {
                collector.collect(record);
        }
    }
    
    

    /**
     * {@inheritDoc}
     */
    @Override
    public Plan getPlan(String... args) {
        // parse job parameters
        int noSubTasks = (args.length > 0 ? Integer.parseInt(args[0]) : 1);
        String dataInput = (args.length > 1 ? args[1] : """");
        String output = (args.length > 2 ? args[2] : """");

        FileDataSource source = new FileDataSource(TextInputFormat.class, dataInput, ""Input Records"");
		//source.setParameter(TextInputFormat.CHARSET_NAME, ""ASCII"");		// comment out this line for UTF-8 inputs
        MapContract map1 = MapContract.builder(Map1.class)
                .input(source)
                .name(""MAP1"")
                .build();
        MapContract map2 = MapContract.builder(Map2.class)
                .input(source)
                .name(""MAP2"")
                .build();
        
        /////////////////// REDUCER part
        ReduceContract reducer = ReduceContract.builder(ReduceSolution.class, PactString.class, 0)
                .input(map1, map2)
                .name(""REDUCER"")
                .build();
        FileDataSink out = new FileDataSink(RecordOutputFormat.class, output+""reduce"", reducer, ""REDUCEROUT"");
        RecordOutputFormat.configureRecordFormat(out)
                .recordDelimiter('\n')
                .fieldDelimiter('#')
                .lenient(true)
                .field(PactString.class, 0)
                .field(PactInteger.class, 1);
        
        //////////////////// COGROUP part
//        CoGroupContract cogroup = CoGroupContract.builder(CogroupSolution.class, PactString.class, 0, 0)
//                .input1(map1)
//                .input2(map2)
//                .name(""COGROUP"")
//                .build();
//        FileDataSink out2 = new FileDataSink(RecordOutputFormat.class, output+""cogroup"", cogroup, ""COGROUP"");
//        RecordOutputFormat.configureRecordFormat(out2)
//                .recordDelimiter('\n')
//                .fieldDelimiter('#')
//                .lenient(true)
//                .field(PactString.class, 0)
//                .field(PactInteger.class, 1);
        
        /////////////////// MAP part
//        MapContract maptest = MapContract.builder(MapTest.class)
//                .input(map1, map2)
//                .name(""MAP TEST"")
//                .build();
//        FileDataSink out3 = new FileDataSink(RecordOutputFormat.class, output + ""map"", maptest, ""MAP"");
//        RecordOutputFormat.configureRecordFormat(out3)
//                .recordDelimiter('\n')
//                .fieldDelimiter('#')
//                .lenient(true)
//                .field(PactString.class, 0)
//                .field(PactInteger.class, 1);
        
       
//        List<GenericDataSink> outlist = Arrays.asList((GenericDataSink) out, (GenericDataSink) out2);
       
//        Plan plan = new Plan(outlist, ""TestDoubleInput"");
//        Plan plan = new Plan(out2, ""TestDoubleInput"");
        Plan plan = new Plan(out, ""TestDoubleInput"");
//        Plan plan = new Plan(out3, ""TestDoubleInput"");
        plan.setDefaultParallelism(noSubTasks);
        return plan;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public String getDescription() {
        return ""Parameters: [noSubStasks] [input] [output]"";
    }
}

</code></pre>

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/192
Created by: [lukacsg|https://github.com/lukacsg]
Labels: bug, 
Created at: Tue Oct 22 13:51:21 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397561,,,Mon Jun 09 11:43:53 UTC 2014,,,,,,,,,,"0|i1wg2f:",397688,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;[Date: Tue Oct 22 13:53:15 CEST 2013, Author: [lukacsg|https://github.com/lukacsg]]

Maybe connecting to:
\#issue109: ([#109|https://github.com/stratosphere/stratosphere/issues/109] | [FLINK-109|https://issues.apache.org/jira/browse/FLINK-109]) 
\#issue124: ([#124|https://github.com/stratosphere/stratosphere/issues/124] | [FLINK-124|https://issues.apache.org/jira/browse/FLINK-124]) ;;;","09/Jun/14 11:43;github-import;[Date: Tue Oct 22 14:41:25 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Just pushed a reorg of some of the input channel logic, as part of another bug.

Please check if the bug still exists after commit [1228a5ec60452200e12cb77a9bb0623d340e87d2|https://github.com/stratosphere/stratosphere/commit/1228a5ec60452200e12cb77a9bb0623d340e87d2];;;","09/Jun/14 11:43;github-import;[Date: Tue Oct 22 14:51:18 CEST 2013, Author: [uce|https://github.com/uce]]

I just checked the code and I also ran into a deadlock with DOP > 1.

@lukacsg Does the code compile for you without the generic type arguments of the Iterator (or without casting the return value of next())? I added them to make the code compile but other than that didn't change anything.

I will check @StephanEwen's changes now and write again.;;;","09/Jun/14 11:43;github-import;[Date: Tue Oct 22 15:09:48 CEST 2013, Author: [uce|https://github.com/uce]]

I've tried it with @StephanEwen's latest commit [1228a5ec60452200e12cb77a9bb0623d340e87d2|https://github.com/stratosphere/stratosphere/commit/1228a5ec60452200e12cb77a9bb0623d340e87d2] and it fixed the issue for me. The job finishes without problems now.

We just recently detected the bug in ([#124|https://github.com/stratosphere/stratosphere/issues/124] | [FLINK-124|https://issues.apache.org/jira/browse/FLINK-124]), so the timing couldn't have been better. :smile: There was a problem with the union readers, which in your case are used when you specify multiple inputs to the reduce.

Thank you very much for posting this issue. We actually thought that it was only a problem in iterations.

Could you check out the latest version and confirm that this fixes your issue?;;;","09/Jun/14 11:43;github-import;[Date: Tue Oct 22 15:10:37 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

@uce Can you turn the example job into a test case and add it?;;;","09/Jun/14 11:43;github-import;[Date: Tue Oct 22 15:11:22 CEST 2013, Author: [uce|https://github.com/uce]]

Was just doing that. :-)

On Oct 22, 2013, at 3:10 PM, Stephan Ewen <notifications@github.com> wrote:

> @uce Can you turn the example job into a test case and add it?
> 
> —
> Reply to this email directly or view it on GitHub.
> ;;;","09/Jun/14 11:43;github-import;[Date: Tue Oct 22 18:05:05 CEST 2013, Author: [lukacsg|https://github.com/lukacsg]]

Yes, the fix works in the scenarios above:)

Thanks for the (very) quick response!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move pact-compiler tests to pact-tests,FLINK-191,12719361,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:43,09/Jun/14 11:43,14/Jul/23 05:57,09/Jun/14 11:43,,,,pre-apache,,,,,,,0,github-import,,"In order to simplify the mess of so many maven projects, we can reduce pact by one project if we move the compiler tests to the pact-tests project.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/191
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Tue Oct 22 12:12:57 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397560,,,Mon Jun 09 11:43:46 UTC 2014,,,,,,,,,,"0|i1wg27:",397687,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;[Date: Tue Oct 22 13:50:31 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

+1;;;","09/Jun/14 11:43;github-import;[Date: Wed Oct 23 11:49:06 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [0ea61264481a1db37dd6bb2ef4172ef629989807|https://github.com/stratosphere/stratosphere/commit/0ea61264481a1db37dd6bb2ef4172ef629989807];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate Pact Tests to TestBase2,FLINK-190,12719360,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:43,09/Jun/14 11:43,14/Jul/23 05:57,09/Jun/14 11:43,,,,pre-apache,,,,,,,0,github-import,,"The original TestBase has problems with tmp files that may cause test failures and frequently does not remove its tmp files. The new TestBase is also easier to use. 

All new tests should go against TestBase2. When an old test is fixed, it should be ported.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/190
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: core, testing, 
Created at: Tue Oct 22 11:15:28 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397559,,,Mon Jun 09 11:43:42 UTC 2014,,,,,,,,,,"0|i1wg1z:",397686,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;[Date: Tue May 06 20:32:47 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [dcda680f89dd055800dda89c6bd3b16567d4f35c|https://github.com/stratosphere/stratosphere/commit/dcda680f89dd055800dda89c6bd3b16567d4f35c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove IPv4 Stack Hint,FLINK-189,12719359,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:43,09/Jun/14 11:43,14/Jul/23 05:57,09/Jun/14 11:43,,,,pre-apache,,,,,,,0,github-import,,"Nephele has in many scripts hints to Java to run an IPv4 stack. If I recall correctly, the hint was necessary for the discovery manager broadcast-message-discovery in cloud setups.

Since we do not use that mechanism any more, we should be able to remove these hints from the scripts and test setups.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/189
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Mon Oct 21 22:42:46 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397558,,,Mon Jun 09 11:43:39 UTC 2014,,,,,,,,,,"0|i1wg1r:",397685,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;[Date: Thu Oct 24 00:10:52 CEST 2013, Author: [uce|https://github.com/uce]]

OK, this was kind of a fail. First I didn't check out the new branch from master, which I noticed after doing the PR. Then I noticed that @StephanEwen had conflicting changes on our master branch in a pom file.

So that's why it says that I reference this issue three times. :zzz: :punch: ;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 28 18:36:59 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed as of [361e469bb9ac742bd3c2ddaf930cdebe160300f1|https://github.com/stratosphere/stratosphere/commit/361e469bb9ac742bd3c2ddaf930cdebe160300f1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upload build SNAPSHOTS as tgz and as directory to DOPA server,FLINK-188,12719358,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:43,09/Jun/14 11:43,14/Jul/23 05:57,09/Jun/14 11:43,,,,pre-apache,,,,,,,0,github-import,,"This pull request will extend Travis so that on every push into stratosphere/master, a `stratosphere-0-4-SNAPSHOT.tgz` will be uploaded into  http://dopa.dima.tu-berlin.de/bin/

This is helpful if users don't want to build the system on their own.

Travis builds the files anyways to we can just ssh them to that server.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/188
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Oct 21 19:49:12 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;pull-request-188-5525666174669595735.patch;https://issues.apache.org/jira/secure/attachment/12649025/pull-request-188-5525666174669595735.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397557,,,Mon Jun 09 11:43:36 UTC 2014,,,,,,,,,,"0|i1wg1j:",397684,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;[Date: Mon Oct 21 21:25:53 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I cancelled the test for the second commit manually. Is being executed from this branch https://travis-ci.org/stratosphere/stratosphere/builds/12843365 
I have to use a separate branch to test this because I'm not deploying anything from pull requests.;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 22:09:14 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Please do not merge.;;;","09/Jun/14 11:43;github-import;[Date: Wed Oct 23 10:16:14 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay. The pull request is ready to merge.

This is how the binaries will look like and where they are placed: http://dopa.dima.tu-berlin.de/bin/
;;;","09/Jun/14 11:43;github-import;[Date: Wed Oct 23 11:00:04 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged in [9fef2778dde5c999e08144416bba9aa79dc74117|https://github.com/stratosphere/stratosphere/commit/9fef2778dde5c999e08144416bba9aa79dc74117];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fixed Nephele Visualization job selection not working on OS X,FLINK-187,12719357,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:43,09/Jun/14 11:43,14/Jul/23 05:57,09/Jun/14 11:43,,,,pre-apache,,,,,,,0,github-import,,"Partial fix for ([#172|https://github.com/stratosphere/stratosphere/issues/172] | [FLINK-172|https://issues.apache.org/jira/browse/FLINK-172])
See https://github.com/stratosphere/stratosphere/issues/172#issuecomment-26712187 for a description of the issue.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/187
Created by: [mleich|https://github.com/mleich]
Labels: 
Created at: Mon Oct 21 15:09:17 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;pull-request-187-6082968904651420.patch;https://issues.apache.org/jira/secure/attachment/12649024/pull-request-187-6082968904651420.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397556,,,Mon Jun 09 11:43:30 UTC 2014,,,,,,,,,,"0|i1wg1b:",397683,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;[Date: Mon Oct 21 16:03:59 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Can a second Mac guy verify this fix, since we do not have tests for that?;;;","09/Jun/14 11:43;github-import;[Date: Tue Oct 22 14:52:41 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged in [ebba4fd74adf7caa5ea5f9f46351ff90ee6c4cb0|https://github.com/stratosphere/stratosphere/commit/ebba4fd74adf7caa5ea5f9f46351ff90ee6c4cb0];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename groupReduce to groupMap in scala frontend,FLINK-186,12719356,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:43,09/Jun/14 11:43,14/Jul/23 05:57,09/Jun/14 11:43,,,,pre-apache,,,,,,,0,github-import,,"The title really says it all.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/186
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Mon Oct 21 14:59:38 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;pull-request-186-167112585199417459.patch;https://issues.apache.org/jira/secure/attachment/12649023/pull-request-186-167112585199417459.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397555,,,Mon Jun 09 11:43:25 UTC 2014,,,,,,,,,,"0|i1wg13:",397682,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;[Date: Mon Oct 21 15:02:31 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@aalexandrov pointed this out today and I also think that it is technically not a reduce (it is a map from a list of records to a record (or something else)). I also think, however, that this might confuse people coming from hadoop.

So what do you think?;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 15:10:51 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Can't we have both names?;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 15:12:47 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Yes, we could. But why?;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 15:14:28 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

To make everyone happy: The people coming from Hadoop and those with a background in functional programming;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 15:20:21 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

I think there should me one name for things. If there are two people will start wondering what the difference is and might therefore not know what to use.;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 15:41:53 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

I agree with Aljoscha in having only one name for each operation. However, we should carefully think about the naming of these and all other functions.

GroupMap basically describes how individual groups are processed (mapped one by one) but does not describe what is happening within a group. One could also interpret this as a mapper over group elements (which would be basically a regular mapper). On the other hand, the function reduce does not indicate that it is applied on individual groups (which is an assumption from the Hadoop context but not true from a functional perspective).

So somebody should have some thoughts about consistent naming of operators.
;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 16:01:32 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I also agree we should have one name. However, our main audience are most likely not going to be functional programmers and we should be careful not to confuse them too badly. So groupMap() is not the best name, though it is technically correct in my opinion.

In term of the Hadoop community, they would expect names like
1. combinableReduce()
2. reduce();
3. reduceAll();

We could go for someting like
1. reduce() -> more in the function sense als combining always two elements, aka, the only really relevant case of a combinable reduce
2. reduceAsGroup() -> Iterator over entire group
3. reduceAll() -> like the first, but this function exists directly on the data stream, i.e., it is not grouped. We could also just name it reduce(), because the distinction is anyways that it is not called on the grouped stream.

Any thoughts?;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 16:13:51 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Technically, there are 6 versions:
Directly on DataStream:
1. reduce() the proper functional reduce
2. mapGroup(): groupwise reduce (functional map) over all records
3. combinableMapGroup(): same as 2. but is combinable

On a GroupByDataStream:
4. reduce() the proper functional reduce
5. mapGroup(): groupwise reduce (functional map) over all records in groups grouped by a key
6 combinableMapGroup(): same as 5. but is combinable

Names are still up for debate, right now they are:
1. 2. 3. not offered in scala frontend, is it sensible to offer these?

4. reduce()
5. groupReduce()
6. combinableGroupReduce();;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 16:30:39 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

mapGroup is not a functional Map over group elements, since the records are not individually processed and the function holds a state.
How about going with fold instead of reduce for the pure functional reduce?

Btw. is there a way to specify a combine function that is different from its associated reduce function?;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 16:34:16 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

No, it is a functional map over groups (group => ?), that's what the name says. As to the second question: Not there is not right now.;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 16:42:54 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

That's exactly what I mean. ;-) It is hard to distinguish between the internal and external function, which might also be because of MapReduce's terminology. A map function is technically not mapping but applied by a map function...

Anyways, maybe calling the function mapGroup*s*() would help to make clear that we are mapping over all groupS and not over the elements of an individual group.;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 17:12:35 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

As to the six variants @aljoscha mentioned:

On Data Stream directly:

1.  reduce() the proper functional reduce  <-- **we need this one, also in Scala**
2.  mapGroup(): groupwise reduce (functional map) over all records <-- **Infeasible, does not parallelize**
3.  combinableMapGroup(): same as 2. but is combinable <-- **is a little more general than (1), but redundant in my opinion**

On a GroupByDataStream:

4.  reduce() the proper functional reduce <-- **most important one, should also be called like this**
5.  mapGroup(): <-- **The name groupReduce() seems more sensible to me, even though it is not correct in functional terms. Or reduceGroup(). Either should work.**
6.  combinableMapGroup(): same as 5. but is combinable <-- **again, somewhat redundant with (4), i would skip this**


So, we would end up with three forms of reduce, out of which two are called reduce and are really a functional reduce.

BTW: I think in Cascading, the case where we struggle with a name is ""groupBy"" and ""every"" to express going over all groups. (As opposed to ""Each"" which is going over all elements (map)).;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 17:22:02 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

So I will just keep the names and add the global reduce() (which is also inherently combinable). Right?;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 17:51:29 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

I had a short discussion with @StephanEwen and we propose the following:
- reduce() for the functional reduce on DataStream and GroupByDataStream. This is an implicitly combinable reduce function on two records at a time.
- reduceGroup() on GroupByDataStream. This is a non-combinable reduce using an iterator over the records of a group.
- reduceAll() on DataStream. This is a non-combinable reduce using an iterator over all records of the stream.

The corner case of a combinable reduce where the combine function is different from the reduce function is currently missing here, but might be added later.

;;;","09/Jun/14 11:43;github-import;[Date: Tue Oct 22 22:54:01 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Alright, I changed the names according to @fhueske's last comment and added reduce/reduceAll on DataStream.;;;","09/Jun/14 11:43;github-import;[Date: Wed Oct 23 00:41:22 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Changed according to @StephanEwen's remark about the reduce.;;;","09/Jun/14 11:43;github-import;[Date: Wed Oct 23 08:23:49 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

The comments are all valid, but can we just merge this for now to have the names in place. We can change the stuff that optimizes for performance later, can't we?;;;","09/Jun/14 11:43;github-import;[Date: Wed Oct 23 08:40:50 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Or should I add the combinable reduceGroup again?;;;","09/Jun/14 11:43;github-import;[Date: Wed Oct 23 08:52:55 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

If.a combinable Reduce is added, it might be a good idea to specify the reduce and combine functions separately.
I guess most cases where the combine function is identical to the reduce function can be expressed with the functional reduce.
Cases where this is not true, can simply provide the reduce function twice. That's not super nice, but that way we keep the interface concise and cover all cases.

What do you think?;;;","09/Jun/14 11:43;github-import;[Date: Wed Oct 23 09:52:36 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

yes, that would be good, maybe
input groupBy {} combineGroup {} reduceGroup {} ?;;;","09/Jun/14 11:43;github-import;[Date: Wed Oct 23 10:14:12 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

@aljoscha Looks good to me.

I also have some comments on the build-in aggregation functions proposed by @StephanEwen. What would be the result of such a function? One tuple per group with the computed aggregate as single value?

That would be probably not enough. Computing more than one aggregate on a group and preserving the grouping key are very common. So, if we provide something like this, we need a more expressive way to construct the output tuple, similar to the SELECT clause in SQL. To ensure deterministic results, we need to reason about the grouping key. Otherwise, non-key fields might be included in the resulting tuple without being aggregated.

Any thoughts?;;;","09/Jun/14 11:43;github-import;[Date: Wed Oct 23 11:05:12 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

For the aggregator functions, the result would be one tuple per group. If Multiple aggrgegates are build in a tuple we can list them all
```
something groupBy { x => (x._1, x._3) } aggregate( ( min, { _._2 } ), ( avg { _._4 }), (count, {_._5}) ) 
```

We can either report an error if fields are neither grouped nor aggregated (ansi SQL style) or simply use the field from some tuple (MYSQL style);;;","09/Jun/14 11:43;github-import;[Date: Wed Oct 23 11:07:56 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I will merge this for now.

@aljoscha Please fix the performance issues then in a separate pull request.;;;","09/Jun/14 11:43;github-import;[Date: Wed Oct 23 11:49:23 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [b117cbf9c3aff87f37b7138286415704f83595ab|https://github.com/stratosphere/stratosphere/commit/b117cbf9c3aff87f37b7138286415704f83595ab];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Information Frontend for JobManager,FLINK-185,12719355,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:42,09/Jun/14 11:43,14/Jul/23 05:57,09/Jun/14 11:43,,,,pre-apache,,,,,,,0,github-import,,"First draft of a webinterface (info server) for the JobManager. At the moment the info server is only started when using the shell script for starting the job manager (i.e. start-local.sh).
The default URL to access the info server is http://localhost:8081

Currently the info server gives an overview over all running jobs and tasks. Dependend Tasks are connected. For sake of human readability I restricted the shown states in the overview to:
started = CREATED || SCHEDULED || ASSIGNED || READY || STARTING
running = RUNNING
finished = FINISHING || FINISHED || CANCELING || CANCELED
failed = FAILED

Since this is only a first outline I'd appreciate your feedback!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/185
Created by: [markus-h|https://github.com/markus-h]
Labels: 
Milestone: Release 0.4
Assignee: [markus-h|https://github.com/markus-h]
Created at: Mon Oct 21 14:29:05 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;pull-request-185-7787820134013851384.patch;https://issues.apache.org/jira/secure/attachment/12649022/pull-request-185-7787820134013851384.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397554,,,Mon Jun 09 11:43:08 UTC 2014,,,,,,,,,,"0|i1wg0v:",397681,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:43;github-import;[Date: Mon Oct 21 14:47:32 CEST 2013, Author: [uce|https://github.com/uce]]

Some screenshots: 

![screen shot 2013-10-21 at 2 40 52 pm|https://f.cloud.github.com/assets/1756620/1372294/aaca1648-3a4e-11e3-8436-ffd346ab4e9b.png]
![screen shot 2013-10-21 at 2 43 59 pm|https://f.cloud.github.com/assets/1756620/1372295/aaf08bd4-3a4e-11e3-9e3a-06bd63a68d6f.png]
![screen shot 2013-10-21 at 2 44 05 pm|https://f.cloud.github.com/assets/1756620/1372296/aaf31002-3a4e-11e3-8ffc-cf9e9f11fa94.png]

Can you say what is further on the roadmap? I couldn't find an issue where we have a list of what we want. Seeing what is running right now is definetly nice to have.

I would like:
- List of finished job with runtimes. I think this will need some further changes in the JobManager to keep track of finished jobs, right?
- Log file viewer for debugging;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 15:13:54 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

Very nice first draft! I like the edges to show the task dependencies!

Here are my comments:
1. The ID of tasks and subtasks is not necessary, in my opinion.
2. Another column with the total number of subtasks would be nice (= sum of all for other columns). 
3. Auto reloading the page on event changes or in regular intervals (configurable?)
4. Having a graphical bar in each cell that shows how many of the total number of tasks are in this state would be easier to parse than reading numbers. Of course, the number should be displayed as well.
5. Showing the history of executed jobs as Ufuk suggested. This requires to store the history of all tasks which will be necessary in any case if the interface should also help debugging jobs.
6. Show for each subtask the timestamps of the state changes.
7. Displaying the log and std-out files of JobManager und TaskManagers would be very nice. Should be easy for JobManager logs if we assume that the webserver runs on the same node. For TaskManager logs and std-outs this will be more complicated, since these need to be transferred to the JM node.
8. The edges between tasks could encode the type of channel (network, in-memory) and the distribution pattern (forward, bipartite) maybe by color and dashing? Not sure if this information is available through the JM management interface. 

Order by importance and/or assumed complexity ;-);;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 15:15:42 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for your pull request!
@uce This is the issue that defines the task: https://github.com/stratosphere/stratosphere/issues/143

I think we should keep it simple first and show only the JobManager log. Transferring logs from the TaskManager with the JobManager needs two additional threads (on each side) plus the network traffic.;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 15:33:51 CEST 2013, Author: [markus-h|https://github.com/markus-h]]

Thanks for your comments! I think I'll do the easy changes and extensions in this pull request, so please don't accept it yet.;;;","09/Jun/14 11:43;github-import;[Date: Mon Oct 21 21:19:46 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

Two other small things, that would help to check the status of a running job with one look.
- Split the finished state into finished and canceled.
- Color the status columns according to the colors of the SWT frontend (Color codes are somewhere in the source code of nephele-visualization)
  - Starting -> Gray
  - Running -> Green
  - Finished -> Bĺue
  - Canceled -> Orange
  - Failed -> Red;;;","09/Jun/14 11:43;github-import;[Date: Fri Oct 25 00:00:12 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

I tried to make a screencast... setup stratosphere and start wordcount job... I managed to demonstrate on a macbook pro (using my own git repo that is faster than github) within <5 minutes. The major drawback is that after submitting the job the user is somehow lost and has to look into the filesystem manually.
I see this patch as one possible solution to make a screenscast that I'd like to publish. The other alternative would be to find a way to x-forward the nephele visualisation from the demo vm to the host computer. The latter solution would require a little more expertise from the end user, since he must manage to use xforward to display the visualisation.
Which mehtod woud you prefere for a screenscast? ;;;","09/Jun/14 11:43;github-import;[Date: Fri Oct 25 00:08:15 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

That's really cool!
I would prefer to use the new web-interface. @markus-h is going to change some design details tomorrow morning, so you should probably wait for 12 hours ;)

Concerning the nephele visualization: You don't necessary need to use x-forwarding.
a) you can start Stratosphere on your Mac Book! One of the core developers is using OS X (@uce), so I'm quite confident that it works on OS X as well (except that you have to manually add a dependency to get Nephele Visualization running on OSX :( )

b) you can connect from a local stratosphere binary into a remote (considering your VM as remote) stratosphere JobManager.
Set the jobManager address in the configuration on your local stratosphere to the ip:port of your VM's JobManager.;;;","09/Jun/14 11:43;github-import;[Date: Fri Oct 25 00:12:32 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

@rmetzger I won't have a chance to start before 4 pm tomorrow. Most probably I'll care about the screencasts on monday....;;;","09/Jun/14 11:43;github-import;[Date: Fri Oct 25 19:21:11 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

just a very first alpha version of a screencast (we probably need text and music)
http://streaming.dima.tu-berlin.de/podcast/mschubotz/strat-inst-01.mov (unfortunately some debian repo seems to be inresposinve so I had to re trigger vagrant provision manually)

http://streaming.dima.tu-berlin.de/podcast/mschubotz/strat-wordcount.mov
;;;","09/Jun/14 11:43;github-import;[Date: Tue Oct 29 22:17:10 CET 2013, Author: [markus-h|https://github.com/markus-h]]

I think I'm ready for the pull. I incorporated most of the feedback. I`ll to further extensions (like archive or debugging function) in a new request.;;;","09/Jun/14 11:43;github-import;[Date: Tue Oct 29 22:19:04 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Great. Thank you. I think @StephanEwen will merge the request.;;;","09/Jun/14 11:43;github-import;[Date: Sun Nov 03 10:50:33 CET 2013, Author: [markus-h|https://github.com/markus-h]]

I removed the wrong comments and changed the reading of the logfiles to a buffered reader;;;","09/Jun/14 11:43;github-import;[Date: Sun Nov 03 17:41:26 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged in [f3db2bc02121d668b64faaf759d4d62e4d984ebb|https://github.com/stratosphere/stratosphere/commit/f3db2bc02121d668b64faaf759d4d62e4d984ebb];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Design and Typo Fixes,FLINK-184,12719354,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:42,09/Jun/14 11:42,14/Jul/23 05:57,09/Jun/14 11:42,,,,pre-apache,,,,,,,0,github-import,,"- Removes the white bar
- Corrects navbar markup
- Fixes typo on frontpage
- Fixes scrollong on people page

With the help of @rmetzger. Thx!


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/184
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Sun Oct 20 18:23:28 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:42;github-import;pull-request-184-3113822663184025269.patch;https://issues.apache.org/jira/secure/attachment/12649021/pull-request-184-3113822663184025269.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397553,,,Mon Jun 09 11:42:53 UTC 2014,,,,,,,,,,"0|i1wg0n:",397680,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:42;github-import;[Date: Sun Oct 20 19:41:34 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

You can preview the changes here: http://robertmetzger.de/ozone/ ;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 19:50:36 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

I liked the old primary color and font. But maybe that's just me.;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 19:55:08 CEST 2013, Author: [uce|https://github.com/uce]]

The preview is not working for me.

I liked the nav bar color more before, but I think the link colors are better now.

We could also use the same old theme (for the navbar) and overwrite the link colors in the main part. Would that be better for you @aljoscha? If others think the same way, we should go for it.;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 19:58:13 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I just renamed my repo, so you have to use this url: http://robertmetzger.de/stratosphere/

There are some things that I like about the new design: the font sizes are better. you can differentiate between h1 and h2.
It seems that this layout works better with our website, than the previous one.;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 20:11:22 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good.

One comment: I liked the shade at the bottom edge of the image in the old design. Kind of acts as a separator between the parts of the page. Is it hard to add that back?;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 20:14:48 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

No. I think it is this part of the main.css
```css
.jumbotron {
[...]
      box-shadow: 0 3px 7px rgba(0, 0, 0, 0.2) inset, 0 -3px 7px rgba(0, 0, 0, 0.2) inset;
```;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 22:51:36 CEST 2013, Author: [uce|https://github.com/uce]]

I added the shadow back in with a new commit (which doesn't appear here yet... any idea why?). It's exactly what @rmetzger has written (but not in .jumbatron but .masthead.. except we really want it for all jumbatrons).

I also added some more emphasize to the active page in the navbar (it's bold now... in the old theme it had a different background color).

I noticed a **bug**: the highlighting script in the documentation page breaks the highlighting of the active page, e.g. the ""Documentation"" nav point is not highlighted even if it's active.;;;","09/Jun/14 11:42;github-import;[Date: Mon Oct 21 08:31:45 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

There is also a bug on the event page: Just scroll down, you'll see it;;;","09/Jun/14 11:42;github-import;[Date: Mon Oct 21 10:46:29 CEST 2013, Author: [uce|https://github.com/uce]]

There is also a problem on the people page with the subnav. When  you directly click the sublinks, the previous one gets highlighted. What should we do about it?

I've added another commit to properly center the jumbatron images.;;;","09/Jun/14 11:42;github-import;[Date: Wed Oct 23 00:05:50 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Some comments for the quickstart page (there are also typos on the current page):

Change heading: What do you want to do? --> What would you like to do?

There is the general question if we want to refer to the programs as __Jobs__ or as __Stratosphere Programs__. I have a slight bias towards the second option.

The sentence below the heading:
_""There are plenty of ways to start using Stratosphere. Install it, if you want to get to know the infrastructure. Application developers should start immediately with their favorite programming language.""_
could be changed to
_""There are plenty of ways to explore Stratosphere. Install it one one or more machines, if you want to get to know the infrastructure. Application developers can also start immediately with their favorite programming language and run programs locally from within their favorite IDE.""_

Under the Install Link:
_""Install on your computer or on a cluster to run jobs.""_ --> ""Install on your computer or on a cluster of machines to run Stratosphere programs distributed.""

Under the Scala Link:
Develop Stratosphere programs with Scala [maybe link to Scale here?] and experience Stratosphere's new easy, concise, and flexible programming abstraction. Run and debug your programs locally.

Under the JavaLink:
Write Stratosphere programs with the classic Java API. Run and debug your programs locally.



Any thoughts?;;;","09/Jun/14 11:42;github-import;[Date: Wed Oct 23 00:38:43 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

I mainly agree with Stephan's proposal and have a few additional remarks.

I would say, you implement a Stratosphere program but you execute a Stratosphere job (basically job = program ""instantiation"" with parameters). Not sure if the difference is to subtle though...

How about ""Install Stratosphere on one or more computers"" for the install link. The current and proposed versions are quite bulky in my opinion.

For the Scala text, I would drop ""easy"" and ""flexible"". Not because it is not easy and flexible but four attributes are too much and new and concise are the most appealing ones from my point of view.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed problems with scripts and default config (fixes #182),FLINK-183,12719353,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:42,09/Jun/14 11:42,14/Jul/23 05:57,09/Jun/14 11:42,,,,pre-apache,,,,,,,0,github-import,,"- added missing environment variables in scripts
- changed default port of web frontend back to 8080
- replaced bc command with 'expr +' for adding heap sizes

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/183
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Sun Oct 20 15:25:36 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:42;github-import;pull-request-183-8789096064396353173.patch;https://issues.apache.org/jira/secure/attachment/12649020/pull-request-183-8789096064396353173.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397552,,,Mon Jun 09 11:42:44 UTC 2014,,,,,,,,,,"0|i1wg0f:",397679,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:42;github-import;[Date: Sun Oct 20 20:12:23 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, I tested on my fresh ubuntu (https://github.com/stratosphere/stratosphere/issues/182) again. Issue is resolved.
Good to merge.;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 20:19:31 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged in [680dfb42fd5e74ac83a1ac00f6bab95a9158e167|https://github.com/stratosphere/stratosphere/commit/680dfb42fd5e74ac83a1ac00f6bab95a9158e167];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"start scripts depend on ""bc"", which is not available on a fresh ubuntu.",FLINK-182,12719352,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:42,09/Jun/14 11:42,14/Jul/23 05:57,09/Jun/14 11:42,,,,pre-apache,,,,,,,0,github-import,,"Introduced by https://github.com/stratosphere/stratosphere/pull/163

I'm running a fresh ubuntu 13.04 ([advertisement]from our new demo-vms: https://github.com/rmetzger/stratosphere-vms [/advertisement])

```./start-local.sh 
/home/vagrant/stratosphere/bin/../bin/nephele-jobmanager.sh: line 34: bc: command not found
```

I hope there are other ways in bash to add two numbers


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/182
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, 
Milestone: Release 0.4
Created at: Sun Oct 20 01:10:30 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397551,,,Mon Jun 09 11:42:39 UTC 2014,,,,,,,,,,"0|i1wg07:",397678,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:42;github-import;[Date: Sun Oct 20 01:13:22 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

More stuff is not working:

```
./start-pact-web.sh 
mkdir: cannot create directory ‘’: No such file or directory
Starting PACT Webfrontend
/home/vagrant/stratosphere/bin/../bin/pact-webfrontend.sh: line 75: /nephele-vagrant-pact-web.pid: Permission denied
```

;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 01:14:02 CEST 2013, Author: [uce|https://github.com/uce]]

I think, we would only need bc for floating point numbers.

Try:
```
a=10
b=10
c=`expr $a + $b`
```

Is this working?;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 01:15:15 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Yes
```
vagrant@packer-virtualbox-ubuntu:~/stratosphere/bin$ a=10
vagrant@packer-virtualbox-ubuntu:~/stratosphere/bin$ b=10
vagrant@packer-virtualbox-ubuntu:~/stratosphere/bin$ c=`expr $a + $b`
vagrant@packer-virtualbox-ubuntu:~/stratosphere/bin$ echo $c
20
```;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 01:17:55 CEST 2013, Author: [uce|https://github.com/uce]]

OK, this last one is a bug I seem to have introduced in the last commit changing the YAML config.

We have two options (I was going with the first one, but maybe we should rethink that):

- Add the following to `pact-webfrontend.sh`:

```
if [ -z ""${NEPHELE_PID_DIR}"" ]; then
    NEPHELE_PID_DIR=$(readFromConfig ${KEY_ENV_PID_DIR} ""${DEFAULT_ENV_PID_DIR}"" ${YAML_CONF})
fi
```
I chose to set the env variables only in the needed files, e.g. if you don't run a script which needs to set the pid dir, it won't get set.
- Put every env setting into `nephele-config.sh`. Not just the common ones. You know what I mean?;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 01:24:07 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

1) fixes the Problem.

You have to explain me the background more detailed. Lets discuss this in person (except somebody else wants to know the details, too).;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 12:42:52 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Can you also change back the port of the webinterface to 8080. All the existing documentation is using this port!;;;","09/Jun/14 11:42;github-import;[Date: Sun Oct 20 12:44:15 CEST 2013, Author: [uce|https://github.com/uce]]

yes, sure. must have been an accident. didn't mean to change it to 8088.

On Oct 20, 2013, at 12:42 PM, Robert Metzger <notifications@github.com> wrote:

> Can you also change back the port of the webinterface to 8080. All the existing documentation is using this port!
> 
> —
> Reply to this email directly or view it on GitHub.
> ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Travis Testing often hangs and terminates on correct tests,FLINK-181,12719351,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:42,09/Jun/14 11:42,14/Jul/23 05:57,09/Jun/14 11:42,,,,pre-apache,,,,,,,0,github-import,,"There seem to be frequent issues with Travis hanging in tests, even if these tests are singlethreaded and without I/O operations.

For example this test
```
https://s3.amazonaws.com/archive.travis-ci.org/jobs/12730622/log.txt
```
hung in the PactrecordITCase, which is a CPU intensive test that takes a while (normally some 10-20 seconds), but is single threaded and isolated.

It seems that the Travis VMs frequently freeze more or less, causing tests to take very long (longer than the threshold) and to get marked as erroneous. The build is then marked as failed, even though all tests cases are actually working.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/181
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: testing, 
Created at: Fri Oct 18 21:28:38 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397550,,,Mon Jun 09 11:42:32 UTC 2014,,,,,,,,,,"0|i1wfzz:",397677,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:42;github-import;[Date: Sun Oct 20 10:32:29 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I asked travis.;;;","09/Jun/14 11:42;github-import;[Date: Fri Oct 25 19:13:25 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Seems that we must accept that test spuriously fail. My last test failed because the maven artifact download took forever and the build timed out.

We may want to remove the build status image,though, because we will get failures marked publicly that are actually no failures.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State Maven 3 as a requirement,FLINK-180,12719350,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:42,09/Jun/14 11:42,14/Jul/23 05:57,09/Jun/14 11:42,,,,pre-apache,,,,,,,0,github-import,,"Our quickstart guides and other documentation states Maven as a requirement. 
However, we need to be more specific and ask for Maven 3.

Maven 2 is still around in some settings and causes builds to fail.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/180
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, documentation, 
Milestone: Release 0.4
Created at: Fri Oct 18 17:38:18 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397549,,,Mon Jun 09 11:42:28 UTC 2014,,,,,,,,,,"0|i1wfzr:",397676,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:42;github-import;[Date: Fri Oct 18 17:46:37 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Agree


On Fri, Oct 18, 2013 at 5:38 PM, Fabian Hueske <notifications@github.com>wrote:

> Our quickstart guides and other documentation states Maven as a
> requirement.
> However, we need to be more specific and ask for Maven 3.
>
> Maven 2 is still around in some settings and causes builds to fail.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/180>
> .
>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix stratosphere-dist to include the scala jars,FLINK-179,12719349,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:42,09/Jun/14 11:42,14/Jul/23 05:57,09/Jun/14 11:42,,,,pre-apache,,,,,,,0,github-import,,"Also fix an annoying bug that made tests fail because a dependency on
scala-reflect was wrongly in the ""test"" scope and also add an example of
how to create a PlanAssembler for a scala job in WordCount

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/179
Created by: [aljoscha|https://github.com/aljoscha]
Labels: scala api, 
Created at: Fri Oct 18 16:25:47 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:42;github-import;pull-request-179-37210471379644661.patch;https://issues.apache.org/jira/secure/attachment/12649019/pull-request-179-37210471379644661.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397548,,,Mon Jun 09 11:42:24 UTC 2014,,,,,,,,,,"0|i1wfzj:",397675,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:42;github-import;[Date: Fri Oct 18 16:29:26 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Looks good;;;","09/Jun/14 11:42;github-import;[Date: Fri Oct 18 19:35:40 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Right as always. :D I'll add the correct exclude and add DOP as a parameter.;;;","09/Jun/14 11:42;github-import;[Date: Sat Oct 19 17:05:34 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

I think this badboy should be ready for merge now. Care to have another look?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change name in base pom and version,FLINK-175,12719345,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:41,09/Jun/14 11:41,14/Jul/23 05:57,09/Jun/14 11:41,,,,pre-apache,,,,,,,0,github-import,,"End the version name craziness and go back to having 0.4-SNAPSHOT as
version number. Also change the name of the toplevel pom back to
stratosphere.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/175
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri Oct 18 10:54:46 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:41;github-import;pull-request-175-6230963146613573302.patch;https://issues.apache.org/jira/secure/attachment/12649018/pull-request-175-6230963146613573302.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397544,,,Mon Jun 09 11:41:53 UTC 2014,,,,,,,,,,"0|i1wfyn:",397671,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:41;github-import;[Date: Fri Oct 18 16:30:58 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Looks good.;;;","09/Jun/14 11:41;github-import;[Date: Fri Oct 18 20:31:22 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged in [2d44c7d1239e581360405435455a5ecdd36c920b|https://github.com/stratosphere/stratosphere/commit/2d44c7d1239e581360405435455a5ecdd36c920b];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add filter on Join and Cross in scala frontend,FLINK-174,12719344,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:41,09/Jun/14 11:41,14/Jul/23 05:57,09/Jun/14 11:41,,,,pre-apache,,,,,,,0,github-import,,"This closes ([#158|https://github.com/stratosphere/stratosphere/issues/158] | [FLINK-158|https://issues.apache.org/jira/browse/FLINK-158])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/174
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri Oct 18 10:37:28 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:41;github-import;pull-request-174-2870056455272426134.patch;https://issues.apache.org/jira/secure/attachment/12649017/pull-request-174-2870056455272426134.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397543,,,2014-06-09 11:41:44.0,,,,,,,,,,"0|i1wfyf:",397670,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Detect and reject programs consisting of multiple disjoint data flows,FLINK-173,12719343,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:41,09/Jun/14 11:41,14/Jul/23 05:57,09/Jun/14 11:41,,,,pre-apache,,,,,,,0,github-import,,"We need to catch programs consisting of multiple disjoint data flows in the pact layer to produce a meaningful message. Otherwise the JobManager will throw a GraphConversionException at runtime, stating that the job graph is not weakly connected.

Below is are two  simple example for such a program. The first is caught by the current logic, the second is not caught in general (depends on order in which sinks are added, as that determines the way in which this graph is augmented with auxiliary nodes until it has a single root node.

<pre>
 sink        sink
   ^          ^
   |          |
source      source
</pre>

<pre>
    (SINK 3) (SINK 1)   (SINK 2) (SINK 4)
        \     /             \     /
        (SRC A)             (SRC B)
</pre>

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/173
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: bug, optimizer, 
Milestone: Release 0.5
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Fri Oct 18 01:46:02 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397542,,,Mon Jun 09 11:41:42 UTC 2014,,,,,,,,,,"0|i1wfy7:",397669,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:41;github-import;[Date: Fri Oct 18 20:33:26 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [0d82ed2985dc51149629e387a1de861d631bce82|https://github.com/stratosphere/stratosphere/commit/0d82ed2985dc51149629e387a1de861d631bce82];;;","09/Jun/14 11:41;github-import;[Date: Wed Oct 23 10:43:19 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

The issue is not fixed.

I have a program that does not contain a disjoint data flow, but is rejected by the compiler.



```
Exception in thread ""main"" eu.stratosphere.pact.compiler.CompilerException: The given Pact program contains multiple disconnected data flows.
	at eu.stratosphere.pact.compiler.plan.SinkJoiner.computeUnclosedBranchStack(SinkJoiner.java:85)
	at eu.stratosphere.pact.compiler.PactCompiler$BranchesVisitor.postVisit(PactCompiler.java:1227)
	at eu.stratosphere.pact.compiler.PactCompiler$BranchesVisitor.postVisit(PactCompiler.java:1208)
	at eu.stratosphere.pact.compiler.plan.TwoInputNode.accept(TwoInputNode.java:812)
	at eu.stratosphere.pact.compiler.plan.TwoInputNode.accept(TwoInputNode.java:810)
	at eu.stratosphere.pact.compiler.plan.TwoInputNode.accept(TwoInputNode.java:810)
	at eu.stratosphere.pact.compiler.plan.TwoInputNode.accept(TwoInputNode.java:810)
	at eu.stratosphere.pact.compiler.plan.TwoInputNode.accept(TwoInputNode.java:810)
	at eu.stratosphere.pact.compiler.plan.TwoInputNode.accept(TwoInputNode.java:810)
	at eu.stratosphere.pact.compiler.plan.TwoInputNode.accept(TwoInputNode.java:810)
	at eu.stratosphere.pact.compiler.plan.TwoInputNode.accept(TwoInputNode.java:810)
	at eu.stratosphere.pact.compiler.PactCompiler.compile(PactCompiler.java:703)
	at eu.stratosphere.pact.compiler.PactCompiler.compile(PactCompiler.java:547)
	at eu.stratosphere.pact.client.LocalExecutor.executePlan(LocalExecutor.java:80)
	at eu.stratosphere.tlabs.risk.shared.Utils.execute(Utils.java:12)
	at eu.stratosphere.tlabs.risk.usecases.CustomerAggregations.main(CustomerAggregations.java:305)
```;;;","09/Jun/14 11:41;github-import;[Date: Wed Oct 23 11:09:19 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, it seems the checks are overly aggressive.

I will remove the check for now. Seems that we can rather live with a disjoint flow than falsely rejected plans.;;;","09/Jun/14 11:41;github-import;[Date: Wed Oct 23 11:17:00 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [119a023137b1d9e22a32e35973241e49b859ce30|https://github.com/stratosphere/stratosphere/commit/119a023137b1d9e22a32e35973241e49b859ce30];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nephele visualization not working on OS X,FLINK-172,12719342,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:41,09/Jun/14 11:41,14/Jul/23 05:57,09/Jun/14 11:41,,,,pre-apache,,,,,,,0,github-import,,"```
$ bin/nephele-visualization.sh 
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: no swt-gtk-3346 or swt-gtk in swt.library.path, java.library.path or the jar file
	at org.eclipse.swt.internal.Library.loadLibrary(Library.java:219)
	at org.eclipse.swt.internal.Library.loadLibrary(Library.java:151)
	at org.eclipse.swt.internal.C.<clinit>(C.java:21)
	at org.eclipse.swt.internal.Converter.wcsToMbcs(Converter.java:63)
	at org.eclipse.swt.internal.Converter.wcsToMbcs(Converter.java:54)
	at org.eclipse.swt.widgets.Display.<clinit>(Display.java:128)
	at eu.stratosphere.nephele.visualization.swt.SWTVisualizationGUI.<init>(SWTVisualizationGUI.java:125)
	at eu.stratosphere.nephele.visualization.swt.SWTVisualization.main(SWTVisualization.java:91)
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/172
Created by: [uce|https://github.com/uce]
Labels: bug, runtime, 
Created at: Thu Oct 17 17:28:08 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397541,,,Mon Jun 09 11:41:36 UTC 2014,,,,,,,,,,"0|i1wfxz:",397668,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:41;github-import;[Date: Fri Oct 18 09:50:53 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

To make it work, you need a different version of the swt dependency. Unfortunately, last time I checked, Maven central had no appropriate one for Mac OS x, but that may have changed now. 

That problem is one more reason to migrate to a Web-based solution. ;;;","09/Jun/14 11:41;github-import;[Date: Mon Oct 21 14:06:14 CEST 2013, Author: [mleich|https://github.com/mleich]]

Last time I checked, there's also a bug in the UI code, that prevents the user from selecting a job for visualization. If it's still there, I'll create a pull request.;;;","09/Jun/14 11:41;github-import;[Date: Mon Oct 21 14:08:40 CEST 2013, Author: [uce|https://github.com/uce]]

I used it last week on Ubuntu and was able to select different jobs for visualization.;;;","09/Jun/14 11:41;github-import;[Date: Mon Oct 21 14:21:01 CEST 2013, Author: [mleich|https://github.com/mleich]]

It's a Mac OSX specific bug.
```
SWTVisualizationGUI#widgetSelected(SelectionEvent arg0)
```
checks whether a widget has been selected twice within the double click interval (this is probable meant to just check whether a widget has been double clicked upon).
On Mac OSX, once a widget has been selected with one click, the system will _not_ issue another selection event if the widget didn't get deselected before.
In short: the double selection event this method checks for will never occur on Mac OSX unless you manage to selected, deselect, and select again the widget within the double click interval.
My dirty hack around the issue was to simply listen for single selection events.
What do you guys say?;;;","09/Jun/14 11:41;github-import;[Date: Mon Oct 21 14:27:38 CEST 2013, Author: [uce|https://github.com/uce]]

(In my last comment I thought that you mean that there was a general bug with selection.)

I think your outlined change is fine. It seems to be the simplest thing to do to get it working and since the long term goal is to get to a web-only interface, I wouldn't invest more time than necessary. 

;;;","09/Jun/14 11:41;github-import;[Date: Mon Oct 21 15:10:58 CEST 2013, Author: [mleich|https://github.com/mleich]]

In ([#187|https://github.com/stratosphere/stratosphere/issues/187] | [FLINK-187|https://issues.apache.org/jira/browse/FLINK-187]) you can find the fix for the job selection issue. ;;;","09/Jun/14 11:41;github-import;[Date: Mon Oct 21 19:17:38 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Is it this library? https://code.google.com/p/swt-repo/;;;","09/Jun/14 11:41;github-import;[Date: Tue Oct 22 14:54:39 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The select issue is fixed in [ebba4fd74adf7caa5ea5f9f46351ff90ee6c4cb0|https://github.com/stratosphere/stratosphere/commit/ebba4fd74adf7caa5ea5f9f46351ff90ee6c4cb0]

@rmetzger That is the library. I thought the cocoa versions are not compatible with the latest MacOS X, but a Mac guy would have to verify that.;;;","09/Jun/14 11:41;github-import;[Date: Tue Oct 22 14:55:16 CEST 2013, Author: [uce|https://github.com/uce]]

Will do :bowtie: ;;;","09/Jun/14 11:41;github-import;[Date: Tue Oct 22 14:56:19 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Eclipse comes with swt, so there is a version that works with the latest MacOS X. Let's try and find a repository for that.

Do we add maven profiles then for visualization on different platforms?;;;","09/Jun/14 11:41;github-import;[Date: Tue Oct 22 14:58:54 CEST 2013, Author: [mleich|https://github.com/mleich]]

Cocoa is just fine on the latest Mac OS X (didn't test the one that's supposed to be released today).

Stay away from anything that has ""Carbon"" in its name, though.
;;;","09/Jun/14 11:41;github-import;[Date: Fri Oct 25 14:02:58 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

If someone can tell me the exact coordinates of a OS X maven dependency, I can easily create a specialized profile for that.

You can activate profiles based on the operating system (http://maven.apache.org/guides/introduction/introduction-to-profiles.html ) (OSX users have to build their own statosphere then);;;","09/Jun/14 11:41;github-import;[Date: Fri Oct 25 15:04:19 CEST 2013, Author: [uce|https://github.com/uce]]

https://code.google.com/p/swt-repo/

(See your *own* comment :laughing:);;;","09/Jun/14 11:41;github-import;[Date: Tue May 06 20:28:58 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The visualization has been moved to stratosphere-experimental and will no longer be maintained. It is also license incompatible with the Apache License.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added getUserCodeClass to UserCodeWrapper,FLINK-167,12719337,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:41,09/Jun/14 11:41,14/Jul/23 05:57,09/Jun/14 11:41,,,,pre-apache,,,,,,,0,github-import,,"This pull request adds getUserCodeClass () to UserCodeWrapper to allow for quick access to the class.

Sopremo and Meteor make heavy use of reflection and often require access to the class of the user code.
Currently, the UserCodeWrapper only allows to return instances of the user code, not the class.
It would be possible to just call getClass() on the user code object, however this leads to unnecessary instantiations of the user code if it is provided as a class, not as object.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/167
Created by: [mleich|https://github.com/mleich]
Labels: 
Milestone: Release 0.4
Assignee: [aljoscha|https://github.com/aljoscha]
Created at: Wed Oct 16 16:21:02 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:41;github-import;pull-request-167-8727117601584071388.patch;https://issues.apache.org/jira/secure/attachment/12649016/pull-request-167-8727117601584071388.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397536,,,Mon Jun 09 11:41:07 UTC 2014,,,,,,,,,,"0|i1wfwv:",397663,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:41;github-import;[Date: Thu Oct 17 11:23:12 CEST 2013, Author: [mleich|https://github.com/mleich]]

This change would also make it easier to adapt ozone-testing to stratosphere 0.4-ozone;;;","09/Jun/14 11:41;github-import;[Date: Thu Oct 17 11:33:04 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

Looks good to me. 
Shouldn't be a problem as it only extends the interface and does not interfere with any existing code.
Will merge your request.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixup scala examples,FLINK-166,12719336,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:40,09/Jun/14 11:41,14/Jul/23 05:57,09/Jun/14 11:41,,,,pre-apache,,,,,,,0,github-import,,"This closes ([#150|https://github.com/stratosphere/stratosphere/issues/150] | [FLINK-150|https://issues.apache.org/jira/browse/FLINK-150])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/166
Created by: [aljoscha|https://github.com/aljoscha]
Labels: scala api, 
Milestone: Release 0.4
Created at: Wed Oct 16 16:12:34 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:41;github-import;pull-request-166-5259995597659501133.patch;https://issues.apache.org/jira/secure/attachment/12649015/pull-request-166-5259995597659501133.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397535,,,2014-06-09 11:40:56.0,,,,,,,,,,"0|i1wfwn:",397662,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added local strategy SORT to DataSinkTask and extended DataSinkTaskTest,FLINK-164,12719334,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:40,09/Jun/14 11:40,14/Jul/23 05:57,09/Jun/14 11:40,,,,pre-apache,,,,,,,0,github-import,,"Fixes ([#146|https://github.com/stratosphere/stratosphere/issues/146] | [FLINK-146|https://issues.apache.org/jira/browse/FLINK-146]) and adds support for locally sorted output. Globally sorted output is not supported yet.
This pull request only adds the local strategy SORT to the DataSinkTask and extends the DataSinkTaskTest.
The Pact optimizer had already set all required task configurations.

@StephanEwen Please check that the canceling logic and tests are OK.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/164
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Milestone: Release 0.4
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Wed Oct 16 00:06:57 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:40;github-import;pull-request-164-3259166630472161353.patch;https://issues.apache.org/jira/secure/attachment/12649014/pull-request-164-3259166630472161353.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397533,,,Mon Jun 09 11:40:51 UTC 2014,,,,,,,,,,"0|i1wfw7:",397660,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:40;github-import;[Date: Fri Oct 18 02:11:22 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged in [e51d6a3a3dc3a026e276c1df9d386ab411861b46|https://github.com/stratosphere/stratosphere/commit/e51d6a3a3dc3a026e276c1df9d386ab411861b46];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unified config and added YAML (syntax) support (#113),FLINK-163,12719333,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:40,09/Jun/14 11:40,14/Jul/23 05:57,09/Jun/14 11:40,,,,pre-apache,,,,,,,0,github-import,,"## conf/stratosphere-conf.yaml

Unified config to `stratosphere-conf.yaml`, which replaces the old `nephele-user.xml` and `pact-user.xml` and is also read by `bin/nephele-config.sh`.

The new file is not a 1:1 copy of the old two. Instead I removed some stuff I thought was not that important:
- instance configuration: I don't think anyone was using this other than the default instance? 
- multicast for broadcast: The comments for this feature said ""use at own risk"" :cake: 
- profiler impl classes: We only have two implementation for the profilers, didn't make sense to point to them in the config
- jobclient.polling.interval: Adjusted the system default from 5 to 2 (which was our config default before)
- pact.runtime.fs_timeout: Default is 0, which means wait forever (which should be ok as long as there are no problems)

@asteriosk, @ktzoumas I think we should add a configuration overview on the website (like http://spark.incubator.apache.org/docs/latest/configuration.html)

@StephanEwen: What about the multicasting (for broadcast) feature? Is it stable? Does anybody ever use it? Do we want to remove it?

The old XML files still work and could also be combined with the YAML file (where YAML overwrites keys in XML), but I think it is ok this way. Also note that the YAML config reader in `GlobalConfiguration` only uses YAML syntax for `key: value` mappings and nothing more.

## bin/nephele-config.sh

The bash script can also parse key-value pairs from the config, for example:

- JM heap size with `jobmanager.heap.mb`
- TM heap size with `taskmanager.heap.mb`

There are also some other things, which were supposed to be directly edited in the bash scripts before (see `KEYS_*` in `bin/nephele-config.sh`).

The defaults and environment variables are still the same, so there should not be any bad surprises.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/163
Created by: [uce|https://github.com/uce]
Labels: 
Milestone: Release 0.4
Created at: Tue Oct 15 14:27:47 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:40;github-import;pull-request-163-8882286183994855248.patch;https://issues.apache.org/jira/secure/attachment/12649013/pull-request-163-8882286183994855248.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397532,,,Mon Jun 09 11:40:45 UTC 2014,,,,,,,,,,"0|i1wfvz:",397659,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:40;github-import;[Date: Tue Oct 15 16:47:37 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Very nice. Even with Testcases. The new configs are nice to read.;;;","09/Jun/14 11:40;github-import;[Date: Fri Oct 18 02:14:49 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged in [7e0cebf36f0051ac1adf62abcbc1e171df2ea06d|https://github.com/stratosphere/stratosphere/commit/7e0cebf36f0051ac1adf62abcbc1e171df2ea06d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple joins with solutionset,FLINK-162,12719332,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:40,09/Jun/14 11:40,14/Jul/23 05:57,09/Jun/14 11:40,,,,pre-apache,,,,,,,0,github-import,,"A set of changes to allow more than one joins with the Solution Set.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/162
Created by: [vasia|https://github.com/vasia]
Labels: 
Milestone: Release 0.4
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Mon Oct 14 18:31:28 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:40;github-import;pull-request-162-1160826354533350517.patch;https://issues.apache.org/jira/secure/attachment/12649012/pull-request-162-1160826354533350517.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397531,,,Mon Jun 09 11:40:36 UTC 2014,,,,,,,,,,"0|i1wfvr:",397658,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:40;github-import;[Date: Thu Apr 24 23:35:48 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

Cleaned up and fixed as of [f863fa345a08ee908e936a02995767fdf1405ebb|https://github.com/stratosphere/stratosphere/commit/f863fa345a08ee908e936a02995767fdf1405ebb] and [fdaadf41c4c6c14f56557aa601fbdd4e0944386c|https://github.com/stratosphere/stratosphere/commit/fdaadf41c4c6c14f56557aa601fbdd4e0944386c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serializer fixes,FLINK-161,12719331,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:40,09/Jun/14 11:40,14/Jul/23 05:57,09/Jun/14 11:40,,,,pre-apache,,,,,,,0,github-import,,"This fixes ([#160|https://github.com/stratosphere/stratosphere/issues/160] | [FLINK-160|https://issues.apache.org/jira/browse/FLINK-160])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/161
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Mon Oct 14 15:44:09 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:40;github-import;pull-request-161-1692884071255942.patch;https://issues.apache.org/jira/secure/attachment/12649011/pull-request-161-1692884071255942.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397530,,,Mon Jun 09 11:40:30 UTC 2014,,,,,,,,,,"0|i1wfvj:",397657,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:40;github-import;[Date: Mon Oct 14 15:45:15 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@fhueske I checked it with your code example in ([#160|https://github.com/stratosphere/stratosphere/issues/160] | [FLINK-160|https://issues.apache.org/jira/browse/FLINK-160]), it does indeed work now.;;;","09/Jun/14 11:40;github-import;[Date: Mon Oct 14 15:49:53 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

:-D Thanks for the fix!;;;","09/Jun/14 11:40;github-import;[Date: Mon Oct 14 18:08:08 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@rmetzger The travis build for this pull request is done and all tests passed as can be seen when clicking on the link here. Github, however, does not seem to notice. Could this be because of the move to stratosphere/stratosphere?;;;","09/Jun/14 11:40;github-import;[Date: Mon Oct 14 18:16:46 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, I'm going to ask the Travis guys tomorrow evening. (I don't have time sooner);;;","09/Jun/14 11:40;github-import;[Date: Wed Oct 16 08:54:33 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

@aljoscha it seems that the Travis integration is working again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala Frontend: PactList Serialization Bug,FLINK-160,12719330,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:40,09/Jun/14 11:40,14/Jul/23 05:57,09/Jun/14 11:40,,,,pre-apache,,,,,,,0,github-import,,"I tried to reimplement the Java version of the ComputeEdgeDegree example as a Scala Pact program.
The job requires to return multiple records from a reducer (one per input record). However, currently the Scala reducer can only return a single record. I tried to solve this issue by returning a Scala list from the Reducer that contains all emitted records and to flat out that list using a flatMap. The Scala list seems to be correctly translated into the PactList type, but during its serialization a NullPointerException is raised.

The code of the failing Scala job is here:

https://github.com/fhueske/ozone/blob/fc914eadc3507cc27a2eb4891b3f491df0cb0e7a/pact-scala/pact-scala-examples/src/main/scala/eu/stratosphere/scala/examples/graph/ComputeEdgeDegrees2.scala

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/160
Created by: [fhueske|https://github.com/fhueske]
Labels: bug, 
Milestone: Release 0.4
Assignee: [aljoscha|https://github.com/aljoscha]
Created at: Mon Oct 14 14:43:19 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397529,,,2014-06-09 11:40:22.0,,,,,,,,,,"0|i1wfvb:",397656,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove commons-io from pact-clients.,FLINK-157,12719327,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:40,09/Jun/14 11:40,14/Jul/23 05:57,09/Jun/14 11:40,,,,pre-apache,,,,,,,0,github-import,,"- it does not seem to be used there
- It is in conflict with commons-io from hadoop yarn (which uses commons-io 2.1)
- The coordinates of the artifact changed (and Andre and I forgot to rename it)
- My Telecom use-cases were breaking because of this: My code expected a method from commons-io:2.1 but 1.3.2 was loaded. I had no troubles loading my code with pact-client.sh, I did, however, not check if the pact webinterface still works. But eclipse did not show any errors after removing commons-io.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/157
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Sun Oct 13 19:23:53 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:40;github-import;pull-request-157-7963367225018826434.patch;https://issues.apache.org/jira/secure/attachment/12649010/pull-request-157-7963367225018826434.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397526,,,Mon Jun 09 11:40:08 UTC 2014,,,,,,,,,,"0|i1wfun:",397653,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:40;github-import;[Date: Mon Oct 14 01:51:59 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

If I recall correctly, commons-io was indeed a dependency from the web-interface. I think either jetty or commons-fileupload required it.;;;","09/Jun/14 11:40;github-import;[Date: Mon Oct 14 01:58:19 CEST 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

I had to configure this for a different project last week, commons-fileupload seems to require commons-io: http://commons.apache.org/proper/commons-fileupload/dependencies.html.

We should check whether jar upload still works with the new configuration.;;;","09/Jun/14 11:40;github-import;[Date: Mon Oct 14 10:43:32 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay. You guys are right. The dependency is required.
The fileupload works fine even without the dependency, because other projects (hadoop) depend on commons-io 2.1, so the jar is in the classpath.
I'll see if everything works with commons-io 2.1, so that we have only one version of it for the whole project. The problem is that our yarn build probably pulls a different version of commons-io.;;;","09/Jun/14 11:40;github-import;[Date: Mon Oct 14 10:57:57 CEST 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

Then I suggest to define a ```commons-io.property``` with different values for hadoop_v1 and hadoop_yarn as part of the maven profiles and use it in ```pact-clients```, or simply define an appropriate entry in the ```<dependencyManagement>``` section in the root pom profiles. 

It's a hack but it will solve the problem, and we should not spend too much time on such issues.;;;","09/Jun/14 11:40;github-import;[Date: Fri Oct 18 02:15:29 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged in [7e0cebf36f0051ac1adf62abcbc1e171df2ea06d|https://github.com/stratosphere/stratosphere/commit/7e0cebf36f0051ac1adf62abcbc1e171df2ea06d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve pom-tooling scripts,FLINK-156,12719326,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:39,09/Jun/14 11:40,14/Jul/23 05:57,09/Jun/14 11:40,,,,pre-apache,,,,,,,0,github-import,,"The pom-generate script now allows to specifiy a filename for the generated pom .This is useful if one wants to overwrite the existing pom with the other profile as default!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/156
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Milestone: Release 0.4
Created at: Sun Oct 13 17:16:44 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:39;github-import;pull-request-156-5326115542682242120.patch;https://issues.apache.org/jira/secure/attachment/12649009/pull-request-156-5326115542682242120.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397525,,,Mon Jun 09 11:40:00 UTC 2014,,,,,,,,,,"0|i1wfuf:",397652,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:40;github-import;[Date: Fri Oct 18 02:16:05 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged in [02ff4805b845a1b5b20fb0a008304307cd600033|https://github.com/stratosphere/stratosphere/commit/02ff4805b845a1b5b20fb0a008304307cd600033];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bug with user-code wrappers,FLINK-155,12719325,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:38,09/Jun/14 11:38,14/Jul/23 05:57,09/Jun/14 11:38,,,,pre-apache,,,,,,,0,github-import,,"The problem was that the user code wrapper was not deserialized from the
config using the user-code class loader. Now we jave a custom
ObjectInputStream that uses the user-code class loader to resolve
classes.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/155
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri Oct 11 16:42:12 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:38;github-import;pull-request-155-1536230503082517602.patch;https://issues.apache.org/jira/secure/attachment/12649008/pull-request-155-1536230503082517602.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397524,,,Mon Jun 09 11:38:54 UTC 2014,,,,,,,,,,"0|i1wfu7:",397651,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:38;github-import;[Date: Fri Oct 11 16:42:44 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@rmetzger Could you please verify that this does indeed fix the bug you had.;;;","09/Jun/14 11:38;github-import;[Date: Sat Oct 12 10:19:29 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Yes. I'm going to do that today.;;;","09/Jun/14 11:38;github-import;[Date: Sat Oct 12 11:55:56 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Fix verified! Thank you @aljoscha!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cancelling tests do not work properly and spuriously fails,FLINK-154,12719324,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:38,09/Jun/14 11:38,14/Jul/23 05:57,09/Jun/14 11:38,,,,pre-apache,,,,,,,0,github-import,,"The cancelling tests do not actually report errors while cancelling and may hang.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/154
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: bug, testing, 
Milestone: Release 0.5
Created at: Fri Oct 11 16:18:31 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397523,,,Mon Jun 09 11:38:47 UTC 2014,,,,,,,,,,"0|i1wftz:",397650,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:38;github-import;[Date: Sat Oct 12 10:26:09 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

(This is a little brain dump from yesterdays meeting with Stephan. I hope I remember everything correctly)
The idea of the cancelling tests is to verify that failed tasks are properly shut down.

The current implementation of the testcase is a bit wrong. 
There is a main thread (a) that starts another thread (b) which is going to be cancelled eventually. (a) waits until (b) has been cancelled.. 
But there are junit-checks in the code of thread (b) which sometimes cause the thread (b) to die. (a) then waits indefinitely for (b) to stop, which causes stalling tests as reported in https://github.com/dimalabs/ozone/issues/94.
What needs to be done is to somehow report test errors to thread (a), so that junit properly recognizes the failure. ;;;","09/Jun/14 11:38;github-import;[Date: Sat Oct 12 11:42:13 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

There is a thread (c) that calls cancel on the tasks. (a) then waits for (b) and (c) to finish.

When the cancelling logic is faulty itself, (c) will terminate without letting the testcase know that something went wrong.;;;","09/Jun/14 11:38;github-import;[Date: Fri Nov 22 14:23:12 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Today, I got this NPE
```
14:19:08,130 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from CANCELING to CANCELED for task Match on string key (6/64)
14:19:08,134 ERROR eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask       - java.lang.NullPointerException
        at eu.stratosphere.nephele.services.memorymanager.spi.DefaultMemoryManager.release(DefaultMemoryManager.java:337)
        at eu.stratosphere.pact.runtime.hash.BuildFirstHashMatchIterator.abort(BuildFirstHashMatchIterator.java:171)
        at eu.stratosphere.pact.runtime.task.MatchDriver.cancel(MatchDriver.java:185)
        at eu.stratosphere.pact.runtime.task.RegularPactTask.cancel(RegularPactTask.java:416)
        at eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask.cancelOrKillExecution(RuntimeTask.java:227)
        at eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask.cancelExecution(RuntimeTask.java:186)
        at eu.stratosphere.nephele.taskmanager.TaskManager$1.run(TaskManager.java:399)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:679)

14:19:08,136 WARN  eu.stratosphere.pact.runtime.task.RegularPactTask             - PACT code cancelled.: Match on string key (13/64)
```
It occurred on a failed task.

The `availableMemory` of the HashMap seem to contain `null` memory segments.

@StephanEwen: Should I open a new issue for this or is it related to this issue?;;;","09/Jun/14 11:38;github-import;[Date: Fri Nov 22 14:44:40 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Open an issue, please...
Am 22.11.2013 14:23 schrieb ""Robert Metzger"" <notifications@github.com>:

> Today, I got this NPE
>
> 14:19:08,130 INFO  eu.stratosphere.nephele.execution.ExecutionStateTransition    - TM: ExecutionState set from CANCELING to CANCELED for task Match on string key (6/64)
> 14:19:08,134 ERROR eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask       - java.lang.NullPointerException
>         at eu.stratosphere.nephele.services.memorymanager.spi.DefaultMemoryManager.release(DefaultMemoryManager.java:337)
>         at eu.stratosphere.pact.runtime.hash.BuildFirstHashMatchIterator.abort(BuildFirstHashMatchIterator.java:171)
>         at eu.stratosphere.pact.runtime.task.MatchDriver.cancel(MatchDriver.java:185)
>         at eu.stratosphere.pact.runtime.task.RegularPactTask.cancel(RegularPactTask.java:416)
>         at eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask.cancelOrKillExecution(RuntimeTask.java:227)
>         at eu.stratosphere.nephele.taskmanager.runtime.RuntimeTask.cancelExecution(RuntimeTask.java:186)
>         at eu.stratosphere.nephele.taskmanager.TaskManager$1.run(TaskManager.java:399)
>         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
>         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>         at java.lang.Thread.run(Thread.java:679)
>
> 14:19:08,136 WARN  eu.stratosphere.pact.runtime.task.RegularPactTask             - PACT code cancelled.: Match on string key (13/64)
>
> It occurred on a failed task.
>
> The availableMemory of the HashMap seem to contain null memory segments.
>
> @StephanEwen <https://github.com/StephanEwen>: Should I open a new issue
> for this or is it related to this issue?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/stratosphere/stratosphere/issues/154#issuecomment-29071859>
> .
>;;;","09/Jun/14 11:38;github-import;[Date: Fri Dec 06 23:44:48 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [7b0c53b4c501690866523468f6c3e7f06438658c|https://github.com/stratosphere/stratosphere/commit/7b0c53b4c501690866523468f6c3e7f06438658c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala examples for Triangle Enumeration,FLINK-153,12719323,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:38,09/Jun/14 11:38,14/Jul/23 05:57,09/Jun/14 11:38,,,,pre-apache,,,,,,,0,github-import,,"

Added first versions of the Scala examples for triangle enumeration.
The examples include:

    Annotation of edges with the corresponding vertex degrees (+ unique projection of vertexes and duplicate removal)
    Enumeration of triangles on edges with degrees.

The examples perform the same job as the corresponding Java versions. However, they use different strategies (joins instead of reduce in some places). Exactly following the Java strategies didn't work for a Scala novice ;-)
However, this might be changed later.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/153
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Fri Oct 11 12:32:52 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:38;github-import;pull-request-153-2357596890115259713.patch;https://issues.apache.org/jira/secure/attachment/12649007/pull-request-153-2357596890115259713.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397522,,,Mon Jun 09 11:38:42 UTC 2014,,,,,,,,,,"0|i1wftr:",397649,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:38;github-import;[Date: Fri Oct 11 14:14:02 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Looks good. Could you also add tests similar to the KMeansIterativeITCase and friends?;;;","09/Jun/14 11:38;github-import;[Date: Fri Oct 11 14:14:35 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Was it hard to figure out how to do stuff, by the way?;;;","09/Jun/14 11:38;github-import;[Date: Fri Oct 11 14:38:26 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

The solution I committed was easy to implement. I also tried to follow more closely the Java version but could not do it.

I have a couple remarks, that will share with you after the weekend. But overall a very nice work!

I suggest that the other examples are also done by people that haven't used the Scala frontend yet. That should get you more feedback.


Aljoscha Krettek <notifications@github.com> wrote:

>Was it hard to figure out how to do stuff, by the way?
>
>—
>Reply to this email directly or view it on GitHub.￼
>
;;;","09/Jun/14 11:38;github-import;[Date: Fri Oct 11 14:41:27 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Yes, I agree. Plus it's less work for me ... :smile: ;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 14 17:32:38 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

I prepared integration tests for the example jobs but they fail due to some missing Scala class. This seems to be some integration issues. The example jobs themselves are working.
I'm waiting for a fix/hint from @aljoscha to merge them together with the tests.;;;","09/Jun/14 11:38;github-import;[Date: Tue Oct 15 17:13:48 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

Integration tests should work now. 
Let's wait for Travis and merge if all tests pass.;;;","09/Jun/14 11:38;github-import;[Date: Tue Oct 15 17:21:46 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@fhueske You added the additional dependency on scala-reflect. Was that what fixed the weird TypeCreator error we saw?;;;","09/Jun/14 11:38;github-import;[Date: Tue Oct 15 17:44:43 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

@aljoscha Yes, that solved the problem. 
I did not further investigate the issue after the bug was fixed. Do you see any problems with the added dependency?;;;","09/Jun/14 11:38;github-import;[Date: Tue Oct 15 17:57:57 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Nah, I don't see a problem with that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala examples for Triangle Enumeration,FLINK-152,12719322,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:38,09/Jun/14 11:38,14/Jul/23 05:57,09/Jun/14 11:38,,,,pre-apache,,,,,,,0,github-import,,"Added first versions of the Scala examples for triangle enumeration.
The examples include:
- Annotation of edges with the corresponding vertex degrees (+ unique projection of vertexes and duplicate removal)
- Enumeration of triangles on edges with degrees.

The examples perform the same job as the corresponding Java versions. However, they use different strategies (joins instead of reduce in some places). Exactly following the Java strategies didn't work for a Scala novice ;-)
However, this might be changed later.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/152
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Fri Oct 11 12:25:14 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:38;github-import;pull-request-152-7633093409789845631.patch;https://issues.apache.org/jira/secure/attachment/12649006/pull-request-152-7633093409789845631.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397521,,,Mon Jun 09 11:38:32 UTC 2014,,,,,,,,,,"0|i1wftj:",397648,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:38;github-import;[Date: Fri Oct 11 12:36:09 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

I did pay attention and included commits into the pull request that did not belong here.
A new pull request was opened ([#153|https://github.com/stratosphere/stratosphere/issues/153] | [FLINK-153|https://issues.apache.org/jira/browse/FLINK-153]).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updated maven surefire plugin version.,FLINK-151,12719321,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:38,09/Jun/14 11:38,14/Jul/23 05:57,09/Jun/14 11:38,,,,pre-apache,,,,,,,0,github-import,,"Updated version of surefile plugin both for build and reporting configurations.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/151
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Fri Oct 11 12:11:27 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:38;github-import;pull-request-151-1017920549848591859.patch;https://issues.apache.org/jira/secure/attachment/12649005/pull-request-151-1017920549848591859.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397520,,,2014-06-09 11:38:25.0,,,,,,,,,,"0|i1wftb:",397647,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Readme outdated,FLINK-149,12719319,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:38,09/Jun/14 11:38,14/Jul/23 05:57,09/Jun/14 11:38,,,,pre-apache,,,,,,,0,github-import,,"The readme file is outdated.

It's said
cd stratosphere-dist/target/stratosphere-dist-0.2-ozone-bin/stratosphere-0.2-ozone/
this folder does not exist.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/149
Created by: [physikerwelt|https://github.com/physikerwelt]
Labels: bug, 
Created at: Thu Oct 10 19:16:35 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397518,,,Mon Jun 09 11:38:20 UTC 2014,,,,,,,,,,"0|i1wfsv:",397645,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:38;github-import;[Date: Thu Oct 10 19:23:59 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Thanks for the hint. :smile: ;;;","09/Jun/14 11:38;github-import;[Date: Thu Oct 10 19:26:28 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I guess the fix is not valid, since our version (currently!) is 0.4-ozone-SNAPSHOT ;)

One other thing, you should probably add to the README is a little note that your eclipse plugin links are only for eclipse x.y.;;;","09/Jun/14 11:38;github-import;[Date: Thu Oct 10 19:35:02 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Ok sorry, maybe I was a bit to quick there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing tests,FLINK-148,12719318,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:38,09/Jun/14 11:38,14/Jul/23 05:57,09/Jun/14 11:38,,,,pre-apache,,,,,,,0,github-import,,"The junit tests still fail.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/148
Created by: [physikerwelt|https://github.com/physikerwelt]
Labels: 
Created at: Thu Oct 10 19:15:41 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397517,,,Mon Jun 09 11:38:15 UTC 2014,,,,,,,,,,"0|i1wfsn:",397644,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:38;github-import;[Date: Thu Oct 10 19:20:42 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Which tests are failing?

@all Shouldn't the ""mvn verify"" that is run on each pull request catch them?;;;","09/Jun/14 11:38;github-import;[Date: Thu Oct 10 19:24:33 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I am aware of one unit test that contains an error. But if executed with maven surefire in the specified version (of the surefire plugin) the overall tests succeed. 

We run the integration tests using ""mvn clean verify"".
Please add more details on how to reproduce the error you reported.

@aljoscha: Yes, they should, but they actually don't. There is one case: If a test has an error, it is not failing. Maven (with the surefire plugin we're using) accepts errors. There is a pending pull request (https://github.com/dimalabs/ozone/pull/110) that fixes maven and the only error we're currently having. I'll most likely merge the pull request tomorrow, after discussing it with @StephanEwen.
;;;","09/Jun/14 11:38;github-import;[Date: Thu Oct 10 19:32:47 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

Jenkins compiles with goal package. Feel free to take a look to the jenkins configuration. (If you are at DIMA.)
;;;","09/Jun/14 11:38;github-import;[Date: Fri Oct 18 16:40:45 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I think the issue is fixed. Please reopen if not.;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 11:08:25 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

Unfortunately there are still many commits that lead to failing tests. It would be nice to check that before merging something to the master branch.
See
http://gerrit.formulasearchengine.com:8081/#/q/status:open,n,z
for details ;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 11:18:29 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you post specific tests and stack traces?

We have the issue that some tests become very low (almost freeze) on travis such that builds get killed and marked as broken, but that seems to be to a large extend an issue travis has with very cpu intensive tests.

Locally, tests have been running successfully for a long time now, at least on my machine, and as far as I know also for @aljoscha , @rmetzger , and @uce ;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 11:37:21 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

Ok.
One example:
https://github.com/stratosphere/stratosphere/commit/c583dc0527a179764032558692888376202c007d
The log is quite long...
[INFO] ------------------------------------------------------------------------
[INFO] Building pact-tests 0.4-ozone-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ pact-tests ---
[INFO] Deleting file set: /var/lib/jenkins/jobs/ozone-build/workspace/pact/pact-tests/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-maven) @ pact-tests ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ pact-tests ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /var/lib/jenkins/jobs/ozone-build/workspace/pact/pact-tests/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ pact-tests ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ pact-tests ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 6 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ pact-tests ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 129 source files to /var/lib/jenkins/jobs/ozone-build/workspace/pact/pact-tests/target/test-classes
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /var/lib/jenkins/jobs/ozone-build/workspace/pact/pact-tests/src/test/java/eu/stratosphere/pact/test/scalaPactPrograms/EnumTrianglesOnEdgesWithDegreesITCase.java:[22,42] method getPlan in class eu.stratosphere.scala.ScalaPlanAssembler cannot be applied to given types;
  required: scala.collection.Seq<java.lang.String>
  found: java.lang.String,java.lang.String,java.lang.String,java.lang.String
  reason: actual and formal argument lists differ in length
[ERROR] /var/lib/jenkins/jobs/ozone-build/workspace/pact/pact-tests/src/test/java/eu/stratosphere/pact/test/scalaPactPrograms/ComputeEdgeDegreesITCase.java:[22,43] method getPlan in class eu.stratosphere.scala.ScalaPlanAssembler cannot be applied to given types;
  required: scala.collection.Seq<java.lang.String>
  found: java.lang.String,java.lang.String,java.lang.String,java.lang.String
  reason: actual and formal argument lists differ in length
;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 11:38:51 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

For me it looks like Gerrit is also checking intermediate commits.
We only merge branches into Stratosphere master that do pass the tests. However, we do not ensure that each individual commit of a branch passes. Hence, the latest version of the main branch should pass all tests, however intermediate states might not.

I am pretty sure that my commits which Gerrit correctly marks as failing, were merged from a non-faining branch into Stratosphere master. So, I had commits with failures but these had been resolved before the branch was merged into the master branch.;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 11:40:33 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

So, the question is whether we want all commits to pass all tests or only ensure this for the latest version.;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 11:41:36 CEST 2013, Author: [uce|https://github.com/uce]]

Isn't the actual stategy to squash all commits to a single one (via rebase) so that we always go from stable to stable?

Or am I misunderstanding something about the problem here? :see_no_evil: ;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 11:42:27 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I would suggest that we use rebase to squash issues into a single commit before merging. Ore one commit per logical unit of that issue. As such, these commits would be passing. But that is only a guideline, I would not enforce it strictly.

@physikerwelt Can you check your gerrit configuration to only test the HEAD after a version change is detected?;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 11:44:00 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

I didn't do that. That's why there were failing commits. 
But sure, that policy sounds reasonable and should be communicated. ;-);;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 11:44:38 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I would say no, because this would make our work quite annoying. Pull requests are our unit of work-transfer. So each pull request is a transition from a stable state to another stable state.
I don't want to change my workflow for a system. The systems have to respect our workflow.

I'm against forcing each pull request into one commit. Each commit should be a logical unit of work. If a pull request (e.g. a new feature) contains a lot of changes, it sure can contain multiple commits.;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 13:02:12 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

Adjusting workflows to systems seems to be wrong approach in general.
The traceable git-history is written in terms of commits.
Pull requests are specific to github and hard to trace via git log. That's an argument for stable commits.

However, most important for me, is that there is stable master branch.
It would be nice if there would be way to navigate through the former stable state in order to trace problems.
Maybe fine grained version tags would be a good idea.
;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 14:31:20 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

I must admit that that I have also managed to create a broken commit now and then. In general, though, I think it is nice to have every commit be a working commit.

I do also realise however that right now it is the wild west out there. We are frantically fixing bugs and cranking out commits, so, I don't know ... :smile_cat: ;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 15:03:12 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

As far as I have understood there is only one major difference between  gerrit/jenkins workflow and github/travis:
In gerrit/jenkins commits can be merged, if they pass the specified tests. Travis seems to check the commits/pull requests after they were merged.
For the github workflow the commits seem to be atomic. In gerrit you change your commit until it passes all tests. That way it glorifies the history;-)
However, beside the pre/post checking the systems are equal except for isomorphism:
commit <-> pull request
patch set <-> commit
;;;","09/Jun/14 11:38;github-import;[Date: Mon Oct 21 16:52:43 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I had a little conversation with @physikerwelt and we resolved the issue.

We won't change the current development process, so that new contributors are not discouraged to help. But we advise everyone to commit only stable code.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes for Combiner in Compiler and Runtime,FLINK-145,12719315,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:37,09/Jun/14 11:37,14/Jul/23 05:57,09/Jun/14 11:37,,,,pre-apache,,,,,,,0,github-import,,"Fixes for ([#138|https://github.com/stratosphere/stratosphere/issues/138] | [FLINK-138|https://issues.apache.org/jira/browse/FLINK-138]).
1. Compiler assigns the same DOP to Combiners as to its preceding node
2. Fixed local strategy for stand-alone (non-chained) combine driver

In addition, some strategy selection related exceptions in compiler and runtime are extended to make them more usable.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/145
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Thu Oct 10 15:17:35 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:37;github-import;pull-request-145-7227989282060916239.patch;https://issues.apache.org/jira/secure/attachment/12649004/pull-request-145-7227989282060916239.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397514,,,Mon Jun 09 11:37:49 UTC 2014,,,,,,,,,,"0|i1wfrz:",397641,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:37;github-import;[Date: Thu Oct 10 15:47:12 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

Damn it. Forgot to adapt the tests to the new local strategy. 
Will fix this soon.;;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 16:57:09 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Why did you close the PR? It looks good (you can fix the build issues, the new commit will be added automatically to the PR);;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 17:09:34 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

OK, didn't know that. Will reopen the request once the fixes are available.;;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 17:10:35 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, perfect.
You should add yourself to the CONTRIBUTORS file.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add meteor to the new ""stratosphere"" account.",FLINK-144,12719314,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:37,09/Jun/14 11:37,14/Jul/23 05:57,09/Jun/14 11:37,,,,pre-apache,,,,,,,0,github-import,,"Meteor stays in its own repository (unlike Scala).

It will be part of the 0.4 release of Stratosphere.

Add travis and sonatype deployment.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/144
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Milestone: Release 0.4
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Thu Oct 10 11:23:21 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397513,,,2014-06-09 11:37:40.0,,,,,,,,,,"0|i1wfrr:",397640,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Unable to compile workset iteration, URGENT",FLINK-142,12719312,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:37,09/Jun/14 11:37,14/Jul/23 05:57,09/Jun/14 11:37,,,,pre-apache,,,,,,,0,github-import,,"Hi, I tried to create a simple incrementally iterative algorithm for the homework for the course here in Trento.

Unfortunately, I can't get the compilation of workset iterations to run.

I get an exception saying that I try to change the partition in the Match updating the solutionset (which I think I don't do :))

Could anyone have a look at the code? Its very urgent as I have to hand out the assignment this week!

Code is here: https://github.com/stratosphere/graphmining-tutorial/blob/master/src/main/java/de/tuberlin/dima/aim3/graphmining/chainletter/ChainLetter.java

```
Exception in thread ""main"" eu.stratosphere.pact.compiler.CompilerException: No viable strategies for solution set delta found during plan enumeration. Possible reason: Partitioning is not preserved when matching with the solution set. Missing annotation?
	at eu.stratosphere.pact.compiler.plan.WorksetIterationNode.instantiate(WorksetIterationNode.java:303)
	at eu.stratosphere.pact.compiler.plan.TwoInputNode.addLocalCandidates(TwoInputNode.java:534)
	at eu.stratosphere.pact.compiler.plan.TwoInputNode.getAlternativePlans(TwoInputNode.java:465)
	at eu.stratosphere.pact.compiler.plan.DataSinkNode.getAlternativePlans(DataSinkNode.java:251)
	at eu.stratosphere.pact.compiler.PactCompiler.compile(PactCompiler.java:712)
	at eu.stratosphere.pact.compiler.PactCompiler.compile(PactCompiler.java:547)
	at eu.stratosphere.pact.client.LocalExecutor.executePlan(LocalExecutor.java:80)
	at eu.stratosphere.pact.client.LocalExecutor.execute(LocalExecutor.java:137)
```


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/142
Created by: [sscdotopen|https://github.com/sscdotopen]
Labels: bug, 
Milestone: Release 0.4
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Wed Oct 09 21:05:59 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397511,,,Mon Jun 09 11:37:31 UTC 2014,,,,,,,,,,"0|i1wfrb:",397638,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:37;github-import;[Date: Thu Oct 10 10:28:05 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Hey!

I think the issue is exactly as the error message says: You are missing an annotation that tells the optimizer that the keys are preserved.

Please add a @ConstantFieldsFirst to the class containing the MatchStub and try it again!;;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 10:37:04 CEST 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

Why do I have to add this annotation? If the Match changes the partitioning, our iteration implementation doesn't work anyways, right?


Adding the annotation gives me another error:

```
Exception in thread ""main"" eu.stratosphere.pact.compiler.CompilerException: An error occurred while translating the optimized plan to a nephele JobGraph: An error occurred while translating the optimized plan to a nephele JobGraph: null
	at eu.stratosphere.pact.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:555)
	at eu.stratosphere.pact.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:96)
	at eu.stratosphere.pact.compiler.plan.candidate.DualInputPlanNode.accept(DualInputPlanNode.java:222)
	at eu.stratosphere.pact.compiler.plan.candidate.SingleInputPlanNode.accept(SingleInputPlanNode.java:141)
	at eu.stratosphere.pact.compiler.plan.candidate.OptimizedPlan.accept(OptimizedPlan.java:161)
	at eu.stratosphere.pact.compiler.plantranslate.NepheleJobGraphGenerator.compileJobGraph(NepheleJobGraphGenerator.java:165)
	at eu.stratosphere.pact.client.LocalExecutor.executePlan(LocalExecutor.java:83)
	at eu.stratosphere.pact.client.LocalExecutor.execute(LocalExecutor.java:137)
	at de.tuberlin.dima.aim3.graphmining.chainletter.ChainLetter.main(ChainLetter.java:126)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
Caused by: eu.stratosphere.pact.compiler.CompilerException: An error occurred while translating the optimized plan to a nephele JobGraph: null
	at eu.stratosphere.pact.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:555)
	at eu.stratosphere.pact.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:96)
	at eu.stratosphere.pact.compiler.plan.candidate.SolutionSetPlanNode.accept(SolutionSetPlanNode.java:76)
	at eu.stratosphere.pact.compiler.plan.candidate.DualInputPlanNode.accept(DualInputPlanNode.java:220)
	at eu.stratosphere.pact.compiler.plan.candidate.WorksetIterationPlanNode.acceptForStepFunction(WorksetIterationPlanNode.java:175)
	at eu.stratosphere.pact.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:389)
	... 13 more
Caused by: java.lang.NullPointerException
	at eu.stratosphere.pact.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:450)
	... 18 more
```;;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 10:50:15 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

That is exactly the point. If the match changes the partitioning, the current code path does not work. It would need to add an extra partitioning step and update the solution set elements deferred ( ([#21|https://github.com/stratosphere/stratosphere/issues/21] | [FLINK-21|https://issues.apache.org/jira/browse/FLINK-21]) ). Since that has not been implemented, yet, the compiler needs to reject the job.

What are suggestions to handle this until the deferred index updates are implemented?

I'll look into the other bug...
;;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 11:47:43 CEST 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

I'm just saying I found it not user-friendly to have to use the annotation. It doesn't add any value to my code and prevented me from using the Workset Iteration :) ;;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 14:35:17 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The system can currently not handle situations where the matcher with the solution set changes the key. We can either say we accept all jobs, optimistically assuming they do not change the key, and potentially produce erroneous results or error reports, or we reject these jobs and only accept the ones where we are sure (the ones with the annotation). We currently does the later, but we can change that.;;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 14:38:14 CEST 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

I think that what is going to happen is that users will simply add the annotation to their code even if their code does not adhere the rules :);;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 14:46:41 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Okay, so we need to get the general code path working as fast as possible. Then the annotation is a performance thing only, and like all semantic assertions, will cause incorrect behavior if the code actually violates it.;;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 19:45:02 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The bug is fixed in [0e53fe573f5bbbd6f33647f746d99fe70153ece7|https://github.com/stratosphere/stratosphere/commit/0e53fe573f5bbbd6f33647f746d99fe70153ece7];;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 19:49:32 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@StephanEwen By the way, if you refer to an issue in your commit message or pull request the issue is closed automatically with a link to the commit that closed it. See here: https://help.github.com/articles/closing-issues-via-commit-messages ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip failing tests,FLINK-141,12719311,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:37,09/Jun/14 11:37,14/Jul/23 05:57,09/Jun/14 11:37,,,,pre-apache,,,,,,,0,github-import,,"Change-Id: I[9d57cfd42913c47eb8551ae156c987fcd65459b7|https://github.com/stratosphere/stratosphere/commit/9d57cfd42913c47eb8551ae156c987fcd65459b7]

see http://dopa-2.dima.tu-berlin.de:8080/job/ozone-build/61/ (from the dima subnet) for details.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/141
Created by: [physikerwelt|https://github.com/physikerwelt]
Labels: 
Created at: Wed Oct 09 19:36:01 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:37;github-import;pull-request-141-5302968263750434055.patch;https://issues.apache.org/jira/secure/attachment/12649003/pull-request-141-5302968263750434055.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397510,,,Mon Jun 09 11:37:23 UTC 2014,,,,,,,,,,"0|i1wfr3:",397637,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:37;github-import;[Date: Wed Oct 09 19:48:15 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for your pull request to Stratosphere / Ozone.

I can't access your link (and we'd like to be a transparent open source project), so I don't know if there is any discussion or explanation there.. 

There is already a pending pull request that fixes the failing test: https://github.com/dimalabs/ozone/pull/110
;;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 09:49:59 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

Hi Robert,
I feel sorry for the restriction... I requested to open the ports for the Jenkins server more than 2 month ago... but TUbit is slow.
It would be nice, if the master branch would not have failing tests. This is further evidence that an open push model does not work.
Again...I'd vote for a combination of gerrit and jenkins... that's the way many large open source projects go.
Best Moritz;;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 09:51:49 CEST 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

which open source projects use this combination?;;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 10:13:01 CEST 2013, Author: [physikerwelt|https://github.com/physikerwelt]]

e.g. typo3, mediawiki, 
http://en.wikipedia.org/wiki/Gerrit_(software)
https://wiki.jenkins-ci.org/pages/viewpage.action?pageId=58001258
I see that we don't want to run a public jenkins instance, since the modified junit test code is a major security risk. Anyhow, pushing something to a public master branch that does not pass the tests is not very helpful. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migration of wiki documentation,FLINK-140,12719309,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:37,09/Jun/14 11:37,14/Jul/23 05:57,09/Jun/14 11:37,,,,pre-apache,,,,,,,0,github-import,,"Hi,

today I made a couple of scripts and migrated the old dokuwiki to the github wiki:
https://github.com/dimalabs/ozone/wiki

Some issues remain however. We have to:
 * find broken links to code that are broken because of the refactoring of Ozone
 * resize images when needed 

Thus, I would like to ask you to do a pass over the documentation in order to find possible problems and fix them. You can declare here (with a comment) which part of the documentation you would be taking over. 

We will close this issue when this is done. 

cheers,
Asterios



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/140
Created by: [asteriosk|https://github.com/asteriosk]
Labels: documentation, 
Milestone: Release 0.4
Created at: Wed Oct 09 19:23:04 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397508,,,Mon Jun 09 11:37:17 UTC 2014,,,,,,,,,,"0|i1wfqn:",397635,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:37;github-import;[Date: Wed Oct 09 19:23:12 CEST 2013, Author: [asteriosk|https://github.com/asteriosk]]

I am taking over the following pages and their ""subtree"":
-   **[Setup &  Configuration|#""wiki:""]**
    -   [Cluster
        Setup|#""wiki:clustersetup""]
    -   [Cloud
        Setup|#""wiki:cloudsetup""]
    -   [Instances and Parallel
        Scheduling|#""wiki:instancesandscheduling""]
    -   [Configuration
        Reference|#""wiki:configreference""]
    -   [Using Nephele with Channel
        Compression|#""wiki:nephelecompression""];;;","09/Jun/14 11:37;github-import;[Date: Thu Oct 10 11:09:17 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

We decided to create a comprehensive introduction/tutorial on the website. 
Scala is the primary languages for the examples.
Assigned to @asteriosk (and @uce)


We are going to have a new Java programming interface that is similar to Scala's. ;;;","09/Jun/14 11:37;github-import;[Date: Mon Nov 25 22:12:09 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I will do a first pass over the Java API section once the renaming ([#257|https://github.com/stratosphere/stratosphere/issues/257] | [FLINK-257|https://issues.apache.org/jira/browse/FLINK-257]) is done. ;;;","09/Jun/14 11:37;github-import;[Date: Fri Dec 13 00:56:57 CET 2013, Author: [fhueske|https://github.com/fhueske]]

I ported and reworked the programming model section of the documentation: ([#334|https://github.com/stratosphere/stratosphere/issues/334] | [FLINK-334|https://issues.apache.org/jira/browse/FLINK-334]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MatchTaskTest sometimes fails with ConcurrentModificationException,FLINK-139,12719308,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:37,09/Jun/14 11:37,14/Jul/23 05:57,09/Jun/14 11:37,,,,pre-apache,,,,,,,0,github-import,,"According to https://github.com/dimalabs/ozone/issues/94 there are at least 3 cases on Travis-CI where the MatchTaskTest failed.

Someone needs to investigate this issue.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/139
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, core, testing, 
Milestone: Release 0.4
Created at: Wed Oct 09 18:48:47 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397507,,,Mon Jun 09 11:37:12 UTC 2014,,,,,,,,,,"0|i1wfqf:",397634,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:37;github-import;[Date: Thu Oct 10 21:39:26 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Refer to ([#94|https://github.com/stratosphere/stratosphere/issues/94] | [FLINK-94|https://issues.apache.org/jira/browse/FLINK-94]) for detailed comments.;;;","09/Jun/14 11:37;github-import;[Date: Fri Dec 06 23:55:09 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [7b0c53b4c501690866523468f6c3e7f06438658c|https://github.com/stratosphere/stratosphere/commit/7b0c53b4c501690866523468f6c3e7f06438658c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ozone-YARN-Integration ,FLINK-137,12719306,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:36,09/Jun/14 11:37,14/Jul/23 05:57,09/Jun/14 11:37,,,,pre-apache,,,,,,,0,github-import,,"Description:
-----------------

- added the 'nephele-yarn' module
    * implemented the 'InstanceManager' interface

- added the 'nephele-common-yarn' module
    * implemented the client API, 'YarnJobClient.java'
    * implemented simple demo job, 'eu.stratosphere.nephele.example.yarn'
  
- rewritten the 'DiscoveryService.java' (by Daniel)

- added in the 'JobManager.java' an additional execution mode 'YARN'

- TaskManager can now determine IPC/DATA ports, if it is bootstrapped in a yarn container
    * multiple TMs can exist on a single cluster node

- a lot of minor changes

YARN Integration:
--------------------------

Two resource managers, in concrete MESOS and YARN, were examined that provide the capability to run multiple instances of the data parallel execution framework Stratosphere alongside other applications (e.g. Hadoop, Hyracks) in a cluster/cloud infrastructure to efficiently share common computing resources. After a detailed analysis of both frameworks we believe that YARN has several significant advantages over MESOS which are listed in the following:

* YARN is primarily written in Java. MESOS is written in C++.
* YARN uses simple unix processes as computing resource abstraction. MESOS uses Linux containers that imply a high overhead.
* YARN integrates pluggable resource schedulers.
* YARN is able to directly handle rack and machine locality in resource requests.
* YARN is going to be the basis for Hadoop MapReduce going forward and other frameworks (e.g. Hyracks).

Due to its essential advantages we decided to integrate the YARN cluster manager in the Stratosphere scheduling kernel, instead of the MESOS framework. To understand the integration mechanism of YARN in Stratosphere do we need to depict the core components of both frameworks

YARN core components:
------------------------------------

* ResourceManager (RM): The central component in YARN. The RM exists as singleton instance in the infrastructure and manages all available computing resources. It acts as pure resource negotiator for all concurrent running systems.

* NodeManager (NM): On every node in the infrastructure runs a NM instance that runs a liveness protocol and do the concrete container allocation with permission of the RM.

* ApplicationMaster (AM): The AM exists per running job and is responsible for allocating/scheduling worker containers (by requesting the RM) and monitoring the job lifecycle.

* YARN Client: The client submits the job in form of starting an AM instance and handing over the concrete executable job. It also performs monitoring of the running jobs.

Stratosphere core components:
----------------------------------------------

* JobManager (JM): The JM is the central component in Stratosphere’s execution layer Nephele and is responsible for allocating resources (part of a subcomponent called InstanceManager) and scheduling/monitoring submitted jobs.

* TaskManager (TM): The TM has similar responsibilities as the NodeManager in YARN and also executes jobs in cooperation with other TMs.

* Job Client: Submits a job and monitors the job execution.

Nephele-YARN Interaction:
---------------------------------------

In the integration of YARN in Nephele we operate YARN as a a meta-level framework that encloses certain components of Nephele. This approach has the advantage that the current code base of Nephele is not dramatically changed and needs only to be extended. A further and more important advantage is the clean process isolation among multiple concurrent Stratosphere instances and other applications (e.g. Hadoop, Hyracks).

The sequence diagram depicts the basic interaction:
![yarn_nephele_seqdia|https://f.cloud.github.com/assets/5525371/1298201/64f55e46-30f0-11e3-9bad-57203b00c881.png)

Interaction Description: The Client establishes a YARN context and requests an ApplicationMaster container. Inside the allocated container, a new instance of Stratosphere’s JobManager is created. The client submits a Stratosphere Job to the JobManager instance, which delegates the resource requests to its own InstanceManager subcomponent, which is a YARN-based implementation of the Nephele InstanceManager interface. The resource requests are then delegated from there to the YARN ResourceManage node, that actually allocates the required containers (if available]. Inside each allocated container a Stratosphere TaskManager is bootstrapped which receives and executes tasks of the submitted job from the the JobManager.

![yarn_nephele_component_interaction|https://f.cloud.github.com/assets/5525371/1298189/1ac85602-30f0-11e3-9fd5-cd3993b38ce3.png)

1.) Client sends a request to the YARN Resource Manager to create an Application Master (AM)
2.) ResourceManager (RM) creates the AM container. The JobManager is bootstrapped inside the AM. (via a shell command that was defined in the Client-RM request). The JobManager is started in YARN mode, with the YARN InstanceManager.
3.) Client sends a Job to the JobManager. The JobManager determines the resource needs.
4.) The JobManagers calls the RM (via InstanceManager.requestInstance) and allocated the determined number of containers.
5.) RM starts the YARN containers. The TaskManagers are bootstrapped inside these containers. The TaskManager have to determine their RPC and data ports (because containers can be scheduled to the same node).
6.) The boostrapped TaskManagers are registered by the JobManager via the first heart-beat signal.
7.] The JobManager can begin to schedule the execution verticies on the TaskManagers.



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/137
Created by: [TobiasHerb|https://github.com/TobiasHerb]
Labels: 
Milestone: Release 0.4
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Wed Oct 09 16:39:23 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:36;github-import;pull-request-137-8390511368058153618.patch;https://issues.apache.org/jira/secure/attachment/12649002/pull-request-137-8390511368058153618.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397505,,,Mon Jun 09 11:37:00 UTC 2014,,,,,,,,,,"0|i1wfpz:",397632,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:36;github-import;[Date: Thu Oct 10 10:34:02 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Open Issues:
 - Memory issues (@TobiasHerb)
 - Integration into Pact job execution (@rmetzger)
 - Integration tests issues.(@rmetzger)

Long-term enhancements:
 - Caching of Yarn Containers;;;","09/Jun/14 11:36;github-import;[Date: Wed Dec 04 13:24:06 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

If anyone is interested in continuing this work, you can find the code where I left of in this branch: https://github.com/rmetzger/stratosphere/tree/yarn_rebased_remove_discovery_rebased (I used this pull request as a basis for a ""deep"" yarn integration (changing Stratosphere's internals))

The code there is well-integrated into the overall stratosphere build system (separate maven packages, integration into build profiles, clean/well-integrated run-scripts); 
I changed the job submission so that one Container comes up first (the compiler has to wait until yarn deployed a container) and did quite some debugging on our cluster.

Current Status:
- I tested the code in normal operation (not on yarn) and it seems to work
- the code expects hadoop 2.0.5-alpha, not the stable (and officially released): 2.2.0
- Yarn works on my local machine (with only one container allocation)
- Yarn does not work on our cluster. (It is able to start the JobManager and TaskTracker and to start processing the job but then it fails because it somehow re-registers the TaskTracker. As I said, the whole TaskTracker registration mechanism is new.
In order to get it to run on the cluster, it expects that there is a shared file system (I'm  sure it will not work if you just invoke it on one machine.)

Please refer to https://github.com/stratosphere/stratosphere/pull/312 for the new Yarn integration (which does basically not change Stratosphere itself, it just adds a control-layer to execute regular Stratosphere code). The code there takes care of all the deployment aspects (code distribution etc.).

I suggest to close this pull request for now. (The work here can still be used if someone wants a deeper integration and a more on-demand behavior of Stratosphere on yarn.);;;","09/Jun/14 11:37;github-import;[Date: Thu Dec 05 16:35:40 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Subsumed by ([#312|https://github.com/stratosphere/stratosphere/issues/312] | [FLINK-312|https://issues.apache.org/jira/browse/FLINK-312]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problems with WorksetConnectedComponents,FLINK-136,12719305,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:36,09/Jun/14 11:36,14/Jul/23 05:57,09/Jun/14 11:36,,,,pre-apache,,,,,,,0,github-import,,"(1) The comments in WorksetConnectedComponents obviously refer to K-Means

(2) The wiring of the iteration seems to be wrong, the next workset is produced by joinWithNeighbors not by updateComponentId

    iteration.setNextWorkset(updateComponentId);
    iteration.setSolutionSetDelta(updateComponentId);

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/136
Created by: [sscdotopen|https://github.com/sscdotopen]
Labels: bug, 
Milestone: Release 0.4
Created at: Wed Oct 09 15:45:27 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397504,,,Mon Jun 09 11:36:51 UTC 2014,,,,,,,,,,"0|i1wfpr:",397631,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:36;github-import;[Date: Tue Oct 22 15:09:25 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Comments are fixed in [83e209b037dcf1f2c53d3a006c765bf5d77df133|https://github.com/stratosphere/stratosphere/commit/83e209b037dcf1f2c53d3a006c765bf5d77df133]

The wiring is correct. There are multiple ways to express the job. You can rotate the operators around The loop formed by the workset feedback, since the workset feedback is logically just another edge. In your variant, the join with the edges is the last operation before the feedback, in the example variant, it is the first after.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add scala code to mainline,FLINK-135,12719304,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:36,09/Jun/14 11:36,14/Jul/23 05:57,09/Jun/14 11:36,,,,pre-apache,,,,,,,0,github-import,,"This closes ([#118|https://github.com/stratosphere/stratosphere/issues/118] | [FLINK-118|https://issues.apache.org/jira/browse/FLINK-118])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/135
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Milestone: Release 0.4
Created at: Wed Oct 09 14:21:20 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:36;github-import;pull-request-135-7245087831188042336.patch;https://issues.apache.org/jira/secure/attachment/12649001/pull-request-135-7245087831188042336.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397503,,,Mon Jun 09 11:36:46 UTC 2014,,,,,,,,,,"0|i1wfpj:",397630,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:36;github-import;[Date: Wed Oct 09 14:32:30 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Very nice. But the tests are disabled, right?;;;","09/Jun/14 11:36;github-import;[Date: Wed Oct 09 14:52:11 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Yes, they are are disabled. I just wanted to get it out so that people can look at it and experiment with it. The examples will all be ported soon (KMeans and WordCount are converted already) and then I will also add integration tests similar to KMeansITCase and friends.;;;","09/Jun/14 11:36;github-import;[Date: Wed Oct 09 15:06:29 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Travis found the following:
```
[WARNING] /home/travis/build/dimalabs/ozone/pact-scala/pact-scala-core/src/main/scala/eu/stratosphere/scala/analysis/postPass/GlobalSchemaCompactor.scala:62: warning: match may not be exhaustive.
[WARNING] It would fail on the following input: Some((x: eu.stratosphere.scala.analysis.UDF[?] forSome x not in (eu.stratosphere.scala.analysis.UDF0[?], eu.stratosphere.scala.analysis.UDF1[?,?], eu.stratosphere.scala.analysis.UDF2[?,?,?])))
[WARNING]         val (forwardPos, outputFields) = node.getUDF match {
[WARNING]                                               ^
[WARNING] /home/travis/build/dimalabs/ozone/pact-scala/pact-scala-core/src/main/scala/eu/stratosphere/scala/codegen/UDTGen.scala:46: warning: match may not be exhaustive.
[WARNING] It would fail on the following inputs: BaseClassDescriptor(_, _, _, _), CaseClassDescriptor(_, _, _, _, _), UnsupportedDescriptor(_, _, _)
[WARNING]       val fieldTypes = getIndexFields(desc).toList map {
[WARNING]                                                        ^
[WARNING] /home/travis/build/dimalabs/ozone/pact-scala/pact-scala-core/src/main/scala/eu/stratosphere/scala/codegen/UDTGen.scala:61: warning: match may not be exhaustive.
[WARNING] It would fail on the following inputs: BaseClassDescriptor(_, _, _, _), CaseClassDescriptor(_, _, _, _, _), UnsupportedDescriptor(_, _, _)
[WARNING]       val fieldIds = getIndexFields(desc).toList map {
[WARNING]                                                      ^
[WARNING] warning: there were 36 deprecation warning(s); re-run with -deprecation for details
[WARNING] warning: there were 67 feature warning(s); re-run with -feature for details
[WARNING] 5 warnings found
```;;;","09/Jun/14 11:36;github-import;[Date: Wed Oct 09 18:42:51 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Ok, I'll remove the duplicate and fix the warnings. One of the travis builds failed with a mysterious error, is this another one for your collection?;;;","09/Jun/14 11:36;github-import;[Date: Wed Oct 09 18:44:12 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Great!
The error is known, it happens sometimes. I'll add it to the list ;);;;","09/Jun/14 11:36;github-import;[Date: Thu Oct 10 14:08:06 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Alright, I fixed the duplicate dependency and got rid of the warnings. :checkered_flag: ;;;","09/Jun/14 11:36;github-import;[Date: Thu Oct 10 14:22:48 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Why did you put pact-scala into the root directory?
Shouldn't it be in /pact/pact-scala or so?

It seems that, except for stratosphere-dist, the directory structure is similar to maven.;;;","09/Jun/14 11:36;github-import;[Date: Thu Oct 10 15:57:22 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Looks like your Scala code made it successfully into maven's snapshot repo: https://oss.sonatype.org/content/repositories/snapshots/eu/stratosphere/pact-scala-core/0.4-ozone-SNAPSHOT/;;;","09/Jun/14 11:36;github-import;[Date: Thu Oct 10 17:12:23 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

It should probably not be named pact-scala in the first place. Users don't ever see that they are working with PACTs internally so I put it in the top-level.;;;","09/Jun/14 11:36;github-import;[Date: Thu Oct 10 17:15:11 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

why not calling it ""scala"", with ""scala-core"", ""scala-examples"" as sub-projects ?;;;","09/Jun/14 11:36;github-import;[Date: Thu Oct 10 17:40:27 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Ok, I'll do that tomorrow.;;;","09/Jun/14 11:36;github-import;[Date: Thu Oct 10 18:03:18 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

Isn't a maven artifact named 'scala' a bit to unspecific? 
What's wrong with scala-pact? It's a frontend for the Pact programming model, right?

Just my 2 cents...

Btw. I'm having trouble with the scala code in Eclipse (ScalaIDE for Scala 2.10 and scala-maven plugins are installed).;;;","09/Jun/14 11:36;github-import;[Date: Thu Oct 10 18:11:44 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Well, we have the group id as well. So people hopefully understand that it is only our scala interface. An entry in a pom would look like this:
```xml
<dependency>
	<groupId>eu.stratosphere</groupId>
	<artifactId>scala</artifactId>
	<version>0.4</version>
</dependency>
```

Concerning your troubles: you probably need an additional plugin (see README);;;","09/Jun/14 11:36;github-import;[Date: Thu Oct 10 18:33:15 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

OK, that did the trick. Thanks!;;;","09/Jun/14 11:36;github-import;[Date: Thu Oct 10 19:07:16 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Yes, what @fhueske said was my reason for naming it pact-scala and not just scala. I still would find it strange to have a package called scala...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Single Object returned by KeyGroupedIterator,FLINK-131,12719300,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:36,09/Jun/14 11:36,14/Jul/23 05:57,09/Jun/14 11:36,,,,pre-apache,,,,,,,0,github-import,,"Extended the KeyGroupedIteratorTest to check that the values iterator always returns the same object. 
This is the expected behavior of a mutable object iterator.
With the current implementation two alternating records are returned, where the one that was not returned with the last next() call is used as a look-a-head record and changes its value with a hasNext() call. This leads to weird behaviors, e.g., to different behavior depending on whether a group has an even or odd number of elements.
The second commit fixes the bug by copying the return record before giving it to the user code. This is less efficient, but ensures that the user always receives the same object and that the value of the (single) returned object is only changed with a next() call.
This is a fix for issue ([#128|https://github.com/stratosphere/stratosphere/issues/128] | [FLINK-128|https://issues.apache.org/jira/browse/FLINK-128]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/131
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Mon Oct 07 11:59:17 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:36;github-import;pull-request-131-3050759469875619951.patch;https://issues.apache.org/jira/secure/attachment/12649000/pull-request-131-3050759469875619951.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397499,,,2014-06-09 11:36:03.0,,,,,,,,,,"0|i1wfon:",397626,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalDistributedExecutor,FLINK-130,12719299,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:35,09/Jun/14 11:36,14/Jul/23 05:57,09/Jun/14 11:36,,,,pre-apache,,,,,,,0,github-import,,"Fix for this issue: https://github.com/dimalabs/ozone/issues/86

The LocalDistributedExecutor simulates a Nephele-Cluster with multiple TaskManagers in one JVM. Data between the TMs is transferred using the OS network stack, not through memory. This is important to find bugs in the network stack.

It would be great to integrate the LDE with `TestBase` and `TestBase2`. I already started with this, but I had some errors.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/130
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Sat Oct 05 19:32:57 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:36;github-import;pull-request-130-3976280186193813187.patch;https://issues.apache.org/jira/secure/attachment/12648999/pull-request-130-3976280186193813187.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397498,,,2014-06-09 11:35:59.0,,,,,,,,,,"0|i1wfof:",397625,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add methods to get optimized plan from LocalExecutor,FLINK-129,12719298,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:35,09/Jun/14 11:35,14/Jul/23 05:57,09/Jun/14 11:35,,,,pre-apache,,,,,,,0,github-import,,"After reading @sscdotopen's comment in ([#128|https://github.com/stratosphere/stratosphere/issues/128] | [FLINK-128|https://issues.apache.org/jira/browse/FLINK-128]), I figured it might be worthwhile to add the option to get the optimized plan from the `LocalExecutor`.

This commit adds a static and non-static method which return the optimized plan as a String (in JSON). The static method is for the case where you just want the optimized plan (start executor, get plan, stop executor) and the non-static method for the general case where you start the executor yourself and probably also run the job afterwards (thanks to @rmetzger for pointing out that this case is also important). 

Different names for basically the same thing kinda suck but I didn't have a better idea right now... open for changes. :)

(We could also add the `PrintWriter`/`FileWriter` versions from the dump generator, but the String version should be fine for now.)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/129
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Fri Oct 04 17:58:08 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:35;github-import;pull-request-129-2079588814425802929.patch;https://issues.apache.org/jira/secure/attachment/12648998/pull-request-129-2079588814425802929.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397497,,,2014-06-09 11:35:55.0,,,,,,,,,,"0|i1wfo7:",397624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generate source attachments for each subproject,FLINK-126,12719295,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:35,09/Jun/14 11:35,14/Jul/23 05:57,09/Jun/14 11:35,,,,pre-apache,,,,,,,0,github-import,,"Maven did not generate source attachments for the jars.

This change should activate it for all subprojects. This is an requirement for stable releases in maven central. It is also nice to have, if you want to look up our sources in eclipse.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/126
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Fri Oct 04 14:04:59 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:35;github-import;pull-request-126-451811667216660771.patch;https://issues.apache.org/jira/secure/attachment/12648997/pull-request-126-451811667216660771.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397494,,,Mon Jun 09 11:35:34 UTC 2014,,,,,,,,,,"0|i1wfnj:",397621,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:35;github-import;[Date: Wed Nov 06 11:45:16 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Is this issue fixed with [a8228ceb96bfba2b4b9d63ace8c77077a0689a7b|https://github.com/stratosphere/stratosphere/commit/a8228ceb96bfba2b4b9d63ace8c77077a0689a7b] ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Versions, Artifact IDs, Maven Central",FLINK-125,12719294,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:35,09/Jun/14 11:35,14/Jul/23 05:57,09/Jun/14 11:35,,,,pre-apache,,,,,,,0,github-import,,"Hello,

I recently merged this https://github.com/dimalabs/ozone/pull/64 pull request  by @Aheise into the main code line.
It automatically uploads successful builds to the Sonatype OSS Snapshot repository (https://oss.sonatype.org/content/repositories/snapshots/eu/stratosphere/ozone/).
I therefore changed the version of Stratosphere to `0.4-ozone-SNAPSHOT`.

You can now add us as a dependency by adding the snapshot repository to your pom:
```xml
 <repositories>
       <repository>
         <id>snapshots-repo</id>
         <url>https://oss.sonatype.org/content/repositories/snapshots</url>
         <releases><enabled>false</enabled></releases>
         <snapshots><enabled>true</enabled></snapshots>
       </repository>
</repositories>
```
And us as a dependency:
```xml
<dependency>
	<groupId>eu.stratosphere</groupId>
	<artifactId>ozone</artifactId>
	<version>0.4-ozone-SNAPSHOT</version>
</dependency>
```
(there is one little issue: it is not really working this way ;) But I'm on it (Help is appreciated ;). You have to add pact-clients and pact-common as a dependency. )

We always push two versions to Sonatype: `0.4-ozone-SNAPSHOT` (which is hadoop v1) and  `0.4-ozone-hadoop2-SNAPSHOT` (hadoop v2, with yarn support).

I'm also going to release a `0.2-ozone` version to maven central! You then won't have to add the snapshot repository manually.

I have the following questions for you. I'd like to know what you think, how other projects are doing this and so on.

1) I'm not happy with the version names. They are too long. The ""-SNAPSHOT"" is currently hardcoded into our poms. I could change the versions back to 0.2-ozone and let a script change the version before deploying them to Sonatype. 

2) @aalexandrov changed the artifact ID in the main pom to `ozone`. This is how our main pom currently looks like:
```xml
<groupId>eu.stratosphere</groupId>
<artifactId>ozone</artifactId>
<version>0.4-ozone-SNAPSHOT</version>
```
This means that our project is called `ozone`, which is not the case. I suggest to change the name back to `stratosphere`.
As @ktzoumas suggested, we can use other atmosphere gases such as Nitrogen, Oxygen, Neon, Helium, Krypton, Xenon, Iodine. We could append these to the version numbers. Like `0.6-neon`, `0.10-helium` etc.




---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/125
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Fri Oct 04 09:46:46 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397493,,,Mon Jun 09 11:35:29 UTC 2014,,,,,,,,,,"0|i1wfnb:",397620,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:35;github-import;[Date: Fri Oct 04 10:09:23 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

What is your problem with having to include pact-clients and pact-common instead of ""ozone""?;;;","09/Jun/14 11:35;github-import;[Date: Fri Oct 04 10:20:37 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Nothing.
I somehow thought that it should work by including just the `ozone` artifact. Based on my experience with open source projects, you usually only have to include one artifact to use the project.
It is also dissatisfying for me not to understand, why it is not working!
;;;","09/Jun/14 11:35;github-import;[Date: Fri Oct 04 10:27:23 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

I think the difference might be that other projects only have one pom to begin with. For us the ""ozone"" pom is just the parent pom of the project, it does not contain any code artifacts. I think it is more correct to have to include the exact pom that you want. If it where not like that you would always get the whole shebang (scala, meteor, nephele stuff, tests) even if all you want to do is develop a simple java pact job.;;;","09/Jun/14 11:35;github-import;[Date: Fri Oct 04 10:40:25 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

In spark you add spark-core as a dependency, this is not the top-level pom of spark.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Strange (and faulty) Iterations Behaviour #2,FLINK-124,12719293,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:35,09/Jun/14 11:35,14/Jul/23 05:57,09/Jun/14 11:35,,,,pre-apache,,,,,,,0,github-import,,"I have a new example job to calculate a transitive closure (both java and scala frontend) where I have a union of the iteration input after using the input earlier. The udf with the union will just stall and nothing will happen. It does not happen when I unroll the iteration and it does also not happen when I emulate the reducer that has the union using a CoGroup.

Source Code: https://github.com/aljoscha/ozone/blob/9ae7afff9308996b16de67f8003822e1813d5059/pact/pact-examples/src/main/java/eu/stratosphere/pact/example/shortestpaths/TransitiveClosure.java

Stacktrace: https://gist.github.com/aljoscha/113353aa6ff89108eb2b

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/124
Created by: [aljoscha|https://github.com/aljoscha]
Labels: bug, 
Milestone: Release 0.4
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Thu Oct 03 09:58:35 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397492,,,Mon Jun 09 11:35:22 UTC 2014,,,,,,,,,,"0|i1wfn3:",397619,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:35;github-import;[Date: Thu Oct 03 10:00:53 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@sscdotopen I don't have a stacktrace, the job will just stall and there will be no more output. I'm using the LocalExecutor, by the way.;;;","09/Jun/14 11:35;github-import;[Date: Thu Oct 03 10:13:04 CEST 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

You can take one with jstack or your IDE;;;","09/Jun/14 11:35;github-import;[Date: Thu Oct 03 10:22:24 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Ok, I'll do that when I'm at DIMA tomorrow where I have the code.;;;","09/Jun/14 11:35;github-import;[Date: Fri Oct 04 13:11:43 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

I've added the stacktrace. It seems that all the tasks are waiting for something.;;;","09/Jun/14 11:35;github-import;[Date: Fri Oct 04 13:15:26 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

This reminds my of an issue that I have with the telecom use-cases https://github.com/dimalabs/ozone/issues/109
It also involves the sorter.;;;","09/Jun/14 11:35;github-import;[Date: Fri Oct 04 13:35:34 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

But you didn't solve that one yet, right?;;;","09/Jun/14 11:35;github-import;[Date: Fri Oct 04 13:40:55 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

No, I have a workaround that is not applicable to you.

(I have two `FileDataSource` for the same file, so that each contract that
needs the data can read it from its own source). The workaround is based on
various experiments. I did not understand the problem. I'm also unable to
create a smaller sample job that repeats the error


On Fri, Oct 4, 2013 at 1:35 PM, Aljoscha Krettek
<notifications@github.com>wrote:

> But you didn't solve that one yet, right?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/dimalabs/ozone/issues/124#issuecomment-25692617>
> .
>



-- 
Robert Metzger, Kontakt: metzgerr@web.de, Mobil: 0171/7424461;;;","09/Jun/14 11:35;github-import;[Date: Tue Oct 08 16:59:57 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@StephanEwen I have added a failing test case for this in https://github.com/aljoscha/ozone/tree/iteration-with-union-bug;;;","09/Jun/14 11:35;github-import;[Date: Mon Oct 21 01:37:51 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

This turned out to be a more general problem in the nephele channels and readers. Union readers and events don't seem to work together in general, it is actually not iteration specific.;;;","09/Jun/14 11:35;github-import;[Date: Mon Oct 21 02:07:25 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

It seems like the logic in the UnionReader is fundamentally broken. I assume that this streches to more cases then we have discovered, yet.

With ([#25|https://github.com/stratosphere/stratosphere/issues/25] | [FLINK-25|https://issues.apache.org/jira/browse/FLINK-25]) this would get fixed as well. It would be good to have a fix for this fast, though. ([#25|https://github.com/stratosphere/stratosphere/issues/25] | [FLINK-25|https://issues.apache.org/jira/browse/FLINK-25]) is not prioritized right now.;;;","09/Jun/14 11:35;github-import;[Date: Mon Oct 21 10:54:32 CEST 2013, Author: [uce|https://github.com/uce]]

@StephanEwen Will you look into this? Or should I try my luck? I was planning to start today with looking into the iterations stuff.

Alternatively I would sync with @vasia and pick something else.;;;","09/Jun/14 11:35;github-import;[Date: Mon Oct 21 11:14:20 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I will try to concoct a quick fix. Extending the iterations is more pressing right now, I suggest you go ahead with that.;;;","09/Jun/14 11:35;github-import;[Date: Tue Oct 22 14:40:27 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [1228a5ec60452200e12cb77a9bb0623d340e87d2|https://github.com/stratosphere/stratosphere/commit/1228a5ec60452200e12cb77a9bb0623d340e87d2];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Strange (and faulty) Iterations Behaviour,FLINK-123,12719292,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,uce,github-import,github-import,09/Jun/14 11:35,10/Mar/22 06:23,14/Jul/23 05:57,09/Jun/14 11:35,,,,pre-apache,,,,,,,0,github-import,,"In KMeansIterative, when you add an identity mapper after the reduceClusterCenter reducer you get:
java.lang.RuntimeException: : BUG: The task must have at least one output
	at eu.stratosphere.pact.runtime.task.RegularPactTask.registerInputOutput(RegularPactTask.java:248)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.<init>(RuntimeEnvironment.java:194)
	at eu.stratosphere.nephele.executiongraph.ExecutionGroupVertex.<init>(ExecutionGroupVertex.java:237)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:497)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:275)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:176)
	at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:532)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:417)
	at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:946)
I discovered this one from a plan in the scala frontend where I also have a final mapper that does some real work after a reducer in an iteration (BulkIteration).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/123
Created by: [aljoscha|https://github.com/aljoscha]
Labels: bug, 
Milestone: Release 0.4
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Thu Oct 03 09:16:36 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397491,,,Mon Jun 09 11:35:11 UTC 2014,,,,,,,,,,"0|i1wfmv:",397618,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:35;github-import;[Date: Thu Oct 03 09:38:41 CEST 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

I suggest you open another issue for the second error as it seems to be a different thing. Could you also add a stacktrace from one of the machines for the case where you saw the stalling? I've seen stalling two, it can either be caused by deadlocks in the network layer or by the compiler failing to insert dams at the correct points.;;;","09/Jun/14 11:35;github-import;[Date: Tue Oct 08 17:00:37 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@StephanEwen I have added a failing test case in https://github.com/aljoscha/ozone/tree/chained-mapper-bug;;;","09/Jun/14 11:35;github-import;[Date: Mon Oct 21 11:27:41 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [eec2840d831637e05a55a16bdd5a3e039a7d1ce0|https://github.com/stratosphere/stratosphere/commit/eec2840d831637e05a55a16bdd5a3e039a7d1ce0];;;","09/Jun/14 11:35;github-import;[Date: Mon Oct 21 14:27:13 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

So it just does not chain them, the mapper is a separate operator for now?;;;","09/Jun/14 11:35;github-import;[Date: Mon Oct 21 14:38:39 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The last operator in an iteration must always be separate now, because the runtime uses the main operator's output collector (and config parameters) for the back channel.

@uce Is reworking the tail tasks anyways, I would suggest to include a proper fix in the reworkings there.

We should actually rework the chaining idea into something more elaborate, such that we have proper trees of operators inside a vertex, not just this poor-mans-version in form of the push chain.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add an example of using LocalExecutor in WordCount example,FLINK-121,12719290,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:34,09/Jun/14 11:35,14/Jul/23 05:57,09/Jun/14 11:35,,,,pre-apache,,,,,,,0,github-import,,"Asterios pointed out that nowhere in the examples do we show that the LocalExecutor exists. So I added a static main method that shows it's usage.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/121
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Tue Oct 01 18:23:28 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:35;github-import;pull-request-121-6874125881011506922.patch;https://issues.apache.org/jira/secure/attachment/12648996/pull-request-121-6874125881011506922.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397489,,,Mon Jun 09 11:35:01 UTC 2014,,,,,,,,,,"0|i1wfmf:",397616,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:35;github-import;[Date: Tue Oct 01 18:26:58 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Great! That is a very good idea.

Once I've finished the ""LocalDistributedExecutor"", we could also write a comment about it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Usercode classes should be preferred over system-classes,FLINK-117,12719286,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:34,09/Jun/14 11:34,14/Jul/23 05:57,09/Jun/14 11:34,,,,pre-apache,,,,,,,0,github-import,,"I have a Stratosphere job that has jodatime as a dependency. It seems that for our hadoop-yarn build, pact-hbase pulls jruby-complete.jar as a dependency. Jodatime is extracted into jruby-complete.
If I call a yodatime function inside my user code, the wrong (outdated) version is called, which causes an exception.

I think we need to configure the classloader in the TaskManager properly.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/117
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, 
Created at: Mon Sep 30 13:55:11 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397485,,,Mon Jun 09 11:34:30 UTC 2014,,,,,,,,,,"0|i1wflj:",397612,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:34;github-import;[Date: Fri Oct 04 09:19:23 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

This commit by @AHeise https://github.com/dimalabs/ozone/commit/aca4323c1c0f05d29afcdfce5e4564663adc41e0 could fix the issue. It was merged with the maven-central deployment pull request.
I'm going to validate the fix once I'm working on that task again.;;;","09/Jun/14 11:34;github-import;[Date: Fri Oct 11 10:47:45 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

The mentioned commit by @AHeise introduces a much bigger problem, currently being in the main codeline:

```
ERROR: The job was not successfully submitted to the nephele job manager: eu.stratosphere.nephele.executiongraph.GraphConversionException: eu.stratosphere.pact.runtime.task.util.CorruptConfigurationException: Could not read the user code wrapper: java.lang.ClassNotFoundException: eu.stratosphere.tlabs.risk.input.AvroInputFormat
```

;;;","09/Jun/14 11:34;github-import;[Date: Fri Oct 11 10:58:25 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

@rmetzger This is in your telekom use-case? This seems to be a problem with the move to allow objects in addition to classes for user-code. There should only be a UserCodeWrapper<AvroInputFormat> in the task config.;;;","09/Jun/14 11:34;github-import;[Date: Fri Oct 11 11:08:23 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Stephan is on it.;;;","09/Jun/14 11:34;github-import;[Date: Fri Oct 11 11:15:21 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Full stack
```
	at eu.stratosphere.pact.runtime.task.util.TaskConfig.getStubWrapper(TaskConfig.java:261)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.instantiateUserCode(RegularPactTask.java:1311)
	at eu.stratosphere.pact.runtime.task.DataSourceTask.initInputFormat(DataSourceTask.java:289)
	at eu.stratosphere.pact.runtime.task.DataSourceTask.registerInputOutput(DataSourceTask.java:95)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.<init>(RuntimeEnvironment.java:194)
	at eu.stratosphere.nephele.executiongraph.ExecutionGroupVertex.<init>(ExecutionGroupVertex.java:236)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createVertex(ExecutionGraph.java:497)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:275)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:176)
	at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:532)
```;;;","09/Jun/14 11:34;github-import;[Date: Thu Feb 27 18:05:31 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think this issue is resolved now, no? We always load first first the user code classloader in the source tasks as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Temporary removed summit event information from the website.,FLINK-116,12719285,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:34,17/Jun/20 13:20,14/Jul/23 05:57,09/Jun/14 11:34,,,,pre-apache,,,,,,,0,github-import,,"On request of Kostas, until we are ready with all details for the announce.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/116
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: website, 
Created at: Mon Sep 30 11:34:13 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,FLINK-18220,,,"09/Jun/14 11:34;github-import;pull-request-116-6739336472085428865.patch;https://issues.apache.org/jira/secure/attachment/12648995/pull-request-116-6739336472085428865.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397484,,,2014-06-09 11:34:22.0,,,,,,,,,,"0|i1wflb:",397611,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Website updates,FLINK-115,12719284,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:34,09/Jun/14 11:34,14/Jul/23 05:57,09/Jun/14 11:34,,,,pre-apache,,,,,,,0,github-import,,"Hi,

I changed ""News"" to ""Blog"", since I'm planning to write some posts with @uce.
Fabian requested to add another publication.
I also updated the google analytics code to the newest (official) version and used a different tracking id.

For some reason, the stratosphere.eu website is currently out of sync with the github repo! The Stratosphere-Event is missing!

I want approval from @ktzoumas on this before I publish it!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/115
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Sep 30 09:18:24 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:34;github-import;pull-request-115-5615171144097269746.patch;https://issues.apache.org/jira/secure/attachment/12648994/pull-request-115-5615171144097269746.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397483,,,Mon Jun 09 11:34:19 UTC 2014,,,,,,,,,,"0|i1wfl3:",397610,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:34;github-import;[Date: Mon Sep 30 09:55:14 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

Is there any good reason to switch the tracking ID? Won't we lose the previous stats?;;;","09/Jun/14 11:34;github-import;[Date: Mon Sep 30 09:59:04 CEST 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

We don't know who has the account data for the previous ID.;;;","09/Jun/14 11:34;github-import;[Date: Mon Sep 30 09:59:23 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

No, there is no good reason ;) 
The last information I got was, that nobody knows the account owner ;);;;","09/Jun/14 11:34;github-import;[Date: Mon Sep 30 10:00:01 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Google Analytics now supports adding multiple administrators to a tracking ID. That's what I wanted to do with the new ID!;;;","09/Jun/14 11:34;github-import;[Date: Mon Sep 30 10:01:05 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

a couple of people, including me :-D


2013/9/30 Alexander Alexandrov <notifications@github.com>

> I don't know who has the account data for the previous ID.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/dimalabs/ozone/pull/115#issuecomment-25343115>
> .
>;;;","09/Jun/14 11:34;github-import;[Date: Mon Sep 30 10:02:45 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Can you add me with ""View"" -Permissions ?;;;","09/Jun/14 11:34;github-import;[Date: Mon Sep 30 10:06:28 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, I removed the commit;;;","09/Jun/14 11:34;github-import;[Date: Mon Sep 30 10:12:10 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

I added you. Anybody else?;;;","09/Jun/14 11:34;github-import;[Date: Mon Sep 30 10:32:28 CEST 2013, Author: [asteriosk|https://github.com/asteriosk]]

Please, add me as well. asteriosk@gmail.com

thanks,
Asterios
On Sep 30, 2013, at 10:12 AM, Fabian Hueske <notifications@github.com> wrote:

> I added you. Anybody else?
> 
> —
> Reply to this email directly or view it on GitHub.
> ;;;","09/Jun/14 11:34;github-import;[Date: Mon Sep 30 10:48:54 CEST 2013, Author: [ktzoumas|https://github.com/ktzoumas]]

Add me as well: kostas.tzoumas@gmail.com

The event is not there as I am waiting confirmation of the venue, this is
not a bug

Rest of the changes approved

Kostas


On Mon, Sep 30, 2013 at 10:32 AM, Asterios Katsifodimos <
notifications@github.com> wrote:

> Please, add me as well. asteriosk@gmail.com
>
> thanks,
> Asterios
> On Sep 30, 2013, at 10:12 AM, Fabian Hueske <notifications@github.com>
> wrote:
>
> > I added you. Anybody else?
> >
> > —
> > Reply to this email directly or view it on GitHub.
> >
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/dimalabs/ozone/pull/115#issuecomment-25344677>
> .
>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mailing List for Ozone-Development,FLINK-114,12719283,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:34,09/Jun/14 11:34,14/Jul/23 05:57,09/Jun/14 11:34,,,,pre-apache,,,,,,,0,github-import,,"Hello,

I created a Google Groups-based mailing list for all ozone developers.

You can find the Group here: https://groups.google.com/forum/#!forum/stratosphere-dev
The address of the mailing list is: stratosphere-dev@googlegroups.com

This is not any kind of official mailing list or so. I did not ask anybody for ""permission"" to create it. I just have the feeling that we need a tool to discuss things which are not an actual issue or question.
I know that there is already an official list for stratosphere. (But its not really used, its TU hosted and not publicly available (or the archives are not working))

If we want to be an *open* open-source project, every discussion should be publicly available.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/114
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Tue Sep 24 21:16:17 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397482,,,Mon Jun 09 11:34:09 UTC 2014,,,,,,,,,,"0|i1wfkv:",397609,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:34;github-import;[Date: Tue Sep 24 23:22:34 CEST 2013, Author: [ktzoumas|https://github.com/ktzoumas]]

Great! We agree :-) Change the welcome message to something more welcoming to newcomers, put a link at the ozone README, and let us use this from now on;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Union on DataSinks does not work,FLINK-112,12719281,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:33,09/Jun/14 11:33,14/Jul/23 05:57,09/Jun/14 11:33,,,,pre-apache,,,,,,,0,github-import,,"When you have a DataSink with several inputs you get this error message:
13/09/24 13:48:09 ERROR client.JobClient: ERROR: eu.stratosphere.nephele.executiongraph.GraphConversionException: Job and execution vertex <Unnamed File Data Sink> have different number of inputs
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.createInitialGroupEdges(ExecutionGraph.java:409)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.constructExecutionGraph(ExecutionGraph.java:282)
	at eu.stratosphere.nephele.executiongraph.ExecutionGraph.<init>(ExecutionGraph.java:176)
	at eu.stratosphere.nephele.jobmanager.JobManager.submitJob(JobManager.java:532)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at eu.stratosphere.nephele.ipc.RPC$Server.call(RPC.java:417)
	at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:946)

Fabian mentioned that having the compiler add a dummy mapper before the sink could be the solution.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/112
Created by: [aljoscha|https://github.com/aljoscha]
Labels: bug, core, optimizer, 
Milestone: Release 0.4
Assignee: [fhueske|https://github.com/fhueske]
Created at: Tue Sep 24 13:49:55 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397480,,,Mon Jun 09 11:33:52 UTC 2014,,,,,,,,,,"0|i1wfkf:",397607,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:33;github-import;[Date: Wed Oct 16 09:27:13 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

Yes, that should be a quick fix. But AFAIK this would result in one additional thread and serialization overhead, since [DataSinkTask|https://github.com/stratosphere/stratosphere/blob/master/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/DataSinkTask.java]s cannot be chained behind [RegularPactTask|https://github.com/stratosphere/stratosphere/blob/master/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/RegularPactTask.java]s, due to the different abstract Nephele interfaces for processing and output tasks.

Another solution could be to integrate the union logic directly into the [DataSinkTask|https://github.com/stratosphere/stratosphere/blob/master/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/DataSinkTask.java) as I did with the Sorter for Sorted output (see ([#164|https://github.com/stratosphere/stratosphere/issues/164] | [FLINK-164|https://issues.apache.org/jira/browse/FLINK-164])]. However, this is actually duplicating complex code that is also taken care of by the [RegularPactTask|https://github.com/stratosphere/stratosphere/blob/master/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/RegularPactTask.java]. ;;;","09/Jun/14 11:33;github-import;[Date: Wed Oct 16 12:12:11 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

That Nephele Class hierarchy has has always bothered us with respect to duplicate logic in souce and sink tasks. The Regular Pact Task has many static methods to set up outputs for example, such that the logic can be called from an input task. We can do similar things for the union (as a quick fix).

We should think about changing that class hierarchy, such that all tasks inherit from the same task class and implicitly become input and outputs. Potentially with a property to be set or a marker interface.

That would also help us to get around fake tails in the iterations.;;;","09/Jun/14 11:33;github-import;[Date: Wed Oct 16 12:23:04 CEST 2013, Author: [uce|https://github.com/uce]]

Yes, I agree with redoing the task hierarchy. I just quoted you and opened a separate issue ([#165|https://github.com/stratosphere/stratosphere/issues/165] | [FLINK-165|https://issues.apache.org/jira/browse/FLINK-165]).

On Oct 16, 2013, at 12:12 PM, Stephan Ewen <notifications@github.com> wrote:

> That Nephele Class hierarchy has has always bothered us with respect to duplicate logic in souce and sink tasks. The Regular Pact Task has many static methods to set up outputs for example, such that the logic can be called from an input task. We can do similar things for the union (as a quick fix).
> 
> We should think about changing that class hierarchy, such that all tasks inherit from the same task class and implicitly become input and outputs. Potentially with a property to be set or a marker interface.
> 
> That would also help us to get around fake tails in the iterations.
> 
> —
> Reply to this email directly or view it on GitHub.
> ;;;","09/Jun/14 11:33;github-import;[Date: Fri Nov 22 20:40:56 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

The suggested quick fix works. We need to fix this soon!;;;","09/Jun/14 11:33;github-import;[Date: Thu Dec 05 15:32:39 CET 2013, Author: [fhueske|https://github.com/fhueske]]

Done in pull request ([#307|https://github.com/stratosphere/stratosphere/issues/307] | [FLINK-307|https://issues.apache.org/jira/browse/FLINK-307]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for different Hadoop versions (yarn!) using activation by properties,FLINK-111,12719280,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:33,09/Jun/14 11:33,14/Jul/23 05:57,09/Jun/14 11:33,,,,pre-apache,,,,,,,0,github-import,,"This pull request is a fix for https://github.com/dimalabs/ozone/issues/95
The results of this PR are mostly based on the work and research of [Andre|https://github.com/andrehacker).

We implemented maven profiles similar to other large projects that depend on hadoop (giraph, hbase].
Similarly to hbase, we chose to supply a little script that generates specialized POMs for the  'hadoop2` profile.
The above mentioned issue contains an extensive discussion of the problem. 

The README lists all relevant commands and the travis build system supports the new profile activation

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/111
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Sep 23 16:42:57 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:33;github-import;pull-request-111-138186186800144245.patch;https://issues.apache.org/jira/secure/attachment/12648993/pull-request-111-138186186800144245.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397479,,,2014-06-09 11:33:44.0,,,,,,,,,,"0|i1wfk7:",397606,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to newer maven surefire (integration tests) version + Fix for failing testcase in ExecutionGraphTest,FLINK-110,12719279,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:33,09/Jun/14 11:33,14/Jul/23 05:57,09/Jun/14 11:33,,,,pre-apache,,,,,,,0,github-import,,"I upgraded to the most recent maven surefire version. It shows more detailed error messages / summaries, which is especially useful for travis builds (because we cannot access the surefire-reports on the test machines)
The new version marks errored tests as failed.

The pull request contains a fix for a failing testcase in the ExecutionGraphTest (that was most probably introduced by https://github.com/dimalabs/ozone/pull/53) which was not detected because it errored (NullPtr)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/110
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Sep 23 16:31:58 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:33;github-import;pull-request-110-5373978517877393658.patch;https://issues.apache.org/jira/secure/attachment/12648992/pull-request-110-5373978517877393658.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397478,,,Mon Jun 09 11:33:41 UTC 2014,,,,,,,,,,"0|i1wfjz:",397605,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:33;github-import;[Date: Fri Oct 11 12:12:17 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Partially manually merged and subsumed by ([#151|https://github.com/stratosphere/stratosphere/issues/151] | [FLINK-151|https://issues.apache.org/jira/browse/FLINK-151]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock (probably in LocalBufferPool),FLINK-109,12719278,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:33,09/Jun/14 11:33,14/Jul/23 05:57,09/Jun/14 11:33,,,,pre-apache,,,,,,,0,github-import,,"The JVM did not do anything anymore.

The job plan that caused this behavior is quite large. I set the DOP to 1, Stratosphere was running in cluster mode.

This stack has been obtained using `jstack`.
```
2013-09-21 13:56:25
Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.6-b01 mixed mode):

""Attach Listener"" daemon prio=10 tid=0x00007f9874001000 nid=0x30d7 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""org.apache.hadoop.hdfs.PeerCache@50ea9f6e"" daemon prio=10 tid=0x00007f982c01f800 nid=0x2c0a waiting on condition [0x00007f98a02e3000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.PeerCache.run(PeerCache.java:252)
	at org.apache.hadoop.hdfs.PeerCache.access$000(PeerCache.java:39)
	at org.apache.hadoop.hdfs.PeerCache$1.run(PeerCache.java:135)
	at java.lang.Thread.run(Thread.java:662)

""Join billing accounts with invoices (1/1)"" daemon prio=10 tid=0x00007f98500a8800 nid=0x2be0 waiting on condition [0x00007f983e1dc000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7dd5e80> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.nephele.io.RuntimeInputGate.waitForAnyChannelToBecomeAvailable(RuntimeInputGate.java:274)
	at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:228)
	at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:129)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:62)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:27)
	at eu.stratosphere.pact.runtime.hash.MutableHashTable.buildInitialTable(MutableHashTable.java:701)
	at eu.stratosphere.pact.runtime.hash.MutableHashTable.open(MutableHashTable.java:435)
	at eu.stratosphere.pact.runtime.hash.BuildSecondHashMatchIterator.open(BuildSecondHashMatchIterator.java:91)
	at eu.stratosphere.pact.runtime.task.MatchDriver.prepare(MatchDriver.java:149)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:342)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""Join billing accounts with invoices (1/1)"" daemon prio=10 tid=0x00007f98500a6800 nid=0x2bdf waiting on condition [0x00007f983e2dd000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7db97f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.nephele.io.RuntimeInputGate.waitForAnyChannelToBecomeAvailable(RuntimeInputGate.java:274)
	at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:228)
	at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:129)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:62)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:27)
	at eu.stratosphere.pact.runtime.hash.MutableHashTable.buildInitialTable(MutableHashTable.java:701)
	at eu.stratosphere.pact.runtime.hash.MutableHashTable.open(MutableHashTable.java:435)
	at eu.stratosphere.pact.runtime.hash.BuildSecondHashMatchIterator.open(BuildSecondHashMatchIterator.java:91)
	at eu.stratosphere.pact.runtime.task.MatchDriver.prepare(MatchDriver.java:149)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:342)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""SortMerger spilling thread"" daemon prio=10 tid=0x00007f97e0005000 nid=0x2bda waiting on condition [0x00007f983e5e0000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7d71eb8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$SpillingThread.go(UnilateralSortMerger.java:1240)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:829)

""SortMerger sorting thread"" daemon prio=10 tid=0x00007f97e0003000 nid=0x2bd9 waiting on condition [0x00007f983e6e1000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7d6c168> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$SortingThread.go(UnilateralSortMerger.java:1140)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:829)

""SortMerger Reading Thread"" daemon prio=10 tid=0x00007f97e0001000 nid=0x2bd8 waiting on condition [0x00007f983e7e2000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7d81ee0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.nephele.io.RuntimeInputGate.waitForAnyChannelToBecomeAvailable(RuntimeInputGate.java:274)
	at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:228)
	at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:129)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:62)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:27)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$ReadingThread.go(UnilateralSortMerger.java:1062)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:829)

""Count active billing accs (1/1)"" daemon prio=10 tid=0x00007f98500a4800 nid=0x2bd7 in Object.wait() [0x00007f983e8e3000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000f7d6c260> (a java.lang.Object)
	at java.lang.Object.wait(Object.java:485)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:638)
	- locked <0x00000000f7d6c260> (a java.lang.Object)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.getInput(RegularPactTask.java:901)
	at eu.stratosphere.pact.runtime.task.ReduceDriver.prepare(ReduceDriver.java:115)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:342)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""Join billing accounts with invoices (1/1)"" daemon prio=10 tid=0x00007f98500a3000 nid=0x2bd3 waiting on condition [0x00007f983e9e4000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7d71fc8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.nephele.io.RuntimeInputGate.waitForAnyChannelToBecomeAvailable(RuntimeInputGate.java:274)
	at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:228)
	at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:129)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:62)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:27)
	at eu.stratosphere.pact.runtime.hash.MutableHashTable.buildInitialTable(MutableHashTable.java:701)
	at eu.stratosphere.pact.runtime.hash.MutableHashTable.open(MutableHashTable.java:435)
	at eu.stratosphere.pact.runtime.hash.BuildSecondHashMatchIterator.open(BuildSecondHashMatchIterator.java:91)
	at eu.stratosphere.pact.runtime.task.MatchDriver.prepare(MatchDriver.java:149)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:342)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""Join invoices with active accounts (1/1)"" daemon prio=10 tid=0x00007f98500a1000 nid=0x2bd1 waiting on condition [0x00007f983eae5000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7caaea0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.nephele.io.RuntimeInputGate.waitForAnyChannelToBecomeAvailable(RuntimeInputGate.java:274)
	at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:228)
	at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:129)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:62)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:27)
	at eu.stratosphere.pact.runtime.hash.MutableHashTable.buildInitialTable(MutableHashTable.java:701)
	at eu.stratosphere.pact.runtime.hash.MutableHashTable.open(MutableHashTable.java:435)
	at eu.stratosphere.pact.runtime.hash.BuildSecondHashMatchIterator.open(BuildSecondHashMatchIterator.java:91)
	at eu.stratosphere.pact.runtime.task.MatchDriver.prepare(MatchDriver.java:149)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:342)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""Join invoices with active accounts (1/1)"" daemon prio=10 tid=0x00007f985009f000 nid=0x2bd0 waiting on condition [0x00007f983ebe6000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7ca4f90> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.nephele.io.RuntimeInputGate.waitForAnyChannelToBecomeAvailable(RuntimeInputGate.java:274)
	at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:228)
	at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:129)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:62)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:27)
	at eu.stratosphere.pact.runtime.hash.MutableHashTable.buildInitialTable(MutableHashTable.java:701)
	at eu.stratosphere.pact.runtime.hash.MutableHashTable.open(MutableHashTable.java:435)
	at eu.stratosphere.pact.runtime.hash.BuildSecondHashMatchIterator.open(BuildSecondHashMatchIterator.java:91)
	at eu.stratosphere.pact.runtime.task.MatchDriver.prepare(MatchDriver.java:149)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:342)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""Join billing accounts with invoices (1/1)"" daemon prio=10 tid=0x00007f985009e000 nid=0x2bcf waiting on condition [0x00007f983ede8000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7ca6ea0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.nephele.io.RuntimeInputGate.waitForAnyChannelToBecomeAvailable(RuntimeInputGate.java:274)
	at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:228)
	at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:129)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:62)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:27)
	at eu.stratosphere.pact.runtime.hash.MutableHashTable.buildInitialTable(MutableHashTable.java:701)
	at eu.stratosphere.pact.runtime.hash.MutableHashTable.open(MutableHashTable.java:435)
	at eu.stratosphere.pact.runtime.hash.BuildSecondHashMatchIterator.open(BuildSecondHashMatchIterator.java:91)
	at eu.stratosphere.pact.runtime.task.MatchDriver.prepare(MatchDriver.java:149)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:342)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""filter active billing accounts (1/1)"" daemon prio=10 tid=0x00007f985009a000 nid=0x2bc8 waiting on condition [0x00007f983efea000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7ca52f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.nephele.io.RuntimeInputGate.waitForAnyChannelToBecomeAvailable(RuntimeInputGate.java:274)
	at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:228)
	at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:129)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:62)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:27)
	at eu.stratosphere.pact.runtime.task.MapDriver.run(MapDriver.java:81)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:372)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""filter invoices: month <= 24 (1/1)"" daemon prio=10 tid=0x00007f9850099000 nid=0x2bc7 in Object.wait() [0x00007f983f2ed000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000f7cab200> (a java.util.ArrayDeque)
	at eu.stratosphere.nephele.taskmanager.bufferprovider.LocalBufferPool.requestBufferInternal(LocalBufferPool.java:152)
	- locked <0x00000000f7cab200> (a java.util.ArrayDeque)
	at eu.stratosphere.nephele.taskmanager.bufferprovider.LocalBufferPool.requestEmptyBufferBlocking(LocalBufferPool.java:103)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputGateContext.requestEmptyBufferBlocking(RuntimeInputGateContext.java:74)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputChannelContext.requestEmptyBufferBlocking(RuntimeInputChannelContext.java:370)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeWithBuffer(ByteBufferedChannelManager.java:398)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelope(ByteBufferedChannelManager.java:331)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeFromOutputChannel(ByteBufferedChannelManager.java:633)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeDispatcher.push(RuntimeDispatcher.java:40)
	at eu.stratosphere.nephele.taskmanager.runtime.ForwardingBarrier.push(ForwardingBarrier.java:58)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.AbstractOutputChannelForwarder.push(AbstractOutputChannelForwarder.java:59)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.OutputChannelForwardingChain.pushEnvelope(OutputChannelForwardingChain.java:50)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeOutputChannelBroker.releaseWriteBuffer(RuntimeOutputChannelBroker.java:188)
	at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedOutputChannel.releaseWriteBuffer(AbstractByteBufferedOutputChannel.java:144)
	at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedOutputChannel.writeRecord(AbstractByteBufferedOutputChannel.java:182)
	at eu.stratosphere.nephele.io.RuntimeOutputGate.writeRecord(RuntimeOutputGate.java:288)
	at eu.stratosphere.nephele.io.AbstractRecordWriter.emit(AbstractRecordWriter.java:101)
	at eu.stratosphere.pact.runtime.shipping.PactRecordOutputCollector.collect(PactRecordOutputCollector.java:79)
	at eu.stratosphere.pact.runtime.shipping.PactRecordOutputCollector.collect(PactRecordOutputCollector.java:31)
	at eu.stratosphere.tlabs.risk.stubs.FilterInvoicesMapper.map(FilterInvoicesMapper.java:46)
	at eu.stratosphere.tlabs.risk.stubs.FilterInvoicesMapper.map(FilterInvoicesMapper.java:17)
	at eu.stratosphere.pact.runtime.task.MapDriver.run(MapDriver.java:82)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:372)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""SortMerger spilling thread"" daemon prio=10 tid=0x00007f9844005800 nid=0x2bbf waiting on condition [0x00007f983f4ef000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7ca7270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$SpillingThread.go(UnilateralSortMerger.java:1240)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:829)

""SortMerger sorting thread"" daemon prio=10 tid=0x00007f9844004000 nid=0x2bbe waiting on condition [0x00007f983f5f0000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7ca5558> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$SortingThread.go(UnilateralSortMerger.java:1140)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:829)

""SortMerger Reading Thread"" daemon prio=10 tid=0x00007f9844003800 nid=0x2bbd waiting on condition [0x00007f98a01e2000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f7cad058> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.nephele.io.RuntimeInputGate.waitForAnyChannelToBecomeAvailable(RuntimeInputGate.java:274)
	at eu.stratosphere.nephele.io.RuntimeInputGate.readRecord(RuntimeInputGate.java:228)
	at eu.stratosphere.nephele.io.MutableRecordReader.next(MutableRecordReader.java:129)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:62)
	at eu.stratosphere.pact.runtime.task.util.PactRecordNepheleReaderIterator.next(PactRecordNepheleReaderIterator.java:27)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$ReadingThread.go(UnilateralSortMerger.java:1062)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:829)

""filter highest rated billing accounts (1/1)"" daemon prio=10 tid=0x00007f9850097000 nid=0x2bbc in Object.wait() [0x00007f983fdfc000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000f7ca9458> (a java.util.ArrayDeque)
	at eu.stratosphere.nephele.taskmanager.bufferprovider.LocalBufferPool.requestBufferInternal(LocalBufferPool.java:152)
	- locked <0x00000000f7ca9458> (a java.util.ArrayDeque)
	at eu.stratosphere.nephele.taskmanager.bufferprovider.LocalBufferPool.requestEmptyBufferBlocking(LocalBufferPool.java:103)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputGateContext.requestEmptyBufferBlocking(RuntimeInputGateContext.java:74)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputChannelContext.requestEmptyBufferBlocking(RuntimeInputChannelContext.java:370)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeWithBuffer(ByteBufferedChannelManager.java:398)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelope(ByteBufferedChannelManager.java:331)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeFromOutputChannel(ByteBufferedChannelManager.java:633)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeDispatcher.push(RuntimeDispatcher.java:40)
	at eu.stratosphere.nephele.taskmanager.runtime.ForwardingBarrier.push(ForwardingBarrier.java:58)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.AbstractOutputChannelForwarder.push(AbstractOutputChannelForwarder.java:59)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.OutputChannelForwardingChain.pushEnvelope(OutputChannelForwardingChain.java:50)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeOutputChannelBroker.releaseWriteBuffer(RuntimeOutputChannelBroker.java:188)
	at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedOutputChannel.releaseWriteBuffer(AbstractByteBufferedOutputChannel.java:144)
	at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedOutputChannel.writeRecord(AbstractByteBufferedOutputChannel.java:182)
	at eu.stratosphere.nephele.io.RuntimeOutputGate.writeRecord(RuntimeOutputGate.java:288)
	at eu.stratosphere.nephele.io.AbstractRecordWriter.emit(AbstractRecordWriter.java:101)
	at eu.stratosphere.pact.runtime.shipping.PactRecordOutputCollector.collect(PactRecordOutputCollector.java:79)
	at eu.stratosphere.pact.runtime.shipping.PactRecordOutputCollector.collect(PactRecordOutputCollector.java:31)
	at eu.stratosphere.tlabs.risk.stubs.FilterBillingAccountsMapper.map(FilterBillingAccountsMapper.java:41)
	at eu.stratosphere.tlabs.risk.stubs.FilterBillingAccountsMapper.map(FilterBillingAccountsMapper.java:14)
	at eu.stratosphere.pact.runtime.task.MapDriver.run(MapDriver.java:82)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:372)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""aggregate Invoices (1/1)"" daemon prio=10 tid=0x00007f9850096800 nid=0x2bba in Object.wait() [0x00007f98a0616000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000f7ca7380> (a java.lang.Object)
	at java.lang.Object.wait(Object.java:485)
	at eu.stratosphere.pact.runtime.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:638)
	- locked <0x00000000f7ca7380> (a java.lang.Object)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.getInput(RegularPactTask.java:901)
	at eu.stratosphere.pact.runtime.task.ReduceDriver.prepare(ReduceDriver.java:115)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.run(RegularPactTask.java:342)
	at eu.stratosphere.pact.runtime.task.RegularPactTask.invoke(RegularPactTask.java:291)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""Billing accounts file (1/1)"" daemon prio=10 tid=0x00007f98500ba800 nid=0x2ba2 in Object.wait() [0x00007f983fffe000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000f7cab4d0> (a java.util.ArrayDeque)
	at eu.stratosphere.nephele.taskmanager.bufferprovider.LocalBufferPool.requestBufferInternal(LocalBufferPool.java:152)
	- locked <0x00000000f7cab4d0> (a java.util.ArrayDeque)
	at eu.stratosphere.nephele.taskmanager.bufferprovider.LocalBufferPool.requestEmptyBufferBlocking(LocalBufferPool.java:103)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputGateContext.requestEmptyBufferBlocking(RuntimeInputGateContext.java:74)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputChannelContext.requestEmptyBufferBlocking(RuntimeInputChannelContext.java:370)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeWithBuffer(ByteBufferedChannelManager.java:398)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelope(ByteBufferedChannelManager.java:331)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeFromOutputChannel(ByteBufferedChannelManager.java:633)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeDispatcher.push(RuntimeDispatcher.java:40)
	at eu.stratosphere.nephele.taskmanager.runtime.ForwardingBarrier.push(ForwardingBarrier.java:58)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.AbstractOutputChannelForwarder.push(AbstractOutputChannelForwarder.java:59)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.OutputChannelForwardingChain.pushEnvelope(OutputChannelForwardingChain.java:50)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeOutputChannelBroker.releaseWriteBuffer(RuntimeOutputChannelBroker.java:188)
	at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedOutputChannel.releaseWriteBuffer(AbstractByteBufferedOutputChannel.java:144)
	at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedOutputChannel.writeRecord(AbstractByteBufferedOutputChannel.java:182)
	at eu.stratosphere.nephele.io.RuntimeOutputGate.writeRecord(RuntimeOutputGate.java:288)
	at eu.stratosphere.nephele.io.AbstractRecordWriter.emit(AbstractRecordWriter.java:101)
	at eu.stratosphere.pact.runtime.shipping.PactRecordOutputCollector.collect(PactRecordOutputCollector.java:79)
	at eu.stratosphere.pact.runtime.task.DataSourceTask.invoke(DataSourceTask.java:156)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""Invoices file (1/1)"" daemon prio=10 tid=0x00007f9850093000 nid=0x2ba1 in Object.wait() [0x00007f98a04e5000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000f7caeed0> (a java.util.ArrayDeque)
	at eu.stratosphere.nephele.taskmanager.bufferprovider.LocalBufferPool.requestBufferInternal(LocalBufferPool.java:152)
	- locked <0x00000000f7caeed0> (a java.util.ArrayDeque)
	at eu.stratosphere.nephele.taskmanager.bufferprovider.LocalBufferPool.requestEmptyBufferBlocking(LocalBufferPool.java:103)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputGateContext.requestEmptyBufferBlocking(RuntimeInputGateContext.java:74)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeInputChannelContext.requestEmptyBufferBlocking(RuntimeInputChannelContext.java:370)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeWithBuffer(ByteBufferedChannelManager.java:398)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelope(ByteBufferedChannelManager.java:331)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.ByteBufferedChannelManager.processEnvelopeFromOutputChannel(ByteBufferedChannelManager.java:633)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeDispatcher.push(RuntimeDispatcher.java:40)
	at eu.stratosphere.nephele.taskmanager.runtime.ForwardingBarrier.push(ForwardingBarrier.java:58)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.AbstractOutputChannelForwarder.push(AbstractOutputChannelForwarder.java:59)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.OutputChannelForwardingChain.pushEnvelope(OutputChannelForwardingChain.java:50)
	at eu.stratosphere.nephele.taskmanager.runtime.RuntimeOutputChannelBroker.releaseWriteBuffer(RuntimeOutputChannelBroker.java:188)
	at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedOutputChannel.releaseWriteBuffer(AbstractByteBufferedOutputChannel.java:144)
	at eu.stratosphere.nephele.io.channels.bytebuffered.AbstractByteBufferedOutputChannel.writeRecord(AbstractByteBufferedOutputChannel.java:182)
	at eu.stratosphere.nephele.io.RuntimeOutputGate.writeRecord(RuntimeOutputGate.java:288)
	at eu.stratosphere.nephele.io.AbstractRecordWriter.emit(AbstractRecordWriter.java:101)
	at eu.stratosphere.pact.runtime.shipping.PactRecordOutputCollector.collect(PactRecordOutputCollector.java:79)
	at eu.stratosphere.pact.runtime.task.DataSourceTask.invoke(DataSourceTask.java:156)
	at eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:344)
	at java.lang.Thread.run(Thread.java:662)

""IOManager reader thread ([#1|https://github.com/stratosphere/stratosphere/issues/1] | [FLINK-1|https://issues.apache.org/jira/browse/FLINK-1])"" daemon prio=10 tid=0x00007f98e4426000 nid=0x2b4b waiting on condition [0x00007f98a0717000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000e077ba88> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.nephele.services.iomanager.IOManager$ReaderThread.run(IOManager.java:537)

""IOManager writer thread ([#1|https://github.com/stratosphere/stratosphere/issues/1] | [FLINK-1|https://issues.apache.org/jira/browse/FLINK-1])"" daemon prio=10 tid=0x00007f98e4425000 nid=0x2b4a waiting on condition [0x00007f98a0818000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f78ba398> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.nephele.services.iomanager.IOManager$WriterThread.run(IOManager.java:630)

""Incoming Connection Thread"" prio=10 tid=0x00007f98e4700000 nid=0x2b49 runnable [0x00007f98a0a20000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
	- locked <0x00000000e0802110> (a sun.nio.ch.Util$2)
	- locked <0x00000000e0802120> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000e08020c8> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.IncomingConnectionThread.run(IncomingConnectionThread.java:113)

""Outgoing Connection Thread"" prio=10 tid=0x00007f98e46fc800 nid=0x2b48 runnable [0x00007f98a0b21000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
	- locked <0x00000000e07ccb10> (a sun.nio.ch.Util$2)
	- locked <0x00000000e07ccb20> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000e07ccac8> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
	at eu.stratosphere.nephele.taskmanager.bytebuffered.OutgoingConnectionThread.run(OutgoingConnectionThread.java:147)

""IPC Server handler 0 on 6122"" daemon prio=10 tid=0x00007f98e46f9000 nid=0x2b46 waiting on condition [0x00007f98a0c22000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000e0756168> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at eu.stratosphere.nephele.ipc.Server$Handler.run(Server.java:938)

""IPC Server listener on 6122"" daemon prio=10 tid=0x00007f98e46f7800 nid=0x2b45 runnable [0x00007f98a0d23000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
	- locked <0x00000000e0756328> (a sun.nio.ch.Util$2)
	- locked <0x00000000e0756338> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000e07562e0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
	at eu.stratosphere.nephele.ipc.Server$Listener.run(Server.java:343)

""IPC Server Responder"" daemon prio=10 tid=0x00007f98e46c6800 nid=0x2b44 runnable [0x00007f98a0e24000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
	- locked <0x00000000e0756448> (a sun.nio.ch.Util$2)
	- locked <0x00000000e0756458> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000e0756400> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
	at eu.stratosphere.nephele.ipc.Server$Responder.run(Server.java:506)

""Low Memory Detector"" daemon prio=10 tid=0x00007f98e40ba800 nid=0x2b42 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread1"" daemon prio=10 tid=0x00007f98e40b8000 nid=0x2b41 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread0"" daemon prio=10 tid=0x00007f98e40b6000 nid=0x2b40 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Signal Dispatcher"" daemon prio=10 tid=0x00007f98e40b3800 nid=0x2b3f runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Finalizer"" daemon prio=10 tid=0x00007f98e4097800 nid=0x2b3e in Object.wait() [0x00007f98a1ce4000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000e07565b8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
	- locked <0x00000000e07565b8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

""Reference Handler"" daemon prio=10 tid=0x00007f98e4095800 nid=0x2b3d in Object.wait() [0x00007f98a1de5000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000e0758230> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:485)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
	- locked <0x00000000e0758230> (a java.lang.ref.Reference$Lock)

""main"" prio=10 tid=0x00007f98e400c000 nid=0x2b24 waiting on condition [0x00007f98e8ff5000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at eu.stratosphere.nephele.taskmanager.TaskManager.runIOLoop(TaskManager.java:397)
	at eu.stratosphere.nephele.taskmanager.TaskManager.main(TaskManager.java:381)

""VM Thread"" prio=10 tid=0x00007f98e408e800 nid=0x2b3c runnable 

""Gang worker([#0|https://github.com/stratosphere/stratosphere/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e401b000 nid=0x2b25 runnable 

""Gang worker([#1|https://github.com/stratosphere/stratosphere/issues/1] | [FLINK-1|https://issues.apache.org/jira/browse/FLINK-1]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e401c800 nid=0x2b26 runnable 

""Gang worker([#2|https://github.com/stratosphere/stratosphere/issues/2] | [FLINK-2|https://issues.apache.org/jira/browse/FLINK-2]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e401e800 nid=0x2b27 runnable 

""Gang worker([#3|https://github.com/stratosphere/stratosphere/issues/3] | [FLINK-3|https://issues.apache.org/jira/browse/FLINK-3]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4020800 nid=0x2b28 runnable 

""Gang worker([#4|https://github.com/stratosphere/stratosphere/issues/4] | [FLINK-4|https://issues.apache.org/jira/browse/FLINK-4]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4022000 nid=0x2b29 runnable 

""Gang worker([#5|https://github.com/stratosphere/stratosphere/issues/5] | [FLINK-5|https://issues.apache.org/jira/browse/FLINK-5]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4024000 nid=0x2b2a runnable 

""Gang worker([#6|https://github.com/stratosphere/stratosphere/issues/6] | [FLINK-6|https://issues.apache.org/jira/browse/FLINK-6]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4026000 nid=0x2b2b runnable 

""Gang worker([#7|https://github.com/stratosphere/stratosphere/issues/7] | [FLINK-7|https://issues.apache.org/jira/browse/FLINK-7]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4027800 nid=0x2b2c runnable 

""Gang worker([#8|https://github.com/stratosphere/stratosphere/issues/8] | [FLINK-8|https://issues.apache.org/jira/browse/FLINK-8]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4029800 nid=0x2b2d runnable 

""Gang worker([#9|https://github.com/stratosphere/stratosphere/issues/9] | [FLINK-9|https://issues.apache.org/jira/browse/FLINK-9]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e402b000 nid=0x2b2e runnable 

""Gang worker([#10|https://github.com/stratosphere/stratosphere/issues/10] | [FLINK-10|https://issues.apache.org/jira/browse/FLINK-10]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e402d000 nid=0x2b2f runnable 

""Gang worker([#11|https://github.com/stratosphere/stratosphere/issues/11] | [FLINK-11|https://issues.apache.org/jira/browse/FLINK-11]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e402f000 nid=0x2b30 runnable 

""Gang worker([#12|https://github.com/stratosphere/stratosphere/issues/12] | [FLINK-12|https://issues.apache.org/jira/browse/FLINK-12]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4030800 nid=0x2b31 runnable 

""Gang worker([#13|https://github.com/stratosphere/stratosphere/issues/13] | [FLINK-13|https://issues.apache.org/jira/browse/FLINK-13]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4032800 nid=0x2b32 runnable 

""Gang worker([#14|https://github.com/stratosphere/stratosphere/issues/14] | [FLINK-14|https://issues.apache.org/jira/browse/FLINK-14]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4034800 nid=0x2b33 runnable 

""Gang worker([#15|https://github.com/stratosphere/stratosphere/issues/15] | [FLINK-15|https://issues.apache.org/jira/browse/FLINK-15]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4036000 nid=0x2b34 runnable 

""Gang worker([#16|https://github.com/stratosphere/stratosphere/issues/16] | [FLINK-16|https://issues.apache.org/jira/browse/FLINK-16]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4038000 nid=0x2b35 runnable 

""Gang worker([#17|https://github.com/stratosphere/stratosphere/issues/17] | [FLINK-17|https://issues.apache.org/jira/browse/FLINK-17]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4039800 nid=0x2b36 runnable 

""Gang worker([#18|https://github.com/stratosphere/stratosphere/issues/18] | [FLINK-18|https://issues.apache.org/jira/browse/FLINK-18]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e403b800 nid=0x2b37 runnable 

""Gang worker([#19|https://github.com/stratosphere/stratosphere/issues/19] | [FLINK-19|https://issues.apache.org/jira/browse/FLINK-19]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e403d800 nid=0x2b38 runnable 

""Gang worker([#20|https://github.com/stratosphere/stratosphere/issues/20] | [FLINK-20|https://issues.apache.org/jira/browse/FLINK-20]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e403f000 nid=0x2b39 runnable 

""Gang worker([#21|https://github.com/stratosphere/stratosphere/issues/21] | [FLINK-21|https://issues.apache.org/jira/browse/FLINK-21]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4041000 nid=0x2b3a runnable 

""Gang worker([#22|https://github.com/stratosphere/stratosphere/issues/22] | [FLINK-22|https://issues.apache.org/jira/browse/FLINK-22]) (Parallel GC Threads)"" prio=10 tid=0x00007f98e4043000 nid=0x2b3b runnable 

""VM Periodic Task Thread"" prio=10 tid=0x00007f98e40cd800 nid=0x2b43 waiting on condition 

JNI global references: 1461

```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/109
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, 
Created at: Sat Sep 21 16:46:02 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397477,,,Mon Jun 09 11:33:35 UTC 2014,,,,,,,,,,"0|i1wfjr:",397604,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:33;github-import;[Date: Sat Sep 21 19:29:58 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

The `ByteBufferedChannelManager.logBufferUtilization()` method shows the following stats when the system is in locked state (thanks to @uce ):

There seem to be more buffers requested than designated.

```
Buffer utilization at 1379783754592
        Unused global buffers: 1633
        Local buffer pool status:
                Input gate 1 of Join invoices with active accounts (1/2): 1 available, 1 requested, 27 designated
                Input gate 0 of Join invoices with active accounts (1/2): 0 available, 57 requested, 53 designated
                filter invoices: month <= 24 (1/2): 5 available, 9 requested, 106 designated
                Input gate 0 of Count active billing accs (1/2): 2 available, 2 requested, 53 designated
                Input gate 0 of Join billing accounts with invoices (1/2): 0 available, 27 requested, 53 designated
                Join invoices with active accounts (1/2): 0 available, 0 requested, 53 designated
                Input gate 1 of Join invoices with active accounts (1/2): 1 available, 1 requested, 27 designated
                Count active billing accs (1/2): 0 available, 0 requested, 53 designated
                Join billing accounts with invoices (1/2): 0 available, 0 requested, 53 designated
                Input gate 0 of aggregate Invoices (1/2): 7 available, 7 requested, 53 designated
                Join billing accounts with invoices (1/2): 0 available, 0 requested, 53 designated
                Input gate 0 of filter active billing accounts (1/2): 3 available, 3 requested, 27 designated
                Input gate 0 of filter invoices: month <= 24 (1/2): 0 available, 29 requested, 27 designated
                aggregate Invoices (1/2): 0 available, 0 requested, 106 designated
                Input gate 0 of Join billing accounts with invoices (1/2): 0 available, 53 requested, 53 designated
                Input gate 1 of Join billing accounts with invoices (1/2): 0 available, 0 requested, 53 designated
                Billing accounts file (1/2): 0 available, 2 requested, 53 designated
                Input gate 0 of Join billing accounts with invoices (1/2): 0 available, 27 requested, 53 designated
                Invoices file (1/2): 1 available, 4 requested, 79 designated
                Join billing accounts with invoices (1/2): 0 available, 0 requested, 53 designated
                Input gate 1 of Join billing accounts with invoices (1/2): 0 available, 0 requested, 53 designated
                filter highest rated billing accounts (1/2): 5 available, 78 requested, 211 designated
                Join billing accounts with invoices (1/2): 0 available, 0 requested, 53 designated
                Input gate 1 of Join billing accounts with invoices (1/2): 0 available, 0 requested, 53 designated
                Input gate 1 of Join billing accounts with invoices (1/2): 0 available, 0 requested, 53 designated
                Join invoices with active accounts (1/2): 0 available, 0 requested, 53 designated
                filter active billing accounts (1/2): 0 available, 4 requested, 106 designated
                Input gate 0 of Join invoices with active accounts (1/2): 0 available, 57 requested, 53 designated
                Input gate 0 of Join billing accounts with invoices (1/2): 0 available, 27 requested, 53 designated
                Input gate 0 of filter highest rated billing accounts (1/2): 0 available, 27 requested, 27 designated
```;;;","09/Jun/14 11:33;github-import;[Date: Mon Oct 07 19:37:13 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

This bug did not use to occur. I wonder if it is an effect of the refactoring that removed file buffers. I remember that the logic for asynchronous events was removed. I would have a look at that code in a previous revision and see whether it could have implications to announcing asynchronous buffer availability.;;;","09/Jun/14 11:33;github-import;[Date: Tue Oct 08 10:04:39 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I also had this thought and tested the Job on the commit before my pull request. The same error occurred.
The only thing that happened when an asynchronous event was registered was replaying a checkpoint.

I think the bug has something to do with the complexity of the Job. It has 3 input files, about 20 output sinks, many joins, maps etc.;;;","09/Jun/14 11:33;github-import;[Date: Tue Oct 22 14:40:07 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Has been known to be a weakness in the plan enumeration's deadlock detection algorithm. We need to rework/extend the compiler for that. Runtime turned out to be fine, manual adjustment of the execution strategies resolved the problem.;;;","09/Jun/14 11:33;github-import;[Date: Mon Jan 27 11:00:10 CET 2014, Author: [twalthr|https://github.com/twalthr]]

While implementing the large test (([#379|https://github.com/stratosphere/stratosphere/issues/379] | [FLINK-379|https://issues.apache.org/jira/browse/FLINK-379])), I also encountered this deadlock issue. Unfortunately, I'm not able to do manual adjustment of a Join within a Iteration due to an `UnsupportedOperationException`.

Here is a code snippet of my test code: 

    DeltaIteration iteration = new DeltaIteration(0);
    iteration.setMaximumNumberOfIterations(10000); // Exception otherwise
    
    // Add a flag field to each customer (initial value: false)
    MapOperator customersWithFlag = MapOperator.builder(AddFlag.class)
    .input(customerSource).build();

    iteration.setInitialSolutionSet(customersWithFlag);
    iteration.setInitialWorkset(customersWithFlag);

    // As input for each iteration
    // Exception otherwise
    JoinOperator iterationInput = JoinOperator
        .builder(WorkSolutionSetJoin.class, IntValue.class, 0, 0)
        .input1(iteration.getWorkset())
        .input2(iteration.getSolutionSet()).build();
    iterationInput.setParameter(PactCompiler.HINT_LOCAL_STRATEGY,
				PactCompiler.HINT_LOCAL_STRATEGY_MERGE); // this does not solve the deadlock

Execution fails with Exception:

	Exception in thread ""main"" java.lang.UnsupportedOperationException
		at java.util.AbstractList.remove(AbstractList.java:161)
		at java.util.AbstractList$Itr.remove(AbstractList.java:374)
		at java.util.AbstractList.removeRange(AbstractList.java:571)
		at java.util.AbstractList.clear(AbstractList.java:234)
		at eu.stratosphere.compiler.dag.MatchNode.fixDriverStrategy(MatchNode.java:203)
		at eu.stratosphere.compiler.PactCompiler$GraphCreatingVisitor.postVisit(PactCompiler.java:1031)
		at eu.stratosphere.compiler.PactCompiler$GraphCreatingVisitor.postVisit(PactCompiler.java:1)
		at eu.stratosphere.api.common.operators.DualInputOperator.accept(DualInputOperator.java:264)
		at eu.stratosphere.api.common.operators.SingleInputOperator.accept(SingleInputOperator.java:164)
		at eu.stratosphere.api.common.operators.SingleInputOperator.accept(SingleInputOperator.java:164)
		at eu.stratosphere.api.common.operators.SingleInputOperator.accept(SingleInputOperator.java:164)
		at eu.stratosphere.api.common.operators.DualInputOperator.accept(DualInputOperator.java:259)
		at eu.stratosphere.api.common.operators.GenericDataSink.accept(GenericDataSink.java:376)
		at eu.stratosphere.api.common.Plan.accept(Plan.java:267)
		at eu.stratosphere.compiler.PactCompiler.compile(PactCompiler.java:661)
		at eu.stratosphere.compiler.PactCompiler.compile(PactCompiler.java:543)
		at eu.stratosphere.client.LocalExecutor.executePlan(LocalExecutor.java:92)
		at eu.stratosphere.client.LocalExecutor.execute(LocalExecutor.java:150)
		at eu.stratosphere.test.testPrograms.manyFunctions.ManyFunctions.main(ManyFunctions.java:74);;;","09/Jun/14 11:33;github-import;[Date: Thu Feb 27 18:04:03 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

I think this is the streaming plan deadlock that @markus-h is fixing now?;;;","09/Jun/14 11:33;github-import;[Date: Mon Mar 03 15:40:27 CET 2014, Author: [markus-h|https://github.com/markus-h]]

If it is here is the corresponding pull request: ([#527|https://github.com/stratosphere/stratosphere/issues/527] | [FLINK-527|https://issues.apache.org/jira/browse/FLINK-527]) 
Can somebody confirm?;;;","09/Jun/14 11:33;github-import;[Date: Mon Mar 03 15:42:46 CET 2014, Author: [rmetzger|https://github.com/rmetzger]]

Hi,
exactly. @markus-h tested his PR with @twalthr's code.
My implementation is not relevant anymore, but the PR is fixing the issue I had back then. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ignore system files when using FileInputFormat,FLINK-108,12719277,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:33,09/Jun/14 11:33,14/Jul/23 05:57,09/Jun/14 11:33,,,,pre-apache,,,,,,,0,github-import,,"The FileInputFormat adds files that start with an underscore ( _ ) when the given input file is a directory during the creation of InputSplits.

MapReduce jobs create an empty `_SUCCESS` file and a `_logs` directory. These are not relevant when reading the files.

This error also caused empty InputSplits.
FileInputFormat is the base class for most InputFormats.
Hadoop has a smiliar mechanism, which is more advanced (they have a Filter-system that supports multiple filters to be registered at the file system). My implementation does use the same filter as their FileInputFormat. This approach also allows to chain filters.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/108
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Sat Sep 21 14:46:17 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:33;github-import;pull-request-108-3825429259075986017.patch;https://issues.apache.org/jira/secure/attachment/12648991/pull-request-108-3825429259075986017.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397476,,,Mon Jun 09 11:33:23 UTC 2014,,,,,,,,,,"0|i1wfjj:",397603,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:33;github-import;[Date: Sat Sep 21 15:47:44 CEST 2013, Author: [fhueske|https://github.com/fhueske]]

Why not make the filter fully configurable instead of static?
Filter on _* and .* could be default setting.;;;","09/Jun/14 11:33;github-import;[Date: Sun Sep 29 17:07:48 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

You are right. It would be actually better to make it configurable, so that it is not necessary to extend the existing FileInputFormat. I'm just too lazy to implement it and I guess the feature is not super relevant, since nobody needed it so far.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AllReducer does not work in iterations with dop>1 (added test case),FLINK-107,12719276,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:33,09/Jun/14 11:33,14/Jul/23 05:57,09/Jun/14 11:33,,,,pre-apache,,,,,,,0,github-import,,"Compiler throws the following error when I use the AllReducer inside iterations with dop>1:
```eu.stratosphere.pact.compiler.CompilerException: Error: All functions that are part of an iteration must have the same degree-of-parallelism as that iteration.```
The dop is automatically set to 1 for the AllReducer (which absolutely makes sense). I extended Roberts test case to catch this bug.

Maybe anyone can explain why we have this restriction? Due to this restrictions I also had to remove some other optimizations where I defined the dop to be 1 for some contracts manually.

BTW: It would be nice to give a hint in the error message which node has a dop different from iteration (e.g. the name and type).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/107
Created by: [andrehacker|https://github.com/andrehacker]
Labels: bug, 
Created at: Fri Sep 20 21:28:23 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:33;github-import;pull-request-107-3060083351332643241.patch;https://issues.apache.org/jira/secure/attachment/12648990/pull-request-107-3060083351332643241.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397475,,,Mon Jun 09 11:33:18 UTC 2014,,,,,,,,,,"0|i1wfjb:",397602,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:33;github-import;[Date: Fri Oct 11 12:20:54 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Th iterations currently have the restriction that all operators have to have the same degree-of-parallelism as the iteration operator. Since All_Reduce produces a final DOP=1 operator, it is rejected.

The rational behind all nodes being of the same DOP is the relationship between nodes and iteration heads, for the distributed aggregators. I assume that an operator with a DOP < Iteration DOP should actually work, so we might be able to relax that condition to allow ALL_REDUCE.
;;;","09/Jun/14 11:33;github-import;[Date: Tue Oct 22 15:14:29 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I looked through the code and it seems like we can run all operators with DOP < Iteration-DOP.

@andrehacker Can you create a simple test case out of this issue?;;;","09/Jun/14 11:33;github-import;[Date: Tue Oct 22 15:19:02 CEST 2013, Author: [andrehacker|https://github.com/andrehacker]]

Good to hear! 
The test case is already done - see above (if this is what you mean);;;","09/Jun/14 11:33;github-import;[Date: Tue Oct 22 15:21:11 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The above code only tests whether the compiler complains, not whether it actually works at runtime. We would need a test like https://github.com/stratosphere/stratosphere/blob/master/pact/pact-tests/src/test/java/eu/stratosphere/pact/test/pactPrograms/ComputeEdgeDegreesITCase.java

Is that feasible?;;;","09/Jun/14 11:33;github-import;[Date: Tue Oct 22 15:27:06 CEST 2013, Author: [andrehacker|https://github.com/andrehacker]]

this makes sense, should be no problem, will do it asap.;;;","09/Jun/14 11:33;github-import;[Date: Thu Nov 21 12:00:35 CET 2013, Author: [andrehacker|https://github.com/andrehacker]]

I wrote the test case, but there is a strange behaviour:
If I specify N iterations, the UDF will be called N+1 times. Is this normal?
The last time it is called with an empty iteration. The result of the last call is ignored.

See the code here:
https://github.com/andrehacker/ozone/blob/all-reduce-iteration-test/pact/pact-tests/src/test/java/eu/stratosphere/pact/test/iterative/IterationWithAllReducer.java#L120;;;","09/Jun/14 11:33;github-import;[Date: Thu Nov 21 13:31:15 CET 2013, Author: [uce|https://github.com/uce]]

Yes, this is normal.

After the n-th iteration, the iteration head will propagate the termination event to the intermediate and tail tasks. If I'm not mistaken, this results in the n+1th UDF call.;;;","09/Jun/14 11:33;github-import;[Date: Mon Nov 25 17:05:57 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Subsumed by ([#280|https://github.com/stratosphere/stratosphere/issues/280] | [FLINK-280|https://issues.apache.org/jira/browse/FLINK-280]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added jquery library to the js sources (external sources load does not w...,FLINK-106,12719275,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:33,09/Jun/14 11:33,14/Jul/23 05:57,09/Jun/14 11:33,,,,pre-apache,,,,,,,0,github-import,,"Added jquery library to the js sources don't work on the current stratosphere.eu server).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/106
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: website, 
Created at: Fri Sep 20 16:57:11 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:33;github-import;pull-request-106-8029828529839898339.patch;https://issues.apache.org/jira/secure/attachment/12648989/pull-request-106-8029828529839898339.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397474,,,2014-06-09 11:33:02.0,,,,,,,,,,"0|i1wfj3:",397601,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added ALL_GROUP strategy to PlanJSONDumpGenerator.,FLINK-105,12719274,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:32,09/Jun/14 11:33,14/Jul/23 05:57,09/Jun/14 11:33,,,,pre-apache,,,,,,,0,github-import,,"This fixes https://github.com/dimalabs/ozone/issues/99

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/105
Created by: [andrehacker|https://github.com/andrehacker]
Labels: 
Created at: Thu Sep 19 18:50:39 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:32;github-import;pull-request-105-8851950618675330769.patch;https://issues.apache.org/jira/secure/attachment/12648988/pull-request-105-8851950618675330769.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397473,,,2014-06-09 11:32:57.0,,,,,,,,,,"0|i1wfiv:",397600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New version of the Stratosphere website,FLINK-104,12719273,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:28,09/Jun/14 11:31,14/Jul/23 05:57,09/Jun/14 11:30,,,,pre-apache,,,,,,,0,github-import,,"New version of the Stratosphere website using a gh-pages branch.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/104
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: website, 
Created at: Thu Sep 19 16:23:33 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397472,,,Mon Jun 09 11:31:23 UTC 2014,,,,,,,,,,"0|i1wfin:",397599,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:31;github-import;This is a pull request, importing the patch failed because it was too large. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove TeraSort from examples,FLINK-102,12719271,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:28,09/Jun/14 11:28,14/Jul/23 05:57,09/Jun/14 11:28,,,,pre-apache,,,,,,,0,github-import,,"The TeraSort job (one of the example jobs) is currently not working, because of some issues with the range partitioner (see issue ([#7|https://github.com/stratosphere/stratosphere/issues/7] | [FLINK-7|https://issues.apache.org/jira/browse/FLINK-7])).

I think we shouldn't have it in the build until this is fixed.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/102
Created by: [uce|https://github.com/uce]
Labels: bug, 
Created at: Wed Sep 18 14:14:25 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:28;github-import;pull-request-102-1415321658721131.patch;https://issues.apache.org/jira/secure/attachment/12648987/pull-request-102-1415321658721131.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397470,,,Mon Jun 09 11:28:48 UTC 2014,,,,,,,,,,"0|i1wfi7:",397597,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:28;github-import;[Date: Mon Sep 23 16:36:31 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Thanks for adding the comment!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reducer with ALL_GROUP causes error in pact-web visualization,FLINK-99,12719268,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:28,09/Jun/14 11:28,14/Jul/23 05:57,09/Jun/14 11:28,,,,pre-apache,,,,,,,0,github-import,,"Plans with a keyless-Reducer cause the pact web visualization to fail:
```Unknown local strategy 'ALL_GROUP' in JSON generator.```

The switch statement in ```PlanJSONDumpGenerator``` has to be updated (anything else?). The bugfix seems to be simple, however I did want to talk to someone about this before adding a pullrequest.

Stephan recently reimplemented keyless-reducers, see https://github.com/dimalabs/ozone/pull/61

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/99
Created by: [andrehacker|https://github.com/andrehacker]
Labels: bug, 
Created at: Sat Sep 14 22:58:37 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397467,,,Mon Jun 09 11:28:35 UTC 2014,,,,,,,,,,"0|i1wfhj:",397594,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:28;github-import;[Date: Sun Sep 22 10:31:13 CEST 2013, Author: [andrehacker|https://github.com/andrehacker]]

merged;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed bug. stub.open() was not called for COMBININGSORT local strategy.,FLINK-97,12719266,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:28,09/Jun/14 11:28,14/Jul/23 05:57,09/Jun/14 11:28,,,,pre-apache,,,,,,,0,github-import,,"open() method is not called on ReduceStub object which is used for COMBINGSORT strategy. 
Hence, stub might not get correctly initialized.
Also made an exception message more meaningful by adding the name of the stub class that fails.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/97
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Fri Sep 13 20:55:26 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:28;github-import;pull-request-97-8297740650752478809.patch;https://issues.apache.org/jira/secure/attachment/12648986/pull-request-97-8297740650752478809.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397465,,,2014-06-09 11:28:28.0,,,,,,,,,,"0|i1wfh3:",397592,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed nephele-config.sh substr command calls on OS X (closes #88),FLINK-96,12719265,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:28,09/Jun/14 11:28,14/Jul/23 05:57,09/Jun/14 11:28,,,,pre-apache,,,,,,,0,github-import,,"This is issue ([#88|https://github.com/stratosphere/stratosphere/issues/88] | [FLINK-88|https://issues.apache.org/jira/browse/FLINK-88]).

On OS X the `expr substr` (sub-)command is not supported. I replaced it with bash's (extended) syntax for string expansions.

If this is not OK an alternative would be to use `uname -s | cut -c1-6`.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/96
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Fri Sep 13 16:10:48 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:28;github-import;pull-request-96-2718965145449524296.patch;https://issues.apache.org/jira/secure/attachment/12648985/pull-request-96-2718965145449524296.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397464,,,Mon Jun 09 11:28:26 UTC 2014,,,,,,,,,,"0|i1wfgv:",397591,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:28;github-import;[Date: Sun Sep 15 20:50:33 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Does the fixed version also work on cygwin and linux?;;;","09/Jun/14 11:28;github-import;[Date: Sun Sep 15 22:13:52 CEST 2013, Author: [uce|https://github.com/uce]]

Yes, it works on Linux. Didn't test it on Cygwin though. But it should work, right? Does anybody run Windows and can test it with Cygwin?;;;","09/Jun/14 11:28;github-import;[Date: Sun Sep 15 22:17:17 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Let's ask Aljoscha tomorrow, he created the cygwin version.;;;","09/Jun/14 11:28;github-import;[Date: Wed Sep 18 18:48:34 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I did test the script with Windows/cygwin.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlocks / Failed Tests on Travis-CI,FLINK-94,12719263,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:28,09/Jun/14 11:28,14/Jul/23 05:57,09/Jun/14 11:28,,,,pre-apache,,,,,,,0,github-import,,"It seems that tests are randomly failing on travis https://travis-ci.org/dimalabs/ozone.
In order to fix these issues, I'd like to document all failed tests to (hopefully) find a pattern.
Please comment if you see failed tests (with build number)

72.5 eu.stratosphere.pact.runtime.sort.AsynchonousPartialSorterITCase
69.1 eu.stratosphere.pact.test.cancelling.MatchJoinCancelingITCase
60.1 eu.stratosphere.pact.runtime.task.MatchTaskTest

    java.util.ConcurrentModificationException
    at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:819)
    	at java.util.ArrayList$Itr.next(ArrayList.java:791)
    	at eu.stratosphere.nephele.services.memorymanager.spi.DefaultMemoryManager.release(DefaultMemoryManager.java:336)
    	at eu.stratosphere.pact.runtime.hash.BuildSecondHashMatchIterator.closuildSecondHashMatchIterator.java:105)
    	at eu.stratosphere.pact.runtime.task.MatchDriver.cleanup(MatchDriver.java:173)
    	at eu.stratosphere.pact.runtime.test.util.DriverTestBase.testDriver(DriverTestBase.java:190)
    	at eu.stratosphere.pact.runtime.task.MatchTaskTest$5.run(MatchTaskTest.java:810)
56.3 eu.stratosphere.pact.test.cancelling.MatchJoinCancelingITCase
33.3 eu.stratosphere.pact.test.contracts.UnionITCase


On my travis (https://travis-ci.org/rmetzger/ozone/builds)

58.4 eu.stratosphere.pact.test.contracts.CrossITCase

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/94
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, testing, 
Created at: Fri Sep 13 09:08:59 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397462,,,Mon Jun 09 11:28:13 UTC 2014,,,,,,,,,,"0|i1wfgf:",397589,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:28;github-import;[Date: Wed Sep 25 11:18:54 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

It happened again (without a code change),
 93.3 https://travis-ci.org/dimalabs/ozone/jobs/11768081

```
Running eu.stratosphere.pact.runtime.task.MatchTaskTest
java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:782)
	at java.util.ArrayList$Itr.next(ArrayList.java:754)
	at eu.stratosphere.nephele.services.memorymanager.spi.DefaultMemoryManager.release(DefaultMemoryManager.java:336)
	at eu.stratosphere.pact.runtime.hash.BuildSecondHashMatchIterator.close(BuildSecondHashMatchIterator.java:105)
	at eu.stratosphere.pact.runtime.task.MatchDriver.cleanup(MatchDriver.java:173)
	at eu.stratosphere.pact.runtime.test.util.DriverTestBase.testDriver(DriverTestBase.java:190)
	at eu.stratosphere.pact.runtime.task.MatchTaskTest$5.run(MatchTaskTest.java:810)
Tests run: 23, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.297 sec <<< FAILURE!
```;;;","09/Jun/14 11:28;github-import;[Date: Fri Sep 27 21:05:27 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

101.6 
eu.stratosphere.nephele.services.iomanager.IOManagerITCase: No output for 10 minutes;;;","09/Jun/14 11:28;github-import;[Date: Tue Oct 01 16:57:53 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

106.1 https://travis-ci.org/dimalabs/ozone/jobs/12010811
eu.stratosphere.pact.runtime.task.chaining.ChainTaskTest
;;;","09/Jun/14 11:28;github-import;[Date: Wed Oct 09 18:47:07 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

124.2 https://travis-ci.org/dimalabs/ozone/jobs/12317633
Running eu.stratosphere.pact.runtime.task.MatchTaskTest
java.util.ConcurrentModificationException

This is the third time for the `MatchTaskTest`. So I think we need to investigate this!;;;","09/Jun/14 11:28;github-import;[Date: Thu Oct 10 21:09:09 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

131.4
MatchTaskTest: https://s3.amazonaws.com/archive.travis-ci.org/jobs/12381122/log.txt;;;","09/Jun/14 11:28;github-import;[Date: Thu Oct 10 21:16:24 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The source of the problems seems to be the following: The hash iterator code performs the following on close:

1. Close the hash table
2. Release the memory

While the hash table checks if it was closed before (even if not atomically), the memory release may be attempted multiple times concurrently. The treatment of the segments themselves is guarded by the memory manager's lock, but the iterator is created outside, leading to concurrent modifications on the collection of memroy segments to be released.

A fix would be to move the iterator instantiation in 
  eu.stratosphere.nephele.services.memorymanager.spi.DefaultMemoryManager line 321 into the critical section.

Should have no performance impact.;;;","09/Jun/14 11:28;github-import;[Date: Thu Oct 10 21:20:50 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Looking at the memory manager code, there is quite a bit of bookkeeping done. Especially maintaining a set of all memory segments per task is intensive, unless it is bulk released. At the moment, each segment is individually added and removed form/to a hash set.

We may want to reduce this at some point, as it will impact large memory setups. Especially when operators are initialized multiple times, such as in the iterative setups.
;;;","09/Jun/14 11:28;github-import;[Date: Thu Oct 10 21:38:55 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Here is a quick fix for one possible scenario.

https://github.com/StephanEwen/ozone/commit/a33e1a39a15f5360a2ae671e682108ff61dc024a

This is only a quick fix to reduce odds of problems. I think most components have potential concurrency issues when close()/release() is called multiple times concurrently. This could be an issue for clean canceling/failing. A really clean approach would require either an atomic close flag in the components that are cancelled/closed to make sure that the call is processed only once.

Read accesses during regular working stay unsynchronized. We generally assume that when data structures are asynchronously destroyed, it is due to abort/cancel calls. The regular worker threads that have not yet picked up the cancelled status are okay with terminating due to an exception. Given that the operator status is set to ""cancelled"", any exceptions in the worker threads are discarded as side effects of asynchronous cancelling.;;;","09/Jun/14 11:28;github-import;[Date: Fri Dec 06 23:54:56 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [7b0c53b4c501690866523468f6c3e7f06438658c|https://github.com/stratosphere/stratosphere/commit/7b0c53b4c501690866523468f6c3e7f06438658c];;;","09/Jun/14 11:28;github-import;[Date: Tue Dec 10 13:38:29 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

Bug not fixed: https://travis-ci.org/stratosphere/stratosphere/jobs/15221967;;;","09/Jun/14 11:28;github-import;[Date: Tue Dec 10 14:24:19 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:819)
	at java.util.ArrayList$Itr.next(ArrayList.java:791)
	at eu.stratosphere.nephele.services.memorymanager.spi.DefaultMemoryManager.release(DefaultMemoryManager.java:336)
	at eu.stratosphere.pact.runtime.hash.BuildFirstHashMatchIterator.closuildFirstHashMatchIterator.java:96)
	at eu.stratosphere.pact.runtime.task.MatchDriver.cleanup(MatchDriver.java:151)
	at eu.stratosphere.pact.runtime.test.util.DriverTestBase.testDriver(DriverTestBase.java:190)
	at eu.stratosphere.pact.runtime.task.MatchTaskTest$4.run(MatchTaskTest.java:766)
Tests run: 24, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 7.494 sec <<< FAILURE!

Failed tests: 
  testCancelHashMatchTaskWhileBuildFirst(eu.stratosphere.pact.runtime.task.MatchTaskTest)
  testCancelHashMatchTaskWhileBuildFirst(eu.stratosphere.pact.runtime.task.MatchTaskTest);;;","09/Jun/14 11:28;github-import;[Date: Wed Apr 16 18:49:54 CEST 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The concurrent modification exception has been fixed.

Sometimes, I/O heavy tasks time out. I guess that is a VM issue, as it happens only sometimes and has never been observed by anyone on his/her local machine. I think we need to accept that some tests time out once in a while.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
slaves file requires blank line at end (otherwise last slave won't be started),FLINK-93,12719262,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:28,09/Jun/14 11:28,14/Jul/23 05:57,09/Jun/14 11:28,,,,pre-apache,,,,,,,0,github-import,,"The conf/slaves file requires an empty/blank line at the end.
If there is no trailing empty line, the start-cluster.sh (and maybe other files) will not start the last slave.

This issue - and a possible workaround - is described [here|http://linux.derkeiler.com/Mailing-Lists/SuSE/2008-11/msg00196.html].

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/93
Created by: [andrehacker|https://github.com/andrehacker]
Labels: bug, runtime, 
Milestone: Release 0.4
Assignee: [markus-h|https://github.com/markus-h]
Created at: Thu Sep 12 18:20:55 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397461,,,Mon Jun 09 11:28:04 UTC 2014,,,,,,,,,,"0|i1wfg7:",397588,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:28;github-import;[Date: Sat Oct 26 14:33:02 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

This needs to be reworked (its easy to do).

Our system should be able to handle empty lines in the slave file.
Currently, you have to exactly ONE newline at the end of the ""slaves"" file.
If you have two at the end, you'll get this exception
``` 
Cannot extract topology information from line """"
        at eu.stratosphere.nephele.topology.NetworkTopology.fromFile(NetworkTopology.java:68)
        at eu.stratosphere.nephele.instance.cluster.ClusterManager.loadNetworkTopology(ClusterManager.java:432)
        at eu.stratosphere.nephele.instance.cluster.ClusterManager.<init>(ClusterManager.java:316)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at java.lang.Class.newInstance0(Class.java:355)
        at java.lang.Class.newInstance(Class.java:308)
        at eu.stratosphere.nephele.jobmanager.JobManagerUtils.loadInstanceManager(JobManagerUtils.java:128)
        at eu.stratosphere.nephele.jobmanager.JobManager.<init>(JobManager.java:234)
        at eu.stratosphere.nephele.jobmanager.JobManager.main(JobManager.java:388)

``` 

And this error from the `start-cluster.sh` script:
```
ssh: Could not resolve hostname nohup /bin/bash /share/hadoop/stratosphere-dev/stratosphere/bin/../bin/nephele-taskmanager.sh start : Name or service not known
```

;;;","09/Jun/14 11:28;github-import;[Date: Mon Oct 28 09:25:51 CET 2013, Author: [rmetzger|https://github.com/rmetzger]]

@markus-h is assigned to this task.;;;","09/Jun/14 11:28;github-import;[Date: Tue Oct 29 12:49:28 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [bb5d7708d8ddd7791d61b7a3f145cdbfbefa6fc3|https://github.com/stratosphere/stratosphere/commit/bb5d7708d8ddd7791d61b7a3f145cdbfbefa6fc3];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed commons-io missing in lib directory + fixed wrong groupId of commons-io,FLINK-92,12719261,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:27,09/Jun/14 11:28,14/Jul/23 05:57,09/Jun/14 11:28,,,,pre-apache,,,,,,,0,github-import,,"This fixes the bug that commons-io was missing in the lib directory.

Furthermore, a wrong (outdated) groupId for commons-io was specified in pact-clients (the [old pom file|http://repo1.maven.org/maven2/org/apache/commons/commons-io/1.3.2/commons-io-1.3.2.pom) specifies the relocation). I also switched to the commons-io versions that is used by hadoop (2.1].

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/92
Created by: [andrehacker|https://github.com/andrehacker]
Labels: 
Created at: Thu Sep 12 17:07:49 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:27;github-import;pull-request-92-2486532476858672865.patch;https://issues.apache.org/jira/secure/attachment/12648984/pull-request-92-2486532476858672865.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397460,,,Mon Jun 09 11:27:59 UTC 2014,,,,,,,,,,"0|i1wffz:",397587,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:27;github-import;[Date: Fri Sep 13 09:16:02 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I should note that the bug is not covered by any testcase. 
Since the TaskManagers are started using bash-scripts, we can not test for those bugs using maven (at least not right now. It is possible: http://stackoverflow.com/questions/3491937/i-want-to-execute-shell-commands-from-mavens-pom-xml). ;;;","09/Jun/14 11:27;github-import;[Date: Fri Sep 13 11:37:49 CEST 2013, Author: [andrehacker|https://github.com/andrehacker]]

Now I use wildcards to exclude all jetty artifacts.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pact Compiler: Potential fix for a NullPointerException in SinkJoiner,FLINK-91,12719260,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:27,09/Jun/14 11:27,14/Jul/23 05:57,09/Jun/14 11:27,,,,pre-apache,,,,,,,0,github-import,,"I got the following exception during my work on the tlabs usecases.

I'm not sure if this fix is a suitable solution. Since the original plan is quite large, it is not very easy for me to reproduce the bug in a dedicated test case.
  
    Exception in thread ""main"" java.lang.NullPointerException
    	at java.util.ArrayList.addAll(ArrayList.java:559)
    	at eu.stratosphere.pact.compiler.plan.SinkJoiner.computeUnclosedBranchStack(SinkJoiner.java:81)
    	at eu.stratosphere.pact.compiler.PactCompiler$BranchesVisitor.postVisit(PactCompiler.java:1233)
    	at eu.stratosphere.pact.compiler.PactCompiler$BranchesVisitor.postVisit(PactCompiler.java:1)
    	at eu.stratosphere.pact.compiler.plan.TwoInputNode.accept(TwoInputNode.java:809)
    	at eu.stratosphere.pact.compiler.plan.TwoInputNode.accept(TwoInputNode.java:807)
    	at eu.stratosphere.pact.compiler.PactCompiler.compile(PactCompiler.java:703)
    	at eu.stratosphere.pact.compiler.PactCompiler.compile(PactCompiler.java:547)
    	at eu.stratosphere.pact.client.LocalExecutor.executePlan(LocalExecutor.java:79)

This PR also contains a little change to show an meaningful exception if a null-input is given into a contract.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/91
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, 
Created at: Thu Sep 12 16:48:01 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:27;github-import;pull-request-91-8515591664438698791.patch;https://issues.apache.org/jira/secure/attachment/12648983/pull-request-91-8515591664438698791.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397459,,,Mon Jun 09 11:27:54 UTC 2014,,,,,,,,,,"0|i1wffr:",397586,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:27;github-import;[Date: Fri Oct 18 01:07:15 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I have gone through the code and the only way I can reproduce that error is with graphs that are disconnected, something like

<pre>
 sink        sink
   ^          ^
   |          |
source      source
</pre>

In that sense, this mechanism might actually be used as a way to detect such situations.;;;","09/Jun/14 11:27;github-import;[Date: Fri Oct 18 01:40:24 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

The logic catches many situations of disjoint data flow graphs, but not all.

We should be able to extend the branch-handling logic to detect it in all cases. Committed a case that is covered and one that is not covered (test deactivated for now).;;;","09/Jun/14 11:27;github-import;[Date: Fri Oct 18 02:14:21 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Part one fixed in [21bcc0acd0ffe413f3df60fd6683debe19da2f93|https://github.com/stratosphere/stratosphere/commit/21bcc0acd0ffe413f3df60fd6683debe19da2f93]

Issue subsumed by ([#173|https://github.com/stratosphere/stratosphere/issues/173] | [FLINK-173|https://issues.apache.org/jira/browse/FLINK-173]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pact Compiler Bug: ""Cannot subtract more cost then there is"" (no fix, just a testcase)",FLINK-90,12719259,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:27,09/Jun/14 11:27,14/Jul/23 05:57,09/Jun/14 11:27,,,,pre-apache,,,,,,,0,github-import,,"I ran into this bug while trying to create a test case for another compiler bug!

I want to focus onto the t-labs usecases right now, so I open this as an unfixed bug.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/90
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, 
Created at: Thu Sep 12 16:37:06 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:27;github-import;pull-request-90-4401380178837505681.patch;https://issues.apache.org/jira/secure/attachment/12648982/pull-request-90-4401380178837505681.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397458,,,Mon Jun 09 11:27:48 UTC 2014,,,,,,,,,,"0|i1wffj:",397585,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:27;github-import;[Date: Fri Oct 18 21:21:23 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [6dcf74f022f4a127260b0e8d52b772fb28fa2249|https://github.com/stratosphere/stratosphere/commit/6dcf74f022f4a127260b0e8d52b772fb28fa2249];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pact Compiler fix+testcase for SinkJoinerPlanNode,FLINK-89,12719258,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:27,09/Jun/14 11:27,14/Jul/23 05:57,09/Jun/14 11:27,,,,pre-apache,,,,,,,0,github-import,,"Hopefully fixed a bug in the pact compiler for branching plans:
	- added a test case provided by Anja Kunkel from HU Berlin
	- my understanding of the bug is the following: During a check if additional pipeline breakers are necessary,
	the compiler checks for the LocalStrategy of a SinkJoinerPlan. The SinkJoinerPlan is a ""fake"" plan element to group multiple output sinks into one sink (the compiler assumes one root node, this is why a ""fake"" plan node is required).
	The local strategy of SinkJoinerPlan is initialized as NONE, which means that we just pass through the elements (which makes sence since we are talking about a sink) The initialization of a NONE-local strategy assumes only one input, not two.
	I replaced the LocalStrategy with BINARY_NO_OP, which is a NONE for two input.
	The compiler is now able to do its pipline breaker checks.



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/89
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Thu Sep 12 15:33:20 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:27;github-import;pull-request-89-1106458092251855613.patch;https://issues.apache.org/jira/secure/attachment/12648981/pull-request-89-1106458092251855613.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397457,,,Mon Jun 09 11:27:44 UTC 2014,,,,,,,,,,"0|i1wffb:",397584,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:27;github-import;[Date: Fri Sep 27 15:53:50 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Seems to fix a bug I had with several sinks in a plan. Made my day. :dancers: ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Start scripts not working on Mac,FLINK-88,12719257,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:27,09/Jun/14 11:27,14/Jul/23 05:57,09/Jun/14 11:27,,,,pre-apache,,,,,,,0,github-import,,"The start scripts `bin/start-local.sh` etc. are not working correctly on Mac OS X. 

```
ozone  ./bin/start-local.sh 
expr: syntax error
expr: syntax error
expr: syntax error
expr: syntax error
expr: syntax error
expr: syntax error
expr: syntax error
Starting Nephele job manager
```

I think this was introduced by Issue ([#60|https://github.com/stratosphere/stratosphere/issues/60] | [FLINK-60|https://issues.apache.org/jira/browse/FLINK-60]).

The problem seems to be the `expr substr` calls in `nephele-config.sh`. On OS X the manpage for `expr` says the following:

> According to the POSIX standard, the use of string arguments length, substr, index, or match produces undefined results. In this version of expr, these arguments are treated just as their respective string values.""

On Linux systems `expr substr` works fine.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/88
Created by: [uce|https://github.com/uce]
Labels: bug, 
Created at: Thu Sep 12 14:37:20 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397456,,,2014-06-09 11:27:36.0,,,,,,,,,,"0|i1wff3:",397583,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Readme.md with project overview and a little tutorial.,FLINK-85,12719254,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:27,09/Jun/14 11:27,14/Jul/23 05:57,09/Jun/14 11:27,,,,pre-apache,,,,,,,0,github-import,,"Please review the new README.md.
I'll commit it tomorrow, if there are no comments.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/85
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Sep 09 17:34:33 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:27;github-import;pull-request-85-1724053312255535742.patch;https://issues.apache.org/jira/secure/attachment/12648980/pull-request-85-1724053312255535742.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397453,,,2014-06-09 11:27:24.0,,,,,,,,,,"0|i1wfef:",397580,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Global (Boradcast) Variables,FLINK-83,12719251,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:27,09/Jun/14 11:27,14/Jul/23 05:57,09/Jun/14 11:27,,,,pre-apache,,,,,,,0,github-import,,"Something like Hadoop's DistributedCache

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/83
Created by: [dimalabs|https://github.com/dimalabs]
Labels: 
Milestone: Release 0.5
Created at: Fri Sep 06 16:15:50 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397450,,,Mon Jun 09 11:27:20 UTC 2014,,,,,,,,,,"0|i1wfdr:",397577,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:27;github-import;[Date: Fri Feb 07 10:45:14 CET 2014, Author: [fhueske|https://github.com/fhueske]]

Will be fixed with ([#460|https://github.com/stratosphere/stratosphere/issues/460] | [FLINK-460|https://issues.apache.org/jira/browse/FLINK-460]).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace PactRecord Datamodel,FLINK-81,12719249,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:27,09/Jun/14 11:27,14/Jul/23 05:57,09/Jun/14 11:27,,,,pre-apache,,,,,,,0,github-import,,"This is a joint task with Andreas.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/81
Created by: [dimalabs|https://github.com/dimalabs]
Labels: 
Milestone: Release 0.5
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Fri Sep 06 16:14:21 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397448,,,Mon Jun 09 11:27:12 UTC 2014,,,,,,,,,,"0|i1wfdb:",397575,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:27;github-import;[Date: Thu Mar 06 12:19:50 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

This issue is addressed by the new java api (([#463|https://github.com/stratosphere/stratosphere/issues/463] | [FLINK-463|https://issues.apache.org/jira/browse/FLINK-463]));;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup issues with instance types,FLINK-78,12719246,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:27,09/Jun/14 11:27,14/Jul/23 05:57,09/Jun/14 11:27,,,,pre-apache,,,,,,,0,github-import,,"This task is for Tobias and Andreas (I don't know their github accounts)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/78
Created by: [dimalabs|https://github.com/dimalabs]
Labels: 
Milestone: Release 0.5
Created at: Fri Sep 06 16:10:37 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397445,,,Mon Jun 09 11:27:03 UTC 2014,,,,,,,,,,"0|i1wfcn:",397572,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:27;github-import;[Date: Wed Oct 16 15:06:10 CEST 2013, Author: [uce|https://github.com/uce]]

Some questions regarding this:
- Do we actually need to distinguish between different instance types at all?
- Wasn't this only relevant for the EC2 cloud manager which we removed?
- Doesn't  YARN make different instance types (which we need to manage) obsolete?

Can someone educate me on this issue? :bowtie:;;;","09/Jun/14 11:27;github-import;[Date: Sun Oct 20 20:24:20 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Since instances do not appear in the configuration any more, this issue is not visible to users any more.

The internal workings of the instance manager need to be rethought, but it should currently work if there is always just one simple single default instance against which all instances are matched.;;;","09/Jun/14 11:27;github-import;[Date: Sun Oct 20 20:36:22 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

This issue dates back to our first Stratosphere open source meeting. The ""bug situation"" has changed, so I'm moving this issue to a later release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Documentation: Github readme, quickstart guide ...",FLINK-77,12719245,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:26,09/Jun/14 11:27,14/Jul/23 05:57,09/Jun/14 11:27,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/77
Created by: [dimalabs|https://github.com/dimalabs]
Labels: documentation, 
Milestone: Release 0.4
Assignee: [uce|https://github.com/uce]
Created at: Fri Sep 06 16:09:58 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397444,,,Mon Jun 09 11:26:59 UTC 2014,,,,,,,,,,"0|i1wfcf:",397571,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:26;github-import;[Date: Sat Oct 26 16:26:13 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

The current documentation on the website is in a non acceptable state.

Links are not working, the Getting Started guide is helplessly outdated, the Images are not in the right proportion.;;;","09/Jun/14 11:26;github-import;[Date: Sat Oct 26 16:28:02 CEST 2013, Author: [uce|https://github.com/uce]]

Btw do we also want to publish the javadocs in the near future?

In parts they are also in a non acceptable state.

On Oct 26, 2013, at 4:26 PM, Robert Metzger <notifications@github.com> wrote:

> The current documentation on the website is in a non acceptable state.
> 
> Links are not working, the Getting Started guide is helplessly outdated, the Images are not in the right proportion.
> 
> —
> Reply to this email directly or view it on GitHub.
> ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redesign stratosphere.eu website,FLINK-76,12719244,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:26,09/Jun/14 11:26,14/Jul/23 05:57,09/Jun/14 11:26,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/76
Created by: [dimalabs|https://github.com/dimalabs]
Labels: 
Milestone: Release 0.4
Created at: Fri Sep 06 16:09:09 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397443,,,Mon Jun 09 11:26:55 UTC 2014,,,,,,,,,,"0|i1wfc7:",397570,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:26;github-import;[Date: Mon Sep 16 22:03:57 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Preview of @aalexandrov's work: http://rmetzger.github.io/ozone/;;;","09/Jun/14 11:26;github-import;[Date: Tue Sep 17 17:50:52 CEST 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

New version of the website is available, minus the input from @ktzoumas , at http://aalexandrov.github.io/ozone. I encourage everybody to check for errors (dead links, inappropriate text, etc.). The homepage will probably be reworked in the coming weeks but this is not time critical.

Once we have the full text for the Stratosphere workshop I'll merge the gh-pages branch with the main repository.

@ktzoumas: Can you do a pull request with your modification of the `events/2013/workshop.html` page to my fork of the ozone repository?
    
;;;","09/Jun/14 11:26;github-import;[Date: Thu Sep 19 16:25:34 CEST 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

Closed by pull request ([#104|https://github.com/stratosphere/stratosphere/issues/104] | [FLINK-104|https://issues.apache.org/jira/browse/FLINK-104]).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port Yarn support to ozone,FLINK-75,12719243,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:26,09/Jun/14 11:26,14/Jul/23 05:57,09/Jun/14 11:26,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/75
Created by: [dimalabs|https://github.com/dimalabs]
Labels: 
Milestone: Release 0.4
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Fri Sep 06 15:47:10 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397442,,,2014-06-09 11:26:49.0,,,,,,,,,,"0|i1wfbz:",397569,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix wrong inputSplits with big BinaryInputsFiles,FLINK-74,12719242,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:26,09/Jun/14 11:26,14/Jul/23 05:57,09/Jun/14 11:26,,,,pre-apache,,,,,,,0,github-import,,"Hi Everyone,

i got some trouble using SequentialInputFormat with one big file and found two bugs in BinaryInputFormat.

1. Unnecessary empty input splits are created if more minNumSplits requested than input files are present.
2. This empty splits are created using invalid ranges. HDFS returns an empty block locations array in this case and ends up with an IndexOutOfBoundsException two lines further.

I am not sure how to test the second bug, but attached a test against the first one.

Regards,
Sascha

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/74
Created by: [derSascha|https://github.com/derSascha]
Labels: 
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Thu Sep 05 20:48:23 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:26;github-import;pull-request-74-93204359727298991.patch;https://issues.apache.org/jira/secure/attachment/12648979/pull-request-74-93204359727298991.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397441,,,Mon Jun 09 11:26:47 UTC 2014,,,,,,,,,,"0|i1wfbr:",397568,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:26;github-import;[Date: Thu Sep 12 15:15:42 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Thank you for your pull request!
I don't understand the second bug: Our code is not able to handle empty block locations from HDFS?

Your pull request looks good, but I'd like to wait for Stephan Ewen (or main developer) .. he has a bit more overview over the project. He is currently on vacation and will be back in 3 weeks.
;;;","09/Jun/14 11:26;github-import;[Date: Fri Oct 11 11:46:26 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Looks good to me!;;;","09/Jun/14 11:26;github-import;[Date: Sat Oct 12 19:09:34 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Merged manually in https://github.com/dimalabs/ozone/commit/4950f1e8a700d9c545b59dd2b5389338ec76f293 and subsequent commits.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switch to using objects instead of classes for usercode,FLINK-73,12719241,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:26,09/Jun/14 11:26,14/Jul/23 05:57,09/Jun/14 11:26,,,,pre-apache,,,,,,,0,github-import,,"Hi Everyone,
I'm opening this pull request to start a discussion on this new feature. I changed the whole stratosphere stack to use objects instead of classes to pass usercode (i.e. Stubs) around. This means that one now uses
MapContract mapper = MapContract.builder(new TokenizeLine())
			.input(source)
			.name(""Tokenize Lines"")
			.build();
instead of
MapContract mapper = MapContract.builder(TokenizeLine.class)
			.input(source)
			.name(""Tokenize Lines"")
			.build();

The initial motivation for this is that I require it for the scala-frontend. I cannot generate classes that would be instantiable by the stratosphere runtime so I need to pass objects to the PACTs. I noticed though that this could also make regular java usercode more concise. The Configuration would not be necessary any more because one can just store the configuration in the stub object and for example have a stub with a constructor where you pass in stuff, i.e.:
MapContract mapper = MapContract.builder(new TokenizeLine(""<some-line-ending>""))
			.input(source)
			.name(""Tokenize Lines"")
			.build();
The line ending would then be stored in the stub, the runtime serializes everything and then when the job is run the runtime deserializes the job and the information is available in every instance of the stub.

On thing that is affected by the change is that all usercode stubs now must be serializable. For this I made base class Stub ""Serializable"" and also other things such as Value and the derived classes.

Now, discuss away... :D

Aljoscha

P.S. We could then also have
        FileDataSource source = new FileDataSource(new TextInputFormat(""ASCII""), dataInput, ""Input Lines"");
instead of
	FileDataSource source = new FileDataSource(TextInputFormat.class, dataInput, ""Input Lines"");
	source.setParameter(TextInputFormat.CHARSET_NAME, ""ASCII"");


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/73
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Milestone: Release 0.4
Created at: Tue Sep 03 13:54:22 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:26;github-import;pull-request-73-3124669180584540877.patch;https://issues.apache.org/jira/secure/attachment/12648978/pull-request-73-3124669180584540877.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397440,,,Mon Jun 09 11:26:41 UTC 2014,,,,,,,,,,"0|i1wfbj:",397567,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:26;github-import;[Date: Mon Sep 30 11:59:10 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

I now changed it so that the old interface stays intact while adding the possibility to pass objects.;;;","09/Jun/14 11:26;github-import;[Date: Mon Sep 30 16:56:46 CEST 2013, Author: [uce|https://github.com/uce]]

:+1: ;;;","09/Jun/14 11:26;github-import;[Date: Mon Sep 30 16:57:08 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I tested the pull request successfully with my telecom-usecase stuff. Worked very well.;;;","09/Jun/14 11:26;github-import;[Date: Wed Oct 02 08:45:51 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Cascading is also passing objects instead of classes. So this approach is not as uncommon in the Hadoop world, as we probably thought.;;;","09/Jun/14 11:26;github-import;[Date: Sat Oct 19 16:29:22 CEST 2013, Author: [uce|https://github.com/uce]]

Users implementing a stub have to also implement the Serializable interface when passing objects, e.g.:
`public static class TokenizeLines extends MapStub implements Serializable { ... } `


Can't we have the stubs directly implement Serializable instead of the user?;;;","09/Jun/14 11:26;github-import;[Date: Sat Oct 19 16:49:23 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

No, see issue ([#147|https://github.com/stratosphere/stratosphere/issues/147] | [FLINK-147|https://issues.apache.org/jira/browse/FLINK-147]) ;;;","09/Jun/14 11:26;github-import;[Date: Sat Oct 19 16:52:04 CEST 2013, Author: [uce|https://github.com/uce]]

I totally forgot this... sorry. :sob: ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ozone build fails for non-yarn hadoop (pact-hbase fails),FLINK-72,12719240,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:20,09/Jun/14 11:20,14/Jul/23 05:57,09/Jun/14 11:20,,,,pre-apache,,,,,,,0,github-import,,"Compiling ozone for a non-yarn version is no longer possible. I tried with the latest stable version 1.2.1.

```mvn -P hadoop_v1 -D hadoop.version=1.2.1 -DskipTests clean compile```

The reason is that pact-hbase uses the constructor ```TaskAttemptIDsPattern(String jtIdentifier, Integer jobId, TaskType type, Integer taskId, Integer attemptId) ``` which is available in yarn only.
Yarn: http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TaskAttemptID.html
1.2.1 http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/TaskAttemptID.html

I was about to move to yarn anyway, but everyone who tries to build a ozone running on the 4-node cluster (with old hadoop) will run into this problem.

Here the complete log
```
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /home/andre/dev/ozone-repo/pact/pact-hbase/src/main/java/eu/stratosphere/pact/common/io/GenericTableOutputFormat.java:[69,49] cannot find symbol
symbol  : constructor TaskAttemptID(java.lang.String,int,org.apache.hadoop.mapreduce.TaskType,int,int)
location: class org.apache.hadoop.mapreduce.TaskAttemptID
[ERROR] /home/andre/dev/ozone-repo/pact/pact-hbase/src/main/java/eu/stratosphere/pact/common/io/GenericTableOutputFormat.java:[71,68] package org.apache.hadoop.mapreduce.task does not exist
[INFO] 2 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] ozone ............................................. SUCCESS [0.289s]
[INFO] nephele ........................................... SUCCESS [0.013s]
[INFO] nephele-common .................................... SUCCESS [6.103s]
[INFO] nephele-management ................................ SUCCESS [0.496s]
[INFO] nephele-server .................................... SUCCESS [2.112s]
[INFO] nephele-profiling ................................. SUCCESS [0.267s]
[INFO] nephele-queuescheduler ............................ SUCCESS [0.114s]
[INFO] nephele-clustermanager ............................ SUCCESS [0.198s]
[INFO] nephele-hdfs ...................................... SUCCESS [0.502s]
[INFO] nephele-s3 ........................................ SUCCESS [0.655s]
[INFO] nephele-visualization ............................. SUCCESS [0.582s]
[INFO] nephele-examples .................................. SUCCESS [0.202s]
[INFO] pact .............................................. SUCCESS [0.006s]
[INFO] pact-common ....................................... SUCCESS [1.082s]
[INFO] pact-array-datamodel .............................. SUCCESS [0.199s]
[INFO] pact-runtime ...................................... SUCCESS [1.636s]
[INFO] pact-compiler ..................................... SUCCESS [1.171s]
[INFO] pact-hbase ........................................ FAILURE [0.374s]
[INFO] pact-examples ..................................... SKIPPED
[INFO] pact-compiler-tests ............................... SKIPPED
[INFO] pact-clients ...................................... SKIPPED
[INFO] pact-tests ........................................ SKIPPED
[INFO] stratosphere-dist ................................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 16.642s
[INFO] Finished at: Fri Aug 30 16:06:37 CEST 2013
[INFO] Final Memory: 28M/260M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project pact-hbase: Compilation failure: Compilation failure:
[ERROR] /home/andre/dev/ozone-repo/pact/pact-hbase/src/main/java/eu/stratosphere/pact/common/io/GenericTableOutputFormat.java:[69,49] cannot find symbol
[ERROR] symbol  : constructor TaskAttemptID(java.lang.String,int,org.apache.hadoop.mapreduce.TaskType,int,int)
[ERROR] location: class org.apache.hadoop.mapreduce.TaskAttemptID
[ERROR] /home/andre/dev/ozone-repo/pact/pact-hbase/src/main/java/eu/stratosphere/pact/common/io/GenericTableOutputFormat.java:[71,68] package org.apache.hadoop.mapreduce.task does not exist
[ERROR] -> [Help 1]
[ERROR] 


```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/72
Created by: [andrehacker|https://github.com/andrehacker]
Labels: bug, 
Created at: Fri Aug 30 16:19:23 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397439,,,Mon Jun 09 11:20:21 UTC 2014,,,,,,,,,,"0|i1wfbb:",397566,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:20;github-import;[Date: Mon Sep 16 09:38:55 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

@andrehacker: I think we can close this issue now, because https://github.com/dimalabs/ozone/pull/71 fixes it;;;","09/Jun/14 11:20;github-import;[Date: Mon Sep 16 09:40:26 CEST 2013, Author: [andrehacker|https://github.com/andrehacker]]

I agree!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven-profiles for hadoop v1 and yarn + default to hadoop_v1 + hbase bugfix,FLINK-71,12719239,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:20,09/Jun/14 11:20,14/Jul/23 05:57,09/Jun/14 11:20,,,,pre-apache,,,,,,,0,github-import,,"### Maven Profiles
Here is the proposal to add maven profiles to easily switch between hadoop versions (as discussed last friday):
* There are two maven profiles: hadoop_yarn and hadoop_v1 (default).
* Maven profiles can be activated with the -P option
* The default profile will be deactivated as soon as another profile becomes active
* I also introduced a new hadoop.version property, that can be changed using the -D option

How to build for yarn using a specific version (e.g. 2.1.0-beta)
```mvn -Phadoop_yarn -Dhadoop.version=<version>  <goal>```

How to build for any hadoop v1 version (will use hadoop-core):
```mvn -Dhadoop.version=<version> <goal>```

The solution was inspired by mahout and giraph https://github.com/apache/giraph.

### Default to hadoop_v1
Addition from Alexander: The default profile is now hadoop v1 (version 1.2.1)

### pact-hbase Bugfix
Addition from Alexander: Currently the ozone build fails for non-yarn hadoop. Pact-hbase does not work with Hadoop v1 because it uses Yarn classes. To solve this, pact-hbase is excluded if the hadoop_v1 profile is enabled.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/71
Created by: [andrehacker|https://github.com/andrehacker]
Labels: 
Created at: Fri Aug 30 16:04:13 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:20;github-import;pull-request-71-4329136314159885137.patch;https://issues.apache.org/jira/secure/attachment/12648975/pull-request-71-4329136314159885137.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397438,,,2014-06-09 11:20:14.0,,,,,,,,,,"0|i1wfb3:",397565,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove pact-examples dependency in pact-clients.,FLINK-70,12719238,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:20,09/Jun/14 11:20,14/Jul/23 05:57,09/Jun/14 11:20,,,,pre-apache,,,,,,,0,github-import,,"pact-clients was dependent on pact-examples in order to run
LocalExecuterTest. The problem with that was, that this prevented the
use of the LocalExecuter within the pact-examples module (circular
dependency). Hence the LocalExecuterTest was moved to the pact-test
module and the pact-examples depedency was removed from pact-clients.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/70
Created by: [carabolic|https://github.com/carabolic]
Labels: 
Created at: Wed Aug 28 16:39:59 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:20;github-import;pull-request-70-3056901349126325733.patch;https://issues.apache.org/jira/secure/attachment/12648974/pull-request-70-3056901349126325733.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397437,,,Mon Jun 09 11:20:12 UTC 2014,,,,,,,,,,"0|i1wfav:",397564,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:20;github-import;[Date: Wed Aug 28 16:52:31 CEST 2013, Author: [carabolic|https://github.com/carabolic]]

Hi,

the problem with the old approach (having pact-examples as an dependency for pact-clients) was that you could not locally test your new examples, if they were part of the pact-examples module, with *.pact.clients.LocalExecutor. Doing so would have produced a circular dependency (pact-clients -> pact-examples -> pact-clients -> Call of Cthulhu). With my change you can now safely write and test new pact-examples without summoning the demons.

Regards,
Christoph;;;","09/Jun/14 11:20;github-import;[Date: Thu Sep 12 11:32:16 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Merge please;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removed Nephele plugins,FLINK-69,12719237,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:20,09/Jun/14 11:20,14/Jul/23 05:57,09/Jun/14 11:20,,,,pre-apache,,,,,,,0,github-import,,"As discussed last Friday, I removed the plugins feature from Nephele.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/69
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Mon Aug 26 14:58:27 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:20;github-import;pull-request-69-6656261903243214254.patch;https://issues.apache.org/jira/secure/attachment/12648973/pull-request-69-6656261903243214254.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397436,,,Mon Jun 09 11:20:07 UTC 2014,,,,,,,,,,"0|i1wfan:",397563,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:20;github-import;[Date: Thu Oct 10 14:31:47 CEST 2013, Author: [uce|https://github.com/uce]]

My comment on this was too short. (We also discussed this offline which we probably shouldn't have in the first place.)

Nephele plugins allow to rewrite the `ExecutionGraph` at runtime. This kind of low level interaction with the system is imho not useful (and if I remember correctly the consensus was that it is not used by anybody or just a single person?!).

If we still agree on this issue, we should finally merge it. It increases the ""complexity"" of the Nephele layer and I don't think that we want to support this kind of job manipulation in the future.;;;","09/Jun/14 11:20;github-import;[Date: Thu Oct 10 17:41:11 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I'm going to discuss the pull request tomorrow with Stephan. I think we are going to merge it.;;;","09/Jun/14 11:20;github-import;[Date: Thu Oct 10 17:58:04 CEST 2013, Author: [sscdotopen|https://github.com/sscdotopen]]

In general I'd always vote in favor of removing stuff that is rarely/never used. Less code means less bugs and less maintenance. ;;;","09/Jun/14 11:20;github-import;[Date: Fri Oct 11 11:43:31 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged in [921278e322eaae008dae75e20a505f53aaeed1ca|https://github.com/stratosphere/stratosphere/commit/921278e322eaae008dae75e20a505f53aaeed1ca];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroInputFormat for Ozone,FLINK-68,12719236,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:19,09/Jun/14 11:20,14/Jul/23 05:57,09/Jun/14 11:20,,,,pre-apache,,,,,,,0,github-import,,"This pull request contains an early prototype of an AvroInputFormat. Avro is a data serialization system that is widely used with Apache Hadoop. Many processing systems such as Hive or Pig support it.
I need to add support for Avro to Ozone because of customer requirements.

Please note that this pull request is not ready to merge: The test case is not done yet and the only a few primitive datatypes are supported (string, boolean, int).

I'd like to discuss the following questions (with you)
1) Do we want Avro Support in Ozone
2) Should we map the avro types to PactRecords or should we add the avro types to the system?

1) Pro: Widely used, easy to implement
Con: New dependencies; in the course of the implementation of Hive for ozone, we will most likely use HCatalog as a generic interface to many input file formats, including Avro. So one HCatalog is integrated, the AvroInputFormat would be redundant 

2) Avro supports complex data types (http://avro.apache.org/docs/current/spec.html#schema_complex) which we can't map into our current data model (that's at least what I understand).
My current approach is to forbid nested or complex data structures with Avro and translate every AvroRecord into a PactRecord, preserving the order of the fields.


If we agree to add the InputFormat the ozone, I'll extend the test cases and type conversion to all primitive data types .

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/68
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Fri Aug 23 12:24:23 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;pull-request-68-3286842719882105648.patch;https://issues.apache.org/jira/secure/attachment/12648972/pull-request-68-3286842719882105648.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397435,,,2014-06-09 11:19:57.0,,,,,,,,,,"0|i1wfaf:",397562,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Changed to snapshot version; added maven repository for deployment",FLINK-64,12719232,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:19,09/Jun/14 11:19,14/Jul/23 05:57,09/Jun/14 11:19,,,,pre-apache,,,,,,,0,github-import,,"Added Maven Repo support (https://oss.sonatype.org/content/repositories/snapshots/eu/stratosphere/)
Snapshots are not pushed to Maven Central!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/64
Created by: [AHeise|https://github.com/AHeise]
Labels: 
Created at: Mon Aug 19 15:50:59 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;pull-request-64-8677813804847504336.patch;https://issues.apache.org/jira/secure/attachment/12648971/pull-request-64-8677813804847504336.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397431,,,Mon Jun 09 11:19:44 UTC 2014,,,,,,,,,,"0|i1wf9j:",397558,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;[Date: Tue Oct 01 10:21:52 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Hi @AHeise,
I'm currently merging the change into the main code line. But travis was unable to upload to sonatype:
```
[INFO] --- maven-deploy-plugin:2.7:deploy (default-deploy) @ ozone ---
Downloading: https://oss.sonatype.org/content/repositories/snapshots/eu/stratosphere/ozone/0.4-ozone-SNAPSHOT/maven-metadata.xml
Uploading: https://oss.sonatype.org/content/repositories/snapshots/eu/stratosphere/ozone/0.4-ozone-SNAPSHOT/ozone-0.4-ozone-20131001.081749-1.pom
Oct 01, 2013 8:17:49 AM org.apache.commons.httpclient.auth.AuthChallengeProcessor selectAuthScheme
INFO: basic authentication scheme selected
Oct 01, 2013 8:17:50 AM org.apache.commons.httpclient.HttpMethodDirector processWWWAuthChallenge
INFO: Failure authenticating with BASIC 'Sonatype Nexus Repository Manager'@oss.sonatype.org:443
```

https://travis-ci.org/rmetzger/ozone/jobs/11997775

Do you have any ideas?

This is the .travis.yml file I was using https://github.com/rmetzger/ozone/blob/AHeise-master/.travis.yml;;;","09/Jun/14 11:19;github-import;[Date: Tue Oct 01 10:41:12 CEST 2013, Author: [AHeise|https://github.com/AHeise]]

Hi Robert,

sounds like an authentication issue. My last ozone built succeeded with the upload but there are some factors that could change that.

The script relies on the travis encryption and it’s very likely that the private/public keys do not match. http://about.travis-ci.org/docs/user/encryption-keys/

There are several solutions.

a)      The best solution would be if I add some dima colleague to the sonatypes repo, so that they can setup their own password with travis encryption with the official dimalabs key. For this option, please follow step 2 in the guide https://docs.sonatype.org/display/Repository/Sonatype+OSS+Maven+Repository+Usage+Guide#SonatypeOSSMavenRepositoryUsageGuide-2.Signup

b)      You could also send me your public key and I try to generate the correct encrypted version of my username/password.

c)       You could tell me when the version is ready and I pull it into my repo, where everything is properly configured.

Please note, that until you change the version to a non-SNAPSHOT version, the artifacts are not uploaded to maven central. They would only be available at https://oss.sonatype.org/content/repositories/snapshots/eu/stratosphere/

Best,

Arvid

From: Robert Metzger [mailto:notifications@github.com]
Sent: Dienstag, 1. Oktober 2013 10:22
To: dimalabs/ozone
Cc: Heise, Arvid
Subject: Re: [ozone] Changed to snapshot version; added maven repository for deployment (([#64|https://github.com/stratosphere/stratosphere/issues/64] | [FLINK-64|https://issues.apache.org/jira/browse/FLINK-64]))


Hi Arvid,
I'm currently merging the change into the main code line. But travis was unable to upload to sonatype:

[INFO] --- maven-deploy-plugin:2.7:deploy (default-deploy) @ ozone ---

Downloading: https://oss.sonatype.org/content/repositories/snapshots/eu/stratosphere/ozone/0.4-ozone-SNAPSHOT/maven-metadata.xml

Uploading: https://oss.sonatype.org/content/repositories/snapshots/eu/stratosphere/ozone/0.4-ozone-SNAPSHOT/ozone-0.4-ozone-20131001.081749-1.pom

Oct 01, 2013 8:17:49 AM org.apache.commons.httpclient.auth.AuthChallengeProcessor selectAuthScheme

INFO: basic authentication scheme selected

Oct 01, 2013 8:17:50 AM org.apache.commons.httpclient.HttpMethodDirector processWWWAuthChallenge

INFO: Failure authenticating with BASIC 'Sonatype Nexus Repository Manager'@oss.sonatype.org:443

https://travis-ci.org/rmetzger/ozone/jobs/11997775

Do you have any ideas?

—
Reply to this email directly or view it on GitHub<https://github.com/dimalabs/ozone/pull/64#issuecomment-25432710>.
;;;","09/Jun/14 11:19;github-import;[Date: Tue Oct 01 16:25:15 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Merged manually in https://github.com/dimalabs/ozone/commit/bbfbc4bac3be5349f6ac8a9e33ee691d85226c91
Thanks to @AHeise  for the help with Travis and Sonatype (maven);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debian / Ubuntu Packaging,FLINK-63,12719231,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:19,09/Jun/14 11:19,14/Jul/23 05:57,09/Jun/14 11:19,,,,pre-apache,,,,,,,0,github-import,,"Hi there,

we added a section to the stratosphere-dist/pom.xml to build a debian package in addition to the java package. We use it to deploy Stratosphere via an Ubuntu repository. I'd be happy if you like it and merge it back.

Thanks,
Christian

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/63
Created by: [christianrichter|https://github.com/christianrichter]
Labels: 
Created at: Fri Aug 16 19:02:01 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;pull-request-63-1816533122945804318.patch;https://issues.apache.org/jira/secure/attachment/12648970/pull-request-63-1816533122945804318.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397430,,,Mon Jun 09 11:19:38 UTC 2014,,,,,,,,,,"0|i1wf9b:",397557,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;[Date: Wed Aug 21 07:23:38 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged pull request in [261baf17c20e8d2ce06dcf06fca743e5aa5a6f5d|https://github.com/stratosphere/stratosphere/commit/261baf17c20e8d2ce06dcf06fca743e5aa5a6f5d].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor bugfix for TextInputFormat + Testcase,FLINK-62,12719230,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:19,09/Jun/14 11:19,14/Jul/23 05:57,09/Jun/14 11:19,,,,pre-apache,,,,,,,0,github-import,,"I found this bug during the development of customer use-cases.

I guess its an easy fix but I want to focus on the use-cases right now (and I have a workaround).

The integration test will obviously fail for this pull request because it contains a test case for an unfixed bug.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/62
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, 
Created at: Mon Aug 12 16:32:25 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;pull-request-62-6030047682972456203.patch;https://issues.apache.org/jira/secure/attachment/12648969/pull-request-62-6030047682972456203.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397429,,,Mon Jun 09 11:19:33 UTC 2014,,,,,,,,,,"0|i1wf93:",397556,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;[Date: Tue Aug 13 11:52:34 CEST 2013, Author: [uce|https://github.com/uce]]

Swapping the failing line with the next one in `TextInputFormat` fixed it for me. The problem was that the current buffer `position` is not allowed to be greater than the current `limit`. I've sent you a patch for this to push to your branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Reducer without grouping key (""ALL_GROUP"") fails in optimizer",FLINK-61,12719229,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:19,09/Jun/14 11:19,14/Jul/23 05:57,09/Jun/14 11:19,,,,pre-apache,,,,,,,0,github-import,,"This pull request contains a test case that validates a bug with the pact-optimizer.

(The test obviously fails, because I did not include a fix for the bug!)
I also fixed a little typo and made an error message a bit more understandable by giving an example.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/61
Created by: [rmetzger|https://github.com/rmetzger]
Labels: bug, 
Created at: Sat Aug 10 17:07:05 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;pull-request-61-2860399956973614787.patch;https://issues.apache.org/jira/secure/attachment/12648968/pull-request-61-2860399956973614787.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397428,,,Mon Jun 09 11:19:29 UTC 2014,,,,,,,,,,"0|i1wf8v:",397555,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;[Date: Fri Aug 16 06:54:56 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged manually in [b8d213da44259b8ed55cb10e01204fac4db8dbe0|https://github.com/stratosphere/stratosphere/commit/b8d213da44259b8ed55cb10e01204fac4db8dbe0] and [88d7305a5267aac598949519275123208195daf7|https://github.com/stratosphere/stratosphere/commit/88d7305a5267aac598949519275123208195daf7].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cygwin Fix Again,FLINK-60,12719228,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:19,09/Jun/14 11:19,14/Jul/23 05:57,09/Jun/14 11:19,,,,pre-apache,,,,,,,0,github-import,,"Somehow I did not have the correct commit in my repository at the time the cygwin feature commit was pulled. So to not disturb the other contributors I made two commits that revert my faulty commit and Stephan's commit on top of that and then added the correct commit on top of that.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/60
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Fri Aug 02 16:36:42 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;pull-request-60-7551060146771514048.patch;https://issues.apache.org/jira/secure/attachment/12648967/pull-request-60-7551060146771514048.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397427,,,2014-06-09 11:19:22.0,,,,,,,,,,"0|i1wf8n:",397554,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed bad assembly of dropin libs in the stratosphere-bin shell scripts.,FLINK-59,12719227,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:19,09/Jun/14 11:19,14/Jul/23 05:57,09/Jun/14 11:19,,,,pre-apache,,,,,,,0,github-import,,"The jarfile loop for the dropins folder doesn't work because of bad folder name (`dropins`, should be `dropin`).

The push request also fixes a bad bash variable name in `pact-webfrontend.sh`.

In addition, I suggest to add the folder itself to the java classpath for extra, non JAR file resources that need to be included in the classpath (e.g. license files for vendor libraries).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/59
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Thu Aug 01 13:16:39 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;pull-request-59-3433264715352224371.patch;https://issues.apache.org/jira/secure/attachment/12648966/pull-request-59-3433264715352224371.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397426,,,2014-06-09 11:19:18.0,,,,,,,,,,"0|i1wf8f:",397553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Smaller backports of 0.2.1 APIs,FLINK-58,12719226,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:19,09/Jun/14 11:19,14/Jul/23 05:57,09/Jun/14 11:19,,,,pre-apache,,,,,,,0,github-import,,"Please add those commits to ozone, so that we can directly link to 0.2-ozone in Sopremo.
Most of these patches either backport small parts of 0.2.1 (to Nephele) or fix some settings.
Message me for further informations.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/58
Created by: [AHeise|https://github.com/AHeise]
Labels: 
Created at: Thu Jul 25 13:10:53 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;pull-request-58-2271343685273481679.patch;https://issues.apache.org/jira/secure/attachment/12648965/pull-request-58-2271343685273481679.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397425,,,Mon Jun 09 11:19:16 UTC 2014,,,,,,,,,,"0|i1wf87:",397552,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;[Date: Mon Aug 05 11:43:48 CEST 2013, Author: [AHeise|https://github.com/AHeise]]

Incorporated your comments;;;","09/Jun/14 11:19;github-import;[Date: Thu Aug 15 07:42:15 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged as of [0f28095f9f79b49d45905e61709ff4fab6fedd54|https://github.com/stratosphere/stratosphere/commit/0f28095f9f79b49d45905e61709ff4fab6fedd54].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Smaller backports of 0.2.1 APIs,FLINK-57,12719225,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:19,09/Jun/14 11:19,14/Jul/23 05:57,09/Jun/14 11:19,,,,pre-apache,,,,,,,0,github-import,,"Please add those commits to ozone, so that we can directly link to 0.2-ozone in Sopremo.
Most of these patches either backport small parts of 0.2.1 (to Nephele) or fix some settings.
Message me for further informations.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/57
Created by: [AHeise|https://github.com/AHeise]
Labels: 
Created at: Mon Jul 22 15:33:15 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;pull-request-57-8238466963797610730.patch;https://issues.apache.org/jira/secure/attachment/12648964/pull-request-57-8238466963797610730.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397424,,,2014-06-09 11:19:04.0,,,,,,,,,,"0|i1wf7z:",397551,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix iteration head merging logic,FLINK-56,12719224,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:19,09/Jun/14 11:19,14/Jul/23 05:57,09/Jun/14 11:19,,,,pre-apache,,,,,,,0,github-import,,"The iteration head task is chained/merged with the first step function operator, if possible, to reduce the number of channels and threads. This is however, only possible in certain (yet common) situations.

Currently, the head is erroneously merged with the operator, even the initial partial solution (or workset) and the later partial solution (or workset) have different local strategies at that operator. Add a condition in the job graph generation to prevent this situation.

Note: This fixes the CoGroupConnectedComponents test.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/56
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Thu Jul 18 08:14:37 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397423,,,Mon Jun 09 11:19:01 UTC 2014,,,,,,,,,,"0|i1wf7r:",397550,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:19;github-import;[Date: Thu Jul 18 08:15:03 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [b1528725f0d5fe2417a90ae1ee550dac8572edb7|https://github.com/stratosphere/stratosphere/commit/b1528725f0d5fe2417a90ae1ee550dac8572edb7]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ArrayIndexOutOfBoundsException when reading test input > 32KB,FLINK-55,12719223,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:18,09/Jun/14 11:18,14/Jul/23 05:57,09/Jun/14 11:18,,,,pre-apache,,,,,,,0,github-import,,"This fixes a bug, where tests in `pact-tests` using `Tuple` or `IntTupleDataInFormat` would throw `ArrayIndexOutOfBoundsException` on input > 32 KB.

I used TPCH-9 query from `pact-tests` to reliably reproduce the multiplexing deadlock (see issue ([#20|https://github.com/stratosphere/stratosphere/issues/20] | [FLINK-20|https://issues.apache.org/jira/browse/FLINK-20])) even on small scale (because of the relatively long processing pipeline).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/55
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Sun Jul 14 14:34:19 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:18;github-import;pull-request-55-1509347497560056730.patch;https://issues.apache.org/jira/secure/attachment/12648963/pull-request-55-1509347497560056730.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397422,,,Mon Jun 09 11:18:58 UTC 2014,,,,,,,,,,"0|i1wf7j:",397549,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:18;github-import;[Date: Thu Jul 18 09:20:39 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Rebased and merged in [05401481550685bcaa3ba22c63590988dde7eaf1|https://github.com/stratosphere/stratosphere/commit/05401481550685bcaa3ba22c63590988dde7eaf1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Multiplexing Deadlock,FLINK-54,12719222,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:18,09/Jun/14 11:18,14/Jul/23 05:57,09/Jun/14 11:18,,,,pre-apache,,,,,,,0,github-import,,"This fixes ([#20|https://github.com/stratosphere/stratosphere/issues/20] | [FLINK-20|https://issues.apache.org/jira/browse/FLINK-20]) by assigning a unique connectionID (per stage) to every ExecutionEdge.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/54
Created by: [uce|https://github.com/uce]
Labels: 
Created at: Sun Jul 14 14:23:15 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:18;github-import;pull-request-54-1449396950029554595.patch;https://issues.apache.org/jira/secure/attachment/12648962/pull-request-54-1449396950029554595.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397421,,,Mon Jun 09 11:18:53 UTC 2014,,,,,,,,,,"0|i1wf7b:",397548,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:18;github-import;[Date: Mon Jul 15 15:15:57 CEST 2013, Author: [uce|https://github.com/uce]]

The failing ~~ConnectedComponentsNepheleITCase~~ `CoGroupConnectedComponentsITCase` passes only randomly for me locally already *before* this fix. I just ran it 10 times and only 1 run passed. The failing runs throw the ""Vertex in wrong component"" `AssertionError`. Can anybody confirm this?

I just noticed it after the travis build failed. Before issuing the pull request, everything passed for me (i.e. it was the 1 out of 10 runs that passes?). Also ([#55|https://github.com/stratosphere/stratosphere/issues/55] | [FLINK-55|https://issues.apache.org/jira/browse/FLINK-55]) doesn't change anything used in the test, but fails.;;;","09/Jun/14 11:18;github-import;[Date: Mon Jul 15 23:17:19 CEST 2013, Author: [uce|https://github.com/uce]]

Stephan just pointed out that I referenced the wrong test... sorry. I meant that `CoGroupConnectedComponentsITCase` works randomly. And this seems to be known problem. So everything's fine.;;;","09/Jun/14 11:18;github-import;[Date: Fri Jul 19 05:58:11 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [4542555ea4cd4e1d7253eb7ce96e140fd0375e83|https://github.com/stratosphere/stratosphere/commit/4542555ea4cd4e1d7253eb7ce96e140fd0375e83];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unification of Nephele Memory with PACT & Removal of Checkpointing and Compression,FLINK-53,12719219,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:17,09/Jun/14 11:17,14/Jul/23 05:57,09/Jun/14 11:17,,,,pre-apache,,,,,,,0,github-import,,"This is issue https://github.com/dimalabs/ozone/issues/24

This pull request reworks the Buffer/FileBuffer/MemoryBuffer classes of Nephele. 
The FileBuffer has been removed as it is not really used. Only the checkpointing feature of nephele was using it. This feature has been completely removed with this pull request.

The MemoryBuffer now wraps a MemorySegment instead of a ByteBuffer in order to unify the different memory models.
Please note that the ByteBuffers were allocated using .allocateDirect(), that is the memory is from outside the heap space. I had to increase the heap space for the integration tests (maven failsafe) because the MemorySegment allocates memory on the heap (using byte-arrays)

I annotated the CoGroupConnectedComponentsITCase test with @Ignore because it causes the integration tests to fail.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/53
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Sun Jul 14 12:09:52 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:17;github-import;pull-request-53-8504277151169512184.patch;https://issues.apache.org/jira/secure/attachment/12648961/pull-request-53-8504277151169512184.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397418,,,Mon Jun 09 11:17:32 UTC 2014,,,,,,,,,,"0|i1wf6n:",397545,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:17;github-import;[Date: Wed Aug 07 15:40:38 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

Okay, I addressed all comments and pushed again. Please note that the pull request contains a commit from Ufuk.

The commit that removes the compression feature from nephele is also contained in this pull request.
If you want, I can open a separate pull request (its a 5k lines change). 
I'm not sure anymore, if it is a good idea to remove the compression feature this way. It seems that it was never used and it does probably not work. But the code that I removed probably contains all the infrastructure to control compression.

There is one issue left: the testConvertJobGraphToExecutionGraph4() of ExecutionGraphTest.
The test case has causes an error (not a failure. JUnit continues testing after erroneous tests).
@StephanEwen: if you are able to fix the issue in a few minutes, I would be glad if you could fix it. If it is not trivial for you, let me know, than I'll invest more time.;;;","09/Jun/14 11:17;github-import;[Date: Fri Aug 23 08:40:01 CEST 2013, Author: [dimalabs|https://github.com/dimalabs]]

Manually merged and committed in [b394c0b5e44e6c90df140eb18b9b37a2413190e5|https://github.com/stratosphere/stratosphere/commit/b394c0b5e44e6c90df140eb18b9b37a2413190e5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Travis-CI configuration,FLINK-52,12719218,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:17,09/Jun/14 11:17,14/Jul/23 05:57,09/Jun/14 11:17,,,,pre-apache,,,,,,,0,github-import,,"This pull request contains the necessary configuration file for travis-ci to work.

Travis will build on each new commit and pull request with three different JVMs. The build result is mailed to the commit author and the repository owner (Sebastian).
The email notification settings are controllable via the travis.yml file.

A full build with integration tests currently needs around 30 minutes. 
The web-viewer for the build results does only show the first 10k lines, but you can click the little wheel at the upper right corner which allows to download the full log (for example: https://s3.amazonaws.com/archive.travis-ci.org/jobs/9013766/log.txt)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/52
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Fri Jul 12 20:53:19 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:17;github-import;pull-request-52-3630730630001565431.patch;https://issues.apache.org/jira/secure/attachment/12648960/pull-request-52-3630730630001565431.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397417,,,2014-06-09 11:17:13.0,,,,,,,,,,"0|i1wf6f:",397544,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
re-openable hashjoin integration test,FLINK-51,12719217,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:17,09/Jun/14 11:17,14/Jul/23 05:57,09/Jun/14 11:17,,,,pre-apache,,,,,,,0,github-import,,"This is the fix for issue: https://github.com/dimalabs/ozone/issues/32

I used some of the tests of HashTableITCase as a starting point to test the new re-openable hash join. The current implementation probes the input five times.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/51
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Fri Jul 12 18:11:37 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:17;github-import;pull-request-51-976229466834817386.patch;https://issues.apache.org/jira/secure/attachment/12648959/pull-request-51-976229466834817386.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397416,,,Mon Jun 09 11:17:11 UTC 2014,,,,,,,,,,"0|i1wf67:",397543,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:17;github-import;[Date: Fri Jul 19 06:35:09 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [42cdca4006f6f521f0e497732f7fda0295b01d7d|https://github.com/stratosphere/stratosphere/commit/42cdca4006f6f521f0e497732f7fda0295b01d7d].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase port from Paris Hackathon,FLINK-50,12719216,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:17,14/Mar/19 10:42,14/Jul/23 05:57,09/Jun/14 11:17,,,,pre-apache,,,Connectors / HBase,,,,0,github-import,,"So this is the initial version of HBase for ozone.
Sample code how to read from HBase is provided in pact-examples

I merged this with dimalabs:master.
The relevant commits are 33d6a1a and 39664be
The all previous commits are from the cloudera support branch, everything after those two is the result of merging.

Get back to me if this is too messy, I could also just copy/paste the HBase stuff into a fresh branch.
The current diff to dimalabs:master looks find to me, though.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/50
Created by: [mleich|https://github.com/mleich]
Labels: 
Created at: Thu Jul 11 12:57:31 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:17;github-import;pull-request-50-7352113113077501853.patch;https://issues.apache.org/jira/secure/attachment/12648958/pull-request-50-7352113113077501853.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397415,,,Mon Jun 09 11:17:06 UTC 2014,,,,,,,,,,"0|i1wf5z:",397542,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:17;github-import;[Date: Thu Aug 22 09:01:36 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually merged and fixed in [8f7484000ae40ae1b5c68e2d21b9e5a44d984234|https://github.com/stratosphere/stratosphere/commit/8f7484000ae40ae1b5c68e2d21b9e5a44d984234];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for Cloudera Hadoop,FLINK-49,12719215,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:16,09/Jun/14 11:16,14/Jul/23 05:57,09/Jun/14 11:16,,,,pre-apache,,,,,,,0,github-import,,"# Cloudera Hadoop Support for Ozone
These changes allow to use and easily switch between different Hadoop versions.

## How to use:
- adjust the hadoop version in the main pom (samples for Hadoop 0.20.203.0 and 2.0.0-cdh4.2.1 are provided)
- adjust all poms that require Hadoop to use the correct jar names (hadoop-core changed to hadoop-common, hadoop-hdfs may need to be added)

## Side effects:
Different Hadoop versions use different jar names and have different dependencies that need to be added to the classpath.
Previously all jar names were explicitly listed in the application startup scripts. While this allows for tailor-made classpaths it is also a major maintenance overhead.
The behavior was changed so the startup scripts now add all jars from the <ozone_root>/lib folder to the classpath.
Jars required by pact-clients have been moved to lib_clients, to avoid unnecessary overhead for worker tasks.
In the end this means far less classpath management overhead while retaining a reasonably clean classpath.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/49
Created by: [mleich|https://github.com/mleich]
Labels: 
Created at: Fri Jul 05 17:18:53 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:16;github-import;pull-request-49-4391911577980376371.patch;https://issues.apache.org/jira/secure/attachment/12648957/pull-request-49-4391911577980376371.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397414,,,Mon Jun 09 11:16:58 UTC 2014,,,,,,,,,,"0|i1wf5r:",397541,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:16;github-import;[Date: Fri Jul 05 17:59:50 CEST 2013, Author: [mleich|https://github.com/mleich]]

OK, right now eu.stratosphere.pact.test.contracts.io.CustomDataTypeTest seems to run infinitely.
I'll take a look at that, not sure why this happens.;;;","09/Jun/14 11:16;github-import;[Date: Sun Jul 07 22:56:06 CEST 2013, Author: [mleich|https://github.com/mleich]]

Don't know why this wasn't triggered before, but changing the Hadoop version also changed the log level to debug during build.
Should work just fine now.;;;","09/Jun/14 11:16;github-import;[Date: Thu Jul 11 06:56:08 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Rebased and merged in [b407f8b37a4ded820a25cc1cc03da064988c2f0d|https://github.com/stratosphere/stratosphere/commit/b407f8b37a4ded820a25cc1cc03da064988c2f0d].

Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Class loading for user-defined aggregates failes,FLINK-48,12719214,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:16,09/Jun/14 11:16,14/Jul/23 05:57,09/Jun/14 11:16,,,,pre-apache,,,,,,,0,github-import,,"Class loading of user-defined data types must always happen with the user-code class loader. The instantiation of the user-defined aggregate types happens in the read() method of the iteration event that transmits them.

Since the event has no access to that particular class loader, it can only receive the serialized data. The actual instantiation of these types must happen lazily when they are accessed by the iteration head or sync tasks, which have the required class loader.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/48
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: bug, 
Created at: Wed Jul 03 04:10:45 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397413,,,Mon Jun 09 11:16:52 UTC 2014,,,,,,,,,,"0|i1wf5j:",397540,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:16;github-import;[Date: Wed Jul 03 04:10:56 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Implemented in [e8fd5846901aec6b94d5855154bc01061dc1d862|https://github.com/stratosphere/stratosphere/commit/e8fd5846901aec6b94d5855154bc01061dc1d862]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PactRecordOutputCollector does not support strategy PARTITION_RANDOM,FLINK-47,12719213,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:16,09/Jun/14 11:16,14/Jul/23 05:57,09/Jun/14 11:16,,,,pre-apache,,,,,,,0,github-import,,"The strategy is actually supported, but there is an early sanity check that overlooks the case of that strategy and throws  an exception.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/47
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: bug, 
Created at: Wed Jul 03 04:07:35 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397412,,,Mon Jun 09 11:16:48 UTC 2014,,,,,,,,,,"0|i1wf5b:",397539,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:16;github-import;[Date: Wed Jul 03 04:07:51 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [76316542959bf927fb583f629a9230c2ed34c211|https://github.com/stratosphere/stratosphere/commit/76316542959bf927fb583f629a9230c2ed34c211]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable Nephele Checkpointing / Recovery by default.,FLINK-44,12719210,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 11:15,09/Jun/14 11:15,14/Jul/23 05:57,09/Jun/14 11:15,,,,pre-apache,,,,,,,0,github-import,,"Nephele Checkpointing / Recovery is highly buggy and almost always deadlocks the system.

Disable it by setting the default number of retries to in the default configuration values.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/44
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: bug, 
Created at: Wed Jul 03 03:52:55 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397409,,,Mon Jun 09 11:15:32 UTC 2014,,,,,,,,,,"0|i1wf4n:",397536,,,,,,,,,,,,,,,,,,,,"09/Jun/14 11:15;github-import;[Date: Wed Jul 03 03:53:07 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [75502d8e1266a9f7f60339a10ccb7a42b29dc197|https://github.com/stratosphere/stratosphere/commit/75502d8e1266a9f7f60339a10ccb7a42b29dc197];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Make stratosphere work on Windows/cygwin,FLINK-42,12719200,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:15,09/Jun/14 10:15,14/Jul/23 05:57,09/Jun/14 10:15,,,,pre-apache,,,,,,,0,github-import,,"Also fix WebInterfaceServer.java to properly check whether path
is absolute on Windows.

Also, default directory entries in nephele-user.xml and pact-user.xml
are now commented out because java code will load proper default.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/42
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Mon Jul 01 10:55:27 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:15;github-import;pull-request-42-3247080641255452906.patch;https://issues.apache.org/jira/secure/attachment/12648951/pull-request-42-3247080641255452906.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397399,,,Mon Jun 09 10:15:53 UTC 2014,,,,,,,,,,"0|i1wf2f:",397526,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:15;github-import;[GitHub Import] [Date: Thu Jul 18 09:14:31 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you please update the code to the latest HEAD? There are two issues:

1) The pull request to split the libs directory into lib (for core libs) and lib_client requires to mangle the client part of the classpath as well in the relevant scripts (webfrontent, pact-client).

2) Not all windows setups are cygwin (I know that Daniel has been running stratosphere on bare windows, I think with gnuwin utils or so). Please check for a cygwin env variable, or the presence of the cygpath command when mangeling paths.;;;","09/Jun/14 10:15;github-import;[GitHub Import] [Date: Mon Jul 22 11:25:43 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

I rebased to the latest HEAD and also fixed the Plain Windows vs. Cygwin thing.

Where is the pull request to split the libs directory? Or was this already merged?;;;","09/Jun/14 10:15;github-import;[GitHub Import] [Date: Mon Jul 22 17:07:39 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

That one is already merged. You should find the references to the client directory in the client shell script files.;;;","09/Jun/14 10:15;github-import;[GitHub Import] [Date: Mon Jul 22 17:54:58 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

Ok, I fixed the issue with lib_client as well. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Updated Maven artifacts version to 0.2-ozone.,FLINK-41,12719199,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:15,09/Jun/14 10:15,14/Jul/23 05:57,09/Jun/14 10:15,,,,pre-apache,,,,,,,0,github-import,,"Allows deploying Ozone to a local repository next to a vanilla Stratosphere distribution. Closes ([#27|https://github.com/stratosphere/stratosphere/issues/27] | [FLINK-27|https://issues.apache.org/jira/browse/FLINK-27]).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/41
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Created at: Sat Jun 22 05:58:39 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:15;github-import;pull-request-41-4424454321234604978.patch;https://issues.apache.org/jira/secure/attachment/12648950/pull-request-41-4424454321234604978.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397398,,,Mon Jun 09 10:15:46 UTC 2014,,,,,,,,,,"0|i1wf27:",397525,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:15;github-import;[GitHub Import] [Date: Thu Jul 18 08:35:26 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Merged in [35ff4dcd2ed84aa8711e4d3fea1caae622428457|https://github.com/stratosphere/stratosphere/commit/35ff4dcd2ed84aa8711e4d3fea1caae622428457];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] #37 Enable visualization of custom iteration statistics,FLINK-38,12719196,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:15,09/Jun/14 10:15,14/Jul/23 05:57,09/Jun/14 10:15,,,,pre-apache,,,,,,,0,github-import,,"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/38
Created by: [sscdotopen|https://github.com/sscdotopen]
Labels: 
Created at: Mon Jun 17 11:13:33 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:15;github-import;pull-request-38-5622722093277050738.patch;https://issues.apache.org/jira/secure/attachment/12648949/pull-request-38-5622722093277050738.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397395,,,Mon Jun 09 10:15:29 UTC 2014,,,,,,,,,,"0|i1wf1j:",397522,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:15;github-import;[GitHub Import] [Date: Thu Feb 06 20:48:13 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

This is unfortunately outdated code for the SWT visualization that was used during the SIGMOD demo. It needs quite a bit of reworking and merging. Since we are pushing for the browser-based visualization at the moment, this code will become obsolete anyways in a bit.;;;","09/Jun/14 10:15;github-import;[GitHub Import] [Date: Tue Mar 25 22:43:10 CET 2014, Author: [StephanEwen|https://github.com/StephanEwen]]

The SWT visualization was removed to to licence issues: LGPL conflicts with the ASL 2. So we need to come up with a new way of doing this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] pact-compiler depends on pact-examples,FLINK-36,12719194,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:15,09/Jun/14 10:15,14/Jul/23 05:57,09/Jun/14 10:15,,,,pre-apache,,,,,,,0,github-import,,"In a layered architecture, you must only depend on layers below. Running into problems when modifying the examples code because of this wrong dependency.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/36
Created by: [sscdotopen|https://github.com/sscdotopen]
Labels: bug, 
Created at: Fri Jun 14 11:30:34 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397393,,,Mon Jun 09 10:15:17 UTC 2014,,,,,,,,,,"0|i1wf13:",397520,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:15;github-import;[GitHub Import] [Date: Sat Jun 22 06:19:48 CEST 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

I'm not sure whether this is a real issue. The dependency is defined only for the `test` scope and is caused by the use of some `eu.stratosphere.pact.example.*` classes located in `pact-examples` from the `eu.stratosphere.pact.compiler` test classes. 

If we want to have a coherent set of regression jobs to be used across different modules (e.g. for `pact-compiler` and `pact4s`), it makes sense to keep them in a separate package.

Alternatively, we can also make a hard copy of the `eu.stratosphere.pact.example` code under `eu.stratosphere.pact.compiler.examples` and resolve the dependency via code duplication.;;;","09/Jun/14 10:15;github-import;[GitHub Import] [Date: Wed Jul 03 03:47:16 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

I split the compiler tests into unit-style tests (isolated compiler behavior) and regression tests (full programs, including checks that reasonable plans are chosen). The later are in an individual project now (pact-compiler-tests) and depend on the examples package.

The pact compiler project itself has no more dependency on the pact-examples.

Fixed in [cf90104cdad1760b594c3ffd7298ab49253d5e2e|https://github.com/stratosphere/stratosphere/commit/cf90104cdad1760b594c3ffd7298ab49253d5e2e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Added salting for each key component to PactRecordComparator#hash(). ,FLINK-35,12719193,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:15,09/Jun/14 10:15,14/Jul/23 05:57,09/Jun/14 10:15,,,,pre-apache,,,,,,,0,github-import,,"This makes the hashing algorithm more robust against correlated or clustered key value groups.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/35
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Wed Jun 12 19:34:12 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:15;github-import;pull-request-35-6317705530911108750.patch;https://issues.apache.org/jira/secure/attachment/12648948/pull-request-35-6317705530911108750.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397392,,,Mon Jun 09 10:15:13 UTC 2014,,,,,,,,,,"0|i1wf0v:",397519,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:15;github-import;[GitHub Import] [Date: Wed Jul 03 03:48:20 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Manually pulled and merged.

Fixed in [6c39b50113dacc87b1750e63c68f8e85b53075a5|https://github.com/stratosphere/stratosphere/commit/6c39b50113dacc87b1750e63c68f8e85b53075a5].

Repaired failing test (join of heterogeneous data types) in [d7205928283fbe2ec7c251781423e005d9216cf3|https://github.com/stratosphere/stratosphere/commit/d7205928283fbe2ec7c251781423e005d9216cf3];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Fix hashing algorithm in the PactRecordComparator,FLINK-34,12719192,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:15,09/Jun/14 10:15,14/Jul/23 05:57,09/Jun/14 10:15,,,,pre-apache,,,,,,,0,github-import,,"The hash() method in the PactRecordComparator does not work well on composite integer keys with correlated values. A workaround that solved the problem for my use-case is to use separate salt prime multipliers for the hashes of each key component.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/34
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Created at: Wed Jun 12 18:11:59 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:15;github-import;pull-request-34-7611567884126172116.patch;https://issues.apache.org/jira/secure/attachment/12648947/pull-request-34-7611567884126172116.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397391,,,2014-06-09 10:15:06.0,,,,,,,,,,"0|i1wf0n:",397518,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Rework instance configuration.,FLINK-33,12719191,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:15,22/Jun/14 21:51,14/Jul/23 05:57,22/Jun/14 21:51,,,,0.6-incubating,,,,,,,0,github-import,,"Right now, Nephele still uses the EC2-inspired instance configuration model. The Pact compiler connects to obtain information about these instances, such as how many are available, and how much memory they have. This is error prone to configure and also a bit buggy, it frequently leads to wrong memory bookkeeping if different instance types are configured.

Do we need support for heterogeneous setups where different nodes have different capabilities and should be assigned a different amount of work? If we defer this to later, we can greatly simplify the logic and configuration:

1) No configuration for the instance type. The internal instance manager has a default profile which is okay for all cluster instances.

2) An explicit value of how many slots for parallel operators we have on each node (such as 8 on an eight core machine). There should be a default value in the config which could be overridden via query-specific parameters.

3) An explicit config entry that defines how much memory should be used for networking and how much for query processing. The query processing memory amount is used to initialize the MemoryManager and is also used by the pact-compiler to parameterize the memory available to the operators. That way we can also get rid of the communication between the compiler and the job manager on plan compilation. Eventually it would be good to run the compiler as a child process of the job-manager anyways.

In the long run we want to make query processing memory and network memory one value (overall system memory, the rest is the UDF Java heap memory) which is shared for materialization in the network stack and the runtime operators.


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/33
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Wed Jun 12 02:58:27 CEST 2013
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397390,,,Sun Jun 22 21:51:20 UTC 2014,,,,,,,,,,"0|i1wf0f:",397517,,,,,,,,,,,,,,,,,,,,"22/Jun/14 21:51;sewen;This is fixed in b4b633eab9a70e14d2e0dd5252f4b092a3689093;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Add integration tests for reopen-able hash join,FLINK-32,12719190,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:14,09/Jun/14 10:14,14/Jul/23 05:57,09/Jun/14 10:14,,,,pre-apache,,,,,,,0,github-import,,"The iteration aware hash join needs thorough integration tests like the regular hash join.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/32
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Tue Jun 11 04:54:37 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397389,,,Mon Jun 09 10:14:47 UTC 2014,,,,,,,,,,"0|i1wf07:",397516,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:14;github-import;[GitHub Import] [Date: Wed Jun 12 09:49:08 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I need some additional input for this task. I think i covered most of the situations in these tests already:
pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/hash/ReOpenableHashTableTest.java

I can adopt some additional cases such as ""testBuildFirstWithMixedDataTypes()"" or ""testBuildFirstWithHighNumberOfCommonKeys()"" 
Or do you want a test in the ""pact-tests"" project that does a whole iterative task with the reopenable hash join?

;;;","09/Jun/14 10:14;github-import;[GitHub Import] [Date: Wed Jun 12 17:31:17 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Please adopt some of the test from the HashTableITCase, especially the ones
that heavily spill and spill recursively. Make them do it trhee rounds.

Best,
Stephan



On Wed, Jun 12, 2013 at 2:49 AM, rmetzger <notifications@github.com> wrote:

> I need some additional input for this task. I think i covered most of the
> situations in these tests already:
>
> pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/hash/ReOpenableHashTableTest.java
>
> I can adopt some additional cases such as
> ""testBuildFirstWithMixedDataTypes()"" or
> ""testBuildFirstWithHighNumberOfCommonKeys()""
> Or do you want a test in the ""pact-tests"" project that does a whole
> iterative task with the reopenable hash join?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/dimalabs/ozone/issues/32#issuecomment-19311105>
> .
>;;;","09/Jun/14 10:14;github-import;[GitHub Import] [Date: Fri Jul 19 06:35:54 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Integrated in [42cdca4006f6f521f0e497732f7fda0295b01d7d|https://github.com/stratosphere/stratosphere/commit/42cdca4006f6f521f0e497732f7fda0295b01d7d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Remove Checkpointing and FileBuffer from Nephele,FLINK-31,12719189,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:13,09/Jun/14 10:13,14/Jul/23 05:57,09/Jun/14 10:13,,,,pre-apache,,,,,,,0,github-import,,"This is part of Issue 24. But since this step involves many changes, I wanted to separate them.

I validated the correctness using ""mvn verify"".


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/31
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Mon Jun 10 15:35:30 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:13;github-import;pull-request-31-7474449754424370061.patch;https://issues.apache.org/jira/secure/attachment/12648946/pull-request-31-7474449754424370061.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397388,,,2014-06-09 10:13:03.0,,,,,,,,,,"0|i1wezz:",397515,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Job compilation fails due to missing commons packages,FLINK-30,12719188,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:12,09/Jun/14 10:12,14/Jul/23 05:57,09/Jun/14 10:12,,,,pre-apache,,,,,,,0,github-import,,"The *commons-lang* and *commons-configuration* are missing from the classpath. 

In particular, *commons-configuration* is required by *hadoop-core* and thereby by *pact-runtime*, while *commons-lang* is required by *commons-configuration*.

In order to solve the issue the appropriate lines for including *commons-configuration* and *commons-lang* to the classpath should be added to *pact-client.sh*, *nephele-jobmanager.sh* and *nephele-taskmanager.sh* (not sure about the last two).

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/30
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: bug, 
Created at: Mon Jun 10 15:08:49 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397387,,,Mon Jun 09 10:12:31 UTC 2014,,,,,,,,,,"0|i1wezr:",397514,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:12;github-import;[GitHub Import] [Date: Tue Jun 11 05:59:54 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [06ef78044b5a571b162b9cc4b65168adc1ec2573|https://github.com/stratosphere/stratosphere/commit/06ef78044b5a571b162b9cc4b65168adc1ec2573];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Update pom.xml,FLINK-29,12719187,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:11,09/Jun/14 10:11,14/Jul/23 05:57,09/Jun/14 10:11,,,,pre-apache,,,,,,,0,github-import,,"The configured dependency version for pact-array-datamodel should be the same as the one of the enclosing project.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/29
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Created at: Sat Jun 08 16:51:48 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:11;github-import;pull-request-29-4678802796593219677.patch;https://issues.apache.org/jira/secure/attachment/12648945/pull-request-29-4678802796593219677.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397386,,,2014-06-09 10:11:32.0,,,,,,,,,,"0|i1wezj:",397513,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Make Pact Contract constructors public,FLINK-28,12719186,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:11,09/Jun/14 10:11,14/Jul/23 05:57,09/Jun/14 10:11,,,,pre-apache,,,,,,,0,github-import,,"Make them public so that they can be invoked with a builder and a mixin trait from scala code.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/28
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Mon Jun 03 13:36:43 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:11;github-import;pull-request-28-7293249608152261750.patch;https://issues.apache.org/jira/secure/attachment/12648944/pull-request-28-7293249608152261750.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397385,,,Mon Jun 09 10:11:23 UTC 2014,,,,,,,,,,"0|i1wezb:",397512,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:11;github-import;[GitHub Import] [Date: Mon Jun 03 15:57:50 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you verify that ""protected"" is not enough? Since mixing in a trait causes the Scala compiler to generate a sub-class, I thought protected suffices.

;;;","09/Jun/14 10:11;github-import;[GitHub Import] [Date: Mon Jun 03 16:49:54 CEST 2013, Author: [aljoscha|https://github.com/aljoscha]]

You're right, I'm sorry. At some point I did something where I did not have the mixin and that wouldn't work without the ctor being public. But now I have mixins again and it works again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Change Maven Artifact Names,FLINK-27,12719185,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:10,09/Jun/14 10:10,14/Jul/23 05:57,09/Jun/14 10:10,,,,pre-apache,,,,,,,0,github-import,,"Currently, ozone uses the same artifact names as Stratosphere 0.2. That means it is not possible to work with both in parallel (as in code migration, cross testing, ...)

Ozone should have new artifact names and a new Version. I propose Version 0.5 (some leap from 0.2)

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/27
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: bug, 
Created at: Tue May 28 10:58:13 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397384,,,Mon Jun 09 10:10:34 UTC 2014,,,,,,,,,,"0|i1wez3:",397511,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:10;github-import;[GitHub Import] [Date: Mon Jun 10 14:38:59 CEST 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

I use 0.2-ozone as version name in my POMs, but a quick grep replace should be able to change it to whatever you want. 

I am not quite sure why we need different artifact names. Changing the version did the trick for the local m2 repository, and in eclipse you can set a custom name template (.e.g. ```[artifactId]-[version]```) for imported Maven projects under the advanced options.;;;","09/Jun/14 10:10;github-import;[GitHub Import] [Date: Sat Jun 22 00:45:28 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

At the moment, several people maintain installations of the stratosphere mainline version (stratosphere-0.2) and the ozone version. Since both will release newer versions, the best way to avoid name clashes in the repositories is to go with different artifact names.

However, since we may abandon the name ""ozone"" and continue distributing it under the label ""stratosphere"", we could leave the artifact names and only change the version as suggested.;;;","09/Jun/14 10:10;github-import;[GitHub Import] [Date: Sat Jun 22 04:18:16 CEST 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

Should I prepare suggest a push request for that?;;;","09/Jun/14 10:10;github-import;[GitHub Import] [Date: Wed Jul 03 05:49:17 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Can you elaborate on the Eclipse template you mentioned? Is that a thing you need to do to make the setup work?;;;","09/Jun/14 10:10;github-import;[GitHub Import] [Date: Wed Jul 03 11:49:54 CEST 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

No, it will work with the standard naming convention for the eclipse projects out of the box. Bit if you already have the ""default"" stratosphere packages imported in eclipse using the standard package names (pact-compiler, pact-runtime, etc.), you have to modify the names of the Ozone projects as you import them. 

One of the options that the maven importer gives you is to use {packageID}-{version} as naming template for the eclipse projects. I used that and now in the workspace I have the old packages (e.g. pact-compiler) next to the new ones (e.g. pact-compiler-0.2-ozone).;;;","09/Jun/14 10:10;github-import;[GitHub Import] [Date: Thu Jul 18 08:36:26 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed though Alexander's pull request in [35ff4dcd2ed84aa8711e4d3fea1caae622428457|https://github.com/stratosphere/stratosphere/commit/35ff4dcd2ed84aa8711e4d3fea1caae622428457];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Fix for #10: Hybrid Hash Join for iterations,FLINK-26,12719183,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:09,09/Jun/14 10:09,14/Jul/23 05:57,09/Jun/14 10:09,,,,pre-apache,,,,,,,0,github-import,,"With this change, ozone's hash join implementation is able to be reopened multiple times with different probe side inputs.

In-memory build-site contents are spilled to disk once the build phase has finished. Spilled build partitions are kept until hybrid hash join's final close() call, so that subsequent probe phases can re-use the build side.
Upon a reopen() call, hybrid hash join will reinitialize itself to the same state as after a regular open() call, but without re-pulling the build side iterator.

If the build side fits completely in-memory, no spilling will happen.

I'm very happy about feedback and comments on my code!

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/26
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Created at: Fri May 24 18:24:07 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:09;github-import;pull-request-26-5211828643781853428.patch;https://issues.apache.org/jira/secure/attachment/12648943/pull-request-26-5211828643781853428.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397382,,,Mon Jun 09 10:09:35 UTC 2014,,,,,,,,,,"0|i1weyn:",397509,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:09;github-import;[GitHub Import] [Date: Wed May 29 13:38:27 CEST 2013, Author: [rmetzger|https://github.com/rmetzger]]

I addressed all issues from the comments here.
""mvn verify"" run through without any errors.

Please have a look at the CONTRIBUTORS file and update your infos.;;;","09/Jun/14 10:09;github-import;[GitHub Import] [Date: Tue Jun 11 06:01:06 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

rebased and merged in [e62e40230e7569420ca654106bdbe7d39e6920b1|https://github.com/stratosphere/stratosphere/commit/e62e40230e7569420ca654106bdbe7d39e6920b1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Fix Multiplexing Deadlock,FLINK-20,12719176,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 10:04,09/Jun/14 10:04,14/Jul/23 05:57,09/Jun/14 10:04,,,,pre-apache,,,,,,,0,github-import,,"Change assignment from logical channels to physical connections to prevent starvation effects that cause distributed deadlocks in the network stack.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/20
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: bug, 
Created at: Tue May 21 21:09:23 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397375,,,Mon Jun 09 10:04:50 UTC 2014,,,,,,,,,,"0|i1wex3:",397502,,,,,,,,,,,,,,,,,,,,"09/Jun/14 10:04;github-import;[GitHub Import] [Date: Fri Jul 19 05:58:36 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

Fixed in [4542555ea4cd4e1d7253eb7ce96e140fd0375e83|https://github.com/stratosphere/stratosphere/commit/4542555ea4cd4e1d7253eb7ce96e140fd0375e83];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Fixed #16 Hiding extra Iteration Elements in Pact Plan Visualization,FLINK-16,12719163,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 09:22,09/Jun/14 09:23,14/Jul/23 05:57,09/Jun/14 09:23,,,,pre-apache,,,,,,,0,github-import,,"Hided unwanted elements in visualization of iteration pact plans

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/16
Created by: [mhuelfen|https://github.com/mhuelfen]
Labels: 
Created at: Mon May 13 15:48:31 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/14 09:23;github-import;pull-request-16461787373773121587patch;https://issues.apache.org/jira/secure/attachment/12648935/pull-request-16461787373773121587patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397362,,,2014-06-09 09:22:45.0,,,,,,,,,,"0|i1weu7:",397489,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Rework Nephele Execution Graph State Machine,FLINK-15,12719162,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,github-import,github-import,09/Jun/14 09:22,21/Sep/14 02:22,14/Jul/23 05:57,21/Sep/14 02:22,,,,0.7.0-incubating,,,,,,,0,github-import,,"The Nephele ExecutionGraph state machine is currently not scalable and is susceptible to race conditions and illegal state transition.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/15
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: bug, runtime, 
Created at: Wed May 08 15:37:25 CEST 2013
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397361,,,Sun Sep 21 02:22:19 UTC 2014,,,,,,,,,,"0|i1wetz:",397488,,,,,,,,,,,,,,,,,,,,"21/Sep/14 02:22;sewen;Fixed in ae139f5ae2199a52e8d7f561f94db51631107d00;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Bug in Nephele Scheduler,FLINK-13,12719159,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,github-import,github-import,09/Jun/14 09:20,21/Sep/14 02:22,14/Jul/23 05:57,21/Sep/14 02:22,,,,0.7.0-incubating,,,,,,,0,github-import,,"For cyclic data flows, the Nephele scheduler may deadlock, causing the job to freeze. This problem is reproducible locally using the PageRankITCase.

A workaround for now is to always use network channels. However, this doe increase scheduling latency, as network-connected tasks are scheduled lazily.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/13
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: bug, runtime, 
Created at: Tue May 07 12:50:42 CEST 2013
State: open
",,github-import,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397358,,,Sun Sep 21 02:22:48 UTC 2014,,,,,,,,,,"0|i1wetb:",397485,,,,,,,,,,,,,,,,,,,,"21/Sep/14 02:22;sewen;Fixed in e6aadfccdaf02b9df65d10ac835cab7fd26e274e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] nephele-ec2cloudmanager dependency still in  stratosphere-dist pom.xml,FLINK-11,12719157,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 09:19,09/Jun/14 09:19,14/Jul/23 05:57,09/Jun/14 09:19,,,,pre-apache,,,,,,,0,github-import,,"E2C is not longer used  eu.stratosphere:nephele-ec2cloudmanager:jar:0.2 is referenced in the pom of stratosphere-dist

Trying to build produces this failure

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.2-beta-5:single (generate-package) on project stratosphere-dist: Failed to create assembly: Failed to resolve dependencies for project: eu.stratosphere:stratosphere-dist:pom:0.2: Missing:
[ERROR] ----------
[ERROR] 1) eu.stratosphere:nephele-ec2cloudmanager:jar:0.2
[ERROR] 


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/11
Created by: [mhuelfen|https://github.com/mhuelfen]
Labels: bug, 
Assignee: [StephanEwen|https://github.com/StephanEwen]
Created at: Mon Apr 29 18:29:08 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397356,,,2014-06-09 09:19:41.0,,,,,,,,,,"0|i1wesv:",397483,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Hybrid HashJoin's spilling doesn't work in iterations,FLINK-10,12719156,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 09:19,01/Nov/21 08:39,14/Jul/23 05:57,09/Jun/14 09:19,,,,pre-apache,,,,,,,0,github-import,pull-request-available,"Hybrid HashJoin's disk spilling currently doesn't work if the join is part of an iterative data flow. This needs to be fixed.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/10
Created by: [dimalabs|https://github.com/dimalabs]
Labels: bug, 
Created at: Mon Apr 29 18:19:31 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397355,,,Mon Jun 09 09:19:11 UTC 2014,,,,,,,,,,"0|i1wesn:",397482,,,,,,,,,,,,,,,,,,,,"09/Jun/14 09:19;github-import;[GitHub Import] [Date: Tue Jun 11 06:01:22 CEST 2013, Author: [StephanEwen|https://github.com/StephanEwen]]

fixed in [e62e40230e7569420ca654106bdbe7d39e6920b1|https://github.com/stratosphere/stratosphere/commit/e62e40230e7569420ca654106bdbe7d39e6920b1]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GitHub] Fix the delimited record format double/decimal parsers.,FLINK-6,12719143,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,github-import,github-import,09/Jun/14 08:48,29/Oct/21 09:50,14/Jul/23 05:57,09/Jun/14 09:06,,,,pre-apache,,,,,,,0,github-import,pull-request-available,"The current parsers create the double value incrementally from the character string and introduce massive rounding errors in several cases.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/6
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: bug, core, 
Created at: Fri Apr 26 12:12:50 CEST 2013
State: closed
",,github-import,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,397342,,,Mon Jun 09 08:49:04 UTC 2014,,,,,,,,,,"0|i1wepr:",397469,,,,,,,,,,,,,,,,,,,,"09/Jun/14 08:49;github-import;[GitHub Import] [Date: Mon Nov 11 13:23:15 CET 2013, Author: [aalexandrov|https://github.com/aalexandrov]]

Shouldn't we close this now? Seems to be resolved by ([#238|https://github.com/stratosphere/stratosphere/issues/238] | [FLINK-238|https://issues.apache.org/jira/browse/FLINK-238]).;;;","09/Jun/14 08:49;github-import;[GitHub Import] [Date: Thu Dec 12 13:04:08 CET 2013, Author: [StephanEwen|https://github.com/StephanEwen]]
Agreed, it is fixed by #238 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
