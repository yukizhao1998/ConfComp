Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Outward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Inward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Inward issue link (Dependent),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Inward issue link (Required),Inward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
The doc of StreamTaskNetworkInput.java may has a redundant 'status',FLINK-18037,13308360,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ZhuShang,ZhuShang,ZhuShang,29/May/20 16:10,13/Apr/21 20:40,13/Jul/23 08:06,04/Jun/20 06:31,1.10.0,1.10.1,1.11.0,,,1.12.0,,,,Documentation,,,,,0,pull-request-available,,,,"The doc of class StreamTaskNetworkInput as follows 
{code:java}
* <p>Forwarding elements, watermarks, or status status elements must be protected by synchronizing
* on the given lock object. This ensures that we don't call methods on a
* {@link StreamInputProcessor} concurrently with the timer callback or other things.{code}
 
 Is one of the 'status' redundant?",,jark,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 06:31:34 UTC 2020,,,,,,,,,,"0|z0fbns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 15:43;ZhuShang;[~jark] [~lzljs3620320];;;","03/Jun/20 05:42;jark;I think so. ;;;","04/Jun/20 06:31;chesnay;master: 079b38a5c2952eeac24661774bfaad58a746c115;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chinese documentation build is broken,FLINK-18036,13308312,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wuxuyang78,aljoscha,aljoscha,29/May/20 12:22,02/Jun/20 07:21,13/Jul/23 08:07,02/Jun/20 07:21,1.11.0,,,,,1.11.0,,,,chinese-translation,Documentation,,,,0,,,,,"Log from one of the builders: https://ci.apache.org/builders/flink-docs-master/builds/1848/steps/Build%20docs/logs/stdio

The problem is that the chinese doc uses ""link"" tags that refer to documents from the english documentation. It should be as easy as adding {{.zh}} in these links.

It seems this change introduced the problem: https://github.com/apache/flink/commit/d40abbf0309f414a6acf8a090c448ba397a08d9c",,aljoscha,jark,klion26,leonard,wuxuyang78,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17967,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 03:30:43 UTC 2020,,,,,,,,,,"0|z0fbd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/20 12:56;jark;I think this one is the same reason? FLINK-17967;;;","29/May/20 15:37;aljoscha;Yes, that's the same issue.;;;","29/May/20 22:38;jark;[~wuxuyang78], do you want to fix this one together? ;;;","31/May/20 13:56;wuxuyang78;Okey, I will fix it. [~jark];;;","01/Jun/20 03:30;jark;This one is fixed by FLINK-17967. I will wait another round of CI build, if the fail message is gone, I will close this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executors#newCachedThreadPool could not work as expected,FLINK-18035,13308304,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,wangyang0918,wangyang0918,29/May/20 11:29,02/Jun/20 12:35,13/Jul/23 08:07,31/May/20 21:45,1.10.2,1.11.0,,,,1.10.2,1.11.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"In FLINK-17558, we introduce {{Executors#newCachedThreadPool}} to create dedicated thread pool for TaskManager io. However, it could not work as expected.

The root cause is about the following constructor of {{ThreadPoolExecutor}}. Only when the workQueue is full, new thread will be started then. So if we set a {{LinkedBlockingQueue}} with {{Integer.MAX_VALUE}} capacity, only one thread will be started. It never grows up.

 
{code:java}
public ThreadPoolExecutor(int corePoolSize,
                          int maximumPoolSize,
                          long keepAliveTime,
                          TimeUnit unit,
                          BlockingQueue<Runnable> workQueue,
                          ThreadFactory threadFactory)
{code}",,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17558,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 31 21:45:09 UTC 2020,,,,,,,,,,"0|z0fbbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/20 11:30;wangyang0918;cc [~chesnay] [~trohrmann] Could you please have a look this problem?;;;","31/May/20 21:45;chesnay;master: 092a9553efb70dcf19f6f93b1afa94c4d496beb3
1.11: 87afb9e08b7a3fb846eaa406a139646096450f50
1.10: d4f940f4bf247f47b0304401b064317de981a724;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the copyright year in the NOTICE file in flink-shaded repo,FLINK-18031,13308251,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hequn8128,hequn8128,hequn8128,29/May/20 07:09,01/Jun/20 02:31,13/Jul/23 08:07,01/Jun/20 02:31,shaded-10.0,,,,,shaded-12.0,,,,,,,,,0,pull-request-available,,,,The year in the root NOTICE file should be updated from `2014-2017` to `2014-2020`.,,hequn8128,liyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 02:31:12 UTC 2020,,,,,,,,,,"0|z0fazk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/20 02:31;hequn8128;Fixed in shaded-12.0 via 1be30c795cdda467cf07272627a7cf35cbf3ca7c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive UDF doesn't accept empty string literal parameter,FLINK-18030,13308249,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lirui,lirui,29/May/20 06:59,11/Jun/20 06:52,13/Jul/23 08:07,11/Jun/20 06:52,,,,,,1.11.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"Empty string literal parameters are considered to be of type {{CHAR(0)}}, which is not supported in Hive. Therefore calling functions like {{REGEXP_REPLACE('abcd', 'a', '')}} would fail with exception:
{noformat}
Caused by: java.lang.RuntimeException: Char length 0 out of allowed range [1, 255]
at org.apache.hadoop.hive.serde2.typeinfo.BaseCharUtils.validateCharParameter(BaseCharUtils.java:39)
at org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo.(CharTypeInfo.java:33)
at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.createPrimitiveTypeInfo(TypeInfoFactory.java:146)
at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo(TypeInfoFactory.java:109)
at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getCharTypeInfo(TypeInfoFactory.java:165)
at org.apache.flink.table.catalog.hive.util.HiveTypeUtil$TypeInfoLogicalTypeVisitor.visit(HiveTypeUtil.java:191)
at org.apache.flink.table.catalog.hive.util.HiveTypeUtil$TypeInfoLogicalTypeVisitor.visit(HiveTypeUtil.java:173)
at org.apache.flink.table.types.logical.CharType.accept(CharType.java:149)
at org.apache.flink.table.catalog.hive.util.HiveTypeUtil.toHiveTypeInfo(HiveTypeUtil.java:84)
at org.apache.flink.table.functions.hive.HiveSimpleUDF.getHiveResultType(HiveSimpleUDF.java:127)
... 67 more
{noformat}",,danny0405,libenchao,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 06:52:43 UTC 2020,,,,,,,,,,"0|z0faz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/20 07:03;lirui;The issue was reported in user ML: https://lists.apache.org/thread.html/r26e10719251a8d29509ce919515ef4db1e3ccbf5cb9de8bd6a0d6056%40%3Cuser-zh.flink.apache.org%3E;;;","02/Jun/20 08:15;danny0405;cc [~lzljs3620320];;;","11/Jun/20 06:52;lzljs3620320;master: c9488eb8a53a07ea9541c6a2c00e196e90274320

release-1.11: 820b662b94f8362ce829fc237fd413d033f492d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ROW value constructor cannot deal with complex expressions,FLINK-18027,13308246,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,,libenchao,libenchao,29/May/20 06:52,13/Apr/23 00:53,13/Jul/23 08:07,11/Apr/23 12:04,,,,,,,,,,Table SQL / API,,,,,4,auto-deprioritized-major,auto-deprioritized-minor,pull-request-available,,"{code:java}
create table my_source (
my_row row<a int, b int, c int>
) with (...);

create table my_sink (
my_row row<a int, b int>
) with (...);

insert into my_sink
select ROW(my_row.a, my_row.b) 
from my_source;{code}
will throw excepions:
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered ""."" at line 1, column 18.Exception in thread ""main"" org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered ""."" at line 1, column 18.Was expecting one of:    "")"" ...    "","" ...     at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:56) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:64) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:627) at com.bytedance.demo.KafkaTableSource.main(KafkaTableSource.java:76)Caused by: org.apache.calcite.sql.parser.SqlParseException: Encountered ""."" at line 1, column 18.Was expecting one of:    "")"" ...    "","" ...     at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.convertException(FlinkSqlParserImpl.java:416) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.normalizeException(FlinkSqlParserImpl.java:201) at org.apache.calcite.sql.parser.SqlParser.handleException(SqlParser.java:148) at org.apache.calcite.sql.parser.SqlParser.parseQuery(SqlParser.java:163) at org.apache.calcite.sql.parser.SqlParser.parseStmt(SqlParser.java:188) at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:54) ... 3 moreCaused by: org.apache.flink.sql.parser.impl.ParseException: Encountered ""."" at line 1, column 18.Was expecting one of:    "")"" ...    "","" ...     at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.generateParseException(FlinkSqlParserImpl.java:36161) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.jj_consume_token(FlinkSqlParserImpl.java:35975) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.ParenthesizedSimpleIdentifierList(FlinkSqlParserImpl.java:21432) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression3(FlinkSqlParserImpl.java:17164) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression2b(FlinkSqlParserImpl.java:16820) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression2(FlinkSqlParserImpl.java:16861) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression(FlinkSqlParserImpl.java:16792) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SelectExpression(FlinkSqlParserImpl.java:11091) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SelectItem(FlinkSqlParserImpl.java:10293) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SelectList(FlinkSqlParserImpl.java:10267) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlSelect(FlinkSqlParserImpl.java:6943) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.LeafQuery(FlinkSqlParserImpl.java:658) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.LeafQueryOrExpr(FlinkSqlParserImpl.java:16775) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.QueryOrExpr(FlinkSqlParserImpl.java:16238) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.OrderedQueryOrExpr(FlinkSqlParserImpl.java:532) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmt(FlinkSqlParserImpl.java:3761) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmtEof(FlinkSqlParserImpl.java:3800) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.parseSqlStmtEof(FlinkSqlParserImpl.java:248) at org.apache.calcite.sql.parser.SqlParser.parseQuery(SqlParser.java:161) ... 5 more
{code}
 ",,aitozi,Christian.Lorenz77,danny0405,dian.fu,flyaruu,fsk119,gkgkgk,godfreyhe,jark,jingzhang,leonard,libenchao,lirui,liuhb86,lsy,morezaei00,nkruber,qyw919867774,twalthr,yingz,zhouqi,,,,,,,,,,,,,,,,,,,,FLINK-20873,,,,FLINK-20873,,,,,,FLINK-20821,FLINK-20922,FLINK-22274,FLINK-11399,FLINK-22951,,,,,,,,,,FLINK-18342,FLINK-31787,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 12 11:25:40 UTC 2023,,,,,,,,,,"0|z0fayg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/20 03:51;gkgkgk;you can try:

insert into my_sink
select ROW(a, b) 
from my_source;;;;","11/Jun/20 04:03;libenchao;[~gkgkgk] Thanks for the comment.

Yes, I knew that. What I want to address here is, we don't support complex expression for row constructor for now.;;;","13/Nov/20 14:50;twalthr;Another problem is this:
{{CAST(ROW(f0 + 12, 'Hello world') AS ROW<i INT, s STRING>)}}

Whereas this works:
{{CAST(ROW(12 + f0, 'Hello world') AS ROW<i INT, s STRING>)}};;;","17/Nov/20 13:51;twalthr;I investigated this issue a bit. The problem lies clearly on the Calcite side. A simple workaround is to remove the {{ROW}} keyword. Because {{ROW(f0 + 12, 'Hello world')}} and {{(f0 + 12, 'Hello world')}} are equal expressions. And `(f0 + 12, 'Hello world')` is parsed correctly. However, a row with a single field is not supported in this workaround. So we should fix the root cause why {{ROW(f0 + 12, 'Hello world')}} is not working.

[~danny0405] you are more familiar with the Calcite code base, could we fix this bug in one of the upcoming Calcite versions?;;;","24/Nov/20 10:37;twalthr;I added some better documentation to this topic in 1.12.0: 8cf28ee58c68a688a3e5e92ac141d4de1d43b296;;;","31/Dec/20 02:44;danny0405;Thanks for the check [~twalthr], let me have a check for the expression `ROW(f0 + 12, 'Hello world')` on the Calcite side. Would give the root cause and solution soon ~;;;","07/Jan/21 02:04;danny0405;Fixed in https://issues.apache.org/jira/browse/CALCITE-4456, the bug would be fixed if we upgrade to CALCITE release 1.27.0 or higher version.;;;","07/Jan/21 02:30;jark;Thanks [~danny0405] for the work! I created FLINK-20873 to upgrade Calcite 1.27 and linked the issue. ;;;","30/Apr/21 08:39;jark;Add a reproduce case: 

{code}
Flink SQL> CREATE TABLE Orders (
    order_number BIGINT,
    price        INT,
    first_name   STRING,
    last_name    STRING,
    buyer_name AS ROW(first_name, last_name)
) WITH (
  'connector' = 'datagen'
);

Flink SQL> select ROW(order_number, ROW(first_name, last_name)) from Orders;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.sql.parser.impl.ParseException: Incorrect syntax near the keyword 'ROW' at line 1, column 26.
Was expecting one of:
    <BRACKET_QUOTED_IDENTIFIER> ...
    <QUOTED_IDENTIFIER> ...
    <BACK_QUOTED_IDENTIFIER> ...
    <HYPHENATED_IDENTIFIER> ...
    <IDENTIFIER> ...
    <UNICODE_QUOTED_IDENTIFIER> ...

	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.convertException(FlinkSqlParserImpl.java:410)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.normalizeException(FlinkSqlParserImpl.java:213)
	at org.apache.calcite.sql.parser.SqlParser.handleException(SqlParser.java:140)
	at org.apache.calcite.sql.parser.SqlParser.parseQuery(SqlParser.java:155)
	at org.apache.calcite.sql.parser.SqlParser.parseStmt(SqlParser.java:180)
	at org.apache.flink.table.planner.parse.CalciteParser.parse(CalciteParser.java:54)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:96)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$parseStatement$1(LocalExecutor.java:176)
	at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:90)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:176)
	at org.apache.flink.table.client.cli.CliClient.parseCommand(CliClient.java:385)
	at org.apache.flink.table.client.cli.CliClient.executeStatement(CliClient.java:326)
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:297)
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:221)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95)
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)
{code};;;","30/Apr/21 11:57;twalthr;[~jark] what do you think about introducing a new built-in function `row_from()` similar to `map_from_arrays` etc. to avoid all these problems with Calcite's parser. `ROW()` seems to cause a lot of issues that could actually be solved by a simple built-in function.;;;","30/Apr/21 12:21;jark;What about {{STRUCT(...)}} ? This is also a well-known word and Spark provides this function to construct row. 

https://spark.apache.org/docs/latest/api/sql/#struct;;;","30/Apr/21 12:28;twalthr;I would keep the naming consistent with our existing data types. `map_`, `array_`, `row_` to find functions easier. Furthermore, we still have user-defined structured types which could cause confusion if we call it `struct`.;;;","30/Apr/21 14:42;jark;Is it possible to register a custom {{ROW}} function to override the built-in {{SqlStdOperatorTable.ROW}}? 
I remember we override the {{JSON_VALUE}} function in internal branch, and {{JSON_VALUE}} is also defined in parser. ;;;","01/Jun/21 23:28;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","09/Jun/21 22:42;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Dec/21 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","16/Dec/21 10:38;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Apr/23 10:23;aitozi;Now calcite version is bundled to 1.29, the complex expression for row is supported (verified). This issue can be closed now. CC [~libenchao]  [~jark] ;;;","11/Apr/23 12:03;libenchao;[~aitozi] Thanks for the catch, I'm closing this. 

I also checked the documentation change introduced in 8cf28ee58c68a688a3e5e92ac141d4de1d43b296, it now looks good.;;;","11/Apr/23 16:06;aitozi;[~libenchao] do you know why the explicit ROW construct is removed from the doc, (the git history of the file seems not trackable)

Does it's not encouraged to use explicit ROW call ? If not, I think we should add it back.;;;","12/Apr/23 05:42;aitozi;The content is modified

from 

[https://github.com/apache/flink/blob/177310ebe3d552ec71a1f1f97e0207cf30b6efed/docs/dev/table/functions/systemFunctions.md]

to

[https://github.com/apache/flink/blob/a5372e0c92c7a0f465beba5e5204b07769cd92e6/docs/data/sql_functions.yml] 

 

I think the content below is removed accidently.  CC [~sjwiesman] 

 
{code:java}
-- explicit ROW constructor ROW(value1 [, value2]*) {% endhighlight %} {code}
 

I think we should add this back, since the explicit ROW constructor's limitation has been solved;;;","12/Apr/23 11:25;libenchao;+1, thanks [~aitozi] for the investigation!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deactivate slot timeout if TaskSlotTable.tryMarkSlotActive is called,FLINK-18012,13308052,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,28/May/20 14:16,02/Jun/20 14:51,13/Jul/23 08:07,02/Jun/20 14:51,1.10.1,1.11.0,1.9.3,,,1.10.2,1.11.0,1.9.4,,Runtime / Coordination,,,,,0,pull-request-available,,,,"With FLINK-9932 we loosened the slot allocation protocol in a way that the {{JobMaster}} can deploy {{Tasks}} into a slot which has not been {{ACTIVATED}} but only {{ALLOCATED}} for a given job. This allowed to better handle the case where the {{JobMasterGateway#offerSlots}} response was late so that it timed out. The way it was solved is to offer a {{TaskSlotTable#tryMarkSlotActive}} method which, in contrast to {{TaskSlotTable#markSlotActive}}, would not fail if the requested slot was not available.

However, the problem is that the former method does not deactivate the slot timeout. Hence, it can happen if the {{offerSlots}} response never arrives at the {{TaskExecutor}} that an {{ACTIVATED}} slot times out.

In order to fix the problem, we should also deactivate the slot timeout when {{TaskSlotTable#tryMarkSlotActive}} is being called.",,trohrmann,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17404,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 14:51:37 UTC 2020,,,,,,,,,,"0|z0f9rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 14:51;trohrmann;Fixed via

master: 3dabc6943410e5906b7da6ca4eeefc857789138d
1.11.0: 6551b6b5928943b80be1309ac3ef439782fe18d0
1.10.2: ffc3d86a6663d54962892549e94949633a52aa9c
1.9.4: aa44ffc72a9a3839a1c39d8717c9d0e085c1296c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HistoryServer does not log environment information on startup,FLINK-18008,13308037,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,28/May/20 13:08,02/Jun/20 12:36,13/Jul/23 08:07,02/Jun/20 12:36,1.10.0,,,,,1.10.2,1.11.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 12:36:59 UTC 2020,,,,,,,,,,"0|z0f9o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 12:36;chesnay;master: 804851729a9aa3858cc4e553b3fd320e7a72fa2f
1.11: 4ee5470344dd30ea8135afc86c7df2fbb5b08159
1.10: 7677674dd2dab41c989b8a66e230b4ca68629eac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
It will throw Invalid lambda deserialization Exception when writing to elastic search with new format,FLINK-18006,13308032,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,fsk119,fsk119,28/May/20 12:57,23/Jun/20 07:49,13/Jul/23 08:07,04/Jun/20 07:15,1.11.0,,,,,1.11.0,,,,Connectors / ElasticSearch,Table SQL / Client,,,,0,pull-request-available,,,,"My job follows:
{code:java}
// 
create table csv( pageId VARCHAR, eventId VARCHAR, recvTime VARCHAR) with ( 'connector' = 'filesystem',
 'path' = '/Users/ohmeatball/Work/flink-sql-etl/data-generator/src/main/resources/user3.csv',
 'format' = 'csv'
 )
-----------------------------------------
CREATE TABLE es_table (
  aggId varchar ,
  pageId varchar ,
  ts varchar ,
  expoCnt int ,
  clkCnt int
) WITH (
'connector' = 'elasticsearch',
'hosts' = 'http://localhost:9200',
'index' = 'cli_test',
'document-id.key-delimiter' = '$',
'sink.bulk-flush.interval' = '1000',
'format' = 'json'
)
-----------------------------------------
INSERT INTO es_table
SELECT  pageId,eventId,cast(recvTime as varchar) as ts, 1, 1 from csv;
{code}
The full exception follows:
{code:java}
Sink(table=[default_catalog.default_database.es_table], fields=[aggId, pageId, ts, expoCnt, clkCnt]) (1/1) (b51209fac96948c20e85b8df137287d3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@bb5ab41.Sink(table=[default_catalog.default_database.es_table], fields=[aggId, pageId, ts, expoCnt, clkCnt]) (1/1) (b51209fac96948c20e85b8df137287d3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@bb5ab41.org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot instantiate user function. at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:291) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:471) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:393) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:459) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:393) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:155) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:518) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]Caused by: java.io.IOException: unexpected exception type at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1682) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1254) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 more
Caused by: java.lang.reflect.InvocationTargetExceptionCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1248) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 moreCaused by: java.lang.IllegalArgumentException: Invalid lambda deserialization at org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink$Builder.$deserializeLambda$(ElasticsearchSink.java:80) ~[flink-sql-connector-elasticsearch7_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1248) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151]
 at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 more
{code}
Notice: everything works fine with former connector grammer.",ElasticSearch version is 7.6.0,dwysakowicz,fsk119,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MSHADE-260,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 07:15:19 UTC 2020,,,,,,,,,,"0|z0f9n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/20 14:19;fsk119;It seems that it just can't write data into elasticsearch7 with new format. In my case,  the easiest insert statement, such as
{code:java}
 INSERT INTO es_table VALUES('1', '1', '1', 1, 1); 
{code}
will fail with the same error message. 

 ;;;","01/Jun/20 08:33;jark;I also reproduced this problem by using SQL CLIENT. But I didn't find the root cause yet... Could you help to have a look [~dwysakowicz]?;;;","01/Jun/20 08:41;jark;This maybe a bug of JDK: https://bugs.java.com/bugdatabase/view_bug.do?bug_id=JDK-8154236
But not sure which line of code triggers this bug.;;;","02/Jun/20 06:53;dwysakowicz;I will look into it.;;;","02/Jun/20 15:18;dwysakowicz;It turned out to be more complicated than I thought in the beginning. Actually the problem is caused by this: https://issues.apache.org/jira/browse/MSHADE-260

The problem is that we shade all Elasticsearch classes in {{flink-sql-connector-elasticsearch7}} module. At the same time {{RestClientFactory}} exposes {{org.elasticsearch.client.RestClientBuilder}} in the interface. The default {{org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink.Builder#restClientFactory}} is specified with a lambda. The shade plugin does not shade the  {{RestClientBuilder}} class in the {{org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink.Builder$deserializeLambda$(java.lang.invoke.SerializedLambda);}} which fails the deserialization of that lambda.

The reason why it worked in the old sink was that we were always overriding the default value. Therefore we never had problems with deserializing that lambda.;;;","04/Jun/20 07:15;dwysakowicz;Fixed in:
master: 640a56fee9d777ff2acb69ab6d77275e7373415d
1.11: 10f1d37841a0b8f3806678df2bab85e71842b06a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update checkpoint UI related pictures in documentation,FLINK-18004,13308014,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,28/May/20 11:49,06/Nov/20 16:52,13/Jul/23 08:07,06/Nov/20 16:51,1.11.0,,,,,1.12.0,,,,Documentation,,,,,0,pull-request-available,,,,"After FLINK-13390 which clarifies what the ""state size"" means on incremental checkpoint, the checkpoint UI has already changed, and we should also update related documentation to not mislead users.",,klion26,liyu,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 16:52:11 UTC 2020,,,,,,,,,,"0|z0f9j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/20 16:52;yunta;Merged in master with 707d824cac77f41fa8ee9072668bc4c1617eb0db;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in CatalogTableStatisticsConverter.convertToColumnStats method,FLINK-17996,13307942,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,nicholasjiang,godfreyhe,godfreyhe,28/May/20 07:03,02/Jun/20 09:59,13/Jul/23 08:07,02/Jun/20 09:59,,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Currently, hive catalog only supports a few kinds of statistics, otherwise return null. (see HiveStatsUtil#createTableColumnStats). If there is a decimal statistics, NPE will occur in CatalogTableStatisticsConverter.convertToColumnStats method

Caused  by:  java.lang.NullPointerException
                at  org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter.convertToColumnStats(CatalogTableStatisticsConverter.java:77)
                at  org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter.convertToColumnStatsMap(CatalogTableStatisticsConverter.java:68)
                at  org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter.convertToTableStats(CatalogTableStatisticsConverter.java:57)
                at  org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.extractTableStats(DatabaseCalciteSchema.java:113)
                at  org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getStatistic(DatabaseCalciteSchema.java:97)
                at  org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.lambda$getTable$0(DatabaseCalciteSchema.java:74)
                at  java.util.Optional.map(Optional.java:215)
                at  org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getTable(DatabaseCalciteSchema.java:72)
                at  org.apache.calcite.",,godfreyhe,jark,lzljs3620320,nicholasjiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 09:59:08 UTC 2020,,,,,,,,,,"0|z0f934:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/20 12:02;nicholasjiang;[~lzljs3620320]Could you please assign this bug to me for fixing? Method createTableColumnStats of HiveStatsUtil could return null, which cause columnStatisticsData could be null.;;;","28/May/20 12:32;lzljs3620320;[~nicholasjiang] Assign to you~;;;","02/Jun/20 09:59;jark;Fixed in
 - master (1.12.0): 65878caddfc05af4a34f22a9d5e92b2d4004d859
 - 1.11.0: 33f852417e7cdca0211232d60ba340927f3f0d81;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the race condition between CheckpointBarrierUnaligner#processBarrier and #notifyBarrierReceived,FLINK-17994,13307929,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zjwang,zjwang,zjwang,28/May/20 05:53,04/Jun/20 02:36,13/Jul/23 08:07,01/Jun/20 08:50,1.11.0,,,,,1.11.0,1.12.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"The race condition issue happens as follow:
 * ch1 is received from network for one input channel by netty thread and schedule the ch1 into mailbox via #notifyBarrierReceived
 * ch2 is received from network for another input channel by netty thread, but before calling #notifyBarrierReceived this barrier was inserted into channel's data queue in advance. Then it would cause task thread process ch2 earlier than #notifyBarrierReceived by netty thread.
 * Task thread would execute checkpoint for ch2 immediately because ch2 > ch1.
 * After that, the previous scheduled ch1 is performed from mailbox by task thread, then it causes the IllegalArgumentException inside SubtaskCheckpointCoordinatorImpl#checkpointState because it breaks the initial assumption that checkpoint is executed in incremental way for aligned mode.

The key problem is that we can not remove the checkpoint action from mailbox queue before the next checkpoint is going to execute now. One possible solution is that we record the previous aborted checkpoint id inside SubtaskCheckpointCoordinatorImpl#abortedCheckpointIds, then when the queued checkpoint inside mailbox is executing, it will exit directly if found the checkpoint id was already aborted before.",,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 08:50:34 UTC 2020,,,,,,,,,,"0|z0f908:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/20 08:50;zjwang;Merged in release-1.11 : 64ca88ac989ee7525cb821670f293404b7b30d2d

Merged in master: aa882c102f1b5d67f0ecb4efaaf9d067b4d8d424;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception from RemoteInputChannel#onBuffer should not fail the whole NetworkClientHandler,FLINK-17992,13307898,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zjwang,zjwang,zjwang,28/May/20 02:58,03/Jun/20 02:33,13/Jul/23 08:07,29/May/20 06:41,1.10.0,1.10.1,1.11.0,,,1.11.0,1.12.0,,,Runtime / Network,,,,,1,pull-request-available,,,,"RemoteInputChannel#onBuffer is invoked by CreditBasedPartitionRequestClientHandler while receiving and decoding the network data. #onBuffer can throw exceptions which would tag the error in client handler and fail all the added input channels inside handler. Then it would cause a tricky potential issue as following.

If the RemoteInputChannel is canceling by canceler thread, then the task thread might exit early than canceler thread terminate. That means the PartitionRequestClient might not be closed (triggered by canceler thread) while the new task attempt is already deployed into this TaskManger. Therefore the new task might reuse the previous PartitionRequestClient while requesting partitions, but note that the respective client handler was already tagged an error before during above RemoteInputChannel#onBuffer. It will cause the next round unnecessary failover.

It is hard to find this potential issue in production because it can be restored normal finally after one or more additional failover. We find this potential problem from UnalignedCheckpointITCase because it will define the precise restart times within configured failures.

The solution is to only fail the respective task when its internal RemoteInputChannel#onBuffer throws any exceptions instead of failing the whole channels inside client handler, then the client is still health and can also be reused by other input channels as long as it is not released yet.",,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 06:41:40 UTC 2020,,,,,,,,,,"0|z0f8tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/20 06:41;zjwang;Merged in release-1.11: 34e6d22bdd179796daf6df46738d85303a839704

Merged in master: 371f3de5371afb78d465315098bebec6ed36656b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrowSourceFunctionTestBase.testParallelProcessing is instable,FLINK-17990,13307879,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,28/May/20 01:51,29/May/20 10:08,13/Jul/23 08:07,29/May/20 10:08,1.11.0,,,,,1.11.0,,,,API / Python,,,,,0,pull-request-available,test-stability,,,"It failed on cron JDK 11 tests with following error:
{code}
2020-05-27T21:37:21.2840832Z [ERROR] testParallelProcessing(org.apache.flink.table.runtime.arrow.sources.ArrowSourceFunctionTest)  Time elapsed: 0.433 s  <<< FAILURE!
2020-05-27T21:37:21.2841551Z java.lang.AssertionError: expected:<4> but was:<5>
2020-05-27T21:37:21.2842025Z 	at org.junit.Assert.fail(Assert.java:88)
2020-05-27T21:37:21.2842470Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-05-27T21:37:21.2842994Z 	at org.junit.Assert.assertEquals(Assert.java:645)
2020-05-27T21:37:21.2843484Z 	at org.junit.Assert.assertEquals(Assert.java:631)
2020-05-27T21:37:21.2844779Z 	at org.apache.flink.table.runtime.arrow.sources.ArrowSourceFunctionTestBase.checkElementsEquals(ArrowSourceFunctionTestBase.java:243)
2020-05-27T21:37:21.2845873Z 	at org.apache.flink.table.runtime.arrow.sources.ArrowSourceFunctionTestBase.testParallelProcessing(ArrowSourceFunctionTestBase.java:233)
{code}
instance: [https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/2304/logs/535]",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 10:08:17 UTC 2020,,,,,,,,,,"0|z0f8qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/20 10:08;dian.fu;Merged to master via 95793258bc9603d8c16c51615408514c9e57b20d and release-1.11 via c5194d1c913b6e536347a8a1049deb3ec701e168.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpointing slows down after reaching state.checkpoints.num-retained,FLINK-17988,13307834,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,roman,roman,27/May/20 19:58,28/May/20 11:53,13/Jul/23 08:07,28/May/20 11:53,1.11.0,,,,,1.11.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"With Unaligned checkpoints, happens always (new checkpoint is never started or triggered).

With Aligned checkpoints - to some degree - depending on state size and thresholds: delayed by 1 minute. Delay grows very slowly with state size.

 

Filesystems: s3p and s3a

Parallelism: 176, repartition (num stages): 5.

Number of files in checkpoint is about 1K (depends on state.backend.fs.memory-threshold and their size). Size doesn't matter (100K..10G).",,pnowojski,roman,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17813,,,,,,,,,"27/May/20 20:14;roman;flink-conf.yaml;https://issues.apache.org/jira/secure/attachment/13004174/flink-conf.yaml","27/May/20 20:09;roman;jobmanager.s3a.dmp;https://issues.apache.org/jira/secure/attachment/13004172/jobmanager.s3a.dmp","27/May/20 20:10;roman;jobmanager.s3p.dmp;https://issues.apache.org/jira/secure/attachment/13004173/jobmanager.s3p.dmp",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 11:53:45 UTC 2020,,,,,,,,,,"0|z0f8gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 20:27;roman;From thread stack traces follows, that fs.delete() on both filesystems causes object listing on s3:
{code:java}
 at com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:895)
 at com.facebook.presto.hive.s3.PrestoS3FileSystem.listPrefix(PrestoS3FileSystem.java:484)
 at com.facebook.presto.hive.s3.PrestoS3FileSystem.getFileStatus(PrestoS3FileSystem.java:315)
 at com.facebook.presto.hive.s3.PrestoS3FileSystem.directory(PrestoS3FileSystem.java:450)
 at com.facebook.presto.hive.s3.PrestoS3FileSystem.delete(PrestoS3FileSystem.java:427)
 at org.apache.flink.fs.s3presto.common.HadoopFileSystem.delete(HadoopFileSystem.java:147)
 at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.delete(PluginFileSystemFactory.java:150)
 at org.apache.flink.runtime.state.filesystem.FileStateHandle.discardState(FileStateHandle.java:86)
 at org.apache.flink.runtime.state.AbstractChannelStateHandle.discardState(AbstractChannelStateHandle.java:55)
 at org.apache.flink.runtime.state.StateUtil$$Lambda$430/787068135.accept(Unknown Source)
 at org.apache.flink.util.LambdaUtil.applyToAllWhileSuppressingExceptions(LambdaUtil.java:55)
 at org.apache.flink.runtime.state.StateUtil.bestEffortDiscardAllStateObjects(StateUtil.java:60)
 at org.apache.flink.runtime.checkpoint.OperatorSubtaskState.discardState(OperatorSubtaskState.java:236)
 at org.apache.flink.runtime.checkpoint.OperatorState.discardState(OperatorState.java:132)
 at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.doDiscard(CompletedCheckpoint.java:263)
 at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.discardOnSubsume(CompletedCheckpoint.java:218)
 at org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.addCheckpoint(StandaloneCompletedCheckpointStore.java:72)
 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1005)
 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:912)


{code}
 
{code:java}
 at org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:1255)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2223)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2149)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.delete(S3AFileSystem.java:1697)
 at org.apache.flink.fs.s3hadoop.common.HadoopFileSystem.delete(HadoopFileSystem.java:147)
 at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.delete(PluginFileSystemFactory.java:150)
 at org.apache.flink.runtime.state.filesystem.FileStateHandle.discardState(FileStateHandle.java:86)
 at org.apache.flink.runtime.state.AbstractChannelStateHandle.discardState(AbstractChannelStateHandle.java:60)
 at org.apache.flink.runtime.state.StateUtil$$Lambda$428/480418082.accept(Unknown Source)
 at org.apache.flink.util.LambdaUtil.applyToAllWhileSuppressingExceptions(LambdaUtil.java:55)
 at org.apache.flink.runtime.state.StateUtil.bestEffortDiscardAllStateObjects(StateUtil.java:60)
 at org.apache.flink.runtime.checkpoint.OperatorSubtaskState.discardState(OperatorSubtaskState.java:236)
 at org.apache.flink.runtime.checkpoint.OperatorState.discardState(OperatorState.java:132)
 at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.doDiscard(CompletedCheckpoint.java:263)
 at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.discardOnSubsume(CompletedCheckpoint.java:218)
 at org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.addCheckpoint(StandaloneCompletedCheckpointStore.java:72)
 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1005)
 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:912){code}
Unclear why aligned mode is less affected (it lists objects too).

Triggering of new requests is blocked on monitor which is held by discardOnSubsume. This seems reasonable (otherwise checkpoints to discard would pile up; and triggered checkpoint wouldn't be able to start anyways).;;;","27/May/20 22:20;roman;The problem is in duplicated discard calls for channel state handles.

With the proposed fix discard times are similar to a stateful job with aligned checkpoints (1-2 minutes).
 ;;;","28/May/20 11:53;pnowojski;Merged to master as 435bf23b14 and release-1.11 as b70ce9d183;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Erroneous check in FsCheckpointStateOutputStream#write(int),FLINK-17986,13307804,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,roman,roman,27/May/20 16:47,28/May/20 11:56,13/Jul/23 08:07,28/May/20 11:54,1.11.0,,,,,1.11.0,,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,"When fixing FLINK-17820 a {{flush}} call was accidentally introduced on every single byte/int write to {{FsCheckpointStateOutputStream}}, which could significantly affect performance.",,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17820,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 11:54:37 UTC 2020,,,,,,,,,,"0|z0f8a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/20 11:54;pnowojski;Merged to release-1.11 as d89eb1a2e8 and master as 0179156ee6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop Configuration is not properly serialized in HBaseRowInputFormat,FLINK-17968,13307721,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,rmetzger,rmetzger,27/May/20 12:20,02/Jun/20 03:29,13/Jul/23 08:07,02/Jun/20 03:29,1.12.0,,,,,1.11.0,,,,Connectors / HBase,Table SQL / Ecosystem,,,,0,pull-request-available,,,,"This pull request mentions an issue with the serialization of the {{Configuration}} field in {{HBaseRowInputFormat}}: https://github.com/apache/flink/pull/12146.
After reviewing the code, it seems that this field is {{transient}}, thus it is always {{null}} at runtime.

Note {{HBaseRowDataInputFormat}} is likely suffering from the same issue (because the code has been copied)",,jackylau,jark,leonard,rmetzger,tartarus,zenfenan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17932,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 03:29:39 UTC 2020,,,,,,,,,,"0|z0f7rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 17:51;zenfenan;[~rmetzger]

I have commented my thoughts in the quoted PR. Please take a look when you have some time.;;;","28/May/20 02:24;tartarus;[~rmetzger] Please assign the issue to me, and I will fix it.

thanks;;;","29/May/20 13:53;tartarus;I removed the logic of generating hbase-site.xml in the unit test, and must set Hbase configuration when initialize HbaseInputFormat.;;;","02/Jun/20 03:29;jark;Fixed in 
 - master (1.12.0): 03b82f9aa81b828b06c61f7f30f92a331ec5120b
 - 1.11.0: 280df7236a796d5b175f94952bea07b207ddf0d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Could not find document *** in docs,FLINK-17967,13307715,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wuxuyang78,wuxuyang78,wuxuyang78,27/May/20 12:00,09/Jul/20 07:39,13/Jul/23 08:07,01/Jun/20 03:29,1.11.0,,,,,1.11.0,,,,Documentation,,,,,0,pull-request-available,,,,"When I run build_docs.sh -p in docs dir with *master*, some error as list down below. And web server cannot started.
{color:#FF0000}Liquid Exception: Could not find document 'concepts/stateful-stream-processing.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/stream/state/state.zh.md{color}

List files also have same problem.
_concepts/flink-architecture.zh.md_
_dev/batch/hadoop_compatibility.zh.md_
_dev/batch/index.zh.md_
_dev/connectors/cassandra.zh.md_
_dev/stream/operators/index.zh.md_
_dev/stream/operators/state.zh.md_
_dev/table/common.zh.md_

1.11 seems has same problem.",,jark,leonard,wuxuyang78,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,FLINK-18036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 03:29:37 UTC 2020,,,,,,,,,,"0|z0f7qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 12:07;wuxuyang78;Rename {color:#FF0000}concepts/stateful-stream-processing.md {color:#172b4d}to{color} concepts/stateful-stream-processing.zh.md{color:#172b4d} and it will work .{color}{color}

{color:#FF0000}{color:#172b4d}I will solve it, could you assign it to me?{color}{color};;;","28/May/20 04:24;jark;I didn't get what's the problem. There is already a file {{concepts/stateful-stream-processing.zh.md}}. I also tried to build the docs on master, and everything is fine. Did you build the docs in docker? ;;;","28/May/20 06:19;wuxuyang78;Hi,

I build the docs with MacOS 14.

Branch master, \{% link concepts/stateful-stream-processing.md %} in file  [https://github.com/apache/flink/blob/master/docs/dev/stream/state/state.zh.md]

is still missing ’zh‘.;;;","29/May/20 12:55;jark;Assigned to you [~wuxuyang78].;;;","01/Jun/20 03:29;jark;Fixed in
 - master (1.12.0): 2075d5b121c00859979a83d856c23827c9b74f08
 - 1.11.0: fe2822c6e54ac538982196653b460777f80e7434;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive dialect doesn't properly handle special character escaping with SQL CLI,FLINK-17965,13307710,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,lirui,lirui,27/May/20 11:42,11/Jun/20 06:53,13/Jul/23 08:07,11/Jun/20 06:51,,,,,,1.11.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"The following DDL runs fine with table api but doesn't work with sql-client:
{code:java}
create table tbl (x int) row format delimited lines terminated by '\n'
{code}
In sql-client, '\' will be escaped and therefore we'll have ""\n"", instead of new line character, as the line terminator.",,libenchao,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 06:51:09 UTC 2020,,,,,,,,,,"0|z0f7pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/20 06:51;lzljs3620320;master: 6f06e3df84a733dd75c8e273fb1e395d905a2e5c

release-1.11: 2d0b4473f3f73b3d43c38bfe5d30bc1421aead41;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Exception: ""CANCELLED: call already cancelled"" is thrown when run python udf",FLINK-17959,13307653,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,hequn8128,hequn8128,27/May/20 08:41,03/Jun/20 15:18,13/Jul/23 08:07,03/Jun/20 15:18,1.10.1,1.11.0,,,,1.10.2,1.11.0,,,API / Python,,,,,0,pull-request-available,,,,"The exception is thrown when running Python UDF:
{code:java}
May 27, 2020 3:20:49 PM org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.SerializingExecutor run
SEVERE: Exception while executing runnable org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1Closed@3960b30e
org.apache.beam.vendor.grpc.v1p21p0.io.grpc.StatusRuntimeException: CANCELLED: call already cancelled
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.Status.asRuntimeException(Status.java:524)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.stub.ServerCalls$ServerCallStreamObserverImpl.onCompleted(ServerCalls.java:366)
	at org.apache.beam.runners.fnexecution.state.GrpcStateService$Inbound.onError(GrpcStateService.java:145)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onCancel(ServerCalls.java:270)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.PartialForwardingServerCallListener.onCancel(PartialForwardingServerCallListener.java:40)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.ForwardingServerCallListener.onCancel(ForwardingServerCallListener.java:23)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onCancel(ForwardingServerCallListener.java:40)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.Contexts$ContextualizedServerCallListener.onCancel(Contexts.java:96)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.closed(ServerCallImpl.java:337)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1Closed.runInContext(ServerImpl.java:793)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

The job can output the right results however it seems something goes wrong during the shutdown procedure.

You can reproduce the exception with the following code(note: the exception happens occasionally):
{code}
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment, DataTypes
from pyflink.table.descriptors import Schema, OldCsv, FileSystem
from pyflink.table.udf import udf

env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)
t_env = StreamTableEnvironment.create(env)

add = udf(lambda i, j: i + j, [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT())

t_env.register_function(""add"", add)

t_env.connect(FileSystem().path('/tmp/input')) \
    .with_format(OldCsv()
                 .field('a', DataTypes.BIGINT())
                 .field('b', DataTypes.BIGINT())) \
    .with_schema(Schema()
                 .field('a', DataTypes.BIGINT())
                 .field('b', DataTypes.BIGINT())) \
    .create_temporary_table('mySource')

t_env.connect(FileSystem().path('/tmp/output')) \
    .with_format(OldCsv()
                 .field('sum', DataTypes.BIGINT())) \
    .with_schema(Schema()
                 .field('sum', DataTypes.BIGINT())) \
    .create_temporary_table('mySink')

t_env.from_path('mySource')\
    .select(""add(a, b)"") \
    .insert_into('mySink')

t_env.execute(""tutorial_job"")
{code}
",,aomidvar,dian.fu,hequn8128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 15:18:09 UTC 2020,,,,,,,,,,"0|z0f7co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/20 15:18;dian.fu;master: 
 80b002ba7232abcece0c839aaf82cbd2034f9965
 b31b180b3bc928d61521d2a67c2028767428c731
 66d1300f9a51245f98977e654041d4a35cfb8bf1

release-1.11:
 5a2afbc01a438ea2f2598eac9c6dac5f58012f35
 de05d81df34a7e7ccfc9e5fb5d75c4cc3cfa1006
 0195b9819d778bb8b3421beed2b431e6066e6239

release-1.10:
 bd2ca499263a9b5819d3cc2320f59ad5cb7ef5e9
 eafc602022668af68e520e734f9faa15eaa6ac61
 c70460ad878a5c33aa4a16a054e21e94df524d81;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes session constantly allocates taskmanagers after cancel a job,FLINK-17958,13307647,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xtsong,wangyang0918,wangyang0918,27/May/20 08:21,28/May/20 07:57,13/Jul/23 08:07,28/May/20 07:57,1.11.0,1.12.0,,,,1.11.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"When i am testing the {{kubernetes-session.sh}}, i find that the {{KubernetesResourceManager}} will constantly allocate taskmanager after cancel a job. I think it may be caused by a bug of the following code. When the {{dividend}} is 0 and {{divisor}} is bigger than 1, the return value will be 1. However, we expect it to be 0.
{code:java}
/**
 * Divide and rounding up to integer.
 * E.g., divideRoundUp(3, 2) returns 2.
 * @param dividend value to be divided by the divisor
 * @param divisor value by which the dividend is to be divided
 * @return the quotient rounding up to integer
 */
public static int divideRoundUp(int dividend, int divisor) {
   return (dividend - 1) / divisor + 1;
}{code}
 

How to reproduce this issue?
 # Start a Kubernetes session
 # Submit a Flink job to the existing session
 # Cancel the job and wait for the TaskManager released via idle timeout
 # More and more TaskManagers will be allocated",,trohrmann,wangyang0918,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 07:57:16 UTC 2020,,,,,,,,,,"0|z0f7bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 08:39;xtsong;True. I agree that the bug in {{MathUtils#divideRoundUp}} is the cause of the problem.

I think we should do the following things.
* Make {{MathUtils#divideRoundUp}} return 0 when {{dividend}} is 0.
* Check that {{dividend >= 0 && divisor > 0}}, and throw an exception otherwise.
* Add more test cases for {{MathUtils#divideRoundUp}}.
* In {{WorkerSpecContainerResourceAdapter#normalize}}, we should return {{unitValue}} in case {{MathUtils#divideRoundUp}} returns 0.;;;","28/May/20 07:57;trohrmann;Fixed via

master: 1386e0db962a794e5cb78b72917a1c5340bd027e
1.11.0: 2cac04d7a74ff8fa7987bd137056381020775f07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Forbidden syntax ""CREATE SYSTEM FUNCTION"" for sql parser",FLINK-17957,13307646,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhaoWeiNan,danny0405,danny0405,27/May/20 08:09,28/May/21 09:00,13/Jul/23 08:07,08/Apr/21 04:37,1.11.0,,,,,1.13.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"This syntax is invalid, but the parser still works.",,danny0405,fsk119,jark,leonard,libenchao,ZhaoWeiNan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 04:37:20 UTC 2021,,,,,,,,,,"0|z0f7b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/21 15:50;ZhaoWeiNan;[~fsk119]，please assgin this issue to to me , i will fix it.;;;","08/Apr/21 04:37;jark;Fixed in master: 43feb550f080c4d927cebc066a68f6d177800f26;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BucketLifeCycleListener should just in Buckets,FLINK-17955,13307610,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,27/May/20 04:43,27/May/20 08:29,13/Jul/23 08:07,27/May/20 08:29,,,,,,1.11.0,,,,Connectors / FileSystem,,,,,0,,,,,We should keep BucketLifeCycleListener just in runtime.,,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 08:29:52 UTC 2020,,,,,,,,,,"0|z0f73c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 04:46;lzljs3620320;PR: [https://github.com/apache/flink/pull/12334];;;","27/May/20 08:29;lzljs3620320;master: 5a9fe5d431cc7722a0af0ab2d6a95460e7e38e77

release-1.11: fe1b533a855b72a9ce3757778a40920e89d7f00c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken Scala env.countinuousSource method ,FLINK-17950,13307533,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sewen,sewen,26/May/20 20:14,27/May/20 20:19,13/Jul/23 08:07,27/May/20 20:19,,,,,,1.11.0,,,,API / Scala,,,,,0,,,,,"The Scala {{StreamExecutionEnvironment.countinuousSource(...)}} method has two critical problems:
  - Its return type is {{Unit}} instead of {{DataStream}}, so that no one can use the created stream
  - It does not forward the TypeInformation identified by the ScalaCompiler but relies on the Java TypeExtraction stack, which cannot handle most Scala types. ",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 20:19:31 UTC 2020,,,,,,,,,,"0|z0f6m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 20:19;sewen;Fixed in

1.11.0 via 
  - 0d2ac14657298f1f749d7de9867f4d46008ce2ab

1.12.0 (master) via
  - b992cef660c26921608bae3fbc407f8e4b60dd1c
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use new type system for SQL Client collect sink,FLINK-17948,13307463,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,26/May/20 14:55,13/Apr/21 02:27,13/Jul/23 08:07,13/Apr/21 02:27,1.11.0,,,,,1.13.0,,,,Table SQL / Client,,,,,0,,,,,"My job is following:

 
{code:java}
CREATE TABLE currency (
  currency_id BIGINT,
  currency_name STRING,
  rate DOUBLE,
  currency_timestamp  TIMESTAMP,
  country STRING,
  precise_timestamp TIMESTAMP(6),
  precise_time TIME(6),
  gdp DECIMAL(10, 6)
) WITH (
   'connector' = 'jdbc',
   'url' = 'jdbc:mysql://localhost:3306/flink',
   'username' = 'root',
   'password' = '123456',
   'table-name' = 'currency',
   'driver' = 'com.mysql.jdbc.Driver',
   'lookup.cache.max-rows' = '500',
   'lookup.cache.ttl' = '10s',
   'lookup.max-retries' = '3')

{code}
When select * from currency, the precision of results are not as same as expected.  The precision of field precise_timestamp is 3 not 6, and the field gdp has more digit as expected. 

 

!image-2020-05-26-22-56-43-835.png!

The data in mysql is following:

!image-2020-05-26-22-58-02-326.png!","mysql:
 image: mysql:8.0
 volumes:
 - ./mysql/mktable.sql:/docker-entrypoint-initdb.d/mktable.sql
 environment:
 MYSQL_ROOT_PASSWORD: 123456
 ports:
 - ""3306:3306""",danny0405,f.pompermaier,fsk119,godfreyhe,jark,leonard,libenchao,txhsj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/20 14:56;fsk119;image-2020-05-26-22-56-43-835.png;https://issues.apache.org/jira/secure/attachment/13004045/image-2020-05-26-22-56-43-835.png","26/May/20 14:58;fsk119;image-2020-05-26-22-58-02-326.png;https://issues.apache.org/jira/secure/attachment/13004046/image-2020-05-26-22-58-02-326.png","13/Apr/21 02:26;fsk119;image-2021-04-13-10-26-34-721.png;https://issues.apache.org/jira/secure/attachment/13023742/image-2021-04-13-10-26-34-721.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 13 02:26:56 UTC 2021,,,,,,,,,,"0|z0f66o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 08:23;danny0405;cc [~Leonard Xu];;;","03/Jun/20 06:37;leonard;The precision lost happened in Sql client rather than JDBC connector, JDBC connector works fine.
 We used deprecated Schema#toRowType method to obtain a [serializer|https://github.com/apache/flink/blob/master/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/result/MaterializedCollectBatchResult.java#L65)] which ignored the precision information.

I think this could be an improvement or bug fix for sql-client.

CC: [~TsReaper] [~godfreyhe]

 ;;;","10/Dec/20 02:56;jark;A user also reported TIMESTAMP WITH LOCAL TIME ZONE type can't be used in SQL Client query. 

http://apache-flink.147419.n8.nabble.com/flink-sql-postgres-timestamp-td9235.html;;;","09/Apr/21 02:29;godfreyhe;cc [~fsk119];;;","13/Apr/21 02:26;fsk119;!image-2021-04-13-10-26-34-721.png!

solved;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The config option ""pipeline.jars"" doesn't work if the job was executed via TableEnvironment.execute_sql and StatementSet.execute",FLINK-17946,13307458,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,dian.fu,dian.fu,26/May/20 14:42,29/May/20 09:53,13/Jul/23 08:07,29/May/20 09:53,1.11.0,,,,,1.11.0,,,,API / Python,,,,,0,pull-request-available,,,,"For the following job:
{code}
import logging
import sys
import tempfile

from pyflink.table import BatchTableEnvironment, EnvironmentSettings


def word_count():
    content = ""line Licensed to the Apache Software Foundation ASF under one "" \
              ""line or more contributor license agreements See the NOTICE file "" \
              ""line distributed with this work for additional information "" \
              ""line regarding copyright ownership The ASF licenses this file "" \
              ""to you under the Apache License Version the "" \
              ""License you may not use this file except in compliance "" \
              ""with the License""

    environment_settings = EnvironmentSettings.new_instance().in_batch_mode().\
        use_blink_planner().build()
    t_env = BatchTableEnvironment.create(environment_settings=environment_settings)
    t_env.get_config().get_configuration().set_string(
        ""pipeline.jars"",
        ""file:///Users/dianfu/workspace/wordcount_python/flink-csv-1.11.0-sql-jar.jar"")

    # register Results table in table environment
    tmp_dir = tempfile.gettempdir()
    result_path = tmp_dir + '/result'

    logging.info(""Results directory: %s"", result_path)

    sink_ddl = """"""
        create table Results(
            word VARCHAR,
            `count` BIGINT
        ) with (
            'connector' = 'filesystem',
            'format' = 'csv',
            'path' = '{}'
        )
        """""".format(result_path)
    t_env.execute_sql(sink_ddl)

    elements = [(word, 1) for word in content.split("" "")]
    table = t_env.from_elements(elements, [""word"", ""count""]) \
        .group_by(""word"") \
        .select(""word, count(1) as count"")

    statement_set = t_env.create_statement_set()
    statement_set.add_insert(""Results"", table, overwrite=True)
    statement_set.execute()


if __name__ == '__main__':
    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=""%(message)s"")

    word_count()
{code}

It will throw exceptions as following:
{code}
Caused by: java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.flink.table.filesystem.FileSystemOutputFormat.formatFactory of type org.apache.flink.table.filesystem.OutputFormatFactory in instance of org.apache.flink.table.filesystem.FileSystemOutputFormat
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2133)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1305)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2237)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2155)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2013)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2231)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2155)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2013)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at java.util.HashMap.readObject(HashMap.java:1404)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2122)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2013)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2231)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2155)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2013)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2231)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2155)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2013)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550)
	at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511)
	at org.apache.flink.runtime.operators.util.TaskConfig.getStubWrapper(TaskConfig.java:288)
	at org.apache.flink.runtime.jobgraph.InputOutputFormatContainer.<init>(InputOutputFormatContainer.java:66)
	... 23 more
{code}

The reason is that the ""pipeline.jars"" option is not handled properly in StatementSet.execute and so it cannot find the jar specified via ""pipeline.jars"". This issue also exists in TableEnvironment.execute_sql",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 09:53:15 UTC 2020,,,,,,,,,,"0|z0f65k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/20 14:42;dian.fu;cc [~zhongwei];;;","29/May/20 09:53;dian.fu;Merged to
- master via cda25e5d185cf04c778ed4d173f9db6dfe3522ad
- release-1.11 via 33b4b4cd8b3d031f6d49196e2517c5deb0150fee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong output in sql client's table mode,FLINK-17944,13307448,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zjffdu,zjffdu,zjffdu,26/May/20 13:53,10/Jun/20 09:45,13/Jul/23 08:07,10/Jun/20 09:45,1.11.0,,,,,1.11.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"When I run the following sql example, I get the wrong output
{code:java}

SELECT name, COUNT(*) AS cnt FROM (VALUES ('Bob'), ('Alice'), ('Greg'), ('Bob')) AS NameTable(name) GROUP BY name; {code}
 
{code:java}
                      Bob                         1
                     Alice                         1
                      Greg                         1
                       Bob                         2 {code}

This is due to we add kind in Row, so the sematics of equals method changes",,jark,twalthr,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 09:45:40 UTC 2020,,,,,,,,,,"0|z0f63c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/20 13:56;zjffdu;I will make a PR for it;;;","10/Jun/20 09:45;twalthr;Fixed in 1.12.0: 7143e6a8200922e2ef7739ff8349bcd5070298bc
Fixed in 1.11.0: d36a2b3f02e9e31ab61d296ae76c3cffd48b4569;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveFunctionWrapper#getUDFClass should use Thread.currentThread().getContextClassLoader(),FLINK-17943,13307432,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,TsReaper,TsReaper,26/May/20 13:06,20/Nov/20 02:58,13/Jul/23 08:07,20/Nov/20 02:58,1.10.1,,,,,1.12.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"{{HiveFunctionWrapper#getUDFClass}} currently uses {{Class.forName(className)}} to load Hive UDF classes, while {{HiveFunctionWrapper#createFunction}} uses {{Thread.currentThread().getContextClassLoader()}}.

{{HiveFunctionWrapper#getUDFClass}} should also use {{Thread.currentThread().getContextClassLoader()}} as it is loading user classes.",,lirui,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 20 02:58:12 UTC 2020,,,,,,,,,,"0|z0f5zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 02:12;TsReaper;cc [~phoenixjiangnan] [~lzljs3620320] could you please take a look?;;;","20/Nov/20 02:58;lzljs3620320;master (1.12): 851300d29e5214c3cb222d8b5257e15530fdba01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Count distinct could not clean state in WindowOperator,FLINK-17942,13307426,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,libenchao,libenchao,libenchao,26/May/20 12:24,28/May/20 03:14,13/Jul/23 08:07,28/May/20 03:14,1.10.1,1.11.0,1.9.3,,,1.11.0,,,,,,,,,0,pull-request-available,,,,"MapView.clear() is generated in NamespaceAggsHandleFunction.cleanup, however it's never been called in WindowOperator in blink planner.",,andrew_lin,jark,leonard,libenchao,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 03:14:24 UTC 2020,,,,,,,,,,"0|z0f5yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/20 12:58;libenchao;CC +[~jark];;;","27/May/20 06:24;jark;Do you want to provide a fix [~libenchao]?;;;","27/May/20 08:42;libenchao;[~jark] Yes. I'd like to fix this. Actually, we need to fix for the regular aggregate count distinct too, do you think we need to open another issue or fix it in this issue?;;;","27/May/20 08:51;jark;Assigned to you [~libenchao]. Didn't we call the cleanup in regular aggregate?;;;","27/May/20 09:00;libenchao;[~jark] I've checked the code, it did call the cleanup already.;;;","28/May/20 03:14;jark;master (1.12.0): b6f92febd1f03152541432492820a5f2a12be6c4
1.11.0: 661541a1f002778df0691b05fe828a72870746dc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot run mvn clean verify flink-yarn-tests,FLINK-17938,13307368,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,26/May/20 09:01,16/Oct/20 10:56,13/Jul/23 08:07,27/May/20 09:03,1.11.0,,,,,1.11.0,,,,Deployment / YARN,Tests,,,,0,pull-request-available,,,,"As part of FLINK-11086, we introduced the setting of the yarn class path in a static initializer of {{YarnTestBase.java:199}}. The yarn class path file will be generated by the {{maven-dependency-plugin}} in the {{package}} phase. Due to this, the {{yarn.classpath}} file won't be accessible to all users of the {{YarnTestBase}} class which are run in a previous phase (e.g. {{UtilsTest.testUberjarLocator}}).",,guoyangze,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-11086,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 09:03:42 UTC 2020,,,,,,,,,,"0|z0f5lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 09:03;trohrmann;Fixed via

master: e1989bf3b598a565501f4d040ef9627aa34e3ded
1.11.0: 8b8ed695714d878fe69ca3dad8e7540b41c5d4e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Logs could not show up when deploying Flink on Yarn via ""--executor""",FLINK-17935,13307338,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kkl0u,wangyang0918,wangyang0918,26/May/20 05:43,05/Jun/20 07:56,13/Jul/23 08:07,04/Jun/20 14:37,1.11.0,1.12.0,,,,1.11.0,1.12.0,,,Deployment / YARN,,,,,0,pull-request-available,,,,"{code:java}
./bin/flink run -d -p 5 -e yarn-per-job examples/streaming/WindowJoin.jar{code}
When we use the {{-e/--executor}} to specify the deploy target to Yarn per-job, the logs could not show up. The root cause is we do not set the logging files in {{ExecutorCLI}}. We only do it in the {{FlinkYarnSessionCli}}.

If we use {{-m yarn-cluster}}, everything works well.

 

Maybe we should move the {{setLogConfigFileInConfig}} to {{YarnClusterDescriptor}} to avoid this problem. cc [~kkl0u]",,Echo Lee,kkl0u,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18149,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 14:37:37 UTC 2020,,,,,,,,,,"0|z0f5ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/20 12:29;Echo Lee;[~fly_in_gis] There is really a problem here. Our approach is to add {{setLogConfigFileInConfig}} to the custom executor.;;;","26/May/20 12:45;kkl0u;I already have a branch for this one. [~fly_in_gis] and [~Echo Lee] I think that the {{setLogConfigFileInConfig}} should go to the {{YarnClusterClientFactory.createClusterDescriptor()}} and remove it from anywhere else, so that we do not spread it everywhere in the codebase. ;;;","27/May/20 01:29;Echo Lee;[~kkl0u] You are right. Because of business needs, we have customized a yarn executor, so only a temporary solution. I look forward to your fix of this issue.;;;","27/May/20 02:09;wangyang0918;[~Echo Lee] Thanks for your comments. I am not fully understand why we could not move the {{setLogConfigFileInConfig}} to {{YarnClusterDescriptor}}. It is more straightforward and could be the only entry for setting logging configurations.;;;","27/May/20 07:56;kkl0u;My only concern with moving the configuration to the {{YarnClusterDescriptor}} itself is that this will tie it to an implicit contract (having to set an environment variable) which is hard to maintain. Also for its testability, it would be nice if we could just pass a complete configuration to the descriptor itself and not set fields inside there (as much as possible).;;;","27/May/20 08:37;wangyang0918;What i mean is not directly get the {{configDir}} from environment in {{YarnClusterDescriptor}}. Instead, we add a new argument {{String configDir}} to constructor of {{YarnClusterDescriptor}}.

 

I do not insist on this. Moving it to {{YarnClusterClientFactory}} also makes sense to me.;;;","27/May/20 09:29;Echo Lee;[~fly_in_gis] Yes, I agree with this unified solution. But first need to solve how flinkConfDir should be set.;;;","04/Jun/20 08:37;kkl0u;Fixed on master with 2eb7377163490878f12e4687db5fb4db0e6f47c2
and on release-1.11 with 32c65903b736f204bfe8b0f1d23c8ccddbd1b2f1;;;","04/Jun/20 14:37;kkl0u;Fixed on master with a674b5e181a2af050aee4c4f23a86a6c61eda94f
and on release-1.11 with d22874ef6322bfb774d60b09c022c3dbbe295d70;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingFileWriter should set chainingStrategy,FLINK-17934,13307334,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,26/May/20 03:57,16/Oct/20 10:57,13/Jul/23 08:07,27/May/20 08:29,,,,,,1.11.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,{{StreamingFileWriter}} should be eagerly chained whenever possible.,,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 08:29:25 UTC 2020,,,,,,,,,,"0|z0f5e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 08:29;lzljs3620320;master: cdfb0304c9e982795fa4c839559ca0283db9b424

release-1.11: 83c0eab3675814cbfb46670a4c4411351bfcd183;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix invalid liquid expressions,FLINK-17929,13307238,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,25/May/20 13:29,16/Oct/20 10:56,13/Jul/23 08:07,26/May/20 12:44,1.11.0,,,,,1.11.0,,,,Documentation,,,,,0,pull-request-available,,,,"The {{{{.ID}}}} expression in ops/deployment/docker.md should be escaped, otherwise it is not properly rendered.

{code}
      Generating... 
    Liquid Warning: Liquid syntax error (line 331): [:dot, "".""] is not a valid expression in ""{{.ID}}"" in ops/deployment/docker.md
    Liquid Warning: Liquid syntax error (line 357): [:dot, "".""] is not a valid expression in ""{{.ID}}"" in ops/deployment/docker.md
    Liquid Warning: Liquid syntax error (line 331): [:dot, "".""] is not a valid expression in ""{{.ID}}"" in ops/deployment/docker.zh.md
    Liquid Warning: Liquid syntax error (line 357): [:dot, "".""] is not a valid expression in ""{{.ID}}"" in ops/deployment/docker.zh.md
{code}",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 12:44:14 UTC 2020,,,,,,,,,,"0|z0f4so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/20 12:44;dwysakowicz;Fixed in:
master: 784bfadbbc37e246384227a42dd1d9e1f533e670
1.11: d4408edfe4fec1b9bbb587d95d665a8f08511306;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect state size reported when using unaligned checkpoints ,FLINK-17928,13307237,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,pnowojski,pnowojski,25/May/20 13:29,28/May/20 06:15,13/Jul/23 08:07,28/May/20 06:15,1.11.0,,,,,1.11.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"Even when checkpoints on HDFS are between 100-300MBs, the reported state size is in orders of magnitude larger with values like:
{noformat}
1GiB  1.5TiB  2.0TiB  2.1TiB  2.1TiB
148GiB  148GiB  148GiB  148GiB  148GiB  148GiB
{noformat}
it's probably because we have multiple {{Collection<InputChannelStateHandle>}}, and each of the individual handle returns the same value from {{AbstractChannelStateHandle#getStateSize}} - the full size of the spilled data, ignoring that only small portion of those data belong to a single input channel/result subpartition. In other words {{
org.apache.flink.runtime.state.AbstractChannelStateHandle#getStateSize}} should be taking the offsets into account and return only the size of the data that belong exclusively to this handle.",,pnowojski,roman,uce,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 06:15:22 UTC 2020,,,,,,,,,,"0|z0f4sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/20 13:31;roman;The numbers were obtained using:

 
{code:java}
writtenBytes() {
        rest_url=""http://`hostname`:20888/proxy/`get_aid`""
        job_id=`curl $rest_url/jobs/ | jq -r '.jobs[0].id'`
        vertex_ids=( `curl $rest_url/jobs/$job_id/ | jq -r '.vertices[].id'` )
        writtenBytes=0
        >&2 echo ""job_id $job_id; vertex_ids: ${vertex_ids[@]}""
        for vertex in ""${vertex_ids[@]}"";
        do
                stats=`curl $rest_url/jobs/$job_id/vertices/$vertex/subtasks/metrics\?get=writtenBytes_input,writtenBytes_output\&agg=sum`
                stats2=`curl $rest_url/jobs/$job_id/vertices/$vertex/subtasks/metrics\?get=writtenBytes\&agg=sum`
                >&2 echo ""vertex $vertex $stats $stats2""
                writtenBytes=$(($writtenBytes + `echo $stats $stats2 | jq -s '[add | .[] | select(.id | test(""writtenBytes.*"")).sum] | add // 0' `))
        done
        echo ""$writtenBytes"" | numfmt --to=iec-i --suffix=B --padding=7
}


transferredBytes() {
 rest_url=""http://`hostname`:20888/proxy/`get_aid`""
 job_id=`curl $rest_url/jobs/ | jq -r '.jobs[0].id'`
 vertex_ids=( `curl $rest_url/jobs/$job_id/ | jq -r '.vertices[].id'` )
 numBytesOut=0
 >&2 echo ""job_id $job_id; vertex_ids: ${vertex_ids[@]}""
 for vertex in ""${vertex_ids[@]}"";
 do
 stats=`curl $rest_url/jobs/$job_id/vertices/$vertex/subtasks/metrics\?get=numBytesOut\&agg=sum`
 >&2 echo ""vertex $vertex $stats""
 numBytesOut=$(($numBytesOut + `echo $stats | jq -s '[add | .[].sum] | add // 0' `))
 done
 echo ""$numBytesOut"" | numfmt --to=iec-i --suffix=B --padding=7
} 
{code}
 

This is how it's called:

 
{code:java}
function experiment() {
 name=$1
 nodes=$2
 slots=$3
 dur=$4
 start_job ${nodes} ${slots}
 duration ${dur}
# writtenBytes=""`writtenBytes` `writtenBytes` `writtenBytes` `writtenBytes` `writtenBytes` `writtenBytes` `writtenBytes`""
 transferredBytes=""`transferredBytes` `transferredBytes` `transferredBytes` `transferredBytes` `transferredBytes` `transferredBytes` `transferredBytes`""
 APPID=`kill_on_yarn`
getLogsFor ${name} ${APPID}
 analyzeLogs ${name} ${APPID}
 truncate -s -1 ${LOG}
 echo "";$transferredBytes"" >> ${LOG}
}
{code}
That is, those are transferred bytes, not written bytes (see commented line in experiment()).

 ;;;","26/May/20 17:57;pnowojski;Maybe, but the problem is still there:

{noformat}
[hadoop@ip-172-31-35-50 logs]$ zcat master17-hdfs-unaligned-throughput2-application_1589453804748_0116.gz | grep INFO | grep -i complet
2020-05-17 09:33:08,854 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 1 for job 7f2de1d00cc862534746985f4c021098 (25766077793 bytes in 38316 ms).
2020-05-17 09:34:23,546 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 2 for job 7f2de1d00cc862534746985f4c021098 (42803408024 bytes in 35183 ms).
2020-05-17 09:35:33,974 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 3 for job 7f2de1d00cc862534746985f4c021098 (42037807324 bytes in 33832 ms).
2020-05-17 09:36:47,447 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 4 for job 7f2de1d00cc862534746985f4c021098 (38890649497 bytes in 34755 ms).
2020-05-17 09:38:00,182 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 5 for job 7f2de1d00cc862534746985f4c021098 (41471661012 bytes in 34806 ms).
2020-05-17 09:39:09,803 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 6 for job 7f2de1d00cc862534746985f4c021098 (40503930228 bytes in 34256 ms).
2020-05-17 09:40:23,162 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 7 for job 7f2de1d00cc862534746985f4c021098 (41961537883 bytes in 35310 ms).
2020-05-17 09:41:35,483 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 8 for job 7f2de1d00cc862534746985f4c021098 (41942565096 bytes in 35262 ms).
{noformat}
It's reported as tens of GBs, while single checkpoint is ~hundreds of MBs:

{noformat}
[hadoop@ip-172-31-35-50 logs]$ hdfs dfs -du -h unaligned/20192aeea3043ae6211c786744837988
175.8 M  unaligned/20192aeea3043ae6211c786744837988/chk-6
183.4 M  unaligned/20192aeea3043ae6211c786744837988/chk-7
{noformat}
;;;","28/May/20 06:15;pnowojski;merged to release-1.11 as a9782a2483
merged to master as f89c606f12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't build flink-web docker image because of EOL of Ubuntu:18.10,FLINK-17926,13307229,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,klion26,klion26,klion26,25/May/20 12:30,04/Jun/20 07:47,13/Jul/23 08:07,04/Jun/20 07:23,,,,,,,,,,Project Website,,,,,0,pull-request-available,,,,"Currently, the Dockerfile[1] in flink-web project is broken because of the EOL of Ubuntu 18.10[2], will encounter the error such as bellow when executing {{./run.sh}}
{code:java}
Err:3 http://security.ubuntu.com/ubuntu cosmic-security Release
  404  Not Found [IP: 91.189.88.152 80]
Ign:4 http://archive.ubuntu.com/ubuntu cosmic-updates InRelease
Ign:5 http://archive.ubuntu.com/ubuntu cosmic-backports InRelease
Err:6 http://archive.ubuntu.com/ubuntu cosmic Release
  404  Not Found [IP: 91.189.88.142 80]
Err:7 http://archive.ubuntu.com/ubuntu cosmic-updates Release
  404  Not Found [IP: 91.189.88.142 80]
Err:8 http://archive.ubuntu.com/ubuntu cosmic-backports Release
  404  Not Found [IP: 91.189.88.142 80]
Reading package lists...
{code}
The current LTS versions can be found in release website[2].

Apache Flink docker image uses fedora:28[3], so it unaffected.

As fedora does not have LTS release[4], I proposal to use Ubuntu for website here, and change the version from {{18.10}} to the closest LTS version {{18.04, tried locally, it works successfully.}}

 [1] [https://github.com/apache/flink-web/blob/bc66f0f0f463ab62a22e81df7d7efd301b76a6b4/docker/Dockerfile#L17]

[2] [https://wiki.ubuntu.com/Releases]

 [3]https://github.com/apache/flink/blob/e92b2bf19bdf03ad3bae906dc5fa3781aeddb3ee/docs/docker/Dockerfile#L17

 [4] https://fedoraproject.org/wiki/Fedora_Release_Life_Cycle#Maintenance_Schedule",,klion26,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 07:23:30 UTC 2020,,,,,,,,,,"0|z0f4qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/20 12:36;klion26;[~rmetzger] what do you think about this, please assign it to me if you think this is valid and the proposal is ok.;;;","25/May/20 12:54;rmetzger;Thanks, this is a good proposal. I assigned you.;;;","04/Jun/20 07:23;rmetzger;Resolved in https://github.com/apache/flink-web/commit/815eaadaafcc8747678f7ea8e5f4959d0b394d8c

Thanks [~klion26]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Filesystem options to default values and types,FLINK-17925,13307224,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,25/May/20 12:07,16/Oct/20 10:57,13/Jul/23 08:07,27/May/20 11:22,,,,,,1.11.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,"Fix Filesystem options:
 * Throws unsupported exception when using metastore commit policy for filesystem table, Filesystem connector has an empty implementation in {{TableMetaStoreFactory}}. We should avoid user configuring this policy.
 * Default value of ""sink.partition-commit.trigger"" should be ""process-time"". Users are hard to figure out what is wrong when they don't have watermark. We can set ""sink.partition-commit.trigger"" to ""process-time"" to have better out-of-box experience.
 * The type of ""sink.rolling-policy.file-size"" should be MemoryType.",,jark,lzljs3620320,swhelan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17927,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 11:22:48 UTC 2020,,,,,,,,,,"0|z0f4pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/20 01:52;jark;Should it be ""processing-time""? ;;;","26/May/20 03:40;lzljs3620320;I am OK with ""proctime"", ""processing-time"" and ""process-time"". Corresponding ""partition-time"", I choose ""process-time"".;;;","27/May/20 04:42;jark;I prefer ""processing-time"" because it is the concept glossary in Flink, {{TimeCharacteristic#ProcessingTime}}.;;;","27/May/20 05:04;lzljs3620320;I have no obvious tendency, but I think ""process-time"" can be understood well.;;;","27/May/20 11:22;lzljs3620320;release-1.11: f667804a969e27c61e8a44bc8c4832aa2d2e8398

master: a3c5455eed19589dc9918e23b0d3b0d1fcbe518a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
It will throw MemoryAllocationException if rocksdb statebackend and Python UDF are used in the same slot  ,FLINK-17923,13307194,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dian.fu,dian.fu,dian.fu,25/May/20 09:33,28/Jul/20 07:56,13/Jul/23 08:07,03/Jun/20 01:56,1.10.0,1.11.0,,,,1.11.0,,,,API / Python,Runtime / State Backends,,,,0,pull-request-available,,,,"For the following job:
{code}
import logging
import os
import shutil
import sys
import tempfile

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import TableConfig, StreamTableEnvironment, DataTypes
from pyflink.table.udf import udf


def word_count():
    content = ""line Licensed to the Apache Software Foundation ASF under one "" \
              ""line or more contributor license agreements See the NOTICE file "" \
              ""line distributed with this work for additional information "" \
              ""line regarding copyright ownership The ASF licenses this file "" \
              ""to you under the Apache License Version the "" \
              ""License you may not use this file except in compliance "" \
              ""with the License""

    t_config = TableConfig()
    env = StreamExecutionEnvironment.get_execution_environment()
    t_env = StreamTableEnvironment.create(env, t_config)

    # register Results table in table environment
    tmp_dir = tempfile.gettempdir()
    result_path = tmp_dir + '/result'
    if os.path.exists(result_path):
        try:
            if os.path.isfile(result_path):
                os.remove(result_path)
            else:
                shutil.rmtree(result_path)
        except OSError as e:
            logging.error(""Error removing directory: %s - %s."", e.filename, e.strerror)

    logging.info(""Results directory: %s"", result_path)

    sink_ddl = """"""
        create table Results(
            word VARCHAR,
            `count` BIGINT
        ) with (
            'connector' = 'blackhole'
        )
        """"""
    t_env.sql_update(sink_ddl)

    @udf(input_types=[DataTypes.BIGINT()], result_type=DataTypes.BIGINT())
    def inc(count):
        return count + 1
    t_env.register_function(""inc"", inc)

    elements = [(word, 1) for word in content.split("" "")]
    t_env.from_elements(elements, [""word"", ""count""]) \
         .group_by(""word"") \
         .select(""word, count(1) as count"") \
         .select(""word, inc(count) as count"") \
         .insert_into(""Results"")

    t_env.execute(""word_count"")


if __name__ == '__main__':
    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=""%(message)s"")

    word_count()
{code}

It will throw the following exception if rocksdb state backend is used:
{code}
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for KeyedProcessOperator_c27dcf7b54ef6bfd6cff02ca8870b681_(1/1) from any of the 1 provided restore options.
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:317)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:144)
	... 9 more
Caused by: java.io.IOException: Failed to acquire shared cache resource for RocksDB
	at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.allocateSharedCachesIfConfigured(RocksDBOperationUtils.java:212)
	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:516)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:301)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)
	... 11 more
Caused by: org.apache.flink.runtime.memory.MemoryAllocationException: Could not created the shared memory resource of size 536870920. Not enough memory left to reserve from the slot's managed memory.
	at org.apache.flink.runtime.memory.MemoryManager.lambda$getSharedMemoryResourceForManagedMemory$8(MemoryManager.java:603)
	at org.apache.flink.runtime.memory.SharedResources.createResource(SharedResources.java:130)
	at org.apache.flink.runtime.memory.SharedResources.getOrAllocateSharedResource(SharedResources.java:72)
	at org.apache.flink.runtime.memory.MemoryManager.getSharedMemoryResourceForManagedMemory(MemoryManager.java:617)
	at org.apache.flink.runtime.memory.MemoryManager.getSharedMemoryResourceForManagedMemory(MemoryManager.java:566)
	at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.allocateSharedCachesIfConfigured(RocksDBOperationUtils.java:208)
	... 15 more
Caused by: org.apache.flink.runtime.memory.MemoryReservationException: Could not allocate 536870920 bytes. Only 454033416 bytes are remaining.
	at org.apache.flink.runtime.memory.MemoryManager.reserveMemory(MemoryManager.java:461)
	at org.apache.flink.runtime.memory.MemoryManager.lambda$getSharedMemoryResourceForManagedMemory$8(MemoryManager.java:601)
	... 20 more
{code}",,dian.fu,felixzheng,guoyangze,hequn8128,liyu,pnowojski,sewen,sunjincheng121,trohrmann,wanglijie,xtsong,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18738,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 01:55:57 UTC 2020,,,,,,,,,,"0|z0dtzw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/20 07:08;xtsong;The problem is that RocksDB assumes it is the only managed memory consumer in streaming scenarios, and tries to take all the managed memory in the slot, while actually Python UDF might also reserve managed memory in streaming scenarios.

h3. More backgrounds
With FLIP-53, when generating the stream graph, we make a plan on how managed memory should be shared by operators within a slot.
* For batch jobs, we calculate a fraction for each operator, representing what fraction of the slot's managed memory the operator should use. Operators will read this fraction in runtime, and reserve the corresponding memory from the memory manager.
* For streaming jobs, we assumed the only managed memory consumer is RocksDBStateBackend. Therefore, calculation of the fraction (when generating stream graph) is omitted. RocksDBStateBackend will always reserve all (fraction = 1) the managed memory from the memory manager. 

h3. Potential solutions
There was an offline discussion between [~dian.fu], [~zhuzh], [~yunta] and me. And here are some ideas we came up with.
# We can say that ATM we do not support Python UDF and RocksDBStateBackend work together. We can add a check at compiling time and throw an exception / warning if they are used together. Given that release 1.11 is already frozen, this could avoid rushing significant changes in the last minute. The drawback is obviously we loose a large portion of steaming Python UDF use cases for release 1.11.
# We can make Python UDF not reserve managed memory. Basically, Python UDF uses memory in the same way how other user codes use off-heap memory. Users need to explicitly configure larger task off-heap memory. The drawbacks for this solutions are 1) it requires more user involvement, 2) it breaks the current 1.10 behavior in batch scenarios where such user involvement was not needed, and 3) we might need to revert the changes in future.
# We can also calculate a fraction for RocksDBStateBackend, making it properly share managed memory with Python UDFs. This is probably the most proper solution. The problem is there are still some open questions, such as how to calculate the fraction (because RocksDBStateBackend allocates managed memory in a per-slot way rather than per-operator), and how to pass the fraction to the state backend. We are not sure whether this is doable in the 1.11 release cycle, given that it's already frozen.

We would like to hear more opinions from the community. Many Thanks.;;;","26/May/20 07:12;xtsong;cc [~jincheng] [~liyu] [~sewen] [~trohrmann];;;","26/May/20 10:40;sunjincheng121;It's a pity that we do not find this issue earlier(We also need to improve the e2e test for PyFlink after fixing this issue). This is a very critical problem for PyFlink as it means that Python UDF could not be used in most streaming jobs(with state).So I think we should address this problem in 1.11. 

We( [~zhuzh] [~xintongsong] [~yunta] [~dian.fu] and me) have a further discussion about this problem and will update the status later.

Appreciate if you can pay attention to this [~pnowojski]  and [~zjwang] .  ;;;","26/May/20 13:49;pnowojski;[~xintongsong] it sounds to me like option 2. is not a proper temporary hotfix. It might be problematic to rush currently with a proper solution, like option 3. (or some other proposal?). 

I see it affects also 1.10, so it is not a new issue, right? In that case, I don't think it should be a release blocker for 1.11. We could fix it for 1.12 and if it's important and fix won't be invasive and causing other side effects, we could also back port it to 1.11.1. ;;;","26/May/20 14:17;sunjincheng121;At present, users can't start jobs as long as they use rocksDB + Python UDF. The core scenario of our Flink is stream computing. In stream computing, as long as it's an analytical application, it needs to use AGG. In this case, if it's a Python User, the demand for Python UDF is our core function of 1.10/1.11. At present, we have china users waiting to use this feature.

We discussed the details of using option 3 today. Later [~zhuzh] will share the design document with you. We can discuss the design first and evaluate whether put this fixing to 1.11 is reasonable.;;;","27/May/20 02:52;xtsong;[~pnowojski],

I posted all the 3 options that were mentioned in our offline discussion, for public visibility.

Personally, I'm not a fan of option 2), because I also think it is an improper temporary fix.

For option 3), as Jincheng mentioned, Zhu is already preparing a design doc (ETA today). I understand that having people evaluating the design is already a distraction from solving the release blockers. However, in this case I think it would be ok to at least take a look at the design proposal before deciding whether this should be fixed in 1.11.0, for its importance for python use cases and the fact that people mostly involved with the proposed change (Jincheng & Zhu) do not have much workload related to the release testing (that's what I see from the current burndown board).

Additionally, if this fix does not make into 1.11.0 at the end, I would suggest a really quick release 1.11.1 (potentially right after fixing this issue). IIUC, the fix of this issue ideally should not affect any other scenarios except for running both RocksDB & Python UDF together (which currently doesn't work).;;;","27/May/20 08:35;zhuzh;We([~yunta]) just find that we can also have option #4 that users set ""state.backend.rocksdb.memory.fixed-per-slot"" for python+RocksDB jobs to let RocksDB instances use off-heap memory instead of managed memory. There would be no managed memory reservation exception since only python operators will reserve managed memory.;;;","27/May/20 10:00;xtsong;Thanks [~zhuzh] & [~yunta].

The option 4) sounds a good workaround for release 1.11.0. IIUC, it does not require code modifications, just the users have to apply some specific configurations in order to use rocksdb and python udf at the same time. I think that would be fair enough. We can mention this as limitations / known issues in the release notes, and take our time to properly fix it in the next release.;;;","27/May/20 10:27;dian.fu;Based on option #4, we can also have option #5 to add a switch to allow Python UDF to use off-heap memory instead of managed memory. The memory needed by the Python process is fixed according to our tests and so it will be easy to set a meaningful default value for Python jobs which could work in most scenarios. Regarding to option #4, as the memory needed by RocksDB has relationship with the memory size of the TM (please correct me if my understanding isn't correct) and so it will be difficult to set a meaningful default value for Python jobs. ;;;","27/May/20 10:37;sunjincheng121;Our consensus is that The final solution is both Python and RocksDB should be managed by Resource Management(using managed memory) . I think both #4 and #5 works for PyFlink, I prefer #5 due to it's much flexible, if we cannot have #3 in 1.11.0 release.;;;","28/May/20 05:43;dian.fu;[~sunjincheng121] [~xintongsong] [~zhuzh] I have submitted a PR according to option #5. Appreciated if you could take a look.;;;","28/May/20 06:02;sunjincheng121;Thanks for the PR Dian Fu.  Overall, It's looks good. If others also agree to this solution for 1.11, we can merge it as soon as possible and also CP to 1.10 branch. What to you think [~xintongsong] [~zhuzh] [~pnowojski] [~zjwang];;;","28/May/20 08:47;xtsong;[~dian.fu], [~sunjincheng121]

I'm not familiar with the flink-python code base, thus I cannot speak much to the PR.

My only concern is regarding auto-magically setting task.off-heap.size for users. I wonder whether we are trying to be a bit over-smart. It might save some user efforts in many cases, but could also make things hard to understand in other cases. It is one of the main motivations for FLIP-49 to make sure all the memory calculations happen at one place, without such kind of implicit logics.

I would suggest not to override the task.off-heap.size configuration. Instead, we can suggest how to set this configuration in both memory configuration and python udf docs. This is similar to RocksDBStateBackend, when managed memory is disabled users need to explicitly make sure enough native memory is reserved for RocksDB.

WDYT?;;;","28/May/20 09:16;dian.fu;[~xintongsong] Appreciated your suggestions. It makes sense to me.
I want to adjust the PR a bit as following:
- Set Python UDF to use managed memory by default.
- If Python UDF and RocksDB is used together and both Python UDF and RocksDB are configured to use managed memory, throw exceptions with meaningful suggestions.
- If Python UDF is configured to use off-heap memory and the task off-heap memory could not meet the requirement, throw exceptions with meaningful suggestions.

In this case, when we support to let Python UDF and RocksDB both use managed memory in the future, we could just remove the checks and there will be no potential backward compatibility issues.

What do you think?;;;","28/May/20 09:21;sewen;Thank you for the active discussion.

One question for clarification: Does Python have one process per TaskManager (shared by all tasks) or one process per slot (shared by all operators) or one process per operator?
;;;","28/May/20 09:23;dian.fu;Currently, there will be one process per operator. It may support to share the Python process among multiple operators in the future.;;;","28/May/20 09:28;xtsong;Thanks [~dian.fu], that sounds good to me.;;;","28/May/20 12:41;sewen;I think that before resolving this in a way that changes RocksDBs memory management, we need to first clarify what out future model for the Python Processes is.

I am somewhat skeptical that the current approach where the Python process takes managed memory by default is the right one.

For example, the Beam/Flink users really like to deploy the Python interpreter in separate containers. For streaming scenarios that is the most robust setup, I think. In that case, the Flink TaskManager cannot dedicate its managed memory to the Python process.

The current model seems also somewhat unpredictable when it comes to performance. If you have many slots and operators, each process's memory budget gets very small. It may eventually start swapping even.
;;;","29/May/20 02:23;sunjincheng121;We can have another way to manage the resource for docker and external mode, but for now TM should manage the resource as we only support the Python worker to run in process mode. So, we can fix this issue by #5. And I think you are right that may be we should set the default value of managed should be set as false if we prefer to use docker or external mode for Python worker in the future.;;;","29/May/20 02:24;xtsong;[~sewen],

-I agree with you that python processes use managed memory by default should be the right approach for long term.-

-I think ideally we want RocksDB and Python processes share the managed memory, wrt the planned fractions. This is the option #3 that we discussed. For this approach, the changes required are mainly on the runtime & rocksdb side, i.e., calculating fractions for operators with rocksdb states and reserve managed memory wrt the fractions, while no changes are needed on the python side. The concern for this approach is that, the required changes might be too significant for the release testing period.-

-A feasible workaround is to make either RocksDB or Python not using managed memory. The workaround is *only needed when RocksDB & python are used together.*-
 * -For RocksDB, there's already a configuration option allowing user to enable/disable reserving managed memory. We only need to tell users to disable this switch when working together with Python and no code changes are needed. This is the option option #4 we discussed.-
 * -For Python, I think the option #5 that Dian & Jincheng suggested is to introduce a similar switch for Python that allows users to enable/disable reserving managed memory. By default, Python still uses managed memory. The benefit for introducing this switch is to allow users to choose between RocksDB and Python for which managed memory is disabled.-

-One problem I see in option #5 is that, the default configuration does not work when RocksDB and Python UDF are used together. In that case, a meaningful error message is provided and users have to manually modify the configurations. But I think this is the right thing to do, that we admit there's a problem and provide a workaround to users, rather than trying to ""fix"" it in a way that may also affect other unproblematic use cases.-

Nevermind. Seems I haven't understand your point correctly.;;;","29/May/20 09:48;dian.fu;[~sewen] [~sunjincheng121] Do you think it makes sense to fix this issue in 1.11 by letting the Python worker to not use managed memory by default? This allows us to run Python worker in separate containers which are not managed by TM in the future. We could further discuss whether to change the memory management to allow both Python worker and RocksDB to use managed memory in the next release.;;;","29/May/20 09:52;sunjincheng121;+1 from my points of view. ;;;","29/May/20 19:21;sewen;We could think about making the Python Processes the responsibility of the deployment framework.

  - The Kubernetes resource manager would deploy pods with multiple containers (one TM and multiple Python processes, one per slot for example).
  - The Yarn resource manager would ask for larger resource containers, to accommodate the additional memory that the python processes require.
  - In standalone, the machine simply needs to have enough memory, the same way as when starting a standalone session. It is the user's responsibility when they set up Flink.

In that case the managed memory would all go to RocksDB or to the batch algorithms.

As hinted above, this probably needs a change that there is one Python process per TaskManager, or at least one Python process per slot. Then the deployment/resource manager can reason about this well. I am not sure what the implications of that change are for the Python language layer.;;;","29/May/20 19:23;sewen;I think my suggestion from above is not a feasible for for 1.11.

[~dian.fu] changing Python to not use managed memory - would it still work on Yarn then? Or would we expect that the TM gets killed very often, for using too much memory?
;;;","29/May/20 19:38;dian.fu;We assume that the Python worker uses task off-heap memory in this case and so it will still work on YARN.;;;","01/Jun/20 02:50;xtsong;[~sewen],

I'm still trying to understand, what is the benefit of making Python processes the responsibility of the deployment framework.

I'm not saying this is not the right approach. I'm asking because, I have several concerns on this approach. These concerns are probably resolvable, in one way or another. But first I would like to understand whether what we gain worth the efforts.

*1. The resources reserved for Python UDFs (python process containers on K8s, and the additional memory on Yarn) might be wasted in use cases without Python UDFs.* I think one of the reasons we make RocksDB uses managed memory in FLIP-49 is that, we want the default configuration works for all use cases while not leaving part of the memory unused. As for Python, if we reserve resources by default, these resources will be wasted in non-python use cases. If we don't reserve resources by default, then the default configuration does not work for python use cases.

*2. Which component is responsible for managing lifecycles for Python Processes?* Do we consider the python processes as part of the Flink framework? If so, the lifecycle of python processes should be decoupled from a job's lifecycle. If the user code does something wrong that makes the python process fail, Flink should be able to bring it back up. This could be achieved naturally on Kubernetes, but not on Yarn. On Yarn, once a container is started the YarnResourceManager can no longer start another process on it. We probably need to start another service in YarnTaskExecutorRunner to start/monitor/recover/stop the python processes. That sounds similarly to just have the TaskManager managing the Python processes.;;;","02/Jun/20 13:25;sewen;[~xintongsong] Just for clarification: I am not saying that the Resource Manager is always responsible for launching the Python Process, but that the resources going to the Python process are planned by the ResourceManager, and not implictly reused from the TaskManager.

But there is definitely an open question in how to handle the Python Process in the first place.
If we want to support the containerized Python processes on K8s, we need the resource manager to be aware of Python and change the deployment specification. The TM then does not need to do anything (just connect to an existing process). On Yarn, on the other hand, things are different. The RM only needs to increase the task offheap memory, the TM needs to launch the process.;;;","02/Jun/20 13:26;sewen;[~dian.fu] Using Task off-heap memory is probably a good workaround for now.;;;","03/Jun/20 01:55;dian.fu;[~sewen] Thanks for the confirmation. 

Merged to master via 77e5494c1c252ba2dd458078380ee862fa423e4e and release-1.11 via c366ea690f75d809f7c7129c5341ca5b8062e6fc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LIMIT queries are failed when adding sleeping time of async checkpoint,FLINK-17918,13307160,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,pnowojski,pnowojski,25/May/20 07:31,22/Jun/21 13:55,13/Jul/23 08:07,04/Jun/20 10:30,1.11.0,,,,,1.11.0,,,,Runtime / Checkpointing,Table SQL / Runtime,,,,0,pull-request-available,,,,"When we change the timing of operations (sleep after emit first record and sleep for async operation of checkpoint) with this [commit|https://github.com/apache/flink/commit/c05a0d865989c9959047cebcf2cd68b3838cc699], the test {{org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase#testDifferentTypesSumWithRetract}} in flink-table-planner-blink is failed. ",,AHeise,godfreyhe,gyfora,jark,kevin.cyj,klion26,leonard,libenchao,lincoln.86xy,liyu,openinx,pnowojski,uce,wind_ljy,ym,yunta,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18060,FLINK-18118,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 10:30:28 UTC 2020,,,,,,,,,,"0|z0f4bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/20 08:11;gyfora;Should this be a blocker for 1.11?;;;","25/May/20 10:43;pnowojski;We are still evaluating the issue (it's extremely difficult to debug this issue, as neither me nor [~zjwang] were able to reproduce it locally after couple days of trying), but  it looks like it's only limited to two input operators - if that's the case, we can always support unaligned checkpoints only for one input operators in 1.11. That's why I created it as CRITICAL.

But I guess either way, this indeed should be a blocker [~gyfora]. If we decide to limit support to one input operators, we still need to at least verify it this upon job submission.;;;","01/Jun/20 09:11;pnowojski;Some status update. [~AHeise] has added quite a big of debug messages, focusing on:

{{org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase#testDifferentTypesSumWithRetract}}

This test is fairly reliably failing locally with unaligned checkpoints enabled by default after 2-15 minutes, when running the following command (
Sometimes you might need to bump fork counts or number of parallel iterations (for loop)):
{noformat}
for i in {0..7}; do mvn -Dlog4j.configurationFile=/Users/arv/workspace/flink/flink-table/flink-table-planner-blink/src/test/resources/log4j2-test.properties -Dflink.forkCount=1 -Dflink.forkCountTestPackage=1 -Dfast -Dflink.tests.with-openssl -Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11 -pl flink-table/flink-table-planner-blink integration-test -Dtest=org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase$testDifferentTypesSumWithRetract &>test_$i.txt & done
{noformat}
After adding a lot of debug messages [~AHeise] pinpointed a problem to the following scenario. Somehow during async part of checkpointing, blink operator (in the case that we are debugging it’s {{AppendOnlyTopNFunction}} implementation of {{LIMIT}} operation) is checkpointing too many records.

# Upstream source task is sending: {{[record1, chk1, record2, record3, record4]}}. Downstream {{LIMIT}} is receiving them in this order
# only record1 is processed 
# sync part of checkpoint chk1 completes
# AFTER that {{LIMIT}} is processing {{record2, record3, record4}}...
# .. but somehow async operation of checkpoint is ending up writing a state as if all of the records were processed. Including {{record2, record3, record4}}, which shouldn’t be part of the checkpoint 1 for {{LIMIT}} operator’s state.
# failure happens
# {{LIMIT}} operator is restoring with 4 records on the state but ...
# source is correctly re-emitting the records {{record2, record3, record4}}
# This means LIMIT operator is reaching it’s limit 3 records sooner, and last 3 expected records are lost.

The same failure can be induced with *aligned checkpoints* by altering a timing of the operations ([~AHeise]? adding a 100ms sleep after emitting first record and adding 100ms sleep in async part of the checkpointing operation). Currently on the master this issue is not visible because (quoting [~AHeise])
{quote}
FailingDataSource is currently only failing at source if half elements are emitted AND a subsequent checkpoint happened. So with aligned the issue is never visible.
{quote}
;;;","01/Jun/20 09:53;ykt836;cc [~jark] 

 ;;;","01/Jun/20 09:55;ykt836;From above comment, it seems that the problem is caused by *step 5* (the checkpoint&state is inconsistent with stream operators)?;;;","01/Jun/20 12:10;arvid;Yes [~ykt836] . I'm suspecting that during [updating the state of Limit|https://github.com/apache/flink/blob/master/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/AppendOnlyTopNFunction.java#L116], we add a mutable list as the value, such that after the state is being snapshotted, the value is still mutable.

Thus, when the value is actually being serialized in the background, there may have been updates to the list by task thread.

I'm assuming a conservative fix is to add a copy of the list as the value. Alternatively, I guess we could also use a keyed list state, but I'm not very knowledgable about states.;;;","01/Jun/20 12:16;arvid;Btw see this [commit|https://github.com/apache/flink/commit/c05a0d865989c9959047cebcf2cd68b3838cc699] for changes in the test setup, which reliably reproduces the failure on my end. These values were especially tweaked for org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase#testDifferentTypesSumWithRetract in flink-table-planner-blink.;;;","01/Jun/20 13:55;pnowojski;I think [~AHeise] is right. We are mutating the list that's used in the state field {{AppendOnlyTopNFunction#dataState}} [here in the line 113|https://github.com/apache/flink/blob/master/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/AppendOnlyTopNFunction.java#L113]

Someone could correct me if I'm wrong, I do not know the guarantees of our state backends very well, but via looking at the relevant code there is an assumption, that sync part of checkpoint should do all of the defensive copies/protections from making sure of ""freezeing"" state snapshot for later IO operations. This seems to be confirmed by the java doc above {{org.apache.flink.runtime.state.heap.CopyOnWriteStateMap#stateSnapshot}} (it's being invoked in sync checkpoint part for the {{AppendOnlyTopNFunction#dataState}} field):
{code:java}
	/**
	 * Creates a snapshot of this {@link CopyOnWriteStateMap}, to be written in checkpointing. The snapshot integrity
	 * is protected through copy-on-write from the {@link CopyOnWriteStateMap}. Users should call
	 * {@link #releaseSnapshot(StateMapSnapshot)} after using the returned object.
	 *
	 * @return a snapshot from this {@link CopyOnWriteStateMap}, for checkpointing.
	 */
{code}
but in the {{org.apache.flink.runtime.state.heap.CopyOnWriteStateMap#snapshotMapArrays}} we are only marking whole {{StateMapEntry}} (the type of {{AppendOnlyTopNFunction#dataState}} is  {{MapState<RowData, List<RowData>>}}. Nothing more. So the defensive copy-on-write works only as long, as someone else is not updating the referenced structures (like the {{List<RowData>}} in the background.;;;","02/Jun/20 03:23;jark;Thanks [~pnowojski] and [~AHeise] for the investigating. After discussing with [~yunta], [~yunta] pointed that user should take care of deep copying when using heap statebackend, the Javadoc of CopyOnWriteStateMap says:

{quote}
IMPORTANT: the contracts for this class rely on the user not holding any references to objects returned by this map
beyond the life cycle of per-element operations. Or phrased differently, all get-update-put operations on a mapping
should be within one call of processElement. Otherwise, the user must take care of taking deep copies, e.g. for
caching purposes.
{quote}

However, I didn't find the note on the documentation. If this is true, I think we should update documentation and update the {{AppendOnlyTopNFunction}} to put the copied list into {{dataState}} (and this will bring additional cost for RocksDB statebackend). We should also go through all the other operators.

Besides of this, I can also reproduce the failure of {{AggregateITCase#testDifferentTypesSumWithRetract}} with [~AHeise]'s commit. But I think this maybe another issue, because the simplest case ""[LocalGlobal=OFF, MiniBatch=OFF, StateBackend=HEAP]"" which uses {{GroupAggFunction}} will also fail. In the {{GroupAggFunction}}, all the {{accumulators}} of [{{accState.update(accumulators)}}|https://github.com/apache/flink/blob/4ff59a72b4c8532510cca349840fcbe668de911e/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/GroupAggFunction.java#L179] is always a new object.
;;;","02/Jun/20 03:37;liyu;Thanks [~jark], I was about to add the same comments, that the observed behavior of `CopyOnWriteStateMap` is by design.

Actually this condition (don't hold the reference and further modifying the object beyond `processElement`) applies to all kinds of `StateTable` for `HeapKeyedStateBackend`, and is the source of performance gain (data kept on heap in object format w/o serialization/deserialization or any similar op like deep copy) for heap backend. And a big +1 to explicitly write this down in our document.;;;","02/Jun/20 06:12;pnowojski;[~jark]
{quote}
But I think this maybe another issue, because the simplest case ""[LocalGlobal=OFF, MiniBatch=OFF, StateBackend=HEAP]"" which uses GroupAggFunction will also fail. 
{quote}
Why do you think this combination should work fine? If the problem is with {{AppendOnlyTopNFunction}} introduced by:
{noformat}
SELECT *, CAST(c AS DECIMAL(3, 2)) AS h FROM T LIMIT 8
{noformat}
then it should happen always, right?;;;","02/Jun/20 06:25;arvid;Yes, the root cause is {{LIMIT}} swallowing up some tail records because it saw the first records two times effectively. Since LIMIT is used in many tests (I guess for correctly working retract streams, but I haven't understood it entirely), many tests are currently prone to fail.

On a side-note, I'd keep the change in my commit on the {{FailingDataSource}} that fails after at least one element was checkpointed (ofc remove all timing related changes). The current way effectively drains all records before triggering the crash, which will hide quite a lot of failures like this one.;;;","02/Jun/20 07:34;yunta;As far as I know, most users would choose RocksDB as state backend on production environment and that might explain why even we do not talk about the limit in documentation, it seems no user has ever reported similar problem. 
I have created related issue FLINK-18060 to add notice about this limitation.;;;","02/Jun/20 08:19;jark;[~pnowojski] Oh, sorry I didn't notice the inner LIMIT in the query (tests are migrated from blink code). Then I think you are right, the root cause is in {{AppendOnlyTopNFunction}}. I can help to prepare a pull request for this. 

But I still can't understand why the join IT cases are failed, the join cases doesn't contain LIMIT and the join operators doesn't mutate values in state. ;;;","02/Jun/20 08:32;arvid;I fear that these errors are independent, but could be related to recovery as well.

But we also have issues with watermarks in unaligned checkpoints (not sure if they are used in the failing join tests).;;;","02/Jun/20 12:04;jark;No, all the failed join tests do not use watermarks. ;;;","03/Jun/20 13:11;pnowojski;I think [~jark] you are right, we have also checked the remaining tests with [~AHeise] yesterday and it seems there is at least one more bug. Probably on our side that indeed has something to do with the two input operators. Anyway, let's split the issues and use this ticket for solving the `LIMIT`, we can create new tickets for the remaining issues.;;;","03/Jun/20 16:32;arvid;The ticket originally was ""[FLINK-17918] Jobs with two input operators are loosing data with unaligned checkpoints"" and indeed the test failures in the description are caused in unaligned checkpoint code.
Discovering the LIMIT bug was more or less just a byproduct apparently.
Since there is already a PR for LIMIT under FLINK-17918, I guess we should fork out a new ticket and update the description of this ticket to only mention LIMIT.;;;","04/Jun/20 07:44;jark;FYI, I have updated the title and description of this issue and forked out a new issue FLINK-18118 for the remaining problems.;;;","04/Jun/20 10:30;jark;[hotfix][table-planner-blink] Store last watermark in state to emit on recovery for EventTimeProcessOperator
 - master (1.12.0): 550d4e168bdf068b2c4eadd51878abaa98f0bb3c
 - 1.11.0: 7c72335a3dbf2a8de7469d2bef8074d1e33284c9

[FLINK-17918][table-blink] Fix AppendOnlyTopNFunction shouldn't mutate list value of MapState
 - master (1.12.0): 5ab403dafa3bc5ee8654173fb543969dfdd746f8
 - 1.11.0: a290c8a3af4083fb32febfc7f955a9bdce08e4b8
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResourceInformationReflector#getExternalResources should ignore the external resource with a value of 0,FLINK-17917,13307146,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,guoyangze,guoyangze,guoyangze,25/May/20 06:10,16/Oct/20 10:55,13/Jul/23 08:07,26/May/20 10:45,1.11.0,,,,,1.11.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"*Background*: In FLINK-17390, we leverage {{WorkerSpecContainerResourceAdapter.InternalContainerResource}} to handle container matching logic. In FLINK-17407, we introduce external resources in {{WorkerSpecContainerResourceAdapter.InternalContainerResource}}.
 On containers returned by Yarn, we try to get the corresponding worker specs by:
 - Convert the container to {{InternalContainerResource}}
 - Get the WorkerResourceSpec from {{containerResourceToWorkerSpecs}} map.

*Problem*: Container mismatch could happen in the below scenario:
 - Flink does not allocate any external resources, the {{externalResources}} of {{InternalContainerResource}} is an empty map.
 - The returned container contains all the resources (with a value of 0) defined in Yarn's {{resource-types.xml}}. The {{externalResources}} of {{InternalContainerResource}} has one or more entries with a value of 0.
 - These two {{InternalContainerResource}} do not match.

To solve this problem, we could ignore all the external resources with a value of 0 in ""ResourceInformationReflector#getExternalResources"".

cc [~trohrmann] Could you assign this to me?",,guoyangze,trohrmann,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 10:45:06 UTC 2020,,,,,,,,,,"0|z0f48g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/20 10:45;trohrmann;Fixed via

master: d1292b5f30508e155d0f733527532d7c671ad263
1.11.0: 4181bb4ddb435d457a9c1c61074c131da2a96238;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HistoryServer deletes cached archives if archive listing fails,FLINK-17914,13307143,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,rmetzger,rmetzger,25/May/20 06:05,15/Dec/21 01:44,13/Jul/23 08:07,18/Oct/21 07:57,1.13.2,1.14.0,,,,1.13.6,1.14.3,1.15.0,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2047&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d

{code}
[ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 4.697 s <<< FAILURE! - in org.apache.flink.runtime.webmonitor.history.HistoryServerTest
[ERROR] testCleanExpiredJob[Flink version less than 1.4: false](org.apache.flink.runtime.webmonitor.history.HistoryServerTest)  Time elapsed: 0.483 s  <<< FAILURE!
java.lang.AssertionError: expected:<2> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.flink.runtime.webmonitor.history.HistoryServerTest.runArchiveExpirationTest(HistoryServerTest.java:214)
	at org.apache.flink.runtime.webmonitor.history.HistoryServerTest.testCleanExpiredJob(HistoryServerTest.java:158)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

{code}",,dwysakowicz,leonard,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 18 07:57:05 UTC 2021,,,,,,,,,,"0|z0f47s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 11:17;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:10;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","15/Sep/21 06:03;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24111&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=9931;;;","04/Oct/21 07:40;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24726&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=81f2da51-a161-54c7-5b84-6001fed26530&l=8946;;;","18/Oct/21 07:57;chesnay;master: 247852d0571e71c02f7dd95d7e720ecb33f9a0f6..fa7b573f1330365486fd62f2f19bb718aefafffc
1.14: 026a7aab323cbdf33efd911bec0e353d5caac0e4..891cc24c9147866260bc4e575d8d4fb51c952d7d
1.13: 3c9374ec3bd3aaff8c27bd58066732370e7aad7d..4a3fab1d39c37aef0a14a31243c4fb453a61fecf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kerberized YARN on Docker e2e test: ""Final Application State: FAILED""",FLINK-17910,13307136,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,25/May/20 05:44,23/Sep/20 04:46,13/Jul/23 08:07,22/Sep/20 06:35,1.12.0,,,,,1.12.0,,,,Deployment / YARN,Tests,,,,0,pull-request-available,test-stability,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2061&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8

{code}
2020-05-22T21:19:22.1943701Z ==============================================================================
2020-05-22T21:19:22.1944878Z Running 'Running Kerberized YARN application on Docker test (custom fs plugin)'
2020-05-22T21:19:22.1945365Z ==============================================================================
2020-05-22T21:19:22.1958773Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-22195494660
2020-05-22T21:19:22.3625117Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT
2020-05-22T21:19:22.3947572Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT
2020-05-22T21:19:22.4043434Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT
2020-05-22T21:19:22.4525323Z Docker version 19.03.9, build 9d988398e7
2020-05-22T21:19:23.0459269Z docker-compose version 1.25.4, build 8d51620a
2020-05-22T21:19:23.0942111Z Flink Tarball directory /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-22195494660
2020-05-22T21:19:23.0943325Z Flink tarball filename flink.tar.gz
2020-05-22T21:19:23.0944263Z Flink distribution directory name flink-1.12-SNAPSHOT
2020-05-22T21:19:23.0945164Z End-to-end directory /home/vsts/work/1/s/flink-end-to-end-tests
2020-05-22T21:19:23.1045503Z Building Hadoop Docker container
2020-05-22T21:19:23.1493298Z Sending build context to Docker daemon  56.83kB
2020-05-22T21:19:23.1494099Z 
2020-05-22T21:19:23.2347114Z Step 1/54 : FROM sequenceiq/pam:ubuntu-14.04
2020-05-22T21:19:23.2351407Z  ---> df7bea4c5f64
2020-05-22T21:19:23.2352674Z Step 2/54 : RUN set -x     && addgroup hadoop     && useradd -d /home/hdfs -ms /bin/bash -G hadoop -p hdfs hdfs     && useradd -d /home/yarn -ms /bin/bash -G hadoop -p yarn yarn     && useradd -d /home/mapred -ms /bin/bash -G hadoop -p mapred mapred     && useradd -d /home/hadoop-user -ms /bin/bash -p hadoop-user hadoop-user
2020-05-22T21:19:23.2355663Z  ---> Using cache
2020-05-22T21:19:23.2356858Z  ---> 6d5924f58cc9
2020-05-22T21:19:23.2360334Z Step 3/54 : RUN set -x     && apt-get update && apt-get install -y     curl tar sudo openssh-server openssh-client rsync unzip krb5-user
2020-05-22T21:19:23.2362691Z  ---> Using cache
2020-05-22T21:19:23.2363645Z  ---> e751c48ace10
2020-05-22T21:19:23.2365365Z Step 4/54 : RUN set -x     && mkdir -p /var/log/kerberos     && touch /var/log/kerberos/kadmind.log
2020-05-22T21:19:23.2369651Z  ---> Using cache
2020-05-22T21:19:23.2370072Z  ---> 23112f030775
2020-05-22T21:19:23.2371522Z Step 5/54 : RUN set -x     && rm -f /etc/ssh/ssh_host_dsa_key /etc/ssh/ssh_host_rsa_key /root/.ssh/id_rsa     && ssh-keygen -q -N """" -t dsa -f /etc/ssh/ssh_host_dsa_key     && ssh-keygen -q -N """" -t rsa -f /etc/ssh/ssh_host_rsa_key     && ssh-keygen -q -N """" -t rsa -f /root/.ssh/id_rsa     && cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys
2020-05-22T21:19:23.2378185Z  ---> Using cache
2020-05-22T21:19:23.2379445Z  ---> b440f5703744
2020-05-22T21:19:23.2383887Z Step 6/54 : RUN set -x     && mkdir -p /usr/java/default     && curl -Ls 'http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz' -H 'Cookie: oraclelicense=accept-securebackup-cookie' |         tar --strip-components=1 -xz -C /usr/java/default/
2020-05-22T21:19:23.2384981Z  ---> Using cache
2020-05-22T21:19:23.2385779Z  ---> 7dc6af910a17
2020-05-22T21:19:23.2386182Z Step 7/54 : ENV JAVA_HOME /usr/java/default
2020-05-22T21:19:23.2387984Z  ---> Using cache
2020-05-22T21:19:23.2391424Z  ---> 6a74039804e6
2020-05-22T21:19:23.2391826Z Step 8/54 : ENV PATH $PATH:$JAVA_HOME/bin
2020-05-22T21:19:23.2394073Z  ---> Using cache
2020-05-22T21:19:23.2401679Z  ---> 7c1c284d77a9
2020-05-22T21:19:23.2403017Z Step 9/54 : RUN set -x     && curl -LOH 'Cookie: oraclelicense=accept-securebackup-cookie' 'http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip'     && unzip jce_policy-8.zip     && cp /UnlimitedJCEPolicyJDK8/local_policy.jar /UnlimitedJCEPolicyJDK8/US_export_policy.jar $JAVA_HOME/jre/lib/security
2020-05-22T21:19:23.2403961Z  ---> Using cache
2020-05-22T21:19:23.2404343Z  ---> 7188070276ed
2020-05-22T21:19:23.2404599Z Step 10/54 : ARG HADOOP_VERSION=2.8.4
2020-05-22T21:19:23.2414901Z  ---> Using cache
2020-05-22T21:19:23.2415439Z  ---> 5b94a16c64a1
2020-05-22T21:19:23.2416140Z Step 11/54 : ENV HADOOP_URL http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz
2020-05-22T21:19:23.2416697Z  ---> Using cache
2020-05-22T21:19:23.2417046Z  ---> 7772dac4cf00
2020-05-22T21:19:23.2417767Z Step 12/54 : RUN set -x     && curl -fSL ""$HADOOP_URL"" -o /tmp/hadoop.tar.gz     && tar -xf /tmp/hadoop.tar.gz -C /usr/local/     && rm /tmp/hadoop.tar.gz*
2020-05-22T21:19:23.2418335Z  ---> Using cache
2020-05-22T21:19:23.2418720Z  ---> e42fb0b26de2
2020-05-22T21:19:23.2418959Z Step 13/54 : WORKDIR /usr/local
2020-05-22T21:19:23.2461496Z  ---> Using cache
2020-05-22T21:19:23.2461932Z  ---> 5febdd652155
2020-05-22T21:19:23.2466548Z Step 14/54 : RUN set -x     && ln -s /usr/local/hadoop-${HADOOP_VERSION} /usr/local/hadoop     && chown root:root -R /usr/local/hadoop-${HADOOP_VERSION}/     && chown root:root -R /usr/local/hadoop/     && chown root:yarn /usr/local/hadoop/bin/container-executor     && chmod 6050 /usr/local/hadoop/bin/container-executor     && mkdir -p /hadoop-data/nm-local-dirs     && mkdir -p /hadoop-data/nm-log-dirs     && chown yarn:yarn /hadoop-data     && chown yarn:yarn /hadoop-data/nm-local-dirs     && chown yarn:yarn /hadoop-data/nm-log-dirs     && chmod 755 /hadoop-data     && chmod 755 /hadoop-data/nm-local-dirs     && chmod 755 /hadoop-data/nm-log-dirs
2020-05-22T21:19:23.2470850Z  ---> Using cache
2020-05-22T21:19:23.2471244Z  ---> 209a70b30085
2020-05-22T21:19:23.2471518Z Step 15/54 : ENV HADOOP_HOME /usr/local/hadoop
2020-05-22T21:19:23.2472098Z  ---> Using cache
2020-05-22T21:19:23.2472467Z  ---> 027905270d57
2020-05-22T21:19:23.2472729Z Step 16/54 : ENV HADOOP_COMMON_HOME /usr/local/hadoop
2020-05-22T21:19:23.2473435Z  ---> Using cache
2020-05-22T21:19:23.2476300Z  ---> c015b1952ff2
2020-05-22T21:19:23.2476618Z Step 17/54 : ENV HADOOP_HDFS_HOME /usr/local/hadoop
2020-05-22T21:19:23.2477050Z  ---> Using cache
2020-05-22T21:19:23.2477393Z  ---> 862fb5b1ec33
2020-05-22T21:19:23.2477670Z Step 18/54 : ENV HADOOP_MAPRED_HOME /usr/local/hadoop
2020-05-22T21:19:23.2478070Z  ---> Using cache
2020-05-22T21:19:23.2478429Z  ---> 9f44aaa9cd29
2020-05-22T21:19:23.2478684Z Step 19/54 : ENV HADOOP_YARN_HOME /usr/local/hadoop
2020-05-22T21:19:23.2481757Z  ---> Using cache
2020-05-22T21:19:23.2482168Z  ---> fb4322d194d5
2020-05-22T21:19:23.2482467Z Step 20/54 : ENV HADOOP_CONF_DIR /usr/local/hadoop/etc/hadoop
2020-05-22T21:19:23.2482899Z  ---> Using cache
2020-05-22T21:19:23.2483240Z  ---> e3360fca4552
2020-05-22T21:19:23.2483535Z Step 21/54 : ENV YARN_CONF_DIR /usr/local/hadoop/etc/hadoop
2020-05-22T21:19:23.2483948Z  ---> Using cache
2020-05-22T21:19:23.2484308Z  ---> 04f42078c888
2020-05-22T21:19:23.2484556Z Step 22/54 : ENV HADOOP_LOG_DIR /var/log/hadoop
2020-05-22T21:19:23.2487607Z  ---> Using cache
2020-05-22T21:19:23.2488004Z  ---> 6c014421f6e1
2020-05-22T21:19:23.2488276Z Step 23/54 : ENV HADOOP_BIN_HOME $HADOOP_HOME/bin
2020-05-22T21:19:23.2488667Z  ---> Using cache
2020-05-22T21:19:23.2489026Z  ---> 4f01fad8b4a0
2020-05-22T21:19:23.2489294Z Step 24/54 : ENV PATH $PATH:$HADOOP_BIN_HOME
2020-05-22T21:19:23.2489744Z  ---> Using cache
2020-05-22T21:19:23.2490111Z  ---> 665af8882b26
2020-05-22T21:19:23.2494731Z Step 25/54 : ENV KRB_REALM EXAMPLE.COM
2020-05-22T21:19:23.2495306Z  ---> Using cache
2020-05-22T21:19:23.2495661Z  ---> c0bf6bdc23c9
2020-05-22T21:19:23.2495915Z Step 26/54 : ENV DOMAIN_REALM example.com
2020-05-22T21:19:23.2496294Z  ---> Using cache
2020-05-22T21:19:23.2496653Z  ---> 6a155297ac88
2020-05-22T21:19:23.2497084Z Step 27/54 : ENV KERBEROS_ADMIN admin/admin
2020-05-22T21:19:23.2499920Z  ---> Using cache
2020-05-22T21:19:23.2500320Z  ---> 07f834909f56
2020-05-22T21:19:23.2500572Z Step 28/54 : ENV KERBEROS_ADMIN_PASSWORD admin
2020-05-22T21:19:23.2500973Z  ---> Using cache
2020-05-22T21:19:23.2501317Z  ---> 49c93eac628b
2020-05-22T21:19:23.2501602Z Step 29/54 : ENV KEYTAB_DIR /etc/security/keytabs
2020-05-22T21:19:23.2501998Z  ---> Using cache
2020-05-22T21:19:23.2502353Z  ---> 913513dd0edd
2020-05-22T21:19:23.2502583Z Step 30/54 : RUN mkdir /var/log/hadoop
2020-05-22T21:19:23.2503019Z  ---> Using cache
2020-05-22T21:19:23.2507301Z  ---> f739ff56e1d3
2020-05-22T21:19:23.2507844Z Step 31/54 : ADD config/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml
2020-05-22T21:19:23.2508311Z  ---> Using cache
2020-05-22T21:19:23.2508659Z  ---> 165b355f63cc
2020-05-22T21:19:23.2509172Z Step 32/54 : ADD config/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml
2020-05-22T21:19:23.2512305Z  ---> Using cache
2020-05-22T21:19:23.2512707Z  ---> bfde303fe392
2020-05-22T21:19:23.2513236Z Step 33/54 : ADD config/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml
2020-05-22T21:19:23.2513684Z  ---> Using cache
2020-05-22T21:19:23.2514038Z  ---> d999695c4795
2020-05-22T21:19:23.2514532Z Step 34/54 : ADD config/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml
2020-05-22T21:19:23.2520094Z  ---> Using cache
2020-05-22T21:19:23.2520477Z  ---> 0024d525d33c
2020-05-22T21:19:23.2521046Z Step 35/54 : ADD config/container-executor.cfg $HADOOP_HOME/etc/hadoop/container-executor.cfg
2020-05-22T21:19:23.2526530Z  ---> Using cache
2020-05-22T21:19:23.2526920Z  ---> 0d0d242c894a
2020-05-22T21:19:23.2527193Z Step 36/54 : ADD config/krb5.conf /etc/krb5.conf
2020-05-22T21:19:23.2531960Z  ---> Using cache
2020-05-22T21:19:23.2533987Z  ---> da033d5d7699
2020-05-22T21:19:23.2537665Z Step 37/54 : ADD config/ssl-server.xml $HADOOP_HOME/etc/hadoop/ssl-server.xml
2020-05-22T21:19:23.2542099Z  ---> Using cache
2020-05-22T21:19:23.2553040Z  ---> 51c4362d205a
2020-05-22T21:19:23.2564394Z Step 38/54 : ADD config/ssl-client.xml $HADOOP_HOME/etc/hadoop/ssl-client.xml
2020-05-22T21:19:23.2574123Z  ---> Using cache
2020-05-22T21:19:23.2577220Z  ---> b794a3aad969
2020-05-22T21:19:23.2577560Z Step 39/54 : ADD config/keystore.jks $HADOOP_HOME/lib/keystore.jks
2020-05-22T21:19:23.2578018Z  ---> Using cache
2020-05-22T21:19:23.2591589Z  ---> 815f53403da8
2020-05-22T21:19:23.2592364Z Step 40/54 : RUN set -x     && chmod 400 $HADOOP_HOME/etc/hadoop/container-executor.cfg     && chown root:yarn $HADOOP_HOME/etc/hadoop/container-executor.cfg
2020-05-22T21:19:23.2592988Z  ---> Using cache
2020-05-22T21:19:23.2593359Z  ---> 165cd094f408
2020-05-22T21:19:23.2593620Z Step 41/54 : ADD config/ssh_config /root/.ssh/config
2020-05-22T21:19:23.2594036Z  ---> Using cache
2020-05-22T21:19:23.2594380Z  ---> 0e45ac4cb4a8
2020-05-22T21:19:23.2594971Z Step 42/54 : RUN set -x     && chmod 600 /root/.ssh/config     && chown root:root /root/.ssh/config
2020-05-22T21:19:23.2595542Z  ---> Using cache
2020-05-22T21:19:23.2595891Z  ---> 2b59d20a9e15
2020-05-22T21:19:23.2596678Z Step 43/54 : RUN set -x     && ls -la /usr/local/hadoop/etc/hadoop/*-env.sh     && chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh     && ls -la /usr/local/hadoop/etc/hadoop/*-env.sh
2020-05-22T21:19:23.2597289Z  ---> Using cache
2020-05-22T21:19:23.2597658Z  ---> b749e6379d22
2020-05-22T21:19:23.2598429Z Step 44/54 : RUN set -x     && sed  -i ""/^[^#]*UsePAM/ s/.*/#&/""  /etc/ssh/sshd_config     && echo ""UsePAM no"" >> /etc/ssh/sshd_config     && echo ""Port 2122"" >> /etc/ssh/sshd_config
2020-05-22T21:19:23.2599052Z  ---> Using cache
2020-05-22T21:19:23.2613702Z  ---> 65408ba9773c
2020-05-22T21:19:23.2614041Z Step 45/54 : EXPOSE 50470 9000 50010 50020 50070 50075 50090 50475 50091 8020
2020-05-22T21:19:23.2614516Z  ---> Using cache
2020-05-22T21:19:23.2614865Z  ---> 32f8cf5e1a0d
2020-05-22T21:19:23.2615096Z Step 46/54 : EXPOSE 19888
2020-05-22T21:19:23.2615447Z  ---> Using cache
2020-05-22T21:19:23.2615809Z  ---> b3ec79058f91
2020-05-22T21:19:23.2616099Z Step 47/54 : EXPOSE 8030 8031 8032 8033 8040 8042 8088 8188
2020-05-22T21:19:23.2616533Z  ---> Using cache
2020-05-22T21:19:23.2616892Z  ---> a3aacc337790
2020-05-22T21:19:23.2617113Z Step 48/54 : EXPOSE 49707 2122
2020-05-22T21:19:23.2617489Z  ---> Using cache
2020-05-22T21:19:23.2617843Z  ---> 4bddf3ee31a2
2020-05-22T21:19:23.2618108Z Step 49/54 : ADD bootstrap.sh /etc/bootstrap.sh
2020-05-22T21:19:23.2618498Z  ---> Using cache
2020-05-22T21:19:23.2618861Z  ---> 17a4b4cd7c92
2020-05-22T21:19:23.2619121Z Step 50/54 : RUN chown root:root /etc/bootstrap.sh
2020-05-22T21:19:23.2619538Z  ---> Using cache
2020-05-22T21:19:23.2619877Z  ---> 3f04c48a5704
2020-05-22T21:19:23.2620133Z Step 51/54 : RUN chmod 700 /etc/bootstrap.sh
2020-05-22T21:19:23.2627484Z  ---> Using cache
2020-05-22T21:19:23.2627874Z  ---> 63250321365f
2020-05-22T21:19:23.2628137Z Step 52/54 : ENV BOOTSTRAP /etc/bootstrap.sh
2020-05-22T21:19:23.2628521Z  ---> Using cache
2020-05-22T21:19:23.2628879Z  ---> 5d12818e48c4
2020-05-22T21:19:23.2632696Z Step 53/54 : ENTRYPOINT [""/etc/bootstrap.sh""]
2020-05-22T21:19:23.2640123Z  ---> Using cache
2020-05-22T21:19:23.2640790Z  ---> 214b0b13e211
2020-05-22T21:19:23.2641175Z Step 54/54 : CMD [""-h""]
2020-05-22T21:19:23.2641542Z  ---> Using cache
2020-05-22T21:19:23.2641884Z  ---> 6e4fbe512093
2020-05-22T21:19:23.2643935Z Successfully built 6e4fbe512093
2020-05-22T21:19:23.2694408Z Successfully tagged flink/docker-hadoop-secure-cluster:latest
2020-05-22T21:19:23.2724103Z Starting Hadoop cluster
2020-05-22T21:19:23.9070951Z Creating network ""docker-hadoop-cluster-network"" with the default driver
2020-05-22T21:19:24.1011749Z Creating kdc ... 
2020-05-22T21:19:28.2896089Z [1A[2K
2020-05-22T21:19:28.2897203Z Creating kdc ... [32mdone[0m
2020-05-22T21:19:28.2978876Z [1BCreating master ... 
2020-05-22T21:19:29.4561823Z [1A[2K
2020-05-22T21:19:29.4583837Z Creating master ... [32mdone[0m
2020-05-22T21:19:29.4735849Z [1BCreating slave1 ... 
2020-05-22T21:19:29.4771024Z Creating slave2 ... 
2020-05-22T21:19:30.6877638Z [2A[2K
2020-05-22T21:19:30.6878727Z Creating slave1 ... [32mdone[0m
2020-05-22T21:19:31.1249534Z [2B[1A[2K
2020-05-22T21:19:31.1250093Z Creating slave2 ... [32mdone[0m
2020-05-22T21:19:31.3107267Z [1BWaiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...
2020-05-22T21:19:36.4389046Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...
2020-05-22T21:19:41.5612168Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...
2020-05-22T21:19:46.6590992Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...
2020-05-22T21:19:51.7628617Z Waiting for hadoop cluster to come up. We have been trying for 20 seconds, retrying ...
2020-05-22T21:19:56.8427407Z Waiting for hadoop cluster to come up. We have been trying for 25 seconds, retrying ...
2020-05-22T21:20:01.8947108Z Waiting for hadoop cluster to come up. We have been trying for 30 seconds, retrying ...
2020-05-22T21:20:07.0212323Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...
2020-05-22T21:20:12.0993651Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...
2020-05-22T21:20:17.1946600Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...
2020-05-22T21:20:22.4292515Z We only have 0 NodeManagers up. We have been trying for 0 seconds, retrying ...
2020-05-22T21:20:25.0995281Z 20/05/22 21:20:25 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:20:25.5793119Z 20/05/22 21:20:25 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:20:26.2098326Z We now have 2 NodeManagers up.
2020-05-22T21:20:47.4997429Z Flink config:
2020-05-22T21:20:47.7979001Z security.kerberos.login.keytab: /home/hadoop-user/hadoop-user.keytab
2020-05-22T21:20:47.7979800Z security.kerberos.login.principal: hadoop-user
2020-05-22T21:20:47.7980135Z slot.request.timeout: 120000
2020-05-22T21:20:48.3951160Z SLF4J: Class path contains multiple SLF4J bindings.
2020-05-22T21:20:48.3952602Z SLF4J: Found binding in [jar:file:/home/hadoop-user/flink-1.12-SNAPSHOT/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-05-22T21:20:48.3953652Z SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.8.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-05-22T21:20:48.3954220Z SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-05-22T21:20:48.4009596Z SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2020-05-22T21:20:49.7367839Z 2020-05-22 21:20:49,736 INFO  org.apache.hadoop.security.UserGroupInformation              [] - Login successful for user hadoop-user using keytab file /home/hadoop-user/hadoop-user.keytab
2020-05-22T21:20:49.9978850Z 2020-05-22 21:20:49,997 INFO  org.apache.hadoop.yarn.client.RMProxy                        [] - Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:20:50.1737353Z 2020-05-22 21:20:50,173 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:20:50.1853317Z 2020-05-22 21:20:50,184 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2020-05-22T21:20:50.2151294Z 2020-05-22 21:20:50,214 WARN  org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] - The configuration directory ('/home/hadoop-user/flink-1.12-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.
2020-05-22T21:20:50.4351339Z 2020-05-22 21:20:50,434 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Cluster specification: ClusterSpecification{masterMemoryMB=1000, taskManagerMemoryMB=1000, slotsPerTaskManager=1}
2020-05-22T21:21:18.5358063Z 2020-05-22 21:21:18,534 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Adding keytab /home/hadoop-user/hadoop-user.keytab to the AM container local resource bucket
2020-05-22T21:21:18.5735098Z 2020-05-22 21:21:18,572 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Adding delegation token to the AM container.
2020-05-22T21:21:18.5924580Z 2020-05-22 21:21:18,591 INFO  org.apache.hadoop.hdfs.DFSClient                             [] - Created HDFS_DELEGATION_TOKEN token 1 for hadoop-user on 172.22.0.3:9000
2020-05-22T21:21:18.6243892Z 2020-05-22 21:21:18,623 INFO  org.apache.hadoop.mapreduce.security.TokenCache              [] - Got dt for hdfs://master.docker-hadoop-cluster-network:9000; Kind: HDFS_DELEGATION_TOKEN, Service: 172.22.0.3:9000, Ident: (HDFS_DELEGATION_TOKEN token 1 for hadoop-user)
2020-05-22T21:21:18.6245507Z 2020-05-22 21:21:18,623 INFO  org.apache.flink.yarn.Utils                                  [] - Attempting to obtain Kerberos security token for HBase
2020-05-22T21:21:18.6270047Z 2020-05-22 21:21:18,626 INFO  org.apache.flink.yarn.Utils                                  [] - HBase is not available (not packaged with this application): ClassNotFoundException : ""org.apache.hadoop.hbase.HBaseConfiguration"".
2020-05-22T21:21:18.6348936Z 2020-05-22 21:21:18,634 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Submitting application master application_1590182392090_0001
2020-05-22T21:21:18.8928412Z 2020-05-22 21:21:18,892 INFO  org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl    [] - Timeline service address: http://master.docker-hadoop-cluster-network:8188/ws/v1/timeline/
2020-05-22T21:21:20.5540928Z 2020-05-22 21:21:20,553 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Submitted application application_1590182392090_0001
2020-05-22T21:21:20.5547910Z 2020-05-22 21:21:20,554 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Waiting for the cluster to be allocated
2020-05-22T21:21:20.5599614Z 2020-05-22 21:21:20,559 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Deploying cluster, current state ACCEPTED
2020-05-22T21:21:52.0789480Z 2020-05-22 21:21:51,760 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - YARN application has been deployed successfully.
2020-05-22T21:21:52.0791023Z 2020-05-22 21:21:51,761 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface slave2.docker-hadoop-cluster-network:39241 of application 'application_1590182392090_0001'.
2020-05-22T21:21:52.0797444Z Flink YARN Application submitted.
2020-05-22T21:21:54.1439855Z 20/05/22 21:21:54 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:21:54.5999714Z 20/05/22 21:21:54 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:21:54.9004370Z Total number of applications (application-types: [] and states: [NEW, NEW_SAVING, SUBMITTED, ACCEPTED, RUNNING, FINISHED, FAILED, KILLED]):1
2020-05-22T21:21:54.9005897Z                 Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL
2020-05-22T21:21:54.9007137Z application_1590182392090_0001	Flink Application Cluster	        Apache Flink	hadoop-user	   default	           RUNNING	         UNDEFINED	             0%	http://slave2.docker-hadoop-cluster-network:39241
2020-05-22T21:21:56.2740266Z 20/05/22 21:21:56 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:21:56.8067474Z 20/05/22 21:21:56 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:21:57.2174030Z Application ID: application_1590182392090_0001
2020-05-22T21:21:57.2200292Z Application application_1590182392090_0001 is in state UNDEFINED. We have been waiting for 0 seconds, looping ...
2020-05-22T21:22:00.1230233Z 20/05/22 21:22:00 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:22:01.3048967Z 20/05/22 21:22:01 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:22:01.8945150Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 4 seconds, looping ...
2020-05-22T21:22:05.0795394Z 20/05/22 21:22:05 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:22:06.0571565Z 20/05/22 21:22:06 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:22:06.6397174Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 9 seconds, looping ...
2020-05-22T21:22:10.0235540Z 20/05/22 21:22:10 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:22:11.0378700Z 20/05/22 21:22:11 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:22:11.6234136Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 14 seconds, looping ...
2020-05-22T21:22:14.9252628Z 20/05/22 21:22:14 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:22:15.9349646Z 20/05/22 21:22:15 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:22:16.6554490Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 19 seconds, looping ...
2020-05-22T21:22:19.9568591Z 20/05/22 21:22:19 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:22:22.2007232Z 20/05/22 21:22:22 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:22:22.7152250Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 25 seconds, looping ...
2020-05-22T21:22:27.0136495Z 20/05/22 21:22:27 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:22:28.4488505Z 20/05/22 21:22:28 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:22:29.3621435Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 32 seconds, looping ...
2020-05-22T21:22:32.4554140Z 20/05/22 21:22:32 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:22:33.5239432Z 20/05/22 21:22:33 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:22:34.0693567Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 37 seconds, looping ...
2020-05-22T21:22:36.9889796Z 20/05/22 21:22:36 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:22:38.0359283Z 20/05/22 21:22:38 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:22:38.6256480Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 41 seconds, looping ...
2020-05-22T21:22:41.6823176Z 20/05/22 21:22:41 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:22:42.4372362Z 20/05/22 21:22:42 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:22:42.8481557Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 45 seconds, looping ...
2020-05-22T21:22:46.6830904Z 20/05/22 21:22:46 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:22:48.0759311Z 20/05/22 21:22:48 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:22:48.9919876Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 51 seconds, looping ...
2020-05-22T21:22:52.8233298Z 20/05/22 21:22:52 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:22:54.3806892Z 20/05/22 21:22:54 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:22:55.2187828Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 58 seconds, looping ...
2020-05-22T21:22:59.0085142Z 20/05/22 21:22:59 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:22:59.6504228Z 20/05/22 21:22:59 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:23:00.0542081Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 63 seconds, looping ...
2020-05-22T21:23:02.3270745Z 20/05/22 21:23:02 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:23:03.0650742Z 20/05/22 21:23:03 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:23:03.4892849Z Application application_1590182392090_0001 is in state RUNNING. We have been waiting for 66 seconds, looping ...
2020-05-22T21:23:06.2065048Z 20/05/22 21:23:06 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:23:06.6894681Z 20/05/22 21:23:06 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:23:08.6649920Z 20/05/22 21:23:08 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:23:09.1570444Z 20/05/22 21:23:09 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:23:09.5164983Z Final Application State: FAILED
2020-05-22T21:23:09.5166627Z Running the Flink Application failed. ðŸ˜ž
2020-05-22T21:23:09.5167241Z Debugging failed YARN Docker test:
2020-05-22T21:23:09.5167704Z Currently running containers
2020-05-22T21:23:09.5625847Z CONTAINER ID        IMAGE                                       COMMAND                  CREATED             STATUS              PORTS                                                                                                                                                                                          NAMES
2020-05-22T21:23:09.5629095Z 1aa2ffd4b7f2        flink/docker-hadoop-secure-cluster:latest   ""/etc/bootstrap.sh wâ€¦""   3 minutes ago       Up 3 minutes        2122/tcp, 8020/tcp, 8030-8033/tcp, 8040/tcp, 8042/tcp, 8088/tcp, 8188/tcp, 9000/tcp, 19888/tcp, 49707/tcp, 50010/tcp, 50020/tcp, 50070/tcp, 50075/tcp, 50090-50091/tcp, 50470/tcp, 50475/tcp   slave2
2020-05-22T21:23:09.5631181Z b0d1568a8825        flink/docker-hadoop-secure-cluster:latest   ""/etc/bootstrap.sh wâ€¦""   3 minutes ago       Up 3 minutes        2122/tcp, 8020/tcp, 8030-8033/tcp, 8040/tcp, 8042/tcp, 8088/tcp, 8188/tcp, 9000/tcp, 19888/tcp, 49707/tcp, 50010/tcp, 50020/tcp, 50070/tcp, 50075/tcp, 50090-50091/tcp, 50470/tcp, 50475/tcp   slave1
2020-05-22T21:23:09.5633542Z 49db5b6cc490        flink/docker-hadoop-secure-cluster:latest   ""/etc/bootstrap.sh mâ€¦""   3 minutes ago       Up 3 minutes        2122/tcp, 8020/tcp, 8030-8033/tcp, 8040/tcp, 8042/tcp, 8088/tcp, 8188/tcp, 9000/tcp, 19888/tcp, 49707/tcp, 50010/tcp, 50020/tcp, 50070/tcp, 50075/tcp, 50090-50091/tcp, 50470/tcp, 50475/tcp   master
2020-05-22T21:23:09.5635489Z 7745982b46e7        sequenceiq/kerberos                         ""/config.sh""             3 minutes ago       Up 3 minutes        88/tcp, 749/tcp                                                                                                                                                                                kdc
2020-05-22T21:23:09.5660894Z Currently running JVMs
2020-05-22T21:23:09.6997631Z 45379 Jps -Dapplication.home=/usr/lib/jvm/zulu-8-azure-amd64 -Xms8m
2020-05-22T21:23:09.7125023Z Hadoop logs:
2020-05-22T21:23:09.8710127Z total 204K
2020-05-22T21:23:09.8711655Z 1840178 4.0K drwxr-xr-x 2 vsts docker 4.0K May 22 21:19 .
2020-05-22T21:23:09.8712628Z 1840177 4.0K drwxr-xr-x 3 vsts docker 4.0K May 22 21:23 ..
2020-05-22T21:23:09.8713424Z 1840179  52K -rw-r--r-- 1 vsts docker  52K May 22 21:20 historyserver.err
2020-05-22T21:23:09.8714275Z 1840180    0 -rw-r--r-- 1 vsts docker    0 May 22 21:19 historyserver.out
2020-05-22T21:23:09.8715109Z 1840181  52K -rw-r--r-- 1 vsts docker  51K May 22 21:23 namenode.err
2020-05-22T21:23:09.8715927Z 1840182    0 -rw-r--r-- 1 vsts docker    0 May 22 21:19 namenode.out
2020-05-22T21:23:09.8716745Z 1840183  64K -rw-r--r-- 1 vsts docker  64K May 22 21:23 resourcemanager.err
2020-05-22T21:23:09.8717597Z 1840184    0 -rw-r--r-- 1 vsts docker    0 May 22 21:19 resourcemanager.out
2020-05-22T21:23:09.8718435Z 1840185  28K -rw-r--r-- 1 vsts docker  25K May 22 21:21 timelineserver.err
2020-05-22T21:23:09.8719264Z 1840186    0 -rw-r--r-- 1 vsts docker    0 May 22 21:19 timelineserver.out
2020-05-22T21:23:09.8720201Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-22195494660/logs/hadoop/historyserver.err:
2020-05-22T21:23:09.8722239Z 20/05/22 21:19:38 INFO hs.JobHistoryServer: STARTUP_MSG: 
2020-05-22T21:23:09.8722867Z /************************************************************
2020-05-22T21:23:09.8723401Z STARTUP_MSG: Starting JobHistoryServer
2020-05-22T21:23:09.8723895Z STARTUP_MSG:   user = mapred
2020-05-22T21:23:09.8724729Z STARTUP_MSG:   host = master.docker-hadoop-cluster-network/172.22.0.3
2020-05-22T21:23:09.8725333Z STARTUP_MSG:   args = []
2020-05-22T21:23:09.8725776Z STARTUP_MSG:   version = 2.8.4
2020-05-22T21:23:09.8773367Z STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/modules/*.jar
2020-05-22T21:23:09.8804932Z STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 17e75c2a11685af3e043aa5e604dc831e5b14674; compiled by 'jdu' on 2018-05-08T02:50Z
2020-05-22T21:23:09.8805491Z STARTUP_MSG:   java = 1.8.0_131
2020-05-22T21:23:09.8805816Z ************************************************************/
2020-05-22T21:23:09.8806231Z 20/05/22 21:19:38 INFO hs.JobHistoryServer: registered UNIX signal handlers for [TERM, HUP, INT]
2020-05-22T21:23:09.8807293Z 20/05/22 21:19:48 INFO security.UserGroupInformation: Login successful for user mapred/master.docker-hadoop-cluster-network@EXAMPLE.COM using keytab file /etc/security/keytabs/mapred.keytab
2020-05-22T21:23:09.8808205Z 20/05/22 21:19:48 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-05-22T21:23:09.8808695Z 20/05/22 21:19:49 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2020-05-22T21:23:09.8809351Z 20/05/22 21:19:49 INFO impl.MetricsSystemImpl: JobHistoryServer metrics system started
2020-05-22T21:23:09.8809774Z 20/05/22 21:19:49 INFO hs.JobHistory: JobHistory Init
2020-05-22T21:23:09.8810555Z 20/05/22 21:19:51 INFO jobhistory.JobHistoryUtils: Default file system [hdfs://master.docker-hadoop-cluster-network:9000]
2020-05-22T21:23:09.8811645Z 20/05/22 21:19:52 WARN ipc.Client: Failed to connect to server: master.docker-hadoop-cluster-network/172.22.0.3:9000: try once and fail.
2020-05-22T21:23:09.8812172Z java.net.ConnectException: Connection refused
2020-05-22T21:23:09.8812500Z 	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2020-05-22T21:23:09.8813022Z 	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2020-05-22T21:23:09.8813489Z 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
2020-05-22T21:23:09.8813949Z 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
2020-05-22T21:23:09.8814377Z 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
2020-05-22T21:23:09.8814849Z 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
2020-05-22T21:23:09.8815313Z 	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
2020-05-22T21:23:09.8815734Z 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
2020-05-22T21:23:09.8816137Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
2020-05-22T21:23:09.8816503Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
2020-05-22T21:23:09.8816949Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
2020-05-22T21:23:09.8817451Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
2020-05-22T21:23:09.8817880Z 	at com.sun.proxy.$Proxy12.getFileInfo(Unknown Source)
2020-05-22T21:23:09.8818387Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
2020-05-22T21:23:09.8818901Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-22T21:23:09.8819332Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-22T21:23:09.8819826Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-22T21:23:09.8820278Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-22T21:23:09.8820747Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
2020-05-22T21:23:09.8821308Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
2020-05-22T21:23:09.8821886Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
2020-05-22T21:23:09.8822439Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
2020-05-22T21:23:09.8822995Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
2020-05-22T21:23:09.8823434Z 	at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)
2020-05-22T21:23:09.8823803Z 	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1649)
2020-05-22T21:23:09.8824222Z 	at org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:133)
2020-05-22T21:23:09.8824621Z 	at org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1177)
2020-05-22T21:23:09.8825053Z 	at org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1173)
2020-05-22T21:23:09.8825478Z 	at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
2020-05-22T21:23:09.8825933Z 	at org.apache.hadoop.fs.FileContext.getFileStatus(FileContext.java:1173)
2020-05-22T21:23:09.8826389Z 	at org.apache.hadoop.fs.FileContext$Util.exists(FileContext.java:1638)
2020-05-22T21:23:09.8826863Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir(HistoryFileManager.java:692)
2020-05-22T21:23:09.8827433Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.tryCreatingHistoryDirs(HistoryFileManager.java:622)
2020-05-22T21:23:09.8828081Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.createHistoryDirs(HistoryFileManager.java:585)
2020-05-22T21:23:09.8828652Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:550)
2020-05-22T21:23:09.8829169Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.8829632Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:95)
2020-05-22T21:23:09.8830112Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.8830582Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
2020-05-22T21:23:09.8831165Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:151)
2020-05-22T21:23:09.8831654Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.8832182Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:231)
2020-05-22T21:23:09.8832738Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:241)
2020-05-22T21:23:09.8833666Z 20/05/22 21:19:52 INFO hs.HistoryFileManager: Waiting for FileSystem at master.docker-hadoop-cluster-network:9000to be available
2020-05-22T21:23:09.8834587Z 20/05/22 21:20:02 INFO jobhistory.JobHistoryUtils: Default file system [hdfs://master.docker-hadoop-cluster-network:9000]
2020-05-22T21:23:09.8836065Z 20/05/22 21:20:02 INFO service.AbstractService: Service org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://master.docker-hadoop-cluster-network:9000/tmp/hadoop-yarn/staging/history/done]
2020-05-22T21:23:09.8837999Z org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://master.docker-hadoop-cluster-network:9000/tmp/hadoop-yarn/staging/history/done]
2020-05-22T21:23:09.8838804Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.tryCreatingHistoryDirs(HistoryFileManager.java:639)
2020-05-22T21:23:09.8839405Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.createHistoryDirs(HistoryFileManager.java:585)
2020-05-22T21:23:09.8839961Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:550)
2020-05-22T21:23:09.8840477Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.8840941Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:95)
2020-05-22T21:23:09.8841423Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.8841920Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
2020-05-22T21:23:09.8842428Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:151)
2020-05-22T21:23:09.8842942Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.8843455Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:231)
2020-05-22T21:23:09.8844008Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:241)
2020-05-22T21:23:09.8844916Z Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
2020-05-22T21:23:09.8845538Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318)
2020-05-22T21:23:09.8846128Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
2020-05-22T21:23:09.8846721Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
2020-05-22T21:23:09.8847292Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)
2020-05-22T21:23:09.8847951Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)
2020-05-22T21:23:09.8848488Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)
2020-05-22T21:23:09.8849027Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
2020-05-22T21:23:09.8849522Z 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3039)
2020-05-22T21:23:09.8850056Z 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)
2020-05-22T21:23:09.8850712Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)
2020-05-22T21:23:09.8854290Z 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
2020-05-22T21:23:09.8858578Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2020-05-22T21:23:09.8859083Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2020-05-22T21:23:09.8859486Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
2020-05-22T21:23:09.8859897Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
2020-05-22T21:23:09.8860265Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-05-22T21:23:09.8860639Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-05-22T21:23:09.8861071Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
2020-05-22T21:23:09.8861540Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)
2020-05-22T21:23:09.8861782Z 
2020-05-22T21:23:09.8862062Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-05-22T21:23:09.8862524Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-05-22T21:23:09.8863099Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-05-22T21:23:09.8863617Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-05-22T21:23:09.8864080Z 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
2020-05-22T21:23:09.8864598Z 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
2020-05-22T21:23:09.8865068Z 	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2474)
2020-05-22T21:23:09.8865477Z 	at org.apache.hadoop.fs.Hdfs.mkdir(Hdfs.java:317)
2020-05-22T21:23:09.8865857Z 	at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:738)
2020-05-22T21:23:09.8866286Z 	at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:734)
2020-05-22T21:23:09.8867367Z 	at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
2020-05-22T21:23:09.8868540Z 	at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:734)
2020-05-22T21:23:09.8869055Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir(HistoryFileManager.java:694)
2020-05-22T21:23:09.8869615Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.tryCreatingHistoryDirs(HistoryFileManager.java:622)
2020-05-22T21:23:09.8870020Z 	... 10 more
2020-05-22T21:23:09.8871004Z Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
2020-05-22T21:23:09.8871747Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318)
2020-05-22T21:23:09.8873553Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
2020-05-22T21:23:09.8874197Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
2020-05-22T21:23:09.8874760Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)
2020-05-22T21:23:09.8875455Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)
2020-05-22T21:23:09.8875995Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)
2020-05-22T21:23:09.8876535Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
2020-05-22T21:23:09.8877049Z 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3039)
2020-05-22T21:23:09.8877565Z 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)
2020-05-22T21:23:09.8878223Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)
2020-05-22T21:23:09.8878978Z 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
2020-05-22T21:23:09.8879593Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2020-05-22T21:23:09.8880087Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2020-05-22T21:23:09.8880469Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
2020-05-22T21:23:09.8880879Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
2020-05-22T21:23:09.8881245Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-05-22T21:23:09.8881618Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-05-22T21:23:09.8882070Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
2020-05-22T21:23:09.8882521Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)
2020-05-22T21:23:09.8882764Z 
2020-05-22T21:23:09.8883054Z 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
2020-05-22T21:23:09.8883463Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
2020-05-22T21:23:09.8883831Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
2020-05-22T21:23:09.8884281Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
2020-05-22T21:23:09.8884780Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
2020-05-22T21:23:09.8885195Z 	at com.sun.proxy.$Proxy12.mkdirs(Unknown Source)
2020-05-22T21:23:09.8885668Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:583)
2020-05-22T21:23:09.8886186Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-22T21:23:09.8887323Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-22T21:23:09.8889877Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-22T21:23:09.8890373Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-22T21:23:09.8890827Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
2020-05-22T21:23:09.8892115Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
2020-05-22T21:23:09.8892697Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
2020-05-22T21:23:09.8893272Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
2020-05-22T21:23:09.8893829Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
2020-05-22T21:23:09.8894244Z 	at com.sun.proxy.$Proxy13.mkdirs(Unknown Source)
2020-05-22T21:23:09.8894635Z 	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2472)
2020-05-22T21:23:09.8894941Z 	... 17 more
2020-05-22T21:23:09.8896369Z 20/05/22 21:20:02 INFO service.AbstractService: Service org.apache.hadoop.mapreduce.v2.hs.JobHistory failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://master.docker-hadoop-cluster-network:9000/tmp/hadoop-yarn/staging/history/done]
2020-05-22T21:23:09.8898042Z org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://master.docker-hadoop-cluster-network:9000/tmp/hadoop-yarn/staging/history/done]
2020-05-22T21:23:09.8898762Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.tryCreatingHistoryDirs(HistoryFileManager.java:639)
2020-05-22T21:23:09.8899363Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.createHistoryDirs(HistoryFileManager.java:585)
2020-05-22T21:23:09.8899935Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:550)
2020-05-22T21:23:09.8900439Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.8901002Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:95)
2020-05-22T21:23:09.8901467Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.8901957Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
2020-05-22T21:23:09.8902472Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:151)
2020-05-22T21:23:09.8902980Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.8903512Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:231)
2020-05-22T21:23:09.8904045Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:241)
2020-05-22T21:23:09.8904962Z Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
2020-05-22T21:23:09.8905585Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318)
2020-05-22T21:23:09.8906176Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
2020-05-22T21:23:09.8906779Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
2020-05-22T21:23:09.8907336Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)
2020-05-22T21:23:09.8907878Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)
2020-05-22T21:23:09.8908408Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)
2020-05-22T21:23:09.8908939Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
2020-05-22T21:23:09.8909451Z 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3039)
2020-05-22T21:23:09.8909966Z 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)
2020-05-22T21:23:09.8910624Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)
2020-05-22T21:23:09.8911299Z 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
2020-05-22T21:23:09.8911913Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2020-05-22T21:23:09.8912392Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2020-05-22T21:23:09.8912772Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
2020-05-22T21:23:09.8913183Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
2020-05-22T21:23:09.8913553Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-05-22T21:23:09.8913925Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-05-22T21:23:09.8914378Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
2020-05-22T21:23:09.8914828Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)
2020-05-22T21:23:09.8915067Z 
2020-05-22T21:23:09.8915342Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-05-22T21:23:09.8915895Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-05-22T21:23:09.8916457Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-05-22T21:23:09.8916969Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-05-22T21:23:09.8917431Z 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
2020-05-22T21:23:09.8917949Z 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
2020-05-22T21:23:09.8921458Z 	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2474)
2020-05-22T21:23:09.8922008Z 	at org.apache.hadoop.fs.Hdfs.mkdir(Hdfs.java:317)
2020-05-22T21:23:09.8922405Z 	at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:738)
2020-05-22T21:23:09.8922814Z 	at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:734)
2020-05-22T21:23:09.8923254Z 	at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
2020-05-22T21:23:09.8923680Z 	at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:734)
2020-05-22T21:23:09.8924158Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir(HistoryFileManager.java:694)
2020-05-22T21:23:09.8924730Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.tryCreatingHistoryDirs(HistoryFileManager.java:622)
2020-05-22T21:23:09.8925109Z 	... 10 more
2020-05-22T21:23:09.8926092Z Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
2020-05-22T21:23:09.8926806Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318)
2020-05-22T21:23:09.8927854Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
2020-05-22T21:23:09.8928481Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
2020-05-22T21:23:09.8929053Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)
2020-05-22T21:23:09.8929598Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)
2020-05-22T21:23:09.8930154Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)
2020-05-22T21:23:09.8930674Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
2020-05-22T21:23:09.8931187Z 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3039)
2020-05-22T21:23:09.8931868Z 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)
2020-05-22T21:23:09.8932538Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)
2020-05-22T21:23:09.8933237Z 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
2020-05-22T21:23:09.8938732Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2020-05-22T21:23:09.8939240Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2020-05-22T21:23:09.8939621Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
2020-05-22T21:23:09.8947810Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
2020-05-22T21:23:09.8950211Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-05-22T21:23:09.8950593Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-05-22T21:23:09.8951047Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
2020-05-22T21:23:09.8951514Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)
2020-05-22T21:23:09.8951767Z 
2020-05-22T21:23:09.8952041Z 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
2020-05-22T21:23:09.8952445Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
2020-05-22T21:23:09.8952962Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
2020-05-22T21:23:09.8953414Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
2020-05-22T21:23:09.8953929Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
2020-05-22T21:23:09.8954329Z 	at com.sun.proxy.$Proxy12.mkdirs(Unknown Source)
2020-05-22T21:23:09.8954820Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:583)
2020-05-22T21:23:09.8955321Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-22T21:23:09.8955819Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-22T21:23:09.8956315Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-22T21:23:09.8956768Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-22T21:23:09.8957244Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
2020-05-22T21:23:09.8957808Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
2020-05-22T21:23:09.8958385Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
2020-05-22T21:23:09.8958938Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
2020-05-22T21:23:09.8959492Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
2020-05-22T21:23:09.8959921Z 	at com.sun.proxy.$Proxy13.mkdirs(Unknown Source)
2020-05-22T21:23:09.8960293Z 	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2472)
2020-05-22T21:23:09.8963487Z 	... 17 more
2020-05-22T21:23:09.8964092Z 20/05/22 21:20:02 INFO hs.JobHistory: Stopping JobHistory
2020-05-22T21:23:09.8965675Z 20/05/22 21:20:02 INFO service.AbstractService: Service org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://master.docker-hadoop-cluster-network:9000/tmp/hadoop-yarn/staging/history/done]
2020-05-22T21:23:09.8967196Z org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://master.docker-hadoop-cluster-network:9000/tmp/hadoop-yarn/staging/history/done]
2020-05-22T21:23:09.8967926Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.tryCreatingHistoryDirs(HistoryFileManager.java:639)
2020-05-22T21:23:09.8968530Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.createHistoryDirs(HistoryFileManager.java:585)
2020-05-22T21:23:09.8969114Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:550)
2020-05-22T21:23:09.8969616Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.8970102Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:95)
2020-05-22T21:23:09.8970571Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.8971067Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
2020-05-22T21:23:09.8971752Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:151)
2020-05-22T21:23:09.8972255Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.8972787Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:231)
2020-05-22T21:23:09.8973320Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:241)
2020-05-22T21:23:09.8974275Z Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
2020-05-22T21:23:09.8974916Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318)
2020-05-22T21:23:09.8975635Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
2020-05-22T21:23:09.8976242Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
2020-05-22T21:23:09.8976793Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)
2020-05-22T21:23:09.8977337Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)
2020-05-22T21:23:09.8977888Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)
2020-05-22T21:23:09.8978478Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
2020-05-22T21:23:09.8978986Z 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3039)
2020-05-22T21:23:09.8979501Z 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)
2020-05-22T21:23:09.8980158Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)
2020-05-22T21:23:09.8980851Z 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
2020-05-22T21:23:09.8981443Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2020-05-22T21:23:09.8981923Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2020-05-22T21:23:09.8982302Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
2020-05-22T21:23:09.8982711Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
2020-05-22T21:23:09.8983081Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-05-22T21:23:09.8983451Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-05-22T21:23:09.8983897Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
2020-05-22T21:23:09.8984351Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)
2020-05-22T21:23:09.8984606Z 
2020-05-22T21:23:09.8984866Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-05-22T21:23:09.8985349Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-05-22T21:23:09.8985906Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-05-22T21:23:09.8986416Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-05-22T21:23:09.8986877Z 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
2020-05-22T21:23:09.8987400Z 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
2020-05-22T21:23:09.8987886Z 	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2474)
2020-05-22T21:23:09.8988279Z 	at org.apache.hadoop.fs.Hdfs.mkdir(Hdfs.java:317)
2020-05-22T21:23:09.8988675Z 	at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:738)
2020-05-22T21:23:09.8989082Z 	at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:734)
2020-05-22T21:23:09.8989518Z 	at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
2020-05-22T21:23:09.8989940Z 	at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:734)
2020-05-22T21:23:09.8990416Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir(HistoryFileManager.java:694)
2020-05-22T21:23:09.8990984Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.tryCreatingHistoryDirs(HistoryFileManager.java:622)
2020-05-22T21:23:09.8991364Z 	... 10 more
2020-05-22T21:23:09.8992263Z Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
2020-05-22T21:23:09.8992988Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318)
2020-05-22T21:23:09.8994899Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
2020-05-22T21:23:09.8995545Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
2020-05-22T21:23:09.8996104Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)
2020-05-22T21:23:09.8996647Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)
2020-05-22T21:23:09.8997199Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)
2020-05-22T21:23:09.8997715Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
2020-05-22T21:23:09.8998342Z 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3039)
2020-05-22T21:23:09.8998861Z 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)
2020-05-22T21:23:09.8999518Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)
2020-05-22T21:23:09.9000216Z 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
2020-05-22T21:23:09.9000810Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2020-05-22T21:23:09.9001292Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2020-05-22T21:23:09.9001687Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
2020-05-22T21:23:09.9002080Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
2020-05-22T21:23:09.9002464Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-05-22T21:23:09.9002818Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-05-22T21:23:09.9003267Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
2020-05-22T21:23:09.9003723Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)
2020-05-22T21:23:09.9003977Z 
2020-05-22T21:23:09.9004249Z 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
2020-05-22T21:23:09.9004653Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
2020-05-22T21:23:09.9005017Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
2020-05-22T21:23:09.9005460Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
2020-05-22T21:23:09.9005975Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
2020-05-22T21:23:09.9006375Z 	at com.sun.proxy.$Proxy12.mkdirs(Unknown Source)
2020-05-22T21:23:09.9006868Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:583)
2020-05-22T21:23:09.9007367Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-22T21:23:09.9007793Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-22T21:23:09.9008307Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-22T21:23:09.9008739Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-22T21:23:09.9009207Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
2020-05-22T21:23:09.9009767Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
2020-05-22T21:23:09.9010344Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
2020-05-22T21:23:09.9010895Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
2020-05-22T21:23:09.9011605Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
2020-05-22T21:23:09.9012046Z 	at com.sun.proxy.$Proxy13.mkdirs(Unknown Source)
2020-05-22T21:23:09.9012412Z 	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2472)
2020-05-22T21:23:09.9012834Z 	... 17 more
2020-05-22T21:23:09.9013160Z 20/05/22 21:20:02 INFO impl.MetricsSystemImpl: Stopping JobHistoryServer metrics system...
2020-05-22T21:23:09.9013642Z 20/05/22 21:20:02 INFO impl.MetricsSystemImpl: JobHistoryServer metrics system stopped.
2020-05-22T21:23:09.9014132Z 20/05/22 21:20:02 INFO impl.MetricsSystemImpl: JobHistoryServer metrics system shutdown complete.
2020-05-22T21:23:09.9014591Z 20/05/22 21:20:02 FATAL hs.JobHistoryServer: Error starting JobHistoryServer
2020-05-22T21:23:09.9015703Z org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://master.docker-hadoop-cluster-network:9000/tmp/hadoop-yarn/staging/history/done]
2020-05-22T21:23:09.9016519Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.tryCreatingHistoryDirs(HistoryFileManager.java:639)
2020-05-22T21:23:09.9017109Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.createHistoryDirs(HistoryFileManager.java:585)
2020-05-22T21:23:09.9017686Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:550)
2020-05-22T21:23:09.9018186Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.9018667Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:95)
2020-05-22T21:23:09.9019130Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.9019620Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
2020-05-22T21:23:09.9020146Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:151)
2020-05-22T21:23:09.9020642Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
2020-05-22T21:23:09.9021171Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:231)
2020-05-22T21:23:09.9021707Z 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:241)
2020-05-22T21:23:09.9022636Z Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
2020-05-22T21:23:09.9023273Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318)
2020-05-22T21:23:09.9023841Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
2020-05-22T21:23:09.9024447Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
2020-05-22T21:23:09.9025000Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)
2020-05-22T21:23:09.9025550Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)
2020-05-22T21:23:09.9026099Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)
2020-05-22T21:23:09.9026614Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
2020-05-22T21:23:09.9027128Z 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3039)
2020-05-22T21:23:09.9027644Z 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)
2020-05-22T21:23:09.9028299Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)
2020-05-22T21:23:09.9028994Z 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
2020-05-22T21:23:09.9029585Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2020-05-22T21:23:09.9030070Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2020-05-22T21:23:09.9030449Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
2020-05-22T21:23:09.9030910Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
2020-05-22T21:23:09.9031371Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-05-22T21:23:09.9031744Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-05-22T21:23:09.9032193Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
2020-05-22T21:23:09.9032644Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)
2020-05-22T21:23:09.9032898Z 
2020-05-22T21:23:09.9033156Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-05-22T21:23:09.9033632Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-05-22T21:23:09.9034250Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-05-22T21:23:09.9035217Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-05-22T21:23:09.9035695Z 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
2020-05-22T21:23:09.9036223Z 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
2020-05-22T21:23:09.9036714Z 	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2474)
2020-05-22T21:23:09.9037109Z 	at org.apache.hadoop.fs.Hdfs.mkdir(Hdfs.java:317)
2020-05-22T21:23:09.9037504Z 	at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:738)
2020-05-22T21:23:09.9037912Z 	at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:734)
2020-05-22T21:23:09.9038349Z 	at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
2020-05-22T21:23:09.9038769Z 	at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:734)
2020-05-22T21:23:09.9039249Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir(HistoryFileManager.java:694)
2020-05-22T21:23:09.9039819Z 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.tryCreatingHistoryDirs(HistoryFileManager.java:622)
2020-05-22T21:23:09.9040199Z 	... 10 more
2020-05-22T21:23:09.9041156Z Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
2020-05-22T21:23:09.9041875Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318)
2020-05-22T21:23:09.9042457Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
2020-05-22T21:23:09.9043060Z 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
2020-05-22T21:23:09.9043617Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)
2020-05-22T21:23:09.9044165Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)
2020-05-22T21:23:09.9044702Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)
2020-05-22T21:23:09.9045236Z 	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
2020-05-22T21:23:09.9045753Z 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3039)
2020-05-22T21:23:09.9046270Z 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)
2020-05-22T21:23:09.9046922Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)
2020-05-22T21:23:09.9047601Z 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
2020-05-22T21:23:09.9048212Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2020-05-22T21:23:09.9048695Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2020-05-22T21:23:09.9049077Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
2020-05-22T21:23:09.9049488Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
2020-05-22T21:23:09.9049976Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-05-22T21:23:09.9050348Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-05-22T21:23:09.9050781Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
2020-05-22T21:23:09.9051420Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)
2020-05-22T21:23:09.9051675Z 
2020-05-22T21:23:09.9051967Z 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
2020-05-22T21:23:09.9052373Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
2020-05-22T21:23:09.9052737Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
2020-05-22T21:23:09.9053760Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
2020-05-22T21:23:09.9054260Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
2020-05-22T21:23:09.9054676Z 	at com.sun.proxy.$Proxy12.mkdirs(Unknown Source)
2020-05-22T21:23:09.9055163Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:583)
2020-05-22T21:23:09.9055678Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-22T21:23:09.9056107Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-22T21:23:09.9056601Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-22T21:23:09.9057052Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-22T21:23:09.9057506Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
2020-05-22T21:23:09.9058084Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
2020-05-22T21:23:09.9058664Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
2020-05-22T21:23:09.9059219Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
2020-05-22T21:23:09.9059778Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
2020-05-22T21:23:09.9060189Z 	at com.sun.proxy.$Proxy13.mkdirs(Unknown Source)
2020-05-22T21:23:09.9060569Z 	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2472)
2020-05-22T21:23:09.9060875Z 	... 17 more
2020-05-22T21:23:09.9061503Z 20/05/22 21:20:02 INFO util.ExitUtil: Exiting with status -1
2020-05-22T21:23:09.9061904Z 20/05/22 21:20:02 INFO hs.JobHistoryServer: SHUTDOWN_MSG: 
2020-05-22T21:23:09.9062261Z /************************************************************
2020-05-22T21:23:09.9062915Z SHUTDOWN_MSG: Shutting down JobHistoryServer at master.docker-hadoop-cluster-network/172.22.0.3
2020-05-22T21:23:09.9063328Z ************************************************************/
2020-05-22T21:23:09.9064041Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-22195494660/logs/hadoop/historyserver.out:
2020-05-22T21:23:09.9064833Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-22195494660/logs/hadoop/namenode.err:
2020-05-22T21:23:09.9065301Z 20/05/22 21:19:39 INFO namenode.NameNode: STARTUP_MSG: 
2020-05-22T21:23:09.9065669Z /************************************************************
2020-05-22T21:23:09.9065958Z STARTUP_MSG: Starting NameNode
2020-05-22T21:23:09.9066215Z STARTUP_MSG:   user = hdfs
2020-05-22T21:23:09.9066729Z STARTUP_MSG:   host = master.docker-hadoop-cluster-network/172.22.0.3
2020-05-22T21:23:09.9067050Z STARTUP_MSG:   args = []
2020-05-22T21:23:09.9067286Z STARTUP_MSG:   version = 2.8.4
2020-05-22T21:23:09.9102792Z STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
2020-05-22T21:23:09.9127729Z STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 17e75c2a11685af3e043aa5e604dc831e5b14674; compiled by 'jdu' on 2018-05-08T02:50Z
2020-05-22T21:23:09.9128293Z STARTUP_MSG:   java = 1.8.0_131
2020-05-22T21:23:09.9128610Z ************************************************************/
2020-05-22T21:23:09.9129018Z 20/05/22 21:19:39 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-05-22T21:23:09.9129458Z 20/05/22 21:19:39 INFO namenode.NameNode: createNameNode []
2020-05-22T21:23:09.9130391Z 20/05/22 21:19:43 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-05-22T21:23:09.9130879Z 20/05/22 21:19:45 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2020-05-22T21:23:09.9131919Z 20/05/22 21:19:45 INFO impl.MetricsSystemImpl: NameNode metrics system started
2020-05-22T21:23:09.9132800Z 20/05/22 21:19:45 INFO namenode.NameNode: fs.defaultFS is hdfs://master.docker-hadoop-cluster-network:9000
2020-05-22T21:23:09.9133712Z 20/05/22 21:19:45 INFO namenode.NameNode: Clients are to use master.docker-hadoop-cluster-network:9000 to access this namenode/service.
2020-05-22T21:23:09.9134759Z 20/05/22 21:19:48 INFO security.UserGroupInformation: Login successful for user hdfs/master.docker-hadoop-cluster-network@EXAMPLE.COM using keytab file /etc/security/keytabs/hdfs.keytab
2020-05-22T21:23:09.9135367Z 20/05/22 21:19:48 INFO util.JvmPauseMonitor: Starting JVM pause monitor
2020-05-22T21:23:09.9136131Z 20/05/22 21:19:48 INFO hdfs.DFSUtil: Starting web server as: HTTP/master.docker-hadoop-cluster-network@EXAMPLE.COM
2020-05-22T21:23:09.9136929Z 20/05/22 21:19:48 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: https://0.0.0.0:50470
2020-05-22T21:23:09.9137456Z 20/05/22 21:19:49 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-05-22T21:23:09.9138069Z 20/05/22 21:19:49 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-05-22T21:23:09.9138633Z 20/05/22 21:19:49 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-05-22T21:23:09.9139478Z 20/05/22 21:19:49 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-05-22T21:23:09.9140143Z 20/05/22 21:19:49 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-05-22T21:23:09.9140844Z 20/05/22 21:19:49 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-05-22T21:23:09.9141551Z 20/05/22 21:19:49 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-05-22T21:23:09.9142517Z 20/05/22 21:19:49 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-05-22T21:23:09.9143244Z 20/05/22 21:19:49 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-05-22T21:23:09.9143913Z 20/05/22 21:19:49 INFO http.HttpServer2: Adding Kerberos (SPNEGO) filter to getDelegationToken
2020-05-22T21:23:09.9144413Z 20/05/22 21:19:49 INFO http.HttpServer2: Adding Kerberos (SPNEGO) filter to renewDelegationToken
2020-05-22T21:23:09.9144905Z 20/05/22 21:19:49 INFO http.HttpServer2: Adding Kerberos (SPNEGO) filter to cancelDelegationToken
2020-05-22T21:23:09.9145383Z 20/05/22 21:19:49 INFO http.HttpServer2: Adding Kerberos (SPNEGO) filter to fsck
2020-05-22T21:23:09.9145832Z 20/05/22 21:19:49 INFO http.HttpServer2: Adding Kerberos (SPNEGO) filter to imagetransfer
2020-05-22T21:23:09.9146415Z 20/05/22 21:19:49 INFO http.HttpServer2: Jetty bound to port 50470
2020-05-22T21:23:09.9147031Z 20/05/22 21:19:49 INFO mortbay.log: jetty-6.1.26
2020-05-22T21:23:09.9147882Z 20/05/22 21:19:50 INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/hdfs.keytab, for principal HTTP/master.docker-hadoop-cluster-network@EXAMPLE.COM
2020-05-22T21:23:09.9148954Z 20/05/22 21:19:50 INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/hdfs.keytab, for principal HTTP/master.docker-hadoop-cluster-network@EXAMPLE.COM
2020-05-22T21:23:09.9149591Z 20/05/22 21:19:50 INFO mortbay.log: Started SslSelectChannelConnectorSecure@0.0.0.0:50470
2020-05-22T21:23:09.9150334Z 20/05/22 21:19:50 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-05-22T21:23:09.9151126Z 20/05/22 21:19:50 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-05-22T21:23:09.9151740Z 20/05/22 21:19:51 INFO namenode.FSEditLog: Edit logging is async:true
2020-05-22T21:23:09.9152166Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: KeyProvider: null
2020-05-22T21:23:09.9152566Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: fsLock is fair: true
2020-05-22T21:23:09.9153045Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-05-22T21:23:09.9153554Z 20/05/22 21:19:51 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-05-22T21:23:09.9154344Z 20/05/22 21:19:51 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2020-05-22T21:23:09.9154968Z 20/05/22 21:19:51 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-05-22T21:23:09.9155595Z 20/05/22 21:19:51 INFO blockmanagement.BlockManager: The block deletion will start around 2020 May 22 21:19:51
2020-05-22T21:23:09.9156102Z 20/05/22 21:19:51 INFO util.GSet: Computing capacity for map BlocksMap
2020-05-22T21:23:09.9156690Z 20/05/22 21:19:51 INFO util.GSet: VM type       = 64-bit
2020-05-22T21:23:09.9157061Z 20/05/22 21:19:51 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-05-22T21:23:09.9157468Z 20/05/22 21:19:51 INFO util.GSet: capacity      = 2^21 = 2097152 entries
2020-05-22T21:23:09.9157898Z 20/05/22 21:19:51 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=true
2020-05-22T21:23:09.9158539Z 20/05/22 21:19:51 INFO blockmanagement.BlockManager: dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-05-22T21:23:09.9159179Z 20/05/22 21:19:51 INFO blockmanagement.BlockManager: defaultReplication         = 1
2020-05-22T21:23:09.9160014Z 20/05/22 21:19:51 INFO blockmanagement.BlockManager: maxReplication             = 512
2020-05-22T21:23:09.9160510Z 20/05/22 21:19:51 INFO blockmanagement.BlockManager: minReplication             = 1
2020-05-22T21:23:09.9160961Z 20/05/22 21:19:51 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-05-22T21:23:09.9161438Z 20/05/22 21:19:51 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-05-22T21:23:09.9161913Z 20/05/22 21:19:51 INFO blockmanagement.BlockManager: encryptDataTransfer        = true
2020-05-22T21:23:09.9162369Z 20/05/22 21:19:51 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-05-22T21:23:09.9163309Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: fsOwner             = hdfs/master.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9163867Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: supergroup          = root
2020-05-22T21:23:09.9164293Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: isPermissionEnabled = true
2020-05-22T21:23:09.9164869Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: HA Enabled: false
2020-05-22T21:23:09.9165381Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: Append Enabled: true
2020-05-22T21:23:09.9165798Z 20/05/22 21:19:51 INFO util.GSet: Computing capacity for map INodeMap
2020-05-22T21:23:09.9166410Z 20/05/22 21:19:51 INFO util.GSet: VM type       = 64-bit
2020-05-22T21:23:09.9166798Z 20/05/22 21:19:51 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-05-22T21:23:09.9167183Z 20/05/22 21:19:51 INFO util.GSet: capacity      = 2^20 = 1048576 entries
2020-05-22T21:23:09.9167591Z 20/05/22 21:19:51 INFO namenode.FSDirectory: ACLs enabled? false
2020-05-22T21:23:09.9167993Z 20/05/22 21:19:51 INFO namenode.FSDirectory: XAttrs enabled? true
2020-05-22T21:23:09.9168495Z 20/05/22 21:19:51 INFO namenode.NameNode: Caching file names occurring more than 10 times
2020-05-22T21:23:09.9168945Z 20/05/22 21:19:51 INFO util.GSet: Computing capacity for map cachedBlocks
2020-05-22T21:23:09.9169547Z 20/05/22 21:19:51 INFO util.GSet: VM type       = 64-bit
2020-05-22T21:23:09.9169943Z 20/05/22 21:19:51 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-05-22T21:23:09.9170330Z 20/05/22 21:19:51 INFO util.GSet: capacity      = 2^18 = 262144 entries
2020-05-22T21:23:09.9171039Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-05-22T21:23:09.9171711Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-05-22T21:23:09.9172182Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-05-22T21:23:09.9172678Z 20/05/22 21:19:51 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-05-22T21:23:09.9173164Z 20/05/22 21:19:51 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-05-22T21:23:09.9173663Z 20/05/22 21:19:51 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-05-22T21:23:09.9174144Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2020-05-22T21:23:09.9175452Z 20/05/22 21:19:51 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-05-22T21:23:09.9176021Z 20/05/22 21:19:51 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2020-05-22T21:23:09.9176710Z 20/05/22 21:19:51 INFO util.GSet: VM type       = 64-bit
2020-05-22T21:23:09.9177129Z 20/05/22 21:19:51 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-05-22T21:23:09.9177546Z 20/05/22 21:19:51 INFO util.GSet: capacity      = 2^15 = 32768 entries
2020-05-22T21:23:09.9178358Z 20/05/22 21:19:51 INFO common.Storage: Lock on /tmp/hadoop-hdfs/dfs/name/in_use.lock acquired by nodename 120@master.docker-hadoop-cluster-network
2020-05-22T21:23:09.9179247Z 20/05/22 21:19:51 INFO namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-hdfs/dfs/name/current
2020-05-22T21:23:09.9179740Z 20/05/22 21:19:51 INFO namenode.FSImage: No edit log streams selected.
2020-05-22T21:23:09.9180645Z 20/05/22 21:19:51 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-hdfs/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-05-22T21:23:09.9181250Z 20/05/22 21:19:52 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.
2020-05-22T21:23:09.9181686Z 20/05/22 21:19:52 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-05-22T21:23:09.9182495Z 20/05/22 21:19:52 INFO namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-hdfs/dfs/name/current/fsimage_0000000000000000000
2020-05-22T21:23:09.9183106Z 20/05/22 21:19:52 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-05-22T21:23:09.9184636Z 20/05/22 21:19:52 INFO namenode.FSEditLog: Starting log segment at 1
2020-05-22T21:23:09.9185086Z 20/05/22 21:19:52 INFO namenode.NameCache: initialized with 0 entries 0 lookups
2020-05-22T21:23:09.9185525Z 20/05/22 21:19:52 INFO namenode.FSNamesystem: Finished loading FSImage in 871 msecs
2020-05-22T21:23:09.9186545Z 20/05/22 21:19:53 INFO namenode.NameNode: RPC server is binding to master.docker-hadoop-cluster-network:9000
2020-05-22T21:23:09.9187286Z 20/05/22 21:19:53 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 10000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-05-22T21:23:09.9187944Z 20/05/22 21:19:53 INFO ipc.Server: Starting Socket Reader #1 for port 9000
2020-05-22T21:23:09.9188371Z 20/05/22 21:19:53 INFO ipc.Server: Starting Socket Reader #2 for port 9000
2020-05-22T21:23:09.9188777Z 20/05/22 21:19:53 INFO ipc.Server: Starting Socket Reader #3 for port 9000
2020-05-22T21:23:09.9189306Z 20/05/22 21:19:53 INFO ipc.Server: Starting Socket Reader #4 for port 9000
2020-05-22T21:23:09.9189710Z 20/05/22 21:19:53 INFO ipc.Server: Starting Socket Reader #5 for port 9000
2020-05-22T21:23:09.9190145Z 20/05/22 21:19:53 INFO namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-05-22T21:23:09.9190611Z 20/05/22 21:19:53 INFO namenode.LeaseManager: Number of blocks under construction: 0
2020-05-22T21:23:09.9191096Z 20/05/22 21:19:53 INFO blockmanagement.BlockManager: initializing replication queues
2020-05-22T21:23:09.9191552Z 20/05/22 21:19:53 INFO hdfs.StateChange: STATE* Leaving safe mode after 2 secs
2020-05-22T21:23:09.9191999Z 20/05/22 21:19:53 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-05-22T21:23:09.9192471Z 20/05/22 21:19:53 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-05-22T21:23:09.9193010Z 20/05/22 21:19:53 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2020-05-22T21:23:09.9194089Z 20/05/22 21:19:53 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2020-05-22T21:23:09.9194795Z 20/05/22 21:19:53 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2020-05-22T21:23:09.9195361Z 20/05/22 21:19:53 INFO blockmanagement.BlockManager: Total number of blocks            = 0
2020-05-22T21:23:09.9195850Z 20/05/22 21:19:53 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-05-22T21:23:09.9196651Z 20/05/22 21:19:53 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0
2020-05-22T21:23:09.9197372Z 20/05/22 21:19:53 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-05-22T21:23:09.9197866Z 20/05/22 21:19:53 INFO blockmanagement.BlockManager: Number of blocks being written    = 0
2020-05-22T21:23:09.9198723Z 20/05/22 21:19:53 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 88 msec
2020-05-22T21:23:09.9199292Z 20/05/22 21:19:53 INFO ipc.Server: IPC Server Responder: starting
2020-05-22T21:23:09.9199710Z 20/05/22 21:19:54 INFO ipc.Server: IPC Server listener on 9000: starting
2020-05-22T21:23:09.9200484Z 20/05/22 21:19:54 INFO namenode.NameNode: NameNode RPC up at: master.docker-hadoop-cluster-network/172.22.0.3:9000
2020-05-22T21:23:09.9201023Z 20/05/22 21:19:54 INFO namenode.FSNamesystem: Starting services required for active state
2020-05-22T21:23:09.9201473Z 20/05/22 21:19:54 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)
2020-05-22T21:23:09.9201948Z 20/05/22 21:19:54 INFO namenode.FSDirectory: Quota initialization completed in 30 milliseconds
2020-05-22T21:23:09.9202276Z name space=1
2020-05-22T21:23:09.9202472Z storage space=0
2020-05-22T21:23:09.9202718Z storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-05-22T21:23:09.9203197Z 20/05/22 21:19:55 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-05-22T21:23:09.9204669Z 20/05/22 21:19:55 INFO ipc.Server: Auth successful for hdfs/slave2.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9205779Z 20/05/22 21:19:55 INFO ipc.Server: Auth successful for hdfs/slave1.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9206997Z 20/05/22 21:19:55 INFO authorize.ServiceAuthorizationManager: Authorization successful for hdfs/slave1.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol
2020-05-22T21:23:09.9208403Z 20/05/22 21:19:55 INFO authorize.ServiceAuthorizationManager: Authorization successful for hdfs/slave2.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol
2020-05-22T21:23:09.9210110Z 20/05/22 21:19:56 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.22.0.5:50010, datanodeUuid=7c82e560-f8ac-43c9-a1a3-976cce1afd79, infoPort=0, infoSecurePort=50475, ipcPort=50020, storageInfo=lv=-57;cid=CID-6e253d18-f0f3-446b-a39f-e6aa75c5db3b;nsid=1964552002;c=1590182373027) storage 7c82e560-f8ac-43c9-a1a3-976cce1afd79
2020-05-22T21:23:09.9211569Z 20/05/22 21:19:56 INFO net.NetworkTopology: Adding a new node: /default-rack/172.22.0.5:50010
2020-05-22T21:23:09.9212539Z 20/05/22 21:19:56 INFO blockmanagement.BlockReportLeaseManager: Registered DN 7c82e560-f8ac-43c9-a1a3-976cce1afd79 (172.22.0.5:50010).
2020-05-22T21:23:09.9214108Z 20/05/22 21:19:56 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.22.0.4:50010, datanodeUuid=8b7bd3e9-0431-4115-a271-cfe2f65d3324, infoPort=0, infoSecurePort=50475, ipcPort=50020, storageInfo=lv=-57;cid=CID-6e253d18-f0f3-446b-a39f-e6aa75c5db3b;nsid=1964552002;c=1590182373027) storage 8b7bd3e9-0431-4115-a271-cfe2f65d3324
2020-05-22T21:23:09.9215306Z 20/05/22 21:19:56 INFO net.NetworkTopology: Adding a new node: /default-rack/172.22.0.4:50010
2020-05-22T21:23:09.9216191Z 20/05/22 21:19:56 INFO blockmanagement.BlockReportLeaseManager: Registered DN 8b7bd3e9-0431-4115-a271-cfe2f65d3324 (172.22.0.4:50010).
2020-05-22T21:23:09.9217210Z 20/05/22 21:19:56 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-fba6d579-dbd2-4569-b80a-bac9254c9d42 for DN 172.22.0.4:50010
2020-05-22T21:23:09.9218257Z 20/05/22 21:19:56 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-48a77302-0c5e-4523-9509-f1b4a82437d3 for DN 172.22.0.5:50010
2020-05-22T21:23:09.9219422Z 20/05/22 21:19:56 INFO BlockStateChange: BLOCK* processReport 0xb0a0dbda3c155444: Processing first storage report for DS-48a77302-0c5e-4523-9509-f1b4a82437d3 from datanode 7c82e560-f8ac-43c9-a1a3-976cce1afd79
2020-05-22T21:23:09.9221866Z 20/05/22 21:19:56 INFO BlockStateChange: BLOCK* processReport 0xb0a0dbda3c155444: from storage DS-48a77302-0c5e-4523-9509-f1b4a82437d3 node DatanodeRegistration(172.22.0.5:50010, datanodeUuid=7c82e560-f8ac-43c9-a1a3-976cce1afd79, infoPort=0, infoSecurePort=50475, ipcPort=50020, storageInfo=lv=-57;cid=CID-6e253d18-f0f3-446b-a39f-e6aa75c5db3b;nsid=1964552002;c=1590182373027), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-05-22T21:23:09.9223887Z 20/05/22 21:19:56 INFO BlockStateChange: BLOCK* processReport 0x36339d36c758be17: Processing first storage report for DS-fba6d579-dbd2-4569-b80a-bac9254c9d42 from datanode 8b7bd3e9-0431-4115-a271-cfe2f65d3324
2020-05-22T21:23:09.9226000Z 20/05/22 21:19:56 INFO BlockStateChange: BLOCK* processReport 0x36339d36c758be17: from storage DS-fba6d579-dbd2-4569-b80a-bac9254c9d42 node DatanodeRegistration(172.22.0.4:50010, datanodeUuid=8b7bd3e9-0431-4115-a271-cfe2f65d3324, infoPort=0, infoSecurePort=50475, ipcPort=50020, storageInfo=lv=-57;cid=CID-6e253d18-f0f3-446b-a39f-e6aa75c5db3b;nsid=1964552002;c=1590182373027), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-05-22T21:23:09.9227309Z 20/05/22 21:19:59 INFO ipc.Server: Auth successful for root@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9227997Z 20/05/22 21:19:59 INFO authorize.ServiceAuthorizationManager: Authorization successful for root@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9228825Z 20/05/22 21:20:01 INFO ipc.Server: Auth successful for root@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9229528Z 20/05/22 21:20:01 INFO authorize.ServiceAuthorizationManager: Authorization successful for root@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9230582Z 20/05/22 21:20:02 INFO ipc.Server: Auth successful for mapred/master.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9231772Z 20/05/22 21:20:02 INFO authorize.ServiceAuthorizationManager: Authorization successful for mapred/master.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9233462Z 20/05/22 21:20:02 INFO ipc.Server: IPC Server handler 17 on 9000, call Call#2 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.22.0.3:44561: org.apache.hadoop.security.AccessControlException: Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
2020-05-22T21:23:09.9234379Z 20/05/22 21:20:04 INFO ipc.Server: Auth successful for root@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9235065Z 20/05/22 21:20:04 INFO authorize.ServiceAuthorizationManager: Authorization successful for root@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9235766Z 20/05/22 21:20:06 INFO ipc.Server: Auth successful for root@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9236463Z 20/05/22 21:20:06 INFO authorize.ServiceAuthorizationManager: Authorization successful for root@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9237152Z 20/05/22 21:20:08 INFO ipc.Server: Auth successful for root@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9237848Z 20/05/22 21:20:08 INFO authorize.ServiceAuthorizationManager: Authorization successful for root@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9238547Z 20/05/22 21:20:10 INFO ipc.Server: Auth successful for root@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9239225Z 20/05/22 21:20:10 INFO authorize.ServiceAuthorizationManager: Authorization successful for root@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9239923Z 20/05/22 21:20:12 INFO ipc.Server: Auth successful for root@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9240598Z 20/05/22 21:20:12 INFO authorize.ServiceAuthorizationManager: Authorization successful for root@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9241299Z 20/05/22 21:20:14 INFO ipc.Server: Auth successful for root@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9241994Z 20/05/22 21:20:15 INFO authorize.ServiceAuthorizationManager: Authorization successful for root@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9242677Z 20/05/22 21:20:17 INFO ipc.Server: Auth successful for root@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9243369Z 20/05/22 21:20:17 INFO authorize.ServiceAuthorizationManager: Authorization successful for root@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9244059Z 20/05/22 21:20:19 INFO ipc.Server: Auth successful for root@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9244737Z 20/05/22 21:20:19 INFO authorize.ServiceAuthorizationManager: Authorization successful for root@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9245434Z 20/05/22 21:20:21 INFO ipc.Server: Auth successful for root@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9246108Z 20/05/22 21:20:21 INFO authorize.ServiceAuthorizationManager: Authorization successful for root@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9247168Z 20/05/22 21:20:50 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9248223Z 20/05/22 21:20:50 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9249430Z 20/05/22 21:20:51 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=172.22.0.5:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-1.2-api-2.12.1.jar
2020-05-22T21:23:09.9250671Z 20/05/22 21:20:51 INFO namenode.FSNamesystem: BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-1.2-api-2.12.1.jar
2020-05-22T21:23:09.9252223Z 20/05/22 21:20:51 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-1.2-api-2.12.1.jar is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9253110Z 20/05/22 21:20:51 INFO namenode.FSEditLog: Number of transactions: 21 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 2 Number of syncs: 19 SyncTimes(ms): 24 
2020-05-22T21:23:09.9254258Z 20/05/22 21:20:51 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-1.2-api-2.12.1.jar
2020-05-22T21:23:09.9255384Z 20/05/22 21:20:51 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=172.22.0.5:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-core-2.12.1.jar
2020-05-22T21:23:09.9256603Z 20/05/22 21:20:52 INFO namenode.FSNamesystem: BLOCK* blk_1073741826_1002 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-core-2.12.1.jar
2020-05-22T21:23:09.9257790Z 20/05/22 21:20:52 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-core-2.12.1.jar is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9258859Z 20/05/22 21:20:53 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-core-2.12.1.jar
2020-05-22T21:23:09.9260005Z 20/05/22 21:20:53 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=172.22.0.5:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/flink-table-blink_2.11-1.12-SNAPSHOT.jar
2020-05-22T21:23:09.9261312Z 20/05/22 21:20:59 INFO namenode.FSNamesystem: BLOCK* blk_1073741827_1003 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/hadoop-user/.flink/application_1590182392090_0001/lib/flink-table-blink_2.11-1.12-SNAPSHOT.jar
2020-05-22T21:23:09.9262569Z 20/05/22 21:20:59 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/lib/flink-table-blink_2.11-1.12-SNAPSHOT.jar is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9263708Z 20/05/22 21:20:59 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/flink-table-blink_2.11-1.12-SNAPSHOT.jar
2020-05-22T21:23:09.9264881Z 20/05/22 21:20:59 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=172.22.0.4:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/flink-table_2.11-1.12-SNAPSHOT.jar
2020-05-22T21:23:09.9266160Z 20/05/22 21:21:03 INFO namenode.FSNamesystem: BLOCK* blk_1073741828_1004 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/hadoop-user/.flink/application_1590182392090_0001/lib/flink-table_2.11-1.12-SNAPSHOT.jar
2020-05-22T21:23:09.9267386Z 20/05/22 21:21:03 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/lib/flink-table_2.11-1.12-SNAPSHOT.jar is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9268652Z 20/05/22 21:21:03 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/flink-table_2.11-1.12-SNAPSHOT.jar
2020-05-22T21:23:09.9269816Z 20/05/22 21:21:03 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=172.22.0.4:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/flink-shaded-zookeeper-3.4.10.jar
2020-05-22T21:23:09.9271007Z 20/05/22 21:21:04 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/lib/flink-shaded-zookeeper-3.4.10.jar is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9272230Z 20/05/22 21:21:04 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/flink-shaded-zookeeper-3.4.10.jar
2020-05-22T21:23:09.9273913Z 20/05/22 21:21:04 INFO hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=172.22.0.5:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-api-2.12.1.jar
2020-05-22T21:23:09.9275185Z 20/05/22 21:21:04 INFO namenode.FSNamesystem: BLOCK* blk_1073741830_1006 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-api-2.12.1.jar
2020-05-22T21:23:09.9276364Z 20/05/22 21:21:05 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-api-2.12.1.jar is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9277432Z 20/05/22 21:21:05 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-api-2.12.1.jar
2020-05-22T21:23:09.9278555Z 20/05/22 21:21:05 INFO hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=172.22.0.5:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-slf4j-impl-2.12.1.jar
2020-05-22T21:23:09.9279740Z 20/05/22 21:21:05 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-slf4j-impl-2.12.1.jar is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9280847Z 20/05/22 21:21:05 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/lib/log4j-slf4j-impl-2.12.1.jar
2020-05-22T21:23:09.9281929Z 20/05/22 21:21:05 INFO hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=172.22.0.5:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/log4j.properties
2020-05-22T21:23:09.9283044Z 20/05/22 21:21:05 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/log4j.properties is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9284078Z 20/05/22 21:21:05 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/log4j.properties
2020-05-22T21:23:09.9285220Z 20/05/22 21:21:05 INFO hdfs.StateChange: BLOCK* allocate blk_1073741833_1009, replicas=172.22.0.5:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/plugins/another-dummy-fs/flink-another-dummy-fs.jar
2020-05-22T21:23:09.9286556Z 20/05/22 21:21:05 INFO namenode.FSNamesystem: BLOCK* blk_1073741833_1009 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/hadoop-user/.flink/application_1590182392090_0001/plugins/another-dummy-fs/flink-another-dummy-fs.jar
2020-05-22T21:23:09.9287844Z 20/05/22 21:21:05 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/plugins/another-dummy-fs/flink-another-dummy-fs.jar is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9289005Z 20/05/22 21:21:05 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/plugins/another-dummy-fs/flink-another-dummy-fs.jar
2020-05-22T21:23:09.9290181Z 20/05/22 21:21:05 INFO hdfs.StateChange: BLOCK* allocate blk_1073741834_1010, replicas=172.22.0.5:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/plugins/dummy-fs/flink-dummy-fs.jar
2020-05-22T21:23:09.9291734Z 20/05/22 21:21:05 INFO namenode.FSNamesystem: BLOCK* blk_1073741834_1010 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/hadoop-user/.flink/application_1590182392090_0001/plugins/dummy-fs/flink-dummy-fs.jar
2020-05-22T21:23:09.9292989Z 20/05/22 21:21:06 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/plugins/dummy-fs/flink-dummy-fs.jar is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9294110Z 20/05/22 21:21:06 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/plugins/dummy-fs/flink-dummy-fs.jar
2020-05-22T21:23:09.9295451Z 20/05/22 21:21:06 INFO hdfs.StateChange: BLOCK* allocate blk_1073741835_1011, replicas=172.22.0.4:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/plugins/metrics_jmx/flink-metrics-jmx-1.12-SNAPSHOT.jar
2020-05-22T21:23:09.9296724Z 20/05/22 21:21:06 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/plugins/metrics_jmx/flink-metrics-jmx-1.12-SNAPSHOT.jar is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9297907Z 20/05/22 21:21:06 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/plugins/metrics_jmx/flink-metrics-jmx-1.12-SNAPSHOT.jar
2020-05-22T21:23:09.9299045Z 20/05/22 21:21:06 INFO hdfs.StateChange: BLOCK* allocate blk_1073741836_1012, replicas=172.22.0.5:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/plugins/README.txt
2020-05-22T21:23:09.9300180Z 20/05/22 21:21:06 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/plugins/README.txt is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9301203Z 20/05/22 21:21:06 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/plugins/README.txt
2020-05-22T21:23:09.9302280Z 20/05/22 21:21:06 INFO hdfs.StateChange: BLOCK* allocate blk_1073741837_1013, replicas=172.22.0.5:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/WordCount.jar
2020-05-22T21:23:09.9303435Z 20/05/22 21:21:06 INFO namenode.FSNamesystem: BLOCK* blk_1073741837_1013 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/hadoop-user/.flink/application_1590182392090_0001/WordCount.jar
2020-05-22T21:23:09.9304557Z 20/05/22 21:21:06 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/WordCount.jar is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9305580Z 20/05/22 21:21:06 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/WordCount.jar
2020-05-22T21:23:09.9306681Z 20/05/22 21:21:06 INFO hdfs.StateChange: BLOCK* allocate blk_1073741838_1014, replicas=172.22.0.4:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/flink-dist_2.11-1.12-SNAPSHOT.jar
2020-05-22T21:23:09.9307628Z 20/05/22 21:21:18 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9308685Z 20/05/22 21:21:18 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9309881Z 20/05/22 21:21:18 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/flink-dist_2.11-1.12-SNAPSHOT.jar is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9310990Z 20/05/22 21:21:18 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/flink-dist_2.11-1.12-SNAPSHOT.jar
2020-05-22T21:23:09.9312231Z 20/05/22 21:21:18 INFO hdfs.StateChange: BLOCK* allocate blk_1073741839_1015, replicas=172.22.0.5:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/application_1590182392090_0001-flink-conf.yaml7266271465280845379.tmp
2020-05-22T21:23:09.9313664Z 20/05/22 21:21:18 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/application_1590182392090_0001-flink-conf.yaml7266271465280845379.tmp is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9314924Z 20/05/22 21:21:18 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/application_1590182392090_0001-flink-conf.yaml7266271465280845379.tmp
2020-05-22T21:23:09.9316193Z 20/05/22 21:21:18 INFO hdfs.StateChange: BLOCK* allocate blk_1073741840_1016, replicas=172.22.0.5:50010 for /user/hadoop-user/.flink/application_1590182392090_0001/hadoop-user.keytab
2020-05-22T21:23:09.9317299Z 20/05/22 21:21:18 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/.flink/application_1590182392090_0001/hadoop-user.keytab is closed by DFSClient_NONMAPREDUCE_331660630_1
2020-05-22T21:23:09.9318341Z 20/05/22 21:21:18 INFO namenode.FSDirectory: Increasing replication from 1 to 1 for /user/hadoop-user/.flink/application_1590182392090_0001/hadoop-user.keytab
2020-05-22T21:23:09.9319416Z 20/05/22 21:21:18 INFO delegation.AbstractDelegationTokenSecretManager: Creating password for identifier: (HDFS_DELEGATION_TOKEN token 1 for hadoop-user), currentKey: 2
2020-05-22T21:23:09.9320397Z 20/05/22 21:21:20 INFO ipc.Server: Auth successful for yarn/master.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9321569Z 20/05/22 21:21:20 INFO authorize.ServiceAuthorizationManager: Authorization successful for yarn/master.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9322842Z 20/05/22 21:21:20 INFO delegation.AbstractDelegationTokenSecretManager: Token renewal for identifier: (HDFS_DELEGATION_TOKEN token 1 for hadoop-user); total currentTokens 1
2020-05-22T21:23:09.9323818Z 20/05/22 21:21:21 INFO ipc.Server: Auth successful for yarn/slave2.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9324997Z 20/05/22 21:21:21 INFO authorize.ServiceAuthorizationManager: Authorization successful for yarn/slave2.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9325992Z 20/05/22 21:21:21 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:TOKEN)
2020-05-22T21:23:09.9327015Z 20/05/22 21:21:21 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9327964Z 20/05/22 21:21:23 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:TOKEN)
2020-05-22T21:23:09.9329000Z 20/05/22 21:21:23 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9329931Z 20/05/22 21:21:37 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:TOKEN)
2020-05-22T21:23:09.9330970Z 20/05/22 21:21:37 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9332065Z 20/05/22 21:21:53 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9333134Z 20/05/22 21:21:53 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9334075Z 20/05/22 21:21:53 INFO namenode.FSEditLog: Number of transactions: 125 Total time for transactions(ms): 27 Number of transactions batched in Syncs: 25 Number of syncs: 100 SyncTimes(ms): 1225 
2020-05-22T21:23:09.9335235Z 20/05/22 21:21:58 INFO ipc.Server: Auth successful for yarn/slave1.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9336419Z 20/05/22 21:21:58 INFO authorize.ServiceAuthorizationManager: Authorization successful for yarn/slave1.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9337414Z 20/05/22 21:21:58 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:TOKEN)
2020-05-22T21:23:09.9338438Z 20/05/22 21:21:58 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9339499Z 20/05/22 21:22:02 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:TOKEN)
2020-05-22T21:23:09.9340538Z 20/05/22 21:22:02 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9341466Z 20/05/22 21:22:21 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:TOKEN)
2020-05-22T21:23:09.9342501Z 20/05/22 21:22:22 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9343432Z 20/05/22 21:22:43 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9344482Z 20/05/22 21:22:43 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9345443Z 20/05/22 21:22:57 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9346472Z 20/05/22 21:22:57 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9347408Z 20/05/22 21:22:57 INFO namenode.FSEditLog: Number of transactions: 126 Total time for transactions(ms): 27 Number of transactions batched in Syncs: 25 Number of syncs: 101 SyncTimes(ms): 1226 
2020-05-22T21:23:09.9348376Z 20/05/22 21:22:58 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9349405Z 20/05/22 21:22:58 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9350498Z 20/05/22 21:23:03 INFO hdfs.StateChange: BLOCK* allocate blk_1073741841_1017, replicas=172.22.0.4:50010 for /user/hadoop-user/wc-out-26536/3
2020-05-22T21:23:09.9351474Z 20/05/22 21:23:03 INFO hdfs.StateChange: BLOCK* allocate blk_1073741842_1018, replicas=172.22.0.5:50010 for /user/hadoop-user/wc-out-26536/1
2020-05-22T21:23:09.9352421Z 20/05/22 21:23:04 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/wc-out-26536/3 is closed by DFSClient_NONMAPREDUCE_-646397215_66
2020-05-22T21:23:09.9353363Z 20/05/22 21:23:04 INFO hdfs.StateChange: DIR* completeFile: /user/hadoop-user/wc-out-26536/1 is closed by DFSClient_NONMAPREDUCE_-587498064_63
2020-05-22T21:23:09.9354172Z 20/05/22 21:23:04 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9355218Z 20/05/22 21:23:04 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9356171Z 20/05/22 21:23:07 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:TOKEN)
2020-05-22T21:23:09.9357188Z 20/05/22 21:23:07 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9358257Z 20/05/22 21:23:07 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:TOKEN)
2020-05-22T21:23:09.9359300Z 20/05/22 21:23:07 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2020-05-22T21:23:09.9360547Z 20/05/22 21:23:08 INFO hdfs.StateChange: BLOCK* allocate blk_1073741843_1019, replicas=172.22.0.5:50010 for /tmp/logs/hadoop-user/logs/application_1590182392090_0001/slave2.docker-hadoop-cluster-network_41369.tmp
2020-05-22T21:23:09.9361929Z 20/05/22 21:23:08 INFO hdfs.StateChange: BLOCK* allocate blk_1073741844_1020, replicas=172.22.0.4:50010 for /tmp/logs/hadoop-user/logs/application_1590182392090_0001/slave1.docker-hadoop-cluster-network_42227.tmp
2020-05-22T21:23:09.9363249Z 20/05/22 21:23:08 INFO namenode.FSNamesystem: BLOCK* blk_1073741843_1019 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /tmp/logs/hadoop-user/logs/application_1590182392090_0001/slave2.docker-hadoop-cluster-network_41369.tmp
2020-05-22T21:23:09.9364519Z 20/05/22 21:23:08 INFO hdfs.StateChange: DIR* completeFile: /tmp/logs/hadoop-user/logs/application_1590182392090_0001/slave1.docker-hadoop-cluster-network_42227.tmp is closed by DFSClient_NONMAPREDUCE_1898733664_91
2020-05-22T21:23:09.9365760Z 20/05/22 21:23:08 INFO hdfs.StateChange: DIR* completeFile: /tmp/logs/hadoop-user/logs/application_1590182392090_0001/slave2.docker-hadoop-cluster-network_41369.tmp is closed by DFSClient_NONMAPREDUCE_-1596104169_91
2020-05-22T21:23:09.9366734Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-22195494660/logs/hadoop/namenode.out:
2020-05-22T21:23:09.9367518Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-22195494660/logs/hadoop/resourcemanager.err:
2020-05-22T21:23:09.9368025Z 20/05/22 21:19:39 INFO resourcemanager.ResourceManager: STARTUP_MSG: 
2020-05-22T21:23:09.9368431Z /************************************************************
2020-05-22T21:23:09.9368734Z STARTUP_MSG: Starting ResourceManager
2020-05-22T21:23:09.9369003Z STARTUP_MSG:   user = yarn
2020-05-22T21:23:09.9369513Z STARTUP_MSG:   host = master.docker-hadoop-cluster-network/172.22.0.3
2020-05-22T21:23:09.9369836Z STARTUP_MSG:   args = []
2020-05-22T21:23:09.9370074Z STARTUP_MSG:   version = 2.8.4
2020-05-22T21:23:09.9424828Z STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/etc/hadoop/rm-config/log4j.properties
2020-05-22T21:23:09.9506435Z STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 17e75c2a11685af3e043aa5e604dc831e5b14674; compiled by 'jdu' on 2018-05-08T02:50Z
2020-05-22T21:23:09.9507039Z STARTUP_MSG:   java = 1.8.0_131
2020-05-22T21:23:09.9507362Z ************************************************************/
2020-05-22T21:23:09.9507798Z 20/05/22 21:19:39 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
2020-05-22T21:23:09.9508698Z 20/05/22 21:19:43 INFO conf.Configuration: found resource core-site.xml at file:/usr/local/hadoop-2.8.4/etc/hadoop/core-site.xml
2020-05-22T21:23:09.9509215Z 20/05/22 21:19:47 INFO security.Groups: clearing userToGroupsMap cache
2020-05-22T21:23:09.9511116Z 20/05/22 21:19:48 INFO conf.Configuration: found resource yarn-site.xml at file:/usr/local/hadoop-2.8.4/etc/hadoop/yarn-site.xml
2020-05-22T21:23:09.9512171Z 20/05/22 21:19:49 INFO security.UserGroupInformation: Login successful for user yarn/master.docker-hadoop-cluster-network@EXAMPLE.COM using keytab file /etc/security/keytabs/yarn.keytab
2020-05-22T21:23:09.9513023Z 20/05/22 21:19:49 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2020-05-22T21:23:09.9514015Z 20/05/22 21:19:49 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2020-05-22T21:23:09.9514770Z 20/05/22 21:19:49 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2020-05-22T21:23:09.9515508Z 20/05/22 21:19:49 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2020-05-22T21:23:09.9516429Z 20/05/22 21:19:49 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2020-05-22T21:23:09.9517357Z 20/05/22 21:19:49 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2020-05-22T21:23:09.9518156Z 20/05/22 21:19:49 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2020-05-22T21:23:09.9518990Z 20/05/22 21:19:49 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher
2020-05-22T21:23:09.9520009Z 20/05/22 21:19:49 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2020-05-22T21:23:09.9520991Z 20/05/22 21:19:49 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2020-05-22T21:23:09.9521969Z 20/05/22 21:19:49 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2020-05-22T21:23:09.9522963Z 20/05/22 21:19:49 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-05-22T21:23:09.9523468Z 20/05/22 21:19:50 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2020-05-22T21:23:09.9523961Z 20/05/22 21:19:50 INFO impl.MetricsSystemImpl: ResourceManager metrics system started
2020-05-22T21:23:09.9524603Z 20/05/22 21:19:50 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2020-05-22T21:23:09.9525507Z 20/05/22 21:19:50 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2020-05-22T21:23:09.9526207Z 20/05/22 21:19:50 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2020-05-22T21:23:09.9526729Z 20/05/22 21:19:50 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instiantiated.
2020-05-22T21:23:09.9527280Z 20/05/22 21:19:50 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
2020-05-22T21:23:09.9528166Z 20/05/22 21:19:50 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/usr/local/hadoop-2.8.4/etc/hadoop/capacity-scheduler.xml
2020-05-22T21:23:09.9528793Z 20/05/22 21:19:50 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2020-05-22T21:23:09.9529353Z 20/05/22 21:19:50 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2020-05-22T21:23:09.9530667Z 20/05/22 21:19:50 INFO capacity.ParentQueue: root, capacity=1.0, asboluteCapacity=1.0, maxCapacity=1.0, asboluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,, offswitchPerHeartbeatLimit = 1, reservationsContinueLooking=true
2020-05-22T21:23:09.9532026Z 20/05/22 21:19:50 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2020-05-22T21:23:09.9532601Z 20/05/22 21:19:50 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2020-05-22T21:23:09.9533192Z 20/05/22 21:19:50 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2020-05-22T21:23:09.9533791Z 20/05/22 21:19:50 INFO capacity.LeafQueue: Initializing default
2020-05-22T21:23:09.9534152Z capacity = 1.0 [= (float) configuredCapacity / 100 ]
2020-05-22T21:23:09.9534494Z asboluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
2020-05-22T21:23:09.9534808Z maxCapacity = 1.0 [= configuredMaxCapacity ]
2020-05-22T21:23:09.9535230Z absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
2020-05-22T21:23:09.9535627Z userLimit = 100 [= configuredUserLimit ]
2020-05-22T21:23:09.9535932Z userLimitFactor = 1.0 [= configuredUserLimitFactor ]
2020-05-22T21:23:09.9536373Z maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
2020-05-22T21:23:09.9536908Z maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
2020-05-22T21:23:09.9537369Z usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
2020-05-22T21:23:09.9537769Z absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
2020-05-22T21:23:09.9538163Z maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
2020-05-22T21:23:09.9538930Z minimumAllocationFactor = 0.8779297 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
2020-05-22T21:23:09.9539433Z maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]
2020-05-22T21:23:09.9539795Z numContainers = 0 [= currentNumContainers ]
2020-05-22T21:23:09.9540060Z state = RUNNING [= configuredState ]
2020-05-22T21:23:09.9540383Z acls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]
2020-05-22T21:23:09.9540668Z nodeLocalityDelay = 40
2020-05-22T21:23:09.9540874Z labels=*,
2020-05-22T21:23:09.9541076Z reservationsContinueLooking = true
2020-05-22T21:23:09.9541325Z preemptionDisabled = true
2020-05-22T21:23:09.9541555Z defaultAppPriorityPerQueue = 0
2020-05-22T21:23:09.9542186Z 20/05/22 21:19:50 INFO capacity.CapacityScheduler: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0
2020-05-22T21:23:09.9543183Z 20/05/22 21:19:50 INFO capacity.CapacityScheduler: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2020-05-22T21:23:09.9544152Z 20/05/22 21:19:50 INFO capacity.CapacityScheduler: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2020-05-22T21:23:09.9544919Z 20/05/22 21:19:50 INFO capacity.CapacityScheduler: Initialized queue mappings, override: false
2020-05-22T21:23:09.9545892Z 20/05/22 21:19:50 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1000, vCores:1>>, maximumAllocation=<<memory:8192, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2020-05-22T21:23:09.9547053Z 20/05/22 21:19:50 INFO conf.Configuration: dynamic-resources.xml not found
2020-05-22T21:23:09.9547865Z 20/05/22 21:19:50 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher$ForwardingEventHandler
2020-05-22T21:23:09.9548880Z 20/05/22 21:19:50 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher$ForwardingEventHandler
2020-05-22T21:23:09.9549891Z 20/05/22 21:19:50 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher$ForwardingEventHandler
2020-05-22T21:23:09.9550924Z 20/05/22 21:19:50 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher$ForwardingEventHandler
2020-05-22T21:23:09.9551924Z 20/05/22 21:19:50 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher$ForwardingEventHandler
2020-05-22T21:23:09.9552917Z 20/05/22 21:19:50 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher$ForwardingEventHandler
2020-05-22T21:23:09.9553909Z 20/05/22 21:19:50 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher$ForwardingEventHandler
2020-05-22T21:23:09.9554885Z 20/05/22 21:19:50 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher$ForwardingEventHandler
2020-05-22T21:23:09.9555877Z 20/05/22 21:19:50 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher$ForwardingEventHandler
2020-05-22T21:23:09.9556870Z 20/05/22 21:19:50 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher$ForwardingEventHandler
2020-05-22T21:23:09.9557618Z 20/05/22 21:19:50 INFO metrics.SystemMetricsPublisher: YARN system metrics publishing service is enabled
2020-05-22T21:23:09.9558570Z 20/05/22 21:19:52 INFO impl.TimelineClientImpl: Timeline service address: http://master.docker-hadoop-cluster-network:8188/ws/v1/timeline/
2020-05-22T21:23:09.9559180Z 20/05/22 21:19:52 INFO resourcemanager.ResourceManager: Transitioning to active state
2020-05-22T21:23:09.9559615Z 20/05/22 21:19:52 INFO util.JvmPauseMonitor: Starting JVM pause monitor
2020-05-22T21:23:09.9560030Z 20/05/22 21:19:52 INFO recovery.RMStateStore: Updating AMRMToken
2020-05-22T21:23:09.9560725Z 20/05/22 21:19:52 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2020-05-22T21:23:09.9561465Z 20/05/22 21:19:52 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2020-05-22T21:23:09.9562046Z 20/05/22 21:19:52 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2020-05-22T21:23:09.9562622Z 20/05/22 21:19:52 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1
2020-05-22T21:23:09.9563088Z 20/05/22 21:19:52 INFO recovery.RMStateStore: Storing RMDTMasterKey.
2020-05-22T21:23:09.9563736Z 20/05/22 21:19:52 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
2020-05-22T21:23:09.9564640Z 20/05/22 21:19:52 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2020-05-22T21:23:09.9565330Z 20/05/22 21:19:52 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2020-05-22T21:23:09.9565896Z 20/05/22 21:19:52 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2
2020-05-22T21:23:09.9566417Z 20/05/22 21:19:52 INFO recovery.RMStateStore: Storing RMDTMasterKey.
2020-05-22T21:23:09.9567071Z 20/05/22 21:19:52 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-05-22T21:23:09.9567718Z 20/05/22 21:19:52 INFO ipc.Server: Starting Socket Reader #1 for port 8031
2020-05-22T21:23:09.9568243Z 20/05/22 21:19:52 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
2020-05-22T21:23:09.9569180Z 20/05/22 21:19:52 INFO conf.Configuration: found resource hadoop-policy.xml at file:/usr/local/hadoop-2.8.4/etc/hadoop/hadoop-policy.xml
2020-05-22T21:23:09.9569729Z 20/05/22 21:19:53 INFO ipc.Server: IPC Server Responder: starting
2020-05-22T21:23:09.9570162Z 20/05/22 21:19:53 INFO ipc.Server: IPC Server listener on 8031: starting
2020-05-22T21:23:09.9570810Z 20/05/22 21:19:53 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-05-22T21:23:09.9571628Z 20/05/22 21:19:53 INFO ipc.Server: Starting Socket Reader #1 for port 8030
2020-05-22T21:23:09.9572148Z 20/05/22 21:19:53 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
2020-05-22T21:23:09.9573130Z 20/05/22 21:19:53 INFO conf.Configuration: found resource hadoop-policy.xml at file:/usr/local/hadoop-2.8.4/etc/hadoop/hadoop-policy.xml
2020-05-22T21:23:09.9573678Z 20/05/22 21:19:53 INFO ipc.Server: IPC Server Responder: starting
2020-05-22T21:23:09.9574094Z 20/05/22 21:19:53 INFO ipc.Server: IPC Server listener on 8030: starting
2020-05-22T21:23:09.9574897Z 20/05/22 21:19:54 INFO ipc.Server: Auth successful for yarn/slave2.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9576079Z 20/05/22 21:19:54 INFO authorize.ServiceAuthorizationManager: Authorization successful for yarn/slave2.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.server.api.ResourceTrackerPB
2020-05-22T21:23:09.9577182Z 20/05/22 21:19:54 INFO ipc.Server: Auth successful for yarn/slave1.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9578379Z 20/05/22 21:19:54 INFO authorize.ServiceAuthorizationManager: Authorization successful for yarn/slave1.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.server.api.ResourceTrackerPB
2020-05-22T21:23:09.9579320Z 20/05/22 21:19:54 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-05-22T21:23:09.9579973Z 20/05/22 21:19:54 INFO ipc.Server: Starting Socket Reader #1 for port 8032
2020-05-22T21:23:09.9580501Z 20/05/22 21:19:54 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
2020-05-22T21:23:09.9581414Z 20/05/22 21:19:54 INFO conf.Configuration: found resource hadoop-policy.xml at file:/usr/local/hadoop-2.8.4/etc/hadoop/hadoop-policy.xml
2020-05-22T21:23:09.9581961Z 20/05/22 21:19:54 INFO ipc.Server: IPC Server Responder: starting
2020-05-22T21:23:09.9582395Z 20/05/22 21:19:54 INFO ipc.Server: IPC Server listener on 8032: starting
2020-05-22T21:23:09.9582943Z 20/05/22 21:19:54 INFO resourcemanager.ResourceManager: Transitioned to active state
2020-05-22T21:23:09.9583808Z 20/05/22 21:19:55 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:23:09.9585139Z 20/05/22 21:19:55 INFO resourcemanager.ResourceTrackerService: NodeManager from node slave1.docker-hadoop-cluster-network(cmPort: 42227 httpPort: 8042) registered with capability: <memory:2500, vCores:1>, assigned nodeId slave1.docker-hadoop-cluster-network:42227
2020-05-22T21:23:09.9588104Z 20/05/22 21:19:55 INFO resourcemanager.ResourceTrackerService: NodeManager from node slave2.docker-hadoop-cluster-network(cmPort: 41369 httpPort: 8042) registered with capability: <memory:2500, vCores:1>, assigned nodeId slave2.docker-hadoop-cluster-network:41369
2020-05-22T21:23:09.9589206Z 20/05/22 21:19:55 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-05-22T21:23:09.9589833Z 20/05/22 21:19:56 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-05-22T21:23:09.9590405Z 20/05/22 21:19:56 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
2020-05-22T21:23:09.9591298Z 20/05/22 21:19:56 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-05-22T21:23:09.9591954Z 20/05/22 21:19:56 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
2020-05-22T21:23:09.9593030Z 20/05/22 21:19:56 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-05-22T21:23:09.9593776Z 20/05/22 21:19:56 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-05-22T21:23:09.9594342Z 20/05/22 21:19:56 INFO http.HttpServer2: adding path spec: /cluster/*
2020-05-22T21:23:09.9594768Z 20/05/22 21:19:56 INFO http.HttpServer2: adding path spec: /ws/*
2020-05-22T21:23:09.9595185Z 20/05/22 21:19:57 INFO webapp.WebApps: Registered webapp guice modules
2020-05-22T21:23:09.9595578Z 20/05/22 21:19:57 INFO http.HttpServer2: Jetty bound to port 8088
2020-05-22T21:23:09.9596228Z 20/05/22 21:19:57 INFO mortbay.log: jetty-6.1.26
2020-05-22T21:23:09.9597129Z 20/05/22 21:19:57 INFO mortbay.log: Extract jar:file:/usr/local/hadoop-2.8.4/share/hadoop/yarn/hadoop-yarn-common-2.8.4.jar!/webapps/cluster to /tmp/Jetty_0_0_0_0_8088_cluster____u0rgz3/webapp
2020-05-22T21:23:09.9597794Z May 22, 2020 9:19:58 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2020-05-22T21:23:09.9598304Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
2020-05-22T21:23:09.9598798Z May 22, 2020 9:19:58 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2020-05-22T21:23:09.9599300Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
2020-05-22T21:23:09.9599786Z May 22, 2020 9:19:58 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2020-05-22T21:23:09.9600256Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2020-05-22T21:23:09.9600725Z May 22, 2020 9:19:58 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2020-05-22T21:23:09.9601401Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2020-05-22T21:23:09.9601908Z May 22, 2020 9:19:58 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2020-05-22T21:23:09.9602470Z INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope ""Singleton""
2020-05-22T21:23:09.9603163Z May 22, 2020 9:19:59 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2020-05-22T21:23:09.9603712Z INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope ""Singleton""
2020-05-22T21:23:09.9604239Z May 22, 2020 9:19:59 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2020-05-22T21:23:09.9604811Z INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope ""Singleton""
2020-05-22T21:23:09.9605399Z 20/05/22 21:19:59 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088
2020-05-22T21:23:09.9605965Z 20/05/22 21:19:59 INFO webapp.WebApps: Web app cluster started at 8088
2020-05-22T21:23:09.9606779Z 20/05/22 21:19:59 INFO rmnode.RMNodeImpl: slave1.docker-hadoop-cluster-network:42227 Node Transitioned from NEW to RUNNING
2020-05-22T21:23:09.9607652Z 20/05/22 21:19:59 INFO rmnode.RMNodeImpl: slave2.docker-hadoop-cluster-network:41369 Node Transitioned from NEW to RUNNING
2020-05-22T21:23:09.9608620Z 20/05/22 21:19:59 INFO capacity.CapacityScheduler: Added node slave1.docker-hadoop-cluster-network:42227 clusterResource: <memory:2500, vCores:1>
2020-05-22T21:23:09.9609631Z 20/05/22 21:19:59 INFO capacity.CapacityScheduler: Added node slave2.docker-hadoop-cluster-network:41369 clusterResource: <memory:5000, vCores:2>
2020-05-22T21:23:09.9610457Z 20/05/22 21:19:59 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-05-22T21:23:09.9611118Z 20/05/22 21:19:59 INFO ipc.Server: Starting Socket Reader #1 for port 8033
2020-05-22T21:23:09.9611834Z 20/05/22 21:19:59 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
2020-05-22T21:23:09.9612850Z 20/05/22 21:19:59 INFO conf.Configuration: found resource hadoop-policy.xml at file:/usr/local/hadoop-2.8.4/etc/hadoop/hadoop-policy.xml
2020-05-22T21:23:09.9613407Z 20/05/22 21:19:59 INFO ipc.Server: IPC Server Responder: starting
2020-05-22T21:23:09.9613822Z 20/05/22 21:19:59 INFO ipc.Server: IPC Server listener on 8033: starting
2020-05-22T21:23:09.9614541Z 20/05/22 21:20:25 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9615602Z 20/05/22 21:20:25 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9616582Z 20/05/22 21:20:50 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9617653Z 20/05/22 21:20:50 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9619017Z 20/05/22 21:20:50 INFO resourcemanager.RMAuditLogger: USER=hadoop-user@EXAMPLE.COM	IP=172.22.0.3	OPERATION=Get Queue Info Request	TARGET=ClientRMService	RESULT=SUCCESS	QUEUENAME=root	INCLUDEAPPS=false	INCLUDECHILDQUEUES=true	RECURSIVE=true
2020-05-22T21:23:09.9619777Z 20/05/22 21:20:50 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1
2020-05-22T21:23:09.9620532Z 20/05/22 21:21:19 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9621587Z 20/05/22 21:21:19 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9622764Z 20/05/22 21:21:19 INFO capacity.CapacityScheduler: Application 'application_1590182392090_0001' is submitted without priority hence considering default queue/cluster priority: 0
2020-05-22T21:23:09.9624021Z 20/05/22 21:21:19 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1590182392090_0001 for the user: hadoop-user
2020-05-22T21:23:09.9624950Z 20/05/22 21:21:19 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user hadoop-user
2020-05-22T21:23:09.9625966Z 20/05/22 21:21:19 INFO resourcemanager.RMAuditLogger: USER=hadoop-user	IP=172.22.0.3	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1590182392090_0001
2020-05-22T21:23:09.9627234Z 20/05/22 21:21:19 INFO security.DelegationTokenRenewer: application_1590182392090_0001 found existing hdfs token Kind: HDFS_DELEGATION_TOKEN, Service: 172.22.0.3:9000, Ident: (HDFS_DELEGATION_TOKEN token 1 for hadoop-user)
2020-05-22T21:23:09.9628793Z 20/05/22 21:21:20 INFO security.DelegationTokenRenewer: Renewed delegation-token= [Kind: HDFS_DELEGATION_TOKEN, Service: 172.22.0.3:9000, Ident: (HDFS_DELEGATION_TOKEN token 1 for hadoop-user);exp=1590268880093; apps=[application_1590182392090_0001]]
2020-05-22T21:23:09.9629991Z 20/05/22 21:21:20 INFO impl.TimelineClientImpl: Timeline service address: http://master.docker-hadoop-cluster-network:8188/ws/v1/timeline/
2020-05-22T21:23:09.9631489Z 20/05/22 21:21:20 INFO security.DelegationTokenRenewer: Renewed delegation-token= [Kind: TIMELINE_DELEGATION_TOKEN, Service: 172.22.0.3:8188, Ident: (owner=hadoop-user, renewer=yarn, realUser=, issueDate=1590182479214, maxDate=1590787279214, sequenceNumber=1, masterKeyId=2);exp=1590268880306; apps=[application_1590182392090_0001]]
2020-05-22T21:23:09.9633410Z 20/05/22 21:21:20 INFO security.DelegationTokenRenewer: Renew Kind: TIMELINE_DELEGATION_TOKEN, Service: 172.22.0.3:8188, Ident: (owner=hadoop-user, renewer=yarn, realUser=, issueDate=1590182479214, maxDate=1590787279214, sequenceNumber=1, masterKeyId=2);exp=1590268880306; apps=[application_1590182392090_0001] in 86399953 ms, appId = [application_1590182392090_0001]
2020-05-22T21:23:09.9635160Z 20/05/22 21:21:20 INFO security.DelegationTokenRenewer: Renew Kind: HDFS_DELEGATION_TOKEN, Service: 172.22.0.3:9000, Ident: (HDFS_DELEGATION_TOKEN token 1 for hadoop-user);exp=1590268880093; apps=[application_1590182392090_0001] in 86399740 ms, appId = [application_1590182392090_0001]
2020-05-22T21:23:09.9636020Z 20/05/22 21:21:20 INFO rmapp.RMAppImpl: Storing application with id application_1590182392090_0001
2020-05-22T21:23:09.9636572Z 20/05/22 21:21:20 INFO rmapp.RMAppImpl: application_1590182392090_0001 State change from NEW to NEW_SAVING on event=START
2020-05-22T21:23:09.9637116Z 20/05/22 21:21:20 INFO recovery.RMStateStore: Storing info for app: application_1590182392090_0001
2020-05-22T21:23:09.9637699Z 20/05/22 21:21:20 INFO rmapp.RMAppImpl: application_1590182392090_0001 State change from NEW_SAVING to SUBMITTED on event=APP_NEW_SAVED
2020-05-22T21:23:09.9638732Z 20/05/22 21:21:20 INFO capacity.ParentQueue: Application added - appId: application_1590182392090_0001 user: hadoop-user leaf-queue of parent: root #applications: 1
2020-05-22T21:23:09.9639791Z 20/05/22 21:21:20 INFO capacity.CapacityScheduler: Accepted application application_1590182392090_0001 from user: hadoop-user, in queue: default
2020-05-22T21:23:09.9640473Z 20/05/22 21:21:20 INFO rmapp.RMAppImpl: application_1590182392090_0001 State change from SUBMITTED to ACCEPTED on event=APP_ACCEPTED
2020-05-22T21:23:09.9641088Z 20/05/22 21:21:20 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1590182392090_0001_000001
2020-05-22T21:23:09.9641702Z 20/05/22 21:21:20 INFO attempt.RMAppAttemptImpl: appattempt_1590182392090_0001_000001 State change from NEW to SUBMITTED
2020-05-22T21:23:09.9642619Z 20/05/22 21:21:20 INFO capacity.LeafQueue: Application application_1590182392090_0001 from user: hadoop-user activated in queue: default
2020-05-22T21:23:09.9643961Z 20/05/22 21:21:20 INFO capacity.LeafQueue: Application added - appId: application_1590182392090_0001 user: hadoop-user, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2020-05-22T21:23:09.9645338Z 20/05/22 21:21:20 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1590182392090_0001_000001 to scheduler from user hadoop-user in queue default
2020-05-22T21:23:09.9646011Z 20/05/22 21:21:20 INFO attempt.RMAppAttemptImpl: appattempt_1590182392090_0001_000001 State change from SUBMITTED to SCHEDULED
2020-05-22T21:23:09.9646615Z 20/05/22 21:21:20 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000001 Container Transitioned from NEW to ALLOCATED
2020-05-22T21:23:09.9647751Z 20/05/22 21:21:20 INFO resourcemanager.RMAuditLogger: USER=hadoop-user	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1590182392090_0001	CONTAINERID=container_1590182392090_0001_01_000001
2020-05-22T21:23:09.9649452Z 20/05/22 21:21:20 INFO scheduler.SchedulerNode: Assigned container container_1590182392090_0001_01_000001 of capacity <memory:1000, vCores:1> on host slave2.docker-hadoop-cluster-network:41369, which has 1 containers, <memory:1000, vCores:1> used and <memory:1500, vCores:0> available after allocation
2020-05-22T21:23:09.9651448Z 20/05/22 21:21:20 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1590182392090_0001_000001 container=container_1590182392090_0001_01_000001 queue=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator@5b47bf57 clusterResource=<memory:5000, vCores:2> type=OFF_SWITCH requestedPartition=
2020-05-22T21:23:09.9653176Z 20/05/22 21:21:20 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : slave2.docker-hadoop-cluster-network:41369 for container : container_1590182392090_0001_01_000001
2020-05-22T21:23:09.9653969Z 20/05/22 21:21:20 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2020-05-22T21:23:09.9654586Z 20/05/22 21:21:20 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1590182392090_0001_000001
2020-05-22T21:23:09.9655797Z 20/05/22 21:21:20 INFO capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:1000, vCores:1>, usedCapacity=0.2, absoluteUsedCapacity=0.2, numApps=1, numContainers=1
2020-05-22T21:23:09.9658025Z 20/05/22 21:21:20 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1590182392090_0001 AttemptId: appattempt_1590182392090_0001_000001 MasterContainer: Container: [ContainerId: container_1590182392090_0001_01_000001, Version: 0, NodeId: slave2.docker-hadoop-cluster-network:41369, NodeHttpAddress: slave2.docker-hadoop-cluster-network:8042, Resource: <memory:1000, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.22.0.5:41369 }, ]
2020-05-22T21:23:09.9659529Z 20/05/22 21:21:20 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.2 absoluteUsedCapacity=0.2 used=<memory:1000, vCores:1> cluster=<memory:5000, vCores:2>
2020-05-22T21:23:09.9660290Z 20/05/22 21:21:20 INFO attempt.RMAppAttemptImpl: appattempt_1590182392090_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING
2020-05-22T21:23:09.9660920Z 20/05/22 21:21:20 INFO attempt.RMAppAttemptImpl: appattempt_1590182392090_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED
2020-05-22T21:23:09.9661474Z 20/05/22 21:21:20 INFO amlauncher.AMLauncher: Launching masterappattempt_1590182392090_0001_000001
2020-05-22T21:23:09.9663213Z 20/05/22 21:21:21 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1590182392090_0001_01_000001, Version: 0, NodeId: slave2.docker-hadoop-cluster-network:41369, NodeHttpAddress: slave2.docker-hadoop-cluster-network:8042, Resource: <memory:1000, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.22.0.5:41369 }, ] for AM appattempt_1590182392090_0001_000001
2020-05-22T21:23:09.9664486Z 20/05/22 21:21:21 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1590182392090_0001_000001
2020-05-22T21:23:09.9665217Z 20/05/22 21:21:21 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1590182392090_0001_000001
2020-05-22T21:23:09.9667026Z 20/05/22 21:21:21 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1590182392090_0001_01_000001, Version: 0, NodeId: slave2.docker-hadoop-cluster-network:41369, NodeHttpAddress: slave2.docker-hadoop-cluster-network:8042, Resource: <memory:1000, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.22.0.5:41369 }, ] for AM appattempt_1590182392090_0001_000001
2020-05-22T21:23:09.9668356Z 20/05/22 21:21:21 INFO attempt.RMAppAttemptImpl: appattempt_1590182392090_0001_000001 State change from ALLOCATED to LAUNCHED
2020-05-22T21:23:09.9668982Z 20/05/22 21:21:21 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
2020-05-22T21:23:09.9669585Z 20/05/22 21:21:51 INFO ipc.Server: Auth successful for appattempt_1590182392090_0001_000001 (auth:SIMPLE)
2020-05-22T21:23:09.9670382Z 20/05/22 21:21:51 INFO authorize.ServiceAuthorizationManager: Authorization successful for appattempt_1590182392090_0001_000001 (auth:TOKEN) for protocol=interface org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB
2020-05-22T21:23:09.9671172Z 20/05/22 21:21:51 INFO resourcemanager.ApplicationMasterService: AM registration appattempt_1590182392090_0001_000001
2020-05-22T21:23:09.9672378Z 20/05/22 21:21:51 INFO resourcemanager.RMAuditLogger: USER=hadoop-user	IP=172.22.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1590182392090_0001	APPATTEMPTID=appattempt_1590182392090_0001_000001
2020-05-22T21:23:09.9673190Z 20/05/22 21:21:51 INFO attempt.RMAppAttemptImpl: appattempt_1590182392090_0001_000001 State change from LAUNCHED to RUNNING
2020-05-22T21:23:09.9673801Z 20/05/22 21:21:51 INFO rmapp.RMAppImpl: application_1590182392090_0001 State change from ACCEPTED to RUNNING on event=ATTEMPT_REGISTERED
2020-05-22T21:23:09.9674380Z 20/05/22 21:21:51 INFO resourcemanager.ApplicationMasterService: Setting client token master key
2020-05-22T21:23:09.9675511Z 20/05/22 21:21:54 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9676685Z 20/05/22 21:21:54 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9677827Z 20/05/22 21:21:54 INFO resourcemanager.RMAuditLogger: USER=hadoop-user@EXAMPLE.COM	IP=172.22.0.3	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2020-05-22T21:23:09.9678708Z 20/05/22 21:21:57 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9679777Z 20/05/22 21:21:57 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9680913Z 20/05/22 21:21:57 INFO resourcemanager.RMAuditLogger: USER=hadoop-user@EXAMPLE.COM	IP=172.22.0.3	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2020-05-22T21:23:09.9681607Z 20/05/22 21:21:57 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000002 Container Transitioned from NEW to ALLOCATED
2020-05-22T21:23:09.9682738Z 20/05/22 21:21:57 INFO resourcemanager.RMAuditLogger: USER=hadoop-user	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1590182392090_0001	CONTAINERID=container_1590182392090_0001_01_000002
2020-05-22T21:23:09.9684932Z 20/05/22 21:21:57 INFO scheduler.SchedulerNode: Assigned container container_1590182392090_0001_01_000002 of capacity <memory:1000, vCores:1> on host slave1.docker-hadoop-cluster-network:42227, which has 1 containers, <memory:1000, vCores:1> used and <memory:1500, vCores:0> available after allocation
2020-05-22T21:23:09.9686629Z 20/05/22 21:21:57 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1590182392090_0001_000001 container=container_1590182392090_0001_01_000002 queue=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator@5b47bf57 clusterResource=<memory:5000, vCores:2> type=OFF_SWITCH requestedPartition=
2020-05-22T21:23:09.9688433Z 20/05/22 21:21:57 INFO capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2000, vCores:2>, usedCapacity=0.4, absoluteUsedCapacity=0.4, numApps=1, numContainers=2
2020-05-22T21:23:09.9689524Z 20/05/22 21:21:57 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.4 absoluteUsedCapacity=0.4 used=<memory:2000, vCores:2> cluster=<memory:5000, vCores:2>
2020-05-22T21:23:09.9690814Z 20/05/22 21:21:57 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : slave1.docker-hadoop-cluster-network:42227 for container : container_1590182392090_0001_01_000002
2020-05-22T21:23:09.9691818Z 20/05/22 21:21:57 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2020-05-22T21:23:09.9692475Z 20/05/22 21:21:57 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1590182392090_0001
2020-05-22T21:23:09.9693096Z 20/05/22 21:21:58 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING
2020-05-22T21:23:09.9693983Z 20/05/22 21:22:01 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9695077Z 20/05/22 21:22:01 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9696031Z 20/05/22 21:22:06 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9697106Z 20/05/22 21:22:06 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9698068Z 20/05/22 21:22:11 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9699119Z 20/05/22 21:22:11 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9700082Z 20/05/22 21:22:16 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9701154Z 20/05/22 21:22:16 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9702114Z 20/05/22 21:22:22 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9703179Z 20/05/22 21:22:22 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9704121Z 20/05/22 21:22:29 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9705181Z 20/05/22 21:22:29 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9706153Z 20/05/22 21:22:33 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9707202Z 20/05/22 21:22:33 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9708313Z 20/05/22 21:22:38 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9709388Z 20/05/22 21:22:38 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9710335Z 20/05/22 21:22:42 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9711396Z 20/05/22 21:22:42 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9712264Z 20/05/22 21:22:43 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000003 Container Transitioned from NEW to ALLOCATED
2020-05-22T21:23:09.9713405Z 20/05/22 21:22:43 INFO resourcemanager.RMAuditLogger: USER=hadoop-user	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1590182392090_0001	CONTAINERID=container_1590182392090_0001_01_000003
2020-05-22T21:23:09.9715011Z 20/05/22 21:22:43 INFO scheduler.SchedulerNode: Assigned container container_1590182392090_0001_01_000003 of capacity <memory:1000, vCores:1> on host slave2.docker-hadoop-cluster-network:41369, which has 2 containers, <memory:2000, vCores:2> used and <memory:500, vCores:-1> available after allocation
2020-05-22T21:23:09.9717063Z 20/05/22 21:22:43 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1590182392090_0001_000001 container=container_1590182392090_0001_01_000003 queue=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator@5b47bf57 clusterResource=<memory:5000, vCores:2> type=OFF_SWITCH requestedPartition=
2020-05-22T21:23:09.9718949Z 20/05/22 21:22:43 INFO capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3000, vCores:3>, usedCapacity=0.6, absoluteUsedCapacity=0.6, numApps=1, numContainers=3
2020-05-22T21:23:09.9719942Z 20/05/22 21:22:43 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.6 absoluteUsedCapacity=0.6 used=<memory:3000, vCores:3> cluster=<memory:5000, vCores:2>
2020-05-22T21:23:09.9720704Z 20/05/22 21:22:43 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000004 Container Transitioned from NEW to ALLOCATED
2020-05-22T21:23:09.9721853Z 20/05/22 21:22:43 INFO resourcemanager.RMAuditLogger: USER=hadoop-user	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1590182392090_0001	CONTAINERID=container_1590182392090_0001_01_000004
2020-05-22T21:23:09.9723438Z 20/05/22 21:22:43 INFO scheduler.SchedulerNode: Assigned container container_1590182392090_0001_01_000004 of capacity <memory:1000, vCores:1> on host slave1.docker-hadoop-cluster-network:42227, which has 2 containers, <memory:2000, vCores:2> used and <memory:500, vCores:-1> available after allocation
2020-05-22T21:23:09.9724980Z 20/05/22 21:22:43 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1590182392090_0001_000001 container=container_1590182392090_0001_01_000004 queue=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator@5b47bf57 clusterResource=<memory:5000, vCores:2> type=OFF_SWITCH requestedPartition=
2020-05-22T21:23:09.9726749Z 20/05/22 21:22:43 INFO capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:4000, vCores:4>, usedCapacity=0.8, absoluteUsedCapacity=0.8, numApps=1, numContainers=4
2020-05-22T21:23:09.9727756Z 20/05/22 21:22:43 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8 absoluteUsedCapacity=0.8 used=<memory:4000, vCores:4> cluster=<memory:5000, vCores:2>
2020-05-22T21:23:09.9728961Z 20/05/22 21:22:43 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : slave2.docker-hadoop-cluster-network:41369 for container : container_1590182392090_0001_01_000003
2020-05-22T21:23:09.9729841Z 20/05/22 21:22:43 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2020-05-22T21:23:09.9730491Z 20/05/22 21:22:43 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000004 Container Transitioned from ALLOCATED to ACQUIRED
2020-05-22T21:23:09.9731133Z 20/05/22 21:22:44 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1590182392090_0001
2020-05-22T21:23:09.9731883Z 20/05/22 21:22:44 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING
2020-05-22T21:23:09.9732625Z 20/05/22 21:22:44 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000004 Container Transitioned from ACQUIRED to RUNNING
2020-05-22T21:23:09.9733519Z 20/05/22 21:22:48 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9734586Z 20/05/22 21:22:48 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9735561Z 20/05/22 21:22:54 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9736611Z 20/05/22 21:22:55 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9737582Z 20/05/22 21:22:59 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9738648Z 20/05/22 21:22:59 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9739601Z 20/05/22 21:23:03 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9740665Z 20/05/22 21:23:03 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9742119Z 20/05/22 21:23:04 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1590182392090_0001_000001 with final state: FINISHING, and exit status: -1000
2020-05-22T21:23:09.9742850Z 20/05/22 21:23:04 INFO attempt.RMAppAttemptImpl: appattempt_1590182392090_0001_000001 State change from RUNNING to FINAL_SAVING
2020-05-22T21:23:09.9743483Z 20/05/22 21:23:04 INFO rmapp.RMAppImpl: Updating application application_1590182392090_0001 with final state: FINISHING
2020-05-22T21:23:09.9744056Z 20/05/22 21:23:04 INFO recovery.RMStateStore: Updating info for app: application_1590182392090_0001
2020-05-22T21:23:09.9744659Z 20/05/22 21:23:04 INFO rmapp.RMAppImpl: application_1590182392090_0001 State change from RUNNING to FINAL_SAVING on event=ATTEMPT_UNREGISTERED
2020-05-22T21:23:09.9745293Z 20/05/22 21:23:04 INFO attempt.RMAppAttemptImpl: appattempt_1590182392090_0001_000001 State change from FINAL_SAVING to FINISHING
2020-05-22T21:23:09.9745899Z 20/05/22 21:23:04 INFO rmapp.RMAppImpl: application_1590182392090_0001 State change from FINAL_SAVING to FINISHING on event=APP_UPDATE_SAVED
2020-05-22T21:23:09.9746522Z 20/05/22 21:23:04 INFO resourcemanager.ApplicationMasterService: application_1590182392090_0001 unregistered successfully. 
2020-05-22T21:23:09.9747148Z 20/05/22 21:23:04 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000003 Container Transitioned from RUNNING to COMPLETED
2020-05-22T21:23:09.9748336Z 20/05/22 21:23:04 INFO resourcemanager.RMAuditLogger: USER=hadoop-user	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1590182392090_0001	CONTAINERID=container_1590182392090_0001_01_000003
2020-05-22T21:23:09.9750134Z 20/05/22 21:23:04 INFO scheduler.SchedulerNode: Released container container_1590182392090_0001_01_000003 of capacity <memory:1000, vCores:1> on host slave2.docker-hadoop-cluster-network:41369, which currently has 1 containers, <memory:1000, vCores:1> used and <memory:1500, vCores:0> available, release resources=true
2020-05-22T21:23:09.9751196Z 20/05/22 21:23:04 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000004 Container Transitioned from RUNNING to COMPLETED
2020-05-22T21:23:09.9752326Z 20/05/22 21:23:04 INFO resourcemanager.RMAuditLogger: USER=hadoop-user	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1590182392090_0001	CONTAINERID=container_1590182392090_0001_01_000004
2020-05-22T21:23:09.9754062Z 20/05/22 21:23:04 INFO scheduler.SchedulerNode: Released container container_1590182392090_0001_01_000004 of capacity <memory:1000, vCores:1> on host slave1.docker-hadoop-cluster-network:42227, which currently has 1 containers, <memory:1000, vCores:1> used and <memory:1500, vCores:0> available, release resources=true
2020-05-22T21:23:09.9755124Z 20/05/22 21:23:04 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000002 Container Transitioned from RUNNING to COMPLETED
2020-05-22T21:23:09.9756270Z 20/05/22 21:23:04 INFO resourcemanager.RMAuditLogger: USER=hadoop-user	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1590182392090_0001	CONTAINERID=container_1590182392090_0001_01_000002
2020-05-22T21:23:09.9757899Z 20/05/22 21:23:04 INFO scheduler.SchedulerNode: Released container container_1590182392090_0001_01_000002 of capacity <memory:1000, vCores:1> on host slave1.docker-hadoop-cluster-network:42227, which currently has 0 containers, <memory:0, vCores:0> used and <memory:2500, vCores:1> available, release resources=true
2020-05-22T21:23:09.9758952Z 20/05/22 21:23:06 INFO rmcontainer.RMContainerImpl: container_1590182392090_0001_01_000001 Container Transitioned from RUNNING to COMPLETED
2020-05-22T21:23:09.9760081Z 20/05/22 21:23:06 INFO resourcemanager.RMAuditLogger: USER=hadoop-user	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1590182392090_0001	CONTAINERID=container_1590182392090_0001_01_000001
2020-05-22T21:23:09.9761703Z 20/05/22 21:23:06 INFO scheduler.SchedulerNode: Released container container_1590182392090_0001_01_000001 of capacity <memory:1000, vCores:1> on host slave2.docker-hadoop-cluster-network:41369, which currently has 0 containers, <memory:0, vCores:0> used and <memory:2500, vCores:1> available, release resources=true
2020-05-22T21:23:09.9762759Z 20/05/22 21:23:06 INFO resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1590182392090_0001_000001
2020-05-22T21:23:09.9763415Z 20/05/22 21:23:06 INFO security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1590182392090_0001_000001
2020-05-22T21:23:09.9764031Z 20/05/22 21:23:06 INFO attempt.RMAppAttemptImpl: appattempt_1590182392090_0001_000001 State change from FINISHING to FINISHED
2020-05-22T21:23:09.9764643Z 20/05/22 21:23:06 INFO rmapp.RMAppImpl: application_1590182392090_0001 State change from FINISHING to FINISHED on event=ATTEMPT_FINISHED
2020-05-22T21:23:09.9765274Z 20/05/22 21:23:06 INFO capacity.CapacityScheduler: Application Attempt appattempt_1590182392090_0001_000001 is done. finalState=FINISHED
2020-05-22T21:23:09.9765855Z 20/05/22 21:23:06 INFO scheduler.AppSchedulingInfo: Application application_1590182392090_0001 requests cleared
2020-05-22T21:23:09.9767135Z 20/05/22 21:23:06 INFO capacity.LeafQueue: Application removed - appId: application_1590182392090_0001 user: hadoop-user queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2020-05-22T21:23:09.9768469Z 20/05/22 21:23:06 INFO capacity.ParentQueue: Application removed - appId: application_1590182392090_0001 user: hadoop-user leaf-queue of parent: root #applications: 0
2020-05-22T21:23:09.9769687Z 20/05/22 21:23:06 INFO resourcemanager.RMAuditLogger: USER=hadoop-user	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1590182392090_0001
2020-05-22T21:23:09.9770354Z 20/05/22 21:23:06 INFO amlauncher.AMLauncher: Cleaning master appattempt_1590182392090_0001_000001
2020-05-22T21:23:09.9773990Z 20/05/22 21:23:06 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1590182392090_0001,name=Flink Application Cluster,user=hadoop-user,queue=default,state=FINISHED,trackingUrl=http://master.docker-hadoop-cluster-network:8088/proxy/application_1590182392090_0001/,appMasterHost=slave2.docker-hadoop-cluster-network,startTime=1590182479362,finishTime=1590182584348,finalStatus=FAILED,memorySeconds=216607,vcoreSeconds=214,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=Apache Flink
2020-05-22T21:23:09.9776854Z 20/05/22 21:23:07 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9778319Z 20/05/22 21:23:07 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9779835Z 20/05/22 21:23:08 INFO scheduler.AbstractYarnScheduler: Container container_1590182392090_0001_01_000004 completed with event FINISHED, but corresponding RMContainer doesn't exist.
2020-05-22T21:23:09.9781281Z 20/05/22 21:23:08 INFO scheduler.AbstractYarnScheduler: Container container_1590182392090_0001_01_000002 completed with event FINISHED, but corresponding RMContainer doesn't exist.
2020-05-22T21:23:09.9782751Z 20/05/22 21:23:08 INFO scheduler.AbstractYarnScheduler: Container container_1590182392090_0001_01_000003 completed with event FINISHED, but corresponding RMContainer doesn't exist.
2020-05-22T21:23:09.9783915Z 20/05/22 21:23:09 INFO ipc.Server: Auth successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:09.9785382Z 20/05/22 21:23:09 INFO authorize.ServiceAuthorizationManager: Authorization successful for hadoop-user@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB
2020-05-22T21:23:09.9786736Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-22195494660/logs/hadoop/resourcemanager.out:
2020-05-22T21:23:09.9787767Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-22195494660/logs/hadoop/timelineserver.err:
2020-05-22T21:23:09.9788335Z 20/05/22 21:19:38 INFO applicationhistoryservice.ApplicationHistoryServer: STARTUP_MSG: 
2020-05-22T21:23:09.9788770Z /************************************************************
2020-05-22T21:23:09.9789107Z STARTUP_MSG: Starting ApplicationHistoryServer
2020-05-22T21:23:09.9789388Z STARTUP_MSG:   user = yarn
2020-05-22T21:23:09.9789922Z STARTUP_MSG:   host = master.docker-hadoop-cluster-network/172.22.0.3
2020-05-22T21:23:09.9790253Z STARTUP_MSG:   args = []
2020-05-22T21:23:09.9790490Z STARTUP_MSG:   version = 2.8.4
2020-05-22T21:23:09.9837616Z STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/etc/hadoop/timelineserver-config/log4j.properties
2020-05-22T21:23:09.9870258Z STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 17e75c2a11685af3e043aa5e604dc831e5b14674; compiled by 'jdu' on 2018-05-08T02:50Z
2020-05-22T21:23:09.9870833Z STARTUP_MSG:   java = 1.8.0_131
2020-05-22T21:23:09.9871149Z ************************************************************/
2020-05-22T21:23:09.9871630Z 20/05/22 21:19:38 INFO applicationhistoryservice.ApplicationHistoryServer: registered UNIX signal handlers for [TERM, HUP, INT]
2020-05-22T21:23:09.9872826Z 20/05/22 21:19:48 INFO security.UserGroupInformation: Login successful for user yarn/master.docker-hadoop-cluster-network@EXAMPLE.COM using keytab file /etc/security/keytabs/yarn.keytab
2020-05-22T21:23:09.9873685Z 20/05/22 21:19:48 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-05-22T21:23:09.9874192Z 20/05/22 21:19:49 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2020-05-22T21:23:09.9874699Z 20/05/22 21:19:49 INFO impl.MetricsSystemImpl: ApplicationHistoryServer metrics system started
2020-05-22T21:23:09.9875507Z 20/05/22 21:19:50 INFO timeline.LeveldbTimelineStore: Using leveldb path /tmp/hadoop-yarn/yarn/timeline/leveldb-timeline-store.ldb
2020-05-22T21:23:09.9876161Z 20/05/22 21:19:50 INFO timeline.LeveldbTimelineStore: Loaded timeline store version info 1.0
2020-05-22T21:23:09.9876712Z 20/05/22 21:19:50 INFO timeline.LeveldbTimelineStore: Starting deletion thread with ttl 604800000 and cycle interval 300000
2020-05-22T21:23:09.9877320Z 20/05/22 21:19:50 INFO timeline.LeveldbTimelineStore: Discarded 0 entities for timestamp 1589577590441 and earlier in 0.007 seconds
2020-05-22T21:23:09.9877965Z 20/05/22 21:19:50 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2020-05-22T21:23:09.9878655Z 20/05/22 21:19:50 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2020-05-22T21:23:09.9879326Z 20/05/22 21:19:50 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2020-05-22T21:23:09.9880098Z 20/05/22 21:19:50 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-05-22T21:23:09.9880753Z 20/05/22 21:19:50 INFO ipc.Server: Starting Socket Reader #1 for port 10200
2020-05-22T21:23:09.9881173Z 20/05/22 21:19:50 INFO ipc.Server: Starting Socket Reader #2 for port 10200
2020-05-22T21:23:09.9881605Z 20/05/22 21:19:50 INFO ipc.Server: Starting Socket Reader #3 for port 10200
2020-05-22T21:23:09.9882014Z 20/05/22 21:19:50 INFO ipc.Server: Starting Socket Reader #4 for port 10200
2020-05-22T21:23:09.9882439Z 20/05/22 21:19:50 INFO ipc.Server: Starting Socket Reader #5 for port 10200
2020-05-22T21:23:09.9882970Z 20/05/22 21:19:51 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationHistoryProtocolPB to the server
2020-05-22T21:23:09.9883482Z 20/05/22 21:19:51 INFO ipc.Server: IPC Server Responder: starting
2020-05-22T21:23:09.9883924Z 20/05/22 21:19:51 INFO ipc.Server: IPC Server listener on 10200: starting
2020-05-22T21:23:09.9884905Z 20/05/22 21:19:51 INFO applicationhistoryservice.ApplicationHistoryClientService: Instantiated ApplicationHistoryClientService at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:23:09.9885631Z 20/05/22 21:19:51 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-05-22T21:23:09.9886250Z 20/05/22 21:19:51 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-05-22T21:23:09.9886847Z 20/05/22 21:19:51 INFO http.HttpRequestLog: Http request log for http.requests.applicationhistory is not defined
2020-05-22T21:23:09.9887684Z 20/05/22 21:19:51 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-05-22T21:23:09.9888735Z 20/05/22 21:19:51 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context applicationhistory
2020-05-22T21:23:09.9889492Z 20/05/22 21:19:51 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-05-22T21:23:09.9890291Z 20/05/22 21:19:51 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-05-22T21:23:09.9891556Z 20/05/22 21:19:52 INFO http.HttpServer2: Added global filter 'Timeline Authentication Filter' (class=org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilter)
2020-05-22T21:23:09.9892197Z 20/05/22 21:19:52 INFO http.HttpServer2: adding path spec: /applicationhistory/*
2020-05-22T21:23:09.9892626Z 20/05/22 21:19:52 INFO http.HttpServer2: adding path spec: /ws/*
2020-05-22T21:23:09.9893046Z 20/05/22 21:19:52 INFO webapp.WebApps: Registered webapp guice modules
2020-05-22T21:23:09.9893550Z 20/05/22 21:19:52 INFO http.HttpServer2: Jetty bound to port 8188
2020-05-22T21:23:09.9894175Z 20/05/22 21:19:52 INFO mortbay.log: jetty-6.1.26
2020-05-22T21:23:09.9895213Z 20/05/22 21:19:53 INFO mortbay.log: Extract jar:file:/usr/local/hadoop-2.8.4/share/hadoop/yarn/hadoop-yarn-common-2.8.4.jar!/webapps/applicationhistory to /tmp/Jetty_master_docker.hadoop.cluster.network_8188_applicationhistory____.xi8yr4/webapp
2020-05-22T21:23:09.9896396Z 20/05/22 21:19:53 INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/yarn.keytab, for principal HTTP/master.docker-hadoop-cluster-network@EXAMPLE.COM
2020-05-22T21:23:09.9897121Z 20/05/22 21:19:53 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2020-05-22T21:23:09.9897809Z 20/05/22 21:19:53 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2020-05-22T21:23:09.9899883Z May 22, 2020 9:19:53 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2020-05-22T21:23:09.9900389Z INFO: Registering org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider as a provider class
2020-05-22T21:23:09.9900866Z May 22, 2020 9:19:53 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2020-05-22T21:23:09.9901375Z INFO: Registering org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices as a root resource class
2020-05-22T21:23:09.9901894Z May 22, 2020 9:19:53 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2020-05-22T21:23:09.9902377Z INFO: Registering org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices as a root resource class
2020-05-22T21:23:09.9902877Z May 22, 2020 9:19:53 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2020-05-22T21:23:09.9903344Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2020-05-22T21:23:09.9903799Z May 22, 2020 9:19:54 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2020-05-22T21:23:09.9904590Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2020-05-22T21:23:09.9905078Z May 22, 2020 9:19:54 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2020-05-22T21:23:09.9905630Z INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope ""Singleton""
2020-05-22T21:23:09.9906178Z May 22, 2020 9:19:54 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2020-05-22T21:23:09.9906717Z INFO: Binding org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider to GuiceManagedComponentProvider with the scope ""Singleton""
2020-05-22T21:23:09.9907272Z May 22, 2020 9:19:56 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2020-05-22T21:23:09.9909188Z INFO: Binding org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices to GuiceManagedComponentProvider with the scope ""Singleton""
2020-05-22T21:23:09.9909842Z May 22, 2020 9:19:56 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2020-05-22T21:23:09.9910408Z INFO: Binding org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices to GuiceManagedComponentProvider with the scope ""Singleton""
2020-05-22T21:23:09.9911614Z 20/05/22 21:19:56 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@master.docker-hadoop-cluster-network:8188
2020-05-22T21:23:09.9912266Z 20/05/22 21:19:56 INFO applicationhistoryservice.ApplicationHistoryServer: Instantiating AHSWebApp at 8188
2020-05-22T21:23:09.9913506Z 20/05/22 21:21:19 INFO delegation.AbstractDelegationTokenSecretManager: Creating password for identifier: (owner=hadoop-user, renewer=yarn, realUser=, issueDate=1590182479214, maxDate=1590787279214, sequenceNumber=1, masterKeyId=2), currentKey: 2
2020-05-22T21:23:09.9915123Z 20/05/22 21:21:20 INFO delegation.AbstractDelegationTokenSecretManager: Token renewal for identifier: (owner=hadoop-user, renewer=yarn, realUser=, issueDate=1590182479214, maxDate=1590787279214, sequenceNumber=1, masterKeyId=2); total currentTokens 1
2020-05-22T21:23:09.9916246Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-22195494660/logs/hadoop/timelineserver.out:
2020-05-22T21:23:09.9916650Z Docker logs:
2020-05-22T21:23:09.9916827Z /usr/local
2020-05-22T21:23:09.9917431Z WARNING: no policy specified for hdfs/master.docker-hadoop-cluster-network@EXAMPLE.COM; defaulting to no policy
2020-05-22T21:23:09.9917850Z Authenticating as principal admin/admin with password.
2020-05-22T21:23:09.9918417Z Principal ""hdfs/master.docker-hadoop-cluster-network@EXAMPLE.COM"" created.
2020-05-22T21:23:09.9919123Z WARNING: no policy specified for mapred/master.docker-hadoop-cluster-network@EXAMPLE.COM; defaulting to no policy
2020-05-22T21:23:09.9919542Z Authenticating as principal admin/admin with password.
2020-05-22T21:23:09.9920118Z Principal ""mapred/master.docker-hadoop-cluster-network@EXAMPLE.COM"" created.
2020-05-22T21:23:09.9920799Z WARNING: no policy specified for yarn/master.docker-hadoop-cluster-network@EXAMPLE.COM; defaulting to no policy
2020-05-22T21:23:09.9921235Z Authenticating as principal admin/admin with password.
2020-05-22T21:23:09.9921786Z Principal ""yarn/master.docker-hadoop-cluster-network@EXAMPLE.COM"" created.
2020-05-22T21:23:09.9922480Z WARNING: no policy specified for HTTP/master.docker-hadoop-cluster-network@EXAMPLE.COM; defaulting to no policy
2020-05-22T21:23:09.9922912Z Authenticating as principal admin/admin with password.
2020-05-22T21:23:09.9923456Z Principal ""HTTP/master.docker-hadoop-cluster-network@EXAMPLE.COM"" created.
2020-05-22T21:23:09.9923822Z Authenticating as principal admin/admin with password.
2020-05-22T21:23:09.9924620Z Entry for principal hdfs/master.docker-hadoop-cluster-network with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:hdfs.keytab.
2020-05-22T21:23:09.9925659Z Entry for principal hdfs/master.docker-hadoop-cluster-network with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:hdfs.keytab.
2020-05-22T21:23:09.9926658Z Entry for principal hdfs/master.docker-hadoop-cluster-network with kvno 2, encryption type des3-cbc-sha1 added to keytab WRFILE:hdfs.keytab.
2020-05-22T21:23:09.9927622Z Entry for principal hdfs/master.docker-hadoop-cluster-network with kvno 2, encryption type arcfour-hmac added to keytab WRFILE:hdfs.keytab.
2020-05-22T21:23:09.9928590Z Entry for principal hdfs/master.docker-hadoop-cluster-network with kvno 2, encryption type des-hmac-sha1 added to keytab WRFILE:hdfs.keytab.
2020-05-22T21:23:09.9929553Z Entry for principal hdfs/master.docker-hadoop-cluster-network with kvno 2, encryption type des-cbc-md5 added to keytab WRFILE:hdfs.keytab.
2020-05-22T21:23:09.9930536Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:hdfs.keytab.
2020-05-22T21:23:09.9931788Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:hdfs.keytab.
2020-05-22T21:23:09.9932834Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 2, encryption type des3-cbc-sha1 added to keytab WRFILE:hdfs.keytab.
2020-05-22T21:23:09.9933956Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 2, encryption type arcfour-hmac added to keytab WRFILE:hdfs.keytab.
2020-05-22T21:23:09.9934921Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 2, encryption type des-hmac-sha1 added to keytab WRFILE:hdfs.keytab.
2020-05-22T21:23:09.9935864Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 2, encryption type des-cbc-md5 added to keytab WRFILE:hdfs.keytab.
2020-05-22T21:23:09.9936385Z Authenticating as principal admin/admin with password.
2020-05-22T21:23:09.9937311Z Entry for principal mapred/master.docker-hadoop-cluster-network with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:mapred.keytab.
2020-05-22T21:23:09.9938343Z Entry for principal mapred/master.docker-hadoop-cluster-network with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:mapred.keytab.
2020-05-22T21:23:09.9939361Z Entry for principal mapred/master.docker-hadoop-cluster-network with kvno 2, encryption type des3-cbc-sha1 added to keytab WRFILE:mapred.keytab.
2020-05-22T21:23:09.9940325Z Entry for principal mapred/master.docker-hadoop-cluster-network with kvno 2, encryption type arcfour-hmac added to keytab WRFILE:mapred.keytab.
2020-05-22T21:23:09.9941306Z Entry for principal mapred/master.docker-hadoop-cluster-network with kvno 2, encryption type des-hmac-sha1 added to keytab WRFILE:mapred.keytab.
2020-05-22T21:23:09.9942284Z Entry for principal mapred/master.docker-hadoop-cluster-network with kvno 2, encryption type des-cbc-md5 added to keytab WRFILE:mapred.keytab.
2020-05-22T21:23:09.9943287Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 3, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:mapred.keytab.
2020-05-22T21:23:09.9944317Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 3, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:mapred.keytab.
2020-05-22T21:23:09.9945326Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 3, encryption type des3-cbc-sha1 added to keytab WRFILE:mapred.keytab.
2020-05-22T21:23:09.9946283Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 3, encryption type arcfour-hmac added to keytab WRFILE:mapred.keytab.
2020-05-22T21:23:09.9947252Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 3, encryption type des-hmac-sha1 added to keytab WRFILE:mapred.keytab.
2020-05-22T21:23:09.9948202Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 3, encryption type des-cbc-md5 added to keytab WRFILE:mapred.keytab.
2020-05-22T21:23:09.9948727Z Authenticating as principal admin/admin with password.
2020-05-22T21:23:09.9949543Z Entry for principal yarn/master.docker-hadoop-cluster-network with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:yarn.keytab.
2020-05-22T21:23:09.9950554Z Entry for principal yarn/master.docker-hadoop-cluster-network with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:yarn.keytab.
2020-05-22T21:23:09.9951937Z Entry for principal yarn/master.docker-hadoop-cluster-network with kvno 2, encryption type des3-cbc-sha1 added to keytab WRFILE:yarn.keytab.
2020-05-22T21:23:09.9952954Z Entry for principal yarn/master.docker-hadoop-cluster-network with kvno 2, encryption type arcfour-hmac added to keytab WRFILE:yarn.keytab.
2020-05-22T21:23:09.9953909Z Entry for principal yarn/master.docker-hadoop-cluster-network with kvno 2, encryption type des-hmac-sha1 added to keytab WRFILE:yarn.keytab.
2020-05-22T21:23:09.9954887Z Entry for principal yarn/master.docker-hadoop-cluster-network with kvno 2, encryption type des-cbc-md5 added to keytab WRFILE:yarn.keytab.
2020-05-22T21:23:09.9955865Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 4, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:yarn.keytab.
2020-05-22T21:23:09.9957028Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 4, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:yarn.keytab.
2020-05-22T21:23:09.9958018Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 4, encryption type des3-cbc-sha1 added to keytab WRFILE:yarn.keytab.
2020-05-22T21:23:09.9958966Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 4, encryption type arcfour-hmac added to keytab WRFILE:yarn.keytab.
2020-05-22T21:23:09.9959932Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 4, encryption type des-hmac-sha1 added to keytab WRFILE:yarn.keytab.
2020-05-22T21:23:09.9960990Z Entry for principal HTTP/master.docker-hadoop-cluster-network with kvno 4, encryption type des-cbc-md5 added to keytab WRFILE:yarn.keytab.
2020-05-22T21:23:09.9961474Z  * Starting OpenBSD Secure Shell server sshd
2020-05-22T21:23:09.9961718Z    ...done.
2020-05-22T21:23:09.9961987Z 20/05/22 21:19:30 INFO namenode.NameNode: STARTUP_MSG: 
2020-05-22T21:23:09.9962362Z /************************************************************
2020-05-22T21:23:09.9962653Z STARTUP_MSG: Starting NameNode
2020-05-22T21:23:09.9962913Z STARTUP_MSG:   user = hdfs
2020-05-22T21:23:09.9963842Z STARTUP_MSG:   host = master.docker-hadoop-cluster-network/172.22.0.3
2020-05-22T21:23:09.9964352Z STARTUP_MSG:   args = [-format]
2020-05-22T21:23:09.9964623Z STARTUP_MSG:   version = 2.8.4
2020-05-22T21:23:10.0001701Z STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
2020-05-22T21:23:10.0033167Z STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 17e75c2a11685af3e043aa5e604dc831e5b14674; compiled by 'jdu' on 2018-05-08T02:50Z
2020-05-22T21:23:10.0033726Z STARTUP_MSG:   java = 1.8.0_131
2020-05-22T21:23:10.0034046Z ************************************************************/
2020-05-22T21:23:10.0034473Z 20/05/22 21:19:30 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-05-22T21:23:10.0035137Z 20/05/22 21:19:30 INFO namenode.NameNode: createNameNode [-format]
2020-05-22T21:23:10.0036054Z 20/05/22 21:19:32 INFO security.UserGroupInformation: Login successful for user hdfs/master.docker-hadoop-cluster-network@EXAMPLE.COM using keytab file /etc/security/keytabs/hdfs.keytab
2020-05-22T21:23:10.0039614Z Formatting using clusterid: CID-6e253d18-f0f3-446b-a39f-e6aa75c5db3b
2020-05-22T21:23:10.0040087Z 20/05/22 21:19:32 INFO namenode.FSEditLog: Edit logging is async:true
2020-05-22T21:23:10.0040522Z 20/05/22 21:19:32 INFO namenode.FSNamesystem: KeyProvider: null
2020-05-22T21:23:10.0040939Z 20/05/22 21:19:32 INFO namenode.FSNamesystem: fsLock is fair: true
2020-05-22T21:23:10.0041616Z 20/05/22 21:19:32 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-05-22T21:23:10.0049679Z 20/05/22 21:19:32 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-05-22T21:23:10.0053670Z 20/05/22 21:19:32 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2020-05-22T21:23:10.0054921Z 20/05/22 21:19:32 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-05-22T21:23:10.0055586Z 20/05/22 21:19:32 INFO blockmanagement.BlockManager: The block deletion will start around 2020 May 22 21:19:32
2020-05-22T21:23:10.0056106Z 20/05/22 21:19:32 INFO util.GSet: Computing capacity for map BlocksMap
2020-05-22T21:23:10.0056843Z 20/05/22 21:19:32 INFO util.GSet: VM type       = 64-bit
2020-05-22T21:23:10.0057401Z 20/05/22 21:19:32 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-05-22T21:23:10.0057811Z 20/05/22 21:19:32 INFO util.GSet: capacity      = 2^21 = 2097152 entries
2020-05-22T21:23:10.0058242Z 20/05/22 21:19:32 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=true
2020-05-22T21:23:10.0058888Z 20/05/22 21:19:32 INFO blockmanagement.BlockManager: dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-05-22T21:23:10.0059530Z 20/05/22 21:19:32 INFO blockmanagement.BlockManager: defaultReplication         = 1
2020-05-22T21:23:10.0059991Z 20/05/22 21:19:32 INFO blockmanagement.BlockManager: maxReplication             = 512
2020-05-22T21:23:10.0060551Z 20/05/22 21:19:32 INFO blockmanagement.BlockManager: minReplication             = 1
2020-05-22T21:23:10.0061023Z 20/05/22 21:19:32 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-05-22T21:23:10.0061478Z 20/05/22 21:19:32 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-05-22T21:23:10.0061958Z 20/05/22 21:19:32 INFO blockmanagement.BlockManager: encryptDataTransfer        = true
2020-05-22T21:23:10.0062417Z 20/05/22 21:19:32 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-05-22T21:23:10.0063332Z 20/05/22 21:19:32 INFO namenode.FSNamesystem: fsOwner             = hdfs/master.docker-hadoop-cluster-network@EXAMPLE.COM (auth:KERBEROS)
2020-05-22T21:23:10.0063900Z 20/05/22 21:19:32 INFO namenode.FSNamesystem: supergroup          = root
2020-05-22T21:23:10.0064314Z 20/05/22 21:19:32 INFO namenode.FSNamesystem: isPermissionEnabled = true
2020-05-22T21:23:10.0064739Z 20/05/22 21:19:32 INFO namenode.FSNamesystem: HA Enabled: false
2020-05-22T21:23:10.0065140Z 20/05/22 21:19:32 INFO namenode.FSNamesystem: Append Enabled: true
2020-05-22T21:23:10.0065552Z 20/05/22 21:19:32 INFO util.GSet: Computing capacity for map INodeMap
2020-05-22T21:23:10.0066146Z 20/05/22 21:19:32 INFO util.GSet: VM type       = 64-bit
2020-05-22T21:23:10.0066519Z 20/05/22 21:19:32 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-05-22T21:23:10.0066927Z 20/05/22 21:19:32 INFO util.GSet: capacity      = 2^20 = 1048576 entries
2020-05-22T21:23:10.0067322Z 20/05/22 21:19:32 INFO namenode.FSDirectory: ACLs enabled? false
2020-05-22T21:23:10.0067724Z 20/05/22 21:19:32 INFO namenode.FSDirectory: XAttrs enabled? true
2020-05-22T21:23:10.0068147Z 20/05/22 21:19:32 INFO namenode.NameNode: Caching file names occurring more than 10 times
2020-05-22T21:23:10.0068595Z 20/05/22 21:19:32 INFO util.GSet: Computing capacity for map cachedBlocks
2020-05-22T21:23:10.0069186Z 20/05/22 21:19:32 INFO util.GSet: VM type       = 64-bit
2020-05-22T21:23:10.0069562Z 20/05/22 21:19:32 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-05-22T21:23:10.0069966Z 20/05/22 21:19:32 INFO util.GSet: capacity      = 2^18 = 262144 entries
2020-05-22T21:23:10.0070657Z 20/05/22 21:19:32 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-05-22T21:23:10.0071163Z 20/05/22 21:19:32 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-05-22T21:23:10.0071637Z 20/05/22 21:19:32 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-05-22T21:23:10.0072119Z 20/05/22 21:19:32 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-05-22T21:23:10.0072614Z 20/05/22 21:19:32 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-05-22T21:23:10.0073094Z 20/05/22 21:19:32 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-05-22T21:23:10.0073579Z 20/05/22 21:19:33 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2020-05-22T21:23:10.0074124Z 20/05/22 21:19:33 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-05-22T21:23:10.0074645Z 20/05/22 21:19:33 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2020-05-22T21:23:10.0075361Z 20/05/22 21:19:33 INFO util.GSet: VM type       = 64-bit
2020-05-22T21:23:10.0075760Z 20/05/22 21:19:33 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-05-22T21:23:10.0076195Z 20/05/22 21:19:33 INFO util.GSet: capacity      = 2^15 = 32768 entries
2020-05-22T21:23:10.0076904Z 20/05/22 21:19:33 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1813050567-172.22.0.3-1590182373027
2020-05-22T21:23:10.0077704Z 20/05/22 21:19:33 INFO common.Storage: Storage directory /tmp/hadoop-hdfs/dfs/name has been successfully formatted.
2020-05-22T21:23:10.0078636Z 20/05/22 21:19:33 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-hdfs/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
2020-05-22T21:23:10.0079759Z 20/05/22 21:19:33 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-hdfs/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 315 bytes saved in 0 seconds.
2020-05-22T21:23:10.0080405Z 20/05/22 21:19:33 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2020-05-22T21:23:10.0080862Z 20/05/22 21:19:33 INFO util.ExitUtil: Exiting with status 0
2020-05-22T21:23:10.0081230Z 20/05/22 21:19:33 INFO namenode.NameNode: SHUTDOWN_MSG: 
2020-05-22T21:23:10.0081604Z /************************************************************
2020-05-22T21:23:10.0082215Z SHUTDOWN_MSG: Shutting down NameNode at master.docker-hadoop-cluster-network/172.22.0.3
2020-05-22T21:23:10.0082622Z ************************************************************/
2020-05-22T21:23:10.0082982Z WARNING: no policy specified for root@EXAMPLE.COM; defaulting to no policy
2020-05-22T21:23:10.0083611Z WARNING: no policy specified for hadoop-user@EXAMPLE.COM; defaulting to no policy
2020-05-22T21:23:10.0084443Z 20/05/22 21:19:51 WARN ipc.Client: Failed to connect to server: master.docker-hadoop-cluster-network/172.22.0.3:9000: try once and fail.
2020-05-22T21:23:10.0084930Z java.net.ConnectException: Connection refused
2020-05-22T21:23:10.0085269Z 	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2020-05-22T21:23:10.0085665Z 	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2020-05-22T21:23:10.0086146Z 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
2020-05-22T21:23:10.0086601Z 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
2020-05-22T21:23:10.0087023Z 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
2020-05-22T21:23:10.0087494Z 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
2020-05-22T21:23:10.0087937Z 	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
2020-05-22T21:23:10.0088378Z 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
2020-05-22T21:23:10.0088763Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
2020-05-22T21:23:10.0089147Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
2020-05-22T21:23:10.0089591Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
2020-05-22T21:23:10.0090094Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
2020-05-22T21:23:10.0090515Z 	at com.sun.proxy.$Proxy10.setSafeMode(Unknown Source)
2020-05-22T21:23:10.0091006Z 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setSafeMode(ClientNamenodeProtocolTranslatorPB.java:691)
2020-05-22T21:23:10.0091713Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-22T21:23:10.0092135Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-22T21:23:10.0092645Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-22T21:23:10.0093100Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-22T21:23:10.0093556Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
2020-05-22T21:23:10.0094135Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
2020-05-22T21:23:10.0094802Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
2020-05-22T21:23:10.0095375Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
2020-05-22T21:23:10.0095931Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
2020-05-22T21:23:10.0096348Z 	at com.sun.proxy.$Proxy11.setSafeMode(Unknown Source)
2020-05-22T21:23:10.0096735Z 	at org.apache.hadoop.hdfs.DFSClient.setSafeMode(DFSClient.java:2143)
2020-05-22T21:23:10.0097207Z 	at org.apache.hadoop.hdfs.DistributedFileSystem.setSafeMode(DistributedFileSystem.java:1359)
2020-05-22T21:23:10.0098117Z 	at org.apache.hadoop.hdfs.DistributedFileSystem.setSafeMode(DistributedFileSystem.java:1343)
2020-05-22T21:23:10.0098636Z 	at org.apache.hadoop.hdfs.tools.DFSAdmin.setSafeMode(DFSAdmin.java:644)
2020-05-22T21:23:10.0102850Z 	at org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:1977)
2020-05-22T21:23:10.0103448Z 	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
2020-05-22T21:23:10.0103853Z 	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
2020-05-22T21:23:10.0104275Z 	at org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2168)
2020-05-22T21:23:10.0109414Z safemode: Call From master.docker-hadoop-cluster-network/172.22.0.3 to master.docker-hadoop-cluster-network:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2020-05-22T21:23:10.0110219Z Authenticating as principal admin/admin with password.
2020-05-22T21:23:10.0110538Z Principal ""root@EXAMPLE.COM"" created.
2020-05-22T21:23:10.0110829Z Authenticating as principal admin/admin with password.
2020-05-22T21:23:10.0111595Z Entry for principal root with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/root/root.keytab.
2020-05-22T21:23:10.0112487Z Entry for principal root with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/root/root.keytab.
2020-05-22T21:23:10.0113390Z Entry for principal root with kvno 2, encryption type des3-cbc-sha1 added to keytab WRFILE:/root/root.keytab.
2020-05-22T21:23:10.0114243Z Entry for principal root with kvno 2, encryption type arcfour-hmac added to keytab WRFILE:/root/root.keytab.
2020-05-22T21:23:10.0115058Z Entry for principal root with kvno 2, encryption type des-hmac-sha1 added to keytab WRFILE:/root/root.keytab.
2020-05-22T21:23:10.0115886Z Entry for principal root with kvno 2, encryption type des-cbc-md5 added to keytab WRFILE:/root/root.keytab.
2020-05-22T21:23:10.0116355Z Authenticating as principal admin/admin with password.
2020-05-22T21:23:10.0116842Z Principal ""hadoop-user@EXAMPLE.COM"" created.
2020-05-22T21:23:10.0117160Z Authenticating as principal admin/admin with password.
2020-05-22T21:23:10.0117944Z Entry for principal hadoop-user with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/home/hadoop-user/hadoop-user.keytab.
2020-05-22T21:23:10.0118955Z Entry for principal hadoop-user with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/home/hadoop-user/hadoop-user.keytab.
2020-05-22T21:23:10.0119927Z Entry for principal hadoop-user with kvno 2, encryption type des3-cbc-sha1 added to keytab WRFILE:/home/hadoop-user/hadoop-user.keytab.
2020-05-22T21:23:10.0120861Z Entry for principal hadoop-user with kvno 2, encryption type arcfour-hmac added to keytab WRFILE:/home/hadoop-user/hadoop-user.keytab.
2020-05-22T21:23:10.0121806Z Entry for principal hadoop-user with kvno 2, encryption type des-hmac-sha1 added to keytab WRFILE:/home/hadoop-user/hadoop-user.keytab.
2020-05-22T21:23:10.0122751Z Entry for principal hadoop-user with kvno 2, encryption type des-cbc-md5 added to keytab WRFILE:/home/hadoop-user/hadoop-user.keytab.
2020-05-22T21:23:10.0123180Z Safe mode is OFF
2020-05-22T21:23:10.0123409Z Finished master initialization
2020-05-22T21:23:10.0123620Z Flink logs:
2020-05-22T21:23:11.4689466Z 20/05/22 21:23:11 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:23:11.9766773Z 20/05/22 21:23:11 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:23:12.2413807Z Total number of applications (application-types: [] and states: [NEW, NEW_SAVING, SUBMITTED, ACCEPTED, RUNNING, FINISHED, FAILED, KILLED]):1
2020-05-22T21:23:12.2415644Z                 Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL
2020-05-22T21:23:12.2418784Z application_1590182392090_0001	Flink Application Cluster	        Apache Flink	hadoop-user	   default	          FINISHED	            FAILED	           100%	                                N/A
2020-05-22T21:23:13.5805530Z 20/05/22 21:23:13 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:23:14.0501769Z 20/05/22 21:23:14 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:23:14.4321458Z Application ID: 
2020-05-22T21:23:15.1754151Z options parsing failed: Missing argument for option: applicationId
2020-05-22T21:23:15.1761443Z Retrieve logs for completed YARN applications.
2020-05-22T21:23:15.1790909Z usage: yarn logs -applicationId <application ID> [OPTIONS]
2020-05-22T21:23:15.1791209Z 
2020-05-22T21:23:15.1799114Z general options are:
2020-05-22T21:23:15.1801018Z  -am <AM Containers>             Prints the AM Container logs for this
2020-05-22T21:23:15.1801688Z                                  application. Specify comma-separated
2020-05-22T21:23:15.1802055Z                                  value to get logs for related AM
2020-05-22T21:23:15.1802639Z                                  Container. For example, If we specify -am
2020-05-22T21:23:15.1803047Z                                  1,2, we will get the logs for the first
2020-05-22T21:23:15.1803417Z                                  AM Container as well as the second AM
2020-05-22T21:23:15.1803794Z                                  Container. To get logs for all AM
2020-05-22T21:23:15.1804365Z                                  Containers, use -am ALL. To get logs for
2020-05-22T21:23:15.1804968Z                                  the latest AM Container, use -am -1. By
2020-05-22T21:23:15.1805361Z                                  default, it will only print out syslog.
2020-05-22T21:23:15.1805925Z                                  Work with -logFiles to get other logs
2020-05-22T21:23:15.1806519Z  -appOwner <Application Owner>   AppOwner (assumed to be current user if
2020-05-22T21:23:15.1806856Z                                  not specified)
2020-05-22T21:23:15.1807391Z  -containerId <Container ID>     ContainerId. By default, it will only
2020-05-22T21:23:15.1807755Z                                  print syslog if the application is
2020-05-22T21:23:15.1808345Z                                  runing. Work with -logFiles to get other
2020-05-22T21:23:15.1808682Z                                  logs.
2020-05-22T21:23:15.1809176Z  -help                           Displays help for all commands.
2020-05-22T21:23:15.1809746Z  -logFiles <Log File Name>       Work with -am/-containerId and specify
2020-05-22T21:23:15.1810310Z                                  comma-separated value to get specified
2020-05-22T21:23:15.1810700Z                                  container log files. Use ""ALL"" to fetch
2020-05-22T21:23:15.1811065Z                                  all the log files for the container.
2020-05-22T21:23:15.1812030Z  -nodeAddress <Node Address>     NodeAddress in the format nodename:port
2020-05-22T21:23:16.1892439Z Stopping slave2 ... 
2020-05-22T21:23:16.1893278Z Stopping slave1 ... 
2020-05-22T21:23:16.1893592Z Stopping master ... 
2020-05-22T21:23:16.1893810Z Stopping kdc    ... 
2020-05-22T21:23:27.1114337Z [4A[2K
2020-05-22T21:23:27.1122258Z Stopping slave2 ... [32mdone[0m
2020-05-22T21:23:27.1127667Z [4B[3A[2K
2020-05-22T21:23:27.1128148Z Stopping slave1 ... [32mdone[0m
2020-05-22T21:23:37.3854759Z [3B[2A[2K
2020-05-22T21:23:37.3856007Z Stopping master ... [32mdone[0m
2020-05-22T21:23:47.6345234Z [2B[1A[2K
2020-05-22T21:23:47.6345744Z Stopping kdc    ... [32mdone[0m
2020-05-22T21:23:47.6802158Z [1BRemoving slave2 ... 
2020-05-22T21:23:47.6802444Z Removing slave1 ... 
2020-05-22T21:23:47.6802672Z Removing master ... 
2020-05-22T21:23:47.6802874Z Removing kdc    ... 
2020-05-22T21:23:47.7175232Z [1A[2K
2020-05-22T21:23:47.7176176Z Removing kdc    ... [32mdone[0m
2020-05-22T21:23:47.7439794Z [1B[3A[2K
2020-05-22T21:23:47.7440633Z Removing slave1 ... [32mdone[0m
2020-05-22T21:23:47.7685690Z [3B[4A[2K
2020-05-22T21:23:47.7686586Z Removing slave2 ... [32mdone[0m
2020-05-22T21:23:47.8104183Z [4B[2A[2K
2020-05-22T21:23:47.8104745Z Removing master ... [32mdone[0m
2020-05-22T21:23:47.8105276Z [2BRemoving network docker-hadoop-cluster-network
2020-05-22T21:23:47.9572035Z [FAIL] Test script contains errors.
2020-05-22T21:23:47.9580367Z Checking for errors...
2020-05-22T21:23:47.9757366Z No errors in log files.
2020-05-22T21:23:47.9758092Z Checking for exceptions...
2020-05-22T21:23:47.9956993Z No exceptions in log files.
2020-05-22T21:23:47.9958318Z Checking for non-empty .out files...
2020-05-22T21:23:47.9977876Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/log/*.out: No such file or directory
2020-05-22T21:23:47.9983348Z No non-empty .out files.
2020-05-22T21:23:47.9983603Z 
2020-05-22T21:23:47.9984288Z [FAIL] 'Running Kerberized YARN application on Docker test (custom fs plugin)' failed after 4 minutes and 25 seconds! Test exited with exit code 1
{code}",,aljoscha,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18117,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 06:35:25 UTC 2020,,,,,,,,,,"0|z0f468:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Sep/20 06:26;rmetzger;I can not debug this issue because accessing the Flink logs doesn't work:
{code}
2020-05-22T21:23:10.0123620Z Flink logs:
2020-05-22T21:23:11.4689466Z 20/05/22 21:23:11 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:23:11.9766773Z 20/05/22 21:23:11 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:23:12.2413807Z Total number of applications (application-types: [] and states: [NEW, NEW_SAVING, SUBMITTED, ACCEPTED, RUNNING, FINISHED, FAILED, KILLED]):1
2020-05-22T21:23:12.2415644Z                 Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL
2020-05-22T21:23:12.2418784Z application_1590182392090_0001	Flink Application Cluster	        Apache Flink	hadoop-user	   default	          FINISHED	            FAILED	           100%	                                N/A
2020-05-22T21:23:13.5805530Z 20/05/22 21:23:13 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.22.0.3:8032
2020-05-22T21:23:14.0501769Z 20/05/22 21:23:14 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.22.0.3:10200
2020-05-22T21:23:14.4321458Z Application ID: 
2020-05-22T21:23:15.1754151Z options parsing failed: Missing argument for option: applicationId
2020-05-22T21:23:15.1761443Z Retrieve logs for completed YARN applications.
{code}

I will improve the test to properly print YARN logs.;;;","22/Sep/20 06:35;rmetzger;Resolved in 1.12: https://github.com/apache/flink/commit/ac776cb9cde9a8d0639b781f70bf47c470786874
Resolved in 1.11: e328c4fc719f4bafacc46b9e21de1a314eed0af2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docs for JDBC connector show licence and markup,FLINK-17905,13307104,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,24/May/20 21:37,25/May/20 02:15,13/Jul/23 08:07,25/May/20 02:15,1.11.0,,,,,1.11.0,,,,Documentation,,,,,0,pull-request-available,,,,,,jark,leonard,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 25 02:15:46 UTC 2020,,,,,,,,,,"0|z0f3z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/20 02:15;jark;Fixed in
 - master (1.12.0): a91db83e0cbc85c9cdb4c572304bc93f5526c08b
 - 1.11.0: ddeebfa8e4409d947094fedeed70f1141cfdd4ee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalog can't work with new table factory because of is_generic,FLINK-17896,13306917,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,23/May/20 03:43,16/Oct/20 10:55,13/Jul/23 08:07,26/May/20 06:33,,,,,,1.11.0,,,,Connectors / Hive,Table SQL / API,,,,0,pull-request-available,,,,"{code:java}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Unsupported options found for connector 'print'.Unsupported options:is_genericSupported options:connector
print-identifier
property-version
standard-error
{code}
This's because HiveCatalog put is_generic property into generic tables, but the new factory will not delete it.",,jark,leonard,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 06:33:36 UTC 2020,,,,,,,,,,"0|z0f2tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/20 06:33;lzljs3620320;master: e215365a4d6081e1a6d4e85343475465485a65da

release-1.11: da1ccb9cfae8be0abda7e85d15db3d38304336d2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default value of rows-per-second in datagen can be limited,FLINK-17895,13306913,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,liufangliang,lzljs3620320,lzljs3620320,23/May/20 03:30,11/Jun/20 12:37,13/Jul/23 08:07,03/Jun/20 09:02,,,,,,1.11.0,,,,Table SQL / API,,,,,0,pull-request-available,starter,,,,,jark,libenchao,liufangliang,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 09:02:28 UTC 2020,,,,,,,,,,"0|z0f2so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/20 10:36;liufangliang;[~lzljs3620320]  could you please assign this issue to me ? I have interest in this ;;;","30/May/20 02:54;liufangliang;[~lzljs3620320] can you add a description?;;;","03/Jun/20 09:02;lzljs3620320;master: c52e5792607d7be2c7fbe8ad1c7fcbcfd6cf3f0a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowGenerator in datagen connector should be serializable,FLINK-17894,13306912,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,23/May/20 03:20,25/May/20 06:23,13/Jul/23 08:07,25/May/20 06:23,,,,,,1.11.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,jark,libenchao,lzljs3620320,yangyichao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 25 06:23:39 UTC 2020,,,,,,,,,,"0|z0f2sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/20 06:23;lzljs3620320;master: edfb7c4d7a004fe60c8cd34ebca88d2f7cc5f212

release-1.11: 3f73694e9c005e62d661ed785c01bdd7060c1485;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL-CLI no exception stack,FLINK-17893,13306910,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,lzljs3620320,lzljs3620320,23/May/20 03:15,09/Jun/20 16:07,13/Jul/23 08:07,09/Jun/20 04:02,,,,,,1.11.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"If write a wrong DDL, only ""[ERROR] Unknown or invalid SQL statement"" message.

No exception stack in client and logs.",,godfreyhe,jark,leonard,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18009,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 04:02:34 UTC 2020,,,,,,,,,,"0|z0f2s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/20 03:56;godfreyhe;sql client should support {{-verbose}} option;;;","25/May/20 02:17;lzljs3620320;I doubt it, I think at least we should provide error message. User have no idea what is wrong.;;;","25/May/20 02:31;jark;I think [~lzljs3620320] means we should print the root cause message on the client, rather than a general error message which doesn't help anything. ;;;","09/Jun/20 04:02;jark;- master (1.12.0): 02b915a80a92307cc16bec229ec5faf175a2251d
- 1.11.0: c211946d83429e626c250bf1c03fdca447cec380;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 FlinkYarnSessionCli sets wrong execution.target type,FLINK-17891,13306842,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kkl0u,tangshangwen,tangshangwen,22/May/20 17:02,16/Jun/20 07:42,13/Jul/23 08:07,16/Jun/20 07:42,1.11.0,,,,,1.10.2,1.11.0,1.12.0,,Command Line Client,Deployment / YARN,,,,0,pull-request-available,,,,"I submitted a flink session job at the local YARN cluster, and I found that the *execution.target* is of the wrong type, which should be of yarn-session type

!image-2020-05-23-00-59-32-702.png|width=545,height=75!

!image-2020-05-23-01-00-19-549.png|width=544,height=94!

 ",,aljoscha,kkl0u,tangshangwen,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/20 16:59;tangshangwen;image-2020-05-23-00-59-32-702.png;https://issues.apache.org/jira/secure/attachment/13003777/image-2020-05-23-00-59-32-702.png","22/May/20 17:00;tangshangwen;image-2020-05-23-01-00-19-549.png;https://issues.apache.org/jira/secure/attachment/13003776/image-2020-05-23-01-00-19-549.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 07:42:37 UTC 2020,,,,,,,,,,"0|z0f2cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/20 17:10;tangshangwen;My idea is that if we start the flink session job with FlinkYarnSessionCli, we should set the *execution.target* to be of yarn-session type, not yarn-per-job. and the problem is in this line
{code:java}
effectiveConfiguration.setString(DeploymentOptions.TARGET, YarnJobClusterExecutor.NAME);{code}
 
{code:java}
// FlinkYarnSessionCli.java
@Override
public Configuration applyCommandLineOptionsToConfiguration(CommandLine commandLine) throws FlinkException {
   // we ignore the addressOption because it can only contain ""yarn-cluster""
   final Configuration effectiveConfiguration = new Configuration(configuration);

   applyDescriptorOptionToConfig(commandLine, effectiveConfiguration);

   final ApplicationId applicationId = getApplicationId(commandLine);
   if (applicationId != null) {
      final String zooKeeperNamespace;
      if (commandLine.hasOption(zookeeperNamespace.getOpt())){
         zooKeeperNamespace = commandLine.getOptionValue(zookeeperNamespace.getOpt());
      } else {
         zooKeeperNamespace = effectiveConfiguration.getString(HA_CLUSTER_ID, applicationId.toString());
      }

      effectiveConfiguration.setString(HA_CLUSTER_ID, zooKeeperNamespace);
      effectiveConfiguration.setString(YarnConfigOptions.APPLICATION_ID, ConverterUtils.toString(applicationId));
      effectiveConfiguration.setString(DeploymentOptions.TARGET, YarnSessionClusterExecutor.NAME);
   } else {
      effectiveConfiguration.setString(DeploymentOptions.TARGET, YarnJobClusterExecutor.NAME);
   }
 ...
}{code};;;","22/May/20 17:17;tangshangwen;This is a simple fix, assign this issue to me and I can submit the code if possible

 ;;;","23/May/20 08:58;tangshangwen;Hi [~lzljs3620320] [~jark] , Can you assign this task to me ? I'll try to fix it. Thanks~;;;","25/May/20 02:06;wangyang0918;[~tangshangwen] Thanks a lot for reporting this issue. Could you share the full command how do you start a Flink session cluster and then submit a job to the existing session? In {{FlinkYarnSessionCli}}, only when the applicationId is not specified, we assume that it is a per-job cluster. I think it is the expected behavior.;;;","25/May/20 11:42;kkl0u;Hi [~tangshangwen], as [~fly_in_gis] said, could you describe what exactly is your scenario?;;;","12/Jun/20 15:09;kkl0u;[~tangshangwen] could you please verify that the PR solves the issue you mention?;;;","15/Jun/20 10:06;wangyang0918;I have verified the PR could solve this issue.;;;","16/Jun/20 07:42;kkl0u;Fixed on master with  cc1f1a47123a05a0f5e935a87e683c00c02c5ba4
on release-1.11 with 092fdf880b39387917b9f6fd49344fbb240f741e
and release-1.10 with 2bf5ea26d8eaa931924ab89c0ae7396adda8bc50;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-hive jar contains wrong class in its SPI config file org.apache.flink.table.factories.TableFactory,FLINK-17889,13306820,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,zjffdu,zjffdu,22/May/20 15:51,16/Oct/20 10:55,13/Jul/23 08:07,26/May/20 05:38,1.11.0,,,,,1.11.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"These 2 classes are in flink-connector-hive jar's SPI config file
{code:java}
org.apache.flink.orc.OrcFileSystemFormatFactory
License.org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory {code}
Due to this issue, I get the following exception in zeppelin side.
{code:java}
Caused by: java.util.ServiceConfigurationError: org.apache.flink.table.factories.TableFactory: Provider org.apache.flink.orc.OrcFileSystemFormatFactory not a subtypeCaused by: java.util.ServiceConfigurationError: org.apache.flink.table.factories.TableFactory: Provider org.apache.flink.orc.OrcFileSystemFormatFactory not a subtype at java.util.ServiceLoader.fail(ServiceLoader.java:239) at java.util.ServiceLoader.access$300(ServiceLoader.java:185) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:376) at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) at java.util.ServiceLoader$1.next(ServiceLoader.java:480) at java.util.Iterator.forEachRemaining(Iterator.java:116) at org.apache.flink.table.factories.TableFactoryService.discoverFactories(TableFactoryService.java:214) ... 35 more {code}",,lzljs3620320,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 05:38:29 UTC 2020,,,,,,,,,,"0|z0f280:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/20 15:55;zjffdu;\cc [~lirui] [~lzljs3620320];;;","26/May/20 05:38;lzljs3620320;master: ec0288c6df4edf63ef6601cde2a7b45eaa85cda3

release-1.11: 643f4a1283b82bcbd9cc08878bcee298e10d2bcd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transient watermark attribute should be initial at runtime in streaming file operators,FLINK-17878,13306700,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhoujira86,zhoujira86,zhoujira86,22/May/20 07:37,25/May/20 02:28,13/Jul/23 08:07,25/May/20 02:28,1.11.0,,,,,1.11.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,"StreamingFileWriter has a 

private transient long currentWatermark = Long.MIN_VALUE;

 

in case developer wants to create a custom bucket assigner, it will receive a currentWatermark as 0, this might be conflict with the original flink approach to handle a min_long.

 

should we remove the transient key word?

 ",,gaoyunhaii,jark,lzljs3620320,mingleizhang,zhoujira86,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17885,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 25 02:28:40 UTC 2020,,,,,,,,,,"0|z0f1hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/20 07:38;zhoujira86;[~lzljs3620320] please help review this comment;;;","22/May/20 08:10;gaoyunhaii;[~zhoujira86] Do you mean StreamingFileWriter#currentWatermark should be initialized to 0 instead of MIN_LONG ?;;;","22/May/20 08:14;zhoujira86;[~gaoyunhaii], No , if we use process time, the watermark will be MIN_LONG. so the currentWatermark should be initialized to MIN_LONG, but with the transient, the currentWatermark will be initialized to 0... As I am working on similar feature , i found this issue. not sure whether i made some mistake...;;;","22/May/20 08:40;zhoujira86;[~gaoyunhaii] for my understanding, the operator is serialized then submit to the flink. and transient value will not be serialized, and then runtime value of the currentWatermark will be initialized to zero...;;;","22/May/20 09:20;gaoyunhaii;Hi [~zhoujira86] I think you are right that the transient value will be set to 0 after deserialization and the behavior is different from StreamingFileSink (which inherits the currentWatermarkField from StreamSink) and it should make differences for the records before the first watermark. [~lzljs3620320] could double confirm on this issue. 

One more thing, this issue seems to be belong to Table SQL / API instead of API / DataStream (labeled in the component field of this issue) ?;;;","22/May/20 09:22;zhoujira86;[~gaoyunhaii] corrected, may i submit a PR for this?;;;","22/May/20 09:25;gaoyunhaii;[~zhoujira86] I think it is ok from my side, but I think it might be better to also wait for Jingsong to confirm the issue. :);;;","22/May/20 11:10;zhoujira86;[~gaoyunhaii] thx a lot, Can you please help assign the issue to me? :) I can get some opinion from [~lzljs3620320] on how to modify code;;;","22/May/20 13:37;lzljs3620320;Thanks [~zhoujira86] for reporting this issue. Yes, there may be a problem when watermark is negative. We can remove transient or initial in runtime. Assigned to you.;;;","22/May/20 13:39;lzljs3620320;[~zhoujira86] Can you create a PR for release-1.11?;;;","22/May/20 14:11;jark;I don't think removing {{transient}} can help here. The serialized watermark doesn't help when job is restored, the value is still the initial value. If you want to restore the watermark to the previous one, you have to use {{State}} in streaming. However, this is another big topic and make the logic complex and will involve some performance concern. Besides, we didn't find any problem with current design so far. The watermark will be adjust to a correct value once there is new records coming in. ;;;","22/May/20 14:23;lzljs3620320;IIUC, operators only be deserialized from {{JobGraph}} which comes from user initialization. So I don't mean removing transient help watermark persistence.

But it can work for initialization we want (Long.Min_value in runtime). If you take a look to {{SinkOperator}} and others, the watermark field works like this.

It is not a big problem. In this case, we don't want to make watermark field persistent.;;;","22/May/20 14:46;jark;Oh! I got what you mean [~lzljs3620320]. Thanks for the explanation. ;;;","23/May/20 02:53;zhoujira86;[~lzljs3620320] Hi Jingsong, thx for the advice, I have created another PR for release-1.11

[~jark] Hi Jark, thx for the advice, when we use the process time mode , the watermark will not be overwritten. And when I tried to work on this issue, I didnt remove the transient, I init the value in initializeState.;;;","25/May/20 02:28;lzljs3620320;master: e345d99a9e55a91d96fc5fc0e575ab7823f6fb74

release-1.11: ff3ea323c8d98e075a742afad6b08ec18a830046;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dependent jars are missing to be shipped to cluster in scala shell,FLINK-17870,13306639,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjffdu,zjffdu,zjffdu,22/May/20 00:17,16/Oct/20 10:55,13/Jul/23 08:07,25/May/20 19:58,1.11.0,,,,,1.10.2,1.11.0,1.12.0,,Scala Shell,,,,,0,pull-request-available,,,,,,kkl0u,zgq25302111,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 25 19:58:57 UTC 2020,,,,,,,,,,"0|z0f13s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/20 19:58;kkl0u;Merged on master with ea54a54be1ea5629ccbd2421aa6f91b19870d40c
on release-1.11 with 714369797781e1a49aabf9e45f6ce8d09cb6336a
and on release-1.10 with 492f9de2b9040a4ca6c45d5060bc776448682f07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the race condition of aborting unaligned checkpoint,FLINK-17869,13306524,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,zjwang,zjwang,21/May/20 15:36,11/Jun/20 03:18,13/Jul/23 08:07,10/Jun/20 20:34,,,,,,1.11.0,1.12.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"On ChannelStateWriter side, the lifecycle of checkpoint should be as follows:

start -> in progress/abort -> stop.

The ChannelStateWriteResult is created during #start, and removed by #abort or #stop processes. There are some potential race conditions here:
 * #start is called while receiving the first barrier by netty thread and schedule to execute the checkpoint
 * The task thread might process cancel checkpoint and call #abort before performing the above respective checkpoint
 * The checkpoint can still be executed by task thread afterwards even thought the above abort happened before, because we can not remove the checkpoint action from mailbox during aborting.
 * While checkpoint executing, it will call `ChannelStateWriter#getWriteResult` then it would cause `IllegalStateException` because the respective result was already removed in advance during handling #abort method before.
 * Therefore it will cause unnecessary task failure during performing checkpoint

I guess we do not want to fail the task when one checkpoint is aborted by design. And the illegal state check during ChannelStateWriter#getWriteResult was mainly proposed for normal process validation I guess.

If we do not remove the ChannelStateWriteResult while handling #abort and rely on #stop to remove it, then it might probably exist another scenario that the checkpoint will never be performed after #start (we have another mechanism to exit the triggering checkpoint in advance if the abort is sent by CheckpointCoordinator), then the legacy ChannelStateWriteResult will be retained inside ChannelStateWriter long time.

Maybe the potential option to fix this issue is to let SubtaskCheckpointCoordinatorImpl handle the exception from ChannelStateWriter#getWriteResult properly to not fail the task in the aborted case.",,fanrui,felixzheng,klion26,pnowojski,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17768,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 03:18:11 UTC 2020,,,,,,,,,,"0|z0f0e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/20 03:18;zjwang;Merged in release-1.11: 7bb3ffa91a9916348d2f0a6a2e6cba4b109be56e, 

d8069249703bbe7858e0c6a044deb54ce75e3989

 

Merged in master: 64ff6765036dd00761f79d9e206f6128c5bad671, 

91df1a5fd0f4937a852a82a6139a1a0ed28165e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Proctime in DDL can not work in batch mode,FLINK-17868,13306497,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,lzljs3620320,lzljs3620320,21/May/20 13:10,28/May/21 07:13,13/Jul/23 08:07,22/Jan/21 11:46,,,,,,1.13.0,,,,Table SQL / Planner,,,,,0,pull-request-available,sprint,,,The data of this proctime column will be all null. Should same to current timestamp.,,felixzheng,fsk119,godfreyhe,hailong wang,jark,leonard,libenchao,lzljs3620320,qingyue,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17189,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 22 11:46:47 UTC 2021,,,,,,,,,,"0|z0f088:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/21 12:47;qingyue;Hi [~jark] and [~lzljs3620320], I'm interested in this issue and want to have a try.

I came up with three possible solutions and listed the proposed and rejected alternatives and want to hear your opinions. Please correct me if I'm wrong.
h3. Proposed plan

Introduce a new rule and program to `FlinkBatchProgram` to rewrite `PROCTIME()` to `CURRENT_TIMESTAMP()` at planning phase.
h3. Alternative plans

1. Introduce `FlinkRelTimeIndicatorProgram` to `FlinkBatchProgram` (i.e. reuse `RelTimeIndicatorConveter`) and rewrite `PROCTIME ` to `PROCTIME_MATERIALIZE ` during planning phase.
 * Pro: translated batch plan is same as stream plan `Calc(select=[a, c, PROCTIME_MATERIALIZE(PROCTIME()) AS EXPR$2])`.

 * Con: `PROCTIME_MATERIALIZE ` is transformed to process function of`AbstractProcessStreamOperator`, which introduces `ctx.timerService()`. While `BatchExecCalc` uses `TableStreamOperator` as operator base class, it causes the generated code to fail to compile. To solve this, either modify the codegen logic to avoid using `timerService` for batch or move `ctx` from `AbstractProcessStreamOperator` to its parent `TableStreamOperator`.

2. We don't change the current plan(i.e., `Calc(select=[a, c, PROCTIME() AS EXPR$2]` remains unchanged). Instead, we modify the codegen logic at the transformation phase. We add a check on `PROCTIME` when `ExprCodeGenerator#generateCallExpression` is called, and if it's a batch transformation, we generate `currentTimestamp` directly.
 * Pro: the modified scope is limited to a minimum.

 * Con: It may not be proper to distinguish stream vs. batch during the transformation phase.

 

Thanks, Jane.;;;","05/Jan/21 05:41;jark;Hi [~qingyue], I'm perfer the Alternative#1, because we should keep the semantic/behavior consistent between streaming and batch. {{CURRENT_TIMESTAMP()}} is not a synonyms for {{PROCTIME()}}.

Regarding to the implementation, I think we can try to merge {{AbstractProcessStreamOperator}} into {{TableStreamOperator}}, then streaming and batch operators can share the same base class. 

I assigned this issue to you. ;;;","22/Jan/21 11:46;jark;Fixed in master: c5bb49039434834b7543bdc58b1e36bf80383e5b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InvalidPathException was thrown when running the test cases of PyFlink on Windows,FLINK-17866,13306480,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,zhongwei,zhongwei,21/May/20 11:43,16/Oct/20 10:54,13/Jul/23 08:07,22/May/20 12:48,1.11.0,,,,,1.11.0,,,,API / Python,,,,,0,pull-request-available,,,,"When running the test_dependency.py on Windows，such exception was thrown:
{code:java}
Error
Traceback (most recent call last):
  File ""C:\Users\zw144119\AppData\Local\Continuum\miniconda3\envs\py36\lib\unittest\case.py"", line 59, in testPartExecutor
    yield
  File ""C:\Users\zw144119\AppData\Local\Continuum\miniconda3\envs\py36\lib\unittest\case.py"", line 605, in run
    testMethod()
  File ""D:\flink\flink-python\pyflink\table\tests\test_dependency.py"", line 55, in test_add_python_file
    self.t_env.execute(""test"")
  File ""D:\flink\flink-python\pyflink\table\table_environment.py"", line 1049, in execute
    return JobExecutionResult(self._j_tenv.execute(job_name))
  File ""C:\Users\zw144119\AppData\Local\Continuum\miniconda3\envs\py36\lib\site-packages\py4j\java_gateway.py"", line 1286, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""D:\flink\flink-python\pyflink\util\exceptions.py"", line 147, in deco
    return f(*a, **kw)
  File ""C:\Users\zw144119\AppData\Local\Continuum\miniconda3\envs\py36\lib\site-packages\py4j\protocol.py"", line 328, in get_return_value
    format(target_id, ""."", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o4.execute.
: java.nio.file.InvalidPathException: Illegal char <:> at index 2: /C:/Users/zw144119/AppData/Local/Temp/tmp0x4273cg/python_file_dir_cfb9e8fe-2812-4a89-ae46-5dc3c844d62c/test_dependency_manage_lib.py
  at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182)
  at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153)
  at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77)
  at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94)
  at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255)
  at java.nio.file.Paths.get(Paths.java:84)
  at org.apache.flink.core.fs.local.LocalFileSystem.pathToFile(LocalFileSystem.java:314)
  at org.apache.flink.core.fs.local.LocalFileSystem.getFileStatus(LocalFileSystem.java:110)
  at org.apache.flink.runtime.jobgraph.JobGraphUtils.addUserArtifactEntries(JobGraphUtils.java:52)
  at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:186)
  at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:109)
  at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:850)
  at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:52)
  at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:43)
  at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:55)
  at org.apache.flink.client.deployment.executors.LocalExecutor.getJobGraph(LocalExecutor.java:98)
  at org.apache.flink.client.deployment.executors.LocalExecutor.execute(LocalExecutor.java:79)
  at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1786)
  at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1687)
  at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:74)
  at org.apache.flink.table.planner.delegation.ExecutorBase.execute(ExecutorBase.java:52)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1167)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
  at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
  at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
  at java.lang.Thread.run(Thread.java:748)
{code}
It seems the windows-style path is not recognized by the ""Paths.get()"" method.",,dian.fu,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 12:48:24 UTC 2020,,,,,,,,,,"0|z0f04g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/20 12:48;dian.fu;Merged via
- master: 50c9f9dc3961a542a5ec3aec252b8e9066749d96
- release-1.11: 8c3664c3b50b01c42857356d03d96af1ab661c84;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Channel state handles, when inlined, duplicate underlying data",FLINK-17861,13306407,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,roman,roman,21/May/20 08:17,28/May/20 06:15,13/Jul/23 08:07,28/May/20 06:15,1.11.0,,,,,1.11.0,,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,"If Unaligned checkpoints are enabled, channel state is written as state handles. Each channel has a handle and each such handle references the same underlying {{streamStateHandle}} (this is done to have a single file per subtask).
But, if the state is less then {{state.backend.fs.memory-threshold}}, the data is sent directly to JM as a byteStreamHandle. This causes each channel state handle to hold the whole subtask state.

This PR solves this by extracting relevant potions of the underlying handles if they are {{byteStreamHandle}}s.",,felixzheng,klion26,pnowojski,roman,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 06:15:56 UTC 2020,,,,,,,,,,"0|z0ezo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/20 06:15;pnowojski;Merged to release-1.11 as bd22ebf971^^..bd22ebf971
Merged to master as 1b824d5947^^..1b824d5947;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PostgresCatalogITCase . testGroupByInsert() fails on CI,FLINK-17850,13306250,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,sewen,sewen,20/May/20 17:08,16/Oct/20 10:53,13/Jul/23 08:07,21/May/20 13:23,1.11.0,,,,,1.11.0,1.12.0,,,Table SQL / Ecosystem,,,,,0,pull-request-available,test-stability,,,"{{org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase . testGroupByInsert}}


Error:
{code}
2020-05-20T16:36:33.9647037Z org.apache.flink.table.api.ValidationException: 
2020-05-20T16:36:33.9647354Z Field types of query result and registered TableSink mypg.postgres.primitive_table2 do not match.

2020-05-20T16:36:33.9648233Z Query schema: [int: INT NOT NULL, EXPR$1: VARBINARY(2147483647) NOT NULL, short: SMALLINT NOT NULL, EXPR$3: BIGINT, EXPR$4: FLOAT, EXPR$5: DOUBLE, EXPR$6: DECIMAL(10, 5), EXPR$7: BOOLEAN, EXPR$8: VARCHAR(2147483647), EXPR$9: CHAR(1) NOT NULL, EXPR$10: CHAR(1) NOT NULL, EXPR$11: VARCHAR(20), EXPR$12: TIMESTAMP(5), EXPR$13: DATE, EXPR$14: TIME(0), EXPR$15: DECIMAL(38, 18)]

2020-05-20T16:36:33.9650272Z Sink schema: [int: INT, bytea: VARBINARY(2147483647), short: SMALLINT, long: BIGINT, real: FLOAT, double_precision: DOUBLE, numeric: DECIMAL(10, 5), decimal: DECIMAL(10, 1), boolean: BOOLEAN, text: VARCHAR(2147483647), char: CHAR(1), character: CHAR(3), character_varying: VARCHAR(20), timestamp: TIMESTAMP(5), date: DATE, time: TIME(0), default_numeric: DECIMAL(38, 18)]

2020-05-20T16:36:33.9651218Z 	at org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSchemaAndApplyImplicitCast(TableSinkUtils.scala:100)
2020-05-20T16:36:33.9651689Z 	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:218)
2020-05-20T16:36:33.9652136Z 	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:193)
2020-05-20T16:36:33.9652936Z 	at scala.Option.map(Option.scala:146)
2020-05-20T16:36:33.9653593Z 	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:193)
2020-05-20T16:36:33.9653993Z 	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:152)
2020-05-20T16:36:33.9654428Z 	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:152)
2020-05-20T16:36:33.9654841Z 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
2020-05-20T16:36:33.9655221Z 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
2020-05-20T16:36:33.9655759Z 	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
2020-05-20T16:36:33.9656072Z 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
2020-05-20T16:36:33.9656413Z 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
2020-05-20T16:36:33.9656890Z 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2020-05-20T16:36:33.9657211Z 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
2020-05-20T16:36:33.9657525Z 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2020-05-20T16:36:33.9657878Z 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:152)
2020-05-20T16:36:33.9658350Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1217)
2020-05-20T16:36:33.9658784Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:663)
2020-05-20T16:36:33.9659391Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:750)
2020-05-20T16:36:33.9659856Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:653)
2020-05-20T16:36:33.9660507Z 	at org.apache.flink.table.planner.runtime.utils.TableEnvUtil$.execInsertSqlAndWaitResult(TableEnvUtil.scala:27)
2020-05-20T16:36:33.9661115Z 	at org.apache.flink.table.planner.runtime.utils.TableEnvUtil.execInsertSqlAndWaitResult(TableEnvUtil.scala)
2020-05-20T16:36:33.9661583Z 	at org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.testGroupByInsert(PostgresCatalogITCase.java:88)
{code}

Full log: https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/25/logs/93",,f.pompermaier,jark,leonard,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 13:23:30 UTC 2020,,,,,,,,,,"0|z0eypc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/20 18:01;rmetzger;also on master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1954&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","20/May/20 19:04;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1963&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","20/May/20 19:08;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1962&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","20/May/20 19:24;f.pompermaier;I'll try to look into it. This test was introduced by [~jark] during the PR review so I need to find out what's wrong with it;;;","21/May/20 13:23;jark;Fixed in 
- master (1.12.0): 4dbc18def80403df3387a2cc468aeb4f17ed3df5
- 1.11.0: baaaf9752c68adeb48f5249f18ac101a5aca9931;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException happens when codegen StreamExec operator,FLINK-17847,13306221,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,20/May/20 15:52,04/Jun/20 14:47,13/Jul/23 08:07,04/Jun/20 14:47,1.10.0,1.11.0,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"user case:
{code:java}
//source table 
create table json_table( 
w_es BIGINT, 
w_type STRING, 
w_isDdl BOOLEAN,
 w_data ARRAY<ROW<pay_info STRING, online_fee DOUBLE, sign STRING, account_pay_fee DOUBLE>>,
 w_ts TIMESTAMP(3), 
w_table STRING) WITH (
  'connector.type' = 'kafka',
  'connector.version' = '0.10',
  'connector.topic' = 'json-test2',
  'connector.properties.zookeeper.connect' = 'localhost:2181',
  'connector.properties.bootstrap.servers' = 'localhost:9092',
  'connector.properties.group.id' = 'test-jdbc',
  'connector.startup-mode' = 'earliest-offset',
  'format.type' = 'json',
  'format.derive-schema' = 'true'
)
// real data:
{""w_es"":1589870637000,""w_type"":""INSERT"",""w_isDdl"":false,""w_data"":[{""pay_info"":""channelId=82&onlineFee=89.0&outTradeNo=0&payId=0&payType=02&rechargeId=4&totalFee=89.0&tradeStatus=success&userId=32590183789575&sign=00"",""online_fee"":""89.0"",""sign"":""00"",""account_pay_fee"":""0.0""}],""w_ts"":""2020-05-20T13:58:37.131Z"",""w_table"":""cccc111""}


//query
select w_ts, 'test' as city1_id,  w_data[0].pay_info AS cate3_id,
 w_data as pay_order_id from json_table

{code}
~exception:~
{code:java}
//
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1427848Caused by: java.lang.ArrayIndexOutOfBoundsException: 1427848 at org.apache.flink.table.runtime.util.SegmentsUtil.getByteMultiSegments(SegmentsUtil.java:598) at org.apache.flink.table.runtime.util.SegmentsUtil.getByte(SegmentsUtil.java:590) at org.apache.flink.table.runtime.util.SegmentsUtil.bitGet(SegmentsUtil.java:534) at org.apache.flink.table.dataformat.BinaryArray.isNullAt(BinaryArray.java:117) at StreamExecCalc$10.processElement(Unknown Source)
{code}
 

Looks like in the codegen StreamExecCalc$10 operator some operation visit a '-1' index which should be wrong, this bug exits both in 1.10 and 1.11

 
{code:java}
public class StreamExecCalc$10 extends org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator
    implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {

    private final Object[] references;

    private final org.apache.flink.table.dataformat.BinaryString str$3 = org.apache.flink.table.dataformat.BinaryString.fromString(""test"");

    private transient org.apache.flink.table.runtime.typeutils.BaseArraySerializer typeSerializer$5;
    final org.apache.flink.table.dataformat.BoxedWrapperRow out = new org.apache.flink.table.dataformat.BoxedWrapperRow(4);
    private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);

    public StreamExecCalc$10(
        Object[] references,
        org.apache.flink.streaming.runtime.tasks.StreamTask task,
        org.apache.flink.streaming.api.graph.StreamConfig config,
        org.apache.flink.streaming.api.operators.Output output) throws Exception {
        this.references = references;
        typeSerializer$5 = (((org.apache.flink.table.runtime.typeutils.BaseArraySerializer) references[0]));
        this.setup(task, config, output);
    }

    @Override
    public void open() throws Exception {
        super.open();

    }

    @Override
    public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
        org.apache.flink.table.dataformat.BaseRow in1 = (org.apache.flink.table.dataformat.BaseRow) element.getValue();

        org.apache.flink.table.dataformat.SqlTimestamp field$2;
        boolean isNull$2;
        org.apache.flink.table.dataformat.BaseArray field$4;
        boolean isNull$4;
        org.apache.flink.table.dataformat.BaseArray field$6;
        org.apache.flink.table.dataformat.BinaryString field$8;
        boolean isNull$8;
        org.apache.flink.table.dataformat.BinaryString result$9;
        boolean isNull$9;

        isNull$2 = in1.isNullAt(4);
        field$2 = null;
        if (!isNull$2) {
            field$2 = in1.getTimestamp(4, 3);
        }

        isNull$4 = in1.isNullAt(3);
        field$4 = null;
        if (!isNull$4) {
            field$4 = in1.getArray(3);
        }
        field$6 = field$4;
        if (!isNull$4) {
            field$6 = (org.apache.flink.table.dataformat.BaseArray) (typeSerializer$5.copy(field$6));
        }
        out.setHeader(in1.getHeader());
        if (isNull$2) {
            out.setNullAt(0);
        } else {
            out.setNonPrimitiveValue(0, field$2);
        }

        if (false) {
            out.setNullAt(1);
        } else {
            out.setNonPrimitiveValue(1, ((org.apache.flink.table.dataformat.BinaryString) str$3));
        }

        boolean isNull$7 = isNull$4 || false || field$6.isNullAt(((int) 0) - 1);
        org.apache.flink.table.dataformat.BaseRow result$7 = isNull$7 ? null : field$6.getRow(((int) 0) - 1, 4);

        if (isNull$7) {
            result$9 = org.apache.flink.table.dataformat.BinaryString.EMPTY_UTF8;
            isNull$9 = true;
        }
        else {
            isNull$8 = result$7.isNullAt(0);
            field$8 = org.apache.flink.table.dataformat.BinaryString.EMPTY_UTF8;
            if (!isNull$8) {
                field$8 = result$7.getString(0);
            }
            result$9 = field$8;
            isNull$9 = isNull$8;
        }

        if (isNull$9) {
            out.setNullAt(2);
        } else {
            out.setNonPrimitiveValue(2, result$9);
        }

        if (isNull$4) {
            out.setNullAt(3);
        } else {
            out.setNonPrimitiveValue(3, field$6);
        }
        output.collect(outElement.replace(out));
   }

    @Override
    public void close() throws Exception {
        super.close();

    }
}
    
{code}
 

 

 

 ",,begginghard,jark,leonard,libenchao,lzljs3620320,yesorno,yuwang0917@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 14:47:22 UTC 2020,,,,,,,,,,"0|z0eyiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/20 02:19;libenchao;[~Leonard Xu] This problem exists since 1.9 which use are using now. And I think there are two points here:
 # The array index in flink starts from 1, then we should throw exception in parsing/validating phase if index <= 0?
 # If the index > 0, which will pass the parsing/validating phase, and we should give a check in codegen, and throws Flink specific exception for out of bounds array access, or just return a null by default.

WDYT?;;;","21/May/20 02:50;jark;Thanks for looking into this [~libenchao]. I'm +1 to the #1. 
I'm not sure about #2, returning null may confuse users what's the true value of the element in the array (null or out of bound?).
A correct way to access array elements should be:

{code:java}
select w_data[5] from T where CARDINALITY(w_data) >= 5;
{code}
;;;","21/May/20 02:58;lzljs3620320;Calcite comments:
{code:java}
/**
 * The item operator {@code [ ... ]}, used to access a given element of an
 * array or map. For example, {@code myArray[3]} or {@code ""myMap['foo']""}.
 *
 * <p>The SQL standard calls the ARRAY variant a
 * &lt;array element reference&gt;. Index is 1-based. The standard says
 * to raise ""data exception - array element error"" but we currently return
 * null.</p>
 *
 * <p>MAP is not standard SQL.</p>
 */
public static final SqlOperator ITEM = new SqlItemOperator();
{code}
Benchao's proposal looks good, here is my thought, we should return null in all case of array index out of bounds. NOTE the index may be an input reference instead of a constant.;;;","21/May/20 03:02;lzljs3620320;For a constant index with ""<= 0"", we can throw exception or not throw. I have no inclination.;;;","21/May/20 03:19;leonard;Thanks [~libenchao] [~jark] [~lzljs3620320] for involving.

I'm +1 to throw a exception in compile phase if the index <=0， and I'm also like follow the calcite to return a null when happened ArrayIndexOutOfBoundsException in runtime phase.

HDYT？

 ;;;","21/May/20 03:23;jark;I'm fine with this.;;;","21/May/20 04:04;libenchao;Also +1 to what [~Leonard Xu] summarized.;;;","04/Jun/20 14:47;jark;- master (1.12.0): 3ee4c1f0a094d7562f4188b9fc93a2313e872cde
- 1.11.0: 79b64fc42598eb799e7e4da28fc2b4ab6e446506;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-walkthrough-table-scala failed on azure,FLINK-17846,13306206,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,20/May/20 14:52,20/May/20 15:06,13/Jul/23 08:07,20/May/20 15:06,1.11.0,,,,,1.11.0,,,,Table SQL / API,Tests,,,,0,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1941&view=results

{code}
2020-05-20T14:38:59.8981285Z [WARNING]  org.apache.flink:flink-scala_2.11:1.12-SNAPSHOT requires scala version: 2.11.12
2020-05-20T14:38:59.8982349Z [WARNING]  org.scala-lang:scala-compiler:2.11.12 requires scala version: 2.11.12
2020-05-20T14:38:59.8983299Z [WARNING]  org.scala-lang.modules:scala-xml_2.11:1.0.5 requires scala version: 2.11.7
2020-05-20T14:38:59.8983897Z [WARNING] Multiple versions of scala libraries detected!
2020-05-20T14:38:59.8984777Z [INFO] /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-53406840715/flink-walkthrough-table-scala/src/main/scala:-1: info: compiling
2020-05-20T14:38:59.8986393Z [INFO] Compiling 1 source files to /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-53406840715/flink-walkthrough-table-scala/target/classes at 1589985538160
2020-05-20T14:38:59.8987734Z [ERROR] /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-53406840715/flink-walkthrough-table-scala/src/main/scala/org/apache/flink/walkthrough/SpendReport.scala:28: error: not found: value BatchTableEnvironment
2020-05-20T14:38:59.8988549Z [ERROR]     val tEnv = BatchTableEnvironment.create(env)
2020-05-20T14:38:59.8988905Z [ERROR]                ^
2020-05-20T14:38:59.8989186Z [ERROR] one error found
2020-05-20T14:38:59.8990571Z [INFO] ------------------------------------------------------------------------
2020-05-20T14:38:59.8991177Z [INFO] BUILD FAILURE
2020-05-20T14:38:59.8992000Z [INFO] ------------------------------------------------------------------------
2020-05-20T14:38:59.8992556Z [INFO] Total time: 3.627 s
2020-05-20T14:38:59.8993292Z [INFO] Finished at: 2020-05-20T14:38:59+00:00
2020-05-20T14:38:59.8993939Z [INFO] Final Memory: 21M/305M
2020-05-20T14:38:59.8994935Z [INFO] ------------------------------------------------------------------------
2020-05-20T14:38:59.8996009Z [ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile (default) on project flink-walkthrough-table-scala: wrap: org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1) -> [Help 1]
2020-05-20T14:38:59.8996670Z [ERROR] 
2020-05-20T14:38:59.8997248Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2020-05-20T14:38:59.8997936Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2020-05-20T14:38:59.8998292Z [ERROR] 
2020-05-20T14:38:59.8998695Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2020-05-20T14:38:59.8999194Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{code}",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 20 15:06:49 UTC 2020,,,,,,,,,,"0|z0eyfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/20 15:06;dwysakowicz;Fixed in:
master: 400df32ab2a18a52c8511a901a4dd22cb6827700
1.11: 8044166877efc42c42a80344992d66ba18748a4d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression on 19.05.2020,FLINK-17842,13306161,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,pnowojski,pnowojski,20/May/20 10:57,16/Oct/20 10:53,13/Jul/23 08:07,29/May/20 14:58,1.11.0,,,,,1.11.0,,,,Benchmarks,,,,,0,pull-request-available,,,,"There is a noticeable performance regression in many benchmarks:
http://codespeed.dak8s.net:8000/timeline/?ben=serializerHeavyString&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=networkThroughput.1000,1ms&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=networkThroughput.100,100ms&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&env=2

that happened on May 19th, probably between 260ef2c and 2f18138",,pnowojski,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 14:58:24 UTC 2020,,,,,,,,,,"0|z0ey5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/20 11:01;pnowojski;Those are the commits in suspected range:

{noformat}
2f18138df2 [26 hours ago] [FLINK-17809][dist] Quote classpath and FLINK_CONF_DIR [Chesnay Schepler]
31ec497b96 [2 days ago] [FLINK-17763][dist] Properly handle log properties and spaces in scala-shell.sh [Chesnay Schepler]
2aacb62c29 [13 days ago] [FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers [Roman Khachatryan]
54155744bd [13 days ago] [FLINK-17547][task] Use RefCountedFile in SpanningWrapper (todo: merge with next?) [Roman Khachatryan]
2fcc1fca7c [13 days ago] [FLINK-17547][task][hotfix] Move RefCountedFile to flink-core to use it in SpanningWrapper [Roman Khachatryan]
179de29f09 [13 days ago] [FLINK-17547][task][hotfix] Extract RefCountedFileWithStream from RefCountedFile Motivation: use RefCountedFile for reading as well. [Roman Khachatryan]
37f441a2fc [2 days ago] [FLINK-17547][task] Use iterator for unconsumed buffers. Motivation: support spilled records Changes: 1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer signature 2. adapt channel state persistence to new types [Roman Khachatryan]
824100e146 [8 days ago] [FLINK-17547][task][hotfix] Extract methods from RecordsDeserializer [Roman Khachatryan]
67d3eae6f1 [8 days ago] [FLINK-17547][task][hotfix] Fix compiler warnings in NonSpanningWrapper [Roman Khachatryan]
d7b29f7bb5 [2 weeks ago] [FLINK-17547][task][hotfix] Extract SpanningWrapper from SpillingAdaptiveSpanningRecordDeserializer (static inner class). As it is, no logical changes. [Roman Khachatryan]
6e3c5abf7b [2 weeks ago] [FLINK-17547][task][hotfix] Extract NonSpanningWrapper from SpillingAdaptiveSpanningRecordDeserializer (static inner class) As it is, no logical changes. [Roman Khachatryan]
8548d37df6 [13 days ago] [FLINK-17547][task][hotfix] Improve error handling 1 catch one more invalid input in DataOutputSerializer.write 2 more informative error messages [Roman Khachatryan]
{noformat}

It means the regression was probably caused by FLINK-17547 CC [~roman_khachatryan];;;","21/May/20 17:41;pnowojski;merged to master as 3ab5d58153
merged to release-1.11 as 2da9ac4b21;;;","25/May/20 13:48;pnowojski;Performance regression was only partially solved. Some benchmarks seems fine:
http://codespeed.dak8s.net:8000/timeline/?ben=serializerHeavyString&env=2 
but some are still showing some slow down:
http://codespeed.dak8s.net:8000/timeline/?ben=networkBroadcastThroughput&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=networkThroughput.1000,100ms&env=2;;;","29/May/20 14:58;pnowojski;2nd fixed merged to release-1.11 as acb16cbdc3 and to master as 955a683 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HA end-to-end gets killed due to timeout,FLINK-17825,13306076,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,20/May/20 06:52,17/Sep/20 08:56,13/Jul/23 08:07,17/Sep/20 07:14,1.12.0,,,,,1.11.3,1.12.0,,,Runtime / Coordination,Tests,,,,0,test-stability,,,,"CI (normal profile): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1867&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5
{code}
2020-05-19T20:46:50.9034002Z Killed TM @ 104061
2020-05-19T20:47:05.8510180Z Killed TM @ 107775
2020-05-19T20:47:55.1181475Z Killed TM @ 108337
2020-05-19T20:48:16.7907005Z Test (pid: 89099) did not finish after 540 seconds.
2020-05-19T20:48:16.7907777Z Printing Flink logs and killing it:

[...]

2020-05-19T20:48:19.1016912Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/test_ha_datastream.sh: line 125: 89099 Terminated              ( cmdpid=$BASHPID; ( sleep $TEST_TIMEOUT_SECONDS; echo ""Test (pid: $cmdpid) did not finish after $TEST_TIMEOUT_SECONDS seconds.""; echo ""Printing Flink logs and killing it:""; cat ${FLINK_DIR}/log/*; kill ""$cmdpid"" ) & watchdog_pid=$!; echo $watchdog_pid > $TEST_DATA_DIR/job_watchdog.pid; run_ha_test 4 ${STATE_BACKEND_TYPE} ${STATE_BACKEND_FILE_ASYNC} ${STATE_BACKEND_ROCKS_INCREMENTAL} ${ZOOKEEPER_VERSION} )
2020-05-19T20:48:19.1017985Z Stopping job timeout watchdog (with pid=89100)
2020-05-19T20:48:19.1018621Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/test_ha_datastream.sh: line 112: kill: (89100) - No such process
2020-05-19T20:48:19.1019000Z Killing JM watchdog @ 91127
2020-05-19T20:48:19.1019199Z Killing TM watchdog @ 91883
2020-05-19T20:48:19.1019424Z [FAIL] Test script contains errors.
2020-05-19T20:48:19.1019639Z Checking of logs skipped.
2020-05-19T20:48:19.1019785Z 
2020-05-19T20:48:19.1020329Z [FAIL] 'Running HA (rocks, non-incremental) end-to-end test' failed after 9 minutes and 0 seconds! Test exited with exit code 1
{code}",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 17 08:56:28 UTC 2020,,,,,,,,,,"0|z0exmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/20 06:53;rmetzger;Looks like the timeout mechanism I introduced in FLINK-16423 is somehow broken. I will take a look.;;;","26/May/20 18:50;rmetzger;Actually, it is not broken. Killing the watchdog won't work if the watchdog detected a timeout.
However, I decided to increase the timeout to 15 minutes (up from 9). 9 was set because of Travis 10 minutes ""no output"" limit.;;;","27/May/20 11:19;rmetzger;The timeout increase is part of this PR: https://github.com/apache/flink/pull/12350;;;","28/May/20 05:49;rmetzger;Another case in the Scala 2.12 profile https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2305&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334
{code}
2020-05-27T21:32:55.3839807Z Starting standalonesession daemon on host fv-az558.
2020-05-27T21:32:56.8802120Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonesession-2*.log: No such file or directory
2020-05-27T21:33:37.5553176Z Killed JM @ 94046
2020-05-27T21:33:37.5556291Z Waiting for text Completed checkpoint [1-9]* for job 2f93c52fd325fa7410fb665dee80bb20 to appear 2 of times in logs...
2020-05-27T21:33:37.5956808Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonesession-3*.log: No such file or directory
2020-05-27T21:33:38.6046010Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonesession-3*.log: No such file or directory
2020-05-27T21:33:39.5862716Z Starting standalonesession daemon on host fv-az558.
2020-05-27T21:33:39.6089277Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonesession-3*.log: No such file or directory
2020-05-27T21:34:32.8925011Z Killed TM @ 94007
2020-05-27T21:34:45.2218956Z Killed TM @ 98235
2020-05-27T21:39:45.2307885Z Test (pid: 85160) did not finish after 540 seconds.

[...]

[FAIL] 'Running HA (rocks, non-incremental) end-to-end test' failed after 9 minutes and 0 seconds! Test exited with exit code 1
{code}
;;;","28/May/20 14:38;rmetzger;I increased the timeout in this hotfix: https://github.com/apache/flink/commit/c4a311cced50ea7e03a7dbad18735a05c7ba9508.
I will still investigate the new case I posted today;;;","02/Jun/20 09:40;rmetzger;I executed the test 200 times on CI but could not reproduce it. I will close the ticket for now.;;;","10/Aug/20 02:07;dian.fu;Another instance: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5321&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=7d4f7375-52df-5ce0-457f-b2ffbb2289a4];;;","17/Sep/20 07:13;rmetzger;The last failure reported by Dian happened on the ""release-1.11"" branch, which doesn't have the increased timeout commit from master yet. I will backport the fix to the release-1.11 branch.;;;","17/Sep/20 07:14;rmetzger;https://github.com/apache/flink/commit/655df30eee25fa9fea5643eb27f7de45899a9cfb;;;","17/Sep/20 08:56;dian.fu;[~rmetzger] Thanks a lot!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Resuming Savepoint"" e2e stalls indefinitely ",FLINK-17824,13306062,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,rmetzger,rmetzger,20/May/20 05:16,05/Jan/21 10:39,13/Jul/23 08:07,16/Jun/20 14:08,1.10.1,1.11.0,,,,1.12.0,,,,Runtime / Checkpointing,Tests,,,,0,pull-request-available,test-stability,,,"CI; https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1887&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8

{code}
2020-05-19T21:05:52.9696236Z ==============================================================================
2020-05-19T21:05:52.9696860Z Running 'Resuming Savepoint (file, async, scale down) end-to-end test'
2020-05-19T21:05:52.9697243Z ==============================================================================
2020-05-19T21:05:52.9713094Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-52970362751
2020-05-19T21:05:53.1194478Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT
2020-05-19T21:05:53.2180375Z Starting cluster.
2020-05-19T21:05:53.9986167Z Starting standalonesession daemon on host fv-az558.
2020-05-19T21:05:55.5997224Z Starting taskexecutor daemon on host fv-az558.
2020-05-19T21:05:55.6223837Z Waiting for Dispatcher REST endpoint to come up...
2020-05-19T21:05:57.0552482Z Waiting for Dispatcher REST endpoint to come up...
2020-05-19T21:05:57.9446865Z Waiting for Dispatcher REST endpoint to come up...
2020-05-19T21:05:59.0098434Z Waiting for Dispatcher REST endpoint to come up...
2020-05-19T21:06:00.0569710Z Dispatcher REST endpoint is up.
2020-05-19T21:06:07.7099937Z Job (a92a74de8446a80403798bb4806b73f3) is running.
2020-05-19T21:06:07.7855906Z Waiting for job to process up to 200 records, current progress: 114 records ...
2020-05-19T21:06:55.5755111Z 
2020-05-19T21:06:55.5756550Z ------------------------------------------------------------
2020-05-19T21:06:55.5757225Z  The program finished with the following exception:
2020-05-19T21:06:55.5757566Z 
2020-05-19T21:06:55.5765453Z org.apache.flink.util.FlinkException: Could not stop with a savepoint job ""a92a74de8446a80403798bb4806b73f3"".
2020-05-19T21:06:55.5766873Z 	at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:485)
2020-05-19T21:06:55.5767980Z 	at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:854)
2020-05-19T21:06:55.5769014Z 	at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:477)
2020-05-19T21:06:55.5770052Z 	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:921)
2020-05-19T21:06:55.5771107Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:982)
2020-05-19T21:06:55.5772223Z 	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
2020-05-19T21:06:55.5773325Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:982)
2020-05-19T21:06:55.5774871Z Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
2020-05-19T21:06:55.5777183Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-05-19T21:06:55.5778884Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-05-19T21:06:55.5779920Z 	at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:483)
2020-05-19T21:06:55.5781175Z 	... 6 more
2020-05-19T21:06:55.5782391Z Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
2020-05-19T21:06:55.5783885Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.lambda$stopWithSavepoint$9(SchedulerBase.java:890)
2020-05-19T21:06:55.5784992Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2020-05-19T21:06:55.5786492Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2020-05-19T21:06:55.5787601Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-05-19T21:06:55.5788682Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)
2020-05-19T21:06:55.5790308Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)
2020-05-19T21:06:55.5791664Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-05-19T21:06:55.5792767Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-05-19T21:06:55.5793756Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-05-19T21:06:55.5794652Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-05-19T21:06:55.5795605Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-05-19T21:06:55.5796551Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-05-19T21:06:55.5797459Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-05-19T21:06:55.5798390Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-19T21:06:55.5799311Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-19T21:06:55.5800175Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-05-19T21:06:55.5801078Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-05-19T21:06:55.5802741Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-05-19T21:06:55.5803579Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-05-19T21:06:55.5804628Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-05-19T21:06:55.5805435Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-05-19T21:06:55.5806194Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-05-19T21:06:55.5807037Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-05-19T21:06:55.5808001Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-05-19T21:06:55.5808984Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-05-19T21:06:55.5809970Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-05-19T21:06:55.5811188Z Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
2020-05-19T21:06:55.5813260Z 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2020-05-19T21:06:55.5814556Z 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2020-05-19T21:06:55.5815578Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2020-05-19T21:06:55.5816604Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-05-19T21:06:55.5817663Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-19T21:06:55.5822918Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-05-19T21:06:55.5824096Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$null$0(CheckpointCoordinator.java:464)
2020-05-19T21:06:55.5825220Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-05-19T21:06:55.5826274Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-05-19T21:06:55.5827334Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-19T21:06:55.5828369Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-05-19T21:06:55.5830735Z 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:493)
2020-05-19T21:06:55.5831962Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:1565)
2020-05-19T21:06:55.5833475Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:1552)
2020-05-19T21:06:55.5834742Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoints(CheckpointCoordinator.java:1440)
2020-05-19T21:06:55.5836006Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoints(CheckpointCoordinator.java:1422)
2020-05-19T21:06:55.5837431Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingAndQueuedCheckpoints(CheckpointCoordinator.java:1660)
2020-05-19T21:06:55.5838737Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.stopCheckpointScheduler(CheckpointCoordinator.java:1410)
2020-05-19T21:06:55.5840060Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorDeActivator.jobStatusChanges(CheckpointCoordinatorDeActivator.java:46)
2020-05-19T21:06:55.5841361Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifyJobStatusChange(ExecutionGraph.java:1668)
2020-05-19T21:06:55.5842509Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.transitionState(ExecutionGraph.java:1250)
2020-05-19T21:06:55.5843916Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.transitionState(ExecutionGraph.java:1228)
2020-05-19T21:06:55.5845083Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.transitionExecutionGraphState(SchedulerBase.java:432)
2020-05-19T21:06:55.5846293Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.addVerticesToRestartPending(DefaultScheduler.java:240)
2020-05-19T21:06:55.5847351Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.restartTasksWithDelay(DefaultScheduler.java:227)
2020-05-19T21:06:55.5847998Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeRestartTasks(DefaultScheduler.java:214)
2020-05-19T21:06:55.5848654Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:193)
2020-05-19T21:06:55.5849327Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)
2020-05-19T21:06:55.5850012Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)
2020-05-19T21:06:55.5850701Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)
2020-05-19T21:06:55.5851473Z 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)
2020-05-19T21:06:55.5852381Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1717)
2020-05-19T21:06:55.5853059Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1268)
2020-05-19T21:06:55.5853663Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1236)
2020-05-19T21:06:55.5854297Z 	at org.apache.flink.runtime.executiongraph.Execution.fail(Execution.java:954)
2020-05-19T21:06:55.5854938Z 	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.signalPayloadRelease(SingleLogicalSlot.java:173)
2020-05-19T21:06:55.5855620Z 	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.release(SingleLogicalSlot.java:165)
2020-05-19T21:06:55.5856296Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:732)
2020-05-19T21:06:55.5857025Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)
2020-05-19T21:06:55.5857747Z 	at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:149)
2020-05-19T21:06:55.5858408Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.releaseTaskManagerInternal(SlotPoolImpl.java:818)
2020-05-19T21:06:55.5859085Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.releaseTaskManager(SlotPoolImpl.java:777)
2020-05-19T21:06:55.5859806Z 	at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManager(JobMaster.java:435)
2020-05-19T21:06:55.5860469Z 	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.notifyHeartbeatTimeout(JobMaster.java:1193)
2020-05-19T21:06:55.5861152Z 	at org.apache.flink.runtime.heartbeat.HeartbeatMonitorImpl.run(HeartbeatMonitorImpl.java:109)
2020-05-19T21:06:55.5861751Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-05-19T21:06:55.5862340Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-05-19T21:06:55.5862732Z 	... 22 more
2020-05-19T21:06:55.5863134Z Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
2020-05-19T21:06:55.5863754Z 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:492)
2020-05-19T21:06:55.5864204Z 	... 57 more
2020-05-19T21:06:55.5864528Z Waiting for job (a92a74de8446a80403798bb4806b73f3) to reach terminal state FINISHED ...
2020-05-20T00:30:52.9000401Z ##[error]The operation was canceled.
2020-05-20T00:30:52.9019065Z ##[section]Finishing: Run e2e tests
{code}",,klion26,pnowojski,rmetzger,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17404,,FLINK-20818,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 14:06:53 UTC 2020,,,,,,,,,,"0|z0exjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/20 14:37;rmetzger;Introduced a timeout for this test so that we can debug future cases: https://github.com/apache/flink/commit/61423e97a034b94dc9c0ffe350dc3667585d6822;;;","03/Jun/20 05:41;rmetzger;Now with logs: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2586&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8 (hadoop 3.1.3 profile, see FLINK-17404);;;","04/Jun/20 06:51;rmetzger;hadoop 3.1.3 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2663&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8
;;;","04/Jun/20 10:28;rmetzger;Another instance on master (which makes the Hadoop 3.1.3 theory less likely) https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2696&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","05/Jun/20 04:39;pnowojski;As it was first reported around feature when we were merging quite a bit of unaligned checkpoints code, this issue might be related to something we did.

It also might be fixed accidentally via one of the in-progress bugs like FLINK-18136 

What do you think [~roman_khachatryan]?;;;","09/Jun/20 07:22;roman;I'm looking into this issue and found so far:
 1. After the test, cluster is not shut down; if the cluster is running failure is more likely to happen; but as seen from CI logs, test also fails when cluster is started afresh
 2. The failure is caused by stopWithSavepoint taking too long
 3. JM (and CheckpointCoordinator in particular) work as expected; no further checkpoints are performed after stop-with-Savepoint
 4. Checkpointing is done in less than 1 minute (rarely it exceeds 60s timeout window); once it's done AND job is finished test proceeds (all checkpoint completion callbacks are executed)
 5. Upon confirming the checkpoint, source subtasks send EndOfPartition event downstream and cancel themselves as expected
 6. Further ""network tasks"" do NOT consume EndOfPartition event for several minutes - delaying finishing job and causing test failure
 7. Upstream subpartitions is NOT blocked after sending aligned barrier (new optimization in 1.11).

Looks like the channels ARE notified and they ARE polled, but return Optional.empty or some data buffers for several minutes.
 Currently, I'm double checking this.

 

Edit:

> After the test, cluster is not shut down

I guess it's my local issue. The test doesn't shut down the cluster or cancel the job explicitly; however, it registers handlers to do so on SIGINT and EXIT;;;","10/Jun/20 17:50;roman;TLDR: If there is an ongoing checkpoint then it slows down stop-with-savepoint command:
 * first by slowing down the barrier itself;
 * then by delaying EndOfPartition because of the need to process all the data accumulated during the checkpoint (32K+ per channel).

 

The fact that sources don't stop immediately (but rather on completion notification) seems deliberate:

 
{code:java}
private void notifyCheckpointComplete(long checkpointId) throws Exception { private void notifyCheckpointComplete(long checkpointId) throws Exception {
    subtaskCheckpointCoordinator.notifyCheckpointComplete(checkpointId, operatorChain, this::isRunning); 
    if (isRunning && isSynchronousSavepointId(checkpointId)) { 
        finishTask();
        ...{code}
 

 The issue is reproducible in 1.10 (*release-1.10 branch as of Jun 10*).

 

I see several ways of how to deal with it:
 # stop the sources immediately: though the checkpoint can fail, checkpoint coordinator is not supposed to continue work after stop-with-savepoint
 # stop downstreams the same way as sources - upon sync_savepoint completion (not waiting for EndOfPartition)
 ** or emit EndOfPartition from source right after the last checkpoint barrier 
 # -reduce max-concurrent-checkpoints and don't force any checkpoints (currently, savepoints are forced, i.e. the limit is ignored)-

In the long run,
 # we could leverage RPC abort notifications to stop alignment- (which will shorten checkpointing time); they were introduced in 1.11 and currently don't affect ""network"" parts
 # probably unaligned checkpoints could help

 

In the meantime, we can try to reduce CI failures probability by playing with test timeout/ sleep time in source; buffer/record size.

WDYT, [~pnowojski]?

 

Edit: also happens with tag: release-1.10.0;;;","11/Jun/20 07:02;rmetzger;[~roman_khachatryan] do you think this is the same problem: FLINK-18148 ?;;;","11/Jun/20 07:39;roman;[~rmetzger], yes I think it's the same issue.;;;","12/Jun/20 07:51;pnowojski;Thanks for investigation [~roman_khachatryan]

> 1. stop the sources immediately: though the checkpoint can fail, checkpoint coordinator is not supposed to continue work after stop-with-savepoint

I think it's important designed feature that if stop with savepoint fails, (in some cases) we continue working.

> 2. stop downstreams the same way as sources - upon sync_savepoint completion (not waiting for EndOfPartition)

This doesn't sound right. Logically {{EndOfPartitionEvent}} is supposed to mark end of data, if we do not wait for it, we are braking some contract. Which might be fine now, but could cause problems later.

> 6. Further ""network tasks"" do NOT consume EndOfPartition event for several minutes - delaying finishing job and causing test failure

Why is it taking so long?  Can not we speed up the test? Is it taking couple of minutes to process the remaining data? It sounds excessive.
;;;","15/Jun/20 08:04;roman;> Is it taking couple of minutes to process the remaining data? 
Yes. Also just to stop the job without savepoint.

I didn't find anything abnormal, just too much records generated and buffered by sources in case of any delay in downstreams.
Without the delay, I guess source and downstreams compete for memory buffers.

> Can not we speed up the test? 
Increasing job parameter sequence_generator_source.sleep_time from 15 to 30 to 30 fixes the issue (locally).

 Decreasing it to 5 makes the test to fail always (each source generates up to seven 32K buffers).

Similar effect have segment-size and number of segments.

 

I suggest to increase sequence_generator_source.sleep_time and consider a proper fix in future.;;;","15/Jun/20 14:10;pnowojski;Ok, let's try to increase the sleep_time. But this parameter doesn't answer this question:
{quote}
> 6. Further ""network tasks"" do NOT consume EndOfPartition event for several minutes - delaying finishing job and causing test failure

Why is it taking so long? Can not we speed up the test? Is it taking couple of minutes to process the remaining data? It sounds excessive.
{quote}
If I understand your previous message, it takes several minutes for downstream tasks to consume buffered data - data that has already been produced by the source, so this sleep time should have no affect on that. So my question remains open, why is it taking couple of minutes to process the remaining data?;;;","16/Jun/20 08:50;roman;I've published a PR to fix the test: [https://github.com/apache/flink/pull/12671]
 

The sleep is between injecting records by the Source, so it slows down and reduces the amount of data to be consumed by the downstream.
 
> So my question remains open, why is it taking couple of minutes to process the remaining data?

Because there is a lot of data and the processing is much heavier then injecting (kryo, java streams with filter, etc.) .

The data are injected by the source during the alignment phase of downstream.

Source is not backpressured because buffers aren't needed by the downstream (this is a local environment) so it's only limited by the aforementioned sleep_time.;;;","16/Jun/20 11:02;pnowojski;{quote}
Waiting for job to process up to 200 records, current progress: 114 records ...
{quote}
I wouldn't call 200 records ""lots of data"", unless something is multiplying records count. Serialisation and other processing of 200 records really shouldn't take more than a second ([kryo is easily capable of hundreds thousands of records per second|http://codespeed.dak8s.net:8000/timeline/?ben=serializerKryo&env=2]).

Or is it a problem with the sleep time on the sources not working anymore after 200 records and quickly backpressuring whole job? ;;;","16/Jun/20 12:47;roman;""200 records"" is measured closer to sink.

Source generates about 5-20K records (depending on delay).;;;","16/Jun/20 14:06;pnowojski;Still I don't see why it is or why does it have to be so slow. But let's wait and see if the increased sleep time helped and let's revisit the problem only if it doesn't help.

merged commit increasing sleep time a9d3074 into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resolve the race condition while releasing RemoteInputChannel,FLINK-17823,13306054,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zjwang,zjwang,zjwang,20/May/20 04:16,16/Oct/20 10:53,13/Jul/23 08:07,22/May/20 03:08,1.11.0,,,,,1.11.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"RemoteInputChannel#releaseAllResources might be called by canceler thread. Meanwhile, the task thread can also call RemoteInputChannel#getNextBuffer. There probably cause two potential problems:
 * Task thread might get null buffer after canceler thread already released all the buffers, then it might cause misleading NPE in getNextBuffer.
 * Task thread and canceler thread might pull the same buffer concurrently, which causes unexpected exception when the same buffer is recycled twice.

The solution is to properly synchronize the buffer queue in release method to avoid the same buffer pulled by both canceler thread and task thread. And in getNextBuffer method, we add some explicit checks to avoid misleading NPE and hint some valid exceptions.",,felixzheng,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 03:08:21 UTC 2020,,,,,,,,,,"0|z0exhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/20 03:08;zjwang;Merged in release-1.11: 3eb1075ded64da20e6f7a5bc268f455eaf6573eb

Merged in master: 8c7c7267be95cddd7122d2b97e5334f5db4cc37c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nightly Flink CLI end-to-end test failed with ""JavaGcCleanerWrapper$PendingCleanersRunner cannot access class jdk.internal.misc.SharedSecrets"" in Java 11 ",FLINK-17822,13306044,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,azagrebin,dian.fu,dian.fu,20/May/20 02:54,16/Oct/20 10:54,13/Jul/23 08:07,25/May/20 07:36,1.11.0,,,,,1.10.2,1.11.0,,,Runtime / Task,Tests,,,,0,pull-request-available,test-stability,,,"Instance: https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/1887/logs/600

{code}
2020-05-19T21:59:39.8829043Z 2020-05-19 21:59:25,193 ERROR org.apache.flink.util.JavaGcCleanerWrapper                   [] - FATAL UNEXPECTED - Failed to invoke waitForReferenceProcessing
2020-05-19T21:59:39.8829849Z java.lang.IllegalAccessException: class org.apache.flink.util.JavaGcCleanerWrapper$PendingCleanersRunner cannot access class jdk.internal.misc.SharedSecrets (in module java.base) because module java.base does not export jdk.internal.misc to unnamed module @54e3658c
2020-05-19T21:59:39.8830707Z 	at jdk.internal.reflect.Reflection.newIllegalAccessException(Reflection.java:361) ~[?:?]
2020-05-19T21:59:39.8831166Z 	at java.lang.reflect.AccessibleObject.checkAccess(AccessibleObject.java:591) ~[?:?]
2020-05-19T21:59:39.8831744Z 	at java.lang.reflect.Method.invoke(Method.java:558) ~[?:?]
2020-05-19T21:59:39.8832596Z 	at org.apache.flink.util.JavaGcCleanerWrapper$PendingCleanersRunner.getJavaLangRefAccess(JavaGcCleanerWrapper.java:362) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8833667Z 	at org.apache.flink.util.JavaGcCleanerWrapper$PendingCleanersRunner.tryRunPendingCleaners(JavaGcCleanerWrapper.java:351) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8834712Z 	at org.apache.flink.util.JavaGcCleanerWrapper$CleanerManager.tryRunPendingCleaners(JavaGcCleanerWrapper.java:207) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8835686Z 	at org.apache.flink.util.JavaGcCleanerWrapper.tryRunPendingCleaners(JavaGcCleanerWrapper.java:158) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8836652Z 	at org.apache.flink.runtime.memory.UnsafeMemoryBudget.reserveMemory(UnsafeMemoryBudget.java:94) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8838033Z 	at org.apache.flink.runtime.memory.UnsafeMemoryBudget.verifyEmpty(UnsafeMemoryBudget.java:64) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8839259Z 	at org.apache.flink.runtime.memory.MemoryManager.verifyEmpty(MemoryManager.java:172) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8840148Z 	at org.apache.flink.runtime.taskexecutor.slot.TaskSlot.verifyMemoryFreed(TaskSlot.java:311) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8841035Z 	at org.apache.flink.runtime.taskexecutor.slot.TaskSlot.lambda$closeAsync$1(TaskSlot.java:301) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8841603Z 	at java.util.concurrent.CompletableFuture.uniRunNow(CompletableFuture.java:815) ~[?:?]
2020-05-19T21:59:39.8842069Z 	at java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:799) ~[?:?]
2020-05-19T21:59:39.8842844Z 	at java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2121) ~[?:?]
2020-05-19T21:59:39.8843828Z 	at org.apache.flink.runtime.taskexecutor.slot.TaskSlot.closeAsync(TaskSlot.java:300) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8844790Z 	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlotInternal(TaskSlotTableImpl.java:404) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8845754Z 	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:365) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8846842Z 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1589) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8847711Z 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:967) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8848295Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
2020-05-19T21:59:39.8848732Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
2020-05-19T21:59:39.8849228Z 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
2020-05-19T21:59:39.8849669Z 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
2020-05-19T21:59:39.8850656Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8851589Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8852497Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8853486Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8854447Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8855190Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8855958Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8856723Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8857663Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8858452Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8859214Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8860030Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8860825Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8861706Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8862652Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8863414Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8864132Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8864940Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8865931Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8866947Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8867921Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-05-19T21:59:39.8868777Z 2020-05-19 21:59:25,242 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job c968ba482982e36e3e30f058c488ac3f from job leader monitoring.
2020-05-19T21:59:39.8869653Z 2020-05-19 21:59:25,257 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job c968ba482982e36e3e30f058c488ac3f.
2020-05-19T21:59:39.8870178Z WARNING: An illegal reflective access operation has occurred
2020-05-19T21:59:39.8871620Z WARNING: Illegal reflective access by org.apache.flink.shaded.akka.org.jboss.netty.util.internal.ByteBufferUtil (file:/home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/lib/flink-dist_2.11-1.12-SNAPSHOT.jar) to method java.nio.DirectByteBuffer.cleaner()
2020-05-19T21:59:39.8872448Z WARNING: Please consider reporting this to the maintainers of org.apache.flink.shaded.akka.org.jboss.netty.util.internal.ByteBufferUtil
2020-05-19T21:59:39.8873159Z WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
2020-05-19T21:59:39.8873558Z WARNING: All illegal access operations will be denied in a future release
2020-05-19T21:59:39.8874019Z Checking for non-empty .out files...
2020-05-19T21:59:39.8874367Z No non-empty .out files.
2020-05-19T21:59:39.8874528Z 
2020-05-19T21:59:39.8875304Z [FAIL] 'Flink CLI end-to-end test' failed after 0 minutes and 39 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files
{code}",,azagrebin,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15758,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 12:46:40 UTC 2020,,,,,,,,,,"0|z0exfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/20 03:02;dian.fu;There are several Java 11 tests in the same cron job failed with the same exception: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1887&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=d4549d78-6fab-5c0c-bdb9-abaafb66ea8b;;;","20/May/20 15:23;azagrebin;FLINK-15758 did not export jdk.internal.misc package used by reflection for GC cleaners of managed memory. Our PR CI does not run Java 11 tests atm.
The package has to be exported by a JVM runtime arg: --add-opens java.base/jdk.internal.misc=ALL-UNNAMED;
If this arg is set for Java 8, it fails the JVM process.
Therefore, the fix is complicated as we have to do it also for e.g. Yarn CLI where client and cluster may run different Java versions.

An alternative quicker fix is to call directly the private method (has to be made accessible via reflection):
- java.lang.ref.Reference.tryHandlePending(false) // for Java 8
- java.lang.ref.Reference.waitForReferenceProcessing() // for Java 11

Unfortunately, this leads to the annoying warning for Java 11:

{code:java}
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.flink.util.JavaGcCleanerWrapper$PendingCleanersRunnerProvider (file:/Users/azagrebin/projects/flink/flink-core/target/classes/) to method java.lang.ref.Reference.waitForReferenceProcessing()
WARNING: Please consider reporting this to the maintainers of org.apache.flink.util.JavaGcCleanerWrapper$PendingCleanersRunnerProvider
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
{code}

We can do the quick fix and think how to tackle the warning in a follow-up.
;;;","22/May/20 06:24;rmetzger;I don't think it is a good idea to introduce {{--add-opens}} wherever we start JVMs just before or during the release testing. This needs proper testing and is, as you say, not quick to do. Also, some users use Flink in a library-style, where we can not control the JVM invocation.

We we are anyways doing illegal reflective accesses with Flink, it is acceptable to have the annoying warning for this one as well (even though the warning is only shown once by default);;;","25/May/20 07:36;azagrebin;merged into master by 702a339fff03402342362d6f98107150638eef67
merged into 1.11 by 44f4383526f3fd4a468b1fdd5525254c335ea392
merged into 1.10 by 71dbbe60faf452fd4d9b24a156e92562c93de5e8;;;","26/May/20 07:39;rmetzger;Can you merge the fix to 1.11 as well? I could only find it in ""release-1.10"" and ""master"" (1.12).

Background: the ""release-1.11"" nightlies are still failing: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2162&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=679407b1-ea2c-5965-2c8d-1467777fff88;;;","26/May/20 10:03;azagrebin;True, sorry, I merged it now into 1.11 as well;;;","26/May/20 10:05;rmetzger;Thx, no problem :) ;;;","26/May/20 12:41;rmetzger;Thanks for correcting the ""fix version/s"" Dian! ;;;","26/May/20 12:46;dian.fu;Thanks you also for the discussion and fix. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory threshold is ignored for channel state,FLINK-17820,13306013,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,roman,roman,19/May/20 21:27,16/Oct/20 10:56,13/Jul/23 08:07,27/May/20 15:35,1.11.0,,,,,1.11.0,,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,"Config parameter state.backend.fs.memory-threshold is ignored for channel state. Causing each subtask to have a file per checkpoint. Regardless of the size of channel state (of this subtask).

This also causes slow cleanup and delays the next checkpoint.

 

The problem is that {{ChannelStateCheckpointWriter.finishWriteAndResult}} calls flush(); which actually flushes the data on disk.

 

From FSDataOutputStream.flush Javadoc:

A completed flush does not mean that the data is necessarily persistent. Data persistence can is only assumed after calls to close() or sync().

 

Possible solutions:

1. not to flush in {{ChannelStateCheckpointWriter.finishWriteAndResult (which can lead to data loss in a wrapping stream).}}

{{2. change }}{{FsCheckpointStateOutputStream.flush behavior}}

{{3. wrap }}{{FsCheckpointStateOutputStream to prevent flush}}{{}}{{}}",,roman,sewen,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17986,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 15:39:41 UTC 2020,,,,,,,,,,"0|z0ex8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/20 22:47;sewen;I would simply avoid flushing here. It makes no difference, persistence wise. Persistence is only guaranteed once you call {{closeAndGetHandle()}}.
There are few reasons ever to call flush on a stream (usually only when it is pipelined to a receiver and you want to make sure that the receiver receives buffered data, which I assume is not the case here, as this is no network stream).

(The reference to {{sync}} is not quite right in the JavaDocs. For some FileSystems, like S3, {{sync()}} does not help at all).

I would not change {{FsCheckpointStateOutputStream}}, this is a class that does exactly what it should do at the moment.

Wrapping also seems like unnecessary complexity, which we should avoid whenever possible.;;;","20/May/20 23:02;roman;The data to FsCheckpointStateOutputStream is written through DataOutputStream.

Without flushing DataOutputStream some data can be left in its buffer. Flushing DataOutputStream also flushes the underlying FsCheckpointStateOutputStream.

Although in current JDK DataOutputStream doesn't buffer data. Do you think we can rely on it?;;;","23/May/20 15:55;sewen;I see. You cannot rely on {{close()}} here as well, because that closes the underlying stream in a ""close for dispose"" manner. That is indeed a bit of a design shortcoming in the {{CheckpointStateOutputStream}} class.

My feeling is that this assumption of {{DataOutputStream}} being non-buffering is made in some other parts of the code.
What do you think about adding a test that guards this assumption?

We also have the {{DataOutputViewStreamWrapper}} class which is an extension of {{DataOutputStream}} - we could use that, guard it with a ""does not buffer"" test and if we ever find it actually buffers, then we need to implement the methods directly, rather than inherit from {{DataOutputStream}}.


;;;","23/May/20 16:02;sewen;Regarding changing the behavior of {{FsCheckpointStateOutputStream#flush()}}  - I cannot say if there is some part of the code at the moment that assumes files are created upon flushing. The tests guard this behavior, so it may very well be.;;;","25/May/20 08:49;roman;{quote}What do you think about adding a test that guards this assumption?
{quote}
I think we can not rely on testing DataOutputStream because the JRE could be different at runtime.

 
{quote}We also have the {{DataOutputViewStreamWrapper}} class which is an extension of {{DataOutputStream}} - we could use that, guard it with a ""does not buffer"" test and if we ever find it actually buffers, then we need to implement the methods directly, rather than inherit from {{DataOutputStream}}.
{quote}
I guess you mean adding a no-op flush method to {{DataOutputViewStreamWrapper.}}I think it will be difficult to detect that some flush calls ""don't work"" anymore. And probably it's better to find incorrect assumptions.

 
{quote}Wrapping also seems like unnecessary complexity
{quote}
 
To me, it is less complicated than baking (counter-intuitive?) assumptions into public code and adding tests for it;;;","25/May/20 17:03;sewen;It is true, the assumption that {{DataOutputStream}} does not buffer is a fragile point, even if an unlikely one (my feeling is too much existing code would be broken if one SDK made the implementation behave differently for such a core class). That said, it should be possible to flush the stream without voiding the usability of the class.

If from the initial options, we don't like (1), then, thinking about it a bit more, I would go for option (2) (adjust {{FsCheckpointStateOutputStream}}).

I did a quick search through the code and it looks like we can drop the assumption that {{FsCheckpointStateOutputStream}} creates the file in {{flush()}} It seems not used in the production code (though possibly in tests). How about this?
  - rename the {{flush()}} method to {{flushToFile()}}, including all existing calls to that method within the class and in relevant tests.
  - override {{flush()}} as a no-op.
;;;","26/May/20 06:10;roman;I agree about (2) (adjust {{FsCheckpointStateOutputStream}}).

After trying it out I found only a couple of tests that depend on flush and also didn't find any production usages of it.

I'll adjust my PR.;;;","26/May/20 07:02;roman;I've created a PR: [https://github.com/apache/flink/pull/12332]

It flushes to disk if buffers are above threshold:

 
{code:java}
public void flush() {
    if (pos > localStateThreshold) {
        flushToFile(); 
    } 
}{code}
(I realized that always doing nothing in flush may be counter-intuitive).

 ;;;","27/May/20 15:39;zjwang;Merged in release-1.11: 05b97924572594ef244236a2f328177a2ec84fc4

Merged in master: 8b4fe87a74d3ec631350ebac4dfdf69094c802e3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV Reader with Pojo Type and no field names fails,FLINK-17818,13305996,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,damjad,damjad,damjad,19/May/20 20:10,19/Feb/21 07:30,13/Jul/23 08:07,29/Sep/20 09:54,1.10.1,,,,,1.12.0,,,,API / DataSet,,,,,0,pull-request-available,,,,"When a file is read with a CSVReader and a POJO is specified and the filed names are not specified, the output is obviously not correct. The API is not throwing any error despite a null check inside the API. i.e.

{code}
Preconditions.checkNotNull(pojoFields, ""POJO fields must be specified (not null) if output type is a POJO."");
{code}

The *root cause* of the problem is that the _fieldNames_ argument is a variable argument and the variable is 'empty but not null' when not given any value. So, the check passes without failing.

I am not sure if it is a feature of the API or an actual bug because there is a test checking the NPE when a null is passed.",,aljoscha,damjad,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 29 09:54:03 UTC 2020,,,,,,,,,,"0|z0ex4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/20 20:10;damjad;Please assign it to me.
Here is the commit: https://github.com/damjad/flink/commit/f3c9c7a99387e7cdc11d913cf5932b1d03b8ef54;;;","16/Aug/20 14:23;zhuzh;cc [~aljoscha][~twalthr];;;","18/Aug/20 08:56;twalthr;[~damjad] Sorry for the late reply. But feel free to open a PR. I will assign you to this issue then.;;;","09/Sep/20 15:25;damjad;[~twalthr] Sorry for the late reply.

I have created a pull request. https://github.com/apache/flink/pull/13367

cc: [~aljoscha];;;","29/Sep/20 09:54;aljoscha;master: 6669ae09b7563251a89e94da15271d2f0bf2f91b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CollectResultFetcher fails with EOFException in AggregateReduceGroupingITCase,FLINK-17817,13305988,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,rmetzger,rmetzger,19/May/20 19:24,16/Oct/20 10:53,13/Jul/23 08:07,21/May/20 03:48,1.11.0,,,,,1.11.0,,,,API / DataStream,Tests,,,,0,pull-request-available,test-stability,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1826&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=f83cd372-208c-5ec4-12a8-337462457129

{code}
2020-05-19T10:34:18.3224679Z [ERROR] testSingleAggOnTable_SortAgg(org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase)  Time elapsed: 7.537 s  <<< ERROR!
2020-05-19T10:34:18.3225273Z java.lang.RuntimeException: Failed to fetch next result
2020-05-19T10:34:18.3227634Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:92)
2020-05-19T10:34:18.3228518Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:63)
2020-05-19T10:34:18.3229170Z 	at org.apache.flink.shaded.guava18.com.google.common.collect.Iterators.addAll(Iterators.java:361)
2020-05-19T10:34:18.3229863Z 	at org.apache.flink.shaded.guava18.com.google.common.collect.Lists.newArrayList(Lists.java:160)
2020-05-19T10:34:18.3230586Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300)
2020-05-19T10:34:18.3231303Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:141)
2020-05-19T10:34:18.3231996Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:107)
2020-05-19T10:34:18.3232847Z 	at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable(AggregateReduceGroupingITCase.scala:176)
2020-05-19T10:34:18.3233694Z 	at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable_SortAgg(AggregateReduceGroupingITCase.scala:122)
2020-05-19T10:34:18.3234461Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-19T10:34:18.3234983Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-19T10:34:18.3235632Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-19T10:34:18.3236615Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-19T10:34:18.3237256Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-19T10:34:18.3237965Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-19T10:34:18.3238750Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-19T10:34:18.3239314Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-19T10:34:18.3239838Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-19T10:34:18.3240362Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-19T10:34:18.3240803Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-19T10:34:18.3243624Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-19T10:34:18.3244531Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-19T10:34:18.3245325Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-19T10:34:18.3246086Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-19T10:34:18.3246765Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-19T10:34:18.3247390Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-19T10:34:18.3248012Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-19T10:34:18.3248779Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-19T10:34:18.3249417Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-19T10:34:18.3250357Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-19T10:34:18.3251021Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-19T10:34:18.3251597Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-19T10:34:18.3252141Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-19T10:34:18.3252798Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-19T10:34:18.3253527Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-19T10:34:18.3254458Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-19T10:34:18.3255420Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-19T10:34:18.3256207Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-19T10:34:18.3257025Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-19T10:34:18.3257719Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-19T10:34:18.3258447Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-19T10:34:18.3258948Z Caused by: java.io.EOFException
2020-05-19T10:34:18.3259379Z 	at java.io.DataInputStream.readUnsignedByte(DataInputStream.java:290)
2020-05-19T10:34:18.3259826Z 	at org.apache.flink.api.java.typeutils.runtime.MaskUtils.readIntoMask(MaskUtils.java:73)
2020-05-19T10:34:18.3260307Z 	at org.apache.flink.api.java.typeutils.runtime.RowSerializer.deserialize(RowSerializer.java:200)
2020-05-19T10:34:18.3261044Z 	at org.apache.flink.api.java.typeutils.runtime.RowSerializer.deserialize(RowSerializer.java:58)
2020-05-19T10:34:18.3261535Z 	at org.apache.flink.api.common.typeutils.base.ListSerializer.deserialize(ListSerializer.java:133)
2020-05-19T10:34:18.3262105Z 	at org.apache.flink.streaming.api.operators.collect.CollectCoordinationResponse.getResults(CollectCoordinationResponse.java:91)
2020-05-19T10:34:18.3262752Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher$ResultBuffer.dealWithResponse(CollectResultFetcher.java:291)
2020-05-19T10:34:18.3263385Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher$ResultBuffer.access$200(CollectResultFetcher.java:249)
2020-05-19T10:34:18.3264077Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:142)
2020-05-19T10:34:18.3264666Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:89)
2020-05-19T10:34:18.3265050Z 	... 40 more
{code}",,dwysakowicz,lzljs3620320,rmetzger,sewen,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17774,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 03:48:54 UTC 2020,,,,,,,,,,"0|z0ex34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/20 19:33;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1810&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=f83cd372-208c-5ec4-12a8-337462457129;;;","19/May/20 19:41;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1779&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=f83cd372-208c-5ec4-12a8-337462457129;;;","20/May/20 02:42;ykt836;cc [~TsReaper];;;","20/May/20 04:28;TsReaper;Thanks for the report. This is because type serializers are not thread safe but I didn't duplicate it in the sink function. I'll fix this immediately.;;;","20/May/20 06:52;TsReaper;I remembered that FLINK-17774 actually solves this problem. To handle object reuse (precisely {{RowData}} reuse), collect sink in FLINK-17774 will serialize the values in {{invoke}} method, so there is no serializing in socket server thread. Let's wait for FLINK-17774 to be merged so that this problem can also be solved.;;;","20/May/20 09:10;rmetzger;Is this test failure caused by the same issue?

{code}
2020-05-20T08:28:39.4310095Z [ERROR] Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.403 s <<< FAILURE! - in org.apache.flink.ml.common.utils.DataStreamConversionUtilTest
2020-05-20T08:28:39.4320654Z [ERROR] testBasicConvert(org.apache.flink.ml.common.utils.DataStreamConversionUtilTest)  Time elapsed: 4.005 s  <<< ERROR!
2020-05-20T08:28:39.4321429Z java.lang.RuntimeException: Failed to fetch next result
2020-05-20T08:28:39.4322299Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:92)
2020-05-20T08:28:39.4323188Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.next(CollectResultIterator.java:71)
2020-05-20T08:28:39.4324093Z 	at org.apache.flink.ml.common.utils.DataStreamConversionUtilTest.testBasicConvert(DataStreamConversionUtilTest.java:102)
2020-05-20T08:28:39.4324817Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-20T08:28:39.4325814Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-20T08:28:39.4326689Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-20T08:28:39.4327223Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-20T08:28:39.4327645Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-20T08:28:39.4328124Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-20T08:28:39.4328658Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-20T08:28:39.4329132Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-20T08:28:39.4329613Z 	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
2020-05-20T08:28:39.4330063Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-20T08:28:39.4330449Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-20T08:28:39.4330866Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-20T08:28:39.4331336Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-20T08:28:39.4331741Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-20T08:28:39.4332135Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-20T08:28:39.4332528Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-20T08:28:39.4332932Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-20T08:28:39.4333333Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-20T08:28:39.4333707Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-20T08:28:39.4334131Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-20T08:28:39.4334610Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-20T08:28:39.4335112Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-20T08:28:39.4335580Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-20T08:28:39.4336181Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-20T08:28:39.4336912Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-20T08:28:39.4337396Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-20T08:28:39.4337817Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-20T08:28:39.4338229Z Caused by: java.io.IOException: Failed to fetch job execution result
2020-05-20T08:28:39.4338795Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:178)
2020-05-20T08:28:39.4339399Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:128)
2020-05-20T08:28:39.4339991Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:89)
2020-05-20T08:28:39.4340555Z 	... 29 more
2020-05-20T08:28:39.4340962Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-20T08:28:39.4341488Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-05-20T08:28:39.4341935Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-05-20T08:28:39.4342452Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175)
2020-05-20T08:28:39.4342857Z 	... 31 more
2020-05-20T08:28:39.4343191Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-20T08:28:39.4343738Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-05-20T08:28:39.4344350Z 	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.lambda$getJobExecutionResult$2(PerJobMiniClusterFactory.java:186)
2020-05-20T08:28:39.4344925Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-05-20T08:28:39.4345393Z 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2020-05-20T08:28:39.4345858Z 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2020-05-20T08:28:39.4346560Z 	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.getJobExecutionResult(PerJobMiniClusterFactory.java:184)
2020-05-20T08:28:39.4347014Z 	... 32 more
2020-05-20T08:28:39.4347342Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-05-20T08:28:39.4347931Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-05-20T08:28:39.4348702Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-05-20T08:28:39.4349303Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)
2020-05-20T08:28:39.4349842Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)
2020-05-20T08:28:39.4350395Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)
2020-05-20T08:28:39.4350977Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)
2020-05-20T08:28:39.4351490Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:386)
2020-05-20T08:28:39.4351908Z 	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
2020-05-20T08:28:39.4352323Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-20T08:28:39.4352732Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-20T08:28:39.4353153Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-05-20T08:28:39.4353630Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-05-20T08:28:39.4354106Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-05-20T08:28:39.4354620Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-05-20T08:28:39.4355038Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-05-20T08:28:39.4355428Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-05-20T08:28:39.4355812Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-05-20T08:28:39.4356343Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-05-20T08:28:39.4356764Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-05-20T08:28:39.4357381Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-20T08:28:39.4358007Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-20T08:28:39.4358391Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-05-20T08:28:39.4358852Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-05-20T08:28:39.4359234Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-05-20T08:28:39.4359605Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-05-20T08:28:39.4359973Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-05-20T08:28:39.4360313Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-05-20T08:28:39.4360649Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-05-20T08:28:39.4361145Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-05-20T08:28:39.4361642Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-05-20T08:28:39.4362082Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-05-20T08:28:39.4362537Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-05-20T08:28:39.4364060Z Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalStateException: Concurrent access to KryoSerializer. Thread 1: Sink: Data stream collect sink (1/1) , Thread 2: Thread-10
2020-05-20T08:28:39.4364717Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-05-20T08:28:39.4365165Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-05-20T08:28:39.4365740Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:161)
2020-05-20T08:28:39.4366486Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:130)
2020-05-20T08:28:39.4367051Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:80)
2020-05-20T08:28:39.4367582Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:302)
2020-05-20T08:28:39.4368063Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:562)
2020-05-20T08:28:39.4368623Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:534)
2020-05-20T08:28:39.4369035Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720)
2020-05-20T08:28:39.4369434Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545)
2020-05-20T08:28:39.4369760Z 	at java.lang.Thread.run(Thread.java:748)
2020-05-20T08:28:39.4370528Z Caused by: java.lang.IllegalStateException: Concurrent access to KryoSerializer. Thread 1: Sink: Data stream collect sink (1/1) , Thread 2: Thread-10
2020-05-20T08:28:39.4371184Z 	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.enterExclusiveThread(KryoSerializer.java:630)
2020-05-20T08:28:39.4371729Z 	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(KryoSerializer.java:285)
2020-05-20T08:28:39.4372265Z 	at org.apache.flink.api.common.typeutils.base.ListSerializer.serialize(ListSerializer.java:123)
2020-05-20T08:28:39.4372816Z 	at org.apache.flink.streaming.api.operators.collect.CollectCoordinationResponse.<init>(CollectCoordinationResponse.java:63)
2020-05-20T08:28:39.4373461Z 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.serializeAccumulatorResult(CollectSinkFunction.java:308)
2020-05-20T08:28:39.4374096Z 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.accumulateFinalResults(CollectSinkFunction.java:271)
2020-05-20T08:28:39.4374663Z 	at org.apache.flink.streaming.api.operators.collect.CollectSinkOperator.close(CollectSinkOperator.java:57)
2020-05-20T08:28:39.4375246Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$closeOperator$5(StreamOperatorWrapper.java:205)
2020-05-20T08:28:39.4375843Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
2020-05-20T08:28:39.4376832Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.closeOperator(StreamOperatorWrapper.java:203)
2020-05-20T08:28:39.4377475Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$deferCloseOperatorToMailbox$3(StreamOperatorWrapper.java:177)
2020-05-20T08:28:39.4378306Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
2020-05-20T08:28:39.4379184Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
2020-05-20T08:28:39.4379953Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:79)
2020-05-20T08:28:39.4380906Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:155)
2020-05-20T08:28:39.4381484Z 	... 8 more
2020-05-20T08:28:39.4381630Z 
2020-05-20T08:28:39.8485928Z [INFO] 
2020-05-20T08:28:39.8486522Z [INFO] Results:
2020-05-20T08:28:39.8486796Z [INFO] 
2020-05-20T08:28:39.8487041Z [ERROR] Errors: 
2020-05-20T08:28:39.8488119Z [ERROR]   DataStreamConversionUtilTest.testBasicConvert:102 Â» Runtime Failed to fetch ne...
{code}
;;;","20/May/20 09:22;TsReaper;[~rmetzger] Yes. It says ""Concurrent access to KryoSerializer"".;;;","20/May/20 09:39;rmetzger;ok;;;","20/May/20 14:48;dwysakowicz;I will assign you [~TsReaper] the issue, as I think you are working on it, ok?;;;","20/May/20 16:04;dwysakowicz;another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1946&view=results;;;","20/May/20 22:50;sewen;Is the fix going in asap? Otherwise we need to do a separate fix now (or deactivate the feature).
We cannot leave unstable tests for long, because they tend to mask other failures.;;;","21/May/20 02:07;TsReaper;[~dwysakowicz] no problem
[~sewen] sorry for the delay... I used to want to wait for FLINK-17774. I've added a [quick fix|https://github.com/apache/flink/pull/12272] and will merge it asap.;;;","21/May/20 03:48;lzljs3620320;master: 1cf06e198bddfa26bc4899b2ad570926f77a9668

release-1.11: 2042a6a88be8f961b9a0f8bc9537c6cee2d69d41

Feel free to re-open this Jira if the bug be re-produced.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Change Latency Marker to work with ""scheduleAtFixedDelay"" instead of ""scheduleAtFixedRate""",FLINK-17816,13305981,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wind_ljy,sewen,sewen,19/May/20 18:18,05/Jun/20 08:47,13/Jul/23 08:07,05/Jun/20 08:47,,,,,,1.12.0,,,,Runtime / Task,,,,,0,pull-request-available,,,,"Latency Markers and other periodic timers are scheduled with {{scheduleAtFixedRate}}. That means every X time the callable is called. If it blocks (backpressure) is can be called immediately again.

I would suggest to switch this to {{scheduleAtFixedDelay}}  to avoid calling for a lot of latency marker injections when there is no way to actually execute the injection call.",,klion26,sewen,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 08:47:31 UTC 2020,,,,,,,,,,"0|z0ex1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/20 05:43;wind_ljy;I guess the change would be simple. We might just need to replace {{scheduleAtFixedRate}} with {{registerTimer}} in `LatencyMarksEmitter` like this: https://github.com/Jiayi-Liao/flink/commit/7f876d6d2a8fe8fc75a41ff17a0c84141f712af6;;;","24/May/20 21:46;sewen;Thanks for the proposal.

I am working on FLINK-17904 as part of another feature. With that in place, I think it is simpler to just switch the method call.;;;","30/May/20 07:17;wind_ljy;[~sewen] Yes it's much simpler and I've created a PR for it.

 

BTW I see that you comment in [#TestProcessingTimeService.java#L117|https://github.com/apache/flink/blob/955a6832b9fc3f68c5accef3a22a0b4d45140d68/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TestProcessingTimeService.java#L117]
{code:java}
// for all testing purposed, there is no difference between the fixed rate and fixed delay
{code}
Are you sure we should replace fixed delay with fixed rate in unit testing? because it may affect the testing result like #StreamSourceOperatorLatencyMetricsTest#testLatencyMarkEmissionEnabledOverrideViaExecutionConfig. (The unit testing passes but it should fail after we use the \{{scheduleAtFixedDelay}});;;","05/Jun/20 08:47;chesnay;master: d991b1ad03fc470e50507141a92a1d56f0f510e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BashJavaUtil script logic does not work for paths with spaces,FLINK-17809,13305852,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,19/May/20 09:00,19/May/20 14:34,13/Jul/23 08:07,19/May/20 14:34,1.10.0,1.11.0,,,,1.10.2,1.11.0,,,Deployment / Scripts,,,,,0,,,,,"Multiple paths aren't quoted (class path, conf_dir) resulting in errors if they contain spaces.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 14:34:50 UTC 2020,,,,,,,,,,"0|z0ew8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/20 14:34;chesnay;master: 2f18138df2b493c7e3af84d6dbf9b1acf09e1089
1.11: fd37030593a00a30316deacd9c4856e93b61b6a8 
1.10: 0e43356c1fa1e30d6879348dcc8c464ced2f181a ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix the broken link ""/zh/ops/memory/mem_detail.html"" in documentation",FLINK-17807,13305840,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,jark,jark,19/May/20 07:48,28/Jun/20 10:07,13/Jul/23 08:07,28/Jun/20 10:07,1.11.0,,,,,1.11.0,,,,Documentation,,,,,0,,,,,"We may need to update {{mem_setup_tm.zh.md}} and {{mem_trouble.zh.md}} to resolve the remaining broken link:

http://localhost:4000/zh/ops/memory/mem_detail.html",,jark,klion26,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 28 10:07:47 UTC 2020,,,,,,,,,,"0|z0ew68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/20 10:07;xtsong;This has been fixed by FLINK-17465.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InputProcessorUtil doesn't handle indexes for multiple inputs operators correctly,FLINK-17805,13305832,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,pnowojski,pnowojski,19/May/20 07:06,16/Oct/20 10:53,13/Jul/23 08:07,21/May/20 19:09,,,,,,1.11.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"This can cause {{ArrayIndexOutOfBound}} exception when input gates passed to {{InputProcessorUtil#createCheckpointedInputGatePair}} have lower IDs in the second input compared to input gates from the first one.

{noformat}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 7
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner$ThreadSafeUnaligner.notifyBufferReceived(CheckpointBarrierUnaligner.java:328)
	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.getNextBuffer(LocalInputChannel.java:218)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:637)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:615)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:603)
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:105)
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:110)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:136)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:178)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:341)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:206)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:196)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:553)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:526)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
",,kevin.cyj,pnowojski,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 19:09:20 UTC 2020,,,,,,,,,,"0|z0ew4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/20 19:09;pnowojski;merged to master as c9fd502c5a^ and c9fd502c5a
merged to release-1.11 as f3268aa8fa^ and f3268aa8fa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should throw a readable exception when group by Map type,FLINK-17803,13305806,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lzljs3620320,zouyunhe,zouyunhe,19/May/20 04:26,28/May/21 09:05,13/Jul/23 08:07,13/Apr/21 11:28,1.10.0,,,,,1.13.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"We use flink 1.10.0 ,  blink planner,  to  submit a batch sql job to read from a hive table which contains map type fields, and then aggregate.   the sql as below:

```
 create view aaa
 as select * from table1 where event_id = '0103002' and `day`='2020-05-13'
 and `hour`='13';
 create view view_1
 as
 select
 `day`,
 a.rtime as itime,
 a.uid as uid,
 trim(BOTH a.event.log_1['scene']) as refer_list,
 T.s as abflags,
 a.hdid as hdid,
 a.country as country
 from aaa as a
 left join LATERAL TABLE(splitByChar(trim(BOTH a.event.log_1['abflag]),
 ',')) as T(s) on true;

{color:#172b4d}CREATE VIEW view_6 as {color}
 {color:#172b4d} SELECT{color}
 {color:#172b4d} `uid`,{color}
 {color:#172b4d} `refer_list`,{color}
 {color:#172b4d} `abflag`,{color}
 {color:#172b4d}        last_value(country){color}
 {color:#172b4d} FROM view_1{color}
 {color:#172b4d} where `refer_list` in ('WELOG_NEARBY', 'WELOG_FOLLOW', 'WELOG_POPULAR'){color}
 {color:#172b4d} GROUP BY  `uid`, `refer_list`, abflag;{color}
 insert into ............
 ``` 

when submit the job, the exception occurs as below:
 org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: scala.MatchError: MAP (of class org.apache.flink.table.types.logical.LogicalTypeRoot)
         at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
         at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
         at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138)
         at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664)
         at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
         at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895)
         at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)
         at java.security.AccessController.doPrivileged(Native Method)
         at javax.security.auth.Subject.doAs(Subject.java:422)
         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754)
         at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
         at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)
 Caused by: java.lang.RuntimeException: scala.MatchError: MAP (of class org.apache.flink.table.types.logical.LogicalTypeRoot)
         at sg.bigo.streaming.sql.StreamingSqlRunner.main(StreamingSqlRunner.java:143)
         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
         at java.lang.reflect.Method.invoke(Method.java:498)
         at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
         ... 11 more
 Caused by: scala.MatchError: MAP (of class org.apache.flink.table.types.logical.LogicalTypeRoot)
         at org.apache.flink.table.planner.codegen.CodeGenUtils$.hashCodeForType(CodeGenUtils.scala:212)
         at org.apache.flink.table.planner.codegen.HashCodeGenerator$.$anonfun$generateCodeBody$1(HashCodeGenerator.scala:97)
         at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
         at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
  
 and then we found the method hashCodeForType  in the CodeGenUtils class do not match MAP type.  and we fix it as below
```
 def hashCodeForType(
 ctx: CodeGeneratorContext, t: LogicalType, term: String): String = t.getTypeRoot match

{ case BOOLEAN => s""$\\{className[JBoolean]}

.hashCode($term)""
 case MAP => s""$\{className[BaseMap]}.getHashCode($term)""  //the code we add
 case TINYINT => s""$\{className[JByte]}.hashCode($term)""
 ```


 then the job can be sumitted, it run for a while, another exception occurs:
 java.lang.RuntimeException: Could not instantiate generated class 'HashAggregateWithKeys$1543'
 at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:67)
 at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:46)
 at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:48)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:156)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:433)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
 at java.lang.Thread.run(Thread.java:745)
 Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
 at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68)
 at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78)
 at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:65)
 ... 8 more
 Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
 at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66)
 ... 10 more
 Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
 at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81)
 at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
 ... 13 more
 Caused by: org.codehaus.commons.compiler.CompileException: Line 459, Column 57: A method named ""compareTo"" is not declared in any enclosing class nor any supertype, nor through a static import
 at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124)
 at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8997)
 at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5060)
 at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
 at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4421)
 at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4394)
 at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062)
 at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394)
 at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575)
 at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2580)
 at org.codehaus.janino.UnitCompiler.access$2700(UnitCompiler.java:215)
  ",,godfreyhe,libenchao,lirui,lzljs3620320,zouyunhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22009,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 13 11:28:22 UTC 2021,,,,,,,,,,"0|z0evyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/20 02:50;lzljs3620320;[~zouyunhe] Re-open this, we should throws a better exception, FYI;;;","13/Apr/21 11:28;lzljs3620320;master (1.13): 3f72fd21bfcb424059e52cb65efd37035076dfed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set offset commit only if group id is configured for new Kafka Table source,FLINK-17802,13305802,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,19/May/20 03:59,16/Oct/20 10:56,13/Jul/23 08:07,26/May/20 13:28,1.11.0,,,,,1.11.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"As https://issues.apache.org/jira/browse/FLINK-17619 described,

the new Kafka Table source exits same problem and should fix too.

note: this  fix both for master and release-1.11

 ",,felixzheng,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17619,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 13:28:40 UTC 2020,,,,,,,,,,"0|z0evxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/20 02:07;leonard;Hi, [~gyfora]

Could you have a look the two PRs ?;;;","26/May/20 13:28;jark;master (1.12.0): 609513166eefd6ca9e993a5839e065f46fdf815c
1.11.0: acf94d7748b08ae56d28214614dec045287e7d26;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskExecutorTest.testHeartbeatTimeoutWithResourceManager timeout,FLINK-17801,13305731,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,rmetzger,rmetzger,18/May/20 18:06,16/Oct/20 10:54,13/Jul/23 08:07,22/May/20 16:52,1.11.0,,,,,1.10.2,1.11.0,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1705&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d

{code}
2020-05-18T10:06:52.8403444Z [ERROR] testHeartbeatTimeoutWithResourceManager(org.apache.flink.runtime.taskexecutor.TaskExecutorTest)  Time elapsed: 0.484 s  <<< ERROR!
2020-05-18T10:06:52.8404158Z java.util.concurrent.TimeoutException
2020-05-18T10:06:52.8404749Z 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)
2020-05-18T10:06:52.8405467Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-05-18T10:06:52.8406269Z 	at org.apache.flink.runtime.taskexecutor.TaskExecutorTest.testHeartbeatTimeoutWithResourceManager(TaskExecutorTest.java:449)
2020-05-18T10:06:52.8407050Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-18T10:06:52.8407685Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-18T10:06:52.8408463Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-18T10:06:52.8409118Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-18T10:06:52.8409804Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-18T10:06:52.8410528Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-18T10:06:52.8411388Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-18T10:06:52.8412167Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-18T10:06:52.8412884Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-18T10:06:52.8413795Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-18T10:06:52.8414435Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-18T10:06:52.8415052Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-18T10:06:52.8415692Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-18T10:06:52.8416251Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-18T10:06:52.8416863Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-18T10:06:52.8417498Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-18T10:06:52.8418235Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-18T10:06:52.8418883Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-18T10:06:52.8419374Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-18T10:06:52.8419775Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-18T10:06:52.8420151Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-18T10:06:52.8420539Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-18T10:06:52.8420912Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-18T10:06:52.8421493Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-18T10:06:52.8422226Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-18T10:06:52.8422977Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-18T10:06:52.8423807Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-18T10:06:52.8424842Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-18T10:06:52.8425680Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-18T10:06:52.8426391Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-18T10:06:52.8427085Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-18T10:06:52.8427490Z 
2020-05-18T10:06:52.8764401Z [INFO] Running org.apache.flink.runtime.taskexecutor.slot.TimerServiceTest
{code}",,rmetzger,trohrmann,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 16:52:08 UTC 2020,,,,,,,,,,"0|z0evhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/20 08:52;trohrmann;It looks to me as if the timeout in {{TaskExecutorTest.java:449}} has been set too aggressively (150 ms) for AZP. I propose to increase it to 10s as this timeout is not relevant for the proper functioning of the test.;;;","22/May/20 16:52;trohrmann;Fixed via

master: a47d7050c77c373c1cb27f7c826bf0af8cfaa700
1.11.0: 828ba1dd7356ac3694f3c4b8688ae8ecbd188771
1.10.2: 713203808b54050431d687dbf1d524c900c7141b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB optimizeForPointLookup results in missing time windows,FLINK-17800,13305703,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunta,YordanPavlov,YordanPavlov,18/May/20 15:32,16/Aug/21 06:27,13/Jul/23 08:07,26/Jun/20 16:48,1.10.0,1.10.1,,,,1.10.2,1.11.0,1.12.0,,Runtime / State Backends,,,,,1,pull-request-available,,,,"+My Setup:+

We have been using the _RocksDb_ option of _optimizeForPointLookup_ and running version 1.7 for years. Upon upgrading to Flink 1.10 we started receiving a strange behavior of missing time windows on a streaming Flink job. For the purpose of testing I experimented with previous Flink version and (1.8, 1.9, 1.9.3) and non of them showed the problem

 

A sample of the code demonstrating the problem is here:
{code:java}
 val datastream = env
 .addSource(KafkaSource.keyedElements(config.kafkaElements, List(config.kafkaBootstrapServer)))

 val result = datastream
 .keyBy( _ => 1)
 .timeWindow(Time.milliseconds(1))
 .print()
{code}
 

 

The source consists of 3 streams (being either 3 Kafka partitions or 3 Kafka topics), the elements in each of the streams are separately increasing. The elements generate increasing timestamps using an event time and start from 1, increasing by 1. The first partitions would consist of timestamps 1, 2, 10, 15..., the second of 4, 5, 6, 11..., the third of 3, 7, 8, 9...

 

+What I observe:+

The time windows would open as I expect for the first 127 timestamps. Then there would be a huge gap with no opened windows, if the source has many elements, then next open window would be having a timestamp in the thousands. A gap of hundred of elements would be created with what appear to be 'lost' elements. Those elements are not reported as late (if tested with the ._sideOutputLateData_ operator). The way we have been using the option is by setting in inside the config like so:

??etherbi.rocksDB.columnOptions.optimizeForPointLookup=268435456??

We have been using it for performance reasons as we have huge RocksDB state backend.",,aljoscha,klion26,knaufk,liyu,Ming Li,rmetzger,sewen,shazeline,wanglijie,wind_ljy,YordanPavlov,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18338,,,,FLINK-14482,FLINK-23789,,,,,,,,,,"19/May/20 10:15;YordanPavlov;MissingWindows.scala;https://issues.apache.org/jira/secure/attachment/13003376/MissingWindows.scala","28/May/20 15:24;YordanPavlov;MyMissingWindows.scala;https://issues.apache.org/jira/secure/attachment/13004270/MyMissingWindows.scala","27/May/20 03:40;yunta;MyMissingWindows.scala;https://issues.apache.org/jira/secure/attachment/13004086/MyMissingWindows.scala",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 26 16:48:13 UTC 2020,,,,,,,,,,"0|z0evbk:",9223372036854775807,"After FLINK-17800 by default we will set `setTotalOrderSeek` to true for RocksDB's `ReadOptions`, to prevent user from miss using `optimizeForPointLookup`. Meantime we support customizing `ReadOptions` through `RocksDBOptionsFactory`. Please set `setTotalOrderSeek` back to false if any performance regression observed (normally won't happen according to our testing).",,,,,,,,,,,,,,,,,,,"19/May/20 06:27;yunta;[~YordanPavlov] Thanks for reporting this. 

Have you tried Flink-1.10 without optimizeForPointLookup, and will you job also meet this problem again?

Basicily, optimizeForPointLookup would create a LRUcache with the size from your configuration (e.g. 268435456 bytes in your example), however, from Flink-1.10, we use managed memory for RocksDB by default, and it will use the size of managed memory to create an internal cache. You could also try to disable [state-backend-rocksdb-memory-managed|https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html#state-backend-rocksdb-memory-managed] to see whether your problem still existed.

Last but not least, would you please give a simple example with fake source to reproduce this problem so that we could figure it out.;;;","19/May/20 10:17;YordanPavlov;[~yunta] Thanks for picking up on this problem. 
I am not observing the problem if I remove the _optimizeForPointLookup_ setting with Flink 1.10.1. I am attaching a sample source code demonstrating the problem.

[^MissingWindows.scala]

 

^Running that code with the optimizeForPointLookup setting what I am seeing as output is:^
{code:java}
TestElement(122)
TestElement(123)
TestElement(124)
TestElement(125)
TestElement(126)
TestElement(127)
Missed window before element TestElement(2780)
TestElement(2780){code}
 

^What I would expect instead is no 'Missed window' warning to be fired and all TestElements to be printed.^;;;","19/May/20 16:36;yunta;[~YordanPavlov]

Is the attachment with the sample code in your description could reproduce this problem?

BTW, have you checked that if turn off managed memory as I said, could this unexpected behavior still happen?;;;","20/May/20 07:33;YordanPavlov;[~yunta]

Yes the attachment reproduces the problem, it is an even simpler code than what I originally described as it does not have a Kafka source. 

I did check with disabling the managed memory:
{noformat}
main] INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: state.backend.rocksdb.memory.managed, false{noformat}
 

that does not prevent the problem from happening.

 ;;;","27/May/20 03:42;yunta;[~YordanPavlov] I have tried your program with some changes. However, the job would run with failure as {{assert(input.size == 1)}} cannot be ensured.

First of all, I cannot run your program as you use internal class {{StreamingScalaJob}}, I have changed your program to general Flink interface so that it could run.

Secondly, the program would fail:

{code:java}
Caused by: TimerException{java.lang.AssertionError: assertion failed}
	... 11 more
Caused by: java.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:156)
	at org.apache.flink.streaming.scala.examples.RateChecker.apply(MissingWindows.scala:31)
	at org.apache.flink.streaming.scala.examples.RateChecker.apply(MissingWindows.scala:28)
	at org.apache.flink.streaming.api.scala.function.util.ScalaWindowFunctionWrapper.apply(ScalaWindowFunctionWrapper.scala:44)
	at org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableWindowFunction.process(InternalIterableWindowFunction.java:44)
	at org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableWindowFunction.process(InternalIterableWindowFunction.java:32)
	at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.emitWindowContents(WindowOperator.java:549)
	at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.onProcessingTime(WindowOperator.java:503)
	at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.onProcessingTime(InternalTimerServiceImpl.java:260)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1501)
	... 10 more
{code}

I have attached my code named as  [^MyMissingWindows.scala] , please give us reproduceable program code without dependency or correct my usage if I am wrong.

;;;","28/May/20 15:25;YordanPavlov;Hello [~yunta],

I am sorry for using internal code in my example. I had modified your code and uploaded it below so that you can reproduce the problem. I did two changes.
 # The program fails with assert error. This Is fixed by using 
{code:java}
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime){code}

 # In order to reproduce the problem the RocksDB optimizeForPointLookup option should be set.

Also note that the program would fail for me if I run it inside a local Flink cluster. The problem would not appear if I start the class from within an IDE (InteliJ in my case). Find below an edited version of your code which fails for me. Please let me know If you need additional information. 

[^MyMissingWindows.scala];;;","29/May/20 05:59;yunta;[~YordanPavlov]
 The quick fix for your scenario: configure {{state.backend.rocksdb.timer-service.factory}} as {{HEAP}}.

The root cause is that RocksDB failed to seek the valid key and values on event time when optimizeForPointLookup is configured. And this bug caused by RocksDB actually also stays in Flink-1.7 (rocksDB version: 5.7.5), the reason why you do not meet is that Flink store timer on heap by default before Flink-1.10. Starting from Flink-1.10, timer starts to store in RocksDB instead, which surprise you.

I have tried already released RocksDB versions and found at least from RocksDB-6.2.2, this bug disappeared. However, as RocksDB-6.2.2 also changed the internal implementation of [OptimizeForPointLookup|https://github.com/facebook/rocksdb/blob/6d113fc066c5816887eb19c84d12c0677a68af2b/options/options.cc#L514-L526], I still need some time to figure out why this bug happened in RocksDB.;;;","29/May/20 17:52;liyu;Thanks for debugging and locating the root cause [~yunta].

From the analysis, this should be a RocksDB bug and requires a fix in FRocksDB and building a new version, which makes it not an easy fix. It's also a long existing issue but with more severe impact when we make RocksDB the default timer store for RocksDB backend in 1.10.0 release, rather than regression caused by changes in 1.11 release cycle.

I agree we should try to fix it before 1.11.0, but tend to not consider it as a release blocker due to the above reasons, and suggest to downgrade the severity to Critical.

Please also estimate the time needed and see whether we could make it in 1.11.0. Thanks.;;;","02/Jun/20 04:11;yunta;[~liyu] Thanks for your suggestion.

[~YordanPavlov], from my local experiments, If I revert the implementation of [OptimizeForPointLookup|https://github.com/facebook/rocksdb/blob/6d113fc066c5816887eb19c84d12c0677a68af2b/options/options.cc#L514-L526], this bug still existed in newer RocksDB. That's to say using {{NoopTransform}} (which is not obvious in RocksDB-java) and {{kHashSearch}} indexType could lead to this bug. And I have also created [related issue|https://github.com/facebook/rocksdb/issues/6893] in RocksDB community.

;;;","02/Jun/20 14:06;sewen;Could we forbid (ignore) the {{OptimizeForPointLookup}} option in Flink 1.11.0 and 1.10.2, as a safety net against data loss as until we upgraded to the patched RocksDB version?;;;","03/Jun/20 10:49;yunta;[~sewen], the root cause is the prefix extractor of {{NoopTransform}} introduced by {{optimizeForPointLookup}}.

From this lesson, we should treat the javadoc of {{#optimizeForPointLookup}} not a suggestion but a hard limit:
{noformat}
Use this if you don't need to keep the data sorted, i.e. you'll never use an iterator, only Put() and Get() API calls.
{noformat}
However, I cannot check whether a prefix extractor has ever been configured from java side to avoid user misuse, that is to say, we cannot disable the {{optimizeForPointLookup}} option from Flink side.

The walk-around solution is to call {{setTotalOrderSeek(true)}} for {{ReadOptions}} when creating iterators, that could ensure the sorted order is correct. I have tested the performance of iterator-like operations of map state below, it looks like no obvious performance change.
||Iterator like opertions||original ops/ms||with setTotalOrderSeek ops/ms||performance change||
|MapStateBenchmark.mapEntries|322.306|320.794|-0.5%|
|MapStateBenchmark.mapIsEmpty|42.012|42.042|0.1%|
|MapStateBenchmark.mapIterator|313.1|319.3|2.0%|
|MapStateBenchmark.mapKeys|324.088|311.491|-3.9%|
|MapStateBenchmark.mapValues|321.387|328.309|2.2%|;;;","16/Jun/20 08:36;liyu;Merged into master via
8ca388ca0225ff22f532c8a65f97d8cfea027c22
f1250625b2ade530fa2619d6e1bb734832748d31

Merged into release-1.11 via
5c0de8d6d5a9eea1a779cf3703412f522bece54c
8f3172962c5033d4a7dbf5232c18d3e872821c02

Working on release-1.10.;;;","16/Jun/20 11:39;liyu;Merged into release-1.10 via:
b8ddbef9a5cc5dc769ba61bd5019dd96843c932f
88c22864504d772764c5838afe0b944f1da50a3a

Closing JIRA since fix merged into all related branches.;;;","17/Jun/20 12:35;rmetzger;Reopening, because I reverted the change in:
(master)
f6f51d8767990cee7f1ca052b040530b646c3efe
59486908060cbb04c9b34d800a935758d06d1c69

(release-1.11)
cdac5e32eb2d3348d711207c96c91f97a17aa2d9
6e367fb06a3f02360bd8e9216dd477d3e3b68186;;;","22/Jun/20 11:47;yunta;After figured out why RocksDB could crash in FLINK-18338, I have created new PR https://github.com/apache/flink/pull/12736;;;","23/Jun/20 05:57;liyu;Escalating the priority to Blocker since the previous observed coredump issue is confirmed to be a UT design problem with PR already in review, and the issue reported here would cause silent data loss thus a critical one for production usage and better get fixed in 1.11.0;;;","23/Jun/20 09:31;liyu;Downgrade to Critical after some offline discussion, will try best to make it in though. Sorry for the back and forth priority change.;;;","26/Jun/20 16:48;liyu;Merged the new fix into master via
3516e37ae0aa4ee040b6844f336541315a455ce9
11d45135d85937edd16fb4f8f94ba71f5f794626
1718f50645ddc01d5e2e13cc5627bafe98191fa2

into release-1.11 via
b2e344a46c5d30ad46231d5c6a42bf09d9e8e559
33caa00e8df88565f022d4258148d09c90d9452b
7e1c83ddcf0e5e4417ccf25fd1d0facce9f30e0e

into release-1.10 via
de6f3aa7e5b2e4fcfbed4adeab12d4d519f1e6fb
3f8649e5c7bf731fac3cc5bfd3c5ed466f1dc561
9b45486ccce31ebef3f91dd4e6102efe3c6d51a3


Since all work done, closing the JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in all network benchmarks,FLINK-17799,13305676,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,pnowojski,pnowojski,18/May/20 13:14,16/Oct/20 10:49,13/Jul/23 08:07,18/May/20 18:40,,,,,,1.11.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"Performance regression took place between May 2nd (commit 18af2a1) and May 15th (commit 282da0dd3e). Unfortunately in this period benchmarking infrastructure was broken, so we do not know on which day has it happened. ",,guoyangze,gyfora,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16536,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 18:40:46 UTC 2020,,,,,,,,,,"0|z0ev5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 13:49;pnowojski;git bisect revealed that this regression is caused by d7525baf093035c9b0fe125602814833388d1973

>  [FLINK-16536][network][checkpointing] Implement InputChannel state recovery for unaligned checkpoint

CC [~zjwang];;;","18/May/20 18:40;pnowojski;Merged to 1.11 branch as b834ff6b2f merged to master as a875235389;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tear down installed software in reverse order in Jepsen Tests,FLINK-17794,13305625,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gjy,gjy,gjy,18/May/20 08:46,16/Oct/20 10:53,13/Jul/23 08:07,21/May/20 16:49,1.10.1,1.11.0,,,,1.11.0,,,,Tests,,,,,0,pull-request-available,,,,"Tear down installed software in reverse order in Jepsen tests. This mitigates the issue that sometimes YARN's NodeManager directories cannot be removed using {{rm -rf}} because Flink processes keep running and generate files after the YARN NodeManager is shut down. {{rm -r}} removes files recursively but if files are created in the background concurrently, the command can still fail with a non-zero exit code.

{noformat}
sh -c \""cd /; rm -rf /opt/hadoop\"""", :exit 1, :out """", :err ""rm: cannot remove '/opt/hadoop/tmp/nm-local-dir/usercache/root/appcache/application_1587567275082_0001/flink-io-3587fdbb-15be-4482-94f2-338bfe6b1acc/job_77be6dd9f1b2aa218348e8b8a2512660_op_StreamMap_5271c210329e73bd743f3227edfb3b71__27_30__uuid_02dbbf1e-d2d5-43e8-ab34-040345f96476/db': Directory not empty\nrm: cannot remove '/opt/hadoop/tmp/nm-local-dir/usercache/root/appcache/application_1587567275082_0001/flink-io-d14f2078-74ee-4b8b-aafe-4299577f214f/job_77be6dd9f1b2aa218348e8b8a2512660_op_StreamMap_7d23c6ceabda05a587f0217e44f21301__17_30__uuid_2de2b67d-0767-4e32-99f0-ddd291460947/db': Directory not empty
{noformat}",,gjy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 16:49:42 UTC 2020,,,,,,,,,,"0|z0euu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/20 16:49;gjy;1.11: 6a4714fdeff96d54db5fde5fac9b0eb355886b47
master: 2b2c574f102689b3cde9deac0bd1bcf78ad7ebc7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing to invoking jstack on TM processes should not fail Jepsen Tests,FLINK-17792,13305606,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gjy,gjy,gjy,18/May/20 07:10,16/Oct/20 10:48,13/Jul/23 08:07,18/May/20 14:11,1.10.1,1.11.0,,,,1.11.0,,,,Tests,,,,,0,pull-request-available,,,,"{{jstack}} can fail if the JVM process exits prematurely while or before we invoke {{jstack}}. If {{jstack}} fails, the exception propagates and exits the Jepsen Tests prematurely.
",,gjy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 14:11:11 UTC 2020,,,,,,,,,,"0|z0euq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 14:11;gjy;1.11: d8a77cbf93007bf970963a4499aa06501c0d9808
master: 417936d8722c7b466f22bc13b9063e7298e0cbd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-kafka-base does not compile on Java11,FLINK-17790,13305601,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,rmetzger,rmetzger,18/May/20 06:52,16/Oct/20 10:50,13/Jul/23 08:07,19/May/20 07:31,1.11.0,,,,,1.11.0,,,,Connectors / Kafka,Table SQL / Ecosystem,,,,0,pull-request-available,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1657&view=logs&j=946871de-358d-5815-3994-8175615bc253&t=e0240c62-4570-5d1c-51af-dd63d2093da1

[ERROR] /__w/3/s/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaOptions.java:[271,41] incompatible types: cannot infer type-variable(s) U,T,T,T,T
    (argument mismatch; bad return type in lambda expression
      java.util.Optional<org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner> cannot be converted to java.util.Optional<? extends org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner<org.apache.flink.table.data.RowData>>)
[INFO] 1 error
",,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 07:31:57 UTC 2020,,,,,,,,,,"0|z0euow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/20 07:31;trohrmann;Fixed via

master: d0fc02839d2f3d99be2c6420758ab50fb4828bc3
1.11.0: fdec46d267ca938522f170a38fd8eb268941aa03;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DelegatingConfiguration should remove prefix instead of add prefix in toMap,FLINK-17789,13305600,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liufangliang,lzljs3620320,lzljs3620320,18/May/20 06:50,22/Jul/20 07:39,13/Jul/23 08:07,22/Jul/20 07:39,,,,,,1.12.0,,,,API / Core,,,,,0,pull-request-available,,,,"{code:java}
Configuration conf = new Configuration();
conf.setString(""k0"", ""v0"");
conf.setString(""prefix.k1"", ""v1"");
DelegatingConfiguration dc = new DelegatingConfiguration(conf, ""prefix."");
System.out.println(dc.getString(""k0"", ""empty"")); // empty
System.out.println(dc.getString(""k1"", ""empty"")); // v1

System.out.println(dc.toMap().get(""k1"")); // should be v1, but null
System.out.println(dc.toMap().get(""prefix.prefix.k1"")); // should be null, but v1
{code}",,liufangliang,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 22 07:39:37 UTC 2020,,,,,,,,,,"0|z0euoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 07:08;chesnay;I disagree. The current behavior is simple; take an option, apply prefix, look it up in the backing config.

If you now start removing trimming stuff that _happens_ to match the configured prefix you just introduce additional edge-cases.

Following your example, an option `k0.prefix.prefix.v1` would never work since you're removing the second `prefix`.;;;","18/May/20 07:21;lzljs3620320;Hi [~chesnay], thanks for quick response.

> an option `k0.prefix.prefix.v1` would never work since you're removing the second `prefix`.

What do you mean? `prefix.k0.prefix.v1` will be removed to `k0.prefix.v1`, but `k0.prefix.prefix.v1` will be filtered. The logical should be like `DelegatingConfiguration.addAllToProperties`.

 

Actually, I think the `toMap` should be consistent with `addAllToProperties`, but now:
{code:java}
System.out.println(dc.toMap()); // {prefix.k0=v0, prefix.prefix.k1=v1}

Properties properties = new Properties();
dc.addAllToProperties(properties);
System.out.println(properties); // {k1=v1}
{code}
 ;;;","18/May/20 07:56;chesnay;I'd argue that the behavior of {{DelegatingConfiguration.addAllToProperties}} is wrong.

But fair enough, my example was bad.
Let's say ""prefix.prefix.v1"" instead; we set ""prefix"" as the prefix (duh), and add ""prefix.v1"" as an option.
You are then removing ""prefix"", assuming it to be an error, but then the look will fail since you check ""prefix.v1"", instead of the supposed ""prefix.prefix.v1"" .;;;","18/May/20 08:08;lzljs3620320;If there is an option = ConfigOptions.key(""prefix.v1""), DelegatingConfiguration.get(option) -> should contains this key.

But I want to say is modify {{toMap}} , DelegatingConfiguration.toMap().get(options) -> should also contains this key.;;;","18/May/20 08:50;chesnay;Alright hold on, this is getting really confusing.

Let's clarify the behaviors a bit.

For a DelegatingConfiguration with prefix P:

1) get(X) reads P.X from the backing configuration
1) set(X) writes P.X to the backing configuration
2) addAllToProperties() writes all options starting with P into the properties, such that properties.get(X) works.
3) toMap() writes all options of the backing configuration, with an added prefix P. map.get(X) will not work, but map.get(P.X) will.

It is not a problem that toMap() behaves differently to addAllToProperties() in general, as they actually serve very different purposes.
I can see that the behavior can be surprising, but currently it is necessary and we would break code if we were to change anything.
Change addAllToProperties() and the metric system breaks (reporters look up options by suffix, since the prefix contains the reporter name and is hence dynamic), change toMap() and the configuration of formats is broken (because suddenly the options are no longer fully qualified.

However, one problem I do have with  3) is the following:
{code}
Configuration conf = new Configuration();
conf.setString(""k0"", ""v0"");
conf.setString(""prefix.k1"", ""v1"");

DelegatingConfiguration dc = new DelegatingConfiguration(conf, ""prefix."");

System.out.println(dc.getString(""k1"", null)); //v1
System.out.println(dc.getString(""k0"", null)); //null

Map<String, String> map = dc.toMap();
System.out.println(map.get(""prefix.k1"")); //v1
System.out.println(map.get(""prefix.k0"")); //v0
{code}

Basically, toMap() conjures settings out of thin air, where it should be an accurate representation of the configuration.;;;","19/May/20 02:27;lzljs3620320;Thanks for your detailed analysis.

Now only {{map.get(""}}{{prefix.}}{{prefix.k1"")}}  can get v1.

I got your point, but I don't think it is good design. DelegatingConfiguration is Configuration, and the toMap should be consistent with addAllToProperties in Configuration. And for toMap, it should be consistent with get too. I mean, DelegatingConfiguration override these methods from base class Configuration.;;;","15/Jul/20 10:09;liufangliang;Hi [~lzljs3620320] 

Can i pick this issue ? 

I have understood your intentions through your dialogue, I will make `toMap` consistent with `addAllToProperties`.

 

 ;;;","15/Jul/20 10:21;chesnay;[~lzljs3620320] I'm not saying it is good design (of course the behaviors _should_ be consistent and unsurprising), but I don't see a way to change this behavior without breaking stuff.

;;;","16/Jul/20 02:16;lzljs3620320;[~chesnay] I see what you mean.

Hi [~liufangliang], thank you for your participation, for this issue, there is no clear answer to change it or not, It seems that the community has not yet reached an agreement. ;;;","16/Jul/20 03:08;liufangliang;Hi [~lzljs3620320] [~chesnay] ,

Sorry for my irregular behavior, I will pay attention next time.

I also think `toMap` should be consistent with `addAllToProperties`. So my PR is to keep them consistent, no more.;;;","22/Jul/20 07:39;lzljs3620320;master: 3ebf0d94c8c0131cd56c34536312fad19b534cf9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scala shell in yarn mode is broken,FLINK-17788,13305594,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kkl0u,zjffdu,zjffdu,18/May/20 06:17,11/Jun/20 18:57,13/Jul/23 08:07,11/Jun/20 18:57,1.10.1,1.11.0,,,,1.10.2,1.11.0,1.12.0,,Scala Shell,,,,,0,pull-request-available,,,,"When I start scala shell in yarn mode, one yarn app will be launched, and after I write some flink code and trigger a flink job, another yarn app will be launched but would failed to launch due to some conflicts.",,kkl0u,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 18:57:12 UTC 2020,,,,,,,,,,"0|z0eunc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/20 14:25;kkl0u;[~zjffdu] can you be more specific on what you see as error? Also do you launch the Yarn cluster from the Flink Shell? Or you expect to find a running Yarn Cluster? And if the second is the case, how do you specify the {{ApplicationId}}?

In addition, you mean that the Yarn cluster is launched and then the first job fails, or that the first job works fine, and then the second fails?

The reason I am asking is because the {{DeploymentOptions.TARGET}} that you remove in the PR has to be set to something. If you launch the yarn cluster from the FlinkYarnSessionCLI, then Flink will find the ""properties"" file that contains the {{ApplicationId}} of the cluster and it will know where to submit the job.;;;","26/May/20 15:22;zjffdu;[~kkl0u] The phenomenon I see is that when I launch scala-shell in yarn mode, it would launch one yarn app for flink yarn session cluster. Then I type some flink code to trigger one flink, then it would launch another yarn app. The root cause is that the deployment option is set to yarn-per-job which is incorrect. Actually deployment option would be set 2 times which cause the wrong setting. 
* https://github.com/apache/flink/blob/master/flink-scala-shell/src/main/scala/org/apache/flink/api/scala/FlinkShell.scala#L248
* https://github.com/apache/flink/blob/master/flink-scala-shell/src/main/scala/org/apache/flink/api/scala/FlinkShell.scala#L287

This is not a clean fix, I think the whole flink scala shell module need to be refactoring to make the code clean. 

;;;","26/May/20 18:09;kkl0u;[~zjffdu] I think that setting the Deployment target twice is not a problem. I agree though that the Scala Shell could use a little refactoring. So you mean that each time you call {{execute()}}, the shell will create a new cluster, right? Will it execute the job and then kill the cluster? ;;;","28/May/20 02:45;zjffdu;[~kkl0u] Yes，each time when I call execute(), scala shell will create a new flink session cluster. But actually the session cluster will fail to launch, I havn't digger into it further, I suspect it  is due to some conflict with the previous flink session cluster.;;;","28/May/20 09:18;kkl0u;Thanks a lot [~zjffdu] for the clarification. Let me know if you find anything and I will also have a look.;;;","01/Jun/20 09:12;zjffdu;[~kkl0u] The second yarn app fail to launch due to port conflict, here's what I see. But anyway, scala shell should not launch another yarn app, instead it should use only one yarn session cluster.

{code}
2020-06-01 17:09:18,860 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting YarnJobClusterEntrypoint down with application status FAILED. Diagnostics org.apache.flink.util.FlinkException: Could not create the DispatcherResourceManagerComponent.
	at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:255)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:216)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:516)
	at org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.main(YarnJobClusterEntrypoint.java:89)
Caused by: java.net.BindException: Could not start rest endpoint on any port in port range 57574
	at org.apache.flink.runtime.rest.RestServerEndpoint.start(RestServerEndpoint.java:222)
	at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:163)
	... 9 more
{code};;;","11/Jun/20 18:57;kkl0u;Fixed on master with 6744033e1c05f51c8213d06f1fe4488c9673ed57
on release-1.11 with 3d0853035c1a70522bf239ced4b17c0f67c5e025
and on release-1.10 with a5de333d20fd56dfd77243f0a43ed4803309bfda;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BucketStateSerializerTest fails on output mismatch,FLINK-17787,13305593,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,aljoscha,rmetzger,rmetzger,18/May/20 06:10,18/May/20 06:49,13/Jul/23 08:07,18/May/20 06:49,1.11.0,,,,,1.11.0,,,,API / DataStream,Tests,,,,0,test-stability,,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1653&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d

{code}
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   BucketStateSerializerTest.testDeserializationEmpty:139 expected:</[__w/2/s]/flink-streaming-jav...> but was:</[Users/aljoscha/Dev/flink]/flink-streaming-jav...>
[ERROR]   BucketStateSerializerTest.testDeserializationFullNoInProgress:260->testDeserializationFull:289 expected:</[__w/2/s]/flink-streaming-jav...> but was:</[Users/aljoscha/Dev/flink]/flink-streaming-jav...>
[ERROR]   BucketStateSerializerTest.testDeserializationOnlyInProgress:184 expected:</[__w/2/s]/flink-streaming-jav...> but was:</[Users/aljoscha/Dev/flink]/flink-streaming-jav...>
[ERROR] Errors: 
[ERROR]   BucketStateSerializerTest.testDeserializationFull:255->testDeserializationFull:284->restoreBucket:332 » FileNotFound
[INFO] 
[ERROR] Tests run: 1491, Failures: 3, Errors: 1, Skipped: 53


2020-05-17T21:04:39.6023179Z [ERROR] Tests run: 8, Failures: 3, Errors: 1, Skipped: 4, Time elapsed: 0.508 s <<< FAILURE! - in org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest
2020-05-17T21:04:39.6024418Z [ERROR] testDeserializationOnlyInProgress[Previous Version = 1](org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest)  Time elapsed: 0.416 s  <<< FAILURE!
2020-05-17T21:04:39.6025661Z org.junit.ComparisonFailure: expected:</[__w/2/s]/flink-streaming-jav...> but was:</[Users/aljoscha/Dev/flink]/flink-streaming-jav...>
2020-05-17T21:04:39.6026162Z 	at org.junit.Assert.assertEquals(Assert.java:115)
2020-05-17T21:04:39.6026509Z 	at org.junit.Assert.assertEquals(Assert.java:144)
2020-05-17T21:04:39.6027088Z 	at org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest.testDeserializationOnlyInProgress(BucketStateSerializerTest.java:184)
2020-05-17T21:04:39.6027655Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-17T21:04:39.6028099Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-17T21:04:39.6028587Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-17T21:04:39.6029077Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-17T21:04:39.6029717Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-17T21:04:39.6030222Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-17T21:04:39.6030724Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-17T21:04:39.6031206Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-17T21:04:39.6031670Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-17T21:04:39.6032129Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-17T21:04:39.6033125Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-17T21:04:39.6033818Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-17T21:04:39.6034557Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-17T21:04:39.6035086Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-17T21:04:39.6035502Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-17T21:04:39.6035937Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-17T21:04:39.6036360Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-17T21:04:39.6036732Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-17T21:04:39.6037236Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-17T21:04:39.6037625Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-17T21:04:39.6038028Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-17T21:04:39.6038471Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-17T21:04:39.6038900Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-17T21:04:39.6039471Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-17T21:04:39.6039923Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-17T21:04:39.6040325Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-17T21:04:39.6040900Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-17T21:04:39.6041333Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-17T21:04:39.6041856Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-17T21:04:39.6042392Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-17T21:04:39.6042887Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-17T21:04:39.6043432Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-17T21:04:39.6043970Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-17T21:04:39.6044532Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-17T21:04:39.6045010Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-17T21:04:39.6045288Z 
2020-05-17T21:04:39.6045777Z [ERROR] testDeserializationEmpty[Previous Version = 1](org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest)  Time elapsed: 0.004 s  <<< FAILURE!
2020-05-17T21:04:39.6046931Z org.junit.ComparisonFailure: expected:</[__w/2/s]/flink-streaming-jav...> but was:</[Users/aljoscha/Dev/flink]/flink-streaming-jav...>
2020-05-17T21:04:39.6047442Z 	at org.junit.Assert.assertEquals(Assert.java:115)
2020-05-17T21:04:39.6047785Z 	at org.junit.Assert.assertEquals(Assert.java:144)
2020-05-17T21:04:39.6048346Z 	at org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest.testDeserializationEmpty(BucketStateSerializerTest.java:139)
2020-05-17T21:04:39.6048904Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-17T21:04:39.6049441Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-17T21:04:39.6049952Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-17T21:04:39.6050385Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-17T21:04:39.6050829Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-17T21:04:39.6051321Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-17T21:04:39.6051827Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-17T21:04:39.6052328Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-17T21:04:39.6052884Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-17T21:04:39.6053348Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-17T21:04:39.6053832Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-17T21:04:39.6054349Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-17T21:04:39.6054756Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-17T21:04:39.6055188Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-17T21:04:39.6055621Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-17T21:04:39.6056032Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-17T21:04:39.6056452Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-17T21:04:39.6056818Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-17T21:04:39.6057190Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-17T21:04:39.6057559Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-17T21:04:39.6057978Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-17T21:04:39.6058405Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-17T21:04:39.6058819Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-17T21:04:39.6059507Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-17T21:04:39.6059931Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-17T21:04:39.6060350Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-17T21:04:39.6060744Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-17T21:04:39.6061179Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-17T21:04:39.6061704Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-17T21:04:39.6062225Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-17T21:04:39.6062742Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-17T21:04:39.6063264Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-17T21:04:39.6063829Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-17T21:04:39.6064387Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-17T21:04:39.6064848Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-17T21:04:39.6065132Z 
2020-05-17T21:04:39.6065610Z [ERROR] testDeserializationFull[Previous Version = 1](org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest)  Time elapsed: 0.006 s  <<< ERROR!
2020-05-17T21:04:39.6066874Z java.io.FileNotFoundException: File Not Found: /Users/aljoscha/Dev/flink/flink-streaming-java/src/test/resources/bucket-state-migration-test/full-v1/bucket/test-bucket/.part-0-5.inprogress.795d25e1-c9b1-4635-afc1-9334651de3c9
2020-05-17T21:04:39.6067672Z 	at org.apache.flink.core.fs.local.LocalRecoverableFsDataOutputStream.<init>(LocalRecoverableFsDataOutputStream.java:68)
2020-05-17T21:04:39.6068259Z 	at org.apache.flink.core.fs.local.LocalRecoverableWriter.recover(LocalRecoverableWriter.java:65)
2020-05-17T21:04:39.6068843Z 	at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.restoreInProgressFile(Bucket.java:144)
2020-05-17T21:04:39.6069509Z 	at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.<init>(Bucket.java:131)
2020-05-17T21:04:39.6070040Z 	at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.restore(Bucket.java:414)
2020-05-17T21:04:39.6070661Z 	at org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest.restoreBucket(BucketStateSerializerTest.java:332)
2020-05-17T21:04:39.6071464Z 	at org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest.testDeserializationFull(BucketStateSerializerTest.java:284)
2020-05-17T21:04:39.6072214Z 	at org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest.testDeserializationFull(BucketStateSerializerTest.java:255)
2020-05-17T21:04:39.6072771Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-17T21:04:39.6073182Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-17T21:04:39.6073685Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-17T21:04:39.6074113Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-17T21:04:39.6074612Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-17T21:04:39.6075125Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-17T21:04:39.6075616Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-17T21:04:39.6076116Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-17T21:04:39.6076557Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-17T21:04:39.6077015Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-17T21:04:39.6077568Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-17T21:04:39.6078025Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-17T21:04:39.6078447Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-17T21:04:39.6078860Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-17T21:04:39.6079340Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-17T21:04:39.6079827Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-17T21:04:39.6080253Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-17T21:04:39.6080624Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-17T21:04:39.6080991Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-17T21:04:39.6081373Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-17T21:04:39.6081774Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-17T21:04:39.6082202Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-17T21:04:39.6082612Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-17T21:04:39.6083038Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-17T21:04:39.6083460Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-17T21:04:39.6083880Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-17T21:04:39.6084322Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-17T21:04:39.6084762Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-17T21:04:39.6085281Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-17T21:04:39.6085796Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-17T21:04:39.6086306Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-17T21:04:39.6086847Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-17T21:04:39.6087379Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-17T21:04:39.6087879Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-17T21:04:39.6088338Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-17T21:04:39.6088619Z 
2020-05-17T21:04:39.6089176Z [ERROR] testDeserializationFullNoInProgress[Previous Version = 1](org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest)  Time elapsed: 0.003 s  <<< FAILURE!
2020-05-17T21:04:39.6090390Z org.junit.ComparisonFailure: expected:</[__w/2/s]/flink-streaming-jav...> but was:</[Users/aljoscha/Dev/flink]/flink-streaming-jav...>
2020-05-17T21:04:39.6090912Z 	at org.junit.Assert.assertEquals(Assert.java:115)
2020-05-17T21:04:39.6091261Z 	at org.junit.Assert.assertEquals(Assert.java:144)
2020-05-17T21:04:39.6091807Z 	at org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest.testDeserializationFull(BucketStateSerializerTest.java:289)
2020-05-17T21:04:39.6092564Z 	at org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest.testDeserializationFullNoInProgress(BucketStateSerializerTest.java:260)
2020-05-17T21:04:39.6093153Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-17T21:04:39.6093573Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-17T21:04:39.6094065Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-17T21:04:39.6094596Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-17T21:04:39.6095021Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-17T21:04:39.6095536Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-17T21:04:39.6096127Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-17T21:04:39.6096610Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-17T21:04:39.6097071Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-17T21:04:39.6097511Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-17T21:04:39.6098009Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-17T21:04:39.6098468Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-17T21:04:39.6098869Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-17T21:04:39.6099533Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-17T21:04:39.6100040Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-17T21:04:39.6100456Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-17T21:04:39.6100848Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-17T21:04:39.6101217Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-17T21:04:39.6101571Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-17T21:04:39.6101925Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-17T21:04:39.6102328Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-17T21:04:39.6102727Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-17T21:04:39.6103148Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-17T21:04:39.6103543Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-17T21:04:39.6103967Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-17T21:04:39.6104423Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-17T21:04:39.6104785Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-17T21:04:39.6105220Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-17T21:04:39.6105702Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-17T21:04:39.6106221Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-17T21:04:39.6106708Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-17T21:04:39.6107209Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-17T21:04:39.6107831Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-17T21:04:39.6108298Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-17T21:04:39.6108764Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 06:49:21 UTC 2020,,,,,,,,,,"0|z0eun4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 06:15;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1651&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d;;;","18/May/20 06:29;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1654&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d;;;","18/May/20 06:48;rmetzger;This was probably resolved through https://github.com/apache/flink/commit/f33819b95f8d0eef5dd6a00c86b5cff6aea24e87;;;","18/May/20 06:49;rmetzger;Yep: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1655&view=results;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot switch dialect in SQL CLI,FLINK-17786,13305579,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,18/May/20 03:22,16/Oct/20 10:50,13/Jul/23 08:07,19/May/20 07:03,,,,,,1.11.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"In {{ExecutionContext::initializeTableEnvironment}} we set dialect with the ExecutionEntry which can override the config option users have set. Moreover, since the dialect is already controlled by a config option, it doesn't make sense to have another config in ExecutionEntry.",,felixzheng,libenchao,lirui,lzljs3620320,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 07:03:47 UTC 2020,,,,,,,,,,"0|z0euk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/20 06:36;lzljs3620320;Why not work? [~lirui] Can you explain this in description?;;;","19/May/20 07:03;lzljs3620320;master: ce0f334713a4d994183cc5f415fcf8896cd793fc

release-1.11: 90982ca1a9595bdadfb3433fbfdbabe6097e254d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Mesos Jepsen Tests pass with Hadoop-free Flink ,FLINK-17777,13305511,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gjy,gjy,gjy,17/May/20 16:07,16/Oct/20 10:48,13/Jul/23 08:07,18/May/20 15:08,1.11.0,,,,,1.11.0,,,,Tests,,,,,0,pull-request-available,,,,"Since FLINK-11086, we can no longer build a Flink distribution with Hadoop. Therefore, we need to set the {{HADOOP_CLASSPATH}} environment variable for the TM processes.",,gjy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 15:08:38 UTC 2020,,,,,,,,,,"0|z0eu4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 15:08;gjy;1.11: f46735cb4963af616c0e8538331bed8739a1d353
master: 81ffe8a271a3d4bf7867f7b8b75ffc4cc6707d85;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_pandas_udf.py: NoClassDefFoundError RowDataArrowPythonScalarFunctionRunner,FLINK-17772,13305463,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,rmetzger,rmetzger,17/May/20 09:03,16/Oct/20 10:34,13/Jul/23 08:07,17/May/20 11:36,1.11.0,,,,,1.11.0,,,,API / Python,Tests,,,,0,pull-request-available,test-stability,,,"Java 11 nightly profile: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1579&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=9739ebd8-9cbd-5d3f-d48a-1fac792a8679

{code}
2020-05-16T23:12:27.0921553Z pyflink/table/tests/test_pandas_udf.py:63: 
2020-05-16T23:12:27.0921999Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-05-16T23:12:27.0922424Z pyflink/table/table_environment.py:1049: in execute
2020-05-16T23:12:27.0923081Z     return JobExecutionResult(self._j_tenv.execute(job_name))
2020-05-16T23:12:27.0923876Z .tox/py35-cython/lib/python3.5/site-packages/py4j/java_gateway.py:1286: in __call__
2020-05-16T23:12:27.0924419Z     answer, self.gateway_client, self.target_id, self.name)
2020-05-16T23:12:27.0924800Z pyflink/util/exceptions.py:147: in deco
2020-05-16T23:12:27.0925086Z     return f(*a, **kw)
2020-05-16T23:12:27.0925662Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-05-16T23:12:27.0926095Z 
2020-05-16T23:12:27.0926604Z answer = 'xro11689'
2020-05-16T23:12:27.0927067Z gateway_client = <py4j.java_gateway.GatewayClient object at 0x7f3788994c18>
2020-05-16T23:12:27.0927746Z target_id = 'o11627', name = 'execute'
2020-05-16T23:12:27.0927931Z 
2020-05-16T23:12:27.0929789Z     def get_return_value(answer, gateway_client, target_id=None, name=None):
2020-05-16T23:12:27.0930237Z         """"""Converts an answer received from the Java gateway into a Python object.
2020-05-16T23:12:27.0930505Z     
2020-05-16T23:12:27.0931277Z         For example, string representation of integers are converted to Python
2020-05-16T23:12:27.0931748Z         integer, string representation of objects are converted to JavaObject
2020-05-16T23:12:27.0932173Z         instances, etc.
2020-05-16T23:12:27.0932449Z     
2020-05-16T23:12:27.0932773Z         :param answer: the string returned by the Java gateway
2020-05-16T23:12:27.0933272Z         :param gateway_client: the gateway client used to communicate with the Java
2020-05-16T23:12:27.0933820Z             Gateway. Only necessary if the answer is a reference (e.g., object,
2020-05-16T23:12:27.0934255Z             list, map)
2020-05-16T23:12:27.0934677Z         :param target_id: the name of the object from which the answer comes from
2020-05-16T23:12:27.0935187Z             (e.g., *object1* in `object1.hello()`). Optional.
2020-05-16T23:12:27.0935692Z         :param name: the name of the member from which the answer comes from
2020-05-16T23:12:27.0936344Z             (e.g., *hello* in `object1.hello()`). Optional.
2020-05-16T23:12:27.0936614Z         """"""
2020-05-16T23:12:27.0936840Z         if is_error(answer)[0]:
2020-05-16T23:12:27.0937186Z             if len(answer) > 1:
2020-05-16T23:12:27.0937696Z                 type = answer[1]
2020-05-16T23:12:27.0938164Z                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2020-05-16T23:12:27.0938688Z                 if answer[1] == REFERENCE_TYPE:
2020-05-16T23:12:27.0939177Z                     raise Py4JJavaError(
2020-05-16T23:12:27.0939530Z                         ""An error occurred while calling {0}{1}{2}.\n"".
2020-05-16T23:12:27.0939943Z >                       format(target_id, ""."", name), value)
2020-05-16T23:12:27.0940706Z E                   py4j.protocol.Py4JJavaError: An error occurred while calling o11627.execute.
2020-05-16T23:12:27.0941428Z E                   : java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-16T23:12:27.0942239Z E                   	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
2020-05-16T23:12:27.0942936Z E                   	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
2020-05-16T23:12:27.0943688Z E                   	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1665)
2020-05-16T23:12:27.0944563Z E                   	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:74)
2020-05-16T23:12:27.0945520Z E                   	at org.apache.flink.table.planner.delegation.ExecutorBase.execute(ExecutorBase.java:52)
2020-05-16T23:12:27.0946337Z E                   	at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1088)
2020-05-16T23:12:27.0947024Z E                   	at jdk.internal.reflect.GeneratedMethodAccessor164.invoke(Unknown Source)
2020-05-16T23:12:27.0947887Z E                   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-16T23:12:27.0948609Z E                   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2020-05-16T23:12:27.0949382Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2020-05-16T23:12:27.0950131Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2020-05-16T23:12:27.0950905Z E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
2020-05-16T23:12:27.0951617Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2020-05-16T23:12:27.0952420Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
2020-05-16T23:12:27.0953156Z E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2020-05-16T23:12:27.0953800Z E                   	at java.base/java.lang.Thread.run(Thread.java:834)
2020-05-16T23:12:27.0954407Z E                   Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-16T23:12:27.0955017Z E                   	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-05-16T23:12:27.0955942Z E                   	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.lambda$getJobExecutionResult$2(PerJobMiniClusterFactory.java:186)
2020-05-16T23:12:27.0956876Z E                   	at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
2020-05-16T23:12:27.0957600Z E                   	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
2020-05-16T23:12:27.0958233Z E                   	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
2020-05-16T23:12:27.0958955Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
2020-05-16T23:12:27.0959665Z E                   	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
2020-05-16T23:12:27.0960453Z E                   	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
2020-05-16T23:12:27.0961042Z E                   	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
2020-05-16T23:12:27.0961782Z E                   	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
2020-05-16T23:12:27.0962451Z E                   	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:890)
2020-05-16T23:12:27.0963100Z E                   	at akka.dispatch.OnComplete.internal(Future.scala:264)
2020-05-16T23:12:27.0963693Z E                   	at akka.dispatch.OnComplete.internal(Future.scala:261)
2020-05-16T23:12:27.0964313Z E                   	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2020-05-16T23:12:27.0964894Z E                   	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2020-05-16T23:12:27.0965542Z E                   	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-05-16T23:12:27.0966301Z E                   	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
2020-05-16T23:12:27.0967132Z E                   	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2020-05-16T23:12:27.0967721Z E                   	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2020-05-16T23:12:27.0968205Z E                   	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2020-05-16T23:12:27.0968876Z E                   	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2020-05-16T23:12:27.0969445Z E                   	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2020-05-16T23:12:27.0973679Z E                   	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2020-05-16T23:12:27.0974397Z E                   	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2020-05-16T23:12:27.0974869Z E                   	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-05-16T23:12:27.0975572Z E                   	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-05-16T23:12:27.0976426Z E                   	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2020-05-16T23:12:27.0977267Z E                   	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-05-16T23:12:27.0978062Z E                   	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-05-16T23:12:27.0978724Z E                   	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2020-05-16T23:12:27.0979414Z E                   	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2020-05-16T23:12:27.0980049Z E                   	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-05-16T23:12:27.0980751Z E                   	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-05-16T23:12:27.0981450Z E                   	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-05-16T23:12:27.0982089Z E                   	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-05-16T23:12:27.0982783Z E                   	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-05-16T23:12:27.0983437Z E                   	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-05-16T23:12:27.0984215Z E                   Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-05-16T23:12:27.0985010Z E                   	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-05-16T23:12:27.0985974Z E                   	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-05-16T23:12:27.0986939Z E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)
2020-05-16T23:12:27.0987857Z E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)
2020-05-16T23:12:27.0988704Z E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)
2020-05-16T23:12:27.0989644Z E                   	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:501)
2020-05-16T23:12:27.0990412Z E                   	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:386)
2020-05-16T23:12:27.0991030Z E                   	at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
2020-05-16T23:12:27.0991961Z E                   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-16T23:12:27.0992658Z E                   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2020-05-16T23:12:27.0993343Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-05-16T23:12:27.0994263Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-05-16T23:12:27.0995051Z E                   	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-05-16T23:12:27.0995836Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-05-16T23:12:27.0996505Z E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-05-16T23:12:27.0997070Z E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-05-16T23:12:27.0997785Z E                   	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-05-16T23:12:27.0998383Z E                   	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-05-16T23:12:27.0999019Z E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-05-16T23:12:27.0999619Z E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-16T23:12:27.1000272Z E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-16T23:12:27.1000832Z E                   	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-05-16T23:12:27.1001328Z E                   	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-05-16T23:12:27.1001932Z E                   	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-05-16T23:12:27.1002448Z E                   	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-05-16T23:12:27.1002972Z E                   	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-05-16T23:12:27.1003550Z E                   	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-05-16T23:12:27.1004071Z E                   	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-05-16T23:12:27.1004442Z E                   	... 4 more
2020-05-16T23:12:27.1005025Z E                   Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.flink.table.runtime.runners.python.scalar.arrow.RowDataArrowPythonScalarFunctionRunner
2020-05-16T23:12:27.1006197Z E                   	at org.apache.flink.table.runtime.operators.python.scalar.arrow.RowDataArrowPythonScalarFunctionOperator.createPythonFunctionRunner(RowDataArrowPythonScalarFunctionOperator.java:98)
2020-05-16T23:12:27.1007310Z E                   	at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.createPythonFunctionRunner(AbstractStatelessFunctionOperator.java:149)
2020-05-16T23:12:27.1008468Z E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:141)
2020-05-16T23:12:27.1009335Z E                   	at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.open(AbstractStatelessFunctionOperator.java:131)
2020-05-16T23:12:27.1010068Z E                   	at org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.open(AbstractPythonScalarFunctionOperator.java:88)
2020-05-16T23:12:27.1010843Z E                   	at org.apache.flink.table.runtime.operators.python.scalar.AbstractRowDataPythonScalarFunctionOperator.open(AbstractRowDataPythonScalarFunctionOperator.java:80)
2020-05-16T23:12:27.1011627Z E                   	at org.apache.flink.table.runtime.operators.python.scalar.arrow.RowDataArrowPythonScalarFunctionOperator.open(RowDataArrowPythonScalarFunctionOperator.java:78)
2020-05-16T23:12:27.1012463Z E                   	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:289)
2020-05-16T23:12:27.1013071Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:469)
2020-05-16T23:12:27.1013739Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
2020-05-16T23:12:27.1014473Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:465)
2020-05-16T23:12:27.1015020Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:518)
2020-05-16T23:12:27.1015526Z E                   	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713)
2020-05-16T23:12:27.1016069Z E                   	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539)
2020-05-16T23:12:27.1016484Z E                   	at java.base/java.lang.Thread.run(Thread.java:834)
2020-05-16T23:12:27.1016721Z 
2020-05-16T23:12:27.1017622Z .tox/py35-cython/lib/python3.5/site-packages/py4j/protocol.py:328: Py4JJavaError

[...]

2020-05-16T23:19:04.5010620Z ___________________________________ summary ____________________________________
2020-05-16T23:19:04.5011582Z ERROR:   py35-cython: commands failed
2020-05-16T23:19:04.5012036Z ERROR:   py36-cython: commands failed
2020-05-16T23:19:04.5012455Z ERROR:   py37-cython: commands failed
2020-05-16T23:19:04.5353287Z ============tox checks... [FAILED]============
2020-05-16T23:19:04.5371085Z PYTHON exited with EXIT CODE: 1.
{code}
",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 17 11:36:06 UTC 2020,,,,,,,,,,"0|z0etu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/20 09:37;dian.fu; Thanks [~rmetzger] for reporting this issue. I will take a look.;;;","17/May/20 11:36;dian.fu;master: 6a58ce0a0c14f64e3415480c7427eb50509d44c8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""PyFlink end-to-end test"" fails with ""The output result: [] is not as expected: [2, 3, 4]!"" on Java11",FLINK-17771,13305461,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,rmetzger,rmetzger,17/May/20 08:58,16/Oct/20 10:53,13/Jul/23 08:07,22/May/20 02:59,1.11.0,,,,,1.11.0,,,,API / Python,Tests,,,,0,pull-request-available,test-stability,,,"Java 11 nightly profile: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1579&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=679407b1-ea2c-5965-2c8d-1467777fff88
{code}
Job has been submitted with JobID ef78030becb3bfd6415d3de2e06420b4
java.lang.AssertionError: The output result: [] is not as expected: [2, 3, 4]!
	at org.apache.flink.python.tests.FlinkStreamPythonUdfSqlJob.main(FlinkStreamPythonUdfSqlJob.java:55)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:148)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:689)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:227)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:906)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:982)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:982)
Stopping taskexecutor daemon (pid: 2705) on host fv-az670.

{code}",,dian.fu,rmetzger,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/20 12:09;zhongwei;image-2020-05-21-20-11-07-626.png;https://issues.apache.org/jira/secure/attachment/13003644/image-2020-05-21-20-11-07-626.png","21/May/20 12:10;zhongwei;image-2020-05-21-20-11-29-389.png;https://issues.apache.org/jira/secure/attachment/13003645/image-2020-05-21-20-11-29-389.png","21/May/20 12:10;zhongwei;image-2020-05-21-20-11-48-220.png;https://issues.apache.org/jira/secure/attachment/13003646/image-2020-05-21-20-11-48-220.png","21/May/20 12:10;zhongwei;image-2020-05-21-20-12-16-889.png;https://issues.apache.org/jira/secure/attachment/13003647/image-2020-05-21-20-12-16-889.png",,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 02:59:39 UTC 2020,,,,,,,,,,"0|z0etts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/20 14:06;dian.fu;cc [~zhongwei];;;","17/May/20 14:09;dian.fu;When I tried to reproduce this problem locally with JDK 11, it will throw exceptions as following:
{code}
Exception in thread ""grpc-default-executor-0"" java.lang.OutOfMemoryError: Direct buffer memory
	at java.base/java.nio.Bits.reserveMemory(Bits.java:175)
	at java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118)
	at java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317)
	at org.apache.beam.vendor.grpc.v1p21p0.io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:777)
	at org.apache.beam.vendor.grpc.v1p21p0.io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:753)
	at org.apache.beam.vendor.grpc.v1p21p0.io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:250)
	at org.apache.beam.vendor.grpc.v1p21p0.io.netty.buffer.PoolArena.allocate(PoolArena.java:220)
	at org.apache.beam.vendor.grpc.v1p21p0.io.netty.buffer.PoolArena.allocate(PoolArena.java:152)
	at org.apache.beam.vendor.grpc.v1p21p0.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:332)
	at org.apache.beam.vendor.grpc.v1p21p0.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
	at org.apache.beam.vendor.grpc.v1p21p0.io.netty.buffer.AbstractByteBufAllocator.buffer(AbstractByteBufAllocator.java:123)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.netty.NettyWritableBufferAllocator.allocate(NettyWritableBufferAllocator.java:51)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.MessageFramer.writeKnownLengthUncompressed(MessageFramer.java:226)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.MessageFramer.writeUncompressed(MessageFramer.java:167)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.MessageFramer.writePayload(MessageFramer.java:140)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.AbstractStream.writeMessage(AbstractStream.java:53)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.ServerCallImpl.sendMessageInternal(ServerCallImpl.java:163)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.ServerCallImpl.sendMessage(ServerCallImpl.java:145)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.stub.ServerCalls$ServerCallStreamObserverImpl.onNext(ServerCalls.java:349)
	at org.apache.beam.runners.fnexecution.provisioning.StaticGrpcProvisionService.getProvisionInfo(StaticGrpcProvisionService.java:48)
	at org.apache.beam.model.fnexecution.v1.ProvisionServiceGrpc$MethodHandlers.invoke(ProvisionServiceGrpc.java:246)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.Contexts$ContextualizedServerCallListener.onHalfClose(Contexts.java:86)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:322)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:762)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.beam.vendor.grpc.v1p21p0.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
{code}

I guess this is the reason why the result is empty as it encounters OutOfMemoryError during running. (It could succeed when switching to JDK 8.);;;","21/May/20 12:18;zhongwei;[~dian.fu] I have investigated the issue for several days. The empty result is indeed caused by the OOM. But the root cause is the memory leaking of the ChildFirstClassLoader of the PyFlink Task. Currently I have found 5 places which caused the memory leaking via the heapdump file:
 # The context class loader of the thread pools named ""flink-file-cache-xxx"".
 # The soft reference from the field ""reflectors"" and ""localDescs"" of the class ""ObjectStreamClass$Caches"". The OOM is cause by the DirectMemory which would not trigger the GC of the soft reference, so we need to consider the soft references here.
 # The context class loader of the process reaper thread.
 # The ""classLoader"" field of the ""org.codehaus.janino.ClassLoaderIClassLoader"" objects.
 # The soft reference of the object ""org.apache.flink.table.runtime.generated.COMPILED_CACHE"".

This is the screenshot:

!image-2020-05-21-20-11-07-626.png|width=754,height=151!

!image-2020-05-21-20-11-29-389.png|width=754,height=74!

!image-2020-05-21-20-11-48-220.png|width=744,height=157!

!image-2020-05-21-20-12-16-889.png|width=743,height=274!

 

Note that the issue also exists on jDK8. When running in JDK8, the shaded Netty in Beam uses the method ""Unsafe.allocateMemory()"" to allocate the Direct Memory instead of the method ""ByteBuffer.allocateDirect()"", which won't be limited by the param ""MaxDirectMemorySize"". So the OOM won't happen in JDK8 but the process memory is still increasing.;;;","21/May/20 12:26;zhongwei;To solve the memory leak issue of the ChildFirstClassLoader completely needs further effort and discussion. Before that we can fix the test case temporarily via adjusting the limit of the off-heap memory. What do you think? [~dian.fu];;;","22/May/20 02:59;dian.fu;Merged via :
master: 8b14cd807d165052da46df2fc0d9536eadc97fe7
release-1.11: c7243c001ba632f412add975a26fe3ae1caff7b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong order of log events on a task failure,FLINK-17769,13305454,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ym,rmetzger,rmetzger,17/May/20 07:04,22/Jun/20 15:46,13/Jul/23 08:07,22/Jun/20 15:44,,,,,,1.12.0,,,,Runtime / Task,,,,,0,pull-request-available,,,,"In this example, errors from the {{close()}} method call are logged before the {{switched from RUNNING to FAILED}} log line with the actual exception (which is confusing, because the exceptions coming from {{close()}} could be considered as the failure root cause, because they are first in the log)

{code}
2020-05-14 10:12:42,660 INFO  org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer [] - Started Kinesis producer instance for region 'eu-central-1'
2020-05-14 10:12:42,660 DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for StreamSource_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) with empty state.
2020-05-14 10:12:42,823 INFO  org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer [] - Closing producer
2020-05-14 10:12:42,823 INFO  org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer [] - Flushing outstanding 2 records
2020-05-14 10:12:42,826 ERROR org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Error during disposal of stream operator.
org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.producer.DaemonException: The child process has been shutdown and can no longer accept messages.
2020-05-14 10:12:42,834 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom Source -> Sink: Unnamed (1/1) (4a49aea047aeb3e67cf79c788df0e558) switched from RUNNING to FAILED.
{code}",,guoyangze,pnowojski,rmetzger,roman,wind_ljy,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 15:44:53 UTC 2020,,,,,,,,,,"0|z0ets8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 06:18;ym;Originally, I was thinking this problem is introduced by FLINK-17663, but the problem seems to be there before the change.

The problem is caused in Exception handling of  

 
{code:java}
StreamTask.disposeAllOperators(true);
  |- StreamTask.cleanUpInvoke
    |- StreamTask.invoke
{code}
 

In ``disposeAllOperators(true)'', ""true"" means logging error only without throwing disposal exceptions. That's why the error during disposal is printed before the real root cause.

I agree these are confusing, here are a couple of ways I am considering to solve this problem:

1. Print the invoke error before ``cleanUpInvoke()'', as shown below; The problem is it is likely the same error can be printed twice.
{code:java}
catch (Exception invokeException) {
 LOG.error(""Invoke Error"", invokeException);
 try {
 cleanUpInvoke();
 }
 catch (Throwable cleanUpException) {
 throw (Exception) ExceptionUtils.firstOrSuppressed(cleanUpException, invokeException);
 }
 throw invokeException;
}{code}
 

2. use disposeAllOperators(false) instead, that is throwing the disposal exception as well. Is there any specific reason why disposal error is expected to be swallowed in this case?

 

3. Be specific in the log of disposeAllOperators(true) that ""this is not the real root cause, is a caused by some other errors"".

 

 ;;;","05/Jun/20 06:30;ym;[~roman_khachatryan] [~pnowojski]

Please let me know what do you think.;;;","05/Jun/20 07:55;pnowojski;1.
I think as you wrote option is not a good one because of duplicated logging problems.
2.
problem will be that we do not clean up all of the resources. In this method we are supposed to clean up everything that we can, regardless of the errors.
3.
Maybe if there was no other way.

One remark, I think the problem might a bit more common. There are other places that are logging errors in {{cleanUpInvoke}}. 

What about an option 4. Remember the first exception and suppress the later ones similar how {{TaskExecutor#stopTaskExecutorServices}} is doing for example? Keep in mind that an exception thrown from {{cleanUpInvoke}} is already subject to a similar logic in {{StreamTask#invoke}} if {{runMailboxLoop}} or {{afterInvoke}} has thrown some exception. So if there was a previous exception thrown during normal execution, an error thrown from {{cleanUpInvoke}} would be suppressed.
;;;","05/Jun/20 08:05;roman;Thanks for you analysis [~ym].

 

I think we could also collect disposal errors without logging on TM and send them as part of failure reason to JM. It would simplify some things. But I'm thinking now that for large deployments it's impractical.

I think we should log the reason before closing operators (something like the 1st option). I'd also increase log level in afterInvoke for ""Finished task"".

 

I also see this simple bug:
{code:java}
LOG.warn(""{} ({}) switched from {} to {}."", taskNameWithSubtask, executionId, currentState, newState, cause); LOG.warn(""{} ({}) switched from {} to {}."", taskNameWithSubtask, executionId, currentState, newState, cause);{code}
 
The last line is intended to also print the cause.

 

Maybe if we print the reason before closing operators, we should fix this LOG.warn and replace cause with cause.getMessage.;;;","05/Jun/20 08:14;pnowojski;{quote}
I think we could also collect disposal errors without logging on TM and send them as part of failure reason to JM. It would simplify some things. But I'm thinking now that for large deployments it's impractical.
{quote}
This is a tough call. If we add the secondary failures from disposal as suppressed exceptions, we are risking braking some things - like end to end tests that are filtering what are expected/allowed error messages. Also we could pollute the JM logs more. On the other hand, some of those failures might be important indicating resource leaks or other problems.;;;","05/Jun/20 09:13;ym;[~pnowojski], Yes, I like option 4, definitely a better and simple solution!

 

[~roman_khachatryan], yep, that's a good catch, I will fix that as well.;;;","05/Jun/20 09:53;roman;How would (4) handle cases when cleanUpInvoke is called NOT from the catch block - when there is no invokeException?

If we throw disposeException when invokeException is null then it will be even less clear in logs.

If we preserve current behavior (only log) when invokeException is null then it will not solve the original issue.

Right?

So I think we have to log the reason of stopping in the very beginning (in addition to adding disposeExceptions as suppressed).;;;","08/Jun/20 06:24;ym;“If we throw disposeException when invokeException is null then it will be even less clear in logs.”

 

Hmm, I was wondering why? If invokeException is null, it means invoke is successful. If anything wrong occurs during `cleanUpInvoke`, disposeException should be expected to be exposed?

 

Let me write a version of the option 4 to see whether we mean the same thing.;;;","08/Jun/20 07:51;roman;Good idea :);;;","22/Jun/20 15:44;pnowojski;Merged to master as ec4d155101. 

I'm not very keen of backporting it to 1.11, as it can in the past error handling logic was ending up more tricky and unstable then expected, so I would prefer to have at least couple of weeks for this to stabilise on the master branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointITCase.shouldPerformUnalignedCheckpointOnLocalAndRemoteChannel is instable,FLINK-17768,13305439,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zjwang,dian.fu,dian.fu,17/May/20 02:31,07/Sep/20 07:28,13/Jul/23 08:07,18/Jun/20 09:30,1.11.0,,,,,1.11.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"UnalignedCheckpointITCase.shouldPerformUnalignedCheckpointOnLocalAndRemoteChannel and shouldPerformUnalignedCheckpointOnParallelRemoteChannel failed in azure:
{code}
2020-05-16T12:41:32.3546620Z [ERROR] shouldPerformUnalignedCheckpointOnLocalAndRemoteChannel(org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 18.865 s  <<< ERROR!
2020-05-16T12:41:32.3548739Z java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-16T12:41:32.3550177Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-05-16T12:41:32.3551416Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-05-16T12:41:32.3552959Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1665)
2020-05-16T12:41:32.3554979Z 	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:74)
2020-05-16T12:41:32.3556584Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1645)
2020-05-16T12:41:32.3558068Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1627)
2020-05-16T12:41:32.3559431Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.execute(UnalignedCheckpointITCase.java:158)
2020-05-16T12:41:32.3560954Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.shouldPerformUnalignedCheckpointOnLocalAndRemoteChannel(UnalignedCheckpointITCase.java:145)
2020-05-16T12:41:32.3562203Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-16T12:41:32.3563433Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-16T12:41:32.3564846Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-16T12:41:32.3565894Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-16T12:41:32.3566870Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-16T12:41:32.3568064Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-16T12:41:32.3569727Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-16T12:41:32.3570818Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-16T12:41:32.3571840Z 	at org.junit.rules.Verifier$1.evaluate(Verifier.java:35)
2020-05-16T12:41:32.3572771Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-16T12:41:32.3574008Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2020-05-16T12:41:32.3575406Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2020-05-16T12:41:32.3576476Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-05-16T12:41:32.3577253Z 	at java.lang.Thread.run(Thread.java:748)
2020-05-16T12:41:32.3578228Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-16T12:41:32.3579520Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-05-16T12:41:32.3580935Z 	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.lambda$getJobExecutionResult$2(PerJobMiniClusterFactory.java:186)
2020-05-16T12:41:32.3582361Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-05-16T12:41:32.3583456Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-05-16T12:41:32.3584816Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-16T12:41:32.3585874Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-05-16T12:41:32.3587059Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
2020-05-16T12:41:32.3588572Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-05-16T12:41:32.3589733Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-05-16T12:41:32.3590860Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-16T12:41:32.3591956Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-05-16T12:41:32.3593042Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:890)
2020-05-16T12:41:32.3594105Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2020-05-16T12:41:32.3595084Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2020-05-16T12:41:32.3595937Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2020-05-16T12:41:32.3596828Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2020-05-16T12:41:32.3597800Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-05-16T12:41:32.3598856Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
2020-05-16T12:41:32.3600084Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2020-05-16T12:41:32.3601108Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2020-05-16T12:41:32.3602249Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2020-05-16T12:41:32.3603396Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2020-05-16T12:41:32.3605032Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2020-05-16T12:41:32.3606307Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2020-05-16T12:41:32.3607287Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2020-05-16T12:41:32.3608294Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-05-16T12:41:32.3609322Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-05-16T12:41:32.3610521Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2020-05-16T12:41:32.3611745Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-05-16T12:41:32.3612950Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-05-16T12:41:32.3614288Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2020-05-16T12:41:32.3615488Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2020-05-16T12:41:32.3616491Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-05-16T12:41:32.3617683Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-05-16T12:41:32.3618815Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-05-16T12:41:32.3619806Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-05-16T12:41:32.3621104Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-05-16T12:41:32.3622154Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-05-16T12:41:32.3623458Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=100)
2020-05-16T12:41:32.3625232Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:112)
2020-05-16T12:41:32.3626779Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-05-16T12:41:32.3628274Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:189)
2020-05-16T12:41:32.3629528Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:183)
2020-05-16T12:41:32.3630831Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:177)
2020-05-16T12:41:32.3632245Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:505)
2020-05-16T12:41:32.3633438Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:386)
2020-05-16T12:41:32.3634641Z 	at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
2020-05-16T12:41:32.3635628Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-16T12:41:32.3636595Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-16T12:41:32.3637729Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-05-16T12:41:32.3638973Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-05-16T12:41:32.3640199Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-05-16T12:41:32.3641385Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-05-16T12:41:32.3642395Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-05-16T12:41:32.3643322Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-05-16T12:41:32.3644357Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-05-16T12:41:32.3645466Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-05-16T12:41:32.3646487Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-05-16T12:41:32.3647729Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-16T12:41:32.3648715Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-16T12:41:32.3649611Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-05-16T12:41:32.3650511Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-05-16T12:41:32.3651410Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-05-16T12:41:32.3652257Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-05-16T12:41:32.3653213Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-05-16T12:41:32.3654166Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-05-16T12:41:32.3655121Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-05-16T12:41:32.3655842Z 	... 4 more
2020-05-16T12:41:32.3656578Z Caused by: java.io.IOException: Could not perform checkpoint 20 for operator Map (1/5).
2020-05-16T12:41:32.3657800Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:857)
2020-05-16T12:41:32.3659129Z 	at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:107)
2020-05-16T12:41:32.3660674Z 	at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner$ThreadSafeUnaligner.lambda$notifyBarrierReceived$0(CheckpointBarrierUnaligner.java:309)
2020-05-16T12:41:32.3662369Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
2020-05-16T12:41:32.3663713Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
2020-05-16T12:41:32.3665348Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:285)
2020-05-16T12:41:32.3666971Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:205)
2020-05-16T12:41:32.3668665Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:196)
2020-05-16T12:41:32.3670150Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:553)
2020-05-16T12:41:32.3671277Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:526)
2020-05-16T12:41:32.3672268Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713)
2020-05-16T12:41:32.3673197Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539)
2020-05-16T12:41:32.3674129Z 	at java.lang.Thread.run(Thread.java:748)
2020-05-16T12:41:32.3675215Z Caused by: java.lang.IllegalArgumentException: channel state write result not found for checkpoint id 20
2020-05-16T12:41:32.3676320Z 	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:139)
2020-05-16T12:41:32.3677583Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.getWriteResult(ChannelStateWriterImpl.java:143)
2020-05-16T12:41:32.3679064Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:266)
2020-05-16T12:41:32.3680598Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:164)
2020-05-16T12:41:32.3682003Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:886)
2020-05-16T12:41:32.3683320Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
2020-05-16T12:41:32.3684798Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:876)
2020-05-16T12:41:32.3686020Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:844)
2020-05-16T12:41:32.3686848Z 	... 12 more
2020-05-16T12:41:32.3687161Z 
2020-05-16T12:41:32.3688455Z [ERROR] shouldPerformUnalignedCheckpointOnParallelRemoteChannel(org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 9.535 s  <<< ERROR!
2020-05-16T12:41:32.3690013Z java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-16T12:41:32.3691222Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-05-16T12:41:32.3692239Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-05-16T12:41:32.3693445Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1665)
2020-05-16T12:41:32.3695048Z 	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:74)
2020-05-16T12:41:32.3696464Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1645)
2020-05-16T12:41:32.3698062Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1627)
2020-05-16T12:41:32.3699413Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.execute(UnalignedCheckpointITCase.java:158)
2020-05-16T12:41:32.3701166Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.shouldPerformUnalignedCheckpointOnParallelRemoteChannel(UnalignedCheckpointITCase.java:140)
2020-05-16T12:41:32.3702419Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-16T12:41:32.3703350Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-16T12:41:32.3704969Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-16T12:41:32.3706119Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-16T12:41:32.3707211Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-16T12:41:32.3708559Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-16T12:41:32.3709677Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-16T12:41:32.3710773Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-16T12:41:32.3711753Z 	at org.junit.rules.Verifier$1.evaluate(Verifier.java:35)
2020-05-16T12:41:32.3712785Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-16T12:41:32.3714049Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2020-05-16T12:41:32.3715748Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2020-05-16T12:41:32.3717001Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-05-16T12:41:32.3717893Z 	at java.lang.Thread.run(Thread.java:748)
2020-05-16T12:41:32.3718795Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-16T12:41:32.3719891Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-05-16T12:41:32.3721412Z 	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.lambda$getJobExecutionResult$2(PerJobMiniClusterFactory.java:186)
2020-05-16T12:41:32.3722853Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-05-16T12:41:32.3724059Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-05-16T12:41:32.3725342Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-16T12:41:32.3726408Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-05-16T12:41:32.3727763Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
2020-05-16T12:41:32.3728991Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-05-16T12:41:32.3730231Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-05-16T12:41:32.3731518Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-16T12:41:32.3732600Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-05-16T12:41:32.3733680Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:890)
2020-05-16T12:41:32.3734977Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2020-05-16T12:41:32.3735847Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2020-05-16T12:41:32.3736701Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2020-05-16T12:41:32.3737668Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2020-05-16T12:41:32.3738572Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-05-16T12:41:32.3739624Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
2020-05-16T12:41:32.3740754Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2020-05-16T12:41:32.3741778Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2020-05-16T12:41:32.3742743Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2020-05-16T12:41:32.3743956Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2020-05-16T12:41:32.3745268Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2020-05-16T12:41:32.3746646Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2020-05-16T12:41:32.3747799Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2020-05-16T12:41:32.3748747Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-05-16T12:41:32.3749750Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-05-16T12:41:32.3751177Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2020-05-16T12:41:32.3752463Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-05-16T12:41:32.3753651Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-05-16T12:41:32.3755004Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2020-05-16T12:41:32.3756026Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2020-05-16T12:41:32.3757013Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-05-16T12:41:32.3758198Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-05-16T12:41:32.3759324Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-05-16T12:41:32.3760334Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-05-16T12:41:32.3761343Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-05-16T12:41:32.3762370Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-05-16T12:41:32.3763673Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=100)
2020-05-16T12:41:32.3765419Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:112)
2020-05-16T12:41:32.3766974Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-05-16T12:41:32.3768440Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:189)
2020-05-16T12:41:32.3769691Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:183)
2020-05-16T12:41:32.3770992Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:177)
2020-05-16T12:41:32.3772437Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:505)
2020-05-16T12:41:32.3773646Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:386)
2020-05-16T12:41:32.3774841Z 	at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
2020-05-16T12:41:32.3775836Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-16T12:41:32.3776796Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-16T12:41:32.3777872Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-05-16T12:41:32.3779026Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-05-16T12:41:32.3780203Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-05-16T12:41:32.3781384Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-05-16T12:41:32.3782447Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-05-16T12:41:32.3783577Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-05-16T12:41:32.3784987Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-05-16T12:41:32.3786104Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-05-16T12:41:32.3787215Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-05-16T12:41:32.3788554Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-16T12:41:32.3789655Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-16T12:41:32.3790672Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-05-16T12:41:32.3791705Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-05-16T12:41:32.3792746Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-05-16T12:41:32.3793686Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-05-16T12:41:32.3794857Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-05-16T12:41:32.3795796Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-05-16T12:41:32.3796678Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-05-16T12:41:32.3797348Z 	... 4 more
2020-05-16T12:41:32.3798370Z Caused by: java.io.IOException: Could not perform checkpoint 18 for operator Sink: Unnamed (4/5).
2020-05-16T12:41:32.3799764Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:857)
2020-05-16T12:41:32.3801253Z 	at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:107)
2020-05-16T12:41:32.3803019Z 	at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner$ThreadSafeUnaligner.lambda$notifyBarrierReceived$0(CheckpointBarrierUnaligner.java:309)
2020-05-16T12:41:32.3804981Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
2020-05-16T12:41:32.3806356Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
2020-05-16T12:41:32.3807744Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:285)
2020-05-16T12:41:32.3809218Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:205)
2020-05-16T12:41:32.3810719Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:196)
2020-05-16T12:41:32.3812119Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:553)
2020-05-16T12:41:32.3813389Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:526)
2020-05-16T12:41:32.3814763Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713)
2020-05-16T12:41:32.3815805Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539)
2020-05-16T12:41:32.3816729Z 	at java.lang.Thread.run(Thread.java:748)
2020-05-16T12:41:32.3818064Z Caused by: java.lang.IllegalArgumentException: channel state write result not found for checkpoint id 18
2020-05-16T12:41:32.3819318Z 	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:139)
2020-05-16T12:41:32.3820670Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.getWriteResult(ChannelStateWriterImpl.java:143)
2020-05-16T12:41:32.3822364Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.finishAndReportAsync(SubtaskCheckpointCoordinatorImpl.java:233)
2020-05-16T12:41:32.3824252Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:165)
2020-05-16T12:41:32.3825917Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:886)
2020-05-16T12:41:32.3827421Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
2020-05-16T12:41:32.3828959Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:876)
2020-05-16T12:41:32.3830320Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:844)
2020-05-16T12:41:32.3831287Z 	... 12 more
{code}
instance: https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/1538/logs/114",,dian.fu,klion26,pnowojski,rmetzger,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17869,,,,,,FLINK-18290,,FLINK-19027,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 09:29:56 UTC 2020,,,,,,,,,,"0|z0etow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/20 02:35;dian.fu;cc [~AHeise];;;","17/May/20 07:08;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1567&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","17/May/20 07:11;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1548&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323
;;;","17/May/20 07:12;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1538&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323
;;;","17/May/20 08:51;rmetzger;upgrading to blocker
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1579&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","17/May/20 08:59;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1579&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=d6363642-ea4a-5c73-7edb-c00d4548b58e;;;","17/May/20 14:00;pnowojski;I think the problem might be because of mishandling of trailing cancelation barriers. When I tried to reproduce the problem, I reproduced the failure for Checkpoint 20, shortly after following log entries were produced:
{noformat}
166935 [Sink: Unnamed (1/5)] WARN  org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner [] - Sink: Unnamed (1/5) (1a16b0a5d4089828a7db77762b2dfa4a): Received cancellation barrier for checkpoint 19 before completing current checkpoint 20. Skipping current checkpoint.
{noformat}
;;;","18/May/20 06:14;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1653&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","18/May/20 06:16;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1653&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","18/May/20 06:23;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1647&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","18/May/20 06:42;pnowojski;Test disabled on master via {{591aebcdd9}};;;","08/Jun/20 10:08;zjwang;There are at-least 5-6 bugs causing this unstable issue, and most of them were already resolved. The last pending ticket https://issues.apache.org/jira/browse/FLINK-17869 would enable this ITCase, so i would close this ticket now.;;;","15/Jun/20 06:41;rmetzger;Test is failing again: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3422&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","15/Jun/20 13:57;pnowojski;I've disabled the test again via 6e54e0ffe4 on release-1.11 and 86b17c2a98 on master.

I'm reducing the priority, as we will no longer wait for unaligned checkpoints to stabilise before the release happen.;;;","18/Jun/20 09:29;pnowojski;Test re-enabled once more as  commit 66192b8 on apache:master and 414a7a1a5e on release-1.11.

The last failures were caused by irrelevant bug FLINK-18290 which has also been fixed on both master and release-1.11;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verbose client error messages,FLINK-17765,13305397,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,16/May/20 16:11,09/Jun/20 16:20,13/Jul/23 08:07,09/Jun/20 11:14,1.10.1,1.11.0,,,,1.11.0,,,,Client / Job Submission,Runtime / Coordination,Runtime / REST,,,0,pull-request-available,,,,"Some client operations if they fail produce very verbose error messages which are hard to decipher for the user. For example, if the job submission fails because the savepoint path does not exist, then the user sees the following:

{code}
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:148)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:689)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:227)
        at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:906)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:982)
        at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:982)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:290)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1766)
        at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:104)
        at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:71)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1645)
        at org.apache.flink.streaming.examples.statemachine.StateMachineExample.main(StateMachineExample.java:142)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)
        ... 8 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1761)
        ... 17 more
Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:366)
        at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870)
        at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:290)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$3(Dispatcher.java:343)
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
        at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)
        ... 6 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
        at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:149)
        at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:84)
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:386)
        at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34)
        ... 7 more
Caused by: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory 'file:///does/not/exist' on file system 'file'.
        at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorage.resolveCheckpointPointer(AbstractFsCheckpointStorage.java:243)
        at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorage.resolveCheckpoint(AbstractFsCheckpointStorage.java:110)
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1300)
        at org.apache.flink.runtime.scheduler.SchedulerBase.tryRestoreExecutionGraphFromSavepoint(SchedulerBase.java:299)
        at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:252)
        at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:228)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:119)
        at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:103)
        at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:284)
        at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:272)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)
        at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:143)
        ... 10 more
End of exception on server side>]
        at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:390)
        at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:374)
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)
        ... 4 more
{code}

We might be able to remove a lot of the clutter by introducing a special exception which represents an exception which should be reported back to the user and which is then not rewrapped as it bubbles up the call stack.",,aljoscha,guoyangze,trohrmann,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18159,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 11:14:57 UTC 2020,,,,,,,,,,"0|z0etfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 11:14;trohrmann;Fixed via

master:
8fb330ad2bf4fa1ca996cd8c09bc620b56908cd4
0864b15ca0f66b87d91e7e5f8712699f1984406a
fa46073222daa2b381626bd529ab9d6d9feb304c

1.11.0:
5a83010ff6be701a0e3bff877e8d1d39b5429f59
e49a853739099b2e705204e7c4a41301d37424f4
3356bab80613f0c5d57e3ce52c2ef2f0eaba8a6d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No log files when starting scala-shell,FLINK-17763,13305392,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,zjffdu,zjffdu,16/May/20 15:31,19/May/20 14:39,13/Jul/23 08:07,19/May/20 14:39,1.10.0,1.9.2,,,,1.10.2,1.11.0,1.9.4,,Scala Shell,,,,,0,,,,,"I see the following error when starting scala shell.

 
{code:java}
Starting Flink Shell:
ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2 {code}",,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13827,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 14:39:06 UTC 2020,,,,,,,,,,"0|z0eteg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/20 14:39;chesnay;master: 31ec497b9678597f87914b607203cc849a8fd583 
1.11: 4fba374386232de92fa4acddeb2205471cc17cfa 
1.10: 57704bec68ddf00f5cec10b06517f52d289d0e95 
1.9: 1be21756fb14969da8ad328a1dbd76ccf3c3d9b0 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop table/view shouldn't take effect on each other,FLINK-17756,13305375,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,danny0405,ykt836,ykt836,16/May/20 12:13,16/Oct/20 10:56,13/Jul/23 08:07,27/May/20 01:53,,,,,,1.11.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"Currently ""DROP VIEW"" can successfully drop a table, and ""DROP TABLE"" can successfully a view. We should disable this.",,danny0405,godfreyhe,jark,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 01:53:22 UTC 2020,,,,,,,,,,"0|z0etao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/20 08:23;danny0405;Hi, [~ykt836], i would like to take this issue ~;;;","20/May/20 09:19;ykt836;assigned to you [~danny0405], thanks;;;","27/May/20 01:53;lzljs3620320;master: c8b265456a905b42c53de5dfed5a511ee93f3f8e

release-1.11: 23d4675520e3edbd3171374ed7a81444965b3d96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Walkthrough Table Java nightly end-to-end test failed to compile,FLINK-17754,13305370,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,ykt836,pnowojski,pnowojski,16/May/20 10:38,16/May/20 15:07,13/Jul/23 08:07,16/May/20 15:07,,,,,,1.11.0,,,,Table SQL / API,Table SQL / Planner,Tests,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1490&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{noformat}
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-41325742846/flink-walkthrough-table-java/src/main/java/org/apache/flink/walkthrough/SpendReport.java:[35,21] cannot find symbol
  symbol:   method registerTableSource(java.lang.String,org.apache.flink.walkthrough.common.table.BoundedTransactionTableSource)
  location: variable tEnv of type org.apache.flink.table.api.java.BatchTableEnvironment
[ERROR] /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-41325742846/flink-walkthrough-table-java/src/main/java/org/apache/flink/walkthrough/SpendReport.java:[36,21] cannot find symbol
  symbol:   method registerTableSink(java.lang.String,org.apache.flink.walkthrough.common.table.SpendReportTableSink)
  location: variable tEnv of type org.apache.flink.table.api.java.BatchTableEnvironment

(...)

[FAIL] 'Walkthrough Table Java nightly end-to-end test' failed after 0 minutes and 5 seconds! Test exited with exit code 1
{noformat}


",,dwysakowicz,pnowojski,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 15:05:11 UTC 2020,,,,,,,,,,"0|z0et9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/20 10:58;dwysakowicz;This might have been fixed by: https://github.com/apache/flink/commit/d17a2e016e4c85badff525410a89489261deaf9d

[~ykt836];;;","16/May/20 12:25;pnowojski;All masters builds are failing in this e2e test. In later builds there is a different error:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1499&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5
{noformat}
[ERROR] /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-48321358152/flink-walkthrough-table-scala/src/main/scala/org/apache/flink/walkthrough/SpendReport.scala:30: error: not found: type TableEnvironmentInternal
[ERROR]     tEnv.asInstanceOf[TableEnvironmentInternal].registerTableSourceInternal(
[ERROR]                       ^
[ERROR] /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-48321358152/flink-walkthrough-table-scala/src/main/scala/org/apache/flink/walkthrough/SpendReport.scala:32: error: not found: type TableEnvironmentInternal
[ERROR]     tEnv.asInstanceOf[TableEnvironmentInternal].registerTableSinkInternal(
[ERROR]                       ^
[ERROR] two errors found

{noformat}
;;;","16/May/20 15:05;ykt836;Yes, it's been fixed. We changed some API, these source codes are not captured by IDEA so we missed them. Sorry about that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
watermark defined in ddl does not work in Table api,FLINK-17753,13305367,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,godfreyhe,godfreyhe,16/May/20 09:27,09/Jun/20 16:10,13/Jul/23 08:07,09/Jun/20 13:53,,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"the following code will get {{org.apache.flink.table.api.ValidationException: A group window expects a time attribute for grouping in a stream environment.}}

{code:java}
@Test
  def testRowTimeTableSourceGroupWindow(): Unit = {
    val ddl =
      s""""""
         |CREATE TABLE rowTimeT (
         |  id int,
         |  rowtime timestamp(3),
         |  val bigint,
         |  name varchar(32),
         |  watermark for rowtime as rowtime
         |) WITH (
         |  'connector' = 'projectable-values',
         |  'bounded' = 'false'
         |)
       """""".stripMargin
    util.tableEnv.executeSql(ddl)

    val t = util.tableEnv.from(""rowTimeT"")
      .where($""val"" > 100)
      .window(Tumble over 10.minutes on 'rowtime as 'w)
      .groupBy('name, 'w)
      .select('name, 'w.end, 'val.avg)
    util.verifyPlan(t)
  }
{code}

The reason is planner does not convert {{watermarkSpecs}} in {{TableSchema}} to correct type when calling {{tableEnv.from}}
",,dian.fu,dwysakowicz,godfreyhe,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 13:53:09 UTC 2020,,,,,,,,,,"0|z0et8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 13:53;dwysakowicz;Fixed:
* master
** 8a8d8a95d0ead424d0ab61105b57fcabf6878e2e
* 1.11
** 655545472d87e7c7d19bbc910cee45870d2b3888;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align the timestamp format with Flink SQL types in JSON format,FLINK-17752,13305366,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,jark,jark,16/May/20 09:23,23/Mar/21 10:00,13/Jul/23 08:07,10/Nov/20 02:11,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,0,,,,,"Currently, we are using RFC3339_TIMESTAMP_FORMAT (which will add timezone at the end of string) to as the timestamp format in JSON. However, the string representation fo {{TIMESTAMP (WITHOUT TIME ZONE)}} shoudn't adding 'Z' at the end. 

Other discussions: 
[1]: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/TIME-TIMESTAMP-parse-in-Flink-TABLE-SQL-API-td33061.html
[2]: http://apache-flink.147419.n8.nabble.com/json-timestamp-json-flink-sql-td1914.html
[3]: http://apache-flink.147419.n8.nabble.com/FLINK-SQL-td2074.html",,dwysakowicz,felixzheng,godfreyhe,jark,jiamo,leonard,libenchao,lzljs3620320,Shizk233,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16725,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 23 10:00:06 UTC 2021,,,,,,,,,,"0|z0et8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/20 09:26;dwysakowicz;Just to add a few more issues with time handling in json format:

* TIMESTAMP(_WITHOUT_TIME_ZONE) - has a LocalDateTime semantics, you can not map it to a single point in time, now it prints in UTC with 'Z' suffix
* TIME_WITHOUT_TIME_ZONE - the same problem as above, sql says no time zone information
* TIMESTAMP_WITH_TIME_ZONE - (we do not support this type yet in the planner, so not a big problem) - it always prints it in UTC (with 'Z' suffix), even though it should print its time zone
* there is no support for TIME_WITH_LOCAL_TIME_ZONE - this could use the current format, it has an Instant semantics which means it points to a particular moment in time, the way we represent it is up to us (printing date time in utc or long since epoch utc);;;","12/Jun/20 06:07;jark;First of all, after some investigation, I would like to summarize the differences between [ISO-8601|https://en.wikipedia.org/wiki/ISO_8601] and [RFC-3339|https://www.ietf.org/rfc/rfc3339.txt] and SQL standard. 

- ISO-8601 requires ""T"" between date and time. But RFC-3339 allows a space character instead, see [1][2].
- The time zone suffix is optional in ISO-8601 [3] (interpreted as local zone by default [4]), but is required in RFC-3339 [1].
- SQL standard uses the format ""YYYY-MM-DD HH:MM:SS[.fractional]"", I didn't find which section mentions this in standard specifiction, but several documentation stand with this, e.g. {{TimestampType}} and [this|https://www.ibm.com/support/knowledgecenter/SSGU8G_12.1.0/com.ibm.esqlc.doc/ids_esqlc_0190.htm]. 
- ISO-8601 has basic format and extended format. ""1981-04-05"" is the extended format and ""19810405"" is the basic format.

I have discussed with [~fsk119], our proposal is introducing a new config option {{'json.timestamp-format'}} and {{'json.time-format'}}, all the valid values are (default is ""SQL""):

The timestamp format behavior works with {{TIMESTAMP}} type:

||{{timestamp-format}} || Data Example || TIMESTAMP [WITHOUT TIME ZONE] || 
| {{SQL}} | ""2020-06-12 11:46:22.123456789"" | Reading: the same to {{CAST(string AS TIMESTAMP)}}, Writing: the same to {{CAST(timestamp AS VARCHAR)}}| 
| {{ISO-8601}} | ""2020-06-12T11:46:22.123456789"" or ""2020-06-12T11:46:22.123456789Z"" | Reading: if data contains time zone suffix, drop the time zone information. Writing: always no time zone suffix   | 
| {{RFC-3339}} | ""2020-06-12T11:46:22.123456789Z"" or  ""2020-06-12 11:46:22.123456789Z"" | Reading: if data contains time zone suffix, drop the time zone information. Writing: always ""T"" separator and append ""Z"" suffix | 
| {{milliseconds-since-epoch}} | 1591940560123 |  Not Supported  |
| {{seconds-since-epoch}} | 1591940560 | Not Supported |


The timestamp format behavior works with {{TIMESTAMP WITH LOCAL TIME ZONE}} type:

||{{timestamp-format}} || Data Example || TIMESTAMP WITH LOCAL TIME ZONE || 
| {{SQL}} | ""2020-06-12 11:46:22.123456789 +08:00"" | Reading: the same to {{CAST(string AS TIMESTAMP WITH LOCAL TIME ZONE)}}, Writing: the same to {{CAST(timestamp AS VARCHAR)}}| 
| {{ISO-8601}} | ""2020-06-12T11:46:22.123456789"" or ""2020-06-12T11:46:22.123456789Z"" |Reading: if data doesn't contain time zone, do nothing in the data, interpret the time in local timezone, otherwise, convert the timestamp to local timezone and drop the time zone information. Writing: always print in UTC zone. |
| {{RFC-3339}} | ""2020-06-12T11:46:22.123456789Z"" or  ""2020-06-12 11:46:22.123456789Z"" | Reading: convert the timestamp to local timezone and drop the time zone information. Writing: always print in UTC zone with ""T"" separator.  | 
| {{milliseconds-since-epoch}} | 1591940560123 |  Reading: interprets the data as number of milliseconds since the epoch {{1970-01-01T00:00:00.000Z}}, e.g. the example data is interprets as {{2020-06-12T05:42:40.123Z}}. Writing: convert this date-time to the number of milliseconds from the epoch of {{1970-01-01T00:00:00.000Z}}. |
| {{seconds-since-epoch}} | 1591940560 | Reading: interprets the data as number of seconds since the epoch {{1970-01-01T00:00:00Z}}, e.g. the example data is interprets as {{2020-06-12T05:42:40Z}}. Writing: convert this date-time to the number of seconds from the epoch of {{1970-01-01T00:00:00Z}}. |

Note: the {{milliseconds-since-epoch}} and {{seconds-since-epoch}} are not supported in {{'json.time-format'}}, but the others are the same. 

What do you think? [~dwysakowicz] [~lzljs3620320]

[1]: https://medium.com/easyread/understanding-about-rfc-3339-for-datetime-formatting-in-software-engineering-940aa5d5f68a
[2]: https://en.wikipedia.org/wiki/ISO_8601#RFCs
[3]: https://www.cryptosys.net/pki/manpki/pki_iso8601datetime.html
[4]: https://en.wikipedia.org/wiki/ISO_8601#Local_time_(unqualified);;;","12/Jun/20 06:43;lzljs3620320;Thanks [~jark] for restarting this discussion and summary. Agree with you.

The conversion of {{TIMESTAMP WITH LOCAL TIME ZONE}} looks good to me. This type is time-zone-domain, so when dealing with string conversion, we must ""correct"" the values of other time zones to our local time zone.

Note TIMESTAMP WITHOUT TIME ZONE does not have time zone concept, so whatever types of string, it just read the ""year,month,day"" value, so the conversion is just remove zone information.

I think more popular type is {{TIMESTAMP WITH LOCAL TIME ZONE}} , this one is more similar to user cases. We should improve it to integration to watermark and eventtime in 1.12.;;;","12/Jun/20 06:51;jark;Agree with you [~lzljs3620320]. The above proposal is a long-term solution, I would like to convert this issue into an umbrella issue and create subtasks. For 1.11, I think we can introduce {{'json.timestamp-format'}} and {{'json.time-format'}} with option values {{SQL, ISO-8601, RFC-3339}}. This can fix the outputing ""Z"" problem, and also keeps the ability to convert and write ""2020-06-12T11:46:22.123456789Z"". The other tasks can be postponed to 1.12. ;;;","12/Jun/20 07:08;leonard;Thanks [~jark] for the insightful summary.

I think TIMESTAMP WITH LOCAL TIME ZONE is what user finally want, user can convert from a milliseconds-since-epoch/seconds-since-epoch to TIMESTAMP WITH LOCAL TIME ZONE. Unfortunately，we do not support TIMESTAMP WITH LOCAL TIME ZONE in eventtime and watermark(the most common scenarios), and +1 to support the improvement of TIMESTAMP WITH LOCAL TIME ZONE  in 1.12.;;;","12/Jun/20 11:19;twalthr;Thanks for the nice summary [~jark]. It helps a lot in understanding the issue and should end up in the docs later. When I read ""drop the time zone information"" or ""convert the timestamp to local timezone and drop the time zone"" I'm wondering if we are actually doing the right thing. All the different standards have introduced these completely different data types for a reason and we should not try to reinvent the wheel. We should avoid that every format needs to implement some time zone conversion logic. We should not try reinterpret/cast because this means that we are modifying the data. A simple `INSERT INTO t SELECT * FROM t;` would not be an identity operation and could cause confusion in the future as well. Instead we should support `ZonedTimestampType` in the planner (at least as a valid column without further built-in functions).

This would make it straight forward for users to declare a JSON format:
- Timestamp has a time zone? Use `TIMESTAMP WITH TIME ZONE`
- Timestamp is epoch? Use `TIMESTAMP WITH LOCAL TIME ZONE`
- Timestamp is a string? Use `TIMESTAMP WITHOUT TIME ZONE`

Everything else is just formatting whether `T` or ` ` is used.;;;","12/Jun/20 12:09;jark;Hi [~twalthr], I can see your points. I'm also fine to have a strict interpreting. To summary the result, for {{TIMESTAMP WITHOUT TIME ZONE}}, if the {{timestamp-format}} is {{""SQL""}}, then parsed and generated value is always ""2020-06-12 11:46:22.123456789"" (without ""T"" and time zone). If the {{timestamp-format}} is {{""ISO-8601""}}, then the parsed and generated value is always ""2020-06-12T11:46:22.123456789"" (with ""T"" and without time zone). Other formats are not supported for {{TIMESTAMP WITHOUT TIME ZONE}}. {{""ISO-8601""}} and {{""RFC-3339""}} are both supported for {{TIMESTAMP WITH LOCAL TIME ZONE}}, but requires the value with ""Z"" suffix. Does that make sense to you?;;;","12/Jun/20 13:10;dwysakowicz;Hi,
First of all thank you [~jark] for the thorough investigation.

I was also thinking in a similar way as [~twalthr] and is also aligned with your last comment [~jark]. In my understanding it should be the data that defines the DataType. We should not drop any information in the {{CREATE TABLE}} statement. I do understand it might be not the most flexible behaviour, but at this stage I really do believe this is actually for the user's benefit that we require proper typing. Otherwise it is really easy to get into a very hard to debug problems when working with dates/times. If user wants to convert one type to another, she/he can use an explicit cast, but at least we force the user to think of the semantics, which in my opinion is in the users best interest.

Moreover I think we can benefit a lot if we try to mimic as much of the {{java.time}} API as possible. Personally I find it rather well designed.
If we want to be really flexible in here, I'd rather have separate configurable formats for all three types:
{{code}}
timestamp-format.without-time-zone
timestamp-format.with-local-time-zone
timestamp-format.with-time-zone
{{code}}

Additionally we can have a set of predefined configurations corresponding to the different formats you mentioned: SQL, ISO-8601, RFC-3339.

SQL (e.g. timestamp-format.standard=SQL):
{code}
timestamp-format.without-time-zone=''yyyy-MM-dd HH:mm:ss.s{precision}""
timestamp-format.with-local-time-zone=''yyyy-MM-dd HH:mm:ss.s{precision}Z""
timestamp-format.with-time-zone=''yyyy-MM-dd HH:mm:ss.s{precision}+HH:MM:ss""
{code}

ISO-8601(e.g. timestamp-format.standard=ISO-8601):
{code}
timestamp-format.without-time-zone=''yyyy-MM-ddTHH:mm:ss.s{precision}""
timestamp-format.with-local-time-zone=''yyyy-MM-ddTHH:mm:ss.s{precision}Z""
timestamp-format.with-time-zone=''yyyy-MM-ddTHH:mm:ss.s{precision}+HH:MM:ss""
{code}

RFC-3339 (e.g. timestamp-format.standard=RFC-3339):
{code}
timestamp-format.without-time-zone=<not supported>
timestamp-format.with-local-time-zone=''yyyy-MM-ddTHH:mm:ss.s{precision}Z""
timestamp-format.with-time-zone=''yyyy-MM-ddTHH:mm:ss.s{precision}+HH:MM:ss""
{code}

This way users could define their custom formats, but again this would be explicit and users would have to think about the consequences:

{code}
timestamp.format.standard=custom
timestamp-format.without-time-zone=<millis since epoch>
timestamp-format.with-local-time-zone=<millis since epoch>
timestamp-format.with-time-zone=''yyyy-MM-dd HH:mm:ss.s{precision}+HH:MM:ss""
{code}

This would be the same as creating a {{DateTimeFormatter}} and calling {{LocalDateTime/Instant/OffsetDateTime.from(...)}} with that formatter. If the user uses one of the standard configuration, she/he can be sure the data will comply with the chosen standard, but still we would give a flexibility to read also any other custom format. But we would force the user to make that choice explicitly. We would not allow multiple different formats for a single type for a single table at the same time. Moreover we would not apply any casting like logic, but we would ensure that the data must comply with the expected format.

BTW, I did not think to much about the options hierarchy so far,  nor the markers for e.g. (<disabled>, <millis since epoch>). It is just an example.;;;","12/Jun/20 15:58;jark;Hi [~dwysakowicz], your proposal sounds good to me. Especially the separated configurable formats will make the format standard more comprehensible.

I have some thoughts about this:

1. If we allow the custom format, that would mean we allow to convert data with time zone information into a type without time zone. This may conflict with [~twalthr]'s point. I think this is powerful, but may still confuse users how the behavior is, for example, {{timestamp-format.without-time-zone=<millis since epoch>}} how the millis converted into TIMESTAMP? Will the millis be converted with +/- local time zone offset or a compile check exception be thrown? 
2. Do we allow to override {{timestamp-format.without-time-zone}} if {{timestamp.format.standard}} is not ""custom""? If allow, do we still need the kind of ""custom""? 

;;;","15/Jun/20 06:34;dwysakowicz;Hmm those are valid questions.

Ad. 1 I think there are no concerns about converting the millis to {{TIMESTAMP WITH LOCAL TIME ZONE}}. The question is shall we allow to create {{TIMESTAMP}} and {{TIMESTAMP WITH TIME ZONE}}. I'd say we should not allow millis for {{TIMESTAMP WITH TIME ZONE}}. I don't see a point of using that type along with millis. I was/am also unsure if millis and {{TIMESTAMP}} should be a valid combination. The reason why I suggested to do so is that some of the formats do that already (I am aware that avro stores both as long: https://issues.apache.org/jira/browse/AVRO-2328 and also see https://github.com/apache/avro/blob/9a33ed5498a68b58fc0d3d667d64899bed1fceeb/doc/src/content/xdocs/spec.xml#L1559)

But we do not need to follow that for JSON. I'd be even fine with forbidding expressing the TIMESTAMP as longs whatsoever. 

As for your concerns that it will confuse users. Yes it might, but imo those are settings that only users that know what they want to achieve should use. For the majority of the users the predefined sets should be more than enough. Similarly as it is with {{java.time}} API. You should rarely define your own formats, but rather use the predefined ones.

Ad.2 Personally I'd prefer not to give the possibility to override a single setting. To make it explicit that user should consider all three cases. The predefined settings should be more than enough anyway.

BTW, we do not need to start with the customizable option. We could start with the three predefined formats, without an option to override any settings. This is mostly in line with what [~twalthr] suggests. It should also cover most of the cases imo. Nevertheless we would have the mechanism behind the scenes, which would make it easier to expose later on if needed.;;;","15/Jun/20 06:47;jark;+1 to postpone the custom option, we can revisit this when we collect more user feedbacks after 1.11. 

Btw, I'm fine with the option name {{timestamp-format.standard}} , which supports future evolution. ;;;","15/Jun/20 13:54;twalthr;I'm also fine with postponing the custom option. We should not move too much logic that can be done in SQL to properties. Supporting SQL, ISO-8601, RFC-3339 should cover 80% of the use cases. For the remaining ones, people can just declare the column as `STRING` and use a computed column with a custom parsing logic wither expressed in SQL or as a user-defined scalar function.;;;","23/Mar/21 10:00;jiamo;Why not offer format in  [https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html]

Only have SQL and ISO-8601 and still don't support the total stand (such like 2021-03-23T09:04:37+00:00 ). The another CAST after string was useless (but now was must!).

We can let user input what the format they want.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
proctime defined in ddl can't work with over window in Table api,FLINK-17751,13305364,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,godfreyhe,godfreyhe,16/May/20 09:07,16/Oct/20 10:57,13/Jul/23 08:07,27/May/20 09:11,,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"the following test will get {{org.apache.flink.table.api.ValidationException: Ordering must be defined on a time attribute.}}
{code:scala}
  @Test
  def testProcTimeTableSourceOverWindow(): Unit = {
    val ddl =
      s""""""
         |CREATE TABLE procTimeT (
         |  id int,
         |  val bigint,
         |  name varchar(32),
         |  proctime as PROCTIME()
         |) WITH (
         |  'connector' = 'projectable-values',
         |  'bounded' = 'false'
         |)
       """""".stripMargin
    util.tableEnv.executeSql(ddl)

    val t = util.tableEnv.from(""procTimeT"")
      .window(Over partitionBy 'id orderBy 'proctime preceding 2.hours as 'w)
      .select('id, 'name, 'val.sum over 'w as 'valSum)
      .filter('valSum > 100)
    util.verifyPlan(t)
  }
{code}

The reason is: the type of proctime is {{TIMESTAMP(3) NOT null}}, while {{LegacyTypeInfoDataTypeConverter}} does not handle the mapping between {{Types.LOCAL_DATE_TIME}} and {{DataTypes.TIMESTAMP(3)}} with not null. 
",,godfreyhe,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 09:11:57 UTC 2020,,,,,,,,,,"0|z0et88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 09:11;jark;- master (1.12.0): 0e67f18846e974881af4487202f48a2e6165e8ac
- 1.11.0: a0800628fcd59ffbbf9889ad1b4fe3a4218b807e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint failed on azure,FLINK-17750,13305355,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,roman,roman,16/May/20 07:19,16/Oct/20 10:57,13/Jul/23 08:07,27/May/20 09:03,1.11.0,,,,,1.11.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"[https://dev.azure.com/khachatryanroman/810e80cc-0656-4d3c-9d8c-186764456a01/_apis/build/builds/6/logs/156]

 
{code:java}
 2020-05-15T23:42:29.5307581Z [ERROR] testKillYarnSessionClusterEntrypoint(org.apache.flink.yarn.YARNHighAvailabilityITCase)  Time elapsed: 21.68 s  <<< ERROR!
 2020-05-15T23:42:29.5308406Z java.util.concurrent.ExecutionException:
 2020-05-15T23:42:29.5308864Z org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
 2020-05-15T23:42:29.5309678Z java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runt ime.dispatcher.DispatcherGateway.requestJob(org.apache.flink.api.common.JobID,org.apache.flink.api.common.time.Time) timed out.
 2020-05-15T23:42:29.5310322Z    at com.sun.proxy.$Proxy33.requestJob(Unknown Source)
 2020-05-15T23:42:29.5311018Z    at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.getExecutionGraphInternal(DefaultExecutionGraphCach e.java:103)
 2020-05-15T23:42:29.5311704Z    at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.getExecutionGraph(DefaultExecutionGraphCache.java:7 1)
 2020-05-15T23:42:29.5312355Z    at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.handleRequest(AbstractExecutionGraphHandler.java:75 )
 2020-05-15T23:42:29.5312924Z    at org.apache.flink.runtime.rest.handler.AbstractRestHandler.respondToRequest(AbstractRestHandler.java:73)
 2020-05-15T23:42:29.5313423Z    at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:172)
 2020-05-15T23:42:29.5314497Z    at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:81)
 2020-05-15T23:42:29.5315083Z    at java.util.Optional.ifPresent(Optional.java:159)
 2020-05-15T23:42:29.5315474Z    at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:46)
 2020-05-15T23:42:29.5315979Z    at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:78)
 2020-05-15T23:42:29.5316520Z    at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49)
 2020-05-15T23:42:29.5317092Z    at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:10 5)
 2020-05-15T23:42:29.5317705Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:374)
 2020-05-15T23:42:29.5318586Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:360)
 2020-05-15T23:42:29.5319249Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext .java:352)
 2020-05-15T23:42:29.5319729Z    at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:110)
 2020-05-15T23:42:29.5320136Z    at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:89)
 2020-05-15T23:42:29.5320742Z    at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:54)
 2020-05-15T23:42:29.5321195Z    at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:10 5)
 2020-05-15T23:42:29.5321730Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:374)
 2020-05-15T23:42:29.5322263Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:360)
 2020-05-15T23:42:29.5322806Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext .java:352)
 2020-05-15T23:42:29.5323335Z    at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
 2020-05-15T23:42:29.5323849Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:374)
 2020-05-15T23:42:29.5324390Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:360)
 2020-05-15T23:42:29.5324910Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext .java:352)
 2020-05-15T23:42:29.5325920Z    at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:174)
 2020-05-15T23:42:29.5326353Z    at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:68)
 2020-05-15T23:42:29.5326867Z    at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:10 5)
 2020-05-15T23:42:29.5327532Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:374)
 2020-05-15T23:42:29.5328140Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:360)
 2020-05-15T23:42:29.5328748Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext .java:352)
 2020-05-15T23:42:29.5329391Z    at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRea d(CombinedChannelDuplexHandler.java:438)
 2020-05-15T23:42:29.5330028Z    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:328)
 2020-05-15T23:42:29.5330593Z    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:302)
 2020-05-15T23:42:29.5331149Z    at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java: 253)
 2020-05-15T23:42:29.5331750Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:374)
 2020-05-15T23:42:29.5332346Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:360)
 2020-05-15T23:42:29.5332956Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext .java:352)
 2020-05-15T23:42:29.5333552Z    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java: 1421)
 2020-05-15T23:42:29.5334135Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:374)
 2020-05-15T23:42:29.5334748Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:360)
 2020-05-15T23:42:29.5335383Z    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:930)
 2020-05-15T23:42:29.5335952Z    at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:1 63)
 2020-05-15T23:42:29.5336485Z    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:697)
 2020-05-15T23:42:29.5337019Z    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632)
 2020-05-15T23:42:29.5337552Z    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549)
 2020-05-15T23:42:29.5338023Z    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511)
 2020-05-15T23:42:29.5338689Z    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
 2020-05-15T23:42:29.5339358Z    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
 2020-05-15T23:42:29.5339869Z    at java.lang.Thread.run(Thread.java:748)
 2020-05-15T23:42:29.5341122Z Caused by: akka.pattern.AskTimeoutException: Recipient [Actor[akka://flink/user/rpc/dispatcher_1#-1875884516]] had already been t erminated. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage].
 2020-05-15T23:42:29.5341660Z    at akka.pattern.AskableActorRef$.recipientTerminatedException(AskSupport.scala:288)
 2020-05-15T23:42:29.5342018Z    at akka.pattern.AskableActorRef$.internalAsk$extension(AskSupport.scala:330)
 2020-05-15T23:42:29.5342321Z    at akka.pattern.AskSupport$class.ask(AskSupport.scala:81)
 2020-05-15T23:42:29.5342592Z    at akka.pattern.package$.ask(package.scala:42)
 2020-05-15T23:42:29.5342833Z    at akka.pattern.Patterns$.ask(Patterns.scala:160)
 2020-05-15T23:42:29.5343180Z    at akka.pattern.Patterns.ask(Patterns.scala)
 2020-05-15T23:42:29.5343485Z    at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.ask(AkkaInvocationHandler.java:361)
 2020-05-15T23:42:29.5343911Z    at org.apache.flink.runtime.rpc.akka.FencedAkkaInvocationHandler.ask(FencedAkkaInvocationHandler.java:123)
 2020-05-15T23:42:29.5344323Z    at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invokeRpc(AkkaInvocationHandler.java:222)
 2020-05-15T23:42:29.5344737Z    at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invoke(AkkaInvocationHandler.java:134)
 2020-05-15T23:42:29.5345147Z    at org.apache.flink.runtime.rpc.akka.FencedAkkaInvocationHandler.invoke(FencedAkkaInvocationHandler.java:79)
 2020-05-15T23:42:29.5345443Z    ... 51 more
 2020-05-15T23:42:29.5345533Z
 2020-05-15T23:42:29.5345682Z End of exception on server side>]
 2020-05-15T23:42:29.5345940Z    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
 2020-05-15T23:42:29.5346289Z    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
 2020-05-15T23:42:29.5346680Z    at org.apache.flink.yarn.YARNHighAvailabilityITCase.lambda$waitUntilJobIsRunning$4(YARNHighAvailabilityITCase.java:312)
 2020-05-15T23:42:29.5347126Z    at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:123)
 2020-05-15T23:42:29.5347520Z    at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:119)
 2020-05-15T23:42:29.5347950Z    at org.apache.flink.yarn.YARNHighAvailabilityITCase.waitUntilJobIsRunning(YARNHighAvailabilityITCase.java:310)
 2020-05-15T23:42:29.5348430Z    at org.apache.flink.yarn.YARNHighAvailabilityITCase.lambda$testKillYarnSessionClusterEntrypoint$0(YARNHighAvailabilityITCase.j*ava:168)
 2020-05-15T23:42:29.5348835Z    at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:258)
 2020-05-15T23:42:29.5349420Z    at org.apache.flink.yarn.YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint(YARNHighAvailabilityITCase.java:156)
 2020-05-15T23:42:29.5349984Z    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 2020-05-15T23:42:29.5350385Z    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 2020-05-15T23:42:29.5350772Z    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 2020-05-15T23:42:29.5351125Z    at java.lang.reflect.Method.invoke(Method.java:498)
 2020-05-15T23:42:29.5351459Z    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
 2020-05-15T23:42:29.5351868Z    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 2020-05-15T23:42:29.5352274Z    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
 2020-05-15T23:42:29.5352661Z    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 2020-05-15T23:42:29.5353219Z    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 2020-05-15T23:42:29.5353559Z    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 2020-05-15T23:42:29.5353886Z    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
 2020-05-15T23:42:29.5354163Z    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
 2020-05-15T23:42:29.5354454Z    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
 2020-05-15T23:42:29.5354776Z    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
 2020-05-15T23:42:29.5355144Z    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
 2020-05-15T23:42:29.5355459Z    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
 2020-05-15T23:42:29.5355764Z    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
 2020-05-15T23:42:29.5356062Z    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
 2020-05-15T23:42:29.5356382Z    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
 2020-05-15T23:42:29.5356681Z    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
 2020-05-15T23:42:29.5357068Z    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 2020-05-15T23:42:29.5357432Z    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 2020-05-15T23:42:29.5357757Z    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
 2020-05-15T23:42:29.5358088Z    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
 2020-05-15T23:42:29.5358402Z    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
 2020-05-15T23:42:29.5358705Z    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
 2020-05-15T23:42:29.5358976Z    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
 2020-05-15T23:42:29.5359307Z    at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
 2020-05-15T23:42:29.5359674Z    at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
 2020-05-15T23:42:29.5360252Z    at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
 2020-05-15T23:42:29.5360862Z    at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
 2020-05-15T23:42:29.5361296Z    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
 2020-05-15T23:42:29.5361723Z    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
 2020-05-15T23:42:29.5362125Z    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
 2020-05-15T23:42:29.5362503Z    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
 2020-05-15T23:42:29.5362969Z    Suppressed: java.lang.AssertionError: There is at least one application on the cluster that is not finished.[App application_1 589586092047_0002 is in state RUNNING.]
 2020-05-15T23:42:29.5363571Z            at org.junit.Assert.fail(Assert.java:88)
 2020-05-15T23:42:29.5364060Z            at org.apache.flink.yarn.YarnTestBase$CleanupYarnApplication.close(YarnTestBase.java:286)
 2020-05-15T23:42:29.5364448Z            at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:259)
 2020-05-15T23:42:29.5364745Z            ... 36 more
 2020-05-15T23:42:29.5365232Z Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
 2020-05-15T23:42:29.5365862Z java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runt ime.dispatcher.DispatcherGateway.requestJob(org.apache.flink.api.common.JobID,org.apache.flink.api.common.time.Time) timed out.
 2020-05-15T23:42:29.5366535Z    at com.sun.proxy.$Proxy33.requestJob(Unknown Source)
 2020-05-15T23:42:29.5366919Z    at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.getExecutionGraphInternal(DefaultExecutionGraphCach e.java:103)
 2020-05-15T23:42:29.5367421Z    at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.getExecutionGraph(DefaultExecutionGraphCache.java:7 1)
 2020-05-15T23:42:29.5367925Z    at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.handleRequest(AbstractExecutionGraphHandler.java:75 )
 2020-05-15T23:42:29.5368376Z    at org.apache.flink.runtime.rest.handler.AbstractRestHandler.respondToRequest(AbstractRestHandler.java:73)
 2020-05-15T23:42:29.5368902Z    at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:172)
 2020-05-15T23:42:29.5369342Z    at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:81)
 2020-05-15T23:42:29.5369695Z    at java.util.Optional.ifPresent(Optional.java:159)
 2020-05-15T23:42:29.5370003Z    at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:46)
 2020-05-15T23:42:29.5370430Z    at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:78)
 2020-05-15T23:42:29.5370872Z    at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49)
 2020-05-15T23:42:29.5371391Z    at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:10 5)
 2020-05-15T23:42:29.5371922Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:374)
 2020-05-15T23:42:29.5372438Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)
 2020-05-15T23:42:29.5372963Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext .java:352)
 2020-05-15T23:42:29.5373420Z    at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:110)
 2020-05-15T23:42:29.5373804Z    at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:89)
 2020-05-15T23:42:29.5374211Z    at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:54)
 2020-05-15T23:42:29.5374655Z    at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:10 5)
 2020-05-15T23:42:29.5375352Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:374)
 2020-05-15T23:42:29.5376349Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:360)
 2020-05-15T23:42:29.5376957Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext .java:352)
 2020-05-15T23:42:29.5377544Z    at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
 2020-05-15T23:42:29.5378119Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:374)
 2020-05-15T23:42:29.5378804Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:360)
 2020-05-15T23:42:29.5379469Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext .java:352)
 2020-05-15T23:42:29.5379997Z    at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:174)
 2020-05-15T23:42:29.5381976Z    at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:68)
 2020-05-15T23:42:29.5382435Z    at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:10 5)
 2020-05-15T23:42:29.5382951Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:374)
 2020-05-15T23:42:29.5383467Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:360)
 2020-05-15T23:42:29.5384003Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext .java:352)
 2020-05-15T23:42:29.5384561Z    at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRea d(CombinedChannelDuplexHandler.java:438)
 2020-05-15T23:42:29.5385116Z    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:328)
 2020-05-15T23:42:29.5385601Z    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:302)
 2020-05-15T23:42:29.5386075Z    at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java: 253)
 2020-05-15T23:42:29.5386588Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:374)
 2020-05-15T23:42:29.5387217Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:360)
 2020-05-15T23:42:29.5387765Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext .java:352)
 2020-05-15T23:42:29.5388263Z    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java: 1421)
 2020-05-15T23:42:29.5388785Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:374)
 2020-05-15T23:42:29.5389313Z    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerConte xt.java:360)
 2020-05-15T23:42:29.5389799Z    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:930)
 2020-05-15T23:42:29.5390295Z    at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:1 63)
 2020-05-15T23:42:29.5390755Z    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:697)
 2020-05-15T23:42:29.5391213Z    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632)
 2020-05-15T23:42:29.5391659Z    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549)
 2020-05-15T23:42:29.5392081Z    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511)
 2020-05-15T23:42:29.5392527Z    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
 2020-05-15T23:42:29.5392978Z    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
 2020-05-15T23:42:29.5393314Z    at java.lang.Thread.run(Thread.java:748)
 2020-05-15T23:42:29.5394130Z Caused by: akka.pattern.AskTimeoutException: Recipient [Actor[akka://flink/user/rpc/dispatcher_1#-1875884516]] had already been t erminated. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage].
 2020-05-15T23:42:29.5394945Z    at akka.pattern.AskableActorRef$.recipientTerminatedException(AskSupport.scala:288)
 2020-05-15T23:42:29.5395309Z    at akka.pattern.AskableActorRef$.internalAsk$extension(AskSupport.scala:330)
 2020-05-15T23:42:29.5395831Z    at akka.pattern.AskSupport$class.ask(AskSupport.scala:81)
 2020-05-15T23:42:29.5396125Z    at akka.pattern.package$.ask(package.scala:42)
 2020-05-15T23:42:29.5396388Z    at akka.pattern.Patterns$.ask(Patterns.scala:160)
 2020-05-15T23:42:29.5396994Z    at akka.pattern.Patterns.ask(Patterns.scala)
 2020-05-15T23:42:29.5397333Z    at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.ask(AkkaInvocationHandler.java:361)
 2020-05-15T23:42:29.5397823Z    at org.apache.flink.runtime.rpc.akka.FencedAkkaInvocationHandler.ask(FencedAkkaInvocationHandler.java:123)
 2020-05-15T23:42:29.5398457Z    at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invokeRpc(AkkaInvocationHandler.java:222)
 2020-05-15T23:42:29.5398923Z    at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invoke(AkkaInvocationHandler.java:134)
 2020-05-15T23:42:29.5399383Z    at org.apache.flink.runtime.rpc.akka.FencedAkkaInvocationHandler.invoke(FencedAkkaInvocationHandler.java:79)
 2020-05-15T23:42:29.5399708Z    ... 51 more
 2020-05-15T23:42:29.5399974Z
 2020-05-15T23:42:29.5400132Z End of exception on server side>]
 2020-05-15T23:42:29.5400567Z    at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:390)
 2020-05-15T23:42:29.5400930Z    at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:374)
 2020-05-15T23:42:29.5401283Z    at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
 2020-05-15T23:42:29.5401652Z    at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
 2020-05-15T23:42:29.5402067Z    at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
 2020-05-15T23:42:29.5402452Z    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 2020-05-15T23:42:29.5402800Z    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 2020-05-15T23:42:29.5403098Z    at java.lang.Thread.run(Thread.java:748)
{code}
 ",,guoyangze,roman,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17849,,,,,,,,FLINK-17947,,,,,,,,,,,,,,,,,"25/May/20 09:22;wangyang0918;jobmanager.log;https://issues.apache.org/jira/secure/attachment/13003928/jobmanager.log",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 09:03:31 UTC 2020,,,,,,,,,,"0|z0et68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 09:23;trohrmann;The problem might be that a previous Yarn test did not properly stop:

{code}
 Suppressed: java.lang.AssertionError: There is at least one application on the cluster that is not finished.[App application_1 589586092047_0002 is in state RUNNING.]
 2020-05-15T23:42:29.5363571Z            at org.junit.Assert.fail(Assert.java:88)
 2020-05-15T23:42:29.5364060Z            at org.apache.flink.yarn.YarnTestBase$CleanupYarnApplication.close(YarnTestBase.java:286)
 2020-05-15T23:42:29.5364448Z            at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:259)
 2020-05-15T23:42:29.5364745Z            ... 36 more
{code};;;","19/May/20 14:41;trohrmann;For future reference: The link to the AZP build is https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=6&view=logs&j=764762df-f65b-572b-3d5c-65518c777be4&t=8d823410-c7c7-5a4d-68bb-fa7b08da17b9.;;;","21/May/20 11:25;wangyang0918;I am afraid the ""AssertionError: There is at least one application on the"" is not the root cause. It happened just because the test does not finished properly.

BTW, I think this is a same case of FLINK-17849. See the analysis of in the comments.;;;","25/May/20 07:52;trohrmann;Yes, I agree that this is not the problem. The problem is as you've written on FLINK-17849: The ZooKeeper connection is suspended and this causes a restart of the {{Dispatcher}}. If we request the job details at the same time, then it might happen that the request times out.

I guess that we are seeing this ZooKeeper problem because of slow hardware. One solution could be to tolerate failures when asking for the job details. Another solution could be to increase the ZooKeeper timeouts in order to make the suspension less likely to happen.;;;","25/May/20 08:28;wangyang0918;Yeah, either increasing the timeout of zookeeper or akka ask timeout could reduce the tests failure possibility.;;;","25/May/20 08:39;trohrmann;I don't think that increasing the akka ask timeout will solve the problem here.;;;","25/May/20 09:21;wangyang0918;The test failed with timed out in {{DispatcherGateway.requestJob}}. If we increase the timeout, even there is something wrong with the zookeeper and the job restarts, it should finally turn into running. And then the Flink client could get the job result correctly.

You could find the job has been restarted and finally turning into running in the log[1].

 

I admin that increasing the timeout of zookeeper should be done first and i also suggest to increase the akka ask timeout. It could help to reduce the failure further.

 

——————

Update:

Maybe i am wrong, it is not {{akka.ask.timeout}}. The {{web.timeout}} could help.

 

[1]. [^jobmanager.log];;;","26/May/20 13:20;trohrmann;Increasing the timeout won't solve the problem because the problem is that the original recipient is no longer running due to losing the connection to ZooKeeper and, thus, also the leadership: {{akka.pattern.AskTimeoutException: Recipient [Actor[akka://flink/user/rpc/dispatcher_1#-1875884516]] had already been terminated. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]}}. The proper solution should be to tolerate that there is a leader change when calling {{YARNHighAvailabilityITCase.waitUntilJobIsRunning}}. A simpler way to harden the test could be to increase the ZooKeeper session timeout via {{HighAvailabilityOptions.ZOOKEEPER_SESSION_TIMEOUT}}.;;;","26/May/20 14:30;trohrmann;A quick update: We actually need to solve the problem that a {{RpcEndpoint}} terminates after a request has been sent to it (e.g. from a REST handler) and, thus, the request will time out with {{akka.pattern.AskTimeoutException: Recipient Actor[akka://flink/user/rpc/dispatcher_1#-1875884516] had already been terminated.}} on a different level. I think in this case, the REST handler should respond to the client with {{HttpResponseStatus.SERVICE_UNAVAILABLE}} similar to the situation when there is no leader. That way, the client can decide whether it should retry the request or fail.

For the time being, I suggest to increase the ZooKeeper session timeout.;;;","26/May/20 14:47;trohrmann;The root cause of the problem is tracked via FLINK-17947.;;;","27/May/20 03:26;wangyang0918;Thanks for your explanation. I think you are right. Increasing the timeout will not help since the leader has changed.;;;","27/May/20 09:03;trohrmann;Fixed via

master: 93b81a38efe4dce41c985cd93862d6cbf0c8d0c4
1.11.0: 0e9ddbcbfae556860c0c5323e616cc82f55efe6a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamContextEnvironment#execute cannot be call JobListener#onJobExecuted,FLINK-17744,13305327,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Echo Lee,Echo Lee,Echo Lee,16/May/20 02:16,16/Oct/20 10:55,13/Jul/23 08:07,28/May/20 19:43,1.10.0,,,,,1.10.2,1.11.0,1.12.0,,Client / Job Submission,,,,,0,pull-request-available,,,,"When I register a jobListener  to stream environment. I want  JobListener#onJobExecuted is executed when job was finished or cancelled. But in StreamContextEnvironment, the method  JobListener#onJobExecuted is not called now.",,Echo Lee,KevinZwx,kkl0u,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18510,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 19:43:53 UTC 2020,,,,,,,,,,"0|z0et00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 11:45;Echo Lee;Hi [~kkl0u] I have modified the PR according to your suggestion, please confirm it, thank you！;;;","28/May/20 19:43;kkl0u;Merged on master with 7dcfd90dd42648133bb7c1216740f47fffe89c13
and on release-1.11 with 0794089ac3337835af64889d185020d30dde973e
and on release-1.10 with 01aa611e2096af1911ae63ba109810582e945d8c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResultPartitionTest.testInitializeMoreStateThanBuffer is unstable,FLINK-17739,13305225,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjwang,pnowojski,pnowojski,15/May/20 15:45,16/Oct/20 10:33,13/Jul/23 08:07,16/May/20 13:23,,,,,,1.11.0,,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,"When run in loop, after ~50-100 runs it throws:

{noformat}
java.lang.AssertionError: 
Expected :2
Actual   :1
<Click to see difference>

	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.flink.runtime.io.network.partition.ResultPartitionTest.testInitializeMoreStateThanBuffer(ResultPartitionTest.java:525)
{noformat}
",,pnowojski,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 13:23:04 UTC 2020,,,,,,,,,,"0|z0esdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/20 13:23;zjwang;Merged in master: 7796f6e9601cdec3e1377acbfa212e6cfb36f793;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyedStateCheckpointingITCase fails in UnalignedCheckpoint mode,FLINK-17737,13305214,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,roman,roman,15/May/20 14:40,16/Oct/20 10:32,13/Jul/23 08:07,19/May/20 13:53,1.11.0,,,,,1.11.0,,,,Runtime / Checkpointing,Runtime / Task,Tests,,,0,pull-request-available,,,,,,complone,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 07:32:52 UTC 2020,,,,,,,,,,"0|z0esaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/20 22:09;complone;Hello, please explain further what the UC mode means · 1, I am happy to do it;;;","16/May/20 06:55;roman;Thanks for offering your help [~complone]!

There is already a PR for this issue (but feel free to peek any other issue :) ).;;;","16/May/20 07:32;pnowojski;[~complone] it's UnalignedCheckpoint mode FLINK-14551. As part of FLINK-17218 we enabling unaligned checkpoints by default on azure and fixing all of the issues found there.

Merged to master as 7a09b55e74..51c689c83a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsStateWithMultiPart times out,FLINK-17730,13305187,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,15/May/20 12:17,16/Oct/20 10:48,13/Jul/23 08:07,20/May/20 17:59,,,,,,1.11.0,1.12.0,,,Build System / Azure Pipelines,FileSystems,Tests,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1374&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8

After 5 minutes 
{code}
2020-05-15T06:56:38.1688341Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fa10800b800 nid=0x1161 runnable [0x00007fa110959000]
2020-05-15T06:56:38.1688709Z    java.lang.Thread.State: RUNNABLE
2020-05-15T06:56:38.1689028Z 	at java.net.SocketInputStream.socketRead0(Native Method)
2020-05-15T06:56:38.1689496Z 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
2020-05-15T06:56:38.1689921Z 	at java.net.SocketInputStream.read(SocketInputStream.java:171)
2020-05-15T06:56:38.1690316Z 	at java.net.SocketInputStream.read(SocketInputStream.java:141)
2020-05-15T06:56:38.1690723Z 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)
2020-05-15T06:56:38.1691196Z 	at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:593)
2020-05-15T06:56:38.1691608Z 	at sun.security.ssl.InputRecord.read(InputRecord.java:532)
2020-05-15T06:56:38.1692023Z 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975)
2020-05-15T06:56:38.1692558Z 	- locked <0x00000000b94644f8> (a java.lang.Object)
2020-05-15T06:56:38.1692946Z 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933)
2020-05-15T06:56:38.1693371Z 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)
2020-05-15T06:56:38.1694151Z 	- locked <0x00000000b9464d20> (a sun.security.ssl.AppInputStream)
2020-05-15T06:56:38.1694908Z 	at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)
2020-05-15T06:56:38.1695475Z 	at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)
2020-05-15T06:56:38.1696007Z 	at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)
2020-05-15T06:56:38.1696509Z 	at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)
2020-05-15T06:56:38.1696993Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:90)
2020-05-15T06:56:38.1697466Z 	at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
2020-05-15T06:56:38.1698069Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:90)
2020-05-15T06:56:38.1698567Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:90)
2020-05-15T06:56:38.1699041Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:90)
2020-05-15T06:56:38.1699624Z 	at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
2020-05-15T06:56:38.1700090Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:90)
2020-05-15T06:56:38.1700584Z 	at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)
2020-05-15T06:56:38.1701282Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:90)
2020-05-15T06:56:38.1701800Z 	at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)
2020-05-15T06:56:38.1702328Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:90)
2020-05-15T06:56:38.1702804Z 	at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$read$3(S3AInputStream.java:445)
2020-05-15T06:56:38.1703270Z 	at org.apache.hadoop.fs.s3a.S3AInputStream$$Lambda$42/1204178174.execute(Unknown Source)
2020-05-15T06:56:38.1703677Z 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
2020-05-15T06:56:38.1704090Z 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260)
2020-05-15T06:56:38.1704607Z 	at org.apache.hadoop.fs.s3a.Invoker$$Lambda$23/1991724700.execute(Unknown Source)
2020-05-15T06:56:38.1705115Z 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)
2020-05-15T06:56:38.1705551Z 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:256)
2020-05-15T06:56:38.1705937Z 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:231)
2020-05-15T06:56:38.1706363Z 	at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:441)
2020-05-15T06:56:38.1707052Z 	- locked <0x00000000b7d98b60> (a org.apache.hadoop.fs.s3a.S3AInputStream)
2020-05-15T06:56:38.1707438Z 	at java.io.DataInputStream.read(DataInputStream.java:149)
2020-05-15T06:56:38.1707904Z 	at org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.read(HadoopDataInputStream.java:94)
2020-05-15T06:56:38.1708366Z 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
2020-05-15T06:56:38.1708770Z 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
2020-05-15T06:56:38.1709150Z 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
2020-05-15T06:56:38.1709784Z 	- locked <0x00000000b7d9a2b0> (a java.io.InputStreamReader)
2020-05-15T06:56:38.1710170Z 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
2020-05-15T06:56:38.1710557Z 	at java.io.BufferedReader.fill(BufferedReader.java:161)
2020-05-15T06:56:38.1710956Z 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
2020-05-15T06:56:38.1711552Z 	- locked <0x00000000b7d9a2b0> (a java.io.InputStreamReader)
2020-05-15T06:56:38.1711930Z 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
2020-05-15T06:56:38.1712451Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getContentsOfFile(HadoopS3RecoverableWriterITCase.java:423)
2020-05-15T06:56:38.1713152Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:411)
2020-05-15T06:56:38.1713922Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)
2020-05-15T06:56:38.1714804Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:344)
2020-05-15T06:56:38.1715395Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-15T06:56:38.1715807Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-15T06:56:38.1716313Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-15T06:56:38.1716754Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-15T06:56:38.1717181Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-15T06:56:38.1717696Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-15T06:56:38.1718191Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-15T06:56:38.1718687Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-15T06:56:38.1719181Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-15T06:56:38.1719822Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-15T06:56:38.1720462Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-15T06:56:38.1720852Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-15T06:56:38.1721321Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-15T06:56:38.1721770Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-15T06:56:38.1722272Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-15T06:56:38.1722731Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-15T06:56:38.1723133Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-15T06:56:38.1723561Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-15T06:56:38.1724113Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-15T06:56:38.1724630Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-15T06:56:38.1725086Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-15T06:56:38.1725553Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-15T06:56:38.1726019Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-15T06:56:38.1726422Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-15T06:56:38.1726808Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-15T06:56:38.1727227Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-15T06:56:38.1727727Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-15T06:56:38.1728245Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-15T06:56:38.1728733Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-15T06:56:38.1729350Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-15T06:56:38.1729898Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-15T06:56:38.1730401Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-15T06:56:38.1730874Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,aljoscha,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16768,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 28 01:55:42 UTC 2020,,,,,,,,,,"0|z0es4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/20 12:17;rmetzger;I'm 99% sure that we can resolve this issue by increasing the timeout to 10 minutes.;;;","17/May/20 02:42;dian.fu;another instance: https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/1551/logs/70;;;","17/May/20 07:10;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1551&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","18/May/20 06:33;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1630&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","18/May/20 07:41;rmetzger;In FLINK-17336, we introduced a timeout of 5 minutes. Based on the error reports there, we saw the following times w/o maven output:
- 6:30 minutes
- 6 m
- 5:07 m
- 11 m
- 25 m
- 8m
- 7m
- 6m
- 7m

Based on this analysis, I propose a timeout of 15 minutes. This would cause timeouts only for the very severely delated case of 25m.;;;","18/May/20 10:20;aljoscha;That's a very long runtime, though...;;;","18/May/20 12:45;rmetzger;Indeed, but these are very rare cases. We currently have ~100 builds a day, and maybe 5 are failing because of that. and even if they are failing, they are usually  <10 minutes.
It's a tradeoff between ""green builds"" and wait time.
The real underlying issue is the network stability of our CI servers. If they were more stable, then we would not have these issues at all.;;;","18/May/20 13:25;rmetzger;Merged in https://github.com/apache/flink/commit/43c3bfa9cf29c229477b9c6bdc3872de44985c20;;;","19/May/20 07:27;rmetzger;Case with 15 minutes :( https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1750&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","19/May/20 19:14;rmetzger;Time to reopen?
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1835&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","20/May/20 05:21;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1888&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=eb5f4d19-2d2d-5856-a4ce-acf5f904a994;;;","20/May/20 17:59;rmetzger;Merged to ""release-1.11"" in https://github.com/apache/flink/commit/e321e483fa0b3154e5e8417809bd669482783a13;;;","22/May/20 03:06;dian.fu;It seems that it's still happening:
https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/2005/logs/96;;;","08/Jun/20 06:35;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2853&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=2f99feaa-7a9b-5916-4c1c-5e61f395079e;;;","24/Jul/20 03:03;dian.fu;1.11 branch:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4822&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8];;;","24/Jul/20 03:19;dian.fu;master: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4837&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361];;;","27/Jul/20 01:30;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4911&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=603cb7fd-6f38-5c99-efca-877e1439232f;;;","28/Jul/20 01:38;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4938&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8];;;","28/Jul/20 01:55;dian.fu;It seems that this issue is similar to FLINK-16768(both hangs on socket read/write although the stack trace is not exactly the same) and so I will post the failure links there and let's try to solve this problem in FLINK-16768.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't subsume checkpoint with no channel state in UC mode,FLINK-17727,13305181,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,roman,roman,15/May/20 11:25,16/Oct/20 10:31,13/Jul/23 08:07,16/May/20 04:47,1.11.0,,,,,1.11.0,,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,"When there are no channel state handles, the underlying FS stream is still created.

On discard it is not deleted because it's not referenced by any state handles.",,pnowojski,roman,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 04:47:58 UTC 2020,,,,,,,,,,"0|z0es3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/20 04:47;pnowojski;Fixed on master as 9207b81cb1 and 91932ccc05;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FileUploadHandlerTest.testUploadCleanupOnFailure fails with ""SocketTimeout timeout""",FLINK-17725,13305165,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,rmetzger,rmetzger,15/May/20 10:05,16/Oct/20 10:52,13/Jul/23 08:07,20/May/20 09:27,1.11.0,,,,,1.11.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1392&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d ( select 1st attempt)

{code}
2020-05-15T09:01:40.0547958Z [ERROR] testUploadCleanupOnFailure(org.apache.flink.runtime.rest.FileUploadHandlerTest)  Time elapsed: 10.415 s  <<< ERROR!
2020-05-15T09:01:40.0548716Z java.net.SocketTimeoutException: timeout
2020-05-15T09:01:40.0549048Z 	at okio.Okio$4.newTimeoutException(Okio.java:227)
2020-05-15T09:01:40.0549361Z 	at okio.AsyncTimeout.exit(AsyncTimeout.java:284)
2020-05-15T09:01:40.0549688Z 	at okio.AsyncTimeout$2.read(AsyncTimeout.java:240)
2020-05-15T09:01:40.0552454Z 	at okio.RealBufferedSource.indexOf(RealBufferedSource.java:344)
2020-05-15T09:01:40.0554987Z 	at okio.RealBufferedSource.readUtf8LineStrict(RealBufferedSource.java:216)
2020-05-15T09:01:40.0555636Z 	at okio.RealBufferedSource.readUtf8LineStrict(RealBufferedSource.java:210)
2020-05-15T09:01:40.0556307Z 	at okhttp3.internal.http1.Http1Codec.readResponseHeaders(Http1Codec.java:189)
2020-05-15T09:01:40.0556856Z 	at okhttp3.internal.http.CallServerInterceptor.intercept(CallServerInterceptor.java:75)
2020-05-15T09:01:40.0557505Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
2020-05-15T09:01:40.0558021Z 	at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:45)
2020-05-15T09:01:40.0558498Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
2020-05-15T09:01:40.0558932Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)
2020-05-15T09:01:40.0559381Z 	at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)
2020-05-15T09:01:40.0559803Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
2020-05-15T09:01:40.0560262Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)
2020-05-15T09:01:40.0561022Z 	at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
2020-05-15T09:01:40.0561701Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
2020-05-15T09:01:40.0562439Z 	at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:120)
2020-05-15T09:01:40.0563170Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
2020-05-15T09:01:40.0565934Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)
2020-05-15T09:01:40.0566781Z 	at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:185)
2020-05-15T09:01:40.0575046Z 	at okhttp3.RealCall.execute(RealCall.java:69)
2020-05-15T09:01:40.0575858Z 	at org.apache.flink.runtime.rest.FileUploadHandlerTest.testUploadCleanupOnFailure(FileUploadHandlerTest.java:250)
2020-05-15T09:01:40.0576567Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-15T09:01:40.0577242Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-15T09:01:40.0577979Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-15T09:01:40.0578594Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-15T09:01:40.0579234Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-15T09:01:40.0580279Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-15T09:01:40.0581129Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-15T09:01:40.0581862Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-15T09:01:40.0582538Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-15T09:01:40.0583174Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-15T09:01:40.0583934Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-15T09:01:40.0584501Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-15T09:01:40.0585282Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-15T09:01:40.0586005Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-15T09:01:40.0586666Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-15T09:01:40.0587257Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-15T09:01:40.0587879Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-15T09:01:40.0588486Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-15T09:01:40.0589101Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-15T09:01:40.0589724Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-15T09:01:40.0590407Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-15T09:01:40.0591102Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-15T09:01:40.0591621Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-15T09:01:40.0592245Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-15T09:01:40.0592978Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-15T09:01:40.0593784Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-15T09:01:40.0594692Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-15T09:01:40.0595445Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-15T09:01:40.0596220Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-15T09:01:40.0596947Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-15T09:01:40.0597646Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-15T09:01:40.0598221Z Caused by: java.net.SocketException: Socket closed
2020-05-15T09:01:40.0598774Z 	at java.net.SocketInputStream.read(SocketInputStream.java:204)
2020-05-15T09:01:40.0599375Z 	at java.net.SocketInputStream.read(SocketInputStream.java:141)
2020-05-15T09:01:40.0599864Z 	at okio.Okio$2.read(Okio.java:138)
2020-05-15T09:01:40.0600350Z 	at okio.AsyncTimeout$2.read(AsyncTimeout.java:236)
2020-05-15T09:01:40.0600804Z 	... 51 more
2020-05-15T09:01:40.0601098Z 
2020-05-15T09:01:40.4326691Z [INFO] Running org.apache.flink.runtime.metrics.groups.TaskIOMetricGroupTest
{code}",,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 20 09:27:00 UTC 2020,,,,,,,,,,"0|z0es00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/20 09:05;trohrmann;What do you mean with select 1st attempt [~rmetzger]? In the linked build, I can only see some e2e test failures and a failure of the log upload.;;;","19/May/20 09:07;trohrmann;Stupid me found the attempts button on AZP. ;;;","19/May/20 10:08;trohrmann;Looking at the logs, the problem seems to be that we do not respond to the file upload request within the write timeout of 10s. Interestingly, we do respond to it after it so my suspicion is that something takes very long on AZP and I don't think that there is a problem with the logic. My best suggestion would be to disable the timeouts for the {{OkHttpClient}} in the {{FileUploadHandlerTest}}.;;;","19/May/20 10:10;chesnay;Sounds very similar to FLINK-15503.;;;","19/May/20 14:25;rmetzger;Thanks for the pointer to FLINK-15503 Chesnay. We are seeing all kinds of weird network issues on the Alibaba-hosted CI machines.
I was not yet able to figure out the problem yet ... but this is the first well-documented case of local network issues.
I will take a closer look at FLINK-15503 (comparing test runtimes on the CI machines with and without docker);;;","20/May/20 09:27;trohrmann;Fixed via

master: e991de19337591cde444fbede010cb8bdc7f118f
1.11.0: 1b0d7fddfb5e948fe9c6fa8b17f6c00569addd02;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PyFlink end-to-end test fails with Cannot run program ""venv.zip/.conda/bin/python"": error=2, No such file or directory",FLINK-17724,13305161,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zhongwei,rmetzger,rmetzger,15/May/20 09:53,16/Oct/20 10:31,13/Jul/23 08:07,16/May/20 01:00,1.11.0,,,,,1.11.0,,,,API / Python,Tests,,,,0,pull-request-available,test-stability,,,"CI: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8001&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=2f5b54d0-1d28-5b01-d344-aa50ffe0cdf8

{code}
2020-05-15T09:21:20.5198725Z Verifying transaction: ...working... done
2020-05-15T09:21:20.5918557Z Executing transaction: ...working... done
2020-05-15T09:22:18.8776831Z DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
2020-05-15T09:22:19.0555591Z Starting cluster.
2020-05-15T09:22:21.9516388Z Starting standalonesession daemon on host fv-az678.
2020-05-15T09:22:23.5833254Z Starting taskexecutor daemon on host fv-az678.
2020-05-15T09:22:23.6192099Z Waiting for Dispatcher REST endpoint to come up...
2020-05-15T09:22:24.6699447Z Waiting for Dispatcher REST endpoint to come up...
2020-05-15T09:22:26.0376695Z Waiting for Dispatcher REST endpoint to come up...
2020-05-15T09:22:27.1345574Z Waiting for Dispatcher REST endpoint to come up...
2020-05-15T09:22:28.1809673Z Dispatcher REST endpoint is up.
2020-05-15T09:22:28.1842051Z Test submitting python job:\n
2020-05-15T09:22:29.6520483Z Results directory: /tmp/result
2020-05-15T09:23:23.2171819Z Traceback (most recent call last):
2020-05-15T09:23:23.2174222Z   File ""/home/vsts/work/1/s/flink-end-to-end-tests/flink-python-test/python/python_job.py"", line 82, in <module>
2020-05-15T09:23:23.2174857Z     word_count()
2020-05-15T09:23:23.2175685Z   File ""/home/vsts/work/1/s/flink-end-to-end-tests/flink-python-test/python/python_job.py"", line 76, in word_count
2020-05-15T09:23:23.2176310Z     t_env.execute(""word_count"")
2020-05-15T09:23:23.2177228Z   File ""/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/opt/python/pyflink.zip/pyflink/table/table_environment.py"", line 1049, in execute
2020-05-15T09:23:23.2179484Z   File ""/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/opt/python/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1286, in __call__
2020-05-15T09:23:23.2181045Z   File ""/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/opt/python/pyflink.zip/pyflink/util/exceptions.py"", line 147, in deco
2020-05-15T09:23:23.2182205Z   File ""/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/opt/python/py4j-0.10.8.1-src.zip/py4j/protocol.py"", line 328, in get_return_value
2020-05-15T09:23:23.2182889Z py4j.protocol.Py4JJavaError: An error occurred while calling o2.execute.
2020-05-15T09:23:23.2183634Z : java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: b0e54b0a1c99b57e04ec32d7879437c4)
2020-05-15T09:23:23.2184387Z 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:290)
2020-05-15T09:23:23.2184960Z 	at org.apache.flink.table.api.internal.BatchTableEnvImpl.execute(BatchTableEnvImpl.scala:325)
2020-05-15T09:23:23.2185508Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-15T09:23:23.2186320Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-15T09:23:23.2186930Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-15T09:23:23.2187472Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-15T09:23:23.2188027Z 	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2020-05-15T09:23:23.2188672Z 	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2020-05-15T09:23:23.2189287Z 	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
2020-05-15T09:23:23.2189911Z 	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2020-05-15T09:23:23.2190546Z 	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
2020-05-15T09:23:23.2191178Z 	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2020-05-15T09:23:23.2191702Z 	at java.lang.Thread.run(Thread.java:748)
2020-05-15T09:23:23.2192357Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: b0e54b0a1c99b57e04ec32d7879437c4)
2020-05-15T09:23:23.2193084Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-05-15T09:23:23.2193645Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-05-15T09:23:23.2194213Z 	at org.apache.flink.table.api.internal.BatchTableEnvImpl.execute(BatchTableEnvImpl.scala:319)
2020-05-15T09:23:23.2194827Z 	... 11 more
2020-05-15T09:23:23.2196472Z Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: b0e54b0a1c99b57e04ec32d7879437c4)
2020-05-15T09:23:23.2198222Z 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:116)
2020-05-15T09:23:23.2198778Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-05-15T09:23:23.2199403Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-05-15T09:23:23.2199858Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-15T09:23:23.2200313Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-05-15T09:23:23.2200826Z 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$22(RestClusterClient.java:602)
2020-05-15T09:23:23.2201336Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-05-15T09:23:23.2201822Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-05-15T09:23:23.2202290Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-15T09:23:23.2224141Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-05-15T09:23:23.2224660Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:307)
2020-05-15T09:23:23.2225150Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-05-15T09:23:23.2225628Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-05-15T09:23:23.2226080Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-15T09:23:23.2226519Z 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
2020-05-15T09:23:23.2226971Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
2020-05-15T09:23:23.2227416Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-05-15T09:23:23.2227864Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-05-15T09:23:23.2228293Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-05-15T09:23:23.2228601Z 	... 1 more
2020-05-15T09:23:23.2228905Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-15T09:23:23.2229343Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-05-15T09:23:23.2229861Z 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:114)
2020-05-15T09:23:23.2230242Z 	... 19 more
2020-05-15T09:23:23.2230569Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-05-15T09:23:23.2231404Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:112)
2020-05-15T09:23:23.2232059Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-05-15T09:23:23.2232647Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:189)
2020-05-15T09:23:23.2233188Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:183)
2020-05-15T09:23:23.2233734Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:177)
2020-05-15T09:23:23.2234253Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:505)
2020-05-15T09:23:23.2234748Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:386)
2020-05-15T09:23:23.2235150Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-15T09:23:23.2235515Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-15T09:23:23.2235967Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-15T09:23:23.2236466Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-15T09:23:23.2236860Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-05-15T09:23:23.2237478Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-05-15T09:23:23.2238115Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-05-15T09:23:23.2238594Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-05-15T09:23:23.2238985Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-05-15T09:23:23.2239352Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-05-15T09:23:23.2239726Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-05-15T09:23:23.2240095Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-05-15T09:23:23.2240569Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-05-15T09:23:23.2240938Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-15T09:23:23.2241326Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-15T09:23:23.2241685Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-05-15T09:23:23.2242024Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-05-15T09:23:23.2242385Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-05-15T09:23:23.2242702Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-05-15T09:23:23.2243033Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-05-15T09:23:23.2243357Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-05-15T09:23:23.2243644Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-05-15T09:23:23.2243987Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-05-15T09:23:23.2244373Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-05-15T09:23:23.2244784Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-05-15T09:23:23.2245181Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-05-15T09:23:23.2246306Z Caused by: java.lang.Exception: The user defined 'open(Configuration)' method in class org.apache.flink.table.runtime.functions.python.PythonScalarFunctionFlatMap caused an exception: Failed to create stage bundle factory!
2020-05-15T09:23:23.2246989Z 	at org.apache.flink.runtime.operators.BatchTask.openUserCode(BatchTask.java:1352)
2020-05-15T09:23:23.2247464Z 	at org.apache.flink.runtime.operators.chaining.ChainedFlatMapDriver.openTask(ChainedFlatMapDriver.java:47)
2020-05-15T09:23:23.2247930Z 	at org.apache.flink.runtime.operators.BatchTask.openChainedTasks(BatchTask.java:1392)
2020-05-15T09:23:23.2248370Z 	at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:157)
2020-05-15T09:23:23.2248765Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713)
2020-05-15T09:23:23.2249133Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539)
2020-05-15T09:23:23.2249456Z 	at java.lang.Thread.run(Thread.java:748)
2020-05-15T09:23:23.2249774Z Caused by: java.lang.RuntimeException: Failed to create stage bundle factory!
2020-05-15T09:23:23.2250246Z 	at org.apache.flink.python.AbstractPythonFunctionRunner.createStageBundleFactory(AbstractPythonFunctionRunner.java:197)
2020-05-15T09:23:23.2250755Z 	at org.apache.flink.python.AbstractPythonFunctionRunner.open(AbstractPythonFunctionRunner.java:164)
2020-05-15T09:23:23.2251342Z 	at org.apache.flink.table.runtime.runners.python.scalar.AbstractGeneralPythonScalarFunctionRunner.open(AbstractGeneralPythonScalarFunctionRunner.java:65)
2020-05-15T09:23:23.2252007Z 	at org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.open(AbstractPythonStatelessFunctionFlatMap.java:211)
2020-05-15T09:23:23.2252624Z 	at org.apache.flink.table.runtime.functions.python.AbstractPythonScalarFunctionFlatMap.open(AbstractPythonScalarFunctionFlatMap.java:69)
2020-05-15T09:23:23.2253173Z 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
2020-05-15T09:23:23.2253718Z 	at org.apache.flink.runtime.operators.BatchTask.openUserCode(BatchTask.java:1350)
2020-05-15T09:23:23.2253997Z 	... 6 more
2020-05-15T09:23:23.2254328Z Caused by: java.io.IOException: Cannot run program ""venv.zip/.conda/bin/python"": error=2, No such file or directory
2020-05-15T09:23:23.2254737Z 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
2020-05-15T09:23:23.2255178Z 	at org.apache.flink.python.util.PythonEnvironmentManagerUtils.execute(PythonEnvironmentManagerUtils.java:176)
2020-05-15T09:23:23.2255723Z 	at org.apache.flink.python.util.PythonEnvironmentManagerUtils.getSitePackagesPath(PythonEnvironmentManagerUtils.java:156)
2020-05-15T09:23:23.2256434Z 	at org.apache.flink.python.util.PythonEnvironmentManagerUtils.pipInstallRequirements(PythonEnvironmentManagerUtils.java:98)
2020-05-15T09:23:23.2257046Z 	at org.apache.flink.python.env.ProcessPythonEnvironmentManager.createEnvironment(ProcessPythonEnvironmentManager.java:172)
2020-05-15T09:23:23.2257609Z 	at org.apache.flink.python.AbstractPythonFunctionRunner.createPythonExecutionEnvironment(AbstractPythonFunctionRunner.java:249)
2020-05-15T09:23:23.2258213Z 	at org.apache.flink.table.runtime.runners.python.AbstractPythonStatelessFunctionRunner.createExecutableStage(AbstractPythonStatelessFunctionRunner.java:158)
2020-05-15T09:23:23.2258821Z 	at org.apache.flink.python.AbstractPythonFunctionRunner.createStageBundleFactory(AbstractPythonFunctionRunner.java:195)
2020-05-15T09:23:23.2259172Z 	... 12 more
2020-05-15T09:23:23.2259405Z Caused by: java.io.IOException: error=2, No such file or directory
2020-05-15T09:23:23.2259710Z 	at java.lang.UNIXProcess.forkAndExec(Native Method)
2020-05-15T09:23:23.2259991Z 	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
2020-05-15T09:23:23.2260313Z 	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
2020-05-15T09:23:23.2260623Z 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
2020-05-15T09:23:23.2260875Z 	... 19 more
2020-05-15T09:23:23.2260980Z 
2020-05-15T09:23:23.2406372Z org.apache.flink.client.program.ProgramAbortException
2020-05-15T09:23:23.2406771Z 	at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:95)
2020-05-15T09:23:23.2407331Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-15T09:23:23.2407892Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-15T09:23:23.2408342Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-15T09:23:23.2408748Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-15T09:23:23.2409138Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)
2020-05-15T09:23:23.2409654Z 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)
2020-05-15T09:23:23.2410157Z 	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:148)
2020-05-15T09:23:23.2410664Z 	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:707)
2020-05-15T09:23:23.2411172Z 	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:245)
2020-05-15T09:23:23.2411577Z 	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:924)
2020-05-15T09:23:23.2412014Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1000)
2020-05-15T09:23:23.2412499Z 	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
2020-05-15T09:23:23.2413178Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1000)
2020-05-15T09:23:23.8192063Z Stopping taskexecutor daemon (pid: 89153) on host fv-az678.
2020-05-15T09:23:23.9113567Z Stopping standalonesession daemon (pid: 88857) on host fv-az678.
2020-05-15T09:23:24.0749389Z [FAIL] Test script contains errors.
2020-05-15T09:23:24.0757935Z Checking of logs skipped.
2020-05-15T09:23:24.0758481Z 
2020-05-15T09:23:24.0759921Z [FAIL] 'PyFlink end-to-end test' failed after 3 minutes and 24 seconds! Test exited with exit code 1
{code}
",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 01:00:17 UTC 2020,,,,,,,,,,"0|z0erz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/20 10:00;dian.fu;cc [~zhongwei];;;","15/May/20 12:13;rmetzger;I assigned you.

Another case on master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1377&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","15/May/20 14:19;rmetzger;Upgrading to blocker: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1385&view=results;;;","16/May/20 01:00;dian.fu;Merged to master via 01abc3f4b054315c8e8edb1f3efe3610d3a77edb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Failed to find the file"" in ""build_wheels"" stage",FLINK-17722,13305133,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,rmetzger,rmetzger,15/May/20 07:48,16/Oct/20 10:56,13/Jul/23 08:07,15/May/20 09:47,1.11.0,,,,,1.11.0,,,,API / Python,Build System / Azure Pipelines,,,,0,pull-request-available,test-stability,,,"CI https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1343&view=logs&j=fe7ebddc-3e2f-5c50-79ee-226c8653f218&t=b2830442-93c7-50ff-36f4-5b3e2dca8c83

{code}
Successfully built dill crcmod httplib2 hdfs oauth2client future avro-python3
Installing collected packages: six, pbr, mock, dill, typing, crcmod, numpy, pyarrow, python-dateutil, typing-extensions, fastavro, httplib2, protobuf, pymongo, docopt, idna, chardet, urllib3, requests, hdfs, pyparsing, pydot, pyasn1, pyasn1-modules, rsa, oauth2client, grpcio, future, avro-python3, pytz, apache-beam, cython
Successfully installed apache-beam-2.19.0 avro-python3-1.9.2.1 chardet-3.0.4 crcmod-1.7 cython-0.29.16 dill-0.3.1.1 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.29.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.9 mock-2.0.0 numpy-1.18.4 oauth2client-3.0.0 pbr-5.4.5 protobuf-3.11.3 pyarrow-0.15.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydot-1.4.1 pymongo-3.10.1 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2020.1 requests-2.23.0 rsa-4.0 six-1.14.0 typing-3.7.4.1 typing-extensions-3.7.4.2 urllib3-1.25.9
+ (( i++ ))
+ (( i<3 ))
+ (( i=0 ))
+ (( i<3 ))
+ /home/vsts/work/1/s/flink-python/dev/.conda/envs/3.5/bin/python setup.py bdist_wheel
Compiling pyflink/fn_execution/fast_coder_impl.pyx because it changed.
Compiling pyflink/fn_execution/fast_operations.pyx because it changed.
[1/2] Cythonizing pyflink/fn_execution/fast_coder_impl.pyx
[2/2] Cythonizing pyflink/fn_execution/fast_operations.pyx
Failed to find the file /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/opt/flink-sql-client_*.jar.
{code}",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 12:05:56 UTC 2020,,,,,,,,,,"0|z0ersw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/20 08:20;dian.fu;[~rmetzger] Thanks a lot for reporting this issue. I will take a look at this problem ASAP.;;;","15/May/20 09:47;dian.fu;Merged to master via 38058087a35d2ae94abd89a2c0592f776c6e16b4;;;","15/May/20 09:55;rmetzger;Thank you for the quick fix!;;;","27/May/20 12:05;dian.fu;Merged the followup fix (otherwise the Python wheel package built in azure doesn't contain all the jars under the lib directory) to release-1.11: 
 c3f117986db7ca0b5a2ffb4a938d515021339b48 and aeef20744dc1a867c447e489be8c009bfa7be246;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractHadoopFileSystemITTest .cleanupDirectoryWithRetry fails with AssertionError ,FLINK-17721,13305132,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xtsong,rmetzger,rmetzger,15/May/20 07:44,16/Oct/20 10:54,13/Jul/23 08:07,25/May/20 12:18,,,,,,1.11.0,,,,FileSystems,Tests,,,,0,pull-request-available,,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1343&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=2f99feaa-7a9b-5916-4c1c-5e61f395079e

{code}
[ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 34.079 s <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase
[ERROR] org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase  Time elapsed: 21.334 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.flink.runtime.fs.hdfs.AbstractHadoopFileSystemITTest.cleanupDirectoryWithRetry(AbstractHadoopFileSystemITTest.java:162)
	at org.apache.flink.runtime.fs.hdfs.AbstractHadoopFileSystemITTest.teardown(AbstractHadoopFileSystemITTest.java:149)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,aljoscha,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 25 12:18:58 UTC 2020,,,,,,,,,,"0|z0erso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/20 07:30;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1713&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","20/May/20 18:00;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1956&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","21/May/20 02:16;xtsong;The test failure is caused by {{cleanupDirectoryWithRetry}} not deleting the files if the deadline is already reached.

Deleting files from a remote file system might fail due to, e.g., network problems. Therefore, {{cleanupDirectoryWithRetry}} keeps trying to delete the files until success or reaching a deadline. The deadline is defined in {{setup}} before executing all test cases. If executing tests takes too long, that the deadline is reached before calling {{cleanupDirectoryWithRetry}}, the files will not be deleted leading to the reported failure.

To fix the problem, we have several options.

# The easiest way is to make sure {{cleanupDirectoryWithRetry}} at least try to delete the files once, no matter the deadline is reached or not. The shortcoming is that, we may still suffer from the deletion failures due to network issues or asynchronism (which FLINK-13483 tries to solve). So practically, this test case might be still unstable, just less unstable than now.
# We can also set a separate timeout or retry limit for {{cleanupDirectoryWithRetry}}. This decouples the handling of each potential deletion failure apart from the overall test execution time.
# The cleanest solution, IMO, is to get rid of the deadlines. Currently, the deadline is used in {{checkPathExistence}} and {{cleanupDirectoryWithRetry}} as a timeout for verifying the FS status. However, it does not make good sense to me that all the checks in the class share a common deadline. We should have an independent timeout for each check.

I'm in favor of option 3. My only concern is that, this change might affect all the subclasses of {{AbstractHadoopFileSystemITTest}}. Currently, Flink integrate file systems with its plugin mechanism. That means users may provide their custom FS plugins. If a user extended {{AbstractHadoopFileSystemITTest}} for testing his custom plugin, our change might lead to compatibility problems. So the questions is, *do we want to maintain backwards compatibility for the testing base class of the FS framework?* From my side, I would not consider {{AbstractHadoopFileSystemITTest}} as public interface, thus would be ok with any modification to it. But I think it would be good to collect more opinions.;;;","21/May/20 02:47;xtsong;One more comment on option 3.
{{AbstractHadoopFileSystemITTest}} is not annotated with {{@Public}} or {{@PublicEvolving}}. So practically we have not promise any kind of compatibility on it.;;;","22/May/20 02:07;xtsong;cc [~aljoscha] [~pnowojski] [~kkl0u];;;","22/May/20 12:43;aljoscha;Yes, I think option 3) is the best option. I recently increased the deadline for FLINK-16911 but I also thought that it's a bit weird how it's used in the tests.;;;","25/May/20 02:15;xtsong;[~aljoscha]
Thanks for the feedback. I've opened a PR for option 3). Could you help take a look?;;;","25/May/20 12:18;aljoscha;release-1.11: 6215e1166aa44ce178ae8c068159eb7efeb53041
master: e92b2bf19bdf03ad3bae906dc5fa3781aeddb3ee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throws for DDL create temporary system function with composite table path,FLINK-17717,13305117,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,danny0405,danny0405,danny0405,15/May/20 06:33,05/Jun/20 02:32,13/Jul/23 08:07,05/Jun/20 02:32,1.11.0,,,,,1.11.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"Currently, we support syntax 
{code:sql}
create temporary system function catalog.db.func_name as function_class
{code}
But actually we drop the catalog and db silently, the temporary system function never has custom table paths, it belongs always to the system and current session, so, we should limit the table path to simple identifier.",,danny0405,dwysakowicz,godfreyhe,jark,leonard,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14841,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 02:32:46 UTC 2020,,,,,,,,,,"0|z0erpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/20 07:37;dwysakowicz;Very good point!;;;","05/Jun/20 02:32;lzljs3620320;master: f5ba41f90411429b6f35b6e8f2968def124bc96e

release-1.11: edfe844db645b7dc1451503662ee86123473fb1e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Stateful stream job upgrade end-to-end test"" fails",FLINK-17713,13305109,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,rmetzger,rmetzger,15/May/20 05:36,15/May/20 05:45,13/Jul/23 08:07,15/May/20 05:45,,,,,,,,,,Runtime / Checkpointing,,,,,0,,,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1348&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

Potentially caused by this:
{code}
2020-05-15T04:46:20.7037837Z 2020-05-15 04:46:11,134 WARN  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Fail to subsume the old checkpoint.
2020-05-15T04:46:20.7038858Z java.io.IOException: Directory /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-56851042201/savepoint-e2e-test-chckpt-dir/33e34191d2f3c84d9b7eb5898d3a34fc/chk-3 is not empty
2020-05-15T04:46:20.7039955Z 	at org.apache.flink.core.fs.local.LocalFileSystem.delete(LocalFileSystem.java:192) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-15T04:46:20.7041042Z 	at org.apache.flink.runtime.state.filesystem.FsCompletedCheckpointStorageLocation.disposeStorageLocation(FsCompletedCheckpointStorageLocation.java:70) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-15T04:46:20.7042317Z 	at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.doDiscard(CompletedCheckpoint.java:264) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-15T04:46:20.7043438Z 	at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.discardOnSubsume(CompletedCheckpoint.java:219) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-15T04:46:20.7044540Z 	at org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.addCheckpoint(StandaloneCompletedCheckpointStore.java:72) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-15T04:46:20.7045684Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1003) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-15T04:46:20.7047005Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:910) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-15T04:46:20.7048039Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.lambda$acknowledgeCheckpoint$4(SchedulerBase.java:802) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-15T04:46:20.7048663Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_252]
2020-05-15T04:46:20.7049114Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_252]
2020-05-15T04:46:20.7049643Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_252]
2020-05-15T04:46:20.7050291Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_252]
2020-05-15T04:46:20.7057390Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_252]
2020-05-15T04:46:20.7058064Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_252]
2020-05-15T04:46:20.7058510Z 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_252]
2
{code}",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17467,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 05:37:45 UTC 2020,,,,,,,,,,"0|z0ernk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/20 05:37;rmetzger;Reverted in https://github.com/apache/flink/commit/3af17562eb791e3f486c38dbd94dc3328309b262;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default execution command fails due 'benchmark' profile being inactive,FLINK-17703,13305049,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,14/May/20 21:03,18/May/20 08:02,13/Jul/23 08:07,18/May/20 08:02,1.11.0,,,,,1.11.0,,,,Benchmarks,,,,,0,,,,,"FLINK-17057 had some unfortunate side effects: by having the ""{{include-netty-tcnative-dynamic""}} profile active by default, the ""{{benchmark""}} profile was not active any more. Thus the following command that was typically used for running the benchmarks failed unless the ""{{benchmark""}} profile was activated manually like this:
{code:java}
mvn -Dflink.version=1.11-SNAPSHOT clean package exec:exec -P benchmark{code}",,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 08:02:15 UTC 2020,,,,,,,,,,"0|z0era8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 08:02;nkruber;Fixed in [https://github.com/dataArtisans/flink-benchmarks] (master) via 8de66edad;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude jdk:tools dependency from all Hadoop dependencies for Java 9+ compatibility,FLINK-17701,13304976,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sewen,sewen,14/May/20 15:35,15/May/20 09:39,13/Jul/23 08:07,15/May/20 08:56,,,,,,1.11.0,,,,Build System,,,,,0,pull-request-available,,,,"Hadoop transitively pulls the system dependency {{jdk:tools}} which is not longer available on Java 9+. This causes errors when importing the code into an IDE with runs Java 11.

This dependency is anyways not needed when running the code, because the classes are always present. It can be safely excluded form the transitive dependencies.",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-11086,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 08:56:59 UTC 2020,,,,,,,,,,"0|z0equ0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/20 08:56;sewen;Fixed in master (1.11.0) via bff623a22807bf6fe70d048afef469981a0cf16a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The callback client of JavaGatewayServer should run in a daemon thread,FLINK-17700,13304971,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,csq,csq,14/May/20 15:19,16/Oct/20 10:36,13/Jul/23 08:07,18/May/20 03:07,1.10.0,1.9.0,,,,1.10.2,1.9.4,,,API / Python,,,,,0,pull-request-available,,,,"When starting the JavaGatewayServer, a callback client would also be created and run in an ScheduleExecutorService's thread which is non-daemon by default. Thus, we need to shut down the non-daemon thread and reset a daemon thread back to the call back client.",,csq,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 03:07:53 UTC 2020,,,,,,,,,,"0|z0eqsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 03:07;dian.fu;Merged to
- release-1.10 via 2bc4628876715698c9bae542e36014344a93c42b
- release-1.9 via 6ce59a8719d721f40aaf6551a91489a74683ddc5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"JDK11: JdbcFullTest.testEnrichedClassCastException: expected:<[java.lang.String cannot be cast to java.lang.Double], field index: 3, fi...> but was:<[class java.lang.String",FLINK-17697,13304962,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,rmetzger,rmetzger,14/May/20 14:37,15/May/20 06:21,13/Jul/23 08:07,15/May/20 06:21,,,,,,1.11.0,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,"{code}

	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1216&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=603cb7fd-6f38-5c99-efca-877e1439232f",,jark,leonard,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 06:21:38 UTC 2020,,,,,,,,,,"0|z0eqqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/20 14:56;jark;There are 2 failure cases:


{code:java}
2020-05-13T23:32:03.7853981Z [ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.161 s <<< FAILURE! - in org.apache.flink.connector.jdbc.JdbcOutputFormatTest
2020-05-13T23:32:03.7863947Z [ERROR] testExceptionOnInvalidType(org.apache.flink.connector.jdbc.JdbcOutputFormatTest)  Time elapsed: 0.038 s  <<< FAILURE!
2020-05-13T23:32:03.7866656Z org.junit.ComparisonFailure: expected:<[java.lang.Long cannot be cast to java.lang.Double], field index: 3, fi...> but was:<[class java.lang.Long cannot be cast to class java.lang.Double (java.lang.Long and java.lang.Double are in module java.base of loader 'bootstrap')], field index: 3, fi...>
2020-05-13T23:32:03.7868280Z 	at org.junit.Assert.assertEquals(Assert.java:115)
2020-05-13T23:32:03.7868898Z 	at org.junit.Assert.assertEquals(Assert.java:144)
2020-05-13T23:32:03.7869597Z 	at org.apache.flink.connector.jdbc.JdbcOutputFormatTest.testExceptionOnInvalidType(JdbcOutputFormatTest.java:182)
2020-05-13T23:32:03.7870363Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-13T23:32:03.7871102Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-13T23:32:03.7871964Z 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-13T23:32:03.7872772Z 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2020-05-13T23:32:03.7873462Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-13T23:32:03.7874342Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-13T23:32:03.7875118Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-13T23:32:03.7875818Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-13T23:32:03.7876655Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-13T23:32:03.7877360Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-13T23:32:03.7878007Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-13T23:32:03.7878774Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-13T23:32:03.7879480Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-13T23:32:03.7880157Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-13T23:32:03.7880755Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-13T23:32:03.7881412Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-13T23:32:03.7882075Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-13T23:32:03.7882703Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-13T23:32:03.7883304Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-13T23:32:03.7883836Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-13T23:32:03.7884503Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-13T23:32:03.7885080Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-13T23:32:03.7885718Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-13T23:32:03.7886481Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-13T23:32:03.7887111Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-13T23:32:03.7887762Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-13T23:32:03.7888377Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-13T23:32:03.7889146Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2020-05-13T23:32:03.7889886Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2020-05-13T23:32:03.7890717Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
2020-05-13T23:32:03.7891506Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
2020-05-13T23:32:03.7892543Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2020-05-13T23:32:03.7893483Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2020-05-13T23:32:03.7894404Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-13T23:32:03.7895246Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-13T23:32:03.7896530Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-13T23:32:03.7897181Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-13T23:32:03.7897599Z 
2020-05-13T23:32:03.7984319Z [INFO] Running org.apache.flink.connector.jdbc.table.JdbcTableSourceSinkFactoryTest
2020-05-13T23:32:04.0204253Z [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.221 s - in org.apache.flink.connector.jdbc.table.JdbcTableSourceSinkFactoryTest
2020-05-13T23:32:04.0262391Z [INFO] Running org.apache.flink.connector.jdbc.internal.JdbcTableOutputFormatTest
2020-05-13T23:32:04.1755030Z [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.15 s - in org.apache.flink.connector.jdbc.internal.JdbcTableOutputFormatTest
2020-05-13T23:32:04.1806654Z [INFO] Running org.apache.flink.connector.jdbc.internal.JdbcFullTest
2020-05-13T23:32:06.3940218Z [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.855 s - in org.apache.flink.connector.jdbc.table.JdbcAppendOnlyWriterTest
2020-05-13T23:32:06.3958227Z [INFO] Running org.apache.flink.connector.jdbc.catalog.PostgresTablePathTest
2020-05-13T23:32:06.3981378Z [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in org.apache.flink.connector.jdbc.catalog.PostgresTablePathTest
2020-05-13T23:32:07.5401216Z [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.356 s <<< FAILURE! - in org.apache.flink.connector.jdbc.internal.JdbcFullTest
2020-05-13T23:32:07.5402565Z [ERROR] testEnrichedClassCastException(org.apache.flink.connector.jdbc.internal.JdbcFullTest)  Time elapsed: 0.055 s  <<< FAILURE!
2020-05-13T23:32:07.5404056Z org.junit.ComparisonFailure: expected:<[java.lang.String cannot be cast to java.lang.Double], field index: 3, fi...> but was:<[class java.lang.String cannot be cast to class java.lang.Double (java.lang.String and java.lang.Double are in module java.base of loader 'bootstrap')], field index: 3, fi...>
2020-05-13T23:32:07.5405175Z 	at org.junit.Assert.assertEquals(Assert.java:115)
2020-05-13T23:32:07.5405828Z 	at org.junit.Assert.assertEquals(Assert.java:144)
2020-05-13T23:32:07.5406619Z 	at org.apache.flink.connector.jdbc.internal.JdbcFullTest.testEnrichedClassCastException(JdbcFullTest.java:104)
2020-05-13T23:32:07.5407102Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-13T23:32:07.5407700Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-13T23:32:07.5408632Z 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-13T23:32:07.5409328Z 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2020-05-13T23:32:07.5409992Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-13T23:32:07.5410557Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-13T23:32:07.5411001Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-13T23:32:07.5411458Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-13T23:32:07.5411890Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-13T23:32:07.5412325Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-13T23:32:07.5412737Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-13T23:32:07.5413421Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-13T23:32:07.5413876Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-13T23:32:07.5414268Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-13T23:32:07.5414653Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-13T23:32:07.5415068Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-13T23:32:07.5415608Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-13T23:32:07.5416090Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-13T23:32:07.5416457Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-13T23:32:07.5416811Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-13T23:32:07.5417126Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-13T23:32:07.5417479Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-13T23:32:07.5417852Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-13T23:32:07.5418246Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-13T23:32:07.5418698Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-13T23:32:07.5419070Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-13T23:32:07.5419522Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-13T23:32:07.5419899Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2020-05-13T23:32:07.5420367Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2020-05-13T23:32:07.5420873Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
2020-05-13T23:32:07.5421547Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
2020-05-13T23:32:07.5422038Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2020-05-13T23:32:07.5422494Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2020-05-13T23:32:07.5423011Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-13T23:32:07.5423512Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-13T23:32:07.5423964Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-13T23:32:07.5424401Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-13T23:32:07.5424643Z 
{code}
;;;","14/May/20 16:01;leonard;[~rmetzger] [~jark] 

Thanks for your report.

I open a PR.;;;","15/May/20 06:21;jark;Fixed in master (1.11.0): 81d3e62884ee07266d5d4f54550e26e52dc7c529;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong min-length check in SimpleVersionedSerialization,FLINK-17694,13304947,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,14/May/20 13:54,16/May/20 15:30,13/Jul/23 08:07,16/May/20 15:29,1.10.0,,,,,1.11.0,,,,API / Core,,,,,0,,,,,"The SimpleVersionedSerialization checks for a minimum of 4 bytes when checking the arguments, but needs at least 8 bytes (two integers).",,sewen,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 15:29:58 UTC 2020,,,,,,,,,,"0|z0eqnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/20 15:29;sewen;Fixed in 1.11.0 via 3ba9b38cd750d12db759007235cc7f8497ee4170;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""flink-end-to-end-tests/test-scripts/hadoop/yarn.classpath"" present after building Flink",FLINK-17692,13304937,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,14/May/20 13:28,16/Oct/20 10:36,13/Jul/23 08:07,18/May/20 07:16,1.11.0,,,,,1.11.0,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"Some changes introduced in FLINK-11086 cause the ""flink-end-to-end-tests/test-scripts/hadoop/yarn.classpath"" file to be generated and present in the source tree after building Flink.

",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 07:16:41 UTC 2020,,,,,,,,,,"0|z0eqlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 07:16;rmetzger;Fixed in https://github.com/apache/flink/commit/1b42120ef62277d30210e4273903bd6020492a9f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaProducer transactional.id too long when using Semantic.EXACTLY_ONCE,FLINK-17691,13304936,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jmathews3773,freezhan,freezhan,14/May/20 13:24,22/May/21 03:04,13/Jul/23 08:07,24/Nov/20 08:58,1.10.0,1.11.0,,,,1.12.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"When sink to Kafka using the {color:#FF0000}Semantic.EXACTLY_ONCE {color}mode.

The flink Kafka Connector Producer will auto set the {color:#FF0000}transactional.id{color}, and the user - defined value are ignored.

 

When the job operator name too long, will send failed

transactional.id is exceeds the kafka  {color:#FF0000}coordinator_key{color} limit

!image-2020-05-14-21-09-01-906.png!

 

*The flink Kafka Connector policy for automatic generation of transaction.id is as follows*

 

1. use the {color:#FF0000}taskName + ""-"" + operatorUniqueID{color} as transactional.id prefix (may be too long)

  getRuntimeContext().getTaskName() + ""-"" + ((StreamingRuntimeContext)    getRuntimeContext()).getOperatorUniqueID()

2. Range of available transactional ids 

[nextFreeTransactionalId, nextFreeTransactionalId + parallelism * kafkaProducersPoolSize)

!image-2020-05-14-20-43-57-414.png!

  !image-2020-05-14-20-45-24-030.png!

!image-2020-05-14-20-45-59-878.png!

 

*The Kafka transaction.id check policy as follows:*

 

{color:#FF0000}string bytes.length can't larger than Short.MAX_VALUE (32767){color}

!image-2020-05-14-21-16-43-810.png!

!image-2020-05-14-21-17-09-784.png!

 

*To reproduce this bug, the following conditions must be met:*

 
 # send msg to kafka with exactly once mode
 # the task TaskName' length + TaskName's length is lager than the 32767 (A very long line of SQL or window statements can appear)

*I suggest a solution:*

 

     1.  Allows users to customize transactional.id 's prefix

or

     2. Do md5 on the prefix before returning the real transactional.id

 

 

 ",,aljoscha,freezhan,jmathews3773,libenchao,nvolynets,Paul Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20227,,,,,,,,,,,,,,,,FLINK-22452,,,,,,,,,"14/May/20 12:44;freezhan;image-2020-05-14-20-43-57-414.png;https://issues.apache.org/jira/secure/attachment/13002938/image-2020-05-14-20-43-57-414.png","14/May/20 12:45;freezhan;image-2020-05-14-20-45-24-030.png;https://issues.apache.org/jira/secure/attachment/13002937/image-2020-05-14-20-45-24-030.png","14/May/20 12:46;freezhan;image-2020-05-14-20-45-59-878.png;https://issues.apache.org/jira/secure/attachment/13002936/image-2020-05-14-20-45-59-878.png","14/May/20 13:09;freezhan;image-2020-05-14-21-09-01-906.png;https://issues.apache.org/jira/secure/attachment/13002935/image-2020-05-14-21-09-01-906.png","14/May/20 13:16;freezhan;image-2020-05-14-21-16-43-810.png;https://issues.apache.org/jira/secure/attachment/13002934/image-2020-05-14-21-16-43-810.png","14/May/20 13:17;freezhan;image-2020-05-14-21-17-09-784.png;https://issues.apache.org/jira/secure/attachment/13002933/image-2020-05-14-21-17-09-784.png",,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 22 03:04:38 UTC 2021,,,,,,,,,,"0|z0eql4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/20 15:53;freezhan;Do md5 on the transactional.id prefix

[https://github.com/apache/flink/pull/12157];;;","19/Nov/20 18:57;jmathews3773;hey [~aljoscha] I am happy to submit a PR to fix this bug, can I simply submit one that truncates the taskName? I think we can truncate either in the TransactionIdGenerator or in the TaskInfo constructor itself, depending on if we want the limit to apply everywhere or not.;;;","19/Nov/20 21:11;jmathews3773;Submitted a fix, https://github.com/apache/flink/pull/14144;;;","24/Nov/20 08:58;aljoscha;master: 8e402832caae9ea40d665cecb7b09204e1530e03;;;","22/Dec/20 12:26;nvolynets;Hi [~freezhan] & [~jmathews3773],

Looks still exists or/and I am missing something. Below are details.

 

Basically have bumped with two issues:
 * first one directly related with this one
 * second - indirectly related

*Issue #1*

+Regarding+

_>> Do md5 on the transactional.id prefix_

+Details+

Flink version:

// build.gradle
{code:java}
ext {
  ...
  flinkVersion = '1.12.0'
  scalaBinaryVersion = '2.11'
  ...
}

dependencies {
  ...
  implementation ""org.apache.flink:flink-streaming-java_${scalaBinaryVersion}:${flinkVersion}""
  ...
}{code}
// App 
{code:java}
public static void main(String[] args) {
  ...
  env.enableCheckpointing(10000);
  env.setStateBackend(new RocksDBStateBackend(""file:///.../config/checkpoints/rocksdb"", true));
  env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
  ...
  FlinkKafkaConsumer<Record> consumer = createConsumer(conf);
  FlinkKafkaProducer<Record> producer = createProducer(conf);


  env
        .addSource(consumer)
        .uid(""kafka-consumer"")
        .addSink(producer)
        .uid(""kafka-producer"")
  ;

  env.execute();
}

public static FlinkKafkaProducer<Record> createProducer(Configuration conf) {
  ...
  FlinkKafkaProducer<Record> producer = new FlinkKafkaProducer<>(topicDefault, new RecordKafkaSerSchema(true), props, FlinkKafkaProducer.Semantic.EXACTLY_ONCE);
  ...
  return producer;
}{code}
// Logs (of App executed/submitted locally from IDE)
{code:java}
// code placeholder
2020-12-22 13:52:08 [ForkJoinPool.commonPool-worker-9] INFO  ProducerConfig:347 - ProducerConfig values: 
  ...
  transactional.id = Source: Custom Source -> Sink: Unnamed-e2b2f358d45860e6d949c8f7417842d6-24
  ...{code}
+Summary+

As we can see transaction-id PREFIX is not md5 as stated above (or I am missing something). It looks that issue should be reopened as it is expected to be fixed in 1.12.0.

 

*Issue #2*

+Regarding+

> 1. use the {color:#ff0000}taskName + ""-"" + operatorUniqueID{color} as transactional.id prefix (may be too long)

In reality `uid` specified after `source` & `sink` are ignored. But specifying of them are highly recommended in Flink official documentation:

[https://ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/savepoints.html#assigning-operator-ids]

Moreover as a workaround there is a possibility to specify `source` name & it is NOT ignored.

But there is NO possibility provided by Flink Java API to specify `sink` name.

+Details+

// build.gradle
{code:java}
ext {
  ...
  flinkVersion = '1.12.0'
  scalaBinaryVersion = '2.11'
  ...
}

dependencies {
  ...
  implementation ""org.apache.flink:flink-streaming-java_${scalaBinaryVersion}:${flinkVersion}""
  ...
}{code}
// App - `uid` are ignored
{code:java}
public static void main(String[] args) {
  ...
  env.enableCheckpointing(10000);
  env.setStateBackend(new RocksDBStateBackend(""file:///.../config/checkpoints/rocksdb"", true));
  env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
  ...
  FlinkKafkaConsumer<Record> consumer = createConsumer(conf);
  FlinkKafkaProducer<Record> producer = createProducer(conf);


  env
        .addSource(consumer)
        .uid(""kafka-consumer"") // is ignored
        .addSink(producer)
        .uid(""kafka-producer"") // is ignored
  ;

  env.execute();
}

public static FlinkKafkaProducer<Record> createProducer(Configuration conf) {
  ...
  FlinkKafkaProducer<Record> producer = new FlinkKafkaProducer<>(topicDefault, new RecordKafkaSerSchema(true), props, FlinkKafkaProducer.Semantic.EXACTLY_ONCE);
  ...
  return producer;
}{code}
// Logs (of App executed/submitted locally from IDE) - `uid` are ignored
{code:java}
2020-12-22 13:52:08 [Source: Custom Source -> Sink: Unnamed (1/1)#0] INFO  ProducerConfig:347 - ProducerConfig values:  	
  ...
  transactional.id = Source: Custom Source -> Sink: Unnamed-e2b2f358d45860e6d949c8f7417842d6-20
  ...{code}
// App - specify `source`/`sink` names
{code:java}
public static void main(String[] args) {
  ...
  env.enableCheckpointing(10000);
  env.setStateBackend(new RocksDBStateBackend(""file:///.../config/checkpoints/rocksdb"", true));
  env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
  ...
  FlinkKafkaConsumer<Record> consumer = createConsumer(conf);
  FlinkKafkaProducer<Record> producer = createProducer(conf);


  env
        .addSource(consumer, ""kafka-consumer"")
        .addSink(producer) // NO way to specify name
  ;

  env.execute();
}

public static FlinkKafkaProducer<Record> createProducer(Configuration conf) {
  ...
  FlinkKafkaProducer<Record> producer = new FlinkKafkaProducer<>(topicDefault, new RecordKafkaSerSchema(true), props, FlinkKafkaProducer.Semantic.EXACTLY_ONCE);
  ...
  return producer;
}{code}
// Logs (of App executed/submitted locally from IDE) - specify `source`/`sink` names
{code:java}
2020-12-22 13:52:08 [Source: kafka-consumer -> Sink: Unnamed (1/1)#0] INFO  ProducerConfig:347 - ProducerConfig values:  	
  ...
  transactional.id = Source: kafka-consumer -> Sink: Unnamed-e2b2f358d45860e6d949c8f7417842d6-20
  ...{code}
+Summary+

As we can see `operatorUniqueID` specified after `source` and/or `sink` are ignored.

Moreover we can specify `source` name & it is taken into account but there is no possibility to do the same for `sink` via Flink Java API.

Should I create new/separate Jira issue for this use case or it is expect behaviour ?

Btw: this issue looks as CRITICAL as from official Flink documentation it blocks as follows:

>  It is *highly recommended* that you adjust your programs as described in this section in order to be able to upgrade your programs in the future.

[https://ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/savepoints.html#assigning-operator-ids];;;","22/Dec/20 12:48;nvolynets;[~freezhan] & [~jmathews3773],

*Issue #3*

+About+

> When sink to Kafka using the {color:#ff0000}Semantic.EXACTLY_ONCE {color}mode.

> The flink Kafka Connector Producer will auto set the {color:#ff0000}transactional.id{color}, and the user - defined value are ignored.

+Details+

There is nothing about such behaviour in official Flink documentation. Or I have missed it.

+Summary+

Should I create new/separate Jira issue to cover this gap ?;;;","22/Dec/20 13:10;nvolynets;[~freezhan] & [~jmathews3773],

*Issue #4*

Why not give a possibility to specify `transaction-id` manually ? What is the main reason ?;;;","28/Dec/20 10:09;aljoscha;Regarding Issues #1 and #2, these were proposed as potential fixes for this problem but in the end we went with the simplest solution, which is to just truncate the name. The Kafka transactional id and the operator UIDs that are mentioned in the documentation are not related, the latter is used only for mapping state to operators.

Regarding your issues #3 and #4, I think you're right. We could make the transactional Id *prefix* configurable, it's just that not enough users have asked for it so far. Or anyone really.;;;","22/May/21 03:04;freezhan;[~nvolynets] https://issues.apache.org/jira/browse/FLINK-22452;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collect TaskManager logs in Mesos Jepsen Tests,FLINK-17687,13304905,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gjy,gjy,gjy,14/May/20 10:05,16/Oct/20 10:49,13/Jul/23 08:07,18/May/20 17:41,1.11.0,,,,,1.11.0,,,,Tests,,,,,0,pull-request-available,,,,"TM logs are collected in standalone mode and YARN. However, for Mesos tests, TM logs are not collected at the end of the test. We should download all log files generated in the in the mesos agent directories.",,gjy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 17:41:15 UTC 2020,,,,,,,,,,"0|z0eqe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 17:41;gjy;1.11:
656d56e99e3c158c7252db04bc034cce77ad39ba
aa2a5709309ef8149607cc6ac696cd990a8aef81
2aa45cdcc88d75837df165fcde71200d796deee7

master:
be8c02e397943d668c4ff64e4c491a560136e2e1
12d662c9da2dc3e18fdd3d752ddeeb07df1f5945
ed74173c087fe879f5728b810e204bafb69bdae6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableEnvironment fromValues not work with map type and SQL,FLINK-17681,13304842,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,lzljs3620320,lzljs3620320,14/May/20 05:33,16/Oct/20 10:37,13/Jul/23 08:07,18/May/20 10:10,,,,,,1.11.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"{code:java}
Map<Integer, Integer> mapData = new HashMap<>();
      mapData.put(1, 1);
      mapData.put(2, 2);
      Row row = Row.of(mapData);
      tEnv().createTemporaryView(""values_t"", tEnv().fromValues(Collections.singletonList(row)));
      Iterator<Row> iter = tEnv().executeSql(""select * from values_t"").collect();

      List<Row> results = new ArrayList<>();
      while (iter.hasNext()) {
         results.add(iter.next());
      }
      System.out.println(results);
{code}
Not work, will occur exception:
{code:java}
java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType((INTEGER NOT NULL, INTEGER NOT NULL) MAP f0) NOT NULL
converted type:
RecordType((INTEGER NOT NULL, INTEGER NOT NULL) MAP NOT NULL f0) NOT NULL
{code}
If change to {{Iterator<Row> iter = tEnv().from(""values_t"").execute().collect();}} will work.",,dwysakowicz,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 10:10:39 UTC 2020,,,,,,,,,,"0|z0eq08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/20 05:35;lzljs3620320;BTW:
{code:java}
Row row = Row.of(1);
tEnv().createTemporaryView(""values_t"", tEnv().fromValues(
      DataTypes.ROW(DataTypes.FIELD(""f0"", DataTypes.INT())),
      Collections.singletonList(row)));
Iterator<Row> iter = tEnv().executeSql(""select * from values_t"").collect();
{code}
Not work too.;;;","14/May/20 05:36;lzljs3620320;CC: [~dwysakowicz] [~twalthr];;;","18/May/20 10:10;dwysakowicz;Fixed in 6de4f4a675e37ba7fd34a2c9114400fa2e5d25ad;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of encoding bytes in cython coder,FLINK-17679,13304823,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,14/May/20 03:17,14/May/20 07:07,13/Jul/23 08:07,14/May/20 07:07,1.11.0,,,,,1.11.0,,,,API / Python,,,,,0,pull-request-available,,,,"The python bytes b'x\x00\x00\x00' will be transposed to b'\x'. If we use strlen() of c function to compute the length of char*, we will get wrong length. So we need to use the Python function of len() to compute the length. ",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 07:07:47 UTC 2020,,,,,,,,,,"0|z0epw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/20 03:20;dian.fu;[~hxbks2ks] could you give an example in which case it will cause errors?;;;","14/May/20 03:38;hxbks2ks;The suffix of bytes is ​​\x00 bytes, such as the example of b'x\x00\x00\x00' I mentioned in the description;;;","14/May/20 07:07;dian.fu;Merged to master via 7b5f8536103ba557f68a7f31ebcc53629d85bd24;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorCoordinator state in checkpoints should always be a ByteStreamStateHandle,FLINK-17674,13304694,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,13/May/20 14:44,16/May/20 16:31,13/Jul/23 08:07,16/May/20 16:31,,,,,,1.11.0,,,,Runtime / Checkpointing,,,,,0,,,,,"State restore to the task vertices and coordinators (even after loading the Checkpoint Metadata) happens in the JobManager's main thread and must consequently not do any potentially blocking I/O operations.

The OperatorCoordinator state is a generic {{StreamStateHandle}} whose state might require I/O to retrieve. This never happens in the current implementation (we always use {{ByteStreamStateHandle}}) the signatures and contracts don't guarantee that and leave this open for a potential future bug.

Typing the OperatorCoordinator state to ByteStreamStateHandle makes sure that we can always retrieve the data directly without I/O and clarifies that no arbitrary StreamStateHandle is supported at that point. 

If state restoring becomes an asynchronous operation we can relax this restriction.",,liyu,openinx,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 16:31:45 UTC 2020,,,,,,,,,,"0|z0ep3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/20 16:31;sewen;Fixes in 1.11.0 via
  - e3413106ccd030e44d5e4690430c841370a1f988;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Savepoint handling of ""non-restored"" state should also take OperatorCoordinator state into account",FLINK-17670,13304686,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,13/May/20 14:21,16/May/20 16:32,13/Jul/23 08:07,16/May/20 16:32,,,,,,1.11.0,,,,Runtime / Checkpointing,,,,,0,,,,,"The savepoint restore should fail when there is unmatched state from an OperatorCoordinator only, and non-restored state is not allowed.

The relevant validation logic is in the {{org.apache.flink.runtime.checkpoint.Checkpoints}} class.",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 16:32:29 UTC 2020,,,,,,,,,,"0|z0ep1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/20 16:32;sewen;Fixed in 1.11.0 via

 (refactor) 90e4708c9a5ab24b5f50423a181d75be375e2d1e
 (feature) 382cf09bbfb4e383e373af8ebf1a9dee13b8a632;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insert into partitioned table can fail with select *,FLINK-17666,13304658,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,danny0405,lirui,lirui,13/May/20 13:08,16/Jun/20 08:49,13/Jul/23 08:07,16/Jun/20 08:49,,,,,,1.11.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"The following test
{code}
	@Test
	public void test() throws Exception {
		hiveShell.execute(""create table src (x int,y string)"");
		hiveShell.insertInto(""default"", ""src"").addRow(1, ""a"").commit();
		hiveShell.execute(""create table dest (x int) partitioned by (p1 int,p2 string)"");
		TableEnvironment tableEnvironment = getTableEnvWithHiveCatalog();
		tableEnvironment.executeSql(""insert into dest partition (p1=1) select * from src"")
				.getJobClient().get().getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();
	}
{code}
Fails with
{noformat}
org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink test-catalog.default.dest do not match.
Query schema: [x: INT, y: VARCHAR(2147483647), EXPR$2: INT NOT NULL]
Sink schema: [x: INT, p1: INT, p2: VARCHAR(2147483647)]

	at org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSchemaAndApplyImplicitCast(TableSinkUtils.scala:95)
	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:199)
	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:191)
	at scala.Option.map(Option.scala:146)
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:191)
	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:150)
	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:150)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:150)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1202)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:687)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:773)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:677)
......
{noformat}

However, the same DML passes if I change ""select *"" to ""select x,y"" in the query.",,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 08:49:01 UTC 2020,,,,,,,,,,"0|z0eovc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 08:49;lzljs3620320;master: 3792f8f0868a3a3a41fb8badb5345891594ad172

release-1.11: fbae8fc97caa6ffd852963cec2979a4ebf08eb07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointBarrierUnaligner.getFlattenedChannelIndex can throw ArrayIndexOutOfBoundsException,FLINK-17663,13304641,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,pnowojski,pnowojski,13/May/20 12:39,14/May/20 05:43,13/Jul/23 08:07,14/May/20 05:43,,,,,,1.11.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"{{KeyedStateCheckpointingITCase}} with enabled unaligned checkpoints always life locks because of infinite failure loop caused by the following exception:

{noformat}
java.io.IOException: java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.flink.runtime.io.network.partition.consumer.InputChannel.checkError(InputChannel.java:224) ~[classes/:?]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.getNextBuffer(RemoteInputChannel.java:173) ~[classes/:?]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:637) ~[classes/:?]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:615) ~[classes/:?]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:603) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:105) ~[classes/:?]
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.waitAndGetNextData(UnionInputGate.java:193) ~[classes/:?]
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.getNextBufferOrEvent(UnionInputGate.java:168) ~[classes/:?]
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.pollNext(UnionInputGate.java:160) ~[classes/:?]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:110) ~[classes/:?]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:136) ~[classes/:?]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:340) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:206) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:196) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:553) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:525) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713) [classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539) [classes/:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
	Suppressed: java.lang.AssertionError: Test ineffective: Function cleanly finished without ever failing.
		at org.junit.Assert.fail(Assert.java:88) ~[junit-4.12.jar:4.12]
		at org.apache.flink.test.checkpointing.KeyedStateCheckpointingITCase$OnceFailingPartitionedSum.close(KeyedStateCheckpointingITCase.java:325) ~[test-classes/:?]
		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43) ~[classes/:?]
		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117) ~[classes/:?]
		at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:698) ~[classes/:?]
		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:630) ~[classes/:?]
		at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:537) ~[classes/:?]
		at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713) [classes/:?]
		at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539) [classes/:?]
		at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.getFlattenedChannelIndex(CheckpointBarrierUnaligner.java:256) ~[classes/:?]
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.access$000(CheckpointBarrierUnaligner.java:55) ~[classes/:?]
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner$ThreadSafeUnaligner.notifyBarrierReceived(CheckpointBarrierUnaligner.java:311) ~[classes/:?]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.onBuffer(RemoteInputChannel.java:472) ~[classes/:?]
	at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.decodeBufferOrEvent(CreditBasedPartitionRequestClientHandler.java:303) ~[classes/:?]
	at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.decodeMsg(CreditBasedPartitionRequestClientHandler.java:267) ~[classes/:?]
	at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelRead(CreditBasedPartitionRequestClientHandler.java:182) ~[classes/:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelRead(NettyMessageClientDecoderDelegate.java:115) ~[classes/:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1421) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:930) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:697) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-shaded-netty-4.1.39.Final-10.0.jar:?]
	... 1 more
{noformat}
",,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17682,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 05:43:54 UTC 2020,,,,,,,,,,"0|z0eork:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/20 05:43;pnowojski;Merged as 7e23ceb72a..0ca918da02;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix reading BIGINT UNSIGNED type field not work in JDBC,FLINK-17657,13304586,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,zhanglun,zhanglun,13/May/20 09:18,28/May/20 06:13,13/Jul/23 08:07,28/May/20 06:13,1.10.0,1.10.1,,,,1.11.0,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,"I use sql client read mysql table, but I found I can't read a table contain `BIGINT UNSIGNED` field. It will 
 Caused by: java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.lang.Long

 

MySQL table:

 

create table tb
 (
 id BIGINT UNSIGNED auto_increment 
 primary key,
 cooper BIGINT(19) null ,

user_sex VARCHAR(2) null 

);

 

my env yaml is env.yaml .",,danny0405,jark,JinxinTang,leonard,libenchao,wanglijie,yutaochina,zhanglun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/20 09:28;zhanglun;env.yaml;https://issues.apache.org/jira/secure/attachment/13002804/env.yaml","13/May/20 09:20;zhanglun;excetion.txt;https://issues.apache.org/jira/secure/attachment/13002801/excetion.txt",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 06:13:49 UTC 2020,,,,,,,,,,"0|z0eofc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/20 01:56;JinxinTang;BigInt in mysql has a bigger range than Long in java, you could use Decimal instead in java to parse.;;;","14/May/20 08:38;zhanglun;Thank for your replay. It seem not ok.I try and get the follow exception.

 

[ERROR] Could not execute SQL statement. Reason:
 java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.math.BigDecimal

 

It's seem the jdbc read the id fielf as java.math.BigInteger.And Sql DataType not support java.math.BigInteger only support (Long)

 ;;;","14/May/20 11:41;JinxinTang; 

hi, [~zhanglun], Could you please provide code detail to reproduce this issus, it seems ok in my side, here is my code piece: [code piece|https://github.com/TJX2014/flink/blob/master-flink17657-jdbc-bigint/flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/jdbc/JdbcReader.java];;;","20/May/20 08:26;danny0405;Hi [~zhanglun], would you plan to fix this in release 1.11.0 ?;;;","21/May/20 01:17;zhanglun;Hi [~danny0405] , I don't know how to fix it.It's  need change flink sql support data type.;;;","21/May/20 01:19;zhanglun;Hi [~JinxinTang]. Sorry I only can reproduce this in sql client.;;;","21/May/20 07:53;leonard;[~zhanglun] Thanks for the report, it's a bug from type mapping between  mysql and flink sql type exists in JDBC connector.

the root cause is that bigint unsigned range is {{0}} to {{18446744073709551615, }}{{bigint rang is }}{{-9223372036854775808}}{{ to }}{{9223372036854775807}}{{. so we cannot map a bigint unsigned to bigint(flink bigint has same range with mysql bigint)}}.

[~JinxinTang] 's work around[1] is read String  rather read the original BigInteger(jdbc treats bigint unsigned as java.math.BigInteger).

[1][https://github.com/TJX2014/flink/blob/master-flink17657-jdbc-bigint/flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/jdbc/JdbcReader.java#L49];;;","25/May/20 09:30;zhanglun;Awesome job!!!;;;","28/May/20 06:13;jark;master (1.12.0): 8bc76955b527b6905049346eb1eaa9ce1ab03409
1.11.0: beb0c1c5bab09feb525f2bd8c1352617072dd118;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Legacy JM heap options should fallback to new JVM_HEAP_MEMORY in standalone,FLINK-17652,13304550,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,azagrebin,azagrebin,azagrebin,13/May/20 07:49,15/May/20 09:56,13/Jul/23 08:07,15/May/20 09:56,,,,,,1.11.0,,,,Runtime / Configuration,Runtime / Coordination,,,,0,pull-request-available,,,,FLINK-16742 states that the legacy JM heap options should fallback to JobManagerOptions.JVM_HEAP_MEMORY in standalone scripts. BashJavaUtils#getJmResourceParams has been implemented to fallback to JobManagerOptions.TOTAL_FLINK_MEMORY. This should be fixed.,,azagrebin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16742,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 09:56:58 UTC 2020,,,,,,,,,,"0|z0eo7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/20 09:56;azagrebin;merged into master by 9b3732db57bb76c77222f548f4ea4931dd4237a4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DecomposeGroupingSetsRule generates wrong plan when there exist distinct agg and simple agg with same filter,FLINK-17651,13304549,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,icshuo,icshuo,icshuo,13/May/20 07:48,25/May/20 05:39,13/Jul/23 08:07,25/May/20 05:39,1.10.1,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Consider adding the following test case to org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase. As you can see, the actual result is wrong.

 
{code:java}
@Test
def testSimpleAndDistinctAggWithCommonFilter(): Unit = {
  val sql =
    """"""
      |SELECT
      |   h,
      |   COUNT(1) FILTER(WHERE d > 1),
      |   COUNT(1) FILTER(WHERE d < 2),
      |   COUNT(DISTINCT e) FILTER(WHERE d > 1)
      |FROM Table5
      |GROUP BY h
      |"""""".stripMargin
  checkResult(
    sql,
    Seq(
      row(1,4,1,4),
      row(2,7,0,7),
      row(3,3,0,3)
    )
  )
}

Results
 == Correct Result ==   == Actual Result ==
 1,4,1,4                    1,0,1,4
 2,7,0,7                    2,0,0,7
 3,3,0,3                    3,0,0,3
{code}
The problem lies in `DecomposeGroupingSetsRule`, which omits filter arg of aggregate call when doing some processing.

 ",,danny0405,icshuo,jark,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 25 05:39:21 UTC 2020,,,,,,,,,,"0|z0eo74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/20 08:21;danny0405;cc [~godfreyhe], can you help to review this PR, thanks ~;;;","25/May/20 05:39;lzljs3620320;release-1.11: 197f3a3c481a3dcdc0fa1661d2606fbae2fe3ef9

master: 9ccdf061f45e635c7e205bbacd9a2972a765dcde;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generated hash aggregate code may produce NPE when there exists an aggregate call with Filter.,FLINK-17649,13304543,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,icshuo,icshuo,icshuo,13/May/20 07:34,16/Oct/20 10:35,13/Jul/23 08:07,17/May/20 16:29,1.10.1,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When generating code for Filter predicate of aggregate call in `HashAggCodeGenHelper`, we should check the nullability of the boolean field firstly rather than `getBoolean` directly, otherwise, NPE may be produced.",,icshuo,jark,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 17 16:29:24 UTC 2020,,,,,,,,,,"0|z0eo60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/20 16:29;lzljs3620320;master: cc8bf3426a0d43560d3e55337ceab48f80db43f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"REAPER_THREAD.start() in SafetyNetCloseableRegistry failed, causing the repeated failover.",FLINK-17645,13304508,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,Zakelly,Zakelly,13/May/20 02:57,16/Oct/20 10:53,13/Jul/23 08:07,21/May/20 02:24,1.10.1,1.11.0,1.12.0,,,1.11.0,1.12.0,,,Runtime / Task,,,,,0,pull-request-available,,,,"I'm running a modified version of Flink, and encountered the exception below when task start:
{code:java}
2020-05-12 00:46:19,037 ERROR [***] org.apache.flink.runtime.taskmanager.Task   - Encountered an unexpected exception
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:802)
        at org.apache.flink.core.fs.SafetyNetCloseableRegistry.<init>(SafetyNetCloseableRegistry.java:73)
        at org.apache.flink.core.fs.FileSystemSafetyNet.initializeSafetyNetForThread(FileSystemSafetyNet.java:89)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:586)
        at java.lang.Thread.run(Thread.java:834)
2020-05-12 00:46:19,038 INFO  [***] org.apache.flink.runtime.taskmanager.Task 
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:802)
        at org.apache.flink.core.fs.SafetyNetCloseableRegistry.<init>(SafetyNetCloseableRegistry.java:73)
        at org.apache.flink.core.fs.FileSystemSafetyNet.initializeSafetyNetForThread(FileSystemSafetyNet.java:89)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:586)
        at java.lang.Thread.run(Thread.java:834)
{code}
The REAPER_THREAD.start() fails because of OOM, and REAPER_THREAD will never be null. Since then, every time SafetyNetCloseableRegistry init in this VM will cause an IllegalStateException:
{code:java}
java.lang.IllegalStateException
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:179)
	at org.apache.flink.core.fs.SafetyNetCloseableRegistry.<init>(SafetyNetCloseableRegistry.java:71)
	at org.apache.flink.core.fs.FileSystemSafetyNet.initializeSafetyNetForThread(FileSystemSafetyNet.java:89)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:586)
	at java.lang.Thread.run(Thread.java:834){code}
This may happen in very old version of Flink as well.",,gaoyunhaii,guoyangze,sewen,wanglijie,Zakelly,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 02:24:45 UTC 2020,,,,,,,,,,"0|z0eny8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/20 03:25;zhuzh;Thanks for reporting this issue [~Zakelly].
Does this problem still exist in 1.9 or 1.10?;;;","13/May/20 03:36;wanglijie;Hi [~zhuzh], yes, it still exists in 1.9 and 1.10. I think it should use try/catch code block and reset the REAPER_THREAD to null if catch OOM exception. I'm pleased to fix this bug, can you assign to me?;;;","13/May/20 03:56;zhuzh;[~wanglijie95] you are right that this issue can still happen in latest FLink versions.
I checked {{SafetyNetCloseableRegistry}}. The static {{REAPER_THREAD}} field will be set in the constructor but will not be nulled if OOM happens, which can lead to the state check failure.;;;","13/May/20 03:57;zhuzh;I have assigned the ticket to you [~wanglijie95].;;;","13/May/20 04:08;wanglijie;Thank you, [~zhuzh];;;","14/May/20 13:50;sewen;What are we gaining by special case handling the reaper thread here?
If the JVM reaches a point where it cannot create native threads any more, then the JVM is unusable.

Could we look for a generic way to ""exit JVM when thread creation fails""? Handling each thread creation specifically seems not feasible, tbh.;;;","15/May/20 03:47;zhuzh;[~sewen] I think this specific fix is valid because the root cause is the inconsistency of {{SafetyNetCloseableRegistry}} if any exception happens in its constructor. This is due to the REAPER_THREAD is static and is not properly reset on unexpected failures, which is not a common case.
In this case, a SafetyNetCloseableRegistry cannot be successfully created anymore even if the native thread creation could succeed later. And any exception thrown from {{REAPER_THREAD.start()}} (maybe not a native thread creation error) would also lead to this problem.

Regarding ""exit JVM when thread creation fails"", I'm not sure whether native thread creation failure is really not recoverable.
If it is, I think we can force the JVM to shutdown in such cases. But I do not have an idea yet that how we can apply it to all the thread creation processes without changing them all.;;;","15/May/20 06:57;wanglijie;Hi, [~sewen], just like [~zhuzh] 's comment, except OOM, maybe there are other errors and exceptions thrown when start the REAPER_THREAD, it will also meet this problem. And once it occured, we can only see the  ""java.lang.IllegalStateException"" lead to job failed, it's confused for user. To get the real reason,  users have to find the first exception at here, and then realized that the first one lead to the subsequent exceptions.;;;","15/May/20 09:00;sewen;I see, fair point. This is a static shared thread where we need to take care about leaks. Makes sense, +1;;;","21/May/20 02:24;zhuzh;Fixed via

master
9f3a71183ea4b14a396ecf66e4377da07b06a689

release-1.1
d40826d23c8993f46270922a40fe23379b3e166e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LaunchCoordinatorTest fails,FLINK-17643,13304432,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,arvid,AHeise,12/May/20 18:28,22/Jun/21 14:07,13/Jul/23 08:07,15/May/20 13:36,,,,,,1.10.2,1.11.0,1.9.4,,Deployment / Mesos,,,,,0,pull-request-available,test-stability,,,"Here is the [instance|https://dev.azure.com/arvidheise0209/arvidheise/_build/results?buildId=234&view=logs&j=764762df-f65b-572b-3d5c-65518c777be4&t=8d823410-c7c7-5a4d-68bb-fa7b08da17b9].

 
{noformat}
[ERROR] Tests run: 24, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 1.828 s <<< FAILURE! - in org.apache.flink.mesos.scheduler.LaunchCoordinatorTest
[ERROR] The LaunchCoordinator when in state GatheringOffers should handle StateTimeout which stays in GatheringOffers when task queue is non-empty(org.apache.flink.mesos.scheduler.LaunchCoordinatorTest)  Time elapsed: 0.021 s  <<< ERROR!
java.lang.IllegalStateException: cannot reserve actor name '$$u': terminating
	at akka.actor.dungeon.ChildrenContainer$TerminatingChildrenContainer.reserve(ChildrenContainer.scala:188)
	at akka.actor.dungeon.Children$class.reserveChild(Children.scala:135)
	at akka.actor.ActorCell.reserveChild(ActorCell.scala:429)
	at akka.testkit.TestActorRef.<init>(TestActorRef.scala:33)
	at akka.testkit.TestFSMRef.<init>(TestFSMRef.scala:40)
	at akka.testkit.TestFSMRef$.apply(TestFSMRef.scala:91)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest$Context.<init>(LaunchCoordinatorTest.scala:254)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$apply$mcV$sp$15$$anonfun$apply$mcV$sp$34$$anon$25.<init>(LaunchCoordinatorTest.scala:459)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$apply$mcV$sp$15$$anonfun$apply$mcV$sp$34.apply(LaunchCoordinatorTest.scala:459)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$apply$mcV$sp$15$$anonfun$apply$mcV$sp$34.apply(LaunchCoordinatorTest.scala:459)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078)
	at org.scalatest.TestSuite$class.withFixture(TestSuite.scala:196)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.withFixture(LaunchCoordinatorTest.scala:57)
	at org.scalatest.WordSpecLike$class.invokeWithFixture$1(WordSpecLike.scala:1075)
	at org.scalatest.WordSpecLike$$anonfun$runTest$1.apply(WordSpecLike.scala:1088)
	at org.scalatest.WordSpecLike$$anonfun$runTest$1.apply(WordSpecLike.scala:1088)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.WordSpecLike$class.runTest(WordSpecLike.scala:1088)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.runTest(LaunchCoordinatorTest.scala:57)
	at org.scalatest.WordSpecLike$$anonfun$runTests$1.apply(WordSpecLike.scala:1147)
	at org.scalatest.WordSpecLike$$anonfun$runTests$1.apply(WordSpecLike.scala:1147)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418){noformat}",,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 13:36:06 UTC 2020,,,,,,,,,,"0|z0enhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/20 07:25;trohrmann;The problem seems to be caused by 

{code}
1:55:03,470 [flink-akka.actor.default-dispatcher-3] ERROR akka.actor.LocalActorRefProvider(akka://flink)               [] - guardian failed, shutting down system
java.lang.NullPointerException: null
	at org.apache.flink.mesos.scheduler.LaunchCoordinator$$anonfun$5.applyOrElse(LaunchCoordinator.scala:208) ~[classes/:?]
	at org.apache.flink.mesos.scheduler.LaunchCoordinator$$anonfun$5.applyOrElse(LaunchCoordinator.scala:162) ~[classes/:?]
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[scala-library-2.11.12.jar:?]
	at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at org.apache.flink.mesos.scheduler.LaunchCoordinator.processEvent(LaunchCoordinator.scala:50) ~[classes/:?]
	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:638) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at org.apache.flink.mesos.scheduler.LaunchCoordinator.aroundReceive(LaunchCoordinator.scala:50) ~[classes/:?]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.testkit.CallingThreadDispatcher.process$1(CallingThreadDispatcher.scala:251) ~[akka-testkit_2.11-2.5.21.jar:2.5.21]
	at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:284) ~[akka-testkit_2.11-2.5.21.jar:2.5.21]
	at akka.testkit.CallingThreadDispatcher.dispatch(CallingThreadDispatcher.scala:208) ~[akka-testkit_2.11-2.5.21.jar:2.5.21]
	at akka.actor.dungeon.Dispatch$class.sendMessage(Dispatch.scala:142) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.sendMessage(ActorCell.scala:429) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.Cell$class.sendMessage(ActorCell.scala:350) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.sendMessage(ActorCell.scala:429) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LocalActorRef.$bang(ActorRef.scala:402) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.Scheduler$$anon$3.run(Scheduler.scala:171) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.testkit.CallingThreadDispatcher.executeTask(CallingThreadDispatcher.scala:213) ~[akka-testkit_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.MessageDispatcher.unbatchedExecute(AbstractDispatcher.scala:146) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.BatchingExecutor$class.execute(BatchingExecutor.scala:123) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.MessageDispatcher.execute(AbstractDispatcher.scala:86) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242]
11:55:03,475 [flink-akka.actor.default-dispatcher-3] ERROR akka.actor.OneForOneStrategy                                 [] - null
java.lang.NullPointerException: null
	at org.apache.flink.mesos.scheduler.LaunchCoordinator$$anonfun$5.applyOrElse(LaunchCoordinator.scala:208) ~[classes/:?]
	at org.apache.flink.mesos.scheduler.LaunchCoordinator$$anonfun$5.applyOrElse(LaunchCoordinator.scala:162) ~[classes/:?]
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[scala-library-2.11.12.jar:?]
	at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at org.apache.flink.mesos.scheduler.LaunchCoordinator.processEvent(LaunchCoordinator.scala:50) ~[classes/:?]
	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:638) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at org.apache.flink.mesos.scheduler.LaunchCoordinator.aroundReceive(LaunchCoordinator.scala:50) ~[classes/:?]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.testkit.CallingThreadDispatcher.process$1(CallingThreadDispatcher.scala:251) ~[akka-testkit_2.11-2.5.21.jar:2.5.21]
	at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:284) ~[akka-testkit_2.11-2.5.21.jar:2.5.21]
	at akka.testkit.CallingThreadDispatcher.dispatch(CallingThreadDispatcher.scala:208) ~[akka-testkit_2.11-2.5.21.jar:2.5.21]
	at akka.actor.dungeon.Dispatch$class.sendMessage(Dispatch.scala:142) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.sendMessage(ActorCell.scala:429) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.Cell$class.sendMessage(ActorCell.scala:350) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.sendMessage(ActorCell.scala:429) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LocalActorRef.$bang(ActorRef.scala:402) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.Scheduler$$anon$3.run(Scheduler.scala:171) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.testkit.CallingThreadDispatcher.executeTask(CallingThreadDispatcher.scala:213) ~[akka-testkit_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.MessageDispatcher.unbatchedExecute(AbstractDispatcher.scala:146) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.BatchingExecutor$class.execute(BatchingExecutor.scala:123) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.MessageDispatcher.execute(AbstractDispatcher.scala:86) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242]
{code};;;","13/May/20 07:38;trohrmann;I think the problem could be caused by concurrent access to the {{LaunchCoordinator}} as indicated by these lines

{code}
11:55:02,526 [main-ScalaTest-running-LaunchCoordinatorTest] INFO  org.apache.flink.mesos.scheduler.LaunchCoordinator           [] - Now gathering offers for at least 1 task(s).
11:55:02,619 [main-ScalaTest-running-LaunchCoordinatorTest] INFO  org.apache.flink.mesos.scheduler.LaunchCoordinator           [] - Processing 1 task(s) against 1 new offer(s) plus outstanding offers.
11:55:02,763 [main-ScalaTest-running-LaunchCoordinatorTest] INFO  org.apache.flink.mesos.scheduler.LaunchCoordinator           [] - Resources considered: (note: expired offers not deducted from below)
11:55:03,416 [   flink-scheduler-1] INFO  org.apache.flink.mesos.scheduler.LaunchCoordinator           [] - Processing 1 task(s) against 3 new offer(s) plus outstanding offers.
11:55:03,417 [   flink-scheduler-1] INFO  org.apache.flink.mesos.scheduler.LaunchCoordinator           [] - Resources considered: (note: expired offers not deducted from below)
{code};;;","13/May/20 08:19;trohrmann;The problem is that some other test of the same suite schedulers a timer action which triggers in another test leading to the concurrent access. The statement https://github.com/apache/flink/blob/master/flink-mesos/src/main/scala/org/apache/flink/mesos/scheduler/LaunchCoordinator.scala#L186 enqueues a {{StateTimeout}} message which only triggers later.;;;","13/May/20 08:37;trohrmann;It was actually not concurrent accesses but incomplete stubbing which caused the problem. Since the tests run a bit slower on AZP it ran into this situation.;;;","15/May/20 13:36;trohrmann;Fixed via 

1.11.0: f9d44281e84be5aa633a3b58739514e0b2a26d5c
1.10.2: 9d149c6b055bf05cba291340ca7e6f58f087ad0a
1.9.4: f32ea2f1d078bd519687cdf766fb18f1d76d8f63;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RecoveredInputChannelTest.testConcurrentReadStateAndProcessAndRelease() failed,FLINK-17640,13304408,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zjwang,arvid,AHeise,12/May/20 16:50,22/Jun/21 14:06,13/Jul/23 08:07,14/May/20 02:36,,,,,,1.11.0,,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,"Here is the instance [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1093&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d].

Easy to reproduce locally by running the test a few 100 times.
{noformat}
java.util.concurrent.ExecutionException: java.lang.AssertionError	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.submitTasksAndWaitForResults(RemoteInputChannelTest.java:1228)
	at org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannelTest.testConcurrentReadStateAndProcessAndRelease(RecoveredInputChannelTest.java:215)
	at org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannelTest.testConcurrentReadStateAndProcessAndRelease(RecoveredInputChannelTest.java:82)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannelTest.lambda$processRecoveredBufferTask$1(RecoveredInputChannelTest.java:257)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748){noformat}",,AHeise,pnowojski,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 02:36:05 UTC 2020,,,,,,,,,,"0|z0enc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/20 17:22;pnowojski;It looks like an incorrect test. Sometimes different {{IllegalStateException}} is being thrown to what is expected:

{noformat}
java.lang.IllegalStateException: Trying to read from released RecoveredInputChannel
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:195)
	at org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel.getNextRecoveredStateBuffer(RecoveredInputChannel.java:141)
	at org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel.getNextBuffer(RecoveredInputChannel.java:169)
	at org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannelTest.lambda$processRecoveredBufferTask$1(RecoveredInputChannelTest.java:246)
	at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)
	at java.util.concurrent.FutureTask.run(FutureTask.java)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
as opposed to expected ""Queried for a buffer after channel has been closed"".

Generally speaking I think this test might be a bit invalid - it shouldn't be expecting and relaying on {{IllegalStateException}} in any form. [~zjwang] what is the purpose of this test?;;;","12/May/20 17:44;pnowojski;I've disabled the test via {{@Ingore}} for now as 84574fbc1c commit on the master.;;;","13/May/20 02:31;zjwang;The purpose of this test is for guaranteeing no deadlock and buffer leak issues in race condition case. 
 # Task main thread is processes the recovered state buffer.
 # Unspilling IO thread is reading recovered state and inserting the buffer into input channel queue.
 # Canceler thread is closing the `SingleInputGate` and releasing the `RecoveredInputChannel`.

All the three processes can happen concurrently, so this test is necessary to verify this scenario. The general unit test can not find the potential bugs in this complicated scenario.

Actually the initial version of this test is stable to execute. After I picked up the commit from [~pnowojski]'s branch which would check `isReleased` state during `RecoverdInputChannel#getNextRecoveredStateBuffer`, then the custom message is changed and I forgot to adjust the verify message is this test accordingly. If we adjust the verify message by ""Trying to read from released RecoveredInputChannel"", it is still stable to run.

But as [~pnowojski] mentioned, maybe it is fragile to rely on this state to verify the result. I will think of another potential way to bypass it.;;;","14/May/20 02:36;zjwang;Merged in master: 3771835ac76822f4d5eefba8a7ea0afbe9057f4e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document which FileSystems are supported by the StreamingFileSink,FLINK-17639,13304396,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,maguowei,kkl0u,kkl0u,12/May/20 15:55,24/Jun/20 12:57,13/Jul/23 08:07,24/Jun/20 12:57,1.10.1,,,,,1.10.2,1.11.0,1.12.0,,Documentation,,,,,0,pull-request-available,,,,"This issue targets at documenting which of the supported filesystems in Flink are also supported by the {{StreamingFileSink}}. Currently there is no such documentation, and, for example, users of Azure can only figure out at runtime that their FS is not supported.",,aljoscha,felixzheng,kkl0u,maguowei,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17444,FLINK-17989,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 12:57:47 UTC 2020,,,,,,,,,,"0|z0en9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/20 00:55;maguowei;Does anyone work on it?  I want to work on it if there is no one. [~kkl0u];;;","12/Jun/20 14:06;kkl0u;Hi [~maguowei], do you still want me to assign the issue to you?;;;","13/Jun/20 04:24;maguowei;yes. :);;;","22/Jun/20 07:14;kkl0u;Hi [~maguowei]! Any news on this issue? :);;;","22/Jun/20 07:49;maguowei;Sorry for opening the pr late. Today I would open the pr. 

Last week I had already checked all the file systems in the `flink-filesystems` module and found that `StreamingFileSink` only supports HDFS/S3/Local FileSystem. The implementations of the other file system are only a simple wrapper of the corresponding file system.;;;","22/Jun/20 08:02;kkl0u;No problem [~maguowei]! The release is not out yet :)

As for the observation that we only support HDFS, S3 and Local FileSystem, I think you are correct but I will verify as soon as you open the PR.;;;","24/Jun/20 12:57;kkl0u;Merged on master with 00863a2802cdcacfb17f8751ce9a0e24dfbd1a06
on release-1.11 with 8c7f366d8abf4106bd02dba1d4574be06d23737e
and on release-1.10 with b50704be84c800a68a00e56fd820d46fd0820904;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopS3RecoverableWriterITCase fails with Expected exception: java.io.FileNotFoundException,FLINK-17637,13304359,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,maguowei,rmetzger,rmetzger,12/May/20 13:09,08/Sep/20 07:54,13/Jul/23 08:07,08/Sep/20 07:54,1.10.2,1.11.0,,,,1.12.0,,,,FileSystems,Tests,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1084&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8

{code}
[ERROR] Tests run: 13, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 70.885 s <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase
[ERROR] testCleanupRecoverableState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 4.864 s  <<< FAILURE!
java.lang.AssertionError: Expected exception: java.io.FileNotFoundException
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:32)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}",,aljoscha,maguowei,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 08 07:54:13 UTC 2020,,,,,,,,,,"0|z0en14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/20 08:36;sewen;This looks like the test's temp directory could not be created (parent JUnit temp dir missing).
I wonder if something just happened to wipe /tmp concurrently.

This is definitely not the fault of the test.

One thing to consider would be to put the testing temp somewhere where ""automatic wipes"" can NOT happen, like in ""user home"" for example.
;;;","02/Jun/20 19:24;rmetzger;Travis in the release 1.10 branch: https://travis-ci.org/github/apache/flink/jobs/692222173;;;","08/Jun/20 00:42;maguowei;I think the s3's eventually consistency might lead to this case fail(testCleanupRecoverableState).

Delete an s3 object and we might still get the content of the object if we immediately read it.

So maybe we could try multi-times to read the ""delete"" file until it failed.;;;","11/Jun/20 18:46;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3292&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","12/Jun/20 00:56;maguowei;I think it is the same reason as before.

Could you assign this ticket to me if no other one works on it? [~rmetzger];;;","12/Jun/20 16:16;sewen;Guowei is right. We need to do something like ""checkEventuallyDeleted()"" where we try (with backoff strategy) for up to X minutes until the object is deleted (probably up to 5 minutes, if we want a stable test).
;;;","08/Sep/20 07:54;aljoscha;master: 390926e61aeb69837c70a024ad6e7ff02eccdf2d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SingleInputGateTest.testConcurrentReadStateAndProcessAndClose: Trying to read from released RecoveredInputChannel,FLINK-17636,13304358,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zjwang,rmetzger,rmetzger,12/May/20 13:04,15/Jul/20 08:20,13/Jul/23 08:07,15/Jul/20 02:42,1.11.0,,,,,1.11.1,1.12.0,,,Runtime / Network,Tests,,,,0,pull-request-available,test-stability,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1080&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d

{code}
2020-05-12T11:39:28.7058732Z [ERROR] Tests run: 22, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.643 s <<< FAILURE! - in org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest
2020-05-12T11:39:28.7066377Z [ERROR] testConcurrentReadStateAndProcessAndClose(org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest)  Time elapsed: 0.032 s  <<< ERROR!
2020-05-12T11:39:28.7067491Z java.util.concurrent.ExecutionException: java.lang.IllegalStateException: Trying to read from released RecoveredInputChannel
2020-05-12T11:39:28.7068238Z 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2020-05-12T11:39:28.7068795Z 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2020-05-12T11:39:28.7069538Z 	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.submitTasksAndWaitForResults(RemoteInputChannelTest.java:1228)
2020-05-12T11:39:28.7070595Z 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.testConcurrentReadStateAndProcessAndClose(SingleInputGateTest.java:235)
2020-05-12T11:39:28.7075974Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-12T11:39:28.7076784Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-12T11:39:28.7077522Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-12T11:39:28.7078212Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-12T11:39:28.7078846Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-12T11:39:28.7079607Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-12T11:39:28.7080383Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-12T11:39:28.7081173Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-12T11:39:28.7081937Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-12T11:39:28.7082708Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-12T11:39:28.7083422Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-12T11:39:28.7084148Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-12T11:39:28.7084933Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-12T11:39:28.7085562Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-12T11:39:28.7086162Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-12T11:39:28.7086806Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-12T11:39:28.7087434Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-12T11:39:28.7088036Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-12T11:39:28.7088647Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-12T11:39:28.7089328Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-12T11:39:28.7090106Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-12T11:39:28.7090811Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-12T11:39:28.7091674Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-12T11:39:28.7102178Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-12T11:39:28.7103048Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-12T11:39:28.7103701Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-12T11:39:28.7104367Z Caused by: java.lang.IllegalStateException: Trying to read from released RecoveredInputChannel
2020-05-12T11:39:28.7105513Z 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:195)
2020-05-12T11:39:28.7106574Z 	at org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel.getNextRecoveredStateBuffer(RecoveredInputChannel.java:141)
2020-05-12T11:39:28.7107647Z 	at org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel.getNextBuffer(RecoveredInputChannel.java:169)
2020-05-12T11:39:28.7108631Z 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:637)
2020-05-12T11:39:28.7109613Z 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:615)
2020-05-12T11:39:28.7110538Z 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:603)
2020-05-12T11:39:28.7111626Z 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.getNextBufferAndVerify(SingleInputGateTest.java:1077)
2020-05-12T11:39:28.7112759Z 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.lambda$testConcurrentReadStateAndProcessAndClose$2(SingleInputGateTest.java:226)
2020-05-12T11:39:28.7113653Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-05-12T11:39:28.7114278Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-05-12T11:39:28.7115192Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-05-12T11:39:28.7115777Z 	at java.lang.Thread.run(Thread.java:748)
2020-05-12T11:39:28.7116099Z 
{code}",,dian.fu,rmetzger,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 15 02:42:38 UTC 2020,,,,,,,,,,"0|z0en0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/20 02:36;zjwang;Merged in master: c781d170e319d4617afc14aba716697ae0d517f0;;;","08/Jul/20 02:04;dian.fu;Another instance: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4315&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=66b5c59a-0094-561d-0e44-b149dfdd586d]

[~zjwang] It seems that this issue still happens.;;;","08/Jul/20 02:10;zjwang;Thanks for reporting this,  I will reopen it to investigate.;;;","15/Jul/20 02:42;zjwang;Merged in release-1.11: 57e57624deded5594749cbccf17a5878c6dc81b8
Merged in master: a9cce5467b3f0628e665cd8c7dd9b2e5958f0fd7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reject multiple handler registrations under the same URL,FLINK-17634,13304348,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/May/20 11:57,16/Oct/20 10:51,13/Jul/23 08:07,19/May/20 13:37,,,,,,1.11.0,,,,Runtime / REST,,,,,0,pull-request-available,,,,"In FLINK-11853 a handler was added the is being registered under the same URL as another handler. This should never happen, and we should add a check to ensure this doesn't happen again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 13:37:51 UTC 2020,,,,,,,,,,"0|z0emyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/20 13:37;chesnay;master: 260ef2cf78970674ca5cad4a9f456b9a93110c69
1.11: fa0362f430165f6ba9e25a50ce5d98e793b6ab38;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fs connector should use FLIP-122 format options style,FLINK-17626,13304288,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,12/May/20 06:28,16/Oct/20 10:50,13/Jul/23 08:07,19/May/20 10:24,,,,,,1.11.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,"format.parquet.compression -> parquet.compression

format.field-delimiter -> csv.field-delimiter",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 10:24:02 UTC 2020,,,,,,,,,,"0|z0emlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/20 10:24;lzljs3620320;master: 92d674245c72a64efca83eb4343f37422ccf9e6f

release-1.11: 70304404cf0261a85897f403a282cc4d4d1ad743;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ArrayIndexOutOfBoundsException in AppendOnlyTopNFunction,FLINK-17625,13304254,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,jark,jark,12/May/20 02:48,09/Jun/20 16:40,13/Jul/23 08:07,09/Jun/20 03:09,,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"This is reported in user mailing list: http://apache-flink.147419.n8.nabble.com/sql-topN-ArrayIndexOutOfBoundsException-td3008.html

We should check list is not empty before removing the last element in {{AppendOnlyTopNFunction#processElementWithoutRowNumber}}.

{code:java}
ava.lang.ArrayIndexOutOfBoundsException: -1
at java.util.ArrayList.elementData(ArrayList.java:422)
at java.util.ArrayList.remove(ArrayList.java:499)
at org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunction.processElementWithoutRowNumber(AppendOnlyTopNFunction.java:205)
at org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunction.processElement(AppendOnlyTopNFunction.java:120)
at org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunction.processElement(AppendOnlyTopNFunction.java:46)
at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:85)
at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173)
at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)
at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
at java.lang.Thread.run(Thread.java:748)
{code}
",,begginghard,danny0405,jark,leonard,libenchao,lsy,yuwang0917@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 03:09:01 UTC 2020,,,,,,,,,,"0|z0emds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/20 04:02;lsy;[~jark] Can I help to fix this bug?;;;","12/May/20 04:08;jark;Sure. Assigned to you [~lsy]. It would be better to have a IT case to reproduce this problem.;;;","12/May/20 04:13;lsy;[~jark] Ok, thanks for your advice.;;;","20/May/20 08:28;danny0405;Hi, [~lsy], are you working on this ?;;;","21/May/20 15:47;lsy;Hi, [~danny0405], sorry, I haven't start the work because of something else recently, I was just going to do it. if you have time you can help do this.

Hi, [~jark] , when I debug AppendOnlyTopNFunction#processElementWithoutRowNumber method, I find TopBuffer.currentTopNum always increase, it will not decrease even we call TopBuffer#removeAll(sortKey) because of list is empty, is it bug?

Besides, can you help provide some hints about what might lead to the ArrayIndexOutOfBoundsException, I have not idea about reproduce this problem, so I don’t know how to code the IT case now, tks.;;;","02/Jun/20 08:26;danny0405;cc [~jark] ~;;;","09/Jun/20 03:09;jark;- master (1.12.0): 926523e14e8dda8d648d56eb0992aaedf08eb8be
- 1.11.0: 11958b0dd424c143184ed8fd5a2943049b27a4b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""TPC-DS end-to-end test (Blink planner)"" is unstable",FLINK-17624,13304252,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,wangyang0918,wangyang0918,12/May/20 02:22,18/May/20 11:34,13/Jul/23 08:07,18/May/20 11:34,,,,,,,,,,Runtime / Network,,,,,0,,,,,"There are two exceptions in the failed tests. The full logs could be found here. [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=996&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]
{code:java}
2020-05-11T21:06:32.4136490Z 2020-05-11 21:06:29,598 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - HashJoin(joinType=[InnerJoin], where=[(ws_sold_date_sk = d_date_sk)], select=[ws_sold_date_sk, ws_bill_customer_sk, ws_ext_discount_amt, ws_ext_sales_price, ws_ext_wholesale_cost, ws_ext_list_price, d_date_sk], isBroadcast=[true], build=[right]) -> Calc(select=[ws_bill_customer_sk, ws_ext_discount_amt, ws_ext_sales_price, ws_ext_wholesale_cost, ws_ext_list_price]) (1/4) (559352d6ad2fe733009b276bfd4454df) switched from DEPLOYING to CANCELING.
2020-05-11T21:06:32.4137682Z 2020-05-11 21:06:29,607 ERROR org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Error during cleanup of stream task
2020-05-11T21:06:32.4138027Z java.lang.NullPointerException: null
2020-05-11T21:06:32.4139060Z 	at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionReader.notifyDataAvailable(BoundedBlockingSubpartitionReader.java:112) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4140165Z 	at org.apache.flink.runtime.io.network.partition.FileChannelBoundedData$FileBufferReader.recycle(FileChannelBoundedData.java:164) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4141159Z 	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:190) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4142199Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4143299Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4144308Z 	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:164) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4145283Z 	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.releaseDeserializer(StreamTaskNetworkInput.java:242) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4146270Z 	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.close(StreamTaskNetworkInput.java:229) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4147229Z 	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.close(StreamTwoInputProcessor.java:243) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4148125Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanup(StreamTask.java:319) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4149067Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:573) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4149945Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:494) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4150772Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4151556Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4151991Z 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_252]

... ...

2020-05-11T21:06:32.4210412Z 2020-05-11 21:06:30,182 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - HashJoin(joinType=[InnerJoin], where=[(ws_sold_date_sk = d_date_sk)], select=[ws_sold_date_sk, ws_bill_customer_sk, ws_ext_discount_amt, ws_ext_sales_price, ws_ext_wholesale_cost, ws_ext_list_price, d_date_sk], isBroadcast=[true], build=[right]) -> Calc(select=[ws_bill_customer_sk, ws_ext_discount_amt, ws_ext_sales_price, ws_ext_wholesale_cost, ws_ext_list_price]) (4/4) (f32675cf1f14841e08ccf99ecb2df2fe) switched from RUNNING to FAILED.
2020-05-11T21:06:32.4211553Z org.apache.flink.runtime.jobmaster.ExecutionGraphException: The execution attempt f32675cf1f14841e08ccf99ecb2df2fe was not found.
2020-05-11T21:06:32.4212422Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:389) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4212922Z 	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source) ~[?:?]
2020-05-11T21:06:32.4213357Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252]
2020-05-11T21:06:32.4213874Z 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252]
2020-05-11T21:06:32.4214719Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4215622Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4216552Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4217466Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4218277Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4219060Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4219908Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4220720Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4221512Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4222320Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4223123Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4223882Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4224658Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4225420Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4226167Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4226895Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4227625Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4228336Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4229082Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4229912Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4230738Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-11T21:06:32.4231582Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]{code}",,begginghard,leonard,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17194,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-12 02:22:19.0,,,,,,,,,,"0|z0emdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Only set offset commit if group id is configured for Kafka Table source,FLINK-17619,13304109,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,11/May/20 12:32,16/Oct/20 10:51,13/Jul/23 08:07,19/May/20 14:36,1.10.0,1.11.0,,,,1.11.0,,,,Connectors / Kafka,Table SQL / API,,,,0,pull-request-available,,,,"Currently the KafkaTableSourceBase always creates the KafkaConsumer with the default offset commiting behavior that will try to commit on checkpoints.

However this will fail if group.id is not specified which is an optional parameter.

We should disable offset commiting if group.id wasnt specified.",,aljoscha,begginghard,gyfora,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17802,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 14:36:09 UTC 2020,,,,,,,,,,"0|z0elhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 09:51;aljoscha;Sounds legit, yes.;;;","18/May/20 09:54;gyfora;Seems like Jira did not pick up the PR:
[https://github.com/apache/flink/pull/12117/files];;;","18/May/20 13:00;jark;cc [~Leonard Xu], should we fix this in the new Kafka table source too?;;;","18/May/20 13:47;leonard;[~jark] I think yes,  the new kafka table source *KafkaDynamicSourceBase* has same problem.

[~gyfora] could you help the fix this too? I can help review.

 ;;;","18/May/20 14:41;jark;Hi [~Leonard Xu], we can review and merge this one first (as many users are still use the legacy one). We can create another issue to fix for the new source.;;;","19/May/20 06:54;gyfora;We could do it either way, If you review the PR and happy I can make the same change to the other source and add a similar test;;;","19/May/20 14:36;gyfora;Merged:
release-1.11: cf29c2c9c21eba72535a5eb86d6f28de2803c0ee
master: 69c7b41558097ae9a39862931742be861904a796;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the outdated comments in the log4j properties files,FLINK-17618,13304107,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,wangyang0918,wangyang0918,11/May/20 12:29,16/Oct/20 10:37,13/Jul/23 08:07,18/May/20 09:46,,,,,,,,,,Command Line Client,,,,,0,pull-request-available,,,,"When we upgrade the log4j to log4j2, there are some residual log4j logger configuration in the comments. Just like following,

log4j.properties and log4j-console.properties
{code:java}
# Uncomment this if you want to _only_ change Flink's logging
#log4j.logger.org.apache.flink=INFO
{code}
We should update them to the log4j2 format.

 
{code:java}
# Uncomment this if you want to _only_ change Flink's logging
logger.flink.name = org.apache.flink
logger.flink.level = INFO
{code}
 

cc [~chesnay]

 ",,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 09:46:05 UTC 2020,,,,,,,,,,"0|z0elh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 09:46;chesnay;master: dc3efe1943d8347bbafffcbf7b98ae53363384b6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL CLI Autocomplete does not return correct hints,FLINK-17617,13304091,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,11/May/20 11:38,16/Oct/20 10:37,13/Jul/23 08:07,27/May/20 12:56,,,,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"The SQL Client does not properly return the hints for commands such as:

USE CATALOG,
SHOW CATALOGS,
SHOW MODULES

Or whenever it does not actually fall back to the table planner.

The reason being is that the hint returned does not take into account the current cursor position but returns the complete text.",,begginghard,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 12:56:50 UTC 2020,,,,,,,,,,"0|z0eldk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 12:17;ykt836;I noticed the PR has been merged, should we also close this issue?;;;","27/May/20 12:56;gyfora;Merged: a17da14fc00a1b13e6e8106863a3e049fbc217f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Execute the script directly when user specified the entry script with ""-py"" rather than run as module.",FLINK-17609,13304034,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,csq,csq,11/May/20 08:08,12/May/20 08:30,13/Jul/23 08:07,12/May/20 08:30,1.11.0,,,,,1.11.0,,,,API / Python,Client / Job Submission,,,,0,pull-request-available,,,,"Currently, the PythonDriver will always construct the python execution command with ""-m"" option, which means uses code files will run as a module, E.g, ""python -m <module_name>"". However, when user specifies the ""-py"" option followed by the entry script file path, we should directly execute the user specified script, such as ""python <entry_script_file_path>"".

The difference between ""python <entry_script_file_path>"" and ""python -m <module_name>"" is as follow:
    ""python <entry_script_file_path>"" will add the parent directory of the entry script to the PYTHONPATH so that all modules under the directly can be found when executing the script.
    ""python -m <module_name>"" will only add the current execution directory to the PYTHONPATH, which may cause ""ModuleNotFoundError"" when the entry module references other modules under the same directory.",,csq,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 08:30:38 UTC 2020,,,,,,,,,,"0|z0el0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/20 08:30;dian.fu;Merged to master via 89d46fac30cdf8b86ae9709bf86f4d9d42802014;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableEnvironmentITCase.testStatementSet fails on travis,FLINK-17601,13303954,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,dwysakowicz,dwysakowicz,10/May/20 19:01,11/May/20 06:37,13/Jul/23 08:07,11/May/20 03:59,,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,test-stability,,,,"{code}
[ERROR] Failures: 
[ERROR]   TableEnvironmentITCase.testStatementSet:446 expected:<... fields=[first])
+- [TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- ]TableSourceScan(tabl...> but was:<... fields=[first])
+- [LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- Legacy]TableSourceScan(tabl...>
[ERROR]   TableEnvironmentITCase.testStatementSet:446 expected:<... fields=[first])
+- [TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- ]TableSourceScan(tabl...> but was:<... fields=[first])
+- [LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- Legacy]TableSourceScan(tabl...>
[ERROR]   TableEnvironmentITCase.testStatementSet:446 expected:<... fields=[first])
+- [TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- ]TableSourceScan(tabl...> but was:<... fields=[first])
+- [LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- Legacy]TableSourceScan(tabl...>
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=912&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=f83cd372-208c-5ec4-12a8-337462457129",,dwysakowicz,godfreyhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17607,,,,,,,,FLINK-16367,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 03:59:25 UTC 2020,,,,,,,,,,"0|z0ekj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/20 19:02;dwysakowicz;Caused by: https://issues.apache.org/jira/browse/FLINK-16367

I think the tests were not run after https://github.com/apache/flink/pull/12047 was merged;;;","10/May/20 19:05;dwysakowicz;cc [~godfreyhe] [~ykt836];;;","11/May/20 03:20;godfreyhe;thanks for reporting this [~dwysakowicz], I have opened a pr https://github.com/apache/flink/pull/12060;;;","11/May/20 03:59;ykt836;master: 51c7d61f3e7661f6aad331245ca94ecb82be3994;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable PyFlinkBlinkStreamUserDefinedFunctionTests testMethod#test_udf_in_join_condition_2,FLINK-17596,13303826,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,yunta,yunta,09/May/20 09:49,15/May/20 05:58,13/Jul/23 08:07,15/May/20 05:57,1.11.0,,,,,1.11.0,,,,API / Python,,,,,0,pull-request-available,test-stability,,,"Unlike FLINK-14865, this failure of PyFlinkBlinkStreamUserDefinedFunctionTests testMethod#test_udf_in_join_condition_2 is caused by ""no such file"" IO exception.

instance link: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=826&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=455fddbf-5921-5b71-25ac-92992ad80b28

{code:java}
 Caused by: java.lang.RuntimeException: Failed to create stage bundle factory!
 	at org.apache.flink.python.AbstractPythonFunctionRunner.createStageBundleFactory(AbstractPythonFunctionRunner.java:197)
 	at org.apache.flink.python.AbstractPythonFunctionRunner.open(AbstractPythonFunctionRunner.java:164)
 	at org.apache.flink.table.runtime.runners.python.scalar.AbstractGeneralPythonScalarFunctionRunner.open(AbstractGeneralPythonScalarFunctionRunner.java:65)
 	at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator$ProjectUdfInputPythonScalarFunctionRunner.open(AbstractStatelessFunctionOperator.java:186)
 	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:142)
 	at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.open(AbstractStatelessFunctionOperator.java:131)
 	at org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.open(AbstractPythonScalarFunctionOperator.java:88)
 	at org.apache.flink.table.runtime.operators.python.scalar.AbstractRowDataPythonScalarFunctionOperator.open(AbstractRowDataPythonScalarFunctionOperator.java:80)
 	at org.apache.flink.table.runtime.operators.python.scalar.RowDataPythonScalarFunctionOperator.open(RowDataPythonScalarFunctionOperator.java:64)
 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:288)
 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:459)
 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:455)
 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:475)
 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713)
 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539)
 	at java.lang.Thread.run(Thread.java:748)
 Caused by: org.apache.beam.vendor.guava.v26_0_jre.com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: Cannot run program ""/tmp/python-dist-38f0c31e-9801-4d53-9694-2e16fd4b2fc5/pyflink-udf-runner.sh"": error=2, No such file or directory
 	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4966)
 	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.<init>(DefaultJobBundleFactory.java:331)
 	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.<init>(DefaultJobBundleFactory.java:320)
 	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory.forStage(DefaultJobBundleFactory.java:250)
 	at org.apache.flink.python.AbstractPythonFunctionRunner.createStageBundleFactory(AbstractPythonFunctionRunner.java:195)
 	... 16 more
 Caused by: java.io.IOException: Cannot run program ""/tmp/python-dist-38f0c31e-9801-4d53-9694-2e16fd4b2fc5/pyflink-udf-runner.sh"": error=2, No such file or directory
 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
 	at org.apache.beam.runners.fnexecution.environment.ProcessManager.startProcess(ProcessManager.java:144)
 	at org.apache.beam.runners.fnexecution.environment.ProcessManager.startProcess(ProcessManager.java:119)
 	at org.apache.beam.runners.fnexecution.environment.ProcessEnvironmentFactory.createEnvironment(ProcessEnvironmentFactory.java:131)
 	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:200)
 	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:184)
 	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3528)
 	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2277)
 	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)
 	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2044)
 	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.get(LocalCache.java:3952)
 	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974)
 	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)
 	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4964)
 	... 20 more
 	Suppressed: java.lang.NullPointerException: Process for id does not exist: 87-1
 		at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull(Preconditions.java:895)
 		at org.apache.beam.runners.fnexecution.environment.ProcessManager.stopProcess(ProcessManager.java:169)
 		at org.apache.beam.runners.fnexecution.environment.ProcessEnvironmentFactory.createEnvironment(ProcessEnvironmentFactory.java:151)
 		... 30 more
 Caused by: java.io.IOException: error=2, No such file or directory
 	at java.lang.UNIXProcess.forkAndExec(Native Method)
 	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
 	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
 	... 33 more

{code}

",,dian.fu,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 05:57:22 UTC 2020,,,,,,,,,,"0|z0ejqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/20 10:39;dian.fu;cc [~zhongwei];;;","15/May/20 05:57;dian.fu;Merged to master via 282da0dd3e2def9e0ea6693f953d2733eb663e70;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-table-planner doesn't compile on Scala 2.12,FLINK-17592,13303804,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,aljoscha,rmetzger,rmetzger,09/May/20 06:52,12/May/20 16:53,13/Jul/23 08:07,12/May/20 16:53,1.11.0,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,,,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=844&view=logs&j=ed6509f5-1153-558c-557a-5ee0afbcdf24&t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065

{code}
[WARNING]                  ^
[ERROR] /__w/1/s/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/utils/testTableSinks.scala:73: error: overriding method getTableSchema in trait TableSink of type ()org.apache.flink.table.api.TableSchema;
[ERROR]  method getTableSchema needs `override' modifier
[ERROR]   def getTableSchema: TableSchema = {
[ERROR]       ^
[WARNING] 8 warnings found
[ERROR] one error found
{code}",,begginghard,godfreyhe,jark,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 16:53:21 UTC 2020,,,,,,,,,,"0|z0ejls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/20 05:39;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1020&view=logs&j=ed6509f5-1153-558c-557a-5ee0afbcdf24&t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065;;;","12/May/20 16:53;rmetzger;fixed through https://github.com/flink-ci/flink-mirror/commit/5f81a36a2aee511801f94532dbe4c0f8f47c80d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableEnvironmentITCase.testExecuteSqlAndToDataStream failed,FLINK-17591,13303799,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,jark,jark,09/May/20 06:30,22/Jun/21 13:55,13/Jul/23 08:07,15/May/20 11:46,,,,,,1.11.0,,,,Table SQL / Legacy Planner,Tests,,,,0,test-stability,,,,"Here is the instance: https://dev.azure.com/imjark/Flink/_build/results?buildId=61&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=6855dd6e-a7b0-5fd1-158e-29fc186b16c8


{code:java}
[ERROR] Tests run: 26, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 12 s <<< FAILURE! - in org.apache.flink.table.api.TableEnvironmentITCase
[ERROR] testExecuteSqlAndToDataStream[StreamTableEnvironment](org.apache.flink.table.api.TableEnvironmentITCase)  Time elapsed: 0.513 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.flink.table.api.TableEnvironmentITCase.testExecuteSqlAndToDataStream(TableEnvironmentITCase.scala:343)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   TableEnvironmentITCase.testExecuteSqlAndToDataStream:343
[INFO] 
[ERROR] Tests run: 791, Failures: 1, Errors: 0, Skipped: 13

{code}
",,AHeise,dwysakowicz,godfreyhe,jark,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 11:46:13 UTC 2020,,,,,,,,,,"0|z0ejko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/20 09:45;dwysakowicz;This fails consistently in a local environment. It is not possible to run the tests locally.;;;","11/May/20 10:29;godfreyhe;I think the reason is `StreamITCase.testResults` is static variable, and is used by many tests. In multiple threads environment, the result may be affected by other running case. ;;;","11/May/20 15:17;rmetzger;Also on my personal azure account: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=324&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=6855dd6e-a7b0-5fd1-158e-29fc186b16c8;;;","12/May/20 16:22;arvid;Another instance on personal AZP: https://dev.azure.com/arvidheise0209/arvidheise/_build/results?buildId=234&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=6855dd6e-a7b0-5fd1-158e-29fc186b16c8;;;","13/May/20 09:06;jark;Fixed in master (1.11.0): d22e965112fa1f7b35c07466f487f27c813c1dfc

;;;","13/May/20 09:11;godfreyhe;it is caused by {{StreamITCase.clear}}, not multi-thread problem. Sorry for the misleading messages.;;;","13/May/20 09:13;dwysakowicz;I still think you are right [~godfreyhe] that this is a multi-thread problem with reusing sink and jobs not being terminated at the end of test cases. The StreamITCase.clear just make it less likely to appear in my opinion.;;;","13/May/20 09:21;godfreyhe;{{StreamITCase}} is introduced since 2016, and it was used in many places. if there is a multi-thread problem, we can find it before.;;;","13/May/20 09:26;ykt836;According to surefire's document [1], the tests will be executed by single thread if {{reuseForks=true}} and a {{forkCount}} value larger than one...

 [1] [https://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-options-and-parallel-execution.html];;;","15/May/20 11:46;jark;Close it since it is not reproduced these days. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Due to the new behavior of the flink-table module the Python dependency management command line options does not work ,FLINK-17586,13303781,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,zhongwei,zhongwei,09/May/20 03:08,09/May/20 11:35,13/Jul/23 08:07,09/May/20 11:33,1.11.0,,,,,1.11.0,,,,API / Python,,,,,0,pull-request-available,,,,"Currently the Python dependency management command line options does not work because of such changes on flink-table module:
 # The ""BatchTableEnvImpl"" creates the job pipeline with an empty cached file list, the cached files registered in the ""BatchTableEnvImpl.execEnv"" are dropped.
 # The ""StreamPlanner"" wraps the real ""StreamExecutionEnvironment"" into a ""DummyStreamExecutionEnvironment"", and the getConfiguration() method in the ""DummyStreamExecutionEnvironment"" is not delegated to the real ""StreamExecutionEnvironment"". It always returns an empty configuration object.",,dian.fu,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 09 11:33:11 UTC 2020,,,,,,,,,,"0|z0ejgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/20 11:33;dian.fu;Merged to master via 1a734c2b95fab7b2a0b62e5aded899b1efd91508 and 61fe14f3a2ebf31891187eff1a9a6fb92b35c776;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""PythonProgramOptions"" changes the entry point class when user submit a Java sql job which contains Python UDF ",FLINK-17585,13303780,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,zhongwei,zhongwei,09/May/20 03:04,09/May/20 11:40,13/Jul/23 08:07,09/May/20 11:31,1.11.0,,,,,1.11.0,,,,API / Python,,,,,0,pull-request-available,,,,"If we running such a command:
{code:java}
flink run -pyfs xxx.py xxx.jar
{code}
The main class will be changed to ""PythonDriver"". It is because the ""PythonProgramOptions"" class changes the entry point class when the python command line options are detected. We should consider the situation that user submit a Java sql job which contains Python UDF. ",,dian.fu,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 09 11:31:36 UTC 2020,,,,,,,,,,"0|z0ejgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/20 11:31;dian.fu;Merged to master via eb33d98eadeb99ef9bbc668cc3e96fba3de5edfa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in unaligned checkpoint for EndOfPartition events,FLINK-17580,13303682,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,arvid,AHeise,08/May/20 16:13,22/Jun/21 14:06,13/Jul/23 08:07,11/May/20 16:19,1.11.0,,,,,1.11.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,Current master does not account for a {{StreamTaskNetworkInput#recordDeserializers }}being nulled after EOP events{{.}},,AHeise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17315,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-08 16:13:02.0,,,,,,,,,,"0|z0eiuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Union of 2 SideOutputs behaviour incorrect,FLINK-17578,13303620,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,damjad,drshade,drshade,08/May/20 11:53,20/Aug/20 07:51,13/Jul/23 08:07,14/May/20 14:22,1.10.0,,,,,1.10.2,1.11.0,,,API / DataStream,,,,,0,pull-request-available,,,,"Strange behaviour when using union() to merge outputs of 2 DataStreams, where both are sourced from SideOutputs.

See example code with comments demonstrating the issue:
{code:java}
  def main(args: Array[String]): Unit = {
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

    val input = env.fromElements(1, 2, 3, 4)

    val oddTag = OutputTag[Int](""odds"")
    val evenTag = OutputTag[Int](""even"")

    val all =
      input.process {
        (value: Int, ctx: ProcessFunction[Int, Int]#Context, out: Collector[Int]) => {
          if (value % 2 != 0)
            ctx.output(oddTag, value)
          else
            ctx.output(evenTag, value)
        }
      }

    val odds = all.getSideOutput(oddTag)
    val evens = all.getSideOutput(evenTag)

    // These print correctly
    //
    odds.print                  // -> 1, 3
    evens.print                 // -> 2, 4

    // This prints incorrectly - BUG?
    //
    odds.union(evens).print       // -> 2, 2, 4, 4
    evens.union(odds).print       // -> 1, 1, 3, 3

    // Another test to understand normal behaviour of .union, using normal inputs
    //
    val odds1 = env.fromElements(1, 3)
    val evens1 = env.fromElements(2, 4)

    // Union of 2 normal inputs
    //
    odds1.union(evens1).print   // -> 1, 2, 3, 4

    // Union of a normal input plus an input from a sideoutput
    //
    odds.union(evens1).print    // -> 1, 2, 3, 4
    evens1.union(odds).print    // -> 1, 2, 3, 4

    //
    // So it seems that when both inputs are from sideoutputs that it behaves incorrectly... BUG?

    env.execute(""Test job"")
  }
{code}",,aljoscha,alpinegizmo,damjad,drshade,gaoyunhaii,kezhuw,yingz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18960,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 14:22:02 UTC 2020,,,,,,,,,,"0|z0eihc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/20 02:41;damjad;The *root cause* of this problem lies in the implementation of [StreamEdge|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamEdge.java].

The hashcode and equals only check *edgeId* for comparison but they don't check *outputTag.* 
 The *edgeId* is formed by concating information such as sourceVertex, targetVertex etc. This information is the same for both of both even and odd side outputs in Union operation.

This has an effect in *OperatorChain* construction where a *Map* is maintained for edge/writer pair. The first one is always overwritten by the second. Later this Map is used for construction of *allOutputs* list that contains the same writer twice which causes the same list to be printed twice.

I have tested my analysis and it is working fine. Here is my code:
 [https://github.com/damjad/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamEdge.java]

Please suggest if there are any more changes (maybe a more sophisticated hashCode?);;;","13/May/20 16:19;aljoscha;[~damjad] do you want to open a PR for this? When doing so, it would also be good to add a test case that verifies the behaviour.;;;","13/May/20 21:45;damjad;Opened [PR|https://github.com/apache/flink/pull/12136].;;;","14/May/20 14:22;aljoscha;master: ff861c2d8d522638821708fbefaea0bfdc77d358
release-1.10: b3023cbf4b0611424a36a9323bceade1c083ef9f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchTableEnvironment#fromValues(Object... values) throws StackOverflowError ,FLINK-17570,13303573,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,zhongwei,zhongwei,08/May/20 07:59,08/May/20 18:15,13/Jul/23 08:07,08/May/20 18:15,1.11.0,,,,,1.11.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"The Error can be reproduced by following code:
{code:java}
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
BatchTableEnvironment tEnv = BatchTableEnvironment.create(env);
tEnv.fromValues(1L, 2L, 3L);
{code}
The Error is as following:
{code:java}
Exception in thread ""main"" java.lang.StackOverflowErrorException in thread ""main"" java.lang.StackOverflowError at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250) at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110) at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.table.expressions.ApiExpressionUtils.convertArray(ApiExpressionUtils.java:142) at org.apache.flink.table.expressions.ApiExpressionUtils.objectToExpression(ApiExpressionUtils.java:100) at org.apache.flink.table.api.internal.TableEnvImpl$$anonfun$2.apply(TableEnvImpl.scala:1030) at org.apache.flink.table.api.internal.TableEnvImpl$$anonfun$2.apply(TableEnvImpl.scala:1030) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1030) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163)
...{code}",,dian.fu,dwysakowicz,libenchao,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 08 18:15:14 UTC 2020,,,,,,,,,,"0|z0ei6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/20 08:00;zhongwei;Hi [~dwysakowicz] could you please take a look at this JIRA?;;;","08/May/20 18:15;dwysakowicz;Fixed via 571c0892fa30a34751b575c10b78472eed8849e2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
to support ViewFileSystem when wait lease revoke of hadoop filesystem,FLINK-17569,13303570,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,wukong,wukong,08/May/20 07:43,04/Aug/20 13:26,13/Jul/23 08:07,04/Aug/20 13:26,,,,,,1.12.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,"Currently, HadoopRecoverableFsDataOutputStream#waitUntilLeaseIsRevoked method is not support ViewFileSystem

 

and it will cause the same issue fail to recover after taskmanager failure

 

 

I try to resolve the real path and FileSystem of ViewFileSystem  and it works
{code:java}
if (fs instanceof ViewFileSystem) {
   ViewFileSystem vfs = (ViewFileSystem) fs;
   Path resolvePath = vfs.resolvePath(path);
   DistributedFileSystem dfs = (DistributedFileSystem) resolvePath.getFileSystem(fs.getConf());
   return waitUntilLeaseIsRevoked(dfs, resolvePath);
}
{code}
 ",,kkl0u,sewen,wukong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 04 13:26:20 UTC 2020,,,,,,,,,,"0|z0ei68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/20 19:26;sewen;Good catch, and the suggested solution makes sense.

+1 for this fix;;;","04/Aug/20 13:26;kkl0u;Merged on master with 27ecf02fb2a8e61b68e93190080f540e53aca171;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task may consume data after checkpoint barrier before performing checkpoint for unaligned checkpoint,FLINK-17568,13303565,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,08/May/20 07:27,09/May/20 11:10,13/Jul/23 08:07,09/May/20 11:10,,,,,,1.11.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"For unaligned checkpoint, task may consume data after the checkpoint barrier before performing checkpoint which lead to consumption of duplicated data and corruption of data stream.

More specifically, when the Netty thread notifies the checkpoint barrier for the first time and enqueue a checkpointing task in the mailbox, the task thread may still in data consumption loop and if it reads a new checkpoint barrier from another channel it will not return to the mailbox and instead it will continue to read data until a all data consumed or we have a full record, meanwhile, the data after checkpoint barrier may be read and consumed which lead to inconsistency.",,kevin.cyj,pnowojski,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 09 11:10:18 UTC 2020,,,,,,,,,,"0|z0ei54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/20 11:10;pnowojski;merged to master as e6c042fd0e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix potential K8s resources leak after JobManager finishes in Applicaion mode,FLINK-17566,13303551,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,felixzheng,felixzheng,08/May/20 06:05,05/Jun/20 03:35,13/Jul/23 08:07,05/Jun/20 03:35,,,,,,,,,,Deployment / Kubernetes,,,,,0,,,,,"FLINK-10934 introduces applicaion mode support in the native K8s setups., but as the discussion in [https://github.com/apache/flink/pull/12003|https://github.com/apache/flink/pull/12003,], there's large probability that all the K8s resources leak after the JobManager finishes except that the replica of Deployment is scaled down to 0. We need to find out the root cause and fix it.

This may be related to the way fabric8 SDK deletes a Deployment. It splits the procedure into three steps as follows:
 # Scales down the replica to 0
 # Wait until the scaling down succeed
 # Delete the ReplicaSet

 

 ",,felixzheng,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17565,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 03:35:07 UTC 2020,,,,,,,,,,"0|z0ei20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/20 08:02;wangyang0918;https://github.com/fabric8io/kubernetes-client/issues/2209

I have created a ticket in fabric8 kubernetes-client project and get the conclusion that the deleting deployment is just one API request to Kubernetes APIServer. Maybe there is some other reason for the residual resources.;;;","08/May/20 08:19;felixzheng;Not exactly. Version 4.5.2 doesn't send one API request when deleting a Deployment since it introduces the so-called {{Reaper}} which I think is for the old Kubernetes version. You can refer to class of {{DeploymentOperationsImpl}} at v4.5.2.

And I find a new PR for removing the Reaper: https://github.com/fabric8io/kubernetes-client/blob/master/kubernetes-client/src/main/java/io/fabric8/kubernetes/client/dsl/internal/apps/v1/DeploymentOperationsImpl.java. That's a good change that makes deletion one single request to the K8s API Server now.

So we should bump the fabric8 version then check if the problem still exists.
;;;","08/May/20 12:26;wangyang0918;Sounds good. But i am not sure whether we need to bump the kubernetes-client version directly to v4.10.1. It seems that there is some big changes in this version. Maybe 4.9.1 is enough?;;;","09/May/20 01:31;felixzheng;Right. According to the suggestion from the author of faric8 SDK, 4.9.1 is more stable than 4.10.X thus should be enough at the moment.;;;","09/May/20 03:26;wangyang0918;Great. Bumping the fabric8 kubernetes-client version to 4.9.1 could solve this problem. I have verified the application mode with multiple times. It always works well. ;;;","05/Jun/20 03:35;wangyang0918;Fixed in FLINK-17565.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inflight data of incoming channel may be disordered for unaligned checkpoint,FLINK-17564,13303537,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,08/May/20 04:07,10/May/20 09:26,13/Jul/23 08:07,10/May/20 09:26,,,,,,1.11.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"For unaligned checkpoint, when checkpointing the inflight data of incoming channel, both task thread and Netty thread may add data to the channel state writer. More specifically, the task thread will first request inflight buffers from the input channel and add the buffers to the channel state writer, and then the Netty thread will add the following up buffers (if any) to the channel state writer. The buffer adding of task thread and Netty thread is not synchronized so the Netty thread may add buffers before the task thread which leads to disorder of the data and corruption of the data stream.",,kevin.cyj,pnowojski,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 10 09:26:42 UTC 2020,,,,,,,,,,"0|z0ehyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/20 09:26;pnowojski; merged commit 5bf121a into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN Setup page has a broken link,FLINK-17563,13303533,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,AT-Fieldless,AT-Fieldless,08/May/20 03:08,13/May/20 14:29,13/Jul/23 08:07,13/May/20 14:29,,,,,,,,,,Project Website,,,,,0,,,,,"In Flink YARN Session column, the _FAQ section_ link is broken.Is it deprecated?",,AT-Fieldless,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 14:29:57 UTC 2020,,,,,,,,,,"0|z0ehy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/20 15:05;rmetzger;You are right, this link is broken. It will be fixed in a pull request to be merged soon: https://github.com/apache/flink/pull/11983/files#diff-8ae07fac4c1f4137fc1ef062aef70115L68
;;;","13/May/20 14:29;rmetzger;resolved in https://github.com/apache/flink/commit/2cc63a6d0f6cec23df5b00797a3876907ecb3342;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POST /jars/:jarid/plan is not working,FLINK-17562,13303532,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nicholasjiang,AT-Fieldless,AT-Fieldless,08/May/20 02:56,13/May/20 07:33,13/Jul/23 08:07,13/May/20 07:33,1.9.0,,,,,1.10.2,1.11.0,1.9.4,,Documentation,Runtime / Web Frontend,,,,0,documentation,,,,"The handlers introduced in FLINK-11853 is not using the correct headers, and registers a second handler under the same URL.",,AT-Fieldless,nicholasjiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-11853,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 07:33:17 UTC 2020,,,,,,,,,,"0|z0ehxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/20 07:35;nicholasjiang;[~hequn8128]Please assign this to me for fixing.;;;","12/May/20 07:39;chesnay;[~nicholasjiang] How do you intend to fix this? The REST API documentation is generated, so either something went wrong during a regeneration or the there's a weird bug in the generator.;;;","12/May/20 09:24;nicholasjiang;[~chesnay]I have already fixed the bug in WebSubmissionExtension. In WebSubmissionExtension, postJarPlanHandler is specified with wrong AbstractJarPlanHeaders.;;;","13/May/20 07:33;chesnay;master: 83d82c1e042c527a2db3673fba41e1ed15075f31
1.10: be5b977ab02a9ebcc485cffffdf47bbdace629bc 
1.9: 2c857610c9ac2fd5e94e4078adcfbc9c7644f24c ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partitions are released in TaskExecutor Main Thread,FLINK-17558,13303465,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,gjy,gjy,07/May/20 18:46,31/May/20 21:45,13/Jul/23 08:07,28/May/20 16:29,1.10.0,1.10.1,1.11.0,,,1.10.2,1.11.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Partitions are released in the main thread of the TaskExecutor (see the stacktrace below). This can lead to missed heartbeats, timeouts of RPCs, etc. because deleting files is blocking I/O. The partitions should be released in a devoted I/O thread pool ({{TaskExecutor#ioExecutor}} is a candidate but requires a higher default thread count). 

{noformat}
2020-05-06T19:13:12.4383402Z ""flink-akka.actor.default-dispatcher-35"" #3555 prio=5 os_prio=0 tid=0x00007f7fcc071000 nid=0x1f3f9 runnable [0x00007f7fd302c000]
2020-05-06T19:13:12.4383983Z    java.lang.Thread.State: RUNNABLE
2020-05-06T19:13:12.4384519Z    at sun.nio.fs.UnixNativeDispatcher.unlink0(Native Method)
2020-05-06T19:13:12.4384971Z    at sun.nio.fs.UnixNativeDispatcher.unlink(UnixNativeDispatcher.java:146)
2020-05-06T19:13:12.4385465Z    at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:231)
2020-05-06T19:13:12.4386000Z    at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
2020-05-06T19:13:12.4386458Z    at java.nio.file.Files.delete(Files.java:1126)
2020-05-06T19:13:12.4386968Z    at org.apache.flink.runtime.io.network.partition.FileChannelBoundedData.close(FileChannelBoundedData.java:93)
2020-05-06T19:13:12.4388088Z    at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.checkReaderReferencesAndDispose(BoundedBlockingSubpartition.java:247)
2020-05-06T19:13:12.4388765Z    at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.release(BoundedBlockingSubpartition.java:208)
2020-05-06T19:13:12.4389444Z    - locked <0x00000000ff836d78> (a java.lang.Object)
2020-05-06T19:13:12.4389905Z    at org.apache.flink.runtime.io.network.partition.ResultPartition.release(ResultPartition.java:290)
2020-05-06T19:13:12.4390481Z    at org.apache.flink.runtime.io.network.partition.ResultPartitionManager.releasePartition(ResultPartitionManager.java:80)
2020-05-06T19:13:12.4391118Z    - locked <0x000000009d452b90> (a java.util.HashMap)
2020-05-06T19:13:12.4391597Z    at org.apache.flink.runtime.io.network.NettyShuffleEnvironment.releasePartitionsLocally(NettyShuffleEnvironment.java:153)
2020-05-06T19:13:12.4392267Z    at org.apache.flink.runtime.io.network.partition.TaskExecutorPartitionTrackerImpl.stopTrackingAndReleaseJobPartitions(TaskExecutorPartitionTrackerImpl.java:62)
2020-05-06T19:13:12.4392914Z    at org.apache.flink.runtime.taskexecutor.TaskExecutor.releaseOrPromotePartitions(TaskExecutor.java:776)
2020-05-06T19:13:12.4393366Z    at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
2020-05-06T19:13:12.4393813Z    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-06T19:13:12.4394257Z    at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-06T19:13:12.4394693Z    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
2020-05-06T19:13:12.4395202Z    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-05-06T19:13:12.4395686Z    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-05-06T19:13:12.4396165Z    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$$Lambda$72/775020844.apply(Unknown Source)
2020-05-06T19:13:12.4396606Z    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-05-06T19:13:12.4397015Z    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-05-06T19:13:12.4397447Z    at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-05-06T19:13:12.4397874Z    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-05-06T19:13:12.4398414Z    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-05-06T19:13:12.4398879Z    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-06T19:13:12.4399321Z    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-06T19:13:12.4399737Z    at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-05-06T19:13:12.4400138Z    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-05-06T19:13:12.4400552Z    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-05-06T19:13:12.4400930Z    at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-05-06T19:13:12.4401390Z    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-05-06T19:13:12.4401763Z    at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-05-06T19:13:12.4402135Z    at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-05-06T19:13:12.4402540Z    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-05-06T19:13:12.4402984Z    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-05-06T19:13:12.4403448Z    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-05-06T19:13:12.4404096Z    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{noformat}",,gjy,trohrmann,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18035,,,,FLINK-17194,,,,,,,,FLINK-17621,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 15:46:38 UTC 2020,,,,,,,,,,"0|z0ehiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/20 15:46;chesnay;master: 44fba5fe8e4be383795211ab3d3f6259331e68f4
1.11: 79cb1a140991d13b2010d10c86e054fcced977c4 
1.10: c6c0839d40571bb9ad47b8dbbec0b0ea14615372 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docstring of pyflink.table.descriptors.FileSystem:1:duplicate object description of pyflink.table.descriptors.FileSystem,FLINK-17555,13303364,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,pnowojski,pnowojski,07/May/20 11:25,07/May/20 16:04,13/Jul/23 08:07,07/May/20 16:04,1.10.0,,,,,1.10.2,1.11.0,1.9.4,,API / Python,,,,,0,pull-request-available,test-stability,,,"Some documentation check failed on travis:
https://api.travis-ci.org/v3/job/684185591/log.txt",,dian.fu,felixzheng,liyu,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 16:04:26 UTC 2020,,,,,,,,,,"0|z0egwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/20 15:06;pnowojski;Another instance: https://api.travis-ci.org/v3/job/684228262/log.txt;;;","07/May/20 15:23;liyu;The above failure is observed in the post-commit check of 1.10.1 RC3: https://travis-ci.org/github/apache/flink/jobs/684228262

[~dian.fu] Could you help take a look here and check whether this is a critical issue? Shall we cancel the RC to get this fixed? Thanks.;;;","07/May/20 15:42;dian.fu;[~liyu] [~pnowojski] Thanks a lot for tracking this issue. I think this issue is not a blocker of 1.10.1 release. This issue only affects the Python doc generation which is not part of the 1.10.1 release (as the Python doc is generated in a separate cron job which is run every day).

Regarding to this issue, it's caused as the check in the latest sphinx is more strict(3.0.3) than before. We have limited the version of sphinx to 2.4.4 for the master branch (as part of the fix of FLINK-17188) and so this issue doesn't occur in master. I will cherry-pick that change to release-1.10 and release-1.9 to limit the sphinx version to 2.4.4 also for release-1.10 and release-1.9. ;;;","07/May/20 16:04;dian.fu;Merged to
- master via 654897102f0354e73c0a552c416e45ad9168c0f7
- release-1.10 via 92f1ce7225e5adcf12e473e7efe992dea193368b
- release-1.9 via 8e921fb921291e34e67f01a062e10005eecb031e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Constant exists in group window key leads to  error:  Unsupported call: TUMBLE_END(TIMESTAMP(3) NOT NULL),FLINK-17553,13303337,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Terry1897,Terry1897,Terry1897,07/May/20 09:57,08/Jun/20 09:46,13/Jul/23 08:07,05/Jun/20 08:29,,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Exception stack is as following:
 !temp.png! 

We can reproduce this problem by add following test in org.apache.flink.table.planner.runtime.stream.sql#TimeAttributeITCase

{code:scala}
// Some comments here
  @Test
  def testWindowAggregateOnConstantValue(): Unit = {
    val ddl1 =
      """"""
        |CREATE TABLE src (
        |  log_ts STRING,
        |  ts TIMESTAMP(3),
        |  a INT,
        |  b DOUBLE,
        |  rowtime AS CAST(log_ts AS TIMESTAMP(3)),
        |  WATERMARK FOR rowtime AS rowtime - INTERVAL '0.001' SECOND
        |) WITH (
        |  'connector' = 'COLLECTION',
        |  'is-bounded' = 'false'
        |)
      """""".stripMargin
    val ddl2 =
      """"""
        |CREATE TABLE dst (
        |  ts TIMESTAMP(3),
        |  a BIGINT,
        |  b DOUBLE
        |) WITH (
        |  'connector.type' = 'filesystem',
        |  'connector.path' = '/tmp/1',
        |  'format.type' = 'csv'
        |)
      """""".stripMargin
    val query =
      """"""
        |INSERT INTO dst
        |SELECT TUMBLE_END(rowtime, INTERVAL '0.003' SECOND), COUNT(ts), SUM(b)
        |FROM src
        | GROUP BY 'a', TUMBLE(rowtime, INTERVAL '0.003' SECOND)
        |-- GROUP BY TUMBLE(rowtime, INTERVAL '0.003' SECOND)
      """""".stripMargin
    tEnv.sqlUpdate(ddl1)
    tEnv.sqlUpdate(ddl2)
    tEnv.sqlUpdate(query)
    println(tEnv.explain(true))
  }

{code}







I spent lots of work digging into this bug, and found the problem may be caused by AggregateProjectPullUpConstantsRule which doesn't generate proper project items correctly.
After I remove AggregateProjectPullUpConstantsRule from FlinkStreamRuleSets#DEFAULT_REWRITE_RULES, the test passed as expect.

The problem is that WindowPropertiesRule can not match RelNodeTree after the transformation of AggregateProjectPullUpConstantsRule, we also can add ProjectMergeRule.INSTANCE in FlinkStreamRuleSets#DEFAULT_REWRITE_RULES after AggregateProjectPullUpConstantsRule.INSTANCE to solve this problem.











",,danny0405,godfreyhe,jark,libenchao,Terry1897,zhanghang-dev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18179,,,,,,,,,,,,,,FLINK-12249,,,,,,,,,"07/May/20 10:02;Terry1897;temp.png;https://issues.apache.org/jira/secure/attachment/13002266/temp.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 08:29:22 UTC 2020,,,,,,,,,,"0|z0egqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/20 06:12;Terry1897;I open a PR https://github.com/apache/flink/pull/12028 to help understanding and solving this bug.;;;","20/May/20 09:24;danny0405;cc [~godfreyhe], can you take a look for this PR ~;;;","02/Jun/20 08:18;danny0405;cc [~jark] cc [~lzljs3620320] please help to merge this PR.;;;","05/Jun/20 08:29;jark;- master (1.12.0): c67109e99437d1dcca9fb6fca9f2741747c82a04
- 1.11.0: 3fe471ffaccadf674f310019db5ccdc9a3ae94e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 UnionInputGate shouldn't be caching InputChannels ,FLINK-17552,13303334,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,07/May/20 09:49,07/May/20 13:36,13/Jul/23 08:07,07/May/20 13:36,,,,,,1.11.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,For example currently UnionInputGate#getChannel can become inconsistent with SingleInputGate#getChannel after updating a channel inside SingleInputGate.,,pnowojski,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 13:36:56 UTC 2020,,,,,,,,,,"0|z0egps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/20 13:36;pnowojski;Merged as f1d6336ebc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rerunning failed azure jobs fails when uploading logs,FLINK-17543,13303108,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,chesnay,chesnay,06/May/20 13:06,16/Oct/20 10:48,13/Jul/23 08:07,18/May/20 13:18,,,,,,1.11.1,,,,Build System / Azure Pipelines,,,,,0,pull-request-available,,,,"{code}
No LastRequestResponse on exception ArtifactExistsException: Artifact logs-ci-tests already exists for build 677.
{code}",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17731,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 03 07:46:19 UTC 2020,,,,,,,,,,"0|z0efmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 13:18;rmetzger;Resolved in https://github.com/apache/flink/commit/caabaf82b5bed86f6a4f1ac931c073006c9d5c78;;;","03/Jul/20 06:06;dian.fu;A similar case on the master branch? 
 [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4108&view=logs&j=e428c73e-5bb8-51ab-b1f4-da736192251d&t=a1a976bb-2472-5564-e31b-a98cbf1a703f]
{code:java}
Information, ApplicationInsightsTelemetrySender correlated 2 events with X-TFS-Session 9454024f-0ee3-4ad3-83f8-77942ea24458
##[error]Artifact logs-ci-e2e already exists for build 4108.
{code}
 

Besides, just found that this PR wasn't present in the 1.11 branch. Should it also be backport to 1.11? [~rmetzger];;;","03/Jul/20 07:46;rmetzger;I pushed a fix to release-1.11 as well: https://github.com/apache/flink/commit/f581764ccf67336dc16add6a183db1a2f9ddb3e9
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskCancelerWatchdog does not kill TaskManager,FLINK-17514,13302812,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,aljoscha,aljoscha,05/May/20 07:48,06/May/20 16:07,13/Jul/23 08:07,06/May/20 07:24,1.10.1,1.11.0,,,,1.10.1,1.11.0,,,Runtime / Task,,,,,0,pull-request-available,,,,"The watchdog reports a fatal error using {{taskManager.notifyFatalError(msg, null)}}. This should normally lead to the TaskManager being terminated. The code introduced in FLINK-16225
 tries to look at the passed exception and will eventually fail with a {{NullPointerException}}, which prevents the TaskManager from being terminated.

Stacktrace:
{code:java}
2020-05-05 09:43:01,588 ERROR org.apache.flink.runtime.taskmanager.Task                     - Task did not exit gracefully within 180 + seconds.
2020-05-05 09:43:01,588 ERROR org.apache.flink.runtime.taskexecutor.TaskExecutor            - Task did not exit gracefully within 180 + seconds.
2020-05-05 09:43:01,588 ERROR org.apache.flink.runtime.taskmanager.Task                     - Error in Task Cancellation Watch Dog
java.lang.NullPointerException
	at org.apache.flink.util.ExceptionUtils.isOutOfMemoryErrorWithMessageStartingWith(ExceptionUtils.java:186)
	at org.apache.flink.util.ExceptionUtils.isMetaspaceOutOfMemoryError(ExceptionUtils.java:170)
	at org.apache.flink.util.ExceptionUtils.enrichTaskManagerOutOfMemoryError(ExceptionUtils.java:144)
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.onFatalError(TaskManagerRunner.java:249)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor$TaskManagerActionsImpl.notifyFatalError(TaskExecutor.java:1751)
	at org.apache.flink.runtime.taskmanager.Task$TaskCancelerWatchDog.run(Task.java:1514)
	at java.lang.Thread.run(Thread.java:748)
{code}",,aljoscha,felixzheng,klion26,liyu,maddisondavid,rmetzger,trohrmann,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16225,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 16:07:35 UTC 2020,,,,,,,,,,"0|z0edsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/20 08:04;rmetzger;Are you sure FLINK-16255 caused this?;;;","05/May/20 08:11;aljoscha;Typo, dammit... 😅;;;","05/May/20 08:13;aljoscha;Btw, FLINK-16225 is not marked as closed and the fix version is 1.10.2, but the code is on release-1.10 and in the RC for 1.10.1;;;","05/May/20 08:55;liyu;If I understand it correctly, the changes merged into release-1.10 from FLINK-16225 were for error messages improvement rather than real fix of the issue, so the fix version was changed to 1.10.2 for a real fix.

OTOH, since the merged changes caused the NPE regression, let's fix this before releasing 1.10.1;;;","05/May/20 09:04;trohrmann;I think the problem is two-fold. The change introduced with FLINK-16225 assumes that the value is non-null. I think we should harden this since fatal failure handling code should not fail. The other part is that the {{TaskCancelerWatchDog}} calls {{TaskManagerActions.notifyFatalError}} with a null argument. The interface does not allow this.;;;","05/May/20 09:08;trohrmann;Quick question: Was the {{TaskExecutor}} still killed? If I'm not mistaken then we should have set an uncaught exception handler which terminates the process because of the NPE.;;;","05/May/20 09:11;trohrmann;I think the problem is that the NPE is not a fatal error and hence it won't be rethrown. This is also something we should change.;;;","06/May/20 07:24;trohrmann;Fixed via

1.11.0: 
0ea539d837c43541110f327a10ec207a259a32ed
221bbf792e60de7cc6197612cd3fa3621893961a
fdd128af3be23ad5b21055f3fee69602bf7712b5

1.10.1:
bd76b42228bde4f25969d65712f7799771fb91e9
f3f1bd3020c473d65fe8963262b8962d20e84d6d
48ae089e15e9a324897869e9cf2577587aceffbf;;;","06/May/20 16:07;aljoscha;Maybe a bit late, but no, the {{TaskExecutor}} was not killed, it kept running forever after wards. (at least until I killed it finally ... 😉);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointEnvironment does not honour 'io.tmp.dirs' property,FLINK-17506,13302685,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,david.artiga,david.artiga,04/May/20 16:55,08/Sep/21 14:53,13/Jul/23 08:07,18/May/20 18:58,1.10.2,1.11.0,1.12.0,,,,,,,API / State Processor,,,,,0,pull-request-available,,,,"{{SavepointEnvironment}} [creates an IOManagerAsync|https://github.com/apache/flink/blob/d6439c8d0e7792961635e3e4297c3dbfb01938e3/flink-libraries/flink-state-processing-api/src/main/java/org/apache/flink/state/api/runtime/SavepointEnvironment.java#L106] using its [default constructor|https://github.com/apache/flink/blob/d6439c8d0e7792961635e3e4297c3dbfb01938e3/flink-runtime/src/main/java/org/apache/flink/runtime/io/disk/iomanager/IOManagerAsync.java#L62], meaning it [uses env var ""java.io.tmpdir""|https://github.com/apache/flink/blob/d6439c8d0e7792961635e3e4297c3dbfb01938e3/flink-runtime/src/main/java/org/apache/flink/runtime/util/EnvironmentInformation.java#L227] instead of values from ""io.tmp.dirs"" config property,",,david.artiga,dcausse,felixzheng,juha.mynttinen,liyu,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 08 14:53:43 UTC 2021,,,,,,,,,,"0|z0ed0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/20 07:55;david.artiga;{code:java}
this.ioManager = new IOManagerAsync(ConfigurationUtils.parseTempDirectories(configuration)); {code}
^^ Made the trick for us;;;","08/May/20 13:14;sjwiesman;Unfortunately, that isn't quite correct. The configuration object in the SavepointEnvironment is empty. I've opened a PR that we can get into 1.11 and 1.10.2. Its unfortunately to late for 1.10.1. ;;;","08/May/20 13:27;david.artiga;Yes, you are right. Had to override {{java.io.tmpdir}} and run it locally to make it work. ;;;","08/May/20 20:16;david.artiga;Applied your change to our fork. Still using '/tmp'. Could it be related to [this|https://github.com/apache/flink/blob/571c0892fa30a34751b575c10b78472eed8849e2/flink-libraries/flink-state-processing-api/src/main/java/org/apache/flink/state/api/runtime/SavepointEnvironment.java#L282] and not calling {{Builder::setConfiguration()}} anywhere [here|https://github.com/apache/flink/blob/571c0892fa30a34751b575c10b78472eed8849e2/flink-libraries/flink-state-processing-api/src/main/java/org/apache/flink/state/api/input/KeyedStateInputFormat.java#L139-L143]?

Including stacktrace:
{code:java}
2020-05-08 21:55:08
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:186)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:180)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:462)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:367)
	at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.io.IOException: Failed to restore state backend
	at org.apache.flink.state.api.input.KeyedStateInputFormat.getStreamOperatorStateContext(KeyedStateInputFormat.java:177)
	at org.apache.flink.state.api.input.KeyedStateInputFormat.open(KeyedStateInputFormat.java:145)
	at org.apache.flink.state.api.input.KeyedStateInputFormat.open(KeyedStateInputFormat.java:63)
	at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:173)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: Exception while creating StreamOperatorStateContext.
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)
	at org.apache.flink.state.api.input.KeyedStateInputFormat.getStreamOperatorStateContext(KeyedStateInputFormat.java:168)
	... 6 more
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for 707eed42a9b74f065cc8bb6798b04782_707eed42a9b74f065cc8bb6798b04782_(8/128) from any of the 1 provided restore options.
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)
	... 7 more
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:324)
	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:560)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)
	... 9 more
Caused by: org.rocksdb.RocksDBException: While appending to file: /tmp/flink-io-b9e45ad7-9765-4225-ab36-210804b9fa36/job_c543bf4d4f8ed7396bf4bc3523ab8ab9_op_707eed42a9b74f065cc8bb6798b04782_707eed42a9b74f065cc8bb6798b04782__8_128__uuid_117bcd9f-f325-4561-9f12-f10e71e7e5ae/db/000509.sst: No space left on device
	at org.rocksdb.RocksDB.write0(Native Method)
	at org.rocksdb.RocksDB.write(RocksDB.java:602)
	at org.apache.flink.contrib.streaming.state.RocksDBWriteBatchWrapper.flush(RocksDBWriteBatchWrapper.java:112)
	at org.apache.flink.contrib.streaming.state.RocksDBWriteBatchWrapper.flushIfNeeded(RocksDBWriteBatchWrapper.java:133)
	at org.apache.flink.contrib.streaming.state.RocksDBWriteBatchWrapper.put(RocksDBWriteBatchWrapper.java:94)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:236)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:256)
	... 13 more
 {code};;;","18/May/20 18:57;sjwiesman;[~david.artiga] That has been resolved. 

Fixed in 
Master: 7934fc04ac098e2143cab9459b216725fe1b8be6
release-1.11: ce10080de0e6c35d60147318381c096cf77ec252
release-1.10: 313051c0ffdacb1bca2d26482bd9bc7036bf9fd2;;;","08/Sep/21 14:53;dcausse;I'm seeing a _similar_ problem (with 1.12.1, 1.13.2 and 1.14-RC0) when running in yarn, rocksdb restores itself to */tmp* while when running with yarn *LOCAL_DIRS* should be used instead I think.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Improve logging in AbstractServerHandler#channelRead(ChannelHandlerContext, Object)",FLINK-17501,13302611,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gjy,gjy,gjy,04/May/20 10:28,06/May/20 08:13,13/Jul/23 08:07,06/May/20 08:13,,,,,,1.11.0,,,,Runtime / Queryable State,,,,,0,pull-request-available,,,,"Improve logging in {{AbstractServerHandler#channelRead(ChannelHandlerContext, Object)}}. If an Error is thrown, it should be logged as early as possible. Currently we try to serialize and send an error response to the client before logging the error; this can fail and mask the original exception.",,gjy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13553,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 08:13:10 UTC 2020,,,,,,,,,,"0|z0eck8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/20 08:13;gjy;master: c2540e44058a313a8dc7251dd5d37d2d52db2b44;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LazyTimerService used to register timers via State Processing API incorrectly mixes event time timers with processing time timers,FLINK-17499,13302517,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,alaczynski,alaczynski,03/May/20 14:36,10/May/20 11:38,13/Jul/23 08:07,04/May/20 14:57,1.10.0,1.11.0,,,,1.10.1,,,,API / State Processor,,,,,0,pull-request-available,,,,"@Override
 public void register*{color:#FF0000}ProcessingTime{color}*Timer(long time) {
   ensureInitialized();
   internalTimerService.register{color:#ff0000}*EventTime*{color}Timer(VoidNamespace.INSTANCE, time);
 }

Same issue for both registerEventTimeTimer and registerProcessingTimeTimer.

[https://github.com/apache/flink/blob/master/flink-libraries/flink-state-processing-api/src/main/java/org/apache/flink/state/api/output/operators/LazyTimerService.java#L62]",,alaczynski,liyu,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 04 14:56:53 UTC 2020,,,,,,,,,,"0|z0ebzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/20 18:28;sjwiesman;Thanks for opening this ticket!;;;","04/May/20 14:56;sjwiesman;Fixed in master: d6439c8d0e7792961635e3e4297c3dbfb01938e3
Fixed in release-1.10: ebba1589d8b407f53bcc4e325cd63eaa6b30870b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MapCancelingITCase.testMapCancelling fails with timeout,FLINK-17498,13302514,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,rmetzger,rmetzger,03/May/20 13:27,10/Jun/20 09:31,13/Jul/23 08:07,10/Jun/20 09:31,1.11.0,,,,,1.11.0,,,,Runtime / Coordination,Runtime / Task,Tests,,,0,pull-request-available,test-stability,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=540&view=logs&j=e824df38-aa5e-5531-f993-88388cf903b8&t=4df9e78d-8d99-5a27-509a-217c7c98d003

{code}

2020-05-02T20:43:28.3489682Z [ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 23.658 s <<< FAILURE! - in org.apache.flink.test.cancelling.MapCancelingITCase
2020-05-02T20:43:28.3490644Z [ERROR] testMapCancelling(org.apache.flink.test.cancelling.MapCancelingITCase)  Time elapsed: 7.841 s  <<< ERROR!
2020-05-02T20:43:28.3492751Z java.util.concurrent.TimeoutException
2020-05-02T20:43:28.3493318Z 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)
2020-05-02T20:43:28.3493948Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-05-02T20:43:28.3494465Z 	at org.apache.flink.test.cancelling.CancelingTestBase.runAndCancelJob(CancelingTestBase.java:122)
2020-05-02T20:43:28.3521753Z 	at org.apache.flink.test.cancelling.MapCancelingITCase.executeTask(MapCancelingITCase.java:65)
2020-05-02T20:43:28.3522609Z 	at org.apache.flink.test.cancelling.MapCancelingITCase.testMapCancelling(MapCancelingITCase.java:37)
2020-05-02T20:43:28.3523432Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-02T20:43:28.3524132Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-02T20:43:28.3524934Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-02T20:43:28.3525613Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-02T20:43:28.3526249Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-02T20:43:28.3526716Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-02T20:43:28.3527291Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-02T20:43:28.3528094Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-02T20:43:28.3528581Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-02T20:43:28.3528986Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-02T20:43:28.3529347Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-02T20:43:28.3529807Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-02T20:43:28.3530248Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-02T20:43:28.3530707Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-02T20:43:28.3531072Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-02T20:43:28.3531504Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-02T20:43:28.3531937Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-02T20:43:28.3532317Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-02T20:43:28.3533026Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-02T20:43:28.3533620Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-02T20:43:28.3534443Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-02T20:43:28.3535012Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-02T20:43:28.3535613Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-02T20:43:28.3536258Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-02T20:43:28.3536875Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-02T20:43:28.3537588Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-02T20:43:28.3538349Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-02T20:43:28.3539048Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-02T20:43:28.3539674Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-02T20:43:28.3540368Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2020-05-02T20:43:28.3541211Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2020-05-02T20:43:28.3542054Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2020-05-02T20:43:28.3543084Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2020-05-02T20:43:28.3544002Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2020-05-02T20:43:28.3544871Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2020-05-02T20:43:28.3545781Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-02T20:43:28.3546588Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-02T20:43:28.3547650Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-02T20:43:28.3548460Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-02T20:43:28.3549085Z 

{code}",,aljoscha,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 09:31:37 UTC 2020,,,,,,,,,,"0|z0ebyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/20 14:01;trohrmann;The problem seems to be that the RPC timeout of 1s is set too low for the CI infrastructure. I propose to increase the timeout to the configured Akka ask timeout.;;;","09/Jun/20 06:46;rmetzger;Sounds good to me.;;;","09/Jun/20 08:01;trohrmann;[~rmetzger] could you take a look at the PR?;;;","09/Jun/20 14:37;rmetzger;Yes, I will!;;;","10/Jun/20 09:31;trohrmann;Fixed via

master: 1839fa57a91723f8ef10bcbd2c271366b5509b0b
1.11.0: b1f32c6efd732787e27f37fbe18fe88f9e38f7c2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Quickstarts Java nightly end-to-end test fails with ""class file has wrong version 55.0, should be 52.0""",FLINK-17497,13302513,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,03/May/20 13:22,17/Jun/20 05:06,13/Jul/23 08:07,12/May/20 11:44,1.11.0,,,,,1.11.0,,,,Build System / Azure Pipelines,Tests,,,,0,pull-request-available,,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=540&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334

{code}

[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
java.io.FileNotFoundException: flink-quickstart-java-0.1.jar (No such file or directory)
	at java.util.zip.ZipFile.open(Native Method)
	at java.util.zip.ZipFile.<init>(ZipFile.java:230)
	at java.util.zip.ZipFile.<init>(ZipFile.java:160)
	at java.util.zip.ZipFile.<init>(ZipFile.java:131)
	at sun.tools.jar.Main.list(Main.java:1115)
	at sun.tools.jar.Main.run(Main.java:293)
	at sun.tools.jar.Main.main(Main.java:1288)
Success: There are no flink core classes are contained in the jar.
Failure: Since Elasticsearch5SinkExample.class and other user classes are not included in the jar. 
[FAIL] Test script contains errors.
Checking for errors...
No errors in log files.
Checking for exceptions...
No exceptions in log files.
Checking for non-empty .out files...
grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directory
No non-empty .out files.

[FAIL] 'Quickstarts Java nightly end-to-end test' failed after 0 minutes and 6 seconds! Test exited with exit code 1


{code}
",,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18341,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 11:44:02 UTC 2020,,,,,,,,,,"0|z0ebyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/20 13:23;rmetzger;Most likely a jdk8, jdk11 cache mixup. I will open a PR for this.;;;","05/May/20 06:09;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=585&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334;;;","08/May/20 06:14;pnowojski;Another instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=773&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334;;;","12/May/20 11:44;rmetzger;Resolved in https://github.com/apache/flink/commit/de9ad1b9705882edb0b94e26f46ae17251ede93c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression with amazon-kinesis-producer 0.13.1 in Flink 1.10.x,FLINK-17496,13302388,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,02/May/20 02:50,06/May/20 05:23,13/Jul/23 08:07,06/May/20 05:23,1.10.0,,,,,1.10.1,1.11.0,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,,"The KPL upgrade in 1.10.0 has introduced a performance issue, which can be addressed by reverting to 0.12.9 or forward fix with 0.14.0. ",liyu,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 05:23:18 UTC 2020,,,,,,,,,,"0|z0eb6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/20 05:23;liyu;Thanks for logging and fixing this issue [~thw]!

Merged (by Thomas) into
master via a737cdcbdb972913e2e31946f7a2bb9175945a29
release-1.10 via 99ea1aac8338fa7d5c947f72943e5a3dfc0c0dbe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support any kind of array in StringUtils.arrayAwareToString(),FLINK-17489,13302285,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,01/May/20 10:04,05/May/20 06:28,13/Jul/23 08:07,05/May/20 06:28,,,,,,1.11.0,,,,API / Core,,,,,0,pull-request-available,,,,"{{StringUtils.arrayAwareToString()}} is a very basic implementation for one-level nested arrays. However, such a prominent utility class should support all kinds of nested arrays. Both FLINK-16817 and FLINK-17175 tried to improve the implementation but the utility still does not support even {{int[][]}}.",,jark,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 05 06:28:03 UTC 2020,,,,,,,,,,"0|z0eajs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/20 06:28;twalthr;Fixed in 1.11.0: 7b2f000da12d719752444544c37933e53b41d573;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update flink-sql-connector-elasticsearch7 NOTICE file to correctly reflect bundled dependencies,FLINK-17483,13302095,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,liyu,liyu,liyu,30/Apr/20 13:37,04/May/20 12:47,13/Jul/23 08:07,04/May/20 12:47,1.10.0,1.11.0,,,,1.10.1,1.11.0,,,Connectors / ElasticSearch,,,,,0,legal,pull-request-available,,,"This issue is found during 1.10.1 RC1 check by Robert, that `com.carrotsearch:hppc` and `com.github:mustachejava` were included into the shaded binary to fix FLINK-16170 but not added into the NOTICE file of flink-sql-connector-elasticsearch7 module. More details please refer to the [ML discussion thread|http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Release-1-10-1-release-candidate-1-tp40724p40894.html].",,jark,libenchao,liyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16170,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 04 12:47:42 UTC 2020,,,,,,,,,,"0|z0e9dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Apr/20 13:38;liyu;Thanks [~jark] for volunteering to fix this issue!;;;","30/Apr/20 18:04;liyu;Let me fix the issue instead, to generate the RC2 for 1.10.1 asap.;;;","01/May/20 04:39;liyu;Merged into release-1.10 via 2c5751bac66190a6eebcd30fa5db5b84a893bc73;;;","04/May/20 12:47;liyu;Merged into master via 7d6aa347eaf4b401c427089dd0e9ebb3fa6c10d5.

Closing issue since all work done.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move LICENSE and NOTICE files to root directory of python distribution,FLINK-17471,13301981,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zhongwei,liyu,liyu,30/Apr/20 04:58,30/Apr/20 17:16,13/Jul/23 08:07,30/Apr/20 12:43,1.10.0,1.11.0,1.9.3,,,1.10.1,1.11.0,1.9.4,,API / Python,,,,,0,legal,pull-request-available,,,"This is observed and proposed by Robert during 1.10.1 RC1 check:
{noformat}
Another question that I had while checking the release was the
""apache-flink-1.10.1.tar.gz"" binary, which I suppose is the python
distribution.
It does not contain a LICENSE and NOTICE file at the root level (which is
okay [1] for binary releases), but in the ""pyflink/"" directory. There is
also a ""deps/"" directory, which contains a full distribution of Flink,
without any license files.
I believe it would be a little bit nicer to have the LICENSE and NOTICE
file in the root directory (if the python wheels format permits) to make
sure it is obvious that all binary release contents are covered by these
files.
{noformat}

http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Release-1-10-1-release-candidate-1-tp40724p40910.html",,dian.fu,hxbks2ks,liyu,rmetzger,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/20 05:57;zhongwei;image-2020-04-30-13-57-28-072.png;https://issues.apache.org/jira/secure/attachment/13001681/image-2020-04-30-13-57-28-072.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 17:16:15 UTC 2020,,,,,,,,,,"0|z0e8o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Apr/20 04:59;liyu;[~jincheng] [~hequn8128] [~dianfu] could you take a look here? Thanks.;;;","30/Apr/20 05:04;dian.fu;[~liyu] Thanks for filing this ticket. I will take a look ASAP.;;;","30/Apr/20 05:05;liyu;Thanks for the quick response [~dian.fu]!;;;","30/Apr/20 05:59;zhongwei;Hi [~chesnay]  [~rmetzger], in current structure of ""apache-flink-1.10.1.tar.gz"" we already packed the license files in 2 place: ""apache-flink-1.10.1.tar.gz/apache-flink-1.10.1/deps/licenses"" and ""apache-flink-1.10.1.tar.gz/apache-flink-1.10.1/pyflink/LICENSE"":

!image-2020-04-30-13-57-28-072.png|width=732,height=660!

Do you mean to move them to the root of ""apache-flink-1.10.1.tar.gz"", i.e. ""apache-flink-1.10.1.tar.gz/apache-flink-1.10.1/""?;;;","30/Apr/20 06:16;rmetzger;Thanks a lot for addressing this!

In my opinion, I would move the apache-flink-1.10.1.tar.gz/apache-flink-1.10.1/pyflink/{LICENSE,NOTICE} files to apache-flink-1.10.1.tar.gz/apache-flink-1.10.1/ (package root), and leave the ""apache-flink-1.10.1.tar.gz/apache-flink-1.10.1/deps/licenses"" files as-is (because they contain the license files of some of Flink's dependencies.

;;;","30/Apr/20 06:59;zhongwei;Thanks for the explanation Robert. I'll submit a PR to move the LICENSE and NOTICE to the package root of the PyFlink distribution.;;;","30/Apr/20 07:03;rmetzger;Thanks. I assigned you to the ticket.;;;","30/Apr/20 12:43;dian.fu;Thanks [~zhongwei] for addressing this and thanks [~rmetzger] for the help.

Merged to
- master via c54293ba385ffc8b6f7a873932da6fa1dee99b8b
- release-1.10 via 752e8a4c204aa4325e5591909c7a175d1c50991a
- release-1.9 via f873a18c9c675fc412e92467593abd6a735cb469;;;","30/Apr/20 17:16;liyu;Thanks for the quick fix folks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink task executor process permanently hangs on `flink-daemon.sh stop`, deletes PID file",FLINK-17470,13301908,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,hherman,hherman,29/Apr/20 20:17,20/Nov/20 12:04,13/Jul/23 08:07,20/Nov/20 12:04,1.10.0,,,,,1.12.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Hi Flink team!

We've attempted to upgrade our flink 1.9 cluster to 1.10, but are experiencing reproducible instability on shutdown. Speciically, it appears that the `kill` issued in the `stop` case of flink-daemon.sh is causing the task executor process to hang permanently. Specifically, the process seems to be hanging in the `org.apache.flink.runtime.util.JvmShutdownSafeguard$DelayedTerminator.run` in a `Thread.sleep()` call. I think this is a bizarre behavior. Also note that every thread in the process is BLOCKED. on a `pthread_cond_wait` call. Is this an OS level issue? Banging my head on a wall here. See attached stack traces for details."," 
{code:java}
$ uname -a
Linux hostname.local 3.10.0-1062.9.1.el7.x86_64 #1 SMP Fri Dec 6 15:49:49 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
$ lsb_release -a
LSB Version:	:core-4.1-amd64:core-4.1-noarch
Distributor ID:	CentOS
Description:	CentOS Linux release 7.7.1908 (Core)
Release:	7.7.1908
Codename:	Core
{code}

Flink version 1.10
 ",aljoscha,hherman,rmetzger,sewen,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16510,,,,,,,,,,,"29/Apr/20 20:17;hherman;flink_jstack.log;https://issues.apache.org/jira/secure/attachment/13001647/flink_jstack.log","29/Apr/20 20:17;hherman;flink_mixed_jstack.log;https://issues.apache.org/jira/secure/attachment/13001646/flink_mixed_jstack.log",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 20 12:04:54 UTC 2020,,,,,,,,,,"0|z0e880:",9223372036854775807,In Flink 1.12 we changed the behavior of the standalone scripts to issue a SIGKILL if a SIGTERM did not succeed in shutting down a Flink process.,,,,,,,,,,,,,,,,,,,"20/May/20 08:04;sewen;What JVM version are you using? Maybe there is different behavior in newer JVM versions?

Nothing changed in The JvmShutdownGuard since many releases, so curious that this changed reproducibly in your setup.;;;","21/May/20 19:38;hherman;Hey [~sewen]! Thanks for the reply. Here's the version of java we're using (we use Azul):
{code:java}
openjdk version ""1.8.0_232""
OpenJDK Runtime Environment (Zulu 8.42.0.14-SA-linux64) (build 1.8.0_232-b18)
OpenJDK 64-Bit Server VM (Zulu 8.42.0.14-SA-linux64) (build 25.232-b18, mixed mode)
{code};;;","23/May/20 15:42;sewen;Have you run Flink 1.9 on the exact same version? And does Flink 1.9 still shut down the JVM properly, while 1.10 does not?;;;","27/May/20 07:38;hherman;We have observed the same problem with 1.9 on the same system setup/java version, however far less frequently. Flink 1.10 experiences this issue around 1 in 3 or 4 restarts, while flink 1.9 seems more like 1 in 15 or so. ;;;","29/May/20 06:47;rmetzger;Just a side note: Flink's end to end tests (which run Flink using the scripts) are also running on a JVM from Azul, specifically: {{OpenJDK 64-Bit Server VM - Azul Systems, Inc. - 1.8/25.252-b14}}. I have never observed shutdown instabilities there. 

From the blog post (https://blogs.oracle.com/poonam/hung-jvm-due-to-the-threads-stuck-in-pthreadcondtimedwait) you've mentioned on the mailing list it seems that the fix in the linux kernel (https://github.com/torvalds/linux/commit/76835b0ebf8a7fe85beb03c75121419a7dec52f0) is available since 3.18. You are on Linux 3.10. Would you be able to validate if this issue still persists on Linux 3.18+ ?;;;","19/Jun/20 18:42;hherman;We're using CentOS 7, which defaults to the 3.10 kernel, so it would be very difficult for us to bump the kernel across the world (https://wiki.centos.org/About/Product); is there any other way for us to go about trying to fix this? ;;;","23/Jun/20 10:25;sewen;I think one think you can do is change the {{""flink-daemon.sh stop""}} command to use SIGKILL instead of SIGTERM.

That should work by avoiding all types of shutdown hooks. It will be not as graceful of an exit, tough. For example, Flink won't clean up its temp directory itself, so you need to eventually clean that up (unless your system environment takes care of that eventually).

An advanced version, you could try to extend that script to send a SIGTERM and a SIGKILL some X seconds later, if the process still exists. That's what Flink does internally but gets hung up due to the JVM/Kernel issue. I don't know from the top of my head how to best do that in bash, though.;;;","24/Jul/20 08:24;trohrmann;Introducing this kind of safety net could be a good idea. Based on https://stackoverflow.com/a/687994/4815083 we could add something like this:

{code}
kill -s SIGTERM $$ && kill -0 $$ || exit 0

((t = delay))

while ((t > 0)); do
    sleep $interval
    kill -0 $$ || exit 0
    ((t -= interval))
done

kill -s SIGKILL $$
{code};;;","29/Sep/20 18:47;hherman;The `kill -9` solution has been working for us in prod. Thanks for taking the time to look at it. The other ticket you referenced [~sewen] is fascinating. ;;;","10/Nov/20 08:10;rmetzger;I will open a PR for the SIGTERM to SIGKILL script extension.;;;","16/Nov/20 13:22;rmetzger;Improved stop behavior in the standalone scripts in https://github.com/apache/flink/commit/30ae028774f389f21bdef0a7fa03c90e178cfa03.;;;","18/Nov/20 13:34;aljoscha;This fails on my machine with

{code}
/Users/aljoscha/Dev/flink/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/bin/flink-daemon.sh: line 98: timeout: command not found
{code}

when running {{bin/stop-cluster.sh}}.

I think that's not a good out-of-box experience for users.;;;","18/Nov/20 13:47;rmetzger;Can you share more details on your environment?;;;","18/Nov/20 13:50;aljoscha;Ah sorry, yes. I'm running macOS Catalina (10.15.7), otherwise I don't know what could be happening. What is {{timeout}} on your machine? Is it a shell builtin, or an actual program? What's the output of {{which timeout}} or {{whereis timeout}}?;;;","18/Nov/20 13:57;rmetzger;Damn. Looks like I have ""timeout"" through ""coreutils"" (installed via homebrew). It is also available in our CI environment, that's why I didn't catch this. I'll reopen this ticket until I've figured a solution.;;;","20/Nov/20 12:04;rmetzger;Fix merged in https://github.com/apache/flink/commit/7e05b335845ee6a0688914cdc862bea107bbf114;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
toRetractStream doesn't work correctly with Pojo conversion class,FLINK-17466,13301828,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,gyfora,gyfora,29/Apr/20 15:30,04/Jun/20 07:30,13/Jul/23 08:07,04/Jun/20 07:30,1.10.0,,,,,1.10.2,1.11.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"The toRetractStream(table, Pojo.class) does not map the query columns properly to the pojo fields.

This either leads to exceptions due to type incompatibility or simply incorrect results.

It can be simple reproduced by the following test code:
{code:java}
@Test
public void testRetract() throws Exception {
 EnvironmentSettings settings = EnvironmentSettings
 .newInstance()
 .useBlinkPlanner()
 .inStreamingMode()
 .build();

 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
 StreamTableEnvironment tableEnv = StreamTableEnvironment
 .create(StreamExecutionEnvironment.getExecutionEnvironment(), settings);

 tableEnv.createTemporaryView(""person"", env.fromElements(new Person()));
 tableEnv.toRetractStream(tableEnv.sqlQuery(""select name, age from person""), Person.class).print();
 tableEnv.execute(""Test"");

}

public static class Person {
 public String name = ""bob"";
 public int age = 1;
}{code}
Runtime Error:
{noformat}
java.lang.ClassCastException: org.apache.flink.table.dataformat.BinaryString cannot be cast to java.lang.Integer{noformat}
Changing the query to ""select age,name from person"" in this case would resolve the problem but it also highlights the possible underlying issue.",,gyfora,jark,knaufk,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/20 12:28;gyfora;retract-issue.patch;https://issues.apache.org/jira/secure/attachment/13001716/retract-issue.patch",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 04:08:27 UTC 2020,,,,,,,,,,"0|z0e7q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/20 15:32;gyfora;cc [~twalthr] , I could not find this issue on Jira before but it could be a duplicate of something else;;;","29/Apr/20 15:45;jark;A duplicate issue of FLINK-16108 ?;;;","29/Apr/20 16:00;gyfora;I think I am running on a branch that already has this fix in, so might be a different bug.;;;","29/Apr/20 16:00;gyfora;But I need to double check and verify this on the release-1.10 branch;;;","29/Apr/20 16:02;gyfora;Also we do not get a planner error as in the linked issue, this is a purely runtime problem that only happens on the retract stream (not the append);;;","30/Apr/20 12:28;gyfora;[~jark] I have confirmed that this is not a duplicate issue and it is still broken on the latest release-1.10 branch. You can verify yourself by applying this patch for the test case: [^retract-issue.patch];;;","01/May/20 14:05;jark;Yes. I checked in master and this is still another bug. Sad...;;;","04/Jun/20 04:08;jark;- master (1.12.0): fabe02177cad5eb99193d2d0cc76ab15be7b3b41
- 1.11.0: 9d7dc33f7c05e6b44293e5bae9dff2fac9f647cf
- 1.10.2: a412840bc077d471813c84d0387a664fd95dae27;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlobCacheCleanupTest.testPermanentBlobCleanup:133->verifyJobCleanup:432 » FileAlreadyExists,FLINK-17463,13301799,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gjy,rmetzger,rmetzger,29/Apr/20 13:27,29/May/20 06:58,13/Jul/23 08:07,29/May/20 06:58,1.11.0,,,,,1.11.0,,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,"CI run: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=317&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d

{code}
[ERROR] Tests run: 5, Failures: 0, Errors: 1, Skipped: 1, Time elapsed: 2.73 s <<< FAILURE! - in org.apache.flink.runtime.blob.BlobCacheCleanupTest
[ERROR] testPermanentBlobCleanup(org.apache.flink.runtime.blob.BlobCacheCleanupTest)  Time elapsed: 2.028 s  <<< ERROR!
java.nio.file.FileAlreadyExistsException: /tmp/junit7984674749832216773/junit1629420330972938723/blobStore-296d1a51-8917-4db1-a920-5d4e17e6fa36/job_3bafac5425979b4fe2fa2c7726f8dd5b
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:727)
	at org.apache.flink.runtime.blob.BlobUtils.getStorageLocation(BlobUtils.java:196)
	at org.apache.flink.runtime.blob.PermanentBlobCache.getStorageLocation(PermanentBlobCache.java:222)
	at org.apache.flink.runtime.blob.BlobServerCleanupTest.checkFilesExist(BlobServerCleanupTest.java:213)
	at org.apache.flink.runtime.blob.BlobCacheCleanupTest.verifyJobCleanup(BlobCacheCleanupTest.java:432)
	at org.apache.flink.runtime.blob.BlobCacheCleanupTest.testPermanentBlobCleanup(BlobCacheCleanupTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)

{code}",,gjy,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 06:58:38 UTC 2020,,,,,,,,,,"0|z0e7js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 12:52;gjy;This is most likely caused due to concurrent calls of
{code}
Files.createDirectories(...);
{code}

and 
{code}
FileUtils.deleteDirectory(...);
{code}

with the same arguments.
;;;","29/May/20 06:58;gjy;1.11: 647f76283c900048e12361cf96d26db2a184b10b
master: c22d01d3bfbb1384f98664361f1491b806e95798;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskExecutorSubmissionTest#testFailingScheduleOrUpdateConsumers,FLINK-17458,13301740,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,klion26,klion26,29/Apr/20 08:27,13/Oct/20 14:08,13/Jul/23 08:07,13/Oct/20 14:08,1.10.0,,,,,1.10.3,1.11.3,1.12.0,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,"When verifying the RC of release-1.10.1, found that `TaskExecutorSubmissionTest#testFailingScheduleOrUpdateConsumers` will fail because of Timeout sometime. 

I run this test locally in IDEA, found the following exception(locally in only encounter 2 in 1000 times)
{code:java}
java.lang.InterruptedExceptionjava.lang.InterruptedException at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1039) at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328) at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:212) at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:222) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:190) at akka.event.LoggingBus$class.akka$event$LoggingBus$$addLogger(Logging.scala:182) at akka.event.LoggingBus$$anonfun$4$$anonfun$apply$4.apply(Logging.scala:117) at akka.event.LoggingBus$$anonfun$4$$anonfun$apply$4.apply(Logging.scala:116) at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) at scala.util.Try$.apply(Try.scala:192) at scala.util.Success.map(Try.scala:237) at akka.event.LoggingBus$$anonfun$4.apply(Logging.scala:116) at akka.event.LoggingBus$$anonfun$4.apply(Logging.scala:113) at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:683) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:682) at akka.event.LoggingBus$class.startDefaultLoggers(Logging.scala:113) at akka.event.EventStream.startDefaultLoggers(EventStream.scala:22) at akka.actor.LocalActorRefProvider.init(ActorRefProvider.scala:662) at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:874) at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:870) at akka.actor.ActorSystemImpl._start(ActorSystem.scala:870) at akka.actor.ActorSystemImpl.start(ActorSystem.scala:891) at akka.actor.RobustActorSystem$.internalApply(RobustActorSystem.scala:96) at akka.actor.RobustActorSystem$.apply(RobustActorSystem.scala:70) at akka.actor.RobustActorSystem$.create(RobustActorSystem.scala:55) at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:125) at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:113) at org.apache.flink.runtime.akka.AkkaUtils$.createLocalActorSystem(AkkaUtils.scala:68) at org.apache.flink.runtime.akka.AkkaUtils.createLocalActorSystem(AkkaUtils.scala) at org.apache.flink.runtime.rpc.TestingRpcService.<init>(TestingRpcService.java:74) at org.apache.flink.runtime.rpc.TestingRpcService.<init>(TestingRpcService.java:67) at org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment$Builder.build(TaskSubmissionTestEnvironment.java:349) at org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.testFailingScheduleOrUpdateConsumers(TaskExecutorSubmissionTest.java:544) at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)
org.junit.runners.model.TestTimedOutException: test timed out after 10000 milliseconds
 at sun.misc.Unsafe.park(Native Method) at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215) at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037) at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328) at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:212) at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:222) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:190) at akka.event.LoggingBus$class.akka$event$LoggingBus$$addLogger(Logging.scala:182) at akka.event.LoggingBus$$anonfun$4$$anonfun$apply$4.apply(Logging.scala:117) at akka.event.LoggingBus$$anonfun$4$$anonfun$apply$4.apply(Logging.scala:116) at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) at scala.util.Try$.apply(Try.scala:192) at scala.util.Success.map(Try.scala:237) at akka.event.LoggingBus$$anonfun$4.apply(Logging.scala:116) at akka.event.LoggingBus$$anonfun$4.apply(Logging.scala:113) at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:683) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:682) at akka.event.LoggingBus$class.startDefaultLoggers(Logging.scala:113) at akka.event.EventStream.startDefaultLoggers(EventStream.scala:22) at akka.actor.LocalActorRefProvider.init(ActorRefProvider.scala:662) at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:874) at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:870) at akka.actor.ActorSystemImpl._start(ActorSystem.scala:870) at akka.actor.ActorSystemImpl.start(ActorSystem.scala:891) at akka.actor.RobustActorSystem$.internalApply(RobustActorSystem.scala:96) at akka.actor.RobustActorSystem$.apply(RobustActorSystem.scala:70) at akka.actor.RobustActorSystem$.create(RobustActorSystem.scala:55) at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:125) at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:113) at org.apache.flink.runtime.akka.AkkaUtils$.createLocalActorSystem(AkkaUtils.scala:68) at org.apache.flink.runtime.akka.AkkaUtils.createLocalActorSystem(AkkaUtils.scala) at org.apache.flink.runtime.rpc.TestingRpcService.<init>(TestingRpcService.java:74) at org.apache.flink.runtime.rpc.TestingRpcService.<init>(TestingRpcService.java:67) at org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment$Builder.build(TaskSubmissionTestEnvironment.java:349) at org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.testFailingScheduleOrUpdateConsumers(TaskExecutorSubmissionTest.java:544) at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)
{code}
 ",,klion26,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 14:08:48 UTC 2020,,,,,,,,,,"0|z0e76o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/20 06:18;zhuzh;I encountered error below when running this test locally for 1000 times. 
Seems the actor system and its threads were not properly released, leading to unhealthy memory status.
It might also be the cause of this JIRA issue.

java.lang.OutOfMemoryError: unable to create new native thread

	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:717)
	at akka.actor.LightArrayRevolverScheduler.<init>(LightArrayRevolverScheduler.scala:298)
	at sun.reflect.GeneratedConstructorAccessor16.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply(ReflectiveDynamicAccess.scala:33)
	at scala.util.Try$.apply(Try.scala:192)
	at akka.actor.ReflectiveDynamicAccess.createInstanceFor(ReflectiveDynamicAccess.scala:28)
	at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(ReflectiveDynamicAccess.scala:39)
	at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(ReflectiveDynamicAccess.scala:39)
	at scala.util.Success.flatMap(Try.scala:231)
	at akka.actor.ReflectiveDynamicAccess.createInstanceFor(ReflectiveDynamicAccess.scala:39)
	at akka.actor.ActorSystemImpl.createScheduler(ActorSystem.scala:925)
	at akka.actor.ActorSystemImpl.<init>(ActorSystem.scala:786)
	at akka.actor.RobustActorSystem.<init>(RobustActorSystem.scala:41)
	at akka.actor.RobustActorSystem$.internalApply(RobustActorSystem.scala:89)
	at akka.actor.RobustActorSystem$.apply(RobustActorSystem.scala:70)
	at akka.actor.RobustActorSystem$.create(RobustActorSystem.scala:55)
	at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:125)
	at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:113)
	at org.apache.flink.runtime.akka.AkkaUtils$.createLocalActorSystem(AkkaUtils.scala:68)
	at org.apache.flink.runtime.akka.AkkaUtils.createLocalActorSystem(AkkaUtils.scala)
	at org.apache.flink.runtime.rpc.TestingRpcService.<init>(TestingRpcService.java:74)
	at org.apache.flink.runtime.rpc.TestingRpcService.<init>(TestingRpcService.java:67)
	at org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment$Builder.build(TaskSubmissionTestEnvironment.java:349)
	at org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.testFailingScheduleOrUpdateConsumers(TaskExecutorSubmissionTest.java:545)
	at sun.reflect.GeneratedMethodAccessor35.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

;;;","09/Oct/20 11:00;trohrmann;The problem seems to be that the {{TaskSubmissionTestEnvironment}} does not stop the internal {{TestingRpcService}} properly.;;;","13/Oct/20 14:08;trohrmann;Fixed via

1.12.0: 31996062388371a938af0bcce1ea05989aff07e3
1.11.3: 8dc13e8c7dab7bd740331a6edd44aa8978b95120
1.10.3: fb34ce741a26fede33c6ae316558c8bd9b10e5d6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_configuration.py ConfigurationTests::test_add_all failed on travis,FLINK-17454,13301710,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,pnowojski,pnowojski,29/Apr/20 05:33,14/May/20 02:56,13/Jul/23 08:07,11/May/20 08:42,,,,,,1.11.0,,,,API / Python,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=383&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=455fddbf-5921-5b71-25ac-92992ad80b28


{code:java}
=========================== short test summary info ============================
FAILED pyflink/common/tests/test_configuration.py::ConfigurationTests::test_add_all
====== 1 failed, 499 passed, 19 skipped, 97 warnings in 182.59s (0:03:02) ======
ERROR: InvocationError for command /__w/1/s/flink-python/.tox/py37-cython/bin/pytest --durations=0 (exited with code 1)
{code}
",,dian.fu,pnowojski,sunjincheng121,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17597,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 08:42:49 UTC 2020,,,,,,,,,,"0|z0e700:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/20 06:13;dian.fu;cc [~zhongwei] ;;;","29/Apr/20 06:40;zhongwei;Hi [~dian.fu], the root cause of the test failure is ""Address already in use"".  It is because the port of the Py4j CallbackServer is occupied. I'll change the port determine logic to solve the problem.;;;","11/May/20 08:42;sunjincheng121;Merged in to Master: 5f744d3f81bcfb8f77164a5ec9caa4594851d4bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential Buffer leak in output unspilling for unaligned checkpoints,FLINK-17440,13301586,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zjwang,pnowojski,pnowojski,28/Apr/20 14:39,29/Apr/20 11:17,13/Jul/23 08:07,29/Apr/20 11:17,,,,,,1.11.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"{code}
@Override
public void PipelinedSubpartition#initializeState(ChannelStateReader stateReader) throws IOException, InterruptedException {
	for (ReadResult readResult = ReadResult.HAS_MORE_DATA; readResult == ReadResult.HAS_MORE_DATA;) {
		BufferBuilder bufferBuilder = parent.getBufferPool().requestBufferBuilderBlocking();
		BufferConsumer bufferConsumer = bufferBuilder.createBufferConsumer();
		readResult = stateReader.readOutputData(subpartitionInfo, bufferBuilder); // <<<<<<<<<<<<<<<<<<<<<<<
		// check whether there are some states data filled in this time
		if (bufferConsumer.isDataAvailable()) {
			add(bufferConsumer, false, false);
			bufferBuilder.finish();
		} else {
			bufferConsumer.close();
		}
	}
}
{code}

There is a memory leak in output unspilling in case of an exception coming from the {{stateReader}}.",,pnowojski,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16537,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 29 11:17:42 UTC 2020,,,,,,,,,,"0|z0e68o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/20 11:17;zjwang;Merged into master: 72f528d0cda2f51c5c71395ad619d49009af2f77;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink StreamingFileSink chinese garbled,FLINK-17438,13301538,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,why198852,why198852,28/Apr/20 11:35,06/May/20 12:09,13/Jul/23 08:07,06/May/20 12:09,1.10.0,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,,,,,"val writer:CompressWriterFactory[String] = new CompressWriterFactory[String](new DefaultExtractor[String]())
 .withHadoopCompression(s""SnappyCodec"")//${compress}

 val fileConfig = OutputFileConfig.builder().withPartPrefix(s""${prefix}"").withPartSuffix(s""${suffix}"").build()

 val bulkFormatBuilder = StreamingFileSink.forBulkFormat(new Path(output), writer)
 // 自定义分桶策略
 bulkFormatBuilder.withBucketAssigner(new DemoAssigner())
 // 自定义输出文件配置
 bulkFormatBuilder.withOutputFileConfig(fileConfig)

 val sink = bulkFormatBuilder.build()

// val rollingPolicy = DefaultRollingPolicy.builder().withRolloverInterval(TimeUnit.MINUTES.toMillis(5)).withInactivityInterval(TimeUnit.MINUTES.toMillis(3)).withMaxPartSize(1 * 1024 * 1024)
// val bulkFormatBuilder = StreamingFileSink.forRowFormat(new Path(output), new SimpleStringEncoder[String]()).withRollingPolicy(rollingPolicy.build())
// val sink = bulkFormatBuilder.build()

 ds.map(_.log).addSink(sink).setParallelism(fileNum).name(""snappy sink to hdfs"")

 

In this way, flink API is called and written to HDFS. There are Chinese fields in the log, and the corresponding scrambled code is after hive is resolved，

CREATE EXTERNAL TABLE `demo_app`(
 `str` string COMMENT '原始记录json')
COMMENT 'app flink埋点日志'
PARTITIONED BY ( 
 `ymd` string COMMENT '日期分区yyyymmdd')
ROW FORMAT SERDE 
 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
STORED AS INPUTFORMAT 
 'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
 'hdfs://nameservice1/user/xxx/inke_back.db'

kafka source data :

{""name"":""inke.dcc.flume.collect"",""type"":""flume"",""status"":""完成"",""batchDuration"":3000,""proccessDelay"":0,""shedulerDelay"":0,""topic"":""newserverlog_opd_operate_log"",""endpoint"":""ali-a-opd-script01.bj"",""batchId"":""xxx"",""batchTime"":1588065997320,""numRecords"":-1,""numBytes"":-1,""totalRecords"":0,""totalBytes"":0,""ipAddr"":""10.111.27.230""}

 

hive data :

{""name"":""inke.dcc.flume.collect"",""type"":""flume"",""status"":""������"",""batchDuration"":3000,""proccessDelay"":0,""shedulerDelay"":0,""topic"":""newserverlog_opd_operate_log"",""endpoint"":""ali-a-opd-script01.bj"",""batchId"":""xxx"",""batchTime"":1588065997320,""numRecords"":-1,""numBytes"":-1,""totalRecords"":0,""totalBytes"":0,""ipAddr"":""10.111.27.230""}

 ",CDH6.0.1 hadoop3.0.0 Flink 1.10.0 ,jark,lzljs3620320,zenfenan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 08:56:47 UTC 2020,,,,,,,,,,"0|z0e5y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/20 11:47;lzljs3620320;Try implement a Extractor:

 
{code:java}
@Override
public byte[] extract(T element) {
   return (element.toString() + System.lineSeparator()).getBytes(StandardCharsets.UTF_8 /**You hive charset*/);
}
{code}
 ;;;","28/Apr/20 11:50;zenfenan;I think this is an encoding issue that came because of using DefaultExtractor of String type.;;;","29/Apr/20 03:25;why198852;[~lzljs3620320] yes，I used this to specify the encoding utf-8, but it didn't work.;;;","29/Apr/20 04:55;why198852;[~zenfenan] I wonder if you have any solution?;;;","29/Apr/20 04:57;lzljs3620320;[~why198852] Have you checked your hive charset? Is that UTF-8 or some others?;;;","29/Apr/20 05:18;zenfenan;[~why198852] I agree with [~lzljs3620320]. Can you check the charset your Hive is configured with? Java Strings completely support unicode so you can use Chinese characters with them. You just have to use the encoding as UTF-8.

Can you try setting {{SERDEPROPERTIES('serialization.encoding'='utf-8') }}in your Hive table DDL?;;;","29/Apr/20 07:48;why198852;[~lzljs3620320] [~zenfenan]Again, no effect . There are other solutions;;;","01/May/20 04:49;zenfenan;If the StringExtractor in StreamingFileSink and Hive serdeproperties didn't help, I think you may have to look into the SerializationSchema implementation used in the FlinkKafkaConsumer. Can you share how your FlinkKafkaConsumer instantiation looks?;;;","06/May/20 06:03;why198852;native kafka 0.11 consumption,code : 

val topics = sourceKafkaTopic.split("","").toList
val consumer = new FlinkKafkaConsumer011[String](topics, new SimpleStringSchema(), sourceProperties)
consumer.setStartFromGroupOffsets()

val ds = env.addSource(consumer)(TypeInformation.of(classOf[String]));;;","06/May/20 08:56;why198852;[~zenfenan] [~lzljs3620320] Problem has been solved, mainly is the flink-shaded-hadoop-2 package version dependency problem, at the same time, you need to specify the DefaultExtractor coding format, thank you for the guidance of the two.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When submitting Python job via ""flink run"" a IllegalAccessError will be raised due to the package's private access control",FLINK-17436,13301535,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,zhongwei,zhongwei,28/Apr/20 11:17,30/Apr/20 12:46,13/Jul/23 08:07,30/Apr/20 12:46,1.11.0,,,,,1.11.0,,,,API / Python,,,,,0,pull-request-available,,,,"Currently when submitting Python job via ""flink run"" a IllegalAccessError will be raised due to the package's private access control. We should fix this issue.

This issue Error can be simply reproduced by such a command:


{code:java}
./flink run -c org.apache.flink.client.python.PythonGatewayServer ../opt/flink-python_2.11-1.11-SNAPSHOT.jar
{code}


The error can be found in the client log file:


{code:java}
org.apache.flink.client.cli.CliArgsException: Python command line option detected but the flink-python module seems to be missing or not working as expected.
	at org.apache.flink.client.cli.ProgramOptionsUtils.createPythonProgramOptions(ProgramOptionsUtils.java:91) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.ProgramOptions.create(ProgramOptions.java:185) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:179) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:868) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:941) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:941) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_172]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_172]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_172]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_172]
	at org.apache.flink.client.cli.ProgramOptionsUtils.createPythonProgramOptions(ProgramOptionsUtils.java:85) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	... 6 more
Caused by: java.lang.IllegalAccessError: tried to access field org.apache.flink.client.cli.CliFrontendParser.ARGS_OPTION from class org.apache.flink.client.cli.PythonProgramOptions
	at org.apache.flink.client.cli.PythonProgramOptions.extractProgramArgs(PythonProgramOptions.java:61) ~[?:?]
	at org.apache.flink.client.cli.ProgramOptions.<init>(ProgramOptions.java:79) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.PythonProgramOptions.<init>(PythonProgramOptions.java:47) ~[?:?]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_172]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_172]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_172]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_172]
	at org.apache.flink.client.cli.ProgramOptionsUtils.createPythonProgramOptions(ProgramOptionsUtils.java:85) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	... 6 more
{code}
",,dian.fu,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 12:46:53 UTC 2020,,,,,,,,,,"0|z0e5xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/20 11:25;chesnay;Can you provide us with the full error?;;;","29/Apr/20 02:00;zhongwei;Hi [~chesnay] I have updated the description. The IllegalAccessError is thrown because though the `PythonProgramOptions` has the same package name with `CliFrontendParser`, they are loaded by different class loader, which are recognized as different package. So the `PythonProgramOptions` can not access the package-private content of `CliFrontendParser` in actual usage i.e ""flink run"".;;;","30/Apr/20 12:46;dian.fu;Merged to master via bdf4f7127353a436d03ce18d145be0e0cdb9fdd4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client end-to-end test (Old planner) Elasticsearch (v7.5.1) failed due to download error,FLINK-17424,13301470,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,twalthr,liyu,liyu,28/Apr/20 07:55,26/Nov/20 13:31,13/Jul/23 08:07,26/Nov/20 08:10,1.11.0,1.12.0,,,,1.12.0,,,,Connectors / ElasticSearch,Tests,,,,0,pull-request-available,test-stability,,,"`SQL Client end-to-end test (Old planner) Elasticsearch (v7.5.1)` failed in release-1.10 crone job with below error:
{noformat}
Preparing Elasticsearch(version=7)...
Downloading Elasticsearch from https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.5.1-linux-x86_64.tar.gz ...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  4  276M    4 13.3M    0     0  28.8M      0  0:00:09 --:--:--  0:00:09 28.8M
 42  276M   42  117M    0     0  80.7M      0  0:00:03  0:00:01  0:00:02 80.7M
 70  276M   70  196M    0     0  79.9M      0  0:00:03  0:00:02  0:00:01 79.9M
 89  276M   89  248M    0     0  82.3M      0  0:00:03  0:00:03 --:--:-- 82.4M
curl: (56) GnuTLS recv error (-54): Error in the pull function.
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
[FAIL] Test script contains errors.
{noformat}

https://api.travis-ci.org/v3/job/680222168/log.txt",,aljoscha,dian.fu,leonard,liyu,rmetzger,twalthr,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18745,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 26 13:31:41 UTC 2020,,,,,,,,,,"0|z0e5iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/20 08:53;rmetzger;Another case: https://travis-ci.org/github/apache/flink/jobs/680222168;;;","01/May/20 08:34;rmetzger;Another case: https://travis-ci.org/github/apache/flink/jobs/681613024;;;","29/Jun/20 01:58;dian.fu;Another instance on the master branch: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4092&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a;;;","12/Oct/20 02:24;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7400&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b;;;","17/Oct/20 09:47;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7776&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729;;;","18/Oct/20 01:25;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7791&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=7f606211-1454-543c-70ab-c7a028a1ce8c;;;","20/Oct/20 02:04;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7880&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6;;;","31/Oct/20 01:41;dian.fu;""SQL Client end-to-end test (Blink planner) Elasticsearch (v6.3.1)"" failed with similar error:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8660&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","06/Nov/20 02:56;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9093&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","17/Nov/20 01:47;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9660&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b;;;","17/Nov/20 07:28;xtsong;Another instance on master branch, and the testing process hangs there until timeouted.

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9668&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529];;;","17/Nov/20 10:20;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9672&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","18/Nov/20 13:56;twalthr;Fixed in 1.12.0: 71ef64f9afab3472f0372415c13f1bc2b14b00cc;;;","24/Nov/20 05:00;dian.fu;Another instance on master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9990&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","24/Nov/20 12:45;twalthr;I will look into it again. But it seems that the Elasticsearch servers are blocking us.;;;","25/Nov/20 01:37;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10042&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","26/Nov/20 08:10;twalthr;Next attempt merged in: ff85aa50861341a6b68d15b241025bff18614bb7;;;","26/Nov/20 13:31;twalthr;For the 1.12 branch: b2c29a7a6918845c5b2b81a92f60cf1399ac3885;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot alias Tuple and Row fields when converting DataStream to Table,FLINK-17420,13301314,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,gyfora,gyfora,27/Apr/20 15:33,07/May/20 11:19,13/Jul/23 08:07,07/May/20 11:19,1.10.0,1.10.1,,,,1.10.2,1.11.0,,,Table SQL / API,,,,,0,pull-request-available,,,,"It is impossible to alias Tuple and Row fields when converting DataStream to Table.


{code:java}
tableEnv.fromDataStream(env.fromElements(Tuple2.of(""a"", 1)), ""f0 as name, f1 as age"");
{code}
This leads to the following error:

Alias 'name' is not allowed if other fields are referenced by position

org.apache.flink.table.typeutils.FieldInfoUtils#isReferenceByPosition method

More info: [https://lists.apache.org/thread.html/re35f37797692af4e9e0e2512dedf0f598e0cdbe1ac4820103d4273c1%40%3Cuser.flink.apache.org%3E]",,dwysakowicz,gyfora,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 11:19:04 UTC 2020,,,,,,,,,,"0|z0e4k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/20 01:16;leonard;Hi, [~dwysakowicz] 

It seems the bug exits in 1.11 too, I'd like to fix it.;;;","28/Apr/20 01:37;ykt836;[~Leonard Xu] Thanks, I will assign this to you ;;;","05/May/20 14:36;dwysakowicz;Fixed in master via: f90740f951111c72b2b96d7b3adec3c940642e91;;;","07/May/20 11:19;dwysakowicz;Implemented in 1.10.2 via: 9e2cf0187accfd7f120cc8d8e5e684e40aea12ac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-kubernetes doesn't work on java 8 8u252,FLINK-17416,13301255,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,wangxiyuan,wangxiyuan,27/Apr/20 12:31,16/Oct/20 10:32,13/Jul/23 08:07,05/Jun/20 03:36,1.10.0,1.11.0,,,,1.11.0,1.12.0,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"When using java-8-8u252 version, the flink container end-to-end failed. The test  `Running 'Run kubernetes session test'` fails with the `Broken pipe` error.

See:

[https://logs.openlabtesting.org/logs/periodic-20-flink-mail/github.com/apache/flink/master/flink-end-to-end-test-arm64-container/fcfdd47/job-output.txt.gz]

 

Flink Azure CI doesn't hit this problem because it runs under jdk-8-8u242

 

The reason is that the okhttp library which flink using doesn't work on java-8-8u252:

[https://github.com/square/okhttp/issues/5970]

 

The problem has been with the PR:

[https://github.com/square/okhttp/pull/5977]

 

Maybe we can wait for a new 3.12.x release and bump the okhttp version in Flink later.

 

 ",,felixzheng,ganeshraju,rmetzger,trohrmann,wangxiyuan,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17565,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17613,,,,,,,,,"08/May/20 02:31;wangyang0918;log.k8s.session.8u252;https://issues.apache.org/jira/secure/attachment/13002356/log.k8s.session.8u252",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 03:36:26 UTC 2020,,,,,,,,,,"0|z0e474:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/20 03:37;wangyang0918;Thanks [~wangxiyuan] for creating this ticket. I think it is a good opportunity to bump the okhttp client in flink-kubernetes to new version.;;;","29/Apr/20 09:48;chesnay;3.12.11 and 3.14.8 have been released with a fix.

I'd suggest to bump everything to 3.12.11, and add a release note that flink-swift-fs-hadoop may not work on later JDKs (it still relies on 2.4.0 transitively via hadoop; we can't bump that easily, but maybe it is not affected?);;;","06/May/20 10:41;chesnay;This is currently breaking the kubernetes e2e test.;;;","06/May/20 11:29;rmetzger;Actually, we have the issue on Azure as well? https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=674&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

For better discoverability:
{code}
2020-05-06T09:43:38.0213229Z 2020-05-06 09:43:38,021 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2020-05-06T09:43:39.1410368Z 2020-05-06 09:43:38,949 ERROR org.apache.flink.kubernetes.cli.KubernetesSessionCli         [] - Error while running the Flink session.
2020-05-06T09:43:39.1412057Z io.fabric8.kubernetes.client.KubernetesClientException: Operation: [get]  for kind: [Service]  with name: [flink-native-k8s-session-1-rest]  in namespace: [default]  failed.
2020-05-06T09:43:39.1413357Z 	at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:64) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1414777Z 	at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:72) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1416398Z 	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:231) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1417559Z 	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.get(BaseOperation.java:164) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1418948Z 	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.getRestService(Fabric8FlinkKubeClient.java:201) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1420222Z 	at org.apache.flink.kubernetes.cli.KubernetesSessionCli.run(KubernetesSessionCli.java:104) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1422055Z 	at org.apache.flink.kubernetes.cli.KubernetesSessionCli.lambda$main$0(KubernetesSessionCli.java:185) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1423731Z 	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1424868Z 	at org.apache.flink.kubernetes.cli.KubernetesSessionCli.main(KubernetesSessionCli.java:185) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1425544Z Caused by: java.net.SocketException: Broken pipe (Write failed)
2020-05-06T09:43:39.1426133Z 	at java.net.SocketOutputStream.socketWrite0(Native Method) ~[?:1.8.0_252]
2020-05-06T09:43:39.1426567Z 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) ~[?:1.8.0_252]
2020-05-06T09:43:39.1427091Z 	at java.net.SocketOutputStream.write(SocketOutputStream.java:155) ~[?:1.8.0_252]
2020-05-06T09:43:39.1478206Z 	at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[?:1.8.0_252]
2020-05-06T09:43:39.1478717Z 	at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[?:1.8.0_252]
2020-05-06T09:43:39.1479815Z 	at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:894) ~[?:1.8.0_252]
2020-05-06T09:43:39.1480353Z 	at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:865) ~[?:1.8.0_252]
2020-05-06T09:43:39.1481024Z 	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:123) ~[?:1.8.0_252]
2020-05-06T09:43:39.1482062Z 	at org.apache.flink.kubernetes.shaded.okio.Okio$1.write(Okio.java:79) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1483535Z 	at org.apache.flink.kubernetes.shaded.okio.AsyncTimeout$1.write(AsyncTimeout.java:180) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1484750Z 	at org.apache.flink.kubernetes.shaded.okio.RealBufferedSink.flush(RealBufferedSink.java:224) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1485790Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http2.Http2Writer.settings(Http2Writer.java:203) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1486874Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http2.Http2Connection.start(Http2Connection.java:515) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1488261Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http2.Http2Connection.start(Http2Connection.java:505) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1490124Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnection.startHttp2(RealConnection.java:298) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1491667Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnection.establishProtocol(RealConnection.java:287) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1493131Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnection.connect(RealConnection.java:168) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1494682Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:257) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1496363Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:135) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1497783Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:114) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1499260Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1501504Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1504529Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1506765Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1508818Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1510932Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1512523Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1515317Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1517038Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:126) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1518474Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1521417Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1522997Z 	at io.fabric8.kubernetes.client.utils.BackwardsCompatibilityInterceptor.intercept(BackwardsCompatibilityInterceptor.java:119) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1524899Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1525970Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1527187Z 	at io.fabric8.kubernetes.client.utils.ImpersonatorInterceptor.intercept(ImpersonatorInterceptor.java:68) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1530414Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1532488Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1533927Z 	at io.fabric8.kubernetes.client.utils.HttpClientUtils.lambda$createHttpClient$3(HttpClientUtils.java:112) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1535766Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1536906Z 	at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1539160Z 	at org.apache.flink.kubernetes.shaded.okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:254) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1540772Z 	at org.apache.flink.kubernetes.shaded.okhttp3.RealCall.execute(RealCall.java:92) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1542910Z 	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:411) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1545053Z 	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:372) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1546526Z 	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:337) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1547709Z 	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:318) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1549744Z 	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleGet(BaseOperation.java:812) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1551119Z 	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:220) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-06T09:43:39.1551712Z 	... 6 more
{code};;;","06/May/20 11:35;chesnay;[~rmetzger] yes, this is what I meant.;;;","06/May/20 11:38;rmetzger;I didn't read your comments yet :) I was in a phone call while posting the failing build :);;;","06/May/20 12:22;rmetzger;I propose a temporary fix by using OpenJDK8u252 on Azure: https://github.com/apache/flink/pull/12007;;;","07/May/20 04:29;rmetzger;My proposed fix doesn't work. I will push a hotfix (once validated) that disables the e2e test until fixed.;;;","07/May/20 04:57;wangyang0918;I am not sure why the java version in azure upgrade from ""1.8.0_242"" to ""1.8.0_252"" from yesterday. It makes the K8 tests failed.;;;","07/May/20 05:03;wangyang0918;I agree with [~chesnay] that if we want to solve the problem thoroughly, we need to bump the okhttp version to 3.12.11 and 3.14.8.;;;","07/May/20 05:42;rmetzger;I also agree that we need to bump okhttp, otherwise our users will run into the issue.
I disabled the failing test to make the build green again in https://github.com/apache/flink/commit/ad46ca3ea8f445dcddde631978ecc7935d5fa8ae;;;","07/May/20 10:50;wangyang0918;It seems that bumping the okhttp version could not solve the problem. And i have found an interesting thing. The k8s session cluster with jdk 8u252 could work in a real K8s cluster. However, it could not work for the minikube.

 

It is not a same issue with [https://github.com/square/okhttp/issues/5970]. Since they have different stack trace.;;;","07/May/20 11:43;wangxiyuan;Thanks for picking this up. Yang.

 

It looks that Flink use io.fabric8.kubernetes-client - 4.5.2 which depends on an old version of okhttp. Is it possbile that the version of okhttp was overrode?

I didn't test it locally, just a guess.

It bumps the version of okhttp in 4.10.0 . [https://github.com/fabric8io/kubernetes-client/releases/tag/v4.10.0]

 

Maybe we should bump the version of kubernetes-client as well?

 ;;;","07/May/20 13:35;pnowojski;Another instance on azure: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=743&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","08/May/20 02:07;wangyang0918;[~wangxiyuan] Thanks for your comments. We have already explicitly set the okhttp version and it could not take effect. Maybe this issue is also related with the minikube version since it could work in a real K8s cluster with java 8u252.
{code:java}
<!-- Set dependency version for transitive dependencies -->
<dependencyManagement>
   <dependencies>
      <dependency>
         <groupId>com.squareup.okhttp3</groupId>
         <artifactId>okhttp</artifactId>
         <version>3.12.11</version>
      </dependency>
   </dependencies>
</dependencyManagement>
{code}
I have attached a log file with setting {{-Djavax.net.debug=all}} so that someone is familiar with ssl could provide more clues.;;;","08/May/20 14:39;rmetzger;Side note: A user is affected by this as well: https://lists.apache.org/thread.html/ra80f7abbceaa68f29e63916a40aaf592d23842c17da382278eb2755f%40%3Cuser.flink.apache.org%3E

Maybe a backport to the Flink 1.10 line should be considered as well.;;;","11/May/20 03:30;wangyang0918;Some updates on this ticket.

After more investigation and testing, i find that this issue is not directly caused by the bug of okhttp. Instead, it is related with both kubernetes version and jdk version.
* For jdk 8u252, the fabric8 kubernetes-client could only work on kubernetes v1.16 and lower versions.
* For other jdk versions(e.g. 8u242, jdk11), i am not aware of the same issues. It seems that native K8s integration works well. The user in the ML have solved the problem after upgrading to jdk11.

I will keep in touch with the author of fabric8 kubernetes-client to verify when this could be resolved.
At the same time i will update the K8s e2e tests to use kubernetes version v1.16.9 and enable the e2e tests so that it will not block our general release cycle.
[~rmetzger] What do you think?;;;","11/May/20 07:14;rmetzger;Thanks a lot for the investigation!

I agree that we can re-enable the e2e test by upgrading K8s.
What I'm a bit worried about is the upcoming feature freeze this week: People with up-to-date K8s and Java versions will face this issue (unless  fabric8 releases something before the release).

I would propose to keep this issue open as a blocker until Flink works again with all K8s / Java versions.;;;","11/May/20 07:22;wangyang0918;Yes, i agree with you that we should keep this ticket open. And i will make an offline discussion with the author of fabric8 kubernetes-client and post the conclusion here.;;;","11/May/20 07:31;rmetzger;Thanks a lot!;;;","11/May/20 12:14;rmetzger;CI is resolved in: https://github.com/apache/flink/commit/5846e5458e20308db4f0dd1c87d511a60817472c and https://github.com/apache/flink/commit/bbf9d955164f14040f891b47fb6cf3d1476009e3

Thank you Yang!;;;","15/May/20 05:32;wangyang0918;Update on this issue.

 

After some discussion with author of the fabric8 kubernetes-client[1], we have get a conclusion that kubernetes-client could not work with jdk 8u252 because of the okhttp bug. Currently, we could use ""export HTTP2_DISABLE=true"" to disable the http2.

Once the fabric8 kubernetes new version(4.9.2, 4.10.2) released, we need to bump the version and get a clean solution.

 

[1]. [https://github.com/fabric8io/kubernetes-client/issues/2212];;;","18/May/20 14:05;trohrmann;Should the priority be Blocker [~rmetzger] and [~fly_in_gis]? If I understand the problem correctly, then there is nothing Flink can directly do about it. Hence, this ticket should also not block the next release imo. If you agree, then please down grade the issue.;;;","19/May/20 02:20;wangyang0918;For now, the reasonable solution is bumping the fabric8 kubernetes-client version to latest(1.10.2). And then this issue will be fixed. I think it makes sense to downgrade the priority just as [~trohrmann] said we could not do anything in Flink to avoid this issue.

 

BTW, let's move the discussion about bumping version to FLINK-17565. I will also have more tests on the new kubernetes-client version.;;;","05/Jun/20 03:36;wangyang0918;Fixed in FLINK-17565.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlockingPartitionBenchmark compilation failed due to changed StreamGraph interface,FLINK-17410,13301210,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuzh,zhuzh,zhuzh,27/Apr/20 09:28,27/Apr/20 09:52,13/Jul/23 08:07,27/Apr/20 09:52,1.11.0,,,,,1.11.0,,,,Benchmarks,,,,,0,,,,,"{{StreamGraph#setGlobalDataExchangeMode(...)}} is introduced in FLINK-17020 to replace         {{StreamGraph#setBlockingConnectionsBetweenChains(...)}}.
{{BlockingPartitionBenchmark}} failed because it relies on the old interface.",,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 09:52:42 UTC 2020,,,,,,,,,,"0|z0e3x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/20 09:52;zhuzh;Fixed via 3125e435b15863794d8fb8b1071ba25668795ac4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Running HA per-job cluster (rocks, non-incremental) gets stuck killing a non-existing pid in Hadoop 3 build profile",FLINK-17404,13301179,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,27/Apr/20 07:26,04/Jun/20 15:18,13/Jul/23 08:07,04/Jun/20 15:16,1.11.0,1.12.0,,,,1.11.0,,,,Runtime / Coordination,Test Infrastructure,Tests,,,0,pull-request-available,test-stability,,,"CI log: https://api.travis-ci.org/v3/job/678609505/log.txt

{code}
Waiting for text Completed checkpoint [1-9]* for job 00000000000000000000000000000000 to appear 2 of times in logs...
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
Starting standalonejob daemon on host travis-job-e606668f-b674-49c0-8590-e3508e22b99d.
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
Killed TM @ 18864
kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]
Killed TM @ 


No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.
Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received

The build has been terminated
{code}",,kkl0u,klion26,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17824,,,,,,FLINK-18012,FLINK-17194,,,,,,,,"12/May/20 11:51;rmetzger;255;https://issues.apache.org/jira/secure/attachment/13002710/255",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 15:16:56 UTC 2020,,,,,,,,,,"0|z0e3q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/20 07:29;rmetzger;There's been a discussion on this case here: https://issues.apache.org/jira/browse/FLINK-16770?focusedCommentId=17091377&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17091377;;;","27/Apr/20 09:43;trohrmann;[~kkloudas] could you respond to https://issues.apache.org/jira/browse/FLINK-16770?focusedCommentId=17092098&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17092098 and take a look whether the issue is cause by what [~SleePy] suspected? You might know best what's happening in {{common_ha.sh#ha_tm_watchdog}}.;;;","27/Apr/20 09:45;trohrmann;[~rmetzger] removing {{Runtime / Coordination}} at the moment since it looks more like a test infrastructure/testing problem. Please add it again once it turns out to be a {{Runtime / Coordination}} problem.;;;","27/Apr/20 10:14;rmetzger;Sorry, you are right. Sounds good to me!;;;","27/Apr/20 11:28;kkl0u;[~trohrmann] I agree with the comment on the other issue that more logs would help to see if there are any TMs running, or that a TM was initialised, but for some reason it failed. Given that there are no such logs, I will try to reproduce it locally to see.;;;","29/Apr/20 13:20;rmetzger;I now added the logging. Let's see if it happens again.;;;","01/May/20 18:39;kkl0u;Thanks [~rmetzger];;;","12/May/20 11:52;rmetzger;[~kkl0u] I have now managed to see the failure in a build: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7981&view=logs&j=60fab2b4-ef88-5658-ee62-1867a1c42337&t=b6d34f17-bbec-5dd6-cc41-dcda70851ff3
I attached the logs as ""255"" to this JIRA.;;;","26/May/20 19:43;rmetzger;I have checked the logs, and it seems that the second JobManager is not able to complete 2 checkpoints, because it can never deploy all tasks, because of this error:
{code}
Could not fulfill slot request 9697389c4ef5ae6a384d3d410ad58ca2. Requested resource profile (ResourceProfile{UNKNOWN}) is unfulfillable.{code}.;;;","27/May/20 07:04;rmetzger;Note to self: The test logs: ""Config uses deprecated configuration key 'high-availability.zookeeper.storageDir' instead of proper key 'high-availability.storageDir'"";;;","27/May/20 15:00;rmetzger;I had an offline discussion with [~trohrmann] about this. We can not explain the failure at the moment.
I will try to reproduce it with DEBUG logs.;;;","28/May/20 05:21;rmetzger;{code}Running HA per-job cluster (file, async) end-to-end test{code} with the same issue: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2304&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8;;;","04/Jun/20 15:16;rmetzger;Merged to master (1.12.0) in https://github.com/apache/flink/commit/9211cb5eaa98f1c51e1569f92c46f65240b0ecc2
Merged to release-1.11 in https://github.com/apache/flink/commit/83644269acdc577d26ed376acbe8068af56edbee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix invalid classpath in BashJavaUtilsITCase,FLINK-17403,13301166,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Paul Lin,Paul Lin,Paul Lin,27/Apr/20 06:25,06/May/20 16:44,13/Jul/23 08:07,06/May/20 08:50,1.10.0,1.11.0,,,,1.10.1,1.11.0,,,Tests,,,,,0,pull-request-available,,,,"runBashJavaUtilsCmd.sh locates flink-dist.jar by `find` with pattern `flink-dist*.jar`, but it doesn't filter out the flink-dist*-source.jar built by maven and the flink-dist jar in the original bin directory, so it might get 3 jars as the result, which might break the command depends on it.

For instance, the result of `find` can be:
```
project_dir/flink-dist/src/test/bin/../../../target/flink-dist_2.11-1.10.0-sources.jar
project_dir/flink-dist/src/test/bin/../../../target/flink-1.10.0-bin/flink-1.10.0/lib/flink-dist_2.11-1.10.0.jar
project_dirflink-dist/src/test/bin/../../../target/flink-dist_2.11-1.10.0.jar
```

Moreover, there's a redundant `}` in the command, which seems to be accidentally skipped by the multiple-line result provided by `find`.
",,aljoscha,azagrebin,Paul Lin,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 08:50:05 UTC 2020,,,,,,,,,,"0|z0e3nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/20 06:44;Paul Lin;I propose to change to `find $FLINK_TARGET_DIR -name 'flink-dist*.jar' -not -path '*-sources.jar' -maxdepth 1` to get the very dist jar under target.;;;","27/Apr/20 09:17;aljoscha;[~xintongsong] Could you please have a look? You initially wrote the script.;;;","27/Apr/20 10:01;xtsong;[~aljoscha]
 Thanks for pulling me in.

[~Paul Lin]
 Thanks for reporting this issue. It seems to me this is a valid problem. 

As for the solution, I think we probably also consider the possibility that there are jar files with other suffix under {{$FLINK_TARGET_DIR}}. E.g., {{flink-dist*-test.jar}}. Of course we could further filter the patterns for the {{find}} command. However, I'm in favor of an alternative approach to properly add all jar files that meets the pattern to the class path. That safes us from closely depending on the certain pattern of the name of flink dist jar file. WDYT?;;;","27/Apr/20 10:04;xtsong;BTW, would you like to work on this? [~Paul Lin];;;","27/Apr/20 10:08;Paul Lin;[~xintongsong] Sounds good. How about restricting the search scope to `-maxdepth 1` under `target` directory, and joining the jars found with `:` before adding them to the classpath?;;;","27/Apr/20 10:09;Paul Lin;[~xintongsong] Yes, please assign it to me.;;;","27/Apr/20 10:13;xtsong;bq. How about restricting the search scope to `-maxdepth 1` under `target` directory, and joining the jars found with `:` before adding them to the classpath?

Sounds good to me.

[~aljoscha], could you help assign this ticket to [~Paul Lin]. Thanks.;;;","28/Apr/20 07:41;aljoscha;Done! 🍵;;;","06/May/20 08:50;azagrebin;merged into master by 40b742f83ca307eac398055ca5231d5a30747051
merged into 1.10 by f99bd715e7d171a1929471dcfa43e38111f912f9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add User to Task Manager ,FLINK-17402,13301164,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,spurthic,spurthic,27/Apr/20 06:12,25/Jun/21 07:02,13/Jul/23 08:07,25/Jun/21 07:00,,,,,,,,,,Deployment / Mesos,,,,,0,auto-deprioritized-major,,,,Currently Task manager user is not set. We should be able to set the task manager user name based on the config that is passed/ or could be derived from Job Manager user name. ,,spurthic,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23118,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 23:12:24 UTC 2021,,,,,,,,,,"0|z0e3mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 11:19;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:12;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalStandaloneKafkaResource.setupKafkaDist fails due to download timeout,FLINK-17400,13301161,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,rmetzger,rmetzger,27/Apr/20 06:04,12/Dec/21 12:14,13/Jul/23 08:07,12/Dec/21 12:14,1.11.0,1.12.2,1.13.0,,,,,,,Build System / Azure Pipelines,Test Infrastructure,,,,0,auto-deprioritized-critical,auto-deprioritized-major,test-stability,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=273&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=375367d9-d72e-5c21-3be0-b45149130f6b

{code}
[ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 215.598 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientKafkaITCase
[ERROR] testKafka[0: kafka-version:0.10 kafka-sql-version:.*kafka-0.10.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase)  Time elapsed: 120.023 s  <<< ERROR!
java.io.IOException: Process ([wget, -q, -P, /tmp/junit6433813062678759117/downloads/1665795946, https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz]) exceeded timeout (120000) or number of retries (3).
	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlockingWithRetry(AutoClosableProcess.java:132)
	at org.apache.flink.tests.util.cache.AbstractDownloadCache.getOrDownload(AbstractDownloadCache.java:127)
	at org.apache.flink.tests.util.cache.LolCache.getOrDownload(LolCache.java:31)
	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.setupKafkaDist(LocalStandaloneKafkaResource.java:98)
	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.before(LocalStandaloneKafkaResource.java:92)
	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:46)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)

{code}",,dwysakowicz,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19158,,FLINK-17510,FLINK-20057,FLINK-19863,FLINK-21025,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 28 10:43:57 UTC 2021,,,,,,,,,,"0|z0e3m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/21 13:02;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16178&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27635;;;","20/Apr/21 14:44;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16867&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=26886;;;","20/Apr/21 15:02;dwysakowicz;Disabled tests that use KafkaResource:
* master
** 13d7e557322ccd2dc6f4c7698e6c5fc23ca445a8
* 1.13
** dbc74ef6ea68379a25f6d44a765266be5de9b871;;;","20/Apr/21 15:04;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16866&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27933;;;","29/Apr/21 10:52;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/May/21 22:56;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","20/Jun/21 10:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","28/Jun/21 10:43;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Container resource cannot be mapped on Hadoop 2.10+,FLINK-17390,13301075,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xtsong,xtsong,xtsong,26/Apr/20 11:47,27/Apr/20 13:27,13/Jul/23 08:07,27/Apr/20 13:27,1.11.0,,,,,1.11.0,,,,Deployment / YARN,,,,,1,pull-request-available,,,,"In FLINK-16438, we introduced {{WorkerSpecContainerResourceAdapter}} for mapping Yarn container {{Resource}} with Flink {{WorkerResourceSpec}}. Inside this class, we use {{Resource}} for hash map keys and set elements, assuming that {{Resource}} instances that describes the same set of resources have the same hash code.

This assumption is not always true. {{Resource}} is an abstract class and may have different implementations. In Hadoop 2.10+, {{LightWeightResource}}, a new implementation of {{Resource}}, is introduced for {{Resource}} generated by {{Resource.newInstance}} on the AM side, which overrides the {{hashCode}} method. That means, a {{Resource}} generated on AM may have a different hash code compared to an equal {{Resource}} returned from Yarn.

To solve this problem, we may introduce an {{InternalResource}} as an inner class of {{WorkerSpecContainerResourceAdapter}}, with {{hashCode}} method depends only on the fields needed by Flink (ATM memroy and vcores). {{WorkerSpecContainerResourceAdapter}} should only use {{InternalResource}} for internal state management, and do conversions for {{Resource}} passed into and returned from it.",,guoyangze,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16438,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 13:27:13 UTC 2020,,,,,,,,,,"0|z0e334:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/20 11:50;xtsong;cc [~trohrmann];;;","27/Apr/20 07:24;trohrmann;Sounds like a good fix to me. I've assigned you to this ticket [~xintongsong].;;;","27/Apr/20 07:25;trohrmann;Thanks for creating the PR so quickly.;;;","27/Apr/20 13:27;trohrmann;Fixed via 537a56441a49b67ed2d8f271e079ef6b5094f506;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception in HadoopSecurityContextFactory.createContext while no shaded-hadoop-lib provided.,FLINK-17386,13301021,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rongr,wenlong.lwl,wenlong.lwl,26/Apr/20 04:19,16/Oct/20 10:35,13/Jul/23 08:07,17/May/20 23:33,1.11.0,,,,,1.11.0,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"java.io.IOException: Process execution failed due error. Error output:java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.security.UserGroupInformation\n\tat org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory.createContext(HadoopSecurityContextFactory.java:59)\n\tat org.apache.flink.runtime.security.SecurityUtils.installContext(SecurityUtils.java:92)\n\tat org.apache.flink.runtime.security.SecurityUtils.install(SecurityUtils.java:60)\n\tat org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:964)\n\n\tat com.alibaba.flink.vvr.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:144)\n\tat com.alibaba.flink.vvr.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:126)\n\tat com.alibaba.flink.vvr.VVRCompileTest.runSingleJobCompileCheck(VVRCompileTest.java:173)\n\tat com.alibaba.flink.vvr.VVRCompileTest.lambda$runJobsCompileCheck$0(VVRCompileTest.java:101)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1147)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)\n\tat java.lang.Thread.run(Thread.java:834)

I think it is because exception throw in the static code block of UserInformation, we should catch Throwable instead of Exception in HadoopSecurityContextFactory#createContext?
[~rongr] what do you think?",,rongr,Terry1897,wenlong.lwl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 17 23:33:43 UTC 2020,,,,,,,,,,"0|z0e2r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/20 14:28;rongr;could you provide more information on how to reproduce this exception?

From what I understand you do want to use shaded-hadoop-lib but somehow forgot to include in the classpath. and your intention is to allow graceful shutdown instead of a hard process exit. correct?;;;","28/Apr/20 02:18;wenlong.lwl;hi [~rongr], I don't want shaded-hadoop-lib in the lib, because I didn't want to submit job to yarn. I did more investigation on this issue, found that it is because there is a customized state backend in the lib which is designed to support write to hdfs and contains some of hadoop libs but not all. The root cause is that: the security loader only check whether Configuration and UserGroupInformation is in classpath or not, which may cause the exception above when no enough lib is in the lib dir.;;;","28/Apr/20 04:44;rongr;hmm. interesting. 
Just to understand more clearer, you are actually using some sort of hadoop functionality but not {{flink-shaded-hadoop-*}} module. correct? that causes Flink to falsely think it should use HadoopSecurityContext but it shouldn't. 

 however the exception said:

{code:java}
java.io.IOException: Process execution failed due error. Error output:java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.security.UserGroupInformation
org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory.createContext(HadoopSecurityContextFactory.java:59)
...
{code}

so this means you are somehow including hadoop but not the hadoop-common? 
;;;","28/Apr/20 04:46;rongr;I might have some idea on why this is happening. but just to be sure: is this issue occurring only to the latest unreleased flink? or does it also affects previous version (I bet it does, but if you can verify that would be great).

;;;","28/Apr/20 07:06;wenlong.lwl;yes, you are right about the use case. We are doing some testing based on the master branch, I can do a test on 1.10, and feed back later.;;;","28/Apr/20 07:23;wenlong.lwl;[~rongr] I run a quick test by copy the statebackend to the lib of 1.10 and run a flink command, no error happen.;;;","28/Apr/20 15:17;rongr;I just create a quick fix for this and attach a PR. If you could help testing the patch later that would be super great!. Thx
;;;","30/Apr/20 06:58;wenlong.lwl;[~rongr] I have verified the patch, it works.;;;","17/May/20 23:33;rongr;closed via: 1892bedeea9fa118b6e3bcb572f63c2e7f6d83e3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix precision problem when converting JDBC numberic into Flink decimal type ,FLINK-17385,13301016,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,f.pompermaier,jark,jark,26/Apr/20 03:13,06/May/20 18:09,13/Jul/23 08:07,06/May/20 18:09,,,,,,1.11.0,,,,Connectors / JDBC,Table SQL / Ecosystem,,,,0,pull-request-available,,,,"This is reported in the mailing list: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/JDBC-error-on-numeric-conversion-because-of-DecimalType-MIN-PRECISION-td34668.html

",,f.pompermaier,jark,leonard,libenchao,phoenixjiangnan,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 18:09:30 UTC 2020,,,,,,,,,,"0|z0e2q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/20 03:14;jark;cc [~phoenixjiangnan] [~f.pompermaier];;;","26/Apr/20 08:46;f.pompermaier;Ok..can you assign this and the other realted ticket to me?;;;","26/Apr/20 12:29;f.pompermaier;Now the 2 PR should be ok;;;","06/May/20 18:09;phoenixjiangnan;master: d33fb620fdb09a4755dd6513f96f0e191da2fcda;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Entropy key is not resolved if flink-s3-fs-hadoop is added as a plugin,FLINK-17359,13300662,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,premsantosh,premsantosh,premsantosh,23/Apr/20 21:39,01/Apr/21 04:48,13/Jul/23 08:07,27/Apr/20 08:25,,,,,,1.10.1,1.11.0,,,FileSystems,,,,,0,pull-request-available,,,,"Using flink 1.10

I added the flink-s3-fs-hadoop jar in plugins dir but I am seeing the checkpoints paths like {{s3://my_app/__ENTROPY__/app_name-staging/flink/checkpoints/e10f47968ae74934bd833108d2272419/chk-3071}} which means the entropy injection key is not being resolved. After some debugging I found that in the [EntropyInjector|https://github.com/apache/flink/blob/release-1.10.0/flink-core/src/main/java/org/apache/flink/core/fs/EntropyInjector.java#L97] we check if the given fileSystem is of type {{SafetyNetWrapperFileSystem}} and if so we check if the delegate is of type {{EntropyInjectingFileSystem}} but don't check for {{[ClassLoaderFixingFileSystem |https://github.com/apache/flink/blob/release-1.10.0/flink-core/src/main/java/org/apache/flink/core/fs/PluginFileSystemFactory.java#L65]}} which would be the type if S3 file system dependencies are added as a plugin.",,foxss,liyu,premsantosh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22081,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 31 01:29:04 UTC 2021,,,,,,,,,,"0|z0e0jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/20 21:51;premsantosh;I've created a PR for this https://github.com/apache/flink/pull/11891;;;","27/Apr/20 08:25;aljoscha;release-1.10: 5f5a80c0e86c50568a4fbb0526b464faff5c87fb
master: 2452321f23e0fd86c37b5ac063fea107f43a81b4

[~liyu] I added this to the release-1.10 branch, so if you cut a new RC for 1.10.1 this might be included because it's a good fix.;;;","27/Apr/20 08:34;liyu;Got it, thanks for the note [~aljoscha]!;;;","30/Apr/20 05:04;liyu;Change the fix version to 1.10.1 since we will have a new RC and this fix will be included.;;;","24/Mar/21 16:43;foxss;Hi  [~premsantosh], We see a regression of this bug with 1.11.2.

detail[ http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Flink-job-cannot-find-recover-path-after-using-entropy-injection-for-s3-file-systems-tp49527p49656.html|http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Flink-job-cannot-find-recover-path-after-using-entropy-injection-for-s3-file-systems-tp49527p49656.html]

do you have cycle to review a small fix pr?;;;","24/Mar/21 17:06;premsantosh;Hey [~foxss]! Absolutely I can.;;;","31/Mar/21 01:29;foxss;[https://github.com/apache/flink/pull/15442]  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken links in Flink docs master,FLINK-17353,13300581,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yangyichao,sjwiesman,sjwiesman,23/Apr/20 15:03,16/Oct/20 10:50,13/Jul/23 08:07,19/May/20 08:33,,,,,,1.11.0,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,"http://localhost:4000/concepts/programming-model.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/concepts/runtime.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/internals/stream_checkpointing.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/concepts/flink-architecture.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/ops/memory/config.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/concepts/programming-model.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/concepts/runtime.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/dev/table/python/installation.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/internals/stream_checkpointing.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/table/sql.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/ops/memory/config.html:
Remote file does not exist -- broken link!!!
",,cellen,jark,klion26,sjwiesman,x1q1j1,yangyichao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 08:33:52 UTC 2020,,,,,,,,,,"0|z0e01c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/20 15:04;sjwiesman;[~jark] Most of these are on pages that have been translated into Chinese so I can't fix them. Could you please help find someone that can fix these links. ;;;","23/Apr/20 16:03;jark;Sure [~sjwiesman], thanks for reporting this.;;;","16/May/20 12:46;yangyichao;Hi [~sjwiesman], I'd like to contribute to this issue, could you help assign this to me?;;;","17/May/20 03:30;jark;Assigned to you [~yangyichao];;;","19/May/20 08:33;jark;- master (1.12.0): 4dc3bdaa460f35ea6e0fa31ce9068a7dba54316d
- 1.11.0: 8078c86c546dc879151eedcbd4c156bfc4e304b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointCoordinator and CheckpointFailureManager ignores checkpoint timeouts,FLINK-17351,13300559,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ym,pnowojski,pnowojski,23/Apr/20 12:54,27/May/20 15:15,13/Jul/23 08:07,27/May/20 15:15,1.10.0,1.9.2,,,,1.11.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"As described in point 2: https://issues.apache.org/jira/browse/FLINK-17327?focusedCommentId=17090576&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17090576

(copy of description from above linked comment):

The logic in how {{CheckpointCoordinator}} handles checkpoint timeouts is broken. In your [~qinjunjerry] examples, your job should have failed after first checkpoint failure, but checkpoints were time outing on CheckpointCoordinator after 5 seconds, before {{FlinkKafkaProducer}} was detecting Kafka failure after 2 minutes. Those timeouts were not checked against {{setTolerableCheckpointFailureNumber(...)}} limit, so the job was keep going with many timed out checkpoints. Now funny thing happens: FlinkKafkaProducer detects Kafka failure. Funny thing is that it depends where the failure was detected:

a) on processing record? no problem, job will failover immediately once failure is detected (in this example after 2 minutes)
b) on checkpoint? heh, the failure is reported to {{CheckpointCoordinator}} *and gets ignored, as PendingCheckpoint has already been discarded 2 minutes ago* :) So theoretically the checkpoints can keep failing forever and the job will not restart automatically, unless something else fails.

Even more funny things can happen if we mix FLINK-17350 . or b) with intermittent external system failure. Sink reports an exception, transaction was lost/aborted, Sink is in failed state, but if there will be a happy coincidence that it manages to accept further records, this exception can be lost and all of the records in those failed checkpoints will be lost forever as well. In all of the examples that [~qinjunjerry] posted it hasn't happened. {{FlinkKafkaProducer}} was not able to recover after the initial failure and it was keep throwing exceptions until the job finally failed (but much later then it should have). And that's not guaranteed anywhere.
",,ana4,klion26,pnowojski,qinjunjerry,roman,stevenz3wu,wind_ljy,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-12364,,FLINK-17327,,,,FLINK-17350,,FLINK-17043,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 15:15:27 UTC 2020,,,,,,,,,,"0|z0dzwg:",9223372036854775807,Checkpoint timeouts will now be treated as normal checkpoint failures and checked against `setTolerableCheckpointFailureNumber(...)`.,,,,,,,,,,,,,,,,,,,"24/Apr/20 11:17;roman;As we discussed offline, the fix would be to increment failure counter `CHECKPOINT_EXPIRED` in `CheckpointFailureManager`. Other reasons should also be checked.

But, some cases are not even reported to `CheckpointFailureManager`: `TOO_MANY_CONCURRENT_CHECKPOINTS` and `MINIMUM_TIME_BETWEEN_CHECKPOINTS` (maybe others).;;;","25/Apr/20 05:59;wind_ljy;[~roman_khachatryan] Interesting issue. But what if users don't set the {{CheckpointFailureManager}}? The exception thrown when producer is doing checkpointing, will also be swallowed because the checkpoint is expired.;;;","27/Apr/20 07:58;pnowojski;[~wind_ljy] for not swallowing the original exceptions there is another related ticket: FLINK-17350 

Here, we just want to account all of the missing failure reasons against failure counter. In FLINK-17350 we make sure that unrecoverable failures (like during synchronous checkpointing) will fail the task (& job) immediately.;;;","20/May/20 03:22;ym; 

Thanks for the pointers [~roman_khachatryan]. I have quite a nice walk ;)

I guess the fix is simple: increase `continuousFailureCounter` for exception `CHECKPOINT_EXPIRED` as well.

However, there is a list of checkpoint failure reasons listed (actually most of the reasons) are ignored.

Hence I am wondering what is the criteria for what should be ignored, and what should not?;;;","20/May/20 10:22;roman;I think we need to increment the counter for any checkpoint failure that is not caused by another checkpoint (like TOO_MANY_CONCURRENT_CHECKPOINTS) and a failure with a wider scope (like TASK_FAILURE).;;;","21/May/20 11:20;klion26;Attach another issue which wants to increase the count under more checkpoint fail reasons;;;","27/May/20 15:15;pnowojski;merged commit 3ec768a into apache:master, f9641411ca into apache:release-1.11

Fix will not be back-ported to previous releases, as it's changing semantic of the configuration (setTolerableCheckpointFailureNumber, setFailTaskOnCheckpointError);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTask should always fail immediately on failures in synchronous part of a checkpoint,FLINK-17350,13300558,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,pnowojski,pnowojski,23/Apr/20 12:49,16/Oct/20 10:32,13/Jul/23 08:07,16/May/20 08:12,1.10.0,1.6.4,1.7.2,1.8.3,1.9.2,1.11.0,,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,"This bugs also Affects 1.5.x branch.

As described in point 1 here: https://issues.apache.org/jira/browse/FLINK-17327?focusedCommentId=17090576&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17090576

{{setTolerableCheckpointFailureNumber(...)}} and its deprecated {{setFailTaskOnCheckpointError(...)}} predecessor are implemented incorrectly. Since Flink 1.5 (https://issues.apache.org/jira/browse/FLINK-4809) they can lead to operators (and especially sinks with an external state) end up in an inconsistent state. That's also true even if they are not used, because of another issue: FLINK-17351

If we mix this with intermittent external system failure. Sink reports an exception, transaction was lost/aborted, Sink is in failed state, but if there will be a happy coincidence that it manages to accept further records, this exception can be lost and all of the records in those failed checkpoints will be lost forever as well.

For details please check FLINK-17327.
",,aljoscha,klion26,liyu,pnowojski,qinjunjerry,stevenz3wu,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-4809,,FLINK-17327,,,,,,FLINK-17351,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 08:12:41 UTC 2020,,,,,,,,,,"0|z0dzw8:",9223372036854775807,"Failures in synchronous part of checkpointing (like an exceptions thrown by an operator) will fail it's Task (and job) immediately, regardless of the configuration parameters. Since Flink 1.5 such failures could be ignored by setting `setTolerableCheckpointFailureNumber(...)` or its deprecated `setFailTaskOnCheckpointError(...)` predecessor. Now both options will only affect asynchronous failures.",,,,,,,,,,,,,,,,,,,"23/Apr/20 13:18;pnowojski;There are two potential solutions. 

Option I.

We should just fail the task immediately on any failure in synchronous checkpoint part as we did before https://issues.apache.org/jira/browse/FLINK-4809 . This also includes failures in {{notifyCheckpointCompleted}} calls. 

Option II.

If we want to keep the {{setTolerableCheckpointFailureNumber(...)}} for synchronous failures, as some operators may be able to tolerate snapshot/committing failures, It would have to be implemented inside operators:
FlinkKafkaProducer would have to remember an exception and keep re-throwing it, or throw ""Rejecting writes because of a previous failure"". Or provide some API (interface? getter boolean canTolerateCheckpointingFailure()? wrapping an exception with FlinkAbortCheckpointException(...) and tolerate failures of only exception type?) for StreamTask to detect if operator can recover from checkpoint failures or not and implement this logic somewhere in the StreamTask/OperatorChain/OperatorWrapper

I'm strongly in favour of Option I, as it's much simpler and Option II doesn't seem to be adding much value and would require support from the operators side.

Asynchronous failures are acceptable, as the in-memory state of operators/functions is kept consistent, just waiting for {{notifyCheckpointCompleted}} call, so from the operators' perspective, ignoring asynchronous failure of Checkpoint N and successfully completing Checkpoint N+1 is indistinguishable from {{notifyCheckpointCompleted}} for Checkpoint N being lost in transit - which is a valid and correctly handled scenario by our exactly once sinks.;;;","16/May/20 08:12;pnowojski;Merged to master as 74e3d9f8bb..8ea458137e

I'm reluctant to back port this fix, as:
#  it's changing behaviour of checkpointing failures
# most likely this bug is only visible for operators with external state (like exactly-once sinks)
# our kafka sink was only theoretically affected by this - practically despite ignoring the failure, sink wouldn't recover and would start throwing some timeout exceptions eventually
# it's not trivial to back port the fix, as the relevant code has been changing over the time ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RecordWriterTest.testIdleTime possibly deadlocks on Travis,FLINK-17344,13300502,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Dragon.L,azagrebin,azagrebin,23/Apr/20 09:02,24/Apr/20 12:01,13/Jul/23 08:07,24/Apr/20 12:01,,,,,,1.11.0,,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,"[https://travis-ci.org/github/apache/flink/jobs/678193214]
 The test was introduced in FLINK-16864.
 It may be an instability as it passed 2 times (core and core-scala) and failed in core-hadoop:
 [https://travis-ci.org/github/apache/flink/builds/678193199]

The PR, for which the test suite was running, is only about JVM memory args for JM process.",,azagrebin,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 24 12:01:35 UTC 2020,,,,,,,,,,"0|z0dzjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/20 12:01;pnowojski;Merged to master as 948a42fceb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
freeSlot in TaskExecutor.closeJobManagerConnection cause ConcurrentModificationException,FLINK-17341,13300487,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,huwh,huwh,23/Apr/20 08:28,09/Oct/20 16:10,13/Jul/23 08:07,09/Oct/20 16:10,1.10.2,1.11.2,1.9.0,,,1.10.3,1.11.3,1.12.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,"TaskExecutor may freeSlot when closeJobManagerConnection. freeSlot will modify the TaskSlotTable.slotsPerJob. this modify will cause ConcurrentModificationException.
{code:java}
Iterator<AllocationID> activeSlots = taskSlotTable.getActiveSlots(jobId);

final FlinkException freeingCause = new FlinkException(""Slot could not be marked inactive."");

while (activeSlots.hasNext()) {
 AllocationID activeSlot = activeSlots.next();

 try {
 if (!taskSlotTable.markSlotInactive(activeSlot, taskManagerConfiguration.getTimeout())) {
 freeSlotInternal(activeSlot, freeingCause);
 }
 } catch (SlotNotFoundException e) {
 log.debug(""Could not mark the slot {} inactive."", jobId, e);
 }
}
{code}
 error log：
{code:java}
2020-04-21 23:37:11,363 ERROR org.apache.flink.runtime.rpc.akka.AkkaRpcActor                - Caught exception while executing runnable in main thread.
java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextNode(HashMap.java:1437)
    at java.util.HashMap$KeyIterator.next(HashMap.java:1461)
    at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable$TaskSlotIterator.hasNext(TaskSlotTable.java:698)
    at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable$AllocationIDIterator.hasNext(TaskSlotTable.java:652)
    at org.apache.flink.runtime.taskexecutor.TaskExecutor.closeJobManagerConnection(TaskExecutor.java:1314)
    at org.apache.flink.runtime.taskexecutor.TaskExecutor.access$1300(TaskExecutor.java:149)
    at org.apache.flink.runtime.taskexecutor.TaskExecutor$JobLeaderListenerImpl.lambda$jobManagerLostLeadership$1(TaskExecutor.java:1726)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
    at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
{code}",,huwh,libenchao,mapohl,trohrmann,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 09 16:10:19 UTC 2020,,,,,,,,,,"0|z0dzgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/20 06:45;trohrmann;Thanks for reporting this issue [~huwh]. I believe that the problem still exists in the current master.;;;","29/Sep/20 07:39;huwh;[~trohrmann]  I would like to fix this issue, could you assign this to me?;;;","29/Sep/20 13:26;trohrmann;How do you want to solve the problem [~huwh]?;;;","09/Oct/20 03:45;huwh;I think we can put the slots that need to be free in a list, and then free them out of the loop. [~trohrmann];;;","09/Oct/20 08:53;mapohl;Hi [~huwh], thanks for volunteering to fix it. Unfortunately, I took over yesterday already. But I followed the same approach you're proposing. Feel free to review [PR #13564|https://github.com/apache/flink/pull/13564].;;;","09/Oct/20 16:10;trohrmann;Fixed via

master: 832a4c19541d9f23017425f53e7d1eb884806c31
1.11.3: 5fd19275cc0b0d1f7d57aa12b2e02cede8b6c0c1
1.10.3: 0d78e806691cd04ac54713fc64372b7aeead6919;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalExecutorITCase.testBatchQueryCancel test timeout,FLINK-17338,13300464,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,rmetzger,rmetzger,23/Apr/20 07:20,27/Apr/20 05:43,13/Jul/23 08:07,26/Apr/20 08:28,1.10.0,1.11.0,,,,1.10.1,1.11.0,,,Table SQL / Client,Table SQL / Legacy Planner,Table SQL / Planner,,,0,test-stability,,,,"CI https://travis-ci.org/github/apache/flink/jobs/678185941

{code}
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.flink.table.client.gateway.local.LocalExecutorITCase
[ERROR] Tests run: 70, Failures: 0, Errors: 1, Skipped: 5, Time elapsed: 226.359 s <<< FAILURE! - in org.apache.flink.table.client.gateway.local.LocalExecutorITCase
[ERROR] testBatchQueryCancel[Planner: old](org.apache.flink.table.client.gateway.local.LocalExecutorITCase)  Time elapsed: 30.009 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 30000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testBatchQueryCancel(LocalExecutorITCase.java:733)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR]   LocalExecutorITCase.testBatchQueryCancel:733 Â» TestTimedOut test timed out aft...
[INFO] 
[ERROR] Tests run: 70, Failures: 0, Errors: 1, Skipped: 5
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] flink-table-common ................................. SUCCESS [  9.955 s]
[INFO] flink-table-api-java ............................... SUCCESS [  4.615 s]
[INFO] flink-table-api-java-bridge ........................ SUCCESS [  4.150 s]
[INFO] flink-table-api-scala .............................. SUCCESS [  2.843 s]
[INFO] flink-table-api-scala-bridge ....................... SUCCESS [  2.843 s]
[INFO] flink-cep .......................................... SUCCESS [ 30.868 s]
[INFO] flink-table-planner ................................ SUCCESS [04:51 min]
[INFO] flink-cep-scala .................................... SUCCESS [  8.361 s]
[INFO] flink-sql-client ................................... FAILURE [04:21 min]
[INFO] flink-state-processor-api .......................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
{code}",,aljoscha,liyu,rmetzger,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17349,,,,,,,,FLINK-17389,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 05:43:57 UTC 2020,,,,,,,,,,"0|z0dzbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/20 07:31;trohrmann;Another instance: https://api.travis-ci.org/v3/job/678204034/log.txt;;;","23/Apr/20 08:06;aljoscha;This test has always been very heavy, on my machine some tests are pushing 10s or more. This means that we can pretty much expect them to timeout on Travis with only a 30s timeout.

I have started investigating the runtime of this test recently. There are some quick fixes like excluding Hive catalogs from those tests but that doesn't solve the overall problem.

For now, we can massively increase the timeout until we fix it properly. WDYT?;;;","23/Apr/20 09:56;rmetzger;Also happening in my private AZP (MSFT hosted): https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7891&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=6855dd6e-a7b0-5fd1-158e-29fc186b16c8

I'm okay with that workaround as long as we mark this ticket a release blocker and really resolve it properly.;;;","23/Apr/20 12:03;aljoscha;I increased the test timeouts master: 008e0afb3c62c059dcdf2c58a43cdd2e2d283512

I don't think we will be able to fix this ""properly"" . Some tests are inefficient, yes, because they often recompile/optimize SQL queries. But I changed a test to only do the bare minimum and it still takes roughly 8s on my machine. The majority of this time is spent in Calcite, parsing and optimising the query, and then in a mix of Flink and Calcite to translate that into a JobGraph. Unless we speed up those parts or radically refactor this test we won't solve that.

I'll look into it a bit more.;;;","23/Apr/20 12:42;aljoscha;master: 008e0afb3c62c059dcdf2c58a43cdd2e2d283512

Fixed by increasing timeouts, please re-open if it fails again.;;;","23/Apr/20 12:54;rmetzger;Okay, thanks!;;;","24/Apr/20 06:47;liyu;Also see this failure in release-1.10 crone job: https://api.travis-ci.org/v3/job/678277566/log.txt

So back ported the fix into release-1.10 via: 0e2b520ec60cc11dce210bc38e574a05fa5a7734;;;","25/Apr/20 18:43;trohrmann;It failed unfortunately again: https://api.travis-ci.com/v3/job/323314732/log.txt;;;","25/Apr/20 18:49;liyu;The same observation in latest release-1.10 crone job: https://api.travis-ci.org/v3/job/679144604/log.txt

Now the error message is different, though (no more timeout but incorrect job status):
{code}
15:51:15.970 [ERROR] testBatchQueryCancel[Planner: blink](org.apache.flink.table.client.gateway.local.LocalExecutorITCase)  Time elapsed: 31.938 s  <<< FAILURE!
java.lang.AssertionError: expected:<CANCELED> but was:<FINISHED>
	at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testBatchQueryCancel(LocalExecutorITCase.java:738)
{code};;;","26/Apr/20 07:37;aljoscha;[~liyu]  It's a different issue, the test added in FLINK-15669 is flaky by design. I'm reverting the test and re-opening FLINK-15669.;;;","26/Apr/20 08:28;aljoscha;I reverted the broken tests: https://issues.apache.org/jira/browse/FLINK-15669?focusedCommentId=17092586&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17092586;;;","27/Apr/20 05:43;liyu;Thanks for the analysis and clarification [~aljoscha];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MVN exited with EXIT CODE: 143. in ""libraries"" test job",FLINK-17336,13300458,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,23/Apr/20 07:10,13/May/20 09:56,13/Jul/23 08:07,13/May/20 09:56,,,,,,1.11.0,,,,Build System / Azure Pipelines,Tests,,,,0,test-stability,,,,"CI:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=89&view=logs&j=56781494-ebb0-5eae-f732-b9c397ec6ede&t=32b25b6b-f46f-5bca-b5eb-2c6936ee77a4

maven reports ""build success"", but the exit code is 143?

{code}
[INFO] flink-state-processor-api .......................... SUCCESS [  0.273 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 19:24 min
[INFO] Finished at: 2020-04-23T00:46:43+00:00
[INFO] Final Memory: 246M/4214M
[INFO] ------------------------------------------------------------------------
[WARNING] The requested profile ""e2e-hadoop"" could not be activated because it does not exist.
MVN exited with EXIT CODE: 143.
Trying to KILL watchdog (265).
==============================================================================

{code}",,pnowojski,rmetzger,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 09:56:20 UTC 2020,,,,,,,,,,"0|z0dza0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/20 12:16;rmetzger;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=204&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=47b5881b-1c8d-5954-c1b9-11b5f629a3e3;;;","25/Apr/20 12:31;rmetzger;Another one: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=222&view=logs&j=0e4fa295-925e-5ac3-fc5b-ba578d0130c2&t=1d0dfc14-a947-5572-a0fd-f4b9d3264766;;;","30/Apr/20 13:06;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=464&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","11/May/20 11:47;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=965&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","11/May/20 11:48;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=969&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa;;;","11/May/20 12:10;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=955&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","11/May/20 12:10;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=955&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa;;;","11/May/20 13:02;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=965&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","11/May/20 13:28;rmetzger;I think I understood of build #965 (the previous link):
- The HadoopS3FileSystemITCase doesn't produce any output for more than 10 minutes (probably because of the Ali network)
- the ""travis watchdog"" detects that there's no output
- the watchdog is supposed to kill the maven process, but it fails to do so. There is this message 
{code}
2020-05-11T10:57:09.3176283Z ./tools/travis_watchdog.sh: line 247:  2434 Terminated              $cmd
{code}
It implies that something was killed, but not maven.
- the test continues (after 1549 seconds ~ 25 minutes)
- maven finishes with a ""Build Success""

Current theory:
We recently introduced the {{run_mvn}} function. Maybe the bash running that function gets killed, and reports 143 (that's what a process returns when it receives a sigterm.).
;;;","11/May/20 17:09;rmetzger;I opened a PR: https://github.com/apache/flink/pull/12089;;;","13/May/20 09:48;pnowojski;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=973&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa

however this happened in the misc job, as job looked almost done:

{noformat}
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 01:00 h
[INFO] Finished at: 2020-05-11T12:11:41+00:00
[INFO] Final Memory: 397M/6300M
[INFO] ------------------------------------------------------------------------
MVN exited with EXIT CODE: 143.
Trying to KILL watchdog (265).
{noformat}
;;;","13/May/20 09:52;rmetzger;The real reason for this error is that maven didn't produce output for more than 5 minutes. If you search the log for ""Maven produced no output for 300 seconds."" you will see where it happened.
I'm waiting for a reviewer to approve my PR that fixes this behavior :) ;;;","13/May/20 09:56;rmetzger;Resolved in https://github.com/apache/flink/commit/dee9583aca51520435cd1893c9f2897cb003c0a9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Flink does not support HIVE UDFs with primitive return types,FLINK-17334,13300436,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,royruan,royruan,royruan,23/Apr/20 05:05,29/Apr/20 08:16,13/Jul/23 08:07,29/Apr/20 08:16,1.10.0,,,,,1.10.1,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"We are currently migrating Hive UDF to Flink. While testing compatibility, we found that Flink cannot support primitive types like boolean, int, etc.

Hive UDF:

public class UDFTest extends UDF {
 public boolean evaluate(String content) {
 if (StringUtils.isEmpty(content))

{ return false; }

else

{ return true; }

}

}

We found that the following error will be reported:
 Caused by: org.apache.flink.table.functions.hive.FlinkHiveUDFException: Class boolean is not supported yet
 at org.apache.flink.table.functions.hive.conversion.HiveInspectors.getObjectInspector(HiveInspectors.java:372)
 at org.apache.flink.table.functions.hive.HiveSimpleUDF.getHiveResultType(HiveSimpleUDF.java:133)

I found that if I add the type comparison in HiveInspectors.getObjectInspector to the primitive type, I can get the correct result.

as follows：
 public static ObjectInspector getObjectInspector(HiveShim hiveShim, Class clazz){       

   ..........

          else if (clazz.equals(boolean.class) || clazz.equals(Boolean.class) ||                  clazz.equals(BooleanWritable.class))

{                                       typeInfo = TypeInfoFactory.booleanTypeInfo;                                 }

         ..........

}

 

 ",,jark,libenchao,lirui,lzljs3620320,royruan,,,,,,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Wed Apr 29 08:16:12 UTC 2020,,,,,,,,,,"0|z0dz54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/20 05:11;lzljs3620320;Hi [~royruan] thanks for reporting. Can you provide more information? Like what hive UDF? Maybe you can show the code.;;;","23/Apr/20 06:38;lzljs3620320;Thanks [~royruan] for updating.

CC: [~lirui];;;","23/Apr/20 06:38;royruan;Thanks for the reply, I added my question;;;","23/Apr/20 08:30;lzljs3620320;Hi [~royruan] assigned to you, before creating pull request, it is better to request assigning.;;;","24/Apr/20 03:40;royruan;Hi all, I want to support array and map types, but I found that I can't get the type of key and value in Map type, and can't create the corresponding object inspector, I have added the picture to my question。

If I want to create Map's object inspector, I need to call

ObjectInspectorFactory.getStandardMapObjectInspector(getObjectInspector(mapType.getMapKeyTypeInfo()), getObjectInspector(mapType.getMapValueTypeInfo()));

And I need to get MapValueTypeInfo() and MapKeyTypeInfo();;;","29/Apr/20 02:52;lzljs3620320;master: e8503986132f0ffaeec91caf5da6ece2d0eb70d3;;;","29/Apr/20 03:18;lzljs3620320;[~royruan] before closing, we should wait for [https://github.com/apache/flink/pull/11939];;;","29/Apr/20 03:36;royruan;ok;;;","29/Apr/20 08:16;lzljs3620320;release-1.10: 8ef57fa9e844d74354aef96531f09d5daf8f121f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix restart policy not equals to Never for native task manager pods,FLINK-17332,13300428,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,felixzheng,felixzheng,felixzheng,23/Apr/20 04:17,06/May/20 02:56,13/Jul/23 08:07,06/May/20 02:56,1.10.0,1.10.1,,,,1.11.0,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"Currently, we do not explicitly set the {{RestartPolicy}} for the task manager pods in the native K8s setups so that it is {{Always}} by default.  The task manager pod itself should not restart the failed Container, the decision should always be made by the job manager.

Therefore, this ticket proposes to set the {{RestartPolicy}} to {{Never}} for the task manager pods.",,felixzheng,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 02:56:20 UTC 2020,,,,,,,,,,"0|z0dz3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/20 02:56;tison;master(1.11) via 04ab8d236254777e16bdabfceb217190e3b2cfde;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka unavailability could cause Flink TM shutdown,FLINK-17327,13300291,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,qinjunjerry,qinjunjerry,22/Apr/20 14:51,26/Feb/21 13:42,13/Jul/23 08:07,15/Jun/20 17:14,1.10.0,,,,,1.11.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"Steps to reproduce:
 # Start a Flink 1.10 standalone cluster
 # Run a Flink job which reads from one Kafka topic and writes to another topic, with exactly-once checkpointing enabled
 # Stop all Kafka Brokers after a few successful checkpoints

When Kafka brokers are down:
 # {{org.apache.kafka.clients.NetworkClient}} reported connection to broker could not be established
 # Then, Flink could not complete snapshot due to {{Timeout expired while initializing transactional state in 60000ms}}
 # After several snapshot failures, Flink reported {{Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.}}
 # Eventually, Flink tried to cancel the task which did not succeed within 3 min. According to logs, consumer was cancelled, but producer is still running
 # Then {{Fatal error occurred while executing the TaskManager. Shutting it down...}}

I will attach the logs to show the details.  Worth to note that if there would be no consumer but producer only in the task, the behavior is different:
 # {{org.apache.kafka.clients.NetworkClient}} reported connection to broker could not be established
 # after {{delivery.timeout.ms}} (2min by default), producer reports: {{FlinkKafkaException: Failed to send data to Kafka: Expiring 4 record(s) for output-topic-0:120001 ms has passed since batch creation}}
 # Flink tried to cancel the upstream tasks and created a new producer
 # The new producer obviously reported connectivity issue to brokers
 # This continues till Kafka brokers are back. 
 # Flink reported {{Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.}}
 # Flink cancelled the tasks and restarted them
 # The job continues, and new checkpoint succeeded. 
 # TM runs all the time in this scenario

I set Kafka transaction time out to 1 hour just to avoid transaction timeout during the test.

To get a producer only task, I called {{env.disableOperatorChaining();}} in the second scenario. 

 

 

 ",,aljoscha,Echo Lee,gaoyunhaii,klion26,libenchao,nikobearrr,pnowojski,qinjunjerry,stevenz3wu,trohrmann,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17351,FLINK-17350,FLINK-18311,,,,,,FLINK-15362,,,,,,,,,"08/Jun/20 12:49;aljoscha;0001-Change-version-to-2.4.2-ALJOSCHA.patch;https://issues.apache.org/jira/secure/attachment/13005114/0001-Change-version-to-2.4.2-ALJOSCHA.patch","08/Jun/20 12:49;aljoscha;0002-Don-t-abort-in-flight-transactions.patch;https://issues.apache.org/jira/secure/attachment/13005115/0002-Don-t-abort-in-flight-transactions.patch","22/Apr/20 20:29;qinjunjerry;Standalonesession.log;https://issues.apache.org/jira/secure/attachment/13000880/Standalonesession.log","22/Apr/20 15:05;qinjunjerry;TM.log;https://issues.apache.org/jira/secure/attachment/13000862/TM.log","22/Apr/20 15:05;qinjunjerry;TM_produer_only_task.log;https://issues.apache.org/jira/secure/attachment/13000863/TM_produer_only_task.log",,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 17:14:58 UTC 2020,,,,,,,,,,"0|z0dy8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/20 15:07;qinjunjerry;{{TM.log}} is for a job where Kafka Source and Kafka Sink are in a single task thread

{{TM_producer_only_task.log}} is for a job with three task threads: Kafka Source, map, Kafka Sink;;;","22/Apr/20 20:31;qinjunjerry;Added also the Standalonesession logs which covers the time frame of both tests. My local standalone cluster is basically one TM with one slot. ;;;","23/Apr/20 08:07;aljoscha;Isn't this the expected behaviour? If Flink cannot write to Kafka then the job will fail. Is that what you're observing?;;;","23/Apr/20 08:42;pnowojski;I'm analysing this [~aljoscha] and there are at least couple of issues here that we need to fix. Once I'm done I will post my findings.;;;","23/Apr/20 12:31;pnowojski;Thanks [~qinjunjerry] for submitting the issue. There are multiple issues that contributed together to the symptoms that you have reported. I will list them in the descending priority order:

1. *CRITICAL bug*: FLINK-17350 {{setTolerableCheckpointFailureNumber(...)}} and its deprecated {{setFailTaskOnCheckpointError(...)}} predecessor are implemented incorrectly. Since Flink 1.5 (https://issues.apache.org/jira/browse/FLINK-4809) they can lead to operators (and especially sinks with an external state) end up in an inconsistent state. That's also true even if they are not used, because...

2. *CRITICAL bug*: FLINK-17351 The logic in how {{CheckpointCoordinator}} handles checkpoint timeouts is broken. In your [~qinjunjerry] examples, your job should have failed after first checkpoint failure, but checkpoints were time outing on CheckpointCoordinator after 5 seconds, before {{FlinkKafkaProducer}} was detecting Kafka failure after 2 minutes. Those timeouts were not checked against {{setTolerableCheckpointFailureNumber(...)}} limit, so the job was keep going with many timed out checkpoints. Now funny thing happens: FlinkKafkaProducer detects Kafka failure. Funny thing is that it depends where the failure was detected:

a) on processing record? no problem, job will failover immediately once failure is detected (in this example after 2 minutes)
b) on checkpoint? heh, the failure is reported to {{CheckpointCoordinator}} *and gets ignored, as PendingCheckpoint has already been discarded 2 minutes ago* :) So theoretically the checkpoints can keep failing forever and the job will not restart automatically, unless something else fails.

Even more funny things can happen if we mix 1. or 2b) with intermittent external system failure. Sink reports an exception, transaction was lost/aborted, Sink is in failed state, but if there will be a happy coincidence that it manages to accept further records, this exception can be lost and all of the records in those failed checkpoints will be lost forever as well. In all of the examples that [~qinjunjerry] posted it hasn’t happened. {{FlinkKafkaProducer}} was not able to recover after the initial failure and it was keep throwing exceptions until the job finally failed (but much later then it should have). And that’s not guaranteed anywhere.

3. *non critical bug*: previously reported FLINK-16482

{{FlinkKafkaConsumer}} is not gracefully closing:
{code}
2020-04-21 17:17:50,612 INFO  org.apache.flink.runtime.taskmanager.Task                     - Attempting to cancel task Source: Custom Source -> Sink: Unnamed (1/1) (8928563344a077ee98377721a2f22790).
2020-04-21 17:17:50,612 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: Custom Source -> Sink: Unnamed (1/1) (8928563344a077ee98377721a2f22790) switched from RUNNING to CANCELING.
2020-04-21 17:17:50,612 INFO  org.apache.flink.runtime.taskmanager.Task                     - Triggering cancellation of task code Source: Custom Source -> Sink: Unnamed (1/1) (8928563344a077ee98377721a2f22790).
2020-04-21 17:17:50,614 WARN  org.apache.flink.streaming.runtime.tasks.StreamTask           - Error while canceling task.
org.apache.flink.streaming.connectors.kafka.internal.Handover$ClosedException
	at org.apache.flink.streaming.connectors.kafka.internal.Handover.close(Handover.java:182)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.cancel(KafkaFetcher.java:175)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.cancel(FlinkKafkaConsumerBase.java:818)
	at org.apache.flink.streaming.api.operators.StreamSource.cancel(StreamSource.java:147)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.cancelTask(SourceStreamTask.java:136)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cancel(StreamTask.java:602)
	at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1355)
	at java.lang.Thread.run(Thread.java:748)
{code}
That could be analysed why is it happening, as this is what brought the cluster down (caused TaskExecutor to restart)

4. You configuration doesn’t make much sense. You are time outing checkpoints in 5-10s, with similar checkpoint interval, but timeouts in Kafka are probably still on default values, like 120 or 60 seconds:
{code}
2020-04-21 17:15:50,585 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator  - Could not complete snapshot 7 for operator Source: Custom Source -> Sink: Unnamed (1/1).
org.apache.kafka.common.errors.TimeoutException: Timeout expired while initializing transactional state in 60000ms.
{code}
That is not a big issue on it’s own, it’s just one of the things that triggered this whole complicated crash ({{CheckpointCoordinator}} timeouting checkpoint, which triggered other bugs)

5. The reason why splitting the job into multiple tasks helped, is probably just a pure lack. In this case FlinkKafkaProducer was failing, triggering cancelation on other tasks which actually completed successfully.

6.
{code}
Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.
{code}
Is just an after shock of {{FlinkKafkaProducer}} ending up in broken/inconsistent state because of previously ignored errors (points 1. and 2.). Task should have failed much sooner, after first snapshotting failure. But it kept going until it run out of producers in the pool, but that’s pure coincidence. After the first failure, there was no way to keep going without a data loss, and we were actually lucky that it didn’t recover and there was some terminal failure after all. (edited) 


As there are 3 different bugs, I will create new tickets for those issues.
;;;","23/Apr/20 13:07;pnowojski;I'm closing this, as it's a duplicate of an existing bug:
FLINK-16482
and I've created two different tickets
FLINK-17350
FLINK-17351
to track remaining issues.

If I have missed anything, please feel to re-open this bug report.;;;","04/May/20 14:45;aljoscha;I'm re-opening for now since I think the KafkaConsumer is working as designed, i.e. FLINK-16482 is not a bug (though I don't like the exception throwing behaviour).

Btw, the Kafka Producer is stuck on a lock, that's why the TM is eventually killed:
{code}
2020-05-04 16:43:21,297 WARN  org.apache.flink.runtime.taskmanager.Task                     - Task 'Map -> Sink: Unnamed (1/1)' did not react to cancelling signal for 30 seconds, but is stuck in method:
 sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
org.apache.kafka.clients.producer.internals.TransactionalRequestResult.await(TransactionalRequestResult.java:50)
org.apache.kafka.clients.producer.KafkaProducer.commitTransaction(KafkaProducer.java:698)
org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.commitTransaction(FlinkKafkaInternalProducer.java:103)
org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.recoverAndCommit(FlinkKafkaProducer.java:920)
org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.recoverAndCommit(FlinkKafkaProducer.java:98)
org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.recoverAndCommitInternal(TwoPhaseCommitSinkFunction.java:405)
org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.initializeState(TwoPhaseCommitSinkFunction.java:358)
org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.initializeState(FlinkKafkaProducer.java:1042)
org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:178)
org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:160)
org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96)
org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:284)
org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:989)
org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:453)
org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$122/1846623322.run(Unknown Source)
org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)
org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:448)
org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:460)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
java.lang.Thread.run(Thread.java:748)
{code};;;","04/May/20 15:31;aljoscha;I believe that {{TransactionalRequestResult.await()}} is the culprit for the indefinite blocking, the latch is not counted down in the failure case: https://github.com/apache/kafka/blob/2.2/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionalRequestResult.java#L38.

I also believe that this bug in Kafka was fixed here as an unrelated change: https://github.com/apache/kafka/commit/df13fc93d0aebfe0ecc40dd4af3c5fb19b35f710#diff-8a2c4f47dcec247ce2ecebf082b3d0b1R42.;;;","05/May/20 07:37;aljoscha;The fix I mentioned is only available on Kafka 2.5.x, so to fix it we should open Kafka Issues and fix it also for earlier versions.;;;","05/May/20 07:45;qinjunjerry;The scenario I described in the first entry of this Jira was tested with kafka_2.12-2.5.0;;;","05/May/20 07:59;pnowojski;[~qinjunjerry] the bug that [~aljoscha] is talking about, that blocked the {{FlinkKafkaProducer}}, is on the Kafka client's side, not on the brokers. To fix it, we would have to upgrade Kafka dependencies in our connector to 2.5.0 (currently it's 2.2.x), which in turn is blocked by FLINK-15362. ;;;","05/May/20 10:16;aljoscha;This is another fix we need: https://issues.apache.org/jira/browse/KAFKA-7763. It's only available from 2.3.x onwards.;;;","05/May/20 11:00;aljoscha;For testing I tried this against Kafka 2.5.0 (which we can't really do, see FLINK-15362). Here the job fails successfully, because we don't wait indefinitely for the result. However, now we have a leftover Kafka {{NetworkClient}} which logs indefinitely:
{code}
2020-05-05 12:59:03,465 WARN  org.apache.kafka.clients.NetworkClient                       [] - [Producer clientId=producer-Map -> Sink: Unnamed-c09dc291fad93d575e015871097bfc60-4, transactionalId=Map -> Sink: Unnamed-c09dc291fad93d575e015871097bfc60-4] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
{code}

Also not good, because then your {{TaskManagers}} will eventually be full of leftover Kafka threads.;;;","05/May/20 11:00;aljoscha;I think the Kafka code doesn't like being interrupted, which Flink does when cancelling.;;;","05/May/20 11:09;qinjunjerry;What do you mean by ""job fails successfully""? With Flink 1.10, I believe you will get the following exception (after 1 min) which will then trigger the job cancellation:
{code:java}
2020-05-04 16:34:36,262 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator  - Could not complete snapshot 2 for operator Source: Custom Source -> Sink: Unnamed (1/1).
org.apache.kafka.common.errors.TimeoutException: Timeout expired while initializing transactional state in 60000ms.
{code};;;","11/May/20 09:01;aljoscha;Btw, this can be reproduced even simpler. A job with just a synthetic source and a {{FlinkKafkaProducer}} will also be stuck in cancelling and eventually kill the TM. So it has nothing to do with FLINK-16482.;;;","08/Jun/20 12:48;aljoscha;*{color:red}edit: this description is not fully accurate anymore, we don't need to patch Kafka but can instead call close with a zero timeout. I'm adding a better description below.{color}*

I managed to find a fix for this:

 - change our code to always use {{close()}} with a timeout on the Kafka Producer, if not, we might leave lingering threads
 - this alone does not work because of KAFKA-7763, i.e. on shutdown requests are not properly cancelled, which leaves lingering threads
this alone does not work because of KAFKA-6635, i.e. on shutdown requests are not properly cancelled, which leaves lingering threads
 - the fix KAFKA-6635 also introduces code that aborts outstanding transactions when cancelling. This doesn't work together with our exactly-once Kafka Producer
 - you need a patched Kafka that includes the fix part of KAFKA-6635, without the code that aborts transactions, I'm attaching a patch for that against the Kafka 2.4 branch

The changes needed in Flink are here: https://github.com/aljoscha/flink/tree/flink-17327-kafka-clean-shutdown-2.4. Patch for Kafka is attached. I don't think the Kafka project will like that patch, though, because aborting outstanding transactions is valid for Kafka Streams/KSQL where pending transactions that are not cancelled with block downstream consumption. ;;;","09/Jun/20 16:44;aljoscha;I was mixing up issues before, KAFKA-6635 has a fix but also introduces the ""feature"" that transactions are aborted on shutdown.;;;","10/Jun/20 14:40;aljoscha;For posterity, here's a summary of the issue:
* our code calls {{close()}} on the {{KafkaProducer}} (Kafka code), which is equivalent to calling {{close}} with a timeout of {{Long.MAX_VALUE}}
* this means threads will leak when a failure happens, for example because of Broker downtime
* the Flink Task Watchdog will kill the Task Manager because of these threads after a timeout

The fix is to always call {{close()}} with a reasonable timeout.

The fix also requires a Kafka version bump because of KAFKA-6635/KAFKA-7763, which mean that resources still leak even when closing with a timeout. Additionally, we need to close with exactly zero as timeout, because otherwise in-flight transactions will be aborted.;;;","15/Jun/20 17:14;aljoscha;release-1.11: 7d4041250dfefeb919b5d2854e993434e8b68401
master: 711f619d3f61b7cf37b13fefb820edff2199b19d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChannelStateReader rejects requests about unkown channels (Unaligned checkpoints),FLINK-17323,13300212,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,22/Apr/20 09:01,23/Apr/20 02:36,13/Jul/23 08:07,23/Apr/20 02:36,1.11.0,,,,,1.11.0,,,,Runtime / Task,,,,,0,pull-request-available,,,,"ChannelStateReader expects requests only for channels or subpartitions that have state.

In case of upscaling or starting from scratch this behavior is incorrect. It should return NO_MORE_DATA.",,roman,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 02:36:15 UTC 2020,,,,,,,,,,"0|z0dxrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/20 02:36;zjwang;Merged in master: 07667d29181be2cd4281d36c8cdfbd9a6c4e704a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable latency tracker would corrupt the broadcast state,FLINK-17322,13300210,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,yunta,yunta,22/Apr/20 08:58,22/Jun/21 14:05,13/Jul/23 08:07,16/Jun/20 07:44,1.10.1,1.11.0,1.12.0,1.9.3,,1.10.2,1.11.0,1.12.0,,Runtime / Network,,,,,0,pull-request-available,,,,"This bug is reported from user mail list:
 [http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Latency-tracking-together-with-broadcast-state-can-cause-job-failure-td34013.html]

Execute {{BroadcastStateIT#broadcastStateWorksWithLatencyTracking}} would easily reproduce this problem.

From current information, the broadcast element would be corrupt once we enable {{env.getConfig().setLatencyTrackingInterval(2000)}}.
 The exception stack trace would be: (based on current master branch)
{code:java}
Caused by: java.io.IOException: Corrupt stream, found tag: 84
	at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:217) ~[classes/:?]
	at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:46) ~[classes/:?]
	at org.apache.flink.runtime.plugable.NonReusingDeserializationDelegate.read(NonReusingDeserializationDelegate.java:55) ~[classes/:?]
	at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:157) ~[classes/:?]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:123) ~[classes/:?]
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:181) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:332) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:206) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:196) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:505) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:485) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:544) ~[classes/:?]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_144]
{code}",,AHeise,felixzheng,hackergin,klion26,kyledong,pnowojski,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/20 08:59;yunta;Telematics2-feature-flink-1.10-latency-tracking-broken.zip;https://issues.apache.org/jira/secure/attachment/13000810/Telematics2-feature-flink-1.10-latency-tracking-broken.zip",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 07:44:33 UTC 2020,,,,,,,,,,"0|z0dxqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/20 09:00;yunta;I have also uploaded the related project to reproduce this problem, which is offered from user Lasse Nedergaard ;;;","21/May/20 11:10;arvid;_Removed old message._

 

Bug seems to be related to random emit within BroadcastRecordWriter. I have found a simple fix, which influences latency measurement on source level (will underestimate lag). I will dig deeper to find a proper fix.;;;","12/Jun/20 22:24;arvid;Proper fix has been merged to master; currently backporting.;;;","15/Jun/20 11:31;arvid;Merged fix to 1.11 and 1.10 is about to be merged.

For 1.9, there seems to be a completely different cause; the buggy code first appeared in 1.10. Since 1.11 is about to pop and latency markers in conjunction with broadcast state is rarely used, we decided to not fix it in 1.9.

I'll close this ticket as soon as 1.10 is merged.;;;","16/Jun/20 07:44;arvid;Merged to master as 7e7b132f611f5a564e8c7b1e6b195692293fa90f
Merged to 1.11 as dfdfdadf445ea055c841c526b1a382424e1e1865
Merged to 1.10 as 5c0f827bf1c1f531ba30f97bdf6ab4393a32b0f0. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken link to walkthrough guide,FLINK-17319,13300192,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wiener,wiener,wiener,22/Apr/20 07:45,22/Apr/20 15:19,13/Jul/23 08:07,22/Apr/20 15:19,statefun-2.0.0,,,,,statefun-2.1.0,,,,Documentation,Stateful Functions,,,,0,pull-request-available,,,,"Currently, there is a broken link in the index.md to the walkthrough guide, referencing to walkthrough.html that does not exist.

see [https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.0/]",,igal,wiener,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 22 15:19:50 UTC 2020,,,,,,,,,,"0|z0dxmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/20 15:19;igal;Fixed at f3ebc7b71e6197b96f8c4ef9884d14460b406192;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointITCase.shouldPerformUnalignedCheckpointMassivelyParallel failed in timeout,FLINK-17315,13300149,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,zjwang,zjwang,22/Apr/20 04:15,22/Jun/21 14:04,13/Jul/23 08:07,25/Sep/20 16:56,,,,,,1.12.0,,,,Runtime / Checkpointing,Tests,,,,0,pull-request-available,test-stability,,,"Build: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323]

logs
{code:java}
2020-04-21T20:25:23.1139147Z [ERROR] Errors: 
2020-04-21T20:25:23.1140908Z [ERROR]   UnalignedCheckpointITCase.shouldPerformUnalignedCheckpointMassivelyParallel:80->execute:87 Â» TestTimedOut
2020-04-21T20:25:23.1141383Z [INFO] 
2020-04-21T20:25:23.1141675Z [ERROR] Tests run: 1525, Failures: 0, Errors: 1, Skipped: 36
{code}
 
I run it in my local machine and it almost takes about 40 seconds to finish, so the configured 90 seconds timeout seems not enough in heavy load environment sometimes. Maybe we can remove the timeout in tests since azure already configured to monitor the timeout.
 ",,AHeise,aljoscha,jark,klion26,liyu,pnowojski,rmetzger,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17580,,,,,,,,,,,,,,,,FLINK-19027,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 15:23:44 UTC 2020,,,,,,,,,,"0|z0dxdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/20 06:19;rmetzger;Another case: https://travis-ci.org/github/apache/flink/jobs/679511829;;;","27/Apr/20 13:46;pnowojski;timeout bumped to 5 minutes by commit c18ba69 into apache:master;;;","30/Apr/20 06:48;rmetzger;The timeout increase does not seem to solve the issue: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=447&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","30/Apr/20 13:05;rmetzger;shouldPerformUnalignedCheckpointOnRemoteChannels also seems to have this issue?

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=454&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323

https://api.travis-ci.com/v3/job/324160498/log.txt;;;","30/Apr/20 14:03;pnowojski;If one of them is susceptible to this issue, probably others are as well. Why are those test taking so much time? Can we make them thiner/faster?
 
Is it because of volume of data being processed? If so, can we slow it down, by adding some sleeps?
Is it because of IO operations? If so, can we reduce the number of IO or use some in memory storage?;;;","30/Apr/20 16:39;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=475&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","30/Apr/20 17:35;rmetzger;Upgrading to blocker https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=478&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","30/Apr/20 19:48;arvid;If even shouldPerformUnalignedCheckpointOnRemoteChannels times out after 5 minutes, then it's safe to say that this is a deadlock. Chances are high that we have already fixed the underlying bug in some lingering PR, but we should disable the test for now.;;;","01/May/20 04:37;rmetzger;Tests are disabled through https://github.com/apache/flink/commit/1f036d74e794eb23feb05c8aa672612d9ecc63d0 now;;;","06/May/20 03:37;jark;I still hit this problem for my pull request CI yesterday: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=631&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","07/May/20 10:10;arvid;[~jark] can you double check that your commit is based on [https://github.com/apache/flink/commit/1f036d74e794eb23feb05c8aa672612d9ecc63d0] ? It seemed like your branched off a master that came before that commit.;;;","12/May/20 17:57;pnowojski;Test re-enabled and underlying issues are seemed to be fixed via commits 755b576bcc..71e85d8b3d on the master branch.;;;","15/May/20 07:50;rmetzger;{{UnalignedCheckpointITCase.shouldPerformUnalignedCheckpointOnRemoteChannels:77->execute:89 » TestTimedOut}}

master build from tonight: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1339&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","15/May/20 16:47;pnowojski;As part of FLINK-17218, we have implemented couple of fixes. It would be good to check if this failure re-appears after merging FLINK-17218 (it's going to happen today).

Also I decreased the priority, as it's not happening so frequently right now.;;;","21/May/20 06:27;zjwang;I close this ticket and try to resolve all the issues in [FLINK-17768|https://issues.apache.org/jira/browse/FLINK-17768];;;","12/Jun/20 06:04;rmetzger;I'm reopening this ticket, as the test failed again: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3323&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","12/Jun/20 12:15;aljoscha;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3389&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4017;;;","12/Jun/20 15:23;pnowojski;I've disabled this test on master branch via 86b17c2a98 and release-1.11 6e54e0ffe4 once more time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validation error when insert decimal/varchar with precision into sink using TypeInformation of row,FLINK-17313,13300139,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Terry1897,Terry1897,Terry1897,22/Apr/20 03:10,05/May/20 18:06,13/Jul/23 08:07,27/Apr/20 09:26,,,,,,1.10.1,1.11.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Test code like follwing(in blink planner):
{code:java}
		tEnv.sqlUpdate(""create table randomSource ("" +
						""		a varchar(10),"" +
						""		b decimal(20,2)"" +
						""	) with ("" +
						""		'type' = 'random',"" +
						""		'count' = '10'"" +
						""	)"");
		tEnv.sqlUpdate(""create table printSink ("" +
						""		a varchar(10),"" +
						""		b decimal(22,2),"" +
						""	) with ("" +
						""	'type' = 'print'"" +
						""	)"");
		tEnv.sqlUpdate(""insert into printSink select * from randomSource"");
		tEnv.execute("""");
{code}

Print TableSink implements UpsertStreamTableSink and it's getReocrdType is as following:


{code:java}
public TypeInformation<Row> getRecordType() {
		return getTableSchema().toRowType();
	}
{code}


Varchar column validation exception is:

org.apache.flink.table.api.ValidationException: Type VARCHAR(10) of table field 'a' does not match with the physical type STRING of the 'a' field of the TableSink consumed type.

	at org.apache.flink.table.utils.TypeMappingUtils.lambda$checkPhysicalLogicalTypeCompatible$4(TypeMappingUtils.java:165)
	at org.apache.flink.table.utils.TypeMappingUtils$1.defaultMethod(TypeMappingUtils.java:278)
	at org.apache.flink.table.utils.TypeMappingUtils$1.defaultMethod(TypeMappingUtils.java:255)
	at org.apache.flink.table.types.logical.utils.LogicalTypeDefaultVisitor.visit(LogicalTypeDefaultVisitor.java:67)
	at org.apache.flink.table.types.logical.VarCharType.accept(VarCharType.java:157)
	at org.apache.flink.table.utils.TypeMappingUtils.checkIfCompatible(TypeMappingUtils.java:255)
	at org.apache.flink.table.utils.TypeMappingUtils.checkPhysicalLogicalTypeCompatible(TypeMappingUtils.java:161)
	at org.apache.flink.table.planner.sinks.TableSinkUtils$$anonfun$validateLogicalPhysicalTypesCompatible$1.apply$mcVI$sp(TableSinkUtils.scala:315)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.flink.table.planner.sinks.TableSinkUtils$.validateLogicalPhysicalTypesCompatible(TableSinkUtils.scala:308)
	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:195)
	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:191)
	at scala.Option.map(Option.scala:146)
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:191)
	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:150)
	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:150)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:150)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:863)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translateAndClearBuffer(TableEnvironmentImpl.java:855)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:822)

Other type validation exception is similar, I dig into and think it's caused by TypeMappingUtils#checkPhysicalLogicalTypeCompatible. It seems that the method doesn't consider the different physical and logical type validation logic of source and sink:   logical type should be able to cover the physical type in source, but physical type should be able to cover the logic type in sink vice verse. Besides, the decimal type should be taken more carefully, when target type is Legacy(Decimal), it should be able to accept any precision decimal type.




",,dwysakowicz,jark,libenchao,lzljs3620320,Terry1897,twalthr,wenlong.lwl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 05 18:06:36 UTC 2020,,,,,,,,,,"0|z0dxb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/20 03:17;Terry1897;cc [~ykt836][~jark][~dwysakowicz]Please have a look on this issue.;;;","22/Apr/20 03:38;Terry1897;I open a [https://github.com/apache/flink/pull/11848|https://github.com/apache/flink/pull/11848] to help understanding and solve this validation exception.;;;","22/Apr/20 05:34;jark;I think the root cause is that you are using the legacy type information which can't connect to planner smoothly. 
Because of the complexity, we don't plan to support new type system for UpsertStreamTableSink. But will fully support new system for FLIP-95 new sink interface. I think your problem will be solved once FLIP-95 is finished. 
;;;","22/Apr/20 06:18;Terry1897;[~jark]  I agree with you that new sink interface of FLIP-95 can work normally, but there still a lot of connector that use the old interface. It's harmless to support such compatibility and useful for users who can not migrate their connector in time, right?;;;","22/Apr/20 06:28;dwysakowicz;Hi [~Terry1897] First of all I do agree type system rework is not finished yet and has more rough edges than we would like. This is one of such cases.

I agree with [~jark] here. UpsertStreamTableSink does not support the new type system. Therefore it can support only the types that originate from the old type system. Therefore e.g. {{VARCHAR(10)}} or {{DECIMAL}} with precision are not supported in this case. As Jark has already said it should be fixed with FLIP-95.;;;","22/Apr/20 06:35;lzljs3620320;A work-around way is here: https://issues.apache.org/jira/browse/FLINK-15469

If this is not so hacky, maybe we can consider it.

CC: [~docete];;;","22/Apr/20 06:49;wenlong.lwl;hi, all, the ticket is trying to fix the bug of the validation of PhysicalDataType and LogicalDataType of TableSink only, I think it is much more clear and clean, worth to consider to fix. Currently the validation on sink reuses the validation on source while they should be different actually.;;;","22/Apr/20 07:02;Terry1897;[~lzljs3620320] There isn't much need to introduce a new interface in TableSink to solve this ticket. Just as [~wenlong.lwl] said, it's a validation bug causing connectors using old TableSink can not work normmaly.;;;","22/Apr/20 07:07;dwysakowicz;The validation does not distinguish validation of source and sink. It validates that a logical type can be mapped to a physical type. The proposed fix (https://github.com/apache/flink/pull/11848) breaks that. It treats a logical as physical type for sinks. 

The purpose of this class is to check that a schema from DDL (the logical schema) matches the physical schema of the source(the purpose of that schema is primarily to provide bridging classes). Those schemas must match exactly. That said I don't think this is a valid solution.;;;","22/Apr/20 07:08;Terry1897;Hi [~dwysakowicz] The types that originate form the old type system of varcahr(10)/decimal(22, 2)/timestamp(3) is String/legacy(Decimal)/timestamp(3) and should be able to accept corresponding logical type in ddl, which is also the [PR|https://github.com/apache/flink/pull/11848] aims to solve.;;;","22/Apr/20 07:13;dwysakowicz;That is incorrect. Old type system does not support {{varchar(10)/decimal(22, 2)}}. It should've failed earlier.

In the old type system we support only DECIMAL(38,18) and STRING(=VARCHAR(Long.MAX));;;","22/Apr/20 07:17;Terry1897;I think you may misunderstand my fix. Source and sink should be in two different validation logic. If the logical type defined in ddl can be consumed by physical type of table sink returned, why we must match those two schema exactly, and the check logic is a relaxed check :  https://github.com/apache/flink/blob/master/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/logical/utils/LogicalTypeCasts.java#L232. I don't know if I express clearly, let me know if u have more questions . [~dwysakowicz];;;","22/Apr/20 07:20;Terry1897;We can not forbid using old-style TableSink in sql, and the old-style TableSink can consume DECIMAL(38,18) and STRING(=VARCHAR(Long.MAX)) type. So it make sense to pass the check while logical type is varchar(10)/decimal(22,2);;;","22/Apr/20 07:26;wenlong.lwl;[~dwysakowicz] I think it is not necessary that the schema of logical schema and physical schema should be matched exactly:

Currently, we allow a column in source : logical type varchar(10), while pyshical type is varchar(5), see `CastAvoidanceChecker` used in the compatible check. 
The requirement on source is that: we need to be able convert a physical record of source to internal record according to physicalDataType and LogicalDataType. 

On sinks, the requirements should be reversed: we need to be able convert an internal record to a physical record for sink: so  we can allow a column of sink whose Logical type is varchar(5) but physical type is varchar(10).

On validation: we has an validation to make sure that the schema of source query of a sink match the logical type, the validation between logical type and physical can be much more loose I think.;;;","22/Apr/20 07:45;dwysakowicz;I must admit I have not fully understood the change in the beginning. I was not aware of introduction of the {{supportsAvoidingCast}} in that method before. Having understood that now I think it's fine to add the bidirectional check.

I am still skeptical about allowing any precision for {{DECIMAL}} type for legacy decimal type. Such conversions might require a cast.  Unless we discard our previous assumptions that {{LEGACY DECIMAL = DECIMAL(38,18)}} [~twalthr] [~jark];;;","22/Apr/20 08:54;wenlong.lwl;[~dwysakowicz] Regarding LEGACY DECIMAL, I think it is a special case good to support: the physical presentation  of LAGECY DECIMAL is BigDecimal, can support any precision and scale, so allow such conversion will not break anything actually and the final precision and scale of the output BigDecimal still limited by logical data type so no data with error precision will be generated. 

What's more, with such support we can easily fill up the support of decimal in all kinds of sink with old interface. ;;;","22/Apr/20 09:05;jark;I still think the root cause is FLINK-15469. And the proposed way in the PR is just a workaround. 

The purpose of {{TypeMappingUtils.checkPhysicalLogicalTypeCompatible}} should check the logical and physical types are compatible, i.e. types are equal but ignore field names. It is used to guarantee the sink connector complies with the DDL. If the connector wants to support precision, then it has to use new type system, not the legacy TypeInformation which loses the precision. 

Any precision mismatch in the logical and physical types should fail (except legacy decimal), so I think {{supportsAvoidingCast}} here is a mistake and should use something more strict. The {{supportsAvoidingCast}} should be used in checks for query schema to sink schema. ;;;","22/Apr/20 09:17;dwysakowicz;[~jark] What you are describing was the original purpose of this class. Introduction of the {{supportsAvoidingCast}} changed it a bit. Since the change it lets some of mismatches pass through. E.g. DDL can define VARCHAR(2) and Source can produce VARCHAR(1). Moreover if we say that the use of {{supportsAvoidingCast}} is valid here than the bug described by [~Terry1897] stands. For sink we should call the {{supportsAvoidingCast}} with reversed arguments.

I am not sure how we should proceed further with it.;;;","22/Apr/20 09:19;jark;[~dwysakowicz], Yes, my point is that the change from {{areTypeCompatible}} to {{supportsAvoidingCast}} changes the behavior of this class, and I think that's wrong.;;;","22/Apr/20 10:21;Terry1897;supportsAvoidingCast method behavior looks right to me, relaxed check will improve user experience of old-style connector and don't affect the correctness.;;;","22/Apr/20 11:15;twalthr;I'm also fine with the proposed PR. The bug around {{supportsAvoidingCast}} should definitely be fixed. And if it helps for legacy connectors, we can also be more lenient around decimal. However, we should not start relaxing new behavior in FLIP-95. Changing the precision and scale of a decimal type will result in completely different data. No user wants that and it is also very hard to debug. Luckily with FLIP-95, the sink must completely implement the schema of the catalog table. There is no possibility of having a mismatch because there is no `getProducedDataType()`.;;;","22/Apr/20 11:40;dwysakowicz;[~twalthr] raised a good point that this method will not be used for the new sources. 

In principle this should not be necessary, if we had a more consistent handling of supported types in DDL and sources/sinks. Nevertheless I think we can relax it here for now to make the old <> new type system integration a bit smoother. 

That said I am also fine with the suggested fix.

;;;","23/Apr/20 07:07;dwysakowicz;What is your opinion [~jark]? I do agree with what you said in principle. The relaxing proposed in the PR does not stretch the assumptions too much and mainly aims to improve support the old types, which truth be told did not care about any precision. Still it verifies that none of the mismatches require a cast. If you are ok with it I will take care of the PR.;;;","23/Apr/20 09:55;jark;[~dwysakowicz] Thanks for taking case of this PR. I'm also fine with the proposed PR. ;;;","27/Apr/20 09:26;dwysakowicz;Fixed in aed8c1957a706c35d824c598e2d610d897059c36;;;","05/May/20 18:06;dwysakowicz;Fixed in 1.10.1: ff7b4f6f0e073477b7a93dbda8243e7fc8647f50;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TPC-DS fail to run data generator,FLINK-17309,13300026,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,dwysakowicz,dwysakowicz,21/Apr/20 16:17,12/May/20 12:14,13/Jul/23 08:07,12/May/20 12:14,1.11.0,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,"{code}
[INFO] Download data generator success.
[INFO] 15:53:41 Generating TPC-DS qualification data, this need several minutes, please wait...
./dsdgen_linux: line 1: 500:: command not found
[FAIL] Test script contains errors.
{code}

https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7849&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5",,dwysakowicz,leonard,lzljs3620320,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 12:14:53 UTC 2020,,,,,,,,,,"0|z0dwm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/20 16:21;dwysakowicz;Another instance: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7850&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","22/Apr/20 01:56;lzljs3620320;CC: [~Leonard Xu];;;","22/Apr/20 02:06;leonard;it's maybe caused by scripts, Let me track this.  [~dwysakowicz] could you assign the ticket to me?;;;","22/Apr/20 03:57;leonard;I checked related scripts hasn't modified, and looked up some materials to find the root cause but not a reasonable answer. 

Hi  [~chesnay], could you help to answer the two questions?  I'm not familiar with Azure pipeline(many thanks).

(1) It seems these tests fails are random, Is the bash of Azuere's machine environment are same between pipelines? 

(2)`dsdgen_linux` need to be execute by root, so the scripts `chmod +x` before execution, Is there any possible the step may happen error when invoke `chmod +x` ?;;;","22/Apr/20 06:56;rmetzger;Regarding (1): Except for the e2e tests, all the compile and test jobs are executed in this docker container {{rmetzger/flink-ci:ubuntu-amd64-bcef226}}. I run the tests inside the container to make sure we always have the same environment, and we can reproduce issues locally.
(2) I guess you are talking about this line here: https://github.com/apache/flink/blob/master/flink-end-to-end-tests/flink-tpcds-test/tpcds-tool/data_generator.sh#L79
What you can do is execute the script (on Azure) with ""set -x"" to get debugging information. I don't think the chmod call fails. I think is is the {{./dsdgen_linux}} call, because the error message looks like this
{code}
./dsdgen_linux: line 1: 500:: command not found
{code}
;;;","22/Apr/20 07:21;rmetzger;The only explanation I can come up with is that the binary is corrupted, due to network issues?
We could print the md5sum of the binary before execution to rule that out? ;;;","22/Apr/20 07:29;leonard;[~rmetzger] 

Thanks for you reply, What I'm considering is similar  to you, I‘m using '-x' to debug and I guess the network issues two, I have an original idea to avoid that is checking the md5sum of binary before execution and retry download for several times.;;;","06/May/20 03:07;leonard;Hi, all 

I prepared the PR that add validation and retry logic when the md5sum of generator is not matched, I triggered many times in past days and only found one fail because network issue as we doubt[1].

And I checked all AZP [2]in past week(AZP number from 4.30.1 to 5.6.1) manually and found no TPC-DS failure comes from data generator.

I think it's time to confirm the PR is worked or not, I can polish the PR soon if we reach a consensus.

What do you think? [~rmetzger] [~dwysakowicz] [~lzljs3620320]

[1][https://github.com/apache/flink/pull/11867#issuecomment-618413808]

[2][https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=2&_a=summary] ;;;","07/May/20 05:49;rmetzger;Thanks a lot for validating the change. In my opinion, we can polish & merge the PR.;;;","07/May/20 10:01;leonard;Hi, [~dwysakowicz]

Could you help assign this ticket to me ? and I have updated the PR.;;;","07/May/20 13:37;dwysakowicz;I assigned this issue to you [~Leonard Xu];;;","12/May/20 12:14;rmetzger;Resolved in https://github.com/apache/flink/commit/79a7bbad457a7128b5e2f059c6fd167f3f4381ba

Thanks a lot!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionGraphCache cachedExecutionGraphs not cleanup cause OOM Bug,FLINK-17308,13300007,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,yujunyong,yujunyong,21/Apr/20 14:40,30/Apr/20 05:03,13/Jul/23 08:07,24/Apr/20 13:21,1.10.0,1.9.0,1.9.1,1.9.2,,1.10.1,1.11.0,1.9.4,,Runtime / REST,,,,,0,pull-request-available,,,,"class org.apache.flink.runtime.rest.handler.legacy.ExecutionGraphCache will cache job execution graph in field 

""cachedExecutionGraphs"" when call method 

""getExecutionGraph"", but never call it's 

cleanup method in flink. it's cause JobManager Out of Memory, When submit a lot of batch job and fetch these job's info. becasue these operation cache all these job execution graph and ""cleanup"" method never called",,libenchao,liyu,sunjincheng121,trohrmann,yujunyong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16399,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 05:02:30 UTC 2020,,,,,,,,,,"0|z0dwhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/20 07:33;trohrmann;Thanks for reporting this issue [~yujunyong]. This is indeed a mistake and should be corrected. I will prepare a PR for it.;;;","24/Apr/20 03:46;sunjincheng121;Reset the fix version as 1.9.3 was released.;;;","24/Apr/20 13:21;trohrmann;Fixed via

1.11.0:

bf0cfa606487c742a38cfccfee780a613894a8d8
3635b54c7b49ea48936ae4baff91a20a96e2f96c
08025ec5d59659adea517d3f363abedd00c4fe7c
851420014584fdfd91308937cd913917c11cf4bd

1.10.1:

138b3501a9143da8992bd35af04d43a97cadb31a
c4ec62f57884f27573026c72db9bc00e14329d0e
ba6b23bc13cbe391a04089856cc6a64f4555f4f9
74c29879810c64100534e7892b5edc6e03bdc2d9

1.9.4:

aa6966bfe48bcdf3d9705815643d047a13032dd2
eb753382ca247ce4a03e5eb70a3373eefefea4b6
d90ec148e5299bf5d4b1d1cf79eb453dccbb96c5
244ca6687e1cdfa6411333511991dee46553b696;;;","24/Apr/20 17:18;liyu;Changing the fix version to 1.10.2 since the first RC for 1.10.1 was built without this fix. Will change the fix version back if we will have a RC2. Or please let me know if this is a must-have in 1.10.1 [~trohrmann]. Thanks.;;;","27/Apr/20 07:17;trohrmann;It's fine to include this fix with 1.10.2.


;;;","30/Apr/20 05:02;liyu;Thanks for the confirmation [~trohrmann]. I've changed the fix version back to 1.10.1 since we will have the next RC.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManagerRunnerStartupTest.testStartupWhenNetworkStackFailsToInitialize fails on OSX,FLINK-17301,13299941,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,rmetzger,rmetzger,21/Apr/20 09:31,27/Apr/20 07:04,13/Jul/23 08:07,27/Apr/20 07:04,1.11.0,,,,,1.11.0,,,,Runtime / Coordination,Tests,,,,0,pull-request-available,,,,"{code}
[ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.841 s <<< FAILURE! - in org.apache.flink.runtime.taskexecutor.TaskManagerRunnerStartupTest
[ERROR] testStartupWhenNetworkStackFailsToInitialize(org.apache.flink.runtime.taskexecutor.TaskManagerRunnerStartupTest)  Time elapsed: 0.531 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:58)
	at org.apache.flink.runtime.taskexecutor.BackPressureSampleService.<init>(BackPressureSampleService.java:56)
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.createBackPressureSampleService(TaskManagerRunner.java:406)
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManager(TaskManagerRunner.java:398)
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunnerStartupTest.startTaskManager(TaskManagerRunnerStartupTest.java:176)
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunnerStartupTest.testStartupWhenNetworkStackFailsToInitialize(TaskManagerRunnerStartupTest.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}","mvn -version

Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)
Maven home: /usr/local/Cellar/maven/3.6.3_1/libexec
Java version: 1.8.0_192, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk1.8.0_192.jdk/Contents/Home/jre
Default locale: en_DE, platform encoding: UTF-8
OS name: ""mac os x"", version: ""10.15.3"", arch: ""x86_64"", family: ""mac""",aljoscha,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 07:04:50 UTC 2020,,,,,,,,,,"0|z0dw34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/20 07:04;trohrmann;Fixed via

cc593954e478c3a09d4e0a50480d09c131281261
af1d21fa831a6c9409abb637891d7d837764d065;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix not calling ResourceManager#closeTaskManagerConnection in KubernetesResourceManager in case of registered TaskExecutor failure,FLINK-17273,13299700,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,felixzheng,felixzheng,20/Apr/20 13:49,14/Jan/21 09:26,13/Jul/23 08:07,28/Aug/20 08:19,1.10.0,1.10.1,,,,1.12.0,,,,Deployment / Kubernetes,Runtime / Coordination,,,,0,pull-request-available,,,,"At the moment, the {{KubernetesResourceManager}} does not call the method of {{ResourceManager#closeTaskManagerConnection}} once it detects that a currently registered task executor has failed. This ticket propoeses to fix this problem.",,felixzheng,rmetzger,tison,trohrmann,wangyang0918,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 28 08:19:26 UTC 2020,,,,,,,,,,"0|z0dulk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/20 03:30;tison;[~trohrmann][~fly_in_gis]

This seems a fast fail path when TM(Pod) failed, which we did in YARN & Mesos code path. It would be better you also have a look.

;;;","21/Apr/20 04:20;felixzheng;cc [~xintongsong];;;","21/Apr/20 06:08;wangyang0918;[~felixzheng] Do you mean when a TaskManager pod crashed exceptionally, we should {{closeTaskManagerConnection}} before removing the pod in {{KubernetesResourceManager}}? If it is, i think it is a valid fix. Otherwise, we need to wait for the timeout.;;;","21/Apr/20 08:48;trohrmann;{{ResourceManager#closeTaskManagerConnection}} cleans up the registration state on the RM side and tries to notify the TM about the closed connection (this might succeed or not). Hence, I guess the K8s RM should also call this method whenever a TM or pod is signalled to have failed/disappeared.;;;","22/Apr/20 02:28;xtsong;I think this is a valid issue. +1 for fixing it.
IIUC, we can call {{ResourceManager#closeTaskManagerConnection}} in {{KubernetesResourceManager#internalStopPod}}?;;;","22/Apr/20 03:19;felixzheng;Thanks for your attention! [~xintongsong] [~trohrmann] [~fly_in_gis] Given the following call stack,
{quote}{{ResourceManager#releaseResource}}

   - {{KubernetesResourceManager#stopWorker}}

          - {{KubernetesResourceManager#internalStopPod}}

   - {{ResourceManager#closeTaskManagerConnection}}
{quote}
 

I think it's enough to explicitly call {{ResourceManager#closeTaskManagerConnection}} in {{KubernetesResourceManager#removePodIfTerminated}} for this issue.;;;","23/Apr/20 12:34;trohrmann;I think part of the problem why we missed to call this function is that {{ResourceManager}} does not enforce a certain control flow. I think it would be better if the {{ResourceManager}} offered some calls like {{notifyWorkerFailed}} which will trigger the failover behaviour controlled by the {{ResourceManager}} and not by the sub class. In order to make this work, I guess we should take a look at the overall architecture and think about what callbacks the {{ResourceManager}} would need in order to do its job. Then the {{ResourceManager}} should be responsible for reacting to failures and other signals and simply call the implementation specific callbacks (e.g. terminating a pod). In contrast to that, our current {{ResourceManager}} implementations handle most of the logic themselves which can lead to problems such as forgetting to call a method in order to not violate the contract.;;;","24/Apr/20 02:09;xtsong;+1 for revisiting the boundary between {{ResourceManager}} and its deployment specific implementations.
I think this would help deduplicating the worker lifecycle control flow across deployments. The RM implementations should only handles the minimum set of deployment specific API/behavior differences.;;;","24/Apr/20 04:13;felixzheng;Thanks a lot for the input [~trohrmann] [~xintongsong]. I agree that we need to revisit the boundary between {{ResourceManager}} and its deployment-specific implementations, especially for the worker lifecycle control flow; I will take a closer look at the overall architecture and get back to further discuss it with you.;;;","24/Apr/20 09:25;wangyang0918;+1 to rethink and make a clear boundary between {{ResourceManager}} and specific cluster deployment implementation. Some future developments will also benefit a lot from this.;;;","03/Jul/20 09:22;rmetzger;[~felixzheng] What's the status of this ticket?
I'm currently checking some old tickets, and I found that there's no updates here in the past weeks.;;;","04/Jul/20 08:32;felixzheng;[~rmetzger] Sorry that I am busy in the past two months. How about changing the fixed version to 1.12?;;;","05/Jul/20 09:54;rmetzger;no problem & done.;;;","25/Aug/20 08:49;xtsong;Hi [~felixzheng],

Is there any updates on this ticket?

I'm asking because, we are making good progress in revisiting the boundary between {{ResourceManager}} and its deployment specific implementations (FLINK-18620), so the previously discussed solution might no longer apply.

If you have not already started working on this, would you be ok with me taking over this ticket?;;;","26/Aug/20 08:46;felixzheng;Hi [~xintongsong], I do not work on this issue, go ahead!;;;","26/Aug/20 09:49;xtsong;Thanks [~felixzheng].;;;","28/Aug/20 08:19;xtsong;Fixed via
* master: 99ab2efa7e40f1ca97455e48134584904afd2da2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceReaderTestBase.testAddSplitToExistingFetcher deadlocks on Travis,FLINK-17268,13299667,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,becket_qin,trohrmann,trohrmann,20/Apr/20 12:13,08/Oct/21 09:48,13/Jul/23 08:07,27/Apr/20 01:14,1.11.0,,,,,1.11.0,,,,Connectors / Common,,,,,0,pull-request-available,test-stability,,,"The {{SourceReaderTestBase.testAddSplitToExistingFetcher}} deadlocks on Travis with the following stack trace:

{code}
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2020-04-20 11:40:52
Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):

""Attach Listener"" #16 daemon prio=9 os_prio=0 tid=0x00007f640046d000 nid=0x67b3 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""SourceFetcher"" #15 prio=5 os_prio=0 tid=0x00007f6400abb000 nid=0x647a waiting on condition [0x00007f63e6c4f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000008fba81e0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:104)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:85)
	at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:42)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""process reaper"" #11 daemon prio=10 os_prio=0 tid=0x00007f6400774000 nid=0x6469 waiting on condition [0x00007f63e718e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080054278> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$WorknputStream.readInt(DataInputStream.java:387)
	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
	at java.lang.Thread.run(Thread.java:748)

""Service Thread"" #8 daemon prio=9 os_prio=0 tid=0x00007f6400203000 nid=0x645a runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread1"" #7 daemon prio=9 os_prio=0 tid=0x00007f6400200000 nid=0x6458 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread0"" #6 daemon prio=9 os_prio=0 tid=0x00007f64001fd800 nid=0x6457 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Signal Dispatcher"" #5 daemon prio=9 os_prio=0 tid=0x00007f64001fb800 nid=0x6456 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x00007f64001fa000 nid=0x6454 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f64001c9800 nid=0x6450 in Object.wait() [0x00007f63e9032000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x0000000080088160> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
	- locked <0x0000000080088160> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)

""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f64001c5000 nid=0x644d in Object.wait() [0x00007f63e9133000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x0000000080088318> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
	- locked <0x0000000080088318> (a java.lang.ref.Reference$Lock)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

""main"" #1 prio=5 os_prio=0 tid=0x00007f640000b800 nid=0x643b runnable [0x00007f640835e000]
   java.lang.Thread.State: RUNNABLE
	at org.apache.flink.connector.base.source.reader.SourceReaderTestBase.testAddSplitToExistingFetcher(SourceReaderTestBase.java:83)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

""VM Thread"" os_prio=0 tid=0x00007f64001bb800 nid=0x644a runnable 

""Gang worker#0 (Parallel GC Threads)"" os_prio=0 tid=0x00007f6400020000 nid=0x643e runnable 

""Gang worker#1 (Parallel GC Threads)"" os_prio=0 tid=0x00007f6400022000 nid=0x643f runnable 

""G1 Main Concurrent Mark GC Thread"" os_prio=0 tid=0x00007f6400046000 nid=0x6443 runnable 

""Gang worker#0 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f6400047800 nid=0x6444 runnable 

""G1 Concurrent Refinement Thread#0"" os_prio=0 tid=0x00007f6400028000 nid
{code}

https://api.travis-ci.org/v3/job/677173224/log.txt",,aljoscha,becket_qin,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 01:14:07 UTC 2020,,,,,,,,,,"0|z0due8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/20 09:41;aljoscha;cc [~jqin];;;","21/Apr/20 18:32;becket_qin;It looks that there is a bug in the interruption handling when waking up the split fetcher. I will submit a patch tomorrow.;;;","25/Apr/20 13:29;trohrmann;Another instance: https://api.travis-ci.com/v3/job/323030543/log.txt;;;","27/Apr/20 01:14;becket_qin;Merged to master.

90a38e8292b629e3d91b55941df00568d60542de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WorkerResourceSpec is not serializable,FLINK-17266,13299656,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,gjy,gjy,20/Apr/20 11:36,21/Apr/20 07:16,13/Jul/23 08:07,21/Apr/20 07:16,1.11.0,,,,,1.11.0,,,,Deployment / Mesos,,,,,0,pull-request-available,,,,"{{MesosResourceManager}} cannot acquire new resources due to {{WorkerResourceSpec}} not being serializable.

{code}
Caused by: java.lang.Exception: Could not open output stream for state backend
        at org.apache.flink.runtime.zookeeper.filesystem.FileSystemStateStorageHelper.store(FileSystemStateStorageHelper.java:70) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.addAndLock(ZooKeeperStateHandleStore.java:136) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        at org.apache.flink.mesos.runtime.clusterframework.store.ZooKeeperMesosWorkerStore.putWorker(ZooKeeperMesosWorkerStore.java:216) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        at org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.startNewWorker(MesosResourceManager.java:441) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        ... 34 more
Caused by: java.io.NotSerializableException: org.apache.flink.runtime.resourcemanager.WorkerResourceSpec
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184) ~[?:1.8.0_242]
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) ~[?:1.8.0_242]
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) ~[?:1.8.0_242]
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) ~[?:1.8.0_242]
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) ~[?:1.8.0_242]
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) ~[?:1.8.0_242]
        at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:594) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        at org.apache.flink.runtime.zookeeper.filesystem.FileSystemStateStorageHelper.store(FileSystemStateStorageHelper.java:62) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.addAndLock(ZooKeeperStateHandleStore.java:136) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        at org.apache.flink.mesos.runtime.clusterframework.store.ZooKeeperMesosWorkerStore.putWorker(ZooKeeperMesosWorkerStore.java:216) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        at org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.startNewWorker(MesosResourceManager.java:441) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        ... 34 more
{code}",,felixzheng,gjy,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16437,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 07:16:52 UTC 2020,,,,,,,,,,"0|z0dubs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/20 07:16;trohrmann;Fixed via 036197e21f16769f937cde610bda0ddc502783e2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
taskmanager.sh could not work on Mac,FLINK-17264,13299627,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,wangyang0918,wangyang0918,20/Apr/20 10:02,21/Apr/20 07:11,13/Jul/23 08:07,21/Apr/20 07:11,,,,,,1.11.0,,,,Deployment / Scripts,,,,,0,pull-request-available,,,,"Start a taskmanager on Mac via {{taskmanager.sh}} will get the following error.
{code:java}
wangyang-pc:build-target danrtsey.wy$ ./bin/taskmanager.sh start-foreground [ERROR] Unexpected result ( 1 lines): BASH_JAVA_UTILS_EXEC_RESULT:-Xmx536870902 -Xms536870902 -XX:MaxDirectMemorySize=268435458 -XX:MaxMetaspaceSize=268435456 [ERROR] extractExecutionParams only accepts exactly one line as the input [ERROR] Could not get JVM parameters properly.
{code}
 

The root cause is FLINK-17023 introduce the following change and it could not work as expected.
{code:java}
local num_lines=$(echo ""$execution_config"" | wc -l)
{code}
On linux environment, the output is ""1"". However, on Mac, it is ""      1"". Maybe we need to add ""tr -d '[:space:]' after wc"".

 

cc [~TsReaper] ",,aljoscha,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17272,FLINK-17270,,,,,,,,,,,,,,,,,,,FLINK-17023,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 07:11:44 UTC 2020,,,,,,,,,,"0|z0du5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/20 10:13;chesnay;My god what kind of behavior is that? Adding the trim should solve the issue; [~fly_in_gis] will you open a PR?;;;","20/Apr/20 10:21;wangyang0918;[~chesnay] :), Mac always has some strange behaviors of scripts. I will attach a PR right now.;;;","20/Apr/20 13:57;aljoscha;The script uses string comparison instead of number comparison, if you use 
{code}
    if [[ ${num_lines} -ne 1 ]]; then
{code}

it works fine and you don't need to trim or anything.;;;","20/Apr/20 14:42;wangyang0918;[~aljoscha] Good suggestion. I think you means to {{-eq}}, not {{-ne}}. I will update the PR.;;;","21/Apr/20 07:11;trohrmann;Fixed via 3ae1da0bed0f365057af8deed3055eba2853484a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Statefun snapshot deployments broken,FLINK-17262,13299604,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tzulitai,chesnay,chesnay,20/Apr/20 08:09,20/Apr/20 09:05,13/Jul/23 08:07,20/Apr/20 09:05,statefun-2.0.0,statefun-2.1.0,,,,,,,,Release System,,,,,0,,,,,"https://builds.apache.org/job/flink-statefun-snapshot-deployment-2.0/12/

{code}
Commit message: ""[FLINK-17193] [python-k8s-example] Abort script on failure. Build SDK distribution if was not previously built""
 > git rev-list --no-walk d660668ff45312f7c3b10529b29b478efe220e57 # timeout=10
[flink-statefun-snapshot-deployment-2.0] $ /bin/bash /tmp/jenkins419419675999100733.sh
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100   341  100   341    0     0    914      0 --:--:-- --:--:-- --:--:--   914

gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now
./tools/snapshots/deploy_snapshot_jars.sh: line 47: mvn: command not found
{code}
Happened twice in a row.",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 20 09:05:22 UTC 2020,,,,,,,,,,"0|z0du08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/20 09:05;tzulitai;Thanks for reporting [~chesnay].

Snapshot deployments are fixed now:
* https://builds.apache.org/job/flink-statefun-snapshot-deployment-2.0/14/
* https://builds.apache.org/job/flink-statefun-snapshot-deployment/17/

Problem was Maven installation somehow stopped working in the Jenkins jobs.
Changed that to something that should always work.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Statefun docs broken,FLINK-17261,13299603,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tzulitai,chesnay,chesnay,20/Apr/20 08:06,20/Apr/20 08:47,13/Jul/23 08:07,20/Apr/20 08:47,statefun-2.0.0,statefun-2.1.0,,,,statefun-2.0.1,statefun-2.1.0,,,Documentation,,,,,0,,,,,"https://ci.apache.org/builders/flink-statefun-docs-master/builds/56

{code}  Liquid Exception: Liquid syntax error (line 67): Unknown tag 'higlight' in deployment-and-operations/packaging.md
Liquid syntax error (line 67): Unknown tag 'higlight'
/home/buildslave/slave/flink-statefun-docs-master/build/docs/.rubydeps/ruby/2.6.0/gems/liquid-4.0.3/lib/liquid/document.rb:23:in `unknown_tag'{code}",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 20 08:46:07 UTC 2020,,,,,,,,,,"0|z0du00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/20 08:17;tzulitai;cc [~sjwiesman]
seems to be caused by recent commit -
https://github.com/apache/flink-statefun/commit/3f4cddc58d6211aa0035a7c6b2ba3ddce186b17e#diff-4d66388f79ce9ddca88b6a4784d17ea6R74;;;","20/Apr/20 08:46;tzulitai;Fixed.

* statefun/master - 8c48701b39d9819d37deefb9e621a148c5db0f93
* statefun/release-2.0 - e4870c7e924dc2bd27116efe898b141dcc6af668;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingKafkaITCase failure on Azure,FLINK-17260,13299602,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,roman,roman,20/Apr/20 08:00,15/Sep/20 15:39,13/Jul/23 08:07,15/Sep/20 15:39,1.11.0,1.12.0,,,,1.12.0,,,,Connectors / Kafka,Tests,,,,0,pull-request-available,test-stability,,,"[https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/7544/logs/165]

 
{code:java}
2020-04-16T00:12:32.2848429Z [INFO] Running org.apache.flink.tests.util.kafka.StreamingKafkaITCase
2020-04-16T00:14:47.9100927Z [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 135.621 s <<< FAILURE! - in org.apache.flink.tests.util.k afka.StreamingKafkaITCase
2020-04-16T00:14:47.9103036Z [ERROR] testKafka[0: kafka-version:0.10.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 46.222 s  <<<  FAILURE!
2020-04-16T00:14:47.9104033Z java.lang.AssertionError: expected:<[elephant,27,64213]> but was:<[]>
2020-04-16T00:14:47.9104638Z    at org.junit.Assert.fail(Assert.java:88)
2020-04-16T00:14:47.9105148Z    at org.junit.Assert.failNotEquals(Assert.java:834)
2020-04-16T00:14:47.9105701Z    at org.junit.Assert.assertEquals(Assert.java:118)
2020-04-16T00:14:47.9106239Z    at org.junit.Assert.assertEquals(Assert.java:144)
2020-04-16T00:14:47.9107177Z    at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:162)
2020-04-16T00:14:47.9107845Z    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-04-16T00:14:47.9108434Z    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-04-16T00:14:47.9109318Z    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-04-16T00:14:47.9109914Z    at java.lang.reflect.Method.invoke(Method.java:498)
2020-04-16T00:14:47.9110434Z    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-04-16T00:14:47.9110985Z    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-04-16T00:14:47.9111548Z    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-04-16T00:14:47.9112083Z    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-04-16T00:14:47.9112629Z    at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-16T00:14:47.9113145Z    at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-16T00:14:47.9113637Z    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-04-16T00:14:47.9114072Z    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-04-16T00:14:47.9114490Z    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-04-16T00:14:47.9115256Z    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-04-16T00:14:47.9115791Z    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-04-16T00:14:47.9116292Z    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-04-16T00:14:47.9116736Z    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-04-16T00:14:47.9117779Z    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-04-16T00:14:47.9118274Z    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-04-16T00:14:47.9118766Z    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-04-16T00:14:47.9119204Z    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-04-16T00:14:47.9119625Z    at org.junit.runners.Suite.runChild(Suite.java:128)
2020-04-16T00:14:47.9120005Z    at org.junit.runners.Suite.runChild(Suite.java:27)
2020-04-16T00:14:47.9120428Z    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-04-16T00:14:47.9120876Z    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-04-16T00:14:47.9121350Z    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-04-16T00:14:47.9121805Z    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-04-16T00:14:47.9122273Z    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-04-16T00:14:47.9122729Z    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-04-16T00:14:47.9123130Z    at org.junit.runners.Suite.runChild(Suite.java:128)
...
2020-04-16T00:14:47.9132530Z
2020-04-16T00:14:47.9134982Z [INFO] Running org.apache.flink.tests.util.kafka.SQLClientKafkaITCase
2020-04-16T00:17:18.7332734Z [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 150.813 s - in org.apache.flink.tests.util.kafka.SQLClient KafkaITCase
2020-04-16T00:17:19.0840872Z [INFO]
2020-04-16T00:17:19.0841522Z [INFO] Results:
2020-04-16T00:17:19.0841820Z [INFO]
2020-04-16T00:17:19.0842133Z [ERROR] Failures:
2020-04-16T00:17:19.0842565Z [ERROR]   StreamingKafkaITCase.testKafka:162 expected:<[elephant,27,64213]> but was:<[]>
2020-04-16T00:17:19.0842993Z [INFO]
2020-04-16T00:17:19.0843766Z [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0
{code}
 

 Occured first while implementing FLINK-17156 (wasn't able to reproduce).",,aljoscha,alpinegizmo,becket_qin,dian.fu,godfreyhe,jark,libenchao,lzljs3620320,rmetzger,roman,sewen,trohrmann,twalthr,wanglijie,zhuzh,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18294,FLINK-18301,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 15:39:05 UTC 2020,,,,,,,,,,"0|z0dtzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/20 09:39;trohrmann;Another instance: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7703&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

It failed with

{code}
expected:<[elephant,27,64213]> but was:<[elephant,41,64213]>
{code};;;","20/Apr/20 09:40;trohrmann;I think FLINK-17156 might actually not be related. If you agree [~roman_khachatryan], then please delete the link.;;;","20/Apr/20 09:45;roman;Yes, I think they are not unrelated.

Deleted the link.;;;","30/Apr/20 06:53;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=443&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","13/May/20 07:36;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1083&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","19/May/20 13:24;trohrmann;Another instance: https://dev.azure.com/tillrohrmann/flink/_build/results?buildId=127&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=2f5b54d0-1d28-5b01-d344-aa50ffe0cdf8;;;","19/May/20 19:35;rmetzger;another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1809&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","26/May/20 19:06;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8084&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=2f5b54d0-1d28-5b01-d344-aa50ffe0cdf8;;;","27/May/20 06:46;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2231&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334;;;","27/May/20 08:26;lzljs3620320;another instance: [https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/2190/logs/133];;;","27/May/20 13:44;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2263&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","29/May/20 06:04;pnowojski;another one:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2404&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","02/Jun/20 19:25;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2555&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5&l=16946;;;","03/Jun/20 05:42;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2586&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334;;;","04/Jun/20 06:50;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2669&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","04/Jun/20 07:33;aljoscha;I think this is two separate issues, for some failures we don't get any data, while for some we get wrong results. For example, the last one says:{code}java.lang.AssertionError: expected:<[elephant,27,64213]> but was:<[elephant,41,64213]>{code}
if this were an actual bug it would indicate that our exactly-once is somehow broken.

[~chesnay] could it be that the tests for the different versions clash with each other? That could explain the issue of seeing wrong results.;;;","05/Jun/20 11:19;godfreyhe;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2673&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","05/Jun/20 13:50;aljoscha;master:
 - 455fb243486682fded9c136e4e2f4a8551d9a962
 - ce7b41e5173d45beebf461a7dcb74dd868807d6a

release-1.11: 
 - 31c3a15731e91d01fb033fe5a8f8173e3ba0cb38
 - e4034ac296d4e8b43af570d3436cd2dec33d9ef1

This potentially fixes the instability, please re-open if the problem persists.;;;","05/Jun/20 18:16;rmetzger;failed again on master (including your commit): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2834&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee;;;","08/Jun/20 06:27;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2861&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334;;;","08/Jun/20 06:29;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2860&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","08/Jun/20 11:34;rmetzger;{{expected:<[elephant,27,64213]> but was:<[]>}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2909&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","08/Jun/20 17:05;aljoscha;I pushed 50b6d9d5ffbcfbcc20c843e8758a0d2326b44ee1 to master to help with debugging. Please keep reporting failures.;;;","09/Jun/20 01:44;godfreyhe;another one:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2961&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","09/Jun/20 06:41;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2947&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","09/Jun/20 08:10;wanglijie;+1
{code:java}
expected:<[elephant,27,64213]> but was:<[elephant,27,64213, elephant,40,64213]>
{code}
 [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2962&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5];;;","09/Jun/20 09:43;zjwang;another instance [https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/3003/logs/143];;;","10/Jun/20 06:13;rmetzger;{code}
StreamingKafkaITCase.testKafka:166 Messages from Kafka 0.10.2.0: [elephant,41,64213, bee,31,65647, giraffe,9,65555, squirrel,86,66413] expected:<[elephant,27,64213]> but was:<[elephant,41,64213]>
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3047&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","10/Jun/20 09:29;aljoscha;Nice! So it seems that we double count the first set of messages. What the test does is:
# create input/output topics
# start job
# inject messages
# increase partitions from 1 to 2 on input topic
# inject more messages
# verify result

The test doesn't inject failures or anything, so it could be that there really is sth wrong in the Kafka connector.;;;","10/Jun/20 09:53;aljoscha;Looking through the logs, it also seems to only occur for Kafka 0.10.x;;;","10/Jun/20 10:02;chesnay;Current theory is that this is caused by FLINK-16389, where we bumped the 0.10 version. Will try reverting the commit and loop it on CI to confirm.;;;","11/Jun/20 07:29;zhuzh;another instance:
https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/3230/logs/137;;;","11/Jun/20 12:11;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3259&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","12/Jun/20 10:59;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3371&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","15/Jun/20 08:58;aljoscha;That last one is for {{SQLClientKafkaITCase}};;;","15/Jun/20 09:00;chesnay;I believe that some other test is interfering. I ran the test over 600 times on CI without it failing.
It seems more likely that some other test is interfering; hence I'm downgrading the priority and closing FLINK-18020 as a duplicate.

I still couldn't get the test to fail, but there is sometimes a leftover kafka process before the java e2e tests are started. There was also a case where we used 13gb more disk space (52%->67% usage); if there is a more extreme case this could maybe also interfere. In short, I don't think the test is a problem, nor kafka for that matter.

I've opened FLINK-18294 to log java processes and disk space usage, which may allow us to narrow down the interfering tests.;;;","15/Jun/20 19:02;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3490&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5
{code}
[ERROR]   StreamingKafkaITCase.testKafka:166 Messages from Kafka 0.10.2.0: [giraffe,9,65555, squirrel,86,66413, elephant,41,64213, bee,31,65647] expected:<[elephant,27,64213]> but was:<[elephant,41,64213]>
{code};;;","17/Jun/20 05:13;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3638&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{noformat}
[ERROR]   SQLClientKafkaITCase.testKafka:177->checkCsvResultFile:246 
Expected: [""2018-03-12 08:00:00.000,Alice,This was a warning.,2,Success constant folding."", ""2018-03-12 09:00:00.000,Bob,This was another warning.,1,Success constant folding."", ""2018-03-12 09:00:00.000,Steve,This was another info.,2,Success constant folding."", ""2018-03-12 09:00:00.000,Alice,This was a info.,1,Success constant folding.""] in any order
     but: Not matched: ""2018-03-12 08:00:00.000,Alice,This was a warning.,6,Success constant folding.""

{noformat}
;;;","17/Jun/20 07:54;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3630&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","17/Jun/20 09:02;chesnay;hmm. In the last run that  [~pnowojski] reported, there was still plenty disk-space available. We have 2 leaked task executors, but those shouldn't cause a problem.
So we're back to square one.

The next time the test fails we should have the kafka logs with FLINK-18301, maybe they contain a hint.;;;","17/Jun/20 09:09;chesnay;In one of the TaskExecutor logs we can see this line:
{code}
2020-06-16 22:03:26,594 WARN  org.apache.flink.kafka011.shaded.org.apache.kafka.clients.NetworkClient [] - Error while fetching metadata with correlation id 1 : {test-avro=LEADER_NOT_AVAILABLE}
{code}
There isn't any debugging info unfortunately, see FLINK-18343.;;;","17/Jun/20 13:13;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3687&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5


{code:java}
[ERROR] Failures: 
[ERROR]   StreamingKafkaITCase.testKafka:166 expected:<[elephant,27,64213]> but was:<[elephant,41,64213]>

{code}
;;;","22/Jul/20 13:40;rmetzger;{code}
[ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 132.418 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.StreamingKafkaITCase
[ERROR] testKafka[0: kafka-version:0.10.2.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 43.356 s  <<< FAILURE!
java.lang.AssertionError: Messages from Kafka 0.10.2.2: [elephant,41,64213, giraffe,9,65555, bee,31,65647, squirrel,86,66413] expected:<[elephant,27,64213]> but was:<[elephant,41,64213]>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:166)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4722&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","23/Jul/20 02:43;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4743&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=7f606211-1454-543c-70ab-c7a028a1ce8c]
{code}
2020-07-22T23:58:46.0860584Z [ERROR] testKafka[0: kafka-version:0.10.2.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 46.849 s <<< FAILURE! 2020-07-22T23:58:46.0861582Z java.lang.AssertionError: Messages from Kafka 0.10.2.2: [elephant,41,64213, giraffe,9,65555, bee,31,65647, squirrel,86,66413] expected:<[elephant,27,64213]> but was:<[elephant,41,64213]>
{code};;;","27/Jul/20 13:01;chesnay;I couldn't find anything obvious in the Kafka logs that hints at a problem; so we're somewhat back to square one.

It is rather curious that the failure cases are consistent; the unexpected failure is always the same value.

[~jqin] Do you maybe have an idea what the problem could be?;;;","28/Jul/20 10:39;becket_qin;I don't have a clue either. From the phenomenon itself, a wild guess is the aggregation value 14 is somehow doubled.
 * The expected final result 27 comes from the aggregation of three events: 5, 9, 13.
 * The error always have a 41, which is 14 greater than the expected result.
 * There are two possibilities for this to happen
 ** There are duplicate messages in Kafka.
 ** The state got wrong.
 * If there are duplicate messages, then 5 and 9 must both be duplicate. In that case, the output sequence should be 14, 19, 28, 41. So there should also be 19 and 28 before 41 is emitted.
 * So mathematically speaking, it seems that the aggregated value of 14 is somehow doubled.

I'll try to reproduce this locally with some logging added.;;;","29/Jul/20 07:49;pnowojski;Another instance:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4972&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
{noformat}
[ERROR] testKafka[0: kafka-version:0.10.2.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 44.769 s  <<< FAILURE!
java.lang.AssertionError: Messages from Kafka 0.10.2.2: [elephant,41,64213, giraffe,9,65555, bee,31,65647, squirrel,86,66413] expected:<[elephant,27,64213]> but was:<[elephant,41,64213]>
{noformat}
;;;","11/Aug/20 14:56;sewen;Just to double check: Is this something that is strictly occurring only with Kafka 0.10.x ?

I think it is probably time to drop that version anyways, so if that is the only affected version, we may not have an issue.

Also: Wasn't 0.10 before the ""idempotent producer"" ? Isn't it inherently possible for Kafka to have duplicates through producer-internal re-tries in 0.10 in some situations?

;;;","24/Aug/20 13:57;aljoscha;I believe you're right: KAFKA-4815. We should remove testing for the 0.10 producer, or maybe remove that producer altogether.;;;","01/Sep/20 11:41;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6041&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","11/Sep/20 08:00;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6451&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=7f606211-1454-543c-70ab-c7a028a1ce8c;;;","15/Sep/20 15:39;aljoscha;Fixed via FLINK-19152, which removes the Kafka 0.10 connector.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractYarnClusterTest does not compile with Hadoop 2.10,FLINK-17257,13299597,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,xtsong,xtsong,20/Apr/20 07:44,20/Apr/20 13:15,13/Jul/23 08:07,20/Apr/20 13:15,1.11.0,,,,,1.11.0,,,,Deployment / YARN,Tests,,,,0,pull-request-available,,,,"In {{AbstractYarnClusterTest}}, we create {{ApplicationReport}} with the static method {{ApplicationReport.newInstance}}, which is annotated as private and unstable. This method is no longer compatible in Hadoop 2.10.

As a workaround, we can create {{ApplicationReport}} with its default constructor and set only the fields that we need.",,xtsong,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 20 13:15:18 UTC 2020,,,,,,,,,,"0|z0dtyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/20 13:15;zhuzh;Fixed via fe78c9b41333996c80c6ef8a65d1351a6d24af7e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support writing to viewfs for hadoop versions < 2.7 when using BulkFormatBuilder in StreamingFileSink,FLINK-17253,13299569,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,spurthic,spurthic,20/Apr/20 04:10,04/Aug/20 13:27,13/Jul/23 08:07,04/Aug/20 13:27,1.8.0,1.8.1,1.8.2,1.8.3,1.9.0,1.12.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,"FLINK-14170 introduced Hadoop version check to support older hadoop versions. However the check only included ""hdfs"" scheme but not ""viewfs"". We are using StreamingFileSink to write data to our federated hadoop cluster with cdh-2.6 hadoop version and we are hit with
{code:java}
java.lang.UnsupportedOperationException: Recoverable writers on Hadoop are only supported for HDFS and for Hadoop version 2.7 or newer at org.apache.flink.runtime.fs.hdfs.HadoopRecoverableWriter.<init>(HadoopRecoverableWriter.java:61) at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.createRecoverableWriter(HadoopFileSystem.java:202) at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.createRecoverableWriter(SafetyNetWrapperFileSystem.java:69) at org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.<init>(Buckets.java:112) at org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink$BulkFormatBuilder.createBuckets(StreamingFileSink.java:317) at org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.initializeState(StreamingFileSink.java:327) at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:178) at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:160) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:281) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:901) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:415) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530) at java.lang.Thread.run(Thread.java:748)
{code}
The change is remove version check when the scheme is viewfs",,felixzheng,kkl0u,spurthic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 04 13:27:19 UTC 2020,,,,,,,,,,"0|z0dtso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/20 06:38;spurthic;Here is the PR that we tested on our cluster and now its actually writing to our cluster.

[https://github.com/apache/flink/pull/11815];;;","04/Aug/20 13:27;kkl0u;Merged on master with e3e9d69eec2a86b45914ded0e76ae1ed53b427c0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix incorrect returned address of Endpoint for the ClusterIP Service,FLINK-17230,13299352,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,felixzheng,felixzheng,felixzheng,18/Apr/20 09:32,16/Oct/20 10:54,13/Jul/23 08:07,01/Jun/20 07:58,1.10.0,1.10.1,,,,1.11.0,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"At the moment, when the type of the external Service is set to {{ClusterIP}}, we return an incorrect address {{KubernetesUtils.getInternalServiceName(clusterId) + ""."" + nameSpace}} for the Endpoint.

This ticket aims to fix this bug by returning {{KubernetesUtils.getRestServiceName(clusterId) + ""."" + nameSpace}} instead.",,felixzheng,LiuZeshan,pnowojski,tison,wangyang0918,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 07:58:37 UTC 2020,,,,,,,,,,"0|z0dsh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/20 17:26;tison;[~zjwang] [~pnowojski] this issue seems like a bugfix which could be included in 1.11.0. [~fly_in_gis] & I have reviewed the PR and I merged it into current master(1.12).

[~felixzheng] [~fly_in_gis]  if you think it is nice to be included in 1.11.0, please explain to our release manage, i.e., [~zjwang] & [~pnowojski].;;;","26/May/20 02:14;wangyang0918;This is a bugfix for wrongly returning a rest endpoint when the service exposed type is {{ClusterIP}}. I think it needs to be picked to branch 1.11. 

[~zjwang] Do you have concerns to integrate this PR to branch 1.11?;;;","26/May/20 02:33;felixzheng;[~zjwang] The scope of this problem is that the end-users would get the wrong Endpoint in HA setups thus can't talk to the JobManager from client-side via that retrieved Endpoint. Hence, it would be nice that we backport this PR to 1.11.;;;","28/May/20 11:33;wangyang0918;[~felixzheng] Could you open a PR to back this fix to 1.11?;;;","28/May/20 11:34;felixzheng;> [~felixzheng] Could you open a PR to back this fix to 1.11?

[~zjwang] WDYT?;;;","28/May/20 15:08;zjwang;I think it makes sense to cover it in release-1.11. Could you change the type field as bug and make the priority as blocker?;;;","29/May/20 01:14;felixzheng;> I think it makes sense to cover it in release-1.11. Could you change the type field as bug and make the priority as blocker?

Thanks, [~zjwang] [~fly_in_gis]! Updated.;;;","01/Jun/20 07:58;pnowojski;merged commit 9737f50 into apache:release-1.11 and b19058a110 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System.IO.IOException: No space left on device in misc profile on free Azure builders,FLINK-17223,13299145,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,17/Apr/20 14:00,19/Apr/20 08:48,13/Jul/23 08:07,19/Apr/20 08:48,1.11.0,,,,,1.11.0,,,,Build System / Azure Pipelines,,,,,0,pull-request-available,,,,"Builds on the free Azure builders are failing with

{code}
##[error]Unhandled exception. System.IO.IOException: No space left on device
   at System.IO.FileStream.WriteNative(ReadOnlySpan`1 source)
   at System.IO.FileStream.FlushWriteBuffer()
   at System.IO.FileStream.Flush(Boolean flushToDisk)
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at System.Diagnostics.TraceSource.Flush()
   at Microsoft.VisualStudio.Services.Agent.TraceManager.Dispose(Boolean disposing)
   at Microsoft.VisualStudio.Services.Agent.TraceManager.Dispose()
   at Microsoft.VisualStudio.Services.Agent.HostContext.Dispose(Boolean disposing)
   at Microsoft.VisualStudio.Services.Agent.HostContext.Dispose()
   at Microsoft.VisualStudio.Services.Agent.Worker.Program.Main(String[] args)
Error reported in diagnostic logs. Please examine the log for more details.
    - /home/vsts/agents/2.165.2/_diag/Worker_20200414-093250-utc.log
System.IO.IOException: No space left on device
   at System.IO.FileStream.WriteNative(ReadOnlySpan`1 source)
   at System.IO.FileStream.FlushWriteBuffer()
   at System.IO.FileStream.Flush(Boolean flushToDisk)
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id)
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message)
   at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message)
   at Microsoft.VisualStudio.Services.Agent.Worker.Worker.RunAsync(String pipeIn, String pipeOut)
   at Microsoft.VisualStudio.Services.Agent.Worker.Program.MainAsync(IHostContext context, String[] args)
System.IO.IOException: No space left on device
   at System.IO.FileStream.WriteNative(ReadOnlySpan`1 source)
   at System.IO.FileStream.FlushWriteBuffer()
   at System.IO.FileStream.Flush(Boolean flushToDisk)
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id)
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message)
   at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message)
   at Microsoft.VisualStudio.Services.Agent.Tracing.Error(Exception exception)
   at Microsoft.VisualStudio.Services.Agent.Worker.Program.MainAsync(IHostContext context, String[] args)
,##[error]The job running on agent Azure Pipelines 9 ran longer than the maximum time of 240 minutes. For more information, see https://go.microsoft.com/fwlink/?linkid=2077134
,##[warning]Agent Azure Pipelines 9 did not respond to a cancelation request with 00:01:00.
{code}

CI run: https://dev.azure.com/chesnay/flink/_build/results?buildId=205&view=logs&j=764762df-f65b-572b-3d5c-65518c777be4",,aljoscha,rmetzger,roman,,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11806: [FLINK-17223][AZP] Clean up disk space on Azure-hosted builders
URL: https://github.com/apache/flink/pull/11806
 
 
   ## What is the purpose of the change
   
   Due to a recent change in the machines provided by Azure, the available disk space has reduced on these machines (https://github.com/actions/virtual-environments/issues/709).
   This has caused some of our build profiles to run out of disk space.
   
   ## Brief change log
   
   - Add a conditional step, executed on the host (not within the container) that cleans up the disk space on azure provided machines.
   
   ## Verifying this change
   
   This change has been tested here: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=302&view=results
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Apr/20 06:08;githubbot;600","rmetzger commented on pull request #11806: [FLINK-17223][AZP] Clean up disk space on Azure-hosted builders
URL: https://github.com/apache/flink/pull/11806
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Apr/20 08:47;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17068,FLINK-17220,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 19 08:48:46 UTC 2020,,,,,,,,,,"0|z0drjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/20 14:02;rmetzger;Another case: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=295&view=logs&j=764762df-f65b-572b-3d5c-65518c777be4;;;","17/Apr/20 14:50;rmetzger;These failures in the python build are also caused by disk space issues: https://dev.azure.com/aljoschakrettek/Flink/_build/results?buildId=88&view=logs&j=584fa981-f71a-5840-1c49-f800c954fe4b&t=eaab0b22-c3d4-542d-ab82-c5217bdef86d

Related issues: FLINK-17068 FLINK-17220;;;","17/Apr/20 17:45;rmetzger;The reduction in disk space is a ""known issue"" that won't be fixed: https://github.com/actions/virtual-environments/issues/709

I will look into freeing up disk space;;;","19/Apr/20 08:48;rmetzger;Resolved in https://github.com/apache/flink/commit/cb8ae2892c37dd37431b1b56f96805a3dee0335d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docker entrypoint scripts may print credentials,FLINK-17214,13299029,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,17/Apr/20 09:32,24/Apr/20 07:41,13/Jul/23 08:07,24/Apr/20 07:41,,,,,,docker-1.10.1.0,docker-1.11.0.0,docker-1.9.3.0,,,,,,,0,pull-request-available,,,,"The docker entrypoint scripts run
{code}
echo ""config file: "" && grep '^[^\n#]' ""${CONF_FILE}""}}
{code}
which prints the entire configuration.

This may leak credentials set in the configuration.",,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #13: [FLINK-17214] Do not print configuration 
URL: https://github.com/apache/flink-docker/pull/13
 
 
   No longer log the entire configuration since it may leak configured credentials. It also should be redundant since all Flink processes are logging a sanitized configuration on startup.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/20 11:10;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 08:02:22 UTC 2020,,,,,,,,,,"0|z0dqtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/20 08:02;chesnay;Fix versions are a temporary crutch; can't find any reference versions right now.

docker-master: e0107b0f85d9b3630db39568729e36408cdc7b78;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some tests in flink-sql-parser module are disabled,FLINK-17211,13299008,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,17/Apr/20 08:21,17/Apr/20 14:39,13/Jul/23 08:07,17/Apr/20 14:39,,,,,,1.11.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,Updating the calcite version 1.22 introduced junit 5 to flink-sql-parser module. JUnit 5 requires additional dependency {{junit-vintage-engine}} to run JUNIT 4 tests. This dependency was not added.,,dwysakowicz,,,,,,,,,,,,,,,,,,,,,"dawidwys commented on pull request #11789: [FLINK-17211] Enable JUnit 4 tests in flink-sql-parser module
URL: https://github.com/apache/flink/pull/11789
 
 
   ## What is the purpose of the change
   
   Added junit 4 vintage engine to run junit 4 tests in junit 5 environment.
   
   ## Verifying this change
   
   Execute `mvn clean install` in `flink-sql-parser` module. Check these tests are executed:
   * org.apache.flink.sql.parser.TableApiIdentifierParsingTest
   * org.apache.flink.sql.parser.FlinkDDLDataTypeTest
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**yes** / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/20 08:36;githubbot;600","dawidwys commented on pull request #11789: [FLINK-17211] Enable JUnit 4 tests in flink-sql-parser module
URL: https://github.com/apache/flink/pull/11789
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/20 14:39;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,FLINK-14338,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 14:39:45 UTC 2020,,,,,,,,,,"0|z0dqow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/20 14:39;dwysakowicz;Fixed in 04337dd94d582033db9729a1888353480a1a3028;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_ci Python fails with unable to execute 'gcc': No such file or directory,FLINK-17205,13298982,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,pnowojski,pnowojski,17/Apr/20 06:29,17/Apr/20 14:33,13/Jul/23 08:07,17/Apr/20 14:33,,,,,,1.11.0,,,,API / Python,,,,,0,test-stability,,,,"
{code:java}
gcc -pthread -B /__w/3/s/flink-python/dev/.conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/__w/3/s/flink-python/dev/.conda/include/python3.7m -c /tmp/pip-install-ivtamal_/cython/Cython/Plex/Scanners.c -o build/temp.linux-x86_64-3.7/tmp/pip-install-ivtamal_/cython/Cython/Plex/Scanners.o
    unable to execute 'gcc': No such file or directory
    error: command 'gcc' failed with exit status 1
    
    ----------------------------------------
Command ""/__w/3/s/flink-python/.tox/py37-cython/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-ivtamal_/cython/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-vj0b5qrd/install-record.txt --single-version-externally-managed --compile --install-headers /__w/3/s/flink-python/.tox/py37-cython/include/site/python3.7/cython"" failed with error code 1 in /tmp/pip-install-ivtamal_/cython/
You are using pip version 10.0.1, however version 20.0.2 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.

=================================== log end ====================================
ERROR: could not install deps [pytest, apache-beam==2.19.0, cython==0.28.1, grpcio>=1.17.0,<=1.26.0, grpcio-tools>=1.3.5,<=1.14.2]; v = InvocationError(""/__w/3/s/flink-python/dev/install_command.sh pytest apache-beam==2.19.0 cython==0.28.1 'grpcio>=1.17.0,<=1.26.0' 'grpcio-tools>=1.3.5,<=1.14.2'"", 1)
___________________________________ summary ____________________________________
ERROR:   py35-cython: InvocationError for command /__w/3/s/flink-python/dev/install_command.sh --exists-action w .tox/.tmp/package/1/apache-flink-1.11.dev0.zip (exited with code 1)
ERROR:   py36-cython: InvocationError for command /__w/3/s/flink-python/dev/install_command.sh --exists-action w .tox/.tmp/package/1/apache-flink-1.11.dev0.zip (exited with code 1)
ERROR:   py37-cython: could not install deps [pytest, apache-beam==2.19.0, cython==0.28.1, grpcio>=1.17.0,<=1.26.0, grpcio-tools>=1.3.5,<=1.14.2]; v = InvocationError(""/__w/3/s/flink-python/dev/install_command.sh pytest apache-beam==2.19.0 cython==0.28.1 'grpcio>=1.17.0,<=1.26.0' 'grpcio-tools>=1.3.5,<=1.14.2'"", 1)

{code}

https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7609&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=14487301-07d2-5d56-5690-6dfab9ffd4d9",,dian.fu,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17119,FLINK-17118,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 14:33:13 UTC 2020,,,,,,,,,,"0|z0dqj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/20 06:36;rmetzger;Do we need to add ""gcc"" to the underlying docker container?;;;","17/Apr/20 06:36;pnowojski;another instances:
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7603&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=14487301-07d2-5d56-5690-6dfab9ffd4d9
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7595&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=14487301-07d2-5d56-5690-6dfab9ffd4d9;;;","17/Apr/20 06:37;pnowojski;Resolved by reverting 5eaf2b50a6 FLINK-17119 (as c201e1dc91 on master) 

The failure was visible in the PR https://github.com/apache/flink/pull/11767;;;","17/Apr/20 06:47;dian.fu;Sorry about this. Will look into it ASAP.;;;","17/Apr/20 14:17;pnowojski;The issue still persists https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7638&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=14487301-07d2-5d56-5690-6dfab9ffd4d9;;;","17/Apr/20 14:24;dian.fu;[~pnowojski] The reason is that gcc is not installed in the azure docker containers. [~hxbks2ks] is working on that now. At the same time, we will disable the cython tests for now. There is already a PR in https://github.com/apache/flink/pull/11790 for that. Will merge it once the tests passed.;;;","17/Apr/20 14:33;pnowojski;Fixed by reverting also 1a5b35b1e1ea79a233cacc88e4574f446aba52ae (as  9f113e4706f4110142c8aad74912e21bb995e497 on master);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
k8s-python-example build-example.sh improvements,FLINK-17193,13298892,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,aroch,aroch,aroch,16/Apr/20 19:18,16/Apr/20 19:48,13/Jul/23 08:07,16/Apr/20 19:48,,,,,,statefun-2.0.1,statefun-2.1.0,,,Stateful Functions,,,,,0,pull-request-available,,,,"* The helm template is printed out instead of being redirected to the resource YAML file.

 * In case the SDK distribution was not previously built - instead of failing the build, build the distribution and continue.

 * Abort script on failure. On failure the script would continue to run all steps and finish in an erroneous state. Also this saves 'if' statements to check exit codes after commands execute.",,aroch,sjwiesman,,,,,,,,,,,,,,,,,,,,"sjwiesman commented on pull request #99: [FLINK-17193] [k8s-python-example] build-example.sh improvements
URL: https://github.com/apache/flink-statefun/pull/99
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/20 19:33;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 19:40:40 UTC 2020,,,,,,,,,,"0|z0dpz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/20 19:40;sjwiesman;Merged in master: 2ceb3742dce6c7ef858e52db7aec2f9e5c4ac71d
and release-2.0: d660668ff45312f7c3b10529b29b478efe220e57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL client does not support views that reference a table from DDL,FLINK-17190,13298803,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,twalthr,twalthr,16/Apr/20 12:35,01/Jul/20 13:48,13/Jul/23 08:07,01/Jul/20 13:48,1.10.0,,,,,1.11.0,,,,Table SQL / Client,,,,,0,,,,,"It seems to be a classloading issue when the view references a DDL table.

{code}
CREATE TABLE PROD_LINEITEM (
  L_ORDERKEY       INTEGER,
  L_PARTKEY        INTEGER,
  L_SUPPKEY        INTEGER,
  L_LINENUMBER     INTEGER,
  L_QUANTITY       DOUBLE,
  L_EXTENDEDPRICE  DOUBLE,
  L_DISCOUNT       DOUBLE,
  L_TAX            DOUBLE,
  L_CURRENCY       STRING,
  L_RETURNFLAG     STRING,
  L_LINESTATUS     STRING,
  L_ORDERTIME      TIMESTAMP(3),
  L_SHIPINSTRUCT   STRING,
  L_SHIPMODE       STRING,
  L_COMMENT        STRING,
  WATERMARK FOR L_ORDERTIME AS L_ORDERTIME - INTERVAL '5' MINUTE,
  L_PROCTIME       AS PROCTIME()
) WITH (
  'connector.type' = 'kafka',
  'connector.version' = 'universal',
  'connector.topic' = 'Lineitem',
  'connector.properties.zookeeper.connect' = 'not-needed',
  'connector.properties.bootstrap.servers' = 'kafka:9092',
  'connector.startup-mode' = 'earliest-offset',
  'format.type' = 'csv',
  'format.field-delimiter' = '|'
);

CREATE VIEW v AS SELECT * FROM PROD_LINEITEM;
{code}

Result:
{code}
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:190)
Caused by: org.apache.flink.table.api.TableException: findAndCreateTableSource failed.
	at org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:55)
	at org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:92)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.findAndCreateTableSource(CatalogSourceTable.scala:156)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.tableSource$lzycompute(CatalogSourceTable.scala:65)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.tableSource(CatalogSourceTable.scala:65)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:76)
	at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3328)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2357)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2005)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:646)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:627)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3181)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:563)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:148)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:135)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:522)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:436)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:154)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:464)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.addView(LocalExecutor.java:300)
	at org.apache.flink.table.client.cli.CliClient.callCreateView(CliClient.java:579)
	at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:308)
	at java.util.Optional.ifPresent(Optional.java:159)
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:200)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:125)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178)
Caused by: org.apache.flink.table.api.NoMatchingTableFactoryException: Could not find a suitable table factory for 'org.apache.flink.table.factories.TableSourceFactory' in
the classpath.

Reason: Required context properties mismatch.

The matching candidates:
org.apache.flink.table.sources.CsvBatchTableSourceFactory
Mismatched properties:
'connector.type' expects 'filesystem', but is 'kafka'

....

The following factories have been considered:
org.apache.flink.table.sources.CsvBatchTableSourceFactory
org.apache.flink.table.sources.CsvAppendTableSourceFactory
{code}",,jark,JinxinTang,leonard,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17728,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 13:47:25 UTC 2020,,,,,,,,,,"0|z0dpfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/20 08:20;JinxinTang;Could you please provide version info and is it can be reproduced in local environment?;;;","20/Apr/20 14:41;twalthr;I tried out Flink 1.10. Should be reproducible in a local environment as well.;;;","01/Jul/20 13:47;leonard;This feature has supported in FLINK-17728,  I close this issue, feel free to reopen if you have any concern.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table with processing time attribute can not be read from Hive catalog,FLINK-17189,13298802,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,twalthr,twalthr,16/Apr/20 12:30,16/Oct/20 10:55,13/Jul/23 08:07,26/May/20 05:35,1.10.1,,,,,1.11.0,,,,Table SQL / Ecosystem,Table SQL / Planner,,,,0,pull-request-available,,,,"DDL:
{code}
CREATE TABLE PROD_LINEITEM (
  L_ORDERKEY       INTEGER,
  L_PARTKEY        INTEGER,
  L_SUPPKEY        INTEGER,
  L_LINENUMBER     INTEGER,
  L_QUANTITY       DOUBLE,
  L_EXTENDEDPRICE  DOUBLE,
  L_DISCOUNT       DOUBLE,
  L_TAX            DOUBLE,
  L_CURRENCY       STRING,
  L_RETURNFLAG     STRING,
  L_LINESTATUS     STRING,
  L_ORDERTIME      TIMESTAMP(3),
  L_SHIPINSTRUCT   STRING,
  L_SHIPMODE       STRING,
  L_COMMENT        STRING,
  WATERMARK FOR L_ORDERTIME AS L_ORDERTIME - INTERVAL '5' MINUTE,
  L_PROCTIME       AS PROCTIME()
) WITH (
  'connector.type' = 'kafka',
  'connector.version' = 'universal',
  'connector.topic' = 'Lineitem',
  'connector.properties.zookeeper.connect' = 'not-needed',
  'connector.properties.bootstrap.servers' = 'kafka:9092',
  'connector.startup-mode' = 'earliest-offset',
  'format.type' = 'csv',
  'format.field-delimiter' = '|'
);
{code}

Query:
{code}
SELECT * FROM prod_lineitem;
{code}

Result:
{code}
[ERROR] Could not execute SQL statement. Reason:
java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType(INTEGER L_ORDERKEY, INTEGER L_PARTKEY, INTEGER L_SUPPKEY, INTEGER L_LINENUMBER, DOUBLE L_QUANTITY, DOUBLE L_EXTENDEDPRICE, DOUBLE L_DISCOUNT, DOUBLE L_TAX, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_CURRENCY, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_RETURNFLAG, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_LINESTATUS, TIME ATTRIBUTE(ROWTIME) L_ORDERTIME, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_SHIPINSTRUCT, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_SHIPMODE, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_COMMENT, TIMESTAMP(3) NOT NULL L_PROCTIME) NOT NULL
converted type:
RecordType(INTEGER L_ORDERKEY, INTEGER L_PARTKEY, INTEGER L_SUPPKEY, INTEGER L_LINENUMBER, DOUBLE L_QUANTITY, DOUBLE L_EXTENDEDPRICE, DOUBLE L_DISCOUNT, DOUBLE L_TAX, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_CURRENCY, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_RETURNFLAG, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_LINESTATUS, TIME ATTRIBUTE(ROWTIME) L_ORDERTIME, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_SHIPINSTRUCT, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_SHIPMODE, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_COMMENT, TIME ATTRIBUTE(PROCTIME) NOT NULL L_PROCTIME) NOT NULL
rel:
LogicalProject(L_ORDERKEY=[$0], L_PARTKEY=[$1], L_SUPPKEY=[$2], L_LINENUMBER=[$3], L_QUANTITY=[$4], L_EXTENDEDPRICE=[$5], L_DISCOUNT=[$6], L_TAX=[$7], L_CURRENCY=[$8], L_RETURNFLAG=[$9], L_LINESTATUS=[$10], L_ORDERTIME=[$11], L_SHIPINSTRUCT=[$12], L_SHIPMODE=[$13], L_COMMENT=[$14], L_PROCTIME=[$15])
  LogicalWatermarkAssigner(rowtime=[L_ORDERTIME], watermark=[-($11, 300000:INTERVAL MINUTE)])
    LogicalProject(L_ORDERKEY=[$0], L_PARTKEY=[$1], L_SUPPKEY=[$2], L_LINENUMBER=[$3], L_QUANTITY=[$4], L_EXTENDEDPRICE=[$5], L_DISCOUNT=[$6], L_TAX=[$7], L_CURRENCY=[$8], L_RETURNFLAG=[$9], L_LINESTATUS=[$10], L_ORDERTIME=[$11], L_SHIPINSTRUCT=[$12], L_SHIPMODE=[$13], L_COMMENT=[$14], L_PROCTIME=[PROCTIME()])
      LogicalTableScan(table=[[hcat, default, prod_lineitem, source: [KafkaTableSource(L_ORDERKEY, L_PARTKEY, L_SUPPKEY, L_LINENUMBER, L_QUANTITY, L_EXTENDEDPRICE, L_DISCOUNT, L_TAX, L_CURRENCY, L_RETURNFLAG, L_LINESTATUS, L_ORDERTIME, L_SHIPINSTRUCT, L_SHIPMODE, L_COMMENT)]]])
{code}",,godfreyhe,jark,libenchao,lirui,lsy,lzljs3620320,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17868,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 05:36:07 UTC 2020,,,,,,,,,,"0|z0dpf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/20 05:59;lirui;The root cause is that {{TimestampKind}} is not encoded into the serializable string of {{TimestampType}}, {{LocalZonedTimestampType}} and {{ZonedTimestampType}}. In {{HiveCatalog}}, the table schema of a generic table is serialized using a {{DescriptorProperties}} which in turn relies on the serializable strings of logical types. Now since {{TimestampKind}} is missing, {{HiveCatalog}} is unable to properly deserialize the table schema and thus the error.

I tried including {{TimestampKind}} in {{DescriptorProperties}}, e.g. by calling {{LogicalType::asSummaryString}} instead of {{LogicalType::asSerializableString}} and that fixes the issue. But I'm not sure whether that's the way to go, because it seems {{TimestampKind}} is excluded from the serializable string by design.

[~twalthr] what do you think is the right way to fix the issue?;;;","20/Apr/20 07:13;twalthr;Hi [~lirui],  
yes, the {{TimestampKind}} is excluded by design. The data types of the table schema should be only {{TIMESTAMP(3)}} in Hive. The information whether a column is a time attribute is encoded in {{L_PROCTIME AS PROCTIME()}} for processing time and {{WATERMARK FOR L_ORDERTIME AS L_ORDERTIME - INTERVAL '5' MINUTE}} for event time. Those important schema parts should also be written out (but in properties) and used when reading in a table again. We should have the same logic already for event time. Just processing time seems to have issues.;;;","20/Apr/20 08:07;lirui;Hi [~twalthr], thanks for the explanations. Then I think we can store the {{PROCTIME}} info in {{DescriptorProperties}}, similar to how we store watermark specs. And I think we just need to store the names of fields which are processing time. When deserializing the table schema, we check whether a field is processing time and if it is, we update the filed type to set the {{TimestampKind}}. Does that make sense to you?;;;","20/Apr/20 09:04;lzljs3620320;This is not a new issue, actually FLINK-16110 is a similar issue. CC: [~godfreyhe]

The thing is: DDL -> SqlCreateTable -> CreateTableOperation, the TableSchema in CreateTableOperation use TimestampKind in column types instead of a separate class field. So the DescriptorProperties ignore the TimestampKind information.

I think this is a table/planner bug instead of hive bug, we should make TableSchema stronger with DescriptorProperties serialization. We can:
 # Store a separate class field in TableSchema like watermark
 # putTableSchema should serialize this proctime class field.;;;","20/Apr/20 09:16;twalthr;This might be a separate topic but I think the current property design to represent `TableSchema` has many shortcomings. I would rather vote for simply aligning the property design to the DDL itself:
{code}
schema:
  - kind: watermark
    ...
  - kind: constaint
    ...
  - kind: column
{code}

This would solve also FLINK-17158.;;;","20/Apr/20 14:43;jark;Hi all, first of all, I admit it is a bug, but not a bug of TimestampKind serialization. The properties of DDL should only describe the information of the DDL, not include the derived DataType of the function. The DataType of the function should be inferred again by planner after deserialization from catalog. 

However, currently, we directly use the deserialized DataType of the function which will lose the TimestampKind information. I think we should fix that in {{DatabaseCalciteSchema}} to re-calcuate the output DataType of functions when get the CatalogTable from CatalogManager. ;;;","20/May/20 04:02;lzljs3620320;{{TableSourceUtil.getSourceRowType}} should not only adjust rowtime, but also adjust proctime fields. I will create a PR for fixing.;;;","26/May/20 05:35;lzljs3620320;master: f08c8309738e519d246aeda163ef1b1f5e7855c7

release-1.11: 86d2777e89b87c842e9fc242757103656e807154;;;","26/May/20 05:36;lzljs3620320;Feel free to re-open this if you think we need fix in release-1.10 too.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to download conda when running python tests,FLINK-17188,13298800,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxbks2ks,dwysakowicz,dwysakowicz,16/Apr/20 12:24,24/Apr/20 01:37,13/Jul/23 08:07,24/Apr/20 01:37,1.11.0,,,,,1.11.0,,,,API / Python,Build System / Azure Pipelines,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7549&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=14487301-07d2-5d56-5690-6dfab9ffd4d9
This pipeline failed to download conda

If this issue starts appearing more often we should come up with some solution for those kinds of problems.

{code}
CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/anaconda/noarch/babel-2.8.0-py_0.tar.bz2>
Elapsed: -

An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your way.


conda install sphinx failed         please try to exec the script again.        if failed many times, you can try to exec in the form of sudo ./lint-python.sh -f
PYTHON exited with EXIT CODE: 1.
{code}
",,dian.fu,dwysakowicz,hxbks2ks,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 24 01:37:47 UTC 2020,,,,,,,,,,"0|z0dpeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/20 12:55;hxbks2ks;Thanks [~dwysakowicz] for finding this error. If it happens frequently, I will add retrying times to avoid this problem.;;;","17/Apr/20 12:59;rmetzger;Another instance: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7628&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=14487301-07d2-5d56-5690-6dfab9ffd4d9;;;","17/Apr/20 13:13;hxbks2ks;It looks like this is not an accident. I will add retrying times to fix it.;;;","19/Apr/20 05:58;rmetzger;Thanks a lot. I assigned you to the ticket.;;;","19/Apr/20 12:24;dian.fu;Another instance: https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/7724/logs/105

It timed out during downloading flake8;;;","19/Apr/20 12:27;dian.fu;The following instance report ""Maven produced no output for 300 seconds."" during downloading sphinx.

Instance: https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/7726/logs/152;;;","19/Apr/20 17:55;rmetzger;Upgrading ticket to Blocker: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7734&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=455fddbf-5921-5b71-25ac-92992ad80b28;;;","20/Apr/20 05:56;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7733&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=455fddbf-5921-5b71-25ac-92992ad80b28;;;","20/Apr/20 06:06;hxbks2ks;Thanks for the sharing. This issue occurs very frenquently and I'll try to fix it ASAP.;;;","23/Apr/20 19:44;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=140&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=455fddbf-5921-5b71-25ac-92992ad80b28;;;","24/Apr/20 01:37;dian.fu;Thanks [~rmetzger], I have merged the PR. Let's see if this issue still happens.;;;","24/Apr/20 01:37;dian.fu;Merged to master via 681371b8328cfbec3904395acddf6453609aeb73;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RemoteInputChannelTest.testConcurrentOnSenderBacklogAndRecycle fail on azure,FLINK-17182,13298724,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,dwysakowicz,dwysakowicz,16/Apr/20 07:09,12/Jun/20 02:59,13/Jul/23 08:07,12/Jun/20 02:58,1.11.0,,,,,1.11.0,,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7546&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=d2c1c472-9d7b-5913-b8e4-461f3092fb7a
{code}
[ERROR] Tests run: 21, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.943 s <<< FAILURE! - in org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest
[ERROR] testConcurrentOnSenderBacklogAndRecycle(org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest)  Time elapsed: 0.011 s  <<< FAILURE!
java.lang.AssertionError: There should be 248 buffers available in channel. expected:<248> but was:<238>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.testConcurrentOnSenderBacklogAndRecycle(RemoteInputChannelTest.java:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dwysakowicz,gaoyunhaii,rmetzger,yunta,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 02:58:28 UTC 2020,,,,,,,,,,"0|z0doxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/20 07:28;zjwang;Thanks for reporting this [~dwysakowicz], I have assigned [~gaoyunhaii] for investigating this issue.;;;","19/Apr/20 13:24;gaoyunhaii;This should be caused by when the unfulfilled buffer is fulfilled by recycled exclusive buffers, the status of waiting on buffer pool is not cleared, thus it could not reuse buffer returned by itself to buffer pool. A simulated case would be:

Suppose initially both the input channel and buffer pool have no buffers, which the case in this test. We also temporarily ignore the initial credit, which does not affect the issue. 
 # (Thread 1)  inputChannel.onSenderBacklog(2), which makes the input channel start waiting on the buffer pool
 # (Thread 2) one buffer is recycled to the buffer pool and assigned to the input channel.
 # (Thread 2) One buffer is going to be recycled to the buffer pool. The listener is taken, and the thread paused before notifying the buffer to the channel.
 # (Thread 3) one exclusive buffer is recycle and returned to the input channel. With current implementation the input channel will not stop waiting, even if its available buffers are already equal to the required.
 # (Thread 3)  one exclusive buffer is recycled and return to the input channel. input channel will return one buffer to the buffer pool. Since the listener is not in the queue now, the buffer could not be allocated to the channel again.
 # (Thread 1)  inputChannel.onSenderBacklog(4), thus one more buffer is required, however, since the channel is still waiting on the buffer pool, it cannot request the available buffer in the buffer pool. There is also no other chances to assign this buffer to the input channel. 
 # (Thread 2) Wake up again and finish notifying the buffer. However, the available buffer recycled in step 5 could not assigned to the channel again.

Thus, one buffer is left in the buffer pool and cannot assign to the input channel.;;;","22/Apr/20 10:45;zjwang;After confirming with [~gaoyunhaii] offline, it has no effect for the normal process, because floating/exclusive buffers recycle can not be executed concurrently except the cancel case.

I think we can adjust this unit test to execute floating/exclusive buffer recycle only in one thread to avoid the unstable issue.;;;","05/May/20 13:34;rmetzger;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=622&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d;;;","13/May/20 06:42;yunta;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=462&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d;;;","11/Jun/20 06:30;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3214&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d;;;","11/Jun/20 07:20;zjwang;I think I should review this PR today to save some efforts for testing stability, although this issue is not a blocker for release. :);;;","12/Jun/20 02:58;gaoyunhaii;Fix via:

   master: f88d98da41957beca84d1807319a5fb004cd02f8
        1.11: 22098b27c32342f3ef74848a86d4b4d8d3c1dfb8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StringUtils.arrayToString() should consider Object[] lastly,FLINK-17175,13298698,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,phoenixjiangnan,phoenixjiangnan,phoenixjiangnan,16/Apr/20 05:18,24/Apr/20 23:46,13/Jul/23 08:07,24/Apr/20 23:46,1.11.0,,,,,1.11.0,,,,API / Core,,,,,0,,,,,,,phoenixjiangnan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 24 23:46:00 UTC 2020,,,,,,,,,,"0|z0dos0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/20 23:46;phoenixjiangnan;master: 6128bd1060e299d798dc486385991d6142bc7d0d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-enable debug level logging in Jepsen Tests,FLINK-17172,13298643,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gjy,gjy,gjy,15/Apr/20 19:12,17/Apr/20 06:04,13/Jul/23 08:07,17/Apr/20 06:04,1.11.0,,,,,1.11.0,,,,Tests,,,,,0,pull-request-available,,,,"Since log4j2 was enabled, logs in Jepsen tests are on INFO level. We should re-enable debug level logging in Jepsen Tests. ",,gjy,,,,,,,,,,,,,,,,,,,,,"GJL commented on pull request #11761: [FLINK-17172][test] Enable DEBUG level logging in Jepsen tests
URL: https://github.com/apache/flink/pull/11761
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/20 16:02;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 06:04:59 UTC 2020,,,,,,,,,,"0|z0dofs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/20 06:04;gjy;master: 2669f5bbfac7e973379ce37bdb7a8837cb7405ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot stop streaming job with savepoint which uses kinesis consumer,FLINK-17170,13298599,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,cvasii,cvasii,15/Apr/20 15:25,28/Aug/21 11:18,13/Jul/23 08:07,09/May/21 17:49,1.10.0,1.11.3,1.12.2,,,1.12.4,1.13.1,1.14.0,,API / DataStream,Connectors / Kinesis,,,,0,pull-request-available,usability,,,"I am encountering a very strange situation where I can't stop with savepoint a streaming job.

The job reads from kinesis and sinks to S3, very simple job, no mapping function, no watermarks, just source->sink. 

Source is using flink-kinesis-consumer, sink is using StreamingFileSink. 

Everything works fine, except stopping the job with savepoints.

The behaviour happens only when multiple task managers are involved, having sub-tasks off the job spread across multiple task manager instances. When a single task manager has all the sub-tasks this issue never occurred.

Using latest Flink 1.10.0 version, deployment done in HA mode (2 job managers), in EC2, savepoints and checkpoints written on S3.

When trying to stop, the savepoint is created correctly and appears on S3, but not all sub-tasks are stopped. Some of them finished, but some just remain hanged. Sometimes, on the same task manager part of the sub-tasks are finished, part aren't.

The logs don't show any errors. For the ones that succeed, the standard messages appear, with ""Source: <....> switched from RUNNING to FINISHED"".

For the sub-tasks hanged the last message is ""org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher - Shutting down the shard consumer threads of subtask 0 ..."" and that's it.

 

I tried using the cli (flink stop <job_id>)

Timeout Message:
{code:java}
root@ec2-XX-XX-XX-XX:/opt/flink/current/bin# ./flink stop cf43cecd9339e8f02a12333e52966a25
root@ec2-XX-XX-XX-XX:/opt/flink/current/bin# ./flink stop cf43cecd9339e8f02a12333e52966a25Suspending job ""cf43cecd9339e8f02a12333e52966a25"" with a savepoint. ------------------------------------------------------------ The program finished with the following exception: org.apache.flink.util.FlinkException: Could not stop with a savepoint job ""cf43cecd9339e8f02a12333e52966a25"". at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:462) at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:843) at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:454) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:907) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)Caused by: java.util.concurrent.TimeoutException at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928) at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:460) ... 9 more{code}
 

Using the monitoring api, I keep getting infinite message when querying based on the savepoint id, that the status id is still ""IN_PROGRESS"".

 

When performing a cancel instead of stop, it works. But cancel is deprecated, so I am a bit concerned that this might fail also, maybe I was just lucky.

 

I attached a screenshot with what the UI is showing when this happens

 ",,aljoscha,cvasii,dannycranmer,kezhuw,klion26,knaufk,limbo,mapohl,nkruber,pnowojski,qinjunjerry,ubyyj,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21030,,,,,,,,,,,"15/Apr/20 15:25;cvasii;Screenshot 2020-04-15 at 18.16.26.png;https://issues.apache.org/jira/secure/attachment/13000028/Screenshot+2020-04-15+at+18.16.26.png","16/Apr/20 06:14;cvasii;threaddump_tm1;https://issues.apache.org/jira/secure/attachment/13000099/threaddump_tm1",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 09 17:48:56 UTC 2021,,,,,,,,,,"0|z0do60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/20 16:33;yunta;[~cvasii] Has the savepoint completed via the web UI? Did the sub-task checkpoint on those hanged task finished? (You could get the information via the web UI of checkpoint details). And the quickest solution to detect the root cause is using jstack to capture what the task thread is doing when it is hanged.;;;","15/Apr/20 16:46;cvasii;I was doing that. And the shard consumer threads are blocked. Looks like it's the checkpointLock they are all waiting on.

 

Here is a sample of one stack:
{code:java}
shardConsumers-Source: source<my_name> -> Sink: sink-s3a://<my_path> (12/12)-thread-0priority:5 - threadId:0x00007f565c854000 - nativeId:0x49e0 - nativeId (decimal):18912 - state:BLOCKED
stackTrace:
java.lang.Thread.State: BLOCKED (on object monitor)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.emitRecordAndUpdateState(KinesisDataFetcher.java:774)
- waiting to lock <0x00000006a4d18620> (a java.lang.Object)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.access$000(KinesisDataFetcher.java:92)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$AsyncKinesisRecordEmitter.emit(KinesisDataFetcher.java:273)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$SyncKinesisRecordEmitter$1.put(KinesisDataFetcher.java:288)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$SyncKinesisRecordEmitter$1.put(KinesisDataFetcher.java:285)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.emitRecordAndUpdateState(KinesisDataFetcher.java:760)
at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.deserializeRecordForCollectionAndUpdateState(ShardConsumer.java:371)
at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:258)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
{code};;;","15/Apr/20 16:49;cvasii;And regarding the savepoint, yes it finished successfully on all sub-tasks.;;;","15/Apr/20 17:20;cvasii;It's quite hard to debug, because I have multiple threads with the same name blocked on the same lock. A sub-tasks consumes multiple shards, so a fetcher per subtask is created, but the method KinesisDataFetcher#createShardConsumersThreadPool just allocates the same identifier for the threads (any ThreadFactory implementation is better there by the way, a thing to improve).

 

There is also one blocked thread which is single, not competing with others for any lock. ;;;","15/Apr/20 17:40;yunta;If the savepoint could finally complete, I doubt the task stuck when notified checkpoint complete to [finish|https://github.com/apache/flink/blob/9ed5f1b4b5e1c8a9f9421b91ac00960b63916ebd/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L930]. Could you share the information of source stream task or the whole jstack info of the hanged JVM? BTW, debug level info logs could also help to know more information.;;;","16/Apr/20 02:34;klion26;Hi, [~cvasii] from the description, seems the savepoint successfully, and ""unfinished"" task was blocked by something.

Currently, the lifetime of task logic is ""trigger savepoint"" -> ""savepoint complete"" -> ""savepoint complete"" -> ""finish task""

From the previous comments you given, seems the stack was waiting for some lock, could you please check what is it waiting for?

or could you please share the whole jstack message about the ""unfinished"" task.;;;","16/Apr/20 06:57;cvasii;Thanks guys for looking into this, [~klion26] [~yunta] I've attached a full thread dump from one of the task managers, file called ""threaddump_tm1"".

 

I've seen that shard consumers are waiting for the checkpoint lock, but the checkpoint lock it's already taken by the thread which performs the stop.

 

Some relevant parts of the thread dump

 

 
{code:java}
shardConsumers-Source: source -> Sink: sink (12/12)-thread-0priority:5 - threadId:0x00007f7c7c298000 - nativeId:0x774c - nativeId (decimal):30540 - state:BLOCKED
stackTrace:
java.lang.Thread.State: BLOCKED (on object monitor)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.emitRecordAndUpdateState(KinesisDataFetcher.java:774)
- waiting to lock <0x00000007abe026c8> (a java.lang.Object)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.access$000(KinesisDataFetcher.java:92)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$AsyncKinesisRecordEmitter.emit(KinesisDataFetcher.java:273)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$SyncKinesisRecordEmitter$1.put(KinesisDataFetcher.java:288)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$SyncKinesisRecordEmitter$1.put(KinesisDataFetcher.java:285)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.emitRecordAndUpdateState(KinesisDataFetcher.java:760)
at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.deserializeRecordForCollectionAndUpdateState(ShardConsumer.java:371)
at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:258)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
{code}
 

 

but the lock 0x00000007abe026c8 it's already taken here
{code:java}
Source: source -> Sink: sink (12/12)priority:5 - threadId:0x00007f7c2c0cf000 - nativeId:0x76fe - nativeId (decimal):30462 - state:TIMED_WAITING
stackTrace:
java.lang.Thread.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000007ac041158> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1475)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.awaitTermination(KinesisDataFetcher.java:637)
at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.cancel(FlinkKinesisConsumer.java:365)
at org.apache.flink.streaming.api.operators.StreamSource.cancel(StreamSource.java:147)
at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.cancelTask(SourceStreamTask.java:136)
at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.finishTask(SourceStreamTask.java:147)
at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:947)
at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$7(StreamTask.java:924)
at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$838/1354138988.run(Unknown Source)
at org.apache.flink.util.function.FunctionUtils.lambda$asCallable$5(FunctionUtils.java:125)
at org.apache.flink.util.function.FunctionUtils$$Lambda$839/1349808364.call(Unknown Source)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87)
- locked <0x00000007abe026c8> (a java.lang.Object)
at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:261)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
at java.lang.Thread.run(Thread.java:748)
{code}
 

So, FlinkKinesisConsumer#close() method is called, which delegates to FlinkKinesisConsumer#cancel(), then KinesisDataFetcher#shutdownFetcher() is called and as it seems it executes successfully, cause I don't  have any errrors in the logs. If there would have been an exception, the message from FlinkKinesisConsumer line 367 ""Error while closing Kinesis data fetcher"" would have been in my logs. Then fetcher.awaitTermination() is called, and in KinesisDataFetcher#awaitTermination it waits, for 1 minute. But there isn't any Thread._setDefaultUncaughtExceptionHandler_ set, so if there's an exception in the shardConsumersExecutor, I can't really see it. Side question: can I set the UncaughtExceptionHandler somehow?

 

For me it is quite obvious that the shardConsumersExecutor is not shutdown and based on my JDK knowledge, calling shutdownNow on an executor service does not guarantee that it will actually terminate. And since multiple threads are competing for the checkpoint lock, I guess the threads from the shardConsumersExecutor must be interrupted. How come it works when doing a cancel? Is there a forced interrupt being done?

 ;;;","16/Apr/20 11:06;yunta;I could reproduce this with a batch lines of code. There existed a dead lock here: one thread is waiting to get the lock, while the other thread which holds the lock is waiting for the previous thread to terminate.

I still need some time to investigate this to see whether there existed more possible dead locks there.;;;","16/Apr/20 13:32;yunta;I think there might be three solutions:
# Use new source operator API which introduced in FLIP-27 to avoid the checkpoint lock with mailbox when emitting records in source thread. However, I think new source operator API is not fully ready now.
# Avoid the endless awaitTermination when {{shutdownFetcher}}, I am not sure whether this could meet kinesis requests, cc [~tzulitai]
# Check whether this fetcher is running within {{emitRecordAndUpdateState}} first before try to access the {{checkpointLock}}. If not running, just return instead of access the  {{checkpointLock}}. I think this could resolve this problem.;;;","16/Apr/20 17:53;cvasii;How come it works when I perform a cancel, instead of stop action?;;;","17/Apr/20 10:26;yunta;[~cvasii] cancel job does not need to trigger a savepoint before shutdown. In other words, cancel would not trigger ""notifyCheckpoitComplete"" on task side once the checkpoint is completed on checkpoint coordinator side. When we call ""notifyCheckpoitComplete"", it will grab the lock which would also be grabbed when await termination of kinesis fetcher.;;;","17/Jun/20 11:20;ubyyj;any update on this?

I hit the exactly same issue here, with 1.10.1.

BTW, call the savepoint rest api can trigger savepoint and as well cancel the job.

something like this:

curl -d '\{""target-directory"": ""s3://dp-flink/savepoints/"",""cancel-job"":true}' :jobmanagerTracking-URL[/jobs/:jobid/savepoints|http://ip-10-0-102-40.ap-northeast-1.compute.internal:41581/jobs/6b1f193a0a2e98a6eabf6fe6d876c3bc/savepoints];;;","17/Nov/20 12:57;qinjunjerry;I also see the exact same behavior happened to one customer with just one TM and parallelism=1.  By 'exact', I mean
 * the source thread is holding the checkpoint lock, and is waiting for the shardConsumers to finish, but the shardConsumers cannot finish because they are waiting to lock the checkpoint lock
 * the code line numbers in the stack trace is same as the one attached there. 

 ;;;","19/Nov/20 19:41;qinjunjerry;The source thread holds the checkpoint lock and is waiting here forever for the shardConsumers thread to finish, while the shardConsumers can only finish once they get the checkpoint lock:
{code:java}
// From: KinesisDataFetcher.java
public void awaitTermination() throws InterruptedException {
   while (!shardConsumersExecutor.awaitTermination(1, TimeUnit.MINUTES)) {
      // Keep waiting.
   }
}
{code};;;","23/Nov/20 03:03;yunta;I think my previous suggestion of ""Check whether this fetcher is running within emitRecordAndUpdateState first before try to access the checkpointLock"" might not solve the problem fundamentally if {{running}} has been set as false while {{checkpointLock}} has been synchronized in the main thread. Shall the endless await termination is a must be for kinesis fetcher [~tzulitai]?;;;","08/Feb/21 20:04;mapohl;We're investigating a bug with stop-with-savepoint where the savepoint is successfully created but the job does not finish due to some failure after savepoint creation. See FLINK-21030 for further details. FLINK-17170 might be related.;;;","09/Feb/21 02:40;kezhuw;I think it is problem of {{FlinkKinesisConsumer.cancel}}, it should not await fetcher to finished, it should do only signalling.

[~qinjunjerry] is correct about the deadlock. In stop-with-savepoint path, {{FlinkKinesisConsumer.cancel}} is called with {{checkpointLock}} hold.;;;","09/Feb/21 02:57;kezhuw;{quote}
In stop-with-savepoint path, {{FlinkKinesisConsumer.cancel}} is called with {{checkpointLock}} hold.
{quote}

This holds after 1.10, so this issue should exist in all versions after 1.10.;;;","22/Apr/21 12:26;flink-jira-bot;This critical issue is unassigned and itself and all of its Sub-Tasks have not been updated for 7 days. So, it has been labeled ""stale-critical"". If this ticket is indeed critical, please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 19:49;dannycranmer;I have taken a read through the issue/code and agree with the diagnosed deadlock. I agree that removing {{fetcher.awaitTermination()}} from {{FlinkKinesisConsumer::cancel}} will fix the deadlock. However this would potentially result in the job transitioning to finished with open resources. The expectation would be that the resources will terminate, but this could still result in temporary leak/dangling objects on the TM. 

I believe we can fix this by moving the {{fetcher.awaitTermination()}} to the [close()|https://github.com/apache/flink/blob/9ed5f1b4b5e1c8a9f9421b91ac00960b63916ebd/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/FlinkKinesisConsumer.java#L373] method. I have taken a quick dive and it looks like this would be called outside of the checkpoint lock. However I am not 100% sure this would be ok for all Source lifecycle possibilities. Does anyone have any reason to believe this is not a good idea?;;;","30/Apr/21 06:19;arvid;Hi [~dannycranmer] , why do we need to have this{{ fetcher.awaitTermination()}} in cancel/close in the first place? Wouldn't it suffice to just rely on the await at the end of {{FlinkKinesisConsumer#run}}? As far as I can see, the {{cancel}} is shutting down the fetcher, so it should return in a graceful manner in {{run}}. Going further, {{runFetcher}} is already invoking {{awaitTermination}} on its own, so we could clean this up even further.;;;","30/Apr/21 11:11;pnowojski;I also think sources shouldn't be doing blocking waits on such conditions inside {{cancel()}}. Basically what [~dannycranmer] suggested makes sense.

edit:
{quote}
Hi Danny Cranmer , why do we need to have this{{ fetcher.awaitTermination()}} in cancel/close in the first place? Wouldn't it suffice to just rely on the await at the end of FlinkKinesisConsumer#run? As far as I can see, the cancel is shutting down the fetcher, so it should return in a graceful manner in run. Going further, runFetcher is already invoking awaitTermination on its own, so we could clean this up even further.
{quote}
[~arvid] {{FlinkKinesisConsumer#run}} can be interrupted, and that's expected behaviour. {{close}} should guarantee that all resources are cleaned up. If resource cleaning up is not possible, it's better for {{close}} to hang until task's cancellation times outs, and whole Task Manager is killed.;;;","06/May/21 14:14;arvid;Merged into 1.12 as 1de3d6345a4132dda4b31f66275fb732afe3ef30.;;;","07/May/21 15:26;arvid;Merged into 1.13 as a102549f08759e177074dad286fef2f56176d005..282f9a3d5505a5aa58d7d9cca466939610d41ed3.;;;","09/May/21 17:48;arvid;Merged into master as 442dc76bc76b9a7628202b33b21126ef3d48a90c..49d9092b31d0a95d727b790ee04a4ed8d72924e3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ES6 ElasticsearchSinkITCase unstable,FLINK-17159,13298539,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,chesnay,chesnay,15/Apr/20 11:40,18/Nov/20 12:31,13/Jul/23 08:07,18/Nov/20 12:30,1.11.0,1.12.0,,,,1.12.0,,,,Connectors / ElasticSearch,Tests,,,,0,pull-request-available,test-stability,,,"[https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7482&view=logs&j=64110e28-73be-50d7-9369-8750330e0bf1&t=aa84fb9a-59ae-5696-70f7-011bc086e59b]
{code:java}
2020-04-15T02:37:04.4289477Z [ERROR] testElasticsearchSinkWithSmile(org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase)  Time elapsed: 0.145 s  <<< ERROR!
2020-04-15T02:37:04.4290310Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-04-15T02:37:04.4290790Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-04-15T02:37:04.4291404Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:659)
2020-04-15T02:37:04.4291956Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:77)
2020-04-15T02:37:04.4292548Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1643)
2020-04-15T02:37:04.4293254Z 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.runElasticSearchSinkTest(ElasticsearchSinkTestBase.java:128)
2020-04-15T02:37:04.4293990Z 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.runElasticsearchSinkSmileTest(ElasticsearchSinkTestBase.java:106)
2020-04-15T02:37:04.4295096Z 	at org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.testElasticsearchSinkWithSmile(ElasticsearchSinkITCase.java:45)
2020-04-15T02:37:04.4295923Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-04-15T02:37:04.4296489Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-04-15T02:37:04.4297076Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-04-15T02:37:04.4297513Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-04-15T02:37:04.4297951Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-04-15T02:37:04.4298688Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-04-15T02:37:04.4299374Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-04-15T02:37:04.4300069Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-04-15T02:37:04.4300960Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-04-15T02:37:04.4301705Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-04-15T02:37:04.4302204Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-04-15T02:37:04.4302661Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-04-15T02:37:04.4303234Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-04-15T02:37:04.4303706Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-04-15T02:37:04.4304127Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-04-15T02:37:04.4304716Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-04-15T02:37:04.4305394Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-04-15T02:37:04.4305965Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-04-15T02:37:04.4306425Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-04-15T02:37:04.4306942Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-04-15T02:37:04.4307466Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-15T02:37:04.4307920Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-15T02:37:04.4308375Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-15T02:37:04.4308782Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-04-15T02:37:04.4309182Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-04-15T02:37:04.4310366Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-04-15T02:37:04.4311140Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-04-15T02:37:04.4311721Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-04-15T02:37:04.4312241Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-04-15T02:37:04.4312796Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-04-15T02:37:04.4313365Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-04-15T02:37:04.4313866Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-04-15T02:37:04.4314361Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-04-15T02:37:04.4315033Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-04-15T02:37:04.4315830Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:112)
2020-04-15T02:37:04.4316563Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-04-15T02:37:04.4317399Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:189)
2020-04-15T02:37:04.4317990Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:183)
2020-04-15T02:37:04.4318606Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:177)
2020-04-15T02:37:04.4319199Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:497)
2020-04-15T02:37:04.4319882Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:384)
2020-04-15T02:37:04.4320391Z 	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
2020-04-15T02:37:04.4320841Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-04-15T02:37:04.4321370Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-04-15T02:37:04.4321970Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-04-15T02:37:04.4322519Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-04-15T02:37:04.4323058Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-04-15T02:37:04.4323605Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-04-15T02:37:04.4324085Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-04-15T02:37:04.4324493Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-04-15T02:37:04.4325143Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-04-15T02:37:04.4325736Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-04-15T02:37:04.4326355Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-04-15T02:37:04.4327020Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-04-15T02:37:04.4327649Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-04-15T02:37:04.4328273Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-04-15T02:37:04.4328892Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-04-15T02:37:04.4329530Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-04-15T02:37:04.4330086Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-04-15T02:37:04.4330628Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-04-15T02:37:04.4331371Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-04-15T02:37:04.4332070Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-04-15T02:37:04.4332649Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-04-15T02:37:04.4333289Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-04-15T02:37:04.4334051Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-15T02:37:04.4335024Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-15T02:37:04.4336070Z Caused by: org.elasticsearch.ElasticsearchStatusException: method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2020-04-15T02:37:04.4337153Z 	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:625)
2020-04-15T02:37:04.4338036Z 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:535)
2020-04-15T02:37:04.4338716Z 	at org.elasticsearch.client.RestHighLevelClient.ping(RestHighLevelClient.java:275)
2020-04-15T02:37:04.4339386Z 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:137)
2020-04-15T02:37:04.4340137Z 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:47)
2020-04-15T02:37:04.4341285Z 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.open(ElasticsearchSinkBase.java:301)
2020-04-15T02:37:04.4341889Z 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
2020-04-15T02:37:04.4342464Z 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
2020-04-15T02:37:04.4343022Z 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:48)
2020-04-15T02:37:04.4343583Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:294)
2020-04-15T02:37:04.4344168Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:445)
2020-04-15T02:37:04.4345048Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
2020-04-15T02:37:04.4345632Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:441)
2020-04-15T02:37:04.4346152Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:462)
2020-04-15T02:37:04.4346631Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:718)
2020-04-15T02:37:04.4347099Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:542)
2020-04-15T02:37:04.4347460Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-15T02:37:04.4348018Z Caused by: org.elasticsearch.client.ResponseException: method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2020-04-15T02:37:04.4348667Z 	at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:705)
2020-04-15T02:37:04.4349324Z 	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:235)
2020-04-15T02:37:04.4349785Z 	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:198)
2020-04-15T02:37:04.4350299Z 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:522)
2020-04-15T02:37:04.4350652Z 	... 15 more
2020-04-15T02:37:04.4351229Z Caused by: org.elasticsearch.client.ResponseException: method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2020-04-15T02:37:04.4351858Z 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:377)
2020-04-15T02:37:04.4352286Z 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:366)
2020-04-15T02:37:04.4352743Z 	at org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:119)
2020-04-15T02:37:04.4353303Z 	at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:177)
2020-04-15T02:37:04.4354080Z 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:436)
2020-04-15T02:37:04.4354827Z 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.responseReceived(HttpAsyncRequestExecutor.java:309)
2020-04-15T02:37:04.4355434Z 	at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:255)
2020-04-15T02:37:04.4356012Z 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81)
2020-04-15T02:37:04.4356559Z 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39)
2020-04-15T02:37:04.4357140Z 	at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114)
2020-04-15T02:37:04.4357661Z 	at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162)
2020-04-15T02:37:04.4358171Z 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337)
2020-04-15T02:37:04.4358728Z 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315)
2020-04-15T02:37:04.4359267Z 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276)
2020-04-15T02:37:04.4363545Z 	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
2020-04-15T02:37:04.4364304Z 	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:588)
2020-04-15T02:37:04.4365089Z 	... 1 more
 {code}",,aljoscha,dian.fu,guoyangze,leonard,leslieyuan,rmetzger,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 12:30:44 UTC 2020,,,,,,,,,,"0|z0dnso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/20 13:07;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2929&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","10/Jun/20 06:45;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3104&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20
{code}
2020-06-10T04:14:17.7230548Z [INFO] -------------------------------------------------------
2020-06-10T04:14:17.7230925Z [INFO]  T E S T S
2020-06-10T04:14:17.7231739Z [INFO] -------------------------------------------------------
2020-06-10T04:14:18.0547289Z [INFO] Running org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase
2020-06-10T04:14:31.9990328Z [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.942 s - in org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase
2020-06-10T04:14:32.7754906Z [INFO] Running org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase
2020-06-10T04:14:43.6018074Z [ERROR] Tests run: 5, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 10.823 s <<< FAILURE! - in org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase
2020-06-10T04:14:43.6019129Z [ERROR] testElasticsearchSink(org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase)  Time elapsed: 0.963 s  <<< ERROR!
2020-06-10T04:14:43.6019676Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-06-10T04:14:43.6022076Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-06-10T04:14:43.6022812Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:677)
2020-06-10T04:14:43.6023339Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:81)
2020-06-10T04:14:43.6023974Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1686)
2020-06-10T04:14:43.6024834Z 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.runElasticSearchSinkTest(ElasticsearchSinkTestBase.java:92)
2020-06-10T04:14:43.6025539Z 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.runElasticsearchSinkTest(ElasticsearchSinkTestBase.java:56)
2020-06-10T04:14:43.6026425Z 	at org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.testElasticsearchSink(ElasticsearchSinkITCase.java:40)
2020-06-10T04:14:43.6026936Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-10T04:14:43.6027337Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-10T04:14:43.6027833Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-10T04:14:43.6028263Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-10T04:14:43.6028772Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-06-10T04:14:43.6029985Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-06-10T04:14:43.6030742Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-06-10T04:14:43.6031292Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-06-10T04:14:43.6031725Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-06-10T04:14:43.6032240Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-10T04:14:43.6032662Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-06-10T04:14:43.6033114Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-06-10T04:14:43.6033600Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-06-10T04:14:43.6034193Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-06-10T04:14:43.6034607Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-06-10T04:14:43.6035042Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-06-10T04:14:43.6035463Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-06-10T04:14:43.6035882Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-06-10T04:14:43.6036454Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-10T04:14:43.6036899Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-10T04:14:43.6037365Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-10T04:14:43.6037828Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-10T04:14:43.6038194Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-06-10T04:14:43.6038983Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-06-10T04:14:43.6039642Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-06-10T04:14:43.6040145Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-06-10T04:14:43.6040645Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-06-10T04:14:43.6041177Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-06-10T04:14:43.6041705Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-06-10T04:14:43.6042305Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-06-10T04:14:43.6042759Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-06-10T04:14:43.6043259Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-06-10T04:14:43.6043864Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-06-10T04:14:43.6044697Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-06-10T04:14:43.6045331Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)
2020-06-10T04:14:43.6045867Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)
2020-06-10T04:14:43.6046642Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)
2020-06-10T04:14:43.6047225Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)
2020-06-10T04:14:43.6047743Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:386)
2020-06-10T04:14:43.6048185Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-10T04:14:43.6048581Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-10T04:14:43.6049133Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-10T04:14:43.6049578Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-10T04:14:43.6050000Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-06-10T04:14:43.6050507Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-06-10T04:14:43.6051018Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-06-10T04:14:43.6051541Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-06-10T04:14:43.6052081Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-06-10T04:14:43.6052476Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-06-10T04:14:43.6052898Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-06-10T04:14:43.6053313Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-06-10T04:14:43.6053740Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-06-10T04:14:43.6054152Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-06-10T04:14:43.6054579Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-06-10T04:14:43.6054985Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-06-10T04:14:43.6055364Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-06-10T04:14:43.6055771Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-06-10T04:14:43.6056293Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-06-10T04:14:43.6056671Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-06-10T04:14:43.6057142Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-06-10T04:14:43.6057556Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-06-10T04:14:43.6057939Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-06-10T04:14:43.6058425Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-06-10T04:14:43.6058966Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-06-10T04:14:43.6059411Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-06-10T04:14:43.6060133Z Caused by: org.elasticsearch.ElasticsearchStatusException: method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2020-06-10T04:14:43.6060990Z 	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:625)
2020-06-10T04:14:43.6061914Z 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:535)
2020-06-10T04:14:43.6062772Z 	at org.elasticsearch.client.RestHighLevelClient.ping(RestHighLevelClient.java:275)
2020-06-10T04:14:43.6063390Z 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:137)
2020-06-10T04:14:43.6064121Z 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:47)
2020-06-10T04:14:43.6064758Z 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.open(ElasticsearchSinkBase.java:300)
2020-06-10T04:14:43.6065284Z 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
2020-06-10T04:14:43.6065823Z 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
2020-06-10T04:14:43.6066524Z 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:48)
2020-06-10T04:14:43.6067047Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:291)
2020-06-10T04:14:43.6067899Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:473)
2020-06-10T04:14:43.6068823Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
2020-06-10T04:14:43.6069565Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:469)
2020-06-10T04:14:43.6070078Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522)
2020-06-10T04:14:43.6070523Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720)
2020-06-10T04:14:43.6070900Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545)
2020-06-10T04:14:43.6071245Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-10T04:14:43.6071878Z Caused by: org.elasticsearch.client.ResponseException: method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2020-06-10T04:14:43.6072515Z 	at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:705)
2020-06-10T04:14:43.6072955Z 	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:235)
2020-06-10T04:14:43.6073367Z 	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:198)
2020-06-10T04:14:43.6073829Z 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:522)
2020-06-10T04:14:43.6074151Z 	... 15 more
2020-06-10T04:14:43.6074626Z Caused by: org.elasticsearch.client.ResponseException: method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2020-06-10T04:14:43.6075221Z 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:377)
2020-06-10T04:14:43.6075632Z 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:366)
2020-06-10T04:14:43.6076194Z 	at org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:119)
2020-06-10T04:14:43.6076871Z 	at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:177)
2020-06-10T04:14:43.6077551Z 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:436)
2020-06-10T04:14:43.6078108Z 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.responseReceived(HttpAsyncRequestExecutor.java:309)
2020-06-10T04:14:43.6078651Z 	at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:255)
2020-06-10T04:14:43.6079259Z 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81)
2020-06-10T04:14:43.6079747Z 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39)
2020-06-10T04:14:43.6080244Z 	at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114)
2020-06-10T04:14:43.6080727Z 	at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162)
2020-06-10T04:14:43.6081206Z 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337)
2020-06-10T04:14:43.6081710Z 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315)
2020-06-10T04:14:43.6082262Z 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276)
2020-06-10T04:14:43.6082733Z 	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
2020-06-10T04:14:43.6083239Z 	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:588)
2020-06-10T04:14:43.6083618Z 	... 1 more
2020-06-10T04:14:43.6083738Z 
2020-06-10T04:14:43.6084161Z [ERROR] testElasticsearchSinkWithSmile(org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase)  Time elapsed: 0.09 s  <<< ERROR!
2020-06-10T04:14:43.6084724Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-06-10T04:14:43.6085165Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-06-10T04:14:43.6085652Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:677)
2020-06-10T04:14:43.6086283Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:81)
2020-06-10T04:14:43.6086845Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1686)
2020-06-10T04:14:43.6087486Z 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.runElasticSearchSinkTest(ElasticsearchSinkTestBase.java:92)
2020-06-10T04:14:43.6088152Z 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.runElasticsearchSinkSmileTest(ElasticsearchSinkTestBase.java:70)
2020-06-10T04:14:43.6088897Z 	at org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.testElasticsearchSinkWithSmile(ElasticsearchSinkITCase.java:45)
2020-06-10T04:14:43.6089402Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-10T04:14:43.6089804Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-10T04:14:43.6090286Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-10T04:14:43.6090688Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-10T04:14:43.6091099Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-06-10T04:14:43.6091559Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-06-10T04:14:43.6092101Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-06-10T04:14:43.6092573Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-06-10T04:14:43.6092989Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-06-10T04:14:43.6093362Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-10T04:14:43.6093870Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-06-10T04:14:43.6094298Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-06-10T04:14:43.6094752Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-06-10T04:14:43.6095178Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-06-10T04:14:43.6095566Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-06-10T04:14:43.6095958Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-06-10T04:14:43.6096494Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-06-10T04:14:43.6096884Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-06-10T04:14:43.6097294Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-10T04:14:43.6097701Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-10T04:14:43.6098130Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-10T04:14:43.6098517Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-10T04:14:43.6098930Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-06-10T04:14:43.6099348Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-06-10T04:14:43.6099814Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-06-10T04:14:43.6100313Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-06-10T04:14:43.6100788Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-06-10T04:14:43.6101280Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-06-10T04:14:43.6101864Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-06-10T04:14:43.6102328Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-06-10T04:14:43.6102773Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-06-10T04:14:43.6103231Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-06-10T04:14:43.6103807Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-06-10T04:14:43.6104472Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-06-10T04:14:43.6105054Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)
2020-06-10T04:14:43.6105589Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)
2020-06-10T04:14:43.6106323Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)
2020-06-10T04:14:43.6106877Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)
2020-06-10T04:14:43.6107382Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:386)
2020-06-10T04:14:43.6107783Z 	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
2020-06-10T04:14:43.6108193Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-10T04:14:43.6108593Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-10T04:14:43.6109077Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-06-10T04:14:43.6109569Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-06-10T04:14:43.6110058Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-06-10T04:14:43.6110650Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-06-10T04:14:43.6111137Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-06-10T04:14:43.6111532Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-06-10T04:14:43.6112003Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-06-10T04:14:43.6112403Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-06-10T04:14:43.6112816Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-06-10T04:14:43.6113211Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-06-10T04:14:43.6113618Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-06-10T04:14:43.6113989Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-06-10T04:14:43.6114381Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-06-10T04:14:43.6114765Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-06-10T04:14:43.6115117Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-06-10T04:14:43.6115479Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-06-10T04:14:43.6115811Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-06-10T04:14:43.6116316Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-06-10T04:14:43.6116699Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-06-10T04:14:43.6117137Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-06-10T04:14:43.6117576Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-06-10T04:14:43.6117999Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-06-10T04:14:43.6118633Z Caused by: org.elasticsearch.ElasticsearchStatusException: method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2020-06-10T04:14:43.6119337Z 	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:625)
2020-06-10T04:14:43.6119851Z 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:535)
2020-06-10T04:14:43.6120333Z 	at org.elasticsearch.client.RestHighLevelClient.ping(RestHighLevelClient.java:275)
2020-06-10T04:14:43.6120903Z 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:137)
2020-06-10T04:14:43.6121607Z 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:47)
2020-06-10T04:14:43.6122293Z 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.open(ElasticsearchSinkBase.java:300)
2020-06-10T04:14:43.6122833Z 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
2020-06-10T04:14:43.6123375Z 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
2020-06-10T04:14:43.6123870Z 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:48)
2020-06-10T04:14:43.6124379Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:291)
2020-06-10T04:14:43.6124905Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:473)
2020-06-10T04:14:43.6125465Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
2020-06-10T04:14:43.6126074Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:469)
2020-06-10T04:14:43.6126647Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522)
2020-06-10T04:14:43.6127085Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720)
2020-06-10T04:14:43.6127462Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545)
2020-06-10T04:14:43.6127882Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-10T04:14:43.6128483Z Caused by: org.elasticsearch.client.ResponseException: method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2020-06-10T04:14:43.6129144Z 	at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:705)
2020-06-10T04:14:43.6129590Z 	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:235)
2020-06-10T04:14:43.6130002Z 	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:198)
2020-06-10T04:14:43.6130467Z 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:522)
2020-06-10T04:14:43.6130804Z 	... 15 more
2020-06-10T04:14:43.6131265Z Caused by: org.elasticsearch.client.ResponseException: method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2020-06-10T04:14:43.6131933Z 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:377)
2020-06-10T04:14:43.6132343Z 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:366)
2020-06-10T04:14:43.6132763Z 	at org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:119)
2020-06-10T04:14:43.6133277Z 	at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:177)
2020-06-10T04:14:43.6133876Z 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:436)
2020-06-10T04:14:43.6134425Z 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.responseReceived(HttpAsyncRequestExecutor.java:309)
2020-06-10T04:14:43.6134966Z 	at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:255)
2020-06-10T04:14:43.6135497Z 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81)
2020-06-10T04:14:43.6135984Z 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39)
2020-06-10T04:14:43.6136690Z 	at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114)
2020-06-10T04:14:43.6137176Z 	at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162)
2020-06-10T04:14:43.6137651Z 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337)
2020-06-10T04:14:43.6138157Z 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315)
2020-06-10T04:14:43.6138635Z 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276)
2020-06-10T04:14:43.6139172Z 	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
2020-06-10T04:14:43.6139697Z 	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:588)
2020-06-10T04:14:43.6140062Z 	... 1 more
2020-06-10T04:14:43.6140258Z 
2020-06-10T04:14:44.0543677Z [INFO] 
2020-06-10T04:14:44.0544114Z [INFO] Results:
2020-06-10T04:14:44.0544336Z [INFO] 
2020-06-10T04:14:44.0544547Z [ERROR] Errors: 
2020-06-10T04:14:44.0546341Z [ERROR]   ElasticsearchSinkITCase.testElasticsearchSink:40->ElasticsearchSinkTestBase.runElasticsearchSinkTest:56->ElasticsearchSinkTestBase.runElasticSearchSinkTest:92 Â» JobExecution
2020-06-10T04:14:44.0547491Z [ERROR]   ElasticsearchSinkITCase.testElasticsearchSinkWithSmile:45->ElasticsearchSinkTestBase.runElasticsearchSinkSmileTest:70->ElasticsearchSinkTestBase.runElasticSearchSinkTest:92 Â» JobExecution
2020-06-10T04:14:44.0548010Z [INFO] 
2020-06-10T04:14:44.0548257Z [ERROR] Tests run: 8, Failures: 0, Errors: 2, Skipped: 0
{code};;;","10/Jun/20 09:57;roman;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3062&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8];;;","17/Jun/20 07:51;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3635&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","21/Jul/20 02:39;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4654&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407;;;","24/Jul/20 03:24;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4821&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20]

{code}
2020-07-23T19:17:21.9040167Z Caused by: org.elasticsearch.ElasticsearchStatusException: method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2020-07-23T19:17:21.9040839Z 	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:625)
2020-07-23T19:17:21.9041512Z 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:535)
2020-07-23T19:17:21.9042010Z 	at org.elasticsearch.client.RestHighLevelClient.ping(RestHighLevelClient.java:275)
2020-07-23T19:17:21.9042651Z 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:137)
2020-07-23T19:17:21.9043348Z 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:47)
2020-07-23T19:17:21.9043970Z 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.open(ElasticsearchSinkBase.java:296)
{code};;;","28/Jul/20 07:13;aljoscha;I think the reason is just that the embedded ES node is not coming fully up before we try and run the tests.;;;","25/Aug/20 15:24;aljoscha;Potential fix on master: f6467816334ae04da9e72ee759ad007d60bfdca7;;;","25/Aug/20 15:38;aljoscha;Fix on release-1.11: 6c97f22913197e7a5a948f67b7a0b7f8fb480fa7

Please re-open if the issue persists.;;;","03/Sep/20 09:02;roman;Occurred again, reopening:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6115&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=d44f43ce-542c-597d-bf94-b0718c71e5e8]

(the code change is a reverse of an unrelated commit - unlikely to cause this);;;","20/Sep/20 13:25;dian.fu;Another instance on 1.11: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6662&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=2f99feaa-7a9b-5916-4c1c-5e61f395079e;;;","22/Sep/20 09:14;aljoscha;I added more sanity checks.

release-1.11: 03397343c958b5fb2c0b035d5222ce6288dc8938
master: 1e40f046f85a74d2ce7e265b4ccd89cccf7e0130;;;","22/Sep/20 09:14;aljoscha;Please keep reporting test failures.;;;","24/Sep/20 02:01;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6870&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","27/Sep/20 00:55;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6984&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","07/Oct/20 06:06;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7253&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","12/Oct/20 17:01;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7438&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","14/Oct/20 02:21;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7517&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","21/Oct/20 01:31;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7966&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","21/Oct/20 02:11;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7969&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","21/Oct/20 08:07;rmetzger;[~aljoscha] I'm assigning this ticket to myself to have a look as well. I don't think I know more than you, but maybe a fresh pair of eyes helps here ;);;;","22/Oct/20 07:42;rmetzger;Uff. So I was really not able to even reproduce this issue, even though it seems to happen quite frequently.

Ideally we would reproduce it on DEBUG log level so that we can see why ES is not able to process the ping request.
Does it make sense to use {{CommonTestUtils.waitUntilCondition()}} and retry for 30 seconds for the cluster to come up properly?;;;","26/Oct/20 14:31;aljoscha;Maybe yes. 🤷‍♂️ But yeah, this issue is quite though... ;;;","26/Oct/20 17:45;rmetzger;[ERROR] Errors: 
[ERROR]   Elasticsearch6DynamicSinkITCase.testWritingDocuments » JobExecution Job execut...
[ERROR]   Elasticsearch6DynamicSinkITCase.testWritingDocumentsFromTableApi:157 » Execution
[ERROR]   Elasticsearch6DynamicSinkITCase.testWritingDocumentsNoPrimaryKey:217 » Execution
[ERROR]   Elasticsearch6DynamicSinkITCase.testWritingDocumentsWithDynamicIndex:287 » Execution
[INFO] 
[ERROR] Tests run: 9, Failures: 0, Errors: 4, Skipped: 0


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8301&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","30/Oct/20 16:22;trohrmann;Another instability where the ES6 server did not start up: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8654&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20

{code}
[ERROR] testInvalidElasticsearchCluster(org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase)  Time elapsed: 0.019 s  <<< ERROR!
ElasticsearchStatusException[method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]]; nested: ResponseException[method [HEAD], host [
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:625)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:535)
	at org.elasticsearch.client.RestHighLevelClient.ping(RestHighLevelClient.java:275)
	at org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.ensureClusterIsUp(ElasticsearchSinkITCase.java:46)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: org.elasticsearch.client.ResponseException: method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]
	at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:705)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:235)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:198)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:522)
	... 33 more
Caused by: org.elasticsearch.client.ResponseException: method [HEAD], host [http://127.0.0.1:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable]
	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:377)
	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:366)
	at org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:119)
	at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:177)
	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:436)
	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.responseReceived(HttpAsyncRequestExecutor.java:309)
	at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:255)
	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81)
	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39)
	at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:588)
	at java.lang.Thread.run(Thread.java:748)
{code};;;","04/Nov/20 09:12;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8956&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","10/Nov/20 19:46;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9408&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","11/Nov/20 01:49;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9426&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=eb5f4d19-2d2d-5856-a4ce-acf5f904a994;;;","11/Nov/20 14:44;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9459&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","13/Nov/20 02:42;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9540&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=603cb7fd-6f38-5c99-efca-877e1439232f;;;","18/Nov/20 12:30;aljoscha;This is hopefully fixed by switching to use testcontainers for the ES instances, which should eliminate conflicts between concurrent tests, which I suspect are the source of the instabilities.

master: a5a6ab4b61f69f4e40ee571c9f31812fe8013a67;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FunctionDefinitionUtil generate wrong resultType and  acc type of AggregateFunctionDefinition,FLINK-17152,13298484,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Terry1897,Terry1897,Terry1897,15/Apr/20 07:21,16/Apr/20 04:01,13/Jul/23 08:07,16/Apr/20 04:01,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"FunctionDefinitionUtil generate wrong resultType and  acc type of AggregateFunctionDefinition. This bug will  lead to unexpect error such as: 
Field types of query result and registered TableSink do not  match.
Query schema: [v:RAW(IAccumulator, ?)]
Sink schema: [v: STRING]

",,godfreyhe,jark,libenchao,liyu,Terry1897,,,,,,,,,,,,,,,,,"zjuwangg commented on pull request #11748: [FLINK-17152]fix wrong Aggfunction resultType and accType in FunctionDefinitionUtil
URL: https://github.com/apache/flink/pull/11748
 
 
   ## What is the purpose of the change
   
   * In [FLINK-16414](https://github.com/apache/flink/pull/11302), I fixed the problem that using using udaf/udtf which doesn't implement getResultType, but also introduce this problem careless. This pr aims to solve aggregate function can not get correct return type.
   
   ## Brief change log
   
     - #9ed7dbb fix bug and add corresponding unit test.
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Added more unit tests in FunctionDefinitionUtilTest*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no )
     - The runtime per-record code paths (performance sensitive): (no )
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/20 07:32;githubbot;600","wuchong commented on pull request #11748: [FLINK-17152]fix wrong Aggfunction resultType and accType in FunctionDefinitionUtil
URL: https://github.com/apache/flink/pull/11748
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/20 03:58;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 04:01:18 UTC 2020,,,,,,,,,,"0|z0dngg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/20 07:36;Terry1897;cc [~liyu] I think this bug should be included in flink 1.10.1 release.;;;","15/Apr/20 13:20;liyu;Thanks for the ping [~Terry1897]. I'm escalating the priority to Blocker and will hold 1.10.1 RC1 until this one is fixed.;;;","16/Apr/20 04:01;jark;Fixed in 
 - master (1.11.0): 4563176fbf8d4f27d2ebd20d5fd42c8c085582b2
 - 1.10.1: cc8faa4fdc7ba6b2761b4c4154a0d8b661880914;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalExecutorITCase.testParameterizedTypes failed on travis,FLINK-17138,13298278,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,pnowojski,pnowojski,14/Apr/20 13:15,23/Apr/20 05:04,13/Jul/23 08:07,23/Apr/20 05:04,1.10.0,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"https://api.travis-ci.org/v3/job/674770944/log.txt
release-1.10 branch build failed with
{code}
11:49:51.608 [INFO] Running org.apache.flink.table.client.gateway.local.LocalExecutorITCase
11:52:40.202 [ERROR] Tests run: 64, Failures: 0, Errors: 1, Skipped: 5, Time elapsed: 168.589 s <<< FAILURE! - in org.apache.flink.table.client.gateway.local.LocalExecutorITCase
11:52:40.209 [ERROR] testParameterizedTypes[Planner: blink](org.apache.flink.table.client.gateway.local.LocalExecutorITCase)  Time elapsed: 5.609 s  <<< ERROR!
org.apache.flink.table.client.gateway.SqlExecutionException: Invalid SQL statement.
	at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testParameterizedTypes(LocalExecutorITCase.java:903)
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. Failed to get PrimaryKey constraints
	at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testParameterizedTypes(LocalExecutorITCase.java:903)
Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to get PrimaryKey constraints
	at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testParameterizedTypes(LocalExecutorITCase.java:903)
Caused by: java.lang.reflect.InvocationTargetException
	at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testParameterizedTypes(LocalExecutorITCase.java:903)
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: No current connection.
	at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testParameterizedTypes(LocalExecutorITCase.java:903)
{code}",,aljoscha,jark,lirui,lzljs3620320,twalthr,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11756: [FLINK-17138][sql-client] Close catalogs when close the session
URL: https://github.com/apache/flink/pull/11756
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   `HiveCatalog` may hold resources that need to be released after use. So we should close loaded catalogs when a session ends.
   
   
   ## Brief change log
   
     - Iterate and close each catalog in `LocalExecutor::closeSession`.
   
   
   ## Verifying this change
   
   Existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/20 13:17;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 05:04:56 UTC 2020,,,,,,,,,,"0|z0dm6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/20 13:40;twalthr;This test should not depend on Hive. It should use a testing catalog instead.;;;","15/Apr/20 03:04;jark;cc [~lzljs3620320]

 ;;;","15/Apr/20 03:10;lzljs3620320;CC: [~lirui] Just like [~twalthr] said, maybe we should let sql-client tests get rid of hive dependency, and use a testing catalog.;;;","15/Apr/20 03:24;lirui;Actually some test cases are intended to test LocalExecutor with Hive catalog/connector. The test cases were put into sql-client because we don't have E2E test for hive connector at the moment. I agree sql-client should get rid of hive dependencies once the E2E test is ready.;;;","15/Apr/20 12:02;lirui;I'm unable to reproduce the failure so it's hard to determine the root cause. One potential problem I see is that we're not closing the HiveCatalog after the test. Since we use embedded metastore, some static/threadlocal variables may not have been properly cleared before each test case is run. I'll submit a PR for that.
BTW, it seems we lack an API to close loaded catalogs from Executor. Ideally I think the close should happen when {{Executor::closeSession}} is called.;;;","23/Apr/20 05:04;lzljs3620320;master: 27d1a48cc5c1ef7baf506d7c0db4d01ebdca6b70

Feel free to re-open if it is reproduced.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonCalcSplitRuleTest.testPandasFunctionMixedWithGeneralPythonFunction failed,FLINK-17135,13298240,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,14/Apr/20 11:06,21/Apr/20 02:39,13/Jul/23 08:07,21/Apr/20 02:39,1.11.0,,,,,1.11.0,,,,API / Python,Tests,,,,0,pull-request-available,test-stability,,,"Shared by Chesnay on https://issues.apache.org/jira/browse/FLINK-17093?focusedCommentId=17083055&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17083055:

PythonCalcSplitRuleTest.testPandasFunctionMixedWithGeneralPythonFunction failed on master:
{code:java}
 [INFO] Running org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest
 [ERROR] Tests run: 19, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.441 s <<< FAILURE! - in org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest
 [ERROR] testPandasFunctionMixedWithGeneralPythonFunction(org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest) Time elapsed: 0.032 s <<< FAILURE!
 java.lang.AssertionError: 
 type mismatch:
 type1:
 INTEGER NOT NULL
 type2:
 INTEGER NOT NULL
 at org.apache.calcite.util.Litmus$1.fail(Litmus.java:31)
 at org.apache.calcite.plan.RelOptUtil.eq(RelOptUtil.java:2188)
 at org.apache.calcite.rex.RexProgramBuilder$RegisterInputShuttle.visitInputRef(RexProgramBuilder.java:948)
{code}",,aljoscha,dian.fu,dwysakowicz,godfreyhe,hequn8128,rmetzger,,,,,,,,,,,,,,,,"dianfu commented on pull request #11771: [FLINK-17135][python][tests] Fix the test testPandasFunctionMixedWithGeneralPythonFunction to make it more stable
URL: https://github.com/apache/flink/pull/11771
 
 
   
   ## What is the purpose of the change
   
   *This pull request changes the test testPandasFunctionMixedWithGeneralPythonFunction a bit to work around the bug introduced in CALCITE-3149.*
   
   ## Brief change log
   
     - *Update the test testPandasFunctionMixedWithGeneralPythonFunction*
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Updated tests testPandasFunctionMixedWithGeneralPythonFunction*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/20 09:13;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 02:38:51 UTC 2020,,,,,,,,,,"0|z0dly0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/20 11:12;dwysakowicz;I also observed this problem locally. It is not deterministic though. I've seen it only once in a couple of runs.;;;","14/Apr/20 11:15;dian.fu;[~dwysakowicz] Thanks for sharing this information. Seems an instable test. I will take a look at it.;;;","14/Apr/20 13:07;aljoscha;Another instance: https://dev.azure.com/aljoschakrettek/Flink/_build/results?buildId=93&view=logs&j=a1590513-d0ea-59c3-3c7b-aad756c48f25&t=912be268-3995-5931-14ba-f3d52302cb3c&l=4792;;;","15/Apr/20 14:35;dian.fu;Status update: I found that this issue could be reproduced every time in the personal Azure Pipeline. However, I still cannot reproduce it in my local environment and also I have not found failures in the Azure Pipeline triggered in the PR. I'm still adding more logs and trying to figure out the root cause.;;;","15/Apr/20 18:31;dwysakowicz;Locally for me it fails every two runs when I execute {{mvn clean install}} in {{flink-table}} module.;;;","16/Apr/20 01:35;dian.fu;[~dwysakowicz] Thanks a lot for this information. Very helpful. I could not reproduce it when executing in the IDE and also only executing PythonCalcSplitRuleTest in the command line. Will try to execute all the tests using `mvn clean install` in the command line.;;;","16/Apr/20 09:07;dian.fu;I have reproduced this issue in my local machine and I think it's a bug introduced in CALCITE-3149 after debugging. CALCITE-3149 ensures that the RelDataType cache(DATATYPE_CACHE) in RelDataTypeFactoryImpl could be garbage collected. However, the equality check still checks the [object reference](https://github.com/apache/calcite/blob/52a57078ba081b24b9d086ed363c715485d1a519/core/src/main/java/org/apache/calcite/rex/RexProgramBuilder.java#L948). It may cause issues in cases that the cache DATATYPE_CACHE has been garbage collected and at the same time there are still references to the old RelDataType (I have saw there are RelDataTypes which are not in the DATATYPE_CACHE cache and this is the root cause of the test failure reported in this JIRA). I will change the tests a bit to work around this issue for the time being. 

However, this is a bug which should also be fixed in calcite. cc [~danny0405];;;","19/Apr/20 15:22;rmetzger;more instances:
- https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=304&view=logs&j=a1590513-d0ea-59c3-3c7b-aad756c48f25&t=d62215ae-261e-5cec-c84f-5abb77c78ded
- https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=305&view=logs&j=a1590513-d0ea-59c3-3c7b-aad756c48f25&t=d62215ae-261e-5cec-c84f-5abb77c78ded
- https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=306&view=logs&j=a1590513-d0ea-59c3-3c7b-aad756c48f25&t=d62215ae-261e-5cec-c84f-5abb77c78ded
- https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=308&view=logs&j=a1590513-d0ea-59c3-3c7b-aad756c48f25&t=d62215ae-261e-5cec-c84f-5abb77c78ded
;;;","21/Apr/20 02:38;hequn8128;Hi, the fix has been merged into master via 34105c708b518f1fc5cc83f62bf10143ff662d13
Sorry for the trouble.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadocs broken for master,FLINK-17131,13298221,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Apr/20 09:22,16/Apr/20 07:54,13/Jul/23 08:07,16/Apr/20 07:54,1.11.0,,,,,1.11.0,,,,Build System,,,,,0,pull-request-available,,,,The javadocs for the master branch aren't being displayed on the website for some reason.,,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11750: [FLINK-17131][build] Downgrade javadoc-plugin
URL: https://github.com/apache/flink/pull/11750
 
 
   Something changed in 3.0.0-M1+ which breaks our builds, so we're reverting back to the last working one.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/20 08:53;githubbot;600","zentol commented on pull request #11750: [FLINK-17131][build] Downgrade javadoc-plugin
URL: https://github.com/apache/flink/pull/11750
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/20 07:53;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,FLINK-15633,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 07:54:11 UTC 2020,,,,,,,,,,"0|z0dlts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/20 14:09;chesnay;The javadoc plugin changed the default phase for the aggregate goal from {{generate-sources}} to {{compile}} in {{3.0.0-M1 }}(MJAVADOC-467{{)}}, which now results in some test dependencies not being visible to the plugin (for some reason, maybe shading).

At the moment I can't find a way to just fix the issue; I can't get the plugin to run in the previous phase, and adding the missing dependencies as {{additionalDependencies}} also didn't do the trick.

We may just have to revert back to an older version.;;;","16/Apr/20 07:54;chesnay;master: b4826954cb7c5468a5015056fcbf5cc73a1b5d36;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The PyFlink Job runs into infinite loop if the Python UDF imports job code,FLINK-17124,13298053,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,13/Apr/20 14:58,14/Apr/20 11:31,13/Jul/23 08:07,14/Apr/20 11:31,1.10.0,,,,,1.10.1,1.11.0,,,API / Python,,,,,0,pull-request-available,,,,"If the UDF file imports job code directly or indirectly, the PyFlink Job will run into a infinite loop as follows:
 - submit job
 - execute job
 - launch UDF worker
 - import UDF
 - (If the job file is depended by UDF or imported as the top level module) import job code
 - (If the job code is executed outside the ""*if __name__ == '__main__':*"") launch gateway server and submit job to local executor
 - execute job in local mode
 - launch UDF worker
 - import UDF
 - import job code
 ...
This infinite loop will create new Java processes and Python processes endlessly until the resources on the machine are exhausted. We should fix it ASAP.",,dian.fu,hxbks2ks,zjffdu,,,,,,,,,,,,,,,,,,,"HuangXingBo commented on pull request #11719: [FLINK-17124][python] Fix The PyFlink Job runs into infinite loop if the UDF file imports job code.
URL: https://github.com/apache/flink/pull/11719
 
 
   ## What is the purpose of the change
   
   *This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
     - *disable the launching of gateway server to prevent from dead loop in ProcessPythonEnvironmentManager*
   
   ## Verifying this change
   
     - *test_set_environment*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Apr/20 15:27;githubbot;600","dianfu commented on pull request #11719: [FLINK-17124][python] Fix The PyFlink Job runs into infinite loop if the UDF file imports job code.
URL: https://github.com/apache/flink/pull/11719
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Apr/20 11:28;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 11:31:45 UTC 2020,,,,,,,,,,"0|z0dks0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/20 15:10;zjffdu;Should it be a blocker issue for 1.10.1 ?;;;","13/Apr/20 15:18;hxbks2ks;I have prepared code to fix it. I think it will be merged into release-1.10 and master quickly.;;;","14/Apr/20 11:31;dian.fu;Merged to master via ec1e863cf21b8381b4d7937e7b4e8f7add3145cf and release-1.10 via f67b4235ec631497c97dab2d4f3d0f9266f3467c

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When the pyflink job runs in local mode and the command ""python"" points to Python 2.7, the startup of the Python UDF worker will fail.",FLINK-17114,13297974,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,zhongwei,zhongwei,13/Apr/20 08:09,14/Apr/20 14:01,13/Jul/23 08:07,14/Apr/20 14:00,1.10.0,,,,,1.10.1,1.11.0,,,API / Python,,,,,0,pull-request-available,,,,"When the PyFlink job runs in local mode and the command ""python"" points to Python 2.7, the startup of the Python UDF worker will fail because ""python"" is the default interpreter of the Python UDF worker. For this case we need to set the default value of ""python.executable"" to `sys.executable` i.e. the python interpreter which launches the job.",,dian.fu,zhongwei,zjffdu,,,,,,,,,,,,,,,,,,,"WeiZhong94 commented on pull request #11714: [FLINK-17114][python] Set the default value of 'python.executable' to the path of the python interpreter which launches the job when the job is executed by LocalExecutor.
URL: https://github.com/apache/flink/pull/11714
 
 
   ## What is the purpose of the change
   
   *This pull request fixes the bug that the startup of the Python UDF worker will fail when the pyflink job runs in local mode and the command ""python"" points to Python 2.7.*
   
   
   ## Brief change log
   
     - *Add logic to the constructor of `TableEnvironment` to set the default value of 'python.executable' to the path of the python interpreter which launches the job if the job is executed by `LocalExecutor`.*
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *test_table_environment_api.py*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Apr/20 08:26;githubbot;600","WeiZhong94 commented on pull request #11729: [FLINK-17114][python] Set the default value of 'python.executable' to the path of the python interpreter which launches the job when the job is executed by LocalExecutor.
URL: https://github.com/apache/flink/pull/11729
 
 
   ## What is the purpose of the change
   
   *This pull request fixes the bug that the startup of the Python UDF worker will fail when the pyflink job runs in local mode and the command ""python"" points to Python 2.7.*
   
   
   ## Brief change log
   
     - *Add logic to the constructor of `TableEnvironment` to set the default value of 'python.executable' to the path of the python interpreter which launches the job if the job is executed by `LocalExecutor`.*
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *test_table_environment_api.py*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Apr/20 09:04;githubbot;600","dianfu commented on pull request #11714: [FLINK-17114][python] Set the default value of 'python.executable' to the path of the python interpreter which launches the job when the job is executed by LocalExecutor.
URL: https://github.com/apache/flink/pull/11714
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Apr/20 09:36;githubbot;600","dianfu commented on pull request #11729: [FLINK-17114][python] Set the default value of 'python.executable' to the path of the python interpreter which launches the job when the job is executed by LocalExecutor.
URL: https://github.com/apache/flink/pull/11729
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Apr/20 14:00;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 14:00:31 UTC 2020,,,,,,,,,,"0|z0dkag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/20 09:50;zjffdu;I think it depends what is the default python in your machine. ;;;","13/Apr/20 12:49;zhongwei;[~zjffdu] Yes, this problem occurs when the python interpreter which launches the PyFlink job is not the default ""python"" command of current environment.  e.g. the default ""python"" command points to Python 2.7 and the python interpreter which launches the PyFlink job is Python 3.7. 

For minicluster execution, it makes more sense that the job and the UDF workers should be launched by the same python interpreter.

Of course users could set the python interpreter of the UDF worker manually via `set_python_executable(sys.executable)`. But it requires users to understand some implementation details of Python UDF. Many users who install multiple python versions on their machine encountered this problem when they run their first PyFlink jobs. So I think we should fix this to improve the user experience and make the learning curve smoother.;;;","14/Apr/20 09:40;dian.fu;Merged to master via 76064f5294d245fc5d741994f50cb32e004bdd8b;;;","14/Apr/20 14:00;dian.fu;Merged to release-1.10 via 6227f0815e9db9956f1d9e1d7548d9c8efe8d895;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode(),FLINK-17107,13297955,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,13/Apr/20 06:35,14/Apr/20 14:37,13/Jul/23 08:07,14/Apr/20 04:04,1.10.0,1.6.3,1.7.2,1.8.0,1.9.0,1.10.1,1.11.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode() when checkpoint is disabled. CheckpointCoordinatorConfiguration#isExactlyOnce() returns true if checkpoint mode is  EXACTLY_ONCE mode and return false if checkpoint mode is AT_LEAST_ONCE while StreamConfig#getCheckpointMode() will always return AT_LEAST_ONCE which means always not exactly once.,,kevin.cyj,liyu,wind_ljy,,,,,,,,,,,,,,,,,,,"zhijiangW commented on pull request #11713: [FLINK-17107][checkpointing] Make CheckpointCoordinatorConfiguration#isExactlyOnce() be consistent with StreamConfig#getCheckpointMode()
URL: https://github.com/apache/flink/pull/11713
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Apr/20 15:17;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 14:37:08 UTC 2020,,,,,,,,,,"0|z0dk68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/20 04:04;kevin.cyj;Fixed via eb4a2115db91cf2908312e8c4ddc673f6c84e6bd on master;;;","14/Apr/20 14:37;liyu;Merged into release-1.10 via 9ed5f1b4b5e1c8a9f9421b91ac00960b63916ebd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mini-batch group aggregation doesn't expire state even if state ttl is enabled,FLINK-17096,13297925,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,lsy,lsy,13/Apr/20 02:40,27/Dec/20 06:19,13/Jul/23 08:07,20/Nov/20 07:30,1.10.0,1.11.0,1.9.0,,,1.12.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"At the moment, MiniBatch Group Agg include Local/Global doesn`t support State TTL, for streaming job, it will lead to OOM in long time running, so we need to make state data expire after ttl, the solution is that use incremental cleanup feature refer to FLINK-16581",,begginghard,dian.fu,felixzheng,godfreyhe,jark,klion26,libenchao,lsy,MinGW,txhsj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17099,FLINK-18872,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 20 07:30:00 UTC 2020,,,,,,,,,,"0|z0djzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/20 03:21;lsy;CC [~jark];;;","11/Nov/20 03:45;dian.fu;What's the status of this issue? Is it still target for 1.12.0?;;;","11/Nov/20 05:27;jark;Yes [~dian.fu]. I will review and merge this pull request these days. ;;;","11/Nov/20 05:38;dian.fu;[~jark] Thanks a lot~;;;","20/Nov/20 07:30;jark;Fixed in master (1.12.0): 
 - d84ad5e585c6573635a388de4a5602fe4229fa06
 - df8de4ac45884bd3134abaf87baeb65c31060b3a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python UDF doesn't work when the input column is from composite field,FLINK-17093,13297843,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dian.fu,dian.fu,dian.fu,12/Apr/20 06:16,15/Apr/20 08:44,13/Jul/23 08:07,15/Apr/20 08:44,1.10.0,,,,,1.10.1,1.11.0,,,API / Python,,,,,0,pull-request-available,,,,"For the following job:
{code}

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import BatchTableEnvironment, StreamTableEnvironment, EnvironmentSettings, CsvTableSink
from pyflink.table.descriptors import Schema, Kafka, Json
from pyflink.table import DataTypes
from pyflink.table.udf import ScalarFunction, udf
import os

@udf(input_types=[DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(),
 DataTypes.STRING()],
 result_type=DataTypes.STRING())
def get_host_ip(source, qr, sip, dip):
    if source == ""NGAF"" and qr == '1':
        return dip
    return sip

@udf(input_types=[DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(),
 DataTypes.STRING()],
 result_type=DataTypes.STRING())
def get_dns_server_ip(source, qr, sip, dip):
    if source == ""NGAF"" and qr == '1':
        return sip
    return dip

def test_case():
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(1)
    t_env = StreamTableEnvironment.create(env)

     from pyflink.table import Row
   table = t_env.from_elements(
      [(""DNS"", Row(source=""source"", devid=""devid"", sip=""sip"", dip=""dip"", qr=""qr"", queries=""queries"", answers=""answers"", qtypes=""qtypes"", atypes=""atypes"", rcode=""rcode"", ts=""ts"",))],
    DataTypes.ROW([DataTypes.FIELD(""stype"", DataTypes.STRING()),
 DataTypes.FIELD(""data"",
 DataTypes.ROW([DataTypes.FIELD('source', DataTypes.STRING()),
 DataTypes.FIELD(""devid"", DataTypes.STRING()),
 DataTypes.FIELD('sip', DataTypes.STRING()),
 DataTypes.FIELD('dip', DataTypes.STRING()),
 DataTypes.FIELD(""qr"", DataTypes.STRING()),
 DataTypes.FIELD(""queries"", DataTypes.STRING()),
 DataTypes.FIELD(""answers"", DataTypes.STRING()),
 DataTypes.FIELD(""qtypes"", DataTypes.STRING()),
 DataTypes.FIELD(""atypes"", DataTypes.STRING()),
 DataTypes.FIELD(""rcode"", DataTypes.STRING()),
 DataTypes.FIELD(""ts"", DataTypes.STRING())]))
 ]
 ))

 result_file = ""/tmp/test.csv""
 if os.path.exists(result_file):
 os.remove(result_file)

 t_env.register_table_sink(""Results"",
 CsvTableSink(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n'],
 [DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(),
 DataTypes.STRING(),
 DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(),
 DataTypes.STRING(), DataTypes.STRING(),
 DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING()],
 ""/tmp/test.csv""))

 t_env.register_function(""get_host_ip"", get_host_ip)
 t_env.register_function(""get_dns_server_ip"", get_dns_server_ip)

 t_env.register_table(""source"", table)
 standard_table = t_env.sql_query(""select data.*, stype as dns_type from source"")\
 .where(""dns_type.in('DNSFULL', 'DNS', 'DNSFULL_FROM_LOG', 'DNS_FROM_LOG')"")
 t_env.register_table(""standard_table"", standard_table)

 final_table = t_env.sql_query(""SELECT *, get_host_ip(source, qr, sip, dip) as host_ip,""
 ""get_dns_server_ip(source, qr, sip, dip) as dns_server_ip FROM standard_table"")

 final_table.insert_into(""Results"")

 t_env.execute(""test"")


if __name__ == '__main__':
 test_case()
{code}

The plan is as following which is not correct:
{code}
 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: KafkaTableSource(type, data) -> Map -> where: (IN(type, _UTF-16LE'DNSFULL', _UTF-16LE'DNS', _UTF-16LE'DNSFULL_FROM_LOG', _UTF-16LE'DNS_FROM_LOG')), select: (data, type) -> select: (type, get_host_ip(type.source, type.qr, type.sip, type.dip) AS f0, get_dns_server_ip(type.source, type.qr, type.sip, type.dip) AS f1) -> select: (f0.source AS source, f0.devid AS devid, f0.sip AS sip, f0.dip AS dip, f0.qr AS qr, f0.queries AS queries, f0.answers AS answers, f0.qtypes AS qtypes, f0.atypes AS atypes, f0.rcode AS rcode, f0.ts AS ts, type AS dns_type, f0 AS host_ip, f1 AS dns_server_ip) -> to: Row -> Sink: KafkaTableSink(source, devid, sip, dip, qr, queries, answers, qtypes, atypes, rcode, ts, dns_type, host_ip, dns_server_ip) (1/4) (8d064ab137866a2a9040392a87bcc59d) switched from RUNNING to FAILED.
{code}",,dian.fu,hequn8128,liyu,,,,,,,,,,,,,,,,,,,"dianfu commented on pull request #11709: [FLINK-17093][python][table-planner][table-planner-blink] Fix Python UDF to make it work with inputs from composite field
URL: https://github.com/apache/flink/pull/11709
 
 
   
   ## What is the purpose of the change
   
   *This pull request fixes Python UDF to make it work with inputs from composite field*
   
   ## Brief change log
   
     - *Add rule PythonCalcExpandProjectRule which expand the input fields which are from composite fields for Python UDF *
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Added tests testPythonFunctionWithCompositeInputs and testChainingPythonFunctionWithCompositeInputs*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Apr/20 10:19;githubbot;600","hequn8128 commented on pull request #11709: [FLINK-17093][python][table-planner][table-planner-blink] Fix Python UDF to make it work with inputs from composite field
URL: https://github.com/apache/flink/pull/11709
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Apr/20 12:11;githubbot;600","dianfu commented on pull request #11717: [FLINK-17093][python][table-planner][table-planner-blink] Fix Python UDF to make it work with inputs from composite field
URL: https://github.com/apache/flink/pull/11717
 
 
   
   ## What is the purpose of the change
   
   *This pull request fix Python UDF to make it work with inputs from composite field*
   
   
   ## Brief change log
   
     - *Add PythonCalcExpandProjectRule to fix the Python UDFs contained in the Calc*
     - *Updates PythonCorrelateSplitRule to fix the Python UDFs contained in the Correlate*
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Added tests testPythonFunctionWithCompositeInputs, testChainingPythonFunctionWithCompositeInputs and testPandasFunctionWithCompositeInputs in PythonCalcSplitRuleTest*
     - *Added tests in testPythonTableFunctionWithCompositeInputs and testJavaTableFunctionWithPythonCalcCompositeInputs in PythonCorrelateSplitRuleTest*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Apr/20 12:13;githubbot;600","hequn8128 commented on pull request #11717: [FLINK-17093][python][table-planner][table-planner-blink] Fix Python UDF to make it work with inputs from composite field
URL: https://github.com/apache/flink/pull/11717
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/20 08:41;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 08:44:32 UTC 2020,,,,,,,,,,"0|z0djhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/20 12:11;hequn8128;Resolved 
in 1.10.1 via 6f8d26d389022e7d901f69524e2e912b9c7e19ca
in 1.11.0 via 48bf9fbd9a79416ce28d76291f23861cb2d38941
;;;","12/Apr/20 15:03;liyu;Thanks for the quick fix [~dian.fu] [~hequn8128]!;;;","14/Apr/20 10:02;chesnay;Is this error related?
{code:java}
[INFO] Running org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest
[ERROR] Tests run: 19, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.441 s <<< FAILURE! - in org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest
[ERROR] testPandasFunctionMixedWithGeneralPythonFunction(org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest)  Time elapsed: 0.032 s  <<< FAILURE!
java.lang.AssertionError: 
type mismatch:
type1:
INTEGER NOT NULL
type2:
INTEGER NOT NULL
	at org.apache.calcite.util.Litmus$1.fail(Litmus.java:31)
	at org.apache.calcite.plan.RelOptUtil.eq(RelOptUtil.java:2188)
	at org.apache.calcite.rex.RexProgramBuilder$RegisterInputShuttle.visitInputRef(RexProgramBuilder.java:948){code};;;","14/Apr/20 10:50;dian.fu;[~chesnay] I'm not sure if the test failure is related to the change of this JIRA. Can I know which branch is this test failure from? release-1.10 or master?;;;","14/Apr/20 10:52;chesnay;[~dian.fu] dev branch based on master: https://dev.azure.com/chesnay/flink/_build/results?buildId=205&view=logs&j=a1590513-d0ea-59c3-3c7b-aad756c48f25&t=912be268-3995-5931-14ba-f3d52302cb3c;;;","14/Apr/20 10:58;dian.fu;Thanks for the sharing:) Then it's not related to this JIRA as the PR for master is still not merged. However, this is definitely a bug which should be addressed. I will create a separate JIRA for this issue.;;;","14/Apr/20 11:07;dian.fu;[~chesnay] Thanks a lot for reporting this issue. I have created FLINK-17135 and will look into it ASAP.;;;","15/Apr/20 08:44;hequn8128;Hi, I will close this jira. We can discuss the problem raised by Chesnay in FLINK-17135.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyflink test BlinkStreamDependencyTests is instable,FLINK-17092,13297787,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,zjwang,zjwang,11/Apr/20 17:14,08/May/20 01:33,13/Jul/23 08:07,08/May/20 01:32,1.10.0,,,,,1.10.2,1.11.0,,,API / Python,Tests,,,,0,pull-request-available,test-stability,,,"Build: [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7324&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=14487301-07d2-5d56-5690-6dfab9ffd4d9]

logs
{code:java}
2020-04-10T13:05:25.7259119Z E                   : java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-04-10T13:05:25.7259755Z E                   	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-04-10T13:05:25.7260301Z E                   	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-04-10T13:05:25.7260927Z E                   	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1663)
2020-04-10T13:05:25.7261772Z E                   	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:74)
2020-04-10T13:05:25.7262405Z E                   	at org.apache.flink.table.planner.delegation.ExecutorBase.execute(ExecutorBase.java:51)
2020-04-10T13:05:25.7263073Z E                   	at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:719)
2020-04-10T13:05:25.7263588Z E                   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-04-10T13:05:25.7264090Z E                   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-04-10T13:05:25.7264668Z E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-04-10T13:05:25.7265175Z E                   	at java.lang.reflect.Method.invoke(Method.java:498)
2020-04-10T13:05:25.7265807Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2020-04-10T13:05:25.7266445Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2020-04-10T13:05:25.7267288Z E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
2020-04-10T13:05:25.7267897Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2020-04-10T13:05:25.7268518Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
2020-04-10T13:05:25.7269130Z E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2020-04-10T13:05:25.7269623Z E                   	at java.lang.Thread.run(Thread.java:748)
2020-04-10T13:05:25.7270112Z E                   Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-04-10T13:05:25.7270700Z E                   	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-04-10T13:05:25.7271406Z E                   	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.lambda$getJobExecutionResult$2(PerJobMiniClusterFactory.java:175)
2020-04-10T13:05:25.7272111Z E                   	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-04-10T13:05:25.7272665Z E                   	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-04-10T13:05:25.7273245Z E                   	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-04-10T13:05:25.7273909Z E                   	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-04-10T13:05:25.7274514Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
2020-04-10T13:05:25.7275147Z E                   	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-04-10T13:05:25.7275800Z E                   	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-04-10T13:05:25.7276447Z E                   	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-04-10T13:05:25.7277239Z E                   	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-04-10T13:05:25.7277805Z E                   	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:874)
2020-04-10T13:05:25.7278328Z E                   	at akka.dispatch.OnComplete.internal(Future.scala:264)
2020-04-10T13:05:25.7278804Z E                   	at akka.dispatch.OnComplete.internal(Future.scala:261)
2020-04-10T13:05:25.7279258Z E                   	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2020-04-10T13:05:25.7279883Z E                   	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2020-04-10T13:05:25.7280352Z E                   	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-04-10T13:05:25.7280917Z E                   	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
2020-04-10T13:05:25.7281501Z E                   	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2020-04-10T13:05:25.7282029Z E                   	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2020-04-10T13:05:25.7282546Z E                   	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2020-04-10T13:05:25.7283089Z E                   	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2020-04-10T13:05:25.7283728Z E                   	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2020-04-10T13:05:25.7284305Z E                   	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2020-04-10T13:05:25.7284811Z E                   	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2020-04-10T13:05:25.7285393Z E                   	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-04-10T13:05:25.7285917Z E                   	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-04-10T13:05:25.7286542Z E                   	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2020-04-10T13:05:25.7287470Z E                   	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-04-10T13:05:25.7288090Z E                   	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-04-10T13:05:25.7288679Z E                   	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2020-04-10T13:05:25.7289260Z E                   	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2020-04-10T13:05:25.7289790Z E                   	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-04-10T13:05:25.7290372Z E                   	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-04-10T13:05:25.7290942Z E                   	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-04-10T13:05:25.7291477Z E                   	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-04-10T13:05:25.7292000Z E                   	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-10T13:05:25.7292640Z E                   	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-10T13:05:25.7293237Z E                   Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-04-10T13:05:25.7293922Z E                   	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:112)
2020-04-10T13:05:25.7294717Z E                   	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-04-10T13:05:25.7295505Z E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:189)
2020-04-10T13:05:25.7296138Z E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:183)
2020-04-10T13:05:25.7296934Z E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:177)
2020-04-10T13:05:25.7297700Z E                   	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:497)
2020-04-10T13:05:25.7298415Z E                   	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:384)
2020-04-10T13:05:25.7298933Z E                   	at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)
2020-04-10T13:05:25.7299428Z E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-04-10T13:05:25.7299950Z E                   	at java.lang.reflect.Method.invoke(Method.java:498)
2020-04-10T13:05:25.7300468Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-04-10T13:05:25.7301072Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-04-10T13:05:25.7301695Z E                   	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-04-10T13:05:25.7302338Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-04-10T13:05:25.7302886Z E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-04-10T13:05:25.7303385Z E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-04-10T13:05:25.7303872Z E                   	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-04-10T13:05:25.7304396Z E                   	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-04-10T13:05:25.7304902Z E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-04-10T13:05:25.7305487Z E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-04-10T13:05:25.7305991Z E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-04-10T13:05:25.7306481Z E                   	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-04-10T13:05:25.7307236Z E                   	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-04-10T13:05:25.7307725Z E                   	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-04-10T13:05:25.7308191Z E                   	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-04-10T13:05:25.7308637Z E                   	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-04-10T13:05:25.7309083Z E                   	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-04-10T13:05:25.7309523Z E                   	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-04-10T13:05:25.7309849Z E                   	... 4 more
2020-04-10T13:05:25.7311264Z E                   Caused by: java.lang.RuntimeException: Failed to create stage bundle factory! INFO:root:Initializing python harness: /__w/3/s/flink-python/pyflink/fn_execution/boot.py --id=15-1 --logging_endpoint=localhost:39057 --artifact_endpoint=localhost:32894 --provision_endpoint=localhost:37718 --control_endpoint=localhost:40214
2020-04-10T13:05:25.7313317Z E                   INFO:root:Run command: /__w/3/s/flink-python/.tox/py37/bin/python -m pip install --ignore-installed -r /tmp/blobStore-ac8e0f93-b8c7-47f7-a367-de17ad11dd74/job_00cbf232f30575e95b57561d7acbcbb0/blob_p-36ae24f7f10f4119f242d182384524abe3e0b284-fe9ee346d25f0564b02f2773d9520e29 --prefix /tmp/python-dist-e077c0d2-17c9-4590-81fc-724ff15f4552/python-requirements
2020-04-10T13:05:25.7314128Z E                   
2020-04-10T13:05:25.7315036Z E                   Collecting cloudpickle==1.2.2 (from -r /tmp/blobStore-ac8e0f93-b8c7-47f7-a367-de17ad11dd74/job_00cbf232f30575e95b57561d7acbcbb0/blob_p-36ae24f7f10f4119f242d182384524abe3e0b284-fe9ee346d25f0564b02f2773d9520e29 (line 1))
2020-04-10T13:05:25.7316644Z E                     Could not find a version that satisfies the requirement cloudpickle==1.2.2 (from -r /tmp/blobStore-ac8e0f93-b8c7-47f7-a367-de17ad11dd74/job_00cbf232f30575e95b57561d7acbcbb0/blob_p-36ae24f7f10f4119f242d182384524abe3e0b284-fe9ee346d25f0564b02f2773d9520e29 (line 1)) (from versions: )
2020-04-10T13:05:25.7318575Z E                   No matching distribution found for cloudpickle==1.2.2 (from -r /tmp/blobStore-ac8e0f93-b8c7-47f7-a367-de17ad11dd74/job_00cbf232f30575e95b57561d7acbcbb0/blob_p-36ae24f7f10f4119f242d182384524abe3e0b284-fe9ee346d25f0564b02f2773d9520e29 (line 1))
2020-04-10T13:05:25.7319332Z E                   You are using pip version 10.0.1, however version 20.0.2 is available.
2020-04-10T13:05:25.7319953Z E                   You should consider upgrading via the 'pip install --upgrade pip' command.
2020-04-10T13:05:25.7320363Z E                   Traceback (most recent call last):
2020-04-10T13:05:25.7320978Z E                     File ""/__w/3/s/flink-python/dev/.conda/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
2020-04-10T13:05:25.7321415Z E                       ""__main__"", mod_spec)
2020-04-10T13:05:25.7322006Z E                     File ""/__w/3/s/flink-python/dev/.conda/lib/python3.7/runpy.py"", line 85, in _run_code
2020-04-10T13:05:25.7322403Z E                       exec(code, run_globals)
2020-04-10T13:05:25.7322986Z E                     File ""/__w/3/s/flink-python/pyflink/fn_execution/boot.py"", line 213, in <module>
2020-04-10T13:05:25.7323375Z E                       pip_install_requirements()
2020-04-10T13:05:25.7323992Z E                     File ""/__w/3/s/flink-python/pyflink/fn_execution/boot.py"", line 121, in pip_install_requirements
2020-04-10T13:05:25.7324462Z E                       ("" "".join(pip_install_commands), exit_code))
2020-04-10T13:05:25.7325920Z E                   Exception: Run command: /__w/3/s/flink-python/.tox/py37/bin/python -m pip install --ignore-installed -r /tmp/blobStore-ac8e0f93-b8c7-47f7-a367-de17ad11dd74/job_00cbf232f30575e95b57561d7acbcbb0/blob_p-36ae24f7f10f4119f242d182384524abe3e0b284-fe9ee346d25f0564b02f2773d9520e29 --prefix /tmp/python-dist-e077c0d2-17c9-4590-81fc-724ff15f4552/python-requirements error! exit code: 1
2020-04-10T13:05:25.7326930Z E                   
2020-04-10T13:05:25.7327638Z E                   	at org.apache.flink.python.AbstractPythonFunctionRunner.createStageBundleFactory(AbstractPythonFunctionRunner.java:197)
2020-04-10T13:05:25.7328311Z E                   	at org.apache.flink.python.AbstractPythonFunctionRunner.open(AbstractPythonFunctionRunner.java:164)
2020-04-10T13:05:25.7329060Z E                   	at org.apache.flink.table.runtime.runners.python.scalar.AbstractGeneralPythonScalarFunctionRunner.open(AbstractGeneralPythonScalarFunctionRunner.java:65)
2020-04-10T13:05:25.7329964Z E                   	at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator$ProjectUdfInputPythonScalarFunctionRunner.open(AbstractStatelessFunctionOperator.java:186)
2020-04-10T13:05:25.7330906Z E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:142)
2020-04-10T13:05:25.7331676Z E                   	at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.open(AbstractStatelessFunctionOperator.java:131)
2020-04-10T13:05:25.7332543Z E                   	at org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.open(AbstractPythonScalarFunctionOperator.java:88)
2020-04-10T13:05:25.7333381Z E                   	at org.apache.flink.table.runtime.operators.python.scalar.AbstractBaseRowPythonScalarFunctionOperator.open(AbstractBaseRowPythonScalarFunctionOperator.java:80)
2020-04-10T13:05:25.7334227Z E                   	at org.apache.flink.table.runtime.operators.python.scalar.BaseRowPythonScalarFunctionOperator.open(BaseRowPythonScalarFunctionOperator.java:64)
2020-04-10T13:05:25.7334983Z E                   	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:294)
2020-04-10T13:05:25.7335707Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:445)
2020-04-10T13:05:25.7336527Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
2020-04-10T13:05:25.7337401Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:441)
2020-04-10T13:05:25.7337953Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:462)
2020-04-10T13:05:25.7338492Z E                   	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:718)
2020-04-10T13:05:25.7338973Z E                   	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:542)
2020-04-10T13:05:25.7339424Z E                   	at java.lang.Thread.run(Thread.java:748)
2020-04-10T13:05:25.7340098Z E                   Caused by: org.apache.beam.vendor.guava.v26_0_jre.com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Process died with exit code 0
2020-04-10T13:05:25.7340908Z E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2050)
2020-04-10T13:05:25.7341593Z E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.get(LocalCache.java:3952)
2020-04-10T13:05:25.7342253Z E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974)
2020-04-10T13:05:25.7342958Z E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)
2020-04-10T13:05:25.7343715Z E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4964)
2020-04-10T13:05:25.7344482Z E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.<init>(DefaultJobBundleFactory.java:331)
2020-04-10T13:05:25.7345342Z E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.<init>(DefaultJobBundleFactory.java:320)
2020-04-10T13:05:25.7346088Z E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory.forStage(DefaultJobBundleFactory.java:250)
2020-04-10T13:05:25.7346778Z E                   	at org.apache.flink.python.AbstractPythonFunctionRunner.createStageBundleFactory(AbstractPythonFunctionRunner.java:195)
2020-04-10T13:05:25.7347380Z E                   	... 16 more
2020-04-10T13:05:25.7347751Z E                   Caused by: java.lang.IllegalStateException: Process died with exit code 0
2020-04-10T13:05:25.7348374Z E                   	at org.apache.beam.runners.fnexecution.environment.ProcessManager$RunningProcess.isAliveOrThrow(ProcessManager.java:72)
2020-04-10T13:05:25.7349245Z E                   	at org.apache.beam.runners.fnexecution.environment.ProcessEnvironmentFactory.createEnvironment(ProcessEnvironmentFactory.java:137)
2020-04-10T13:05:25.7349959Z E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:200)
2020-04-10T13:05:25.7350650Z E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:184)
2020-04-10T13:05:25.7351381Z E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3528)
2020-04-10T13:05:25.7352105Z E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2277)
2020-04-10T13:05:25.7352834Z E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)
2020-04-10T13:05:25.7353547Z E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2044)
2020-04-10T13:05:25.7354141Z E                   	... 24 more
2020-04-10T13:05:25.7354303Z 
2020-04-10T13:05:25.7354844Z .tox/py37/lib/python3.7/site-packages/py4j/protocol.py:328: Py4JJavaError{code}",,dian.fu,zhongwei,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 08 01:32:41 UTC 2020,,,,,,,,,,"0|z0dj4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/20 01:25;dian.fu;Thanks [~zjwang] for reporting this issue. Will take a look!;;;","12/Apr/20 02:04;dian.fu;Update: there are two test failures: BlinkStreamDependencyTests and StreamPandasUDFITTests, however, the root cause is the test failure of BlinkStreamDependencyTests. The collected execution results of StreamPandasUDFITTests(which is stored in a static field at the client side) is mixed with the execution results of BlinkStreamDependencyTests as the execution results of BlinkStreamDependencyTests is not cleared correctly when it fails and this cause the failure of StreamPandasUDFITTests. (Maybe we should improve the tests to avoid this kind of confuse).;;;","12/Apr/20 02:06;dian.fu;cc [~zhongwei] ;;;","13/Apr/20 06:05;zhongwei;[~zjwang] [~dian.fu] The root cause in BlinkStreamDependencyTests looks like a network problem. I'll improve this test case to prevent it from failing due to network reasons.;;;","13/Apr/20 06:06;dian.fu;Thanks [~zhongwei]!;;;","28/Apr/20 01:52;dian.fu;Another instance: https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/333/logs/144;;;","08/May/20 01:32;dian.fu;Merged to
- master via 967b89c63d07ef366bf319461763c27c77943b0e
- release-1.10 via 62c058984d4212a70c6b61c8ca09052fa90ce8c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroRow(De)SerializationSchema doesn't support new Timestamp conversion classes,FLINK-17091,13297738,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Paul Lin,Paul Lin,Paul Lin,11/Apr/20 05:36,19/Feb/21 07:26,13/Jul/23 08:07,24/Sep/20 06:34,1.10.0,,,,,1.11.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,"AvroRow(De)SerializationSchema doesn't know how to convert the new physical classes of Timestamp (eg. java.time.Date) to/from Avro's int/long based timestamp. Currently, when encountering objects of the new physical classes, AvroRow(De)SerializationSchema just ignores them and passes them to Avro's GenericDatumWriter/Reader, which leads to ClassCastException thrown by GenericDatumWriter/Reader. See [AvroRowSerializationSchema|https://github.com/apache/flink/blob/master/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowSerializationSchema.java#L251].

To fix this problem, we should support LocalTime/LocalDate/LocalDateTime conversion to int/long in AvroRowSerializationSchema, and support int/long conversion to LocalTime/LocalDate/LocalDateTime based on logical types(Types.LOCAL_TIME/Types.LOCAL_DATE/Types.LOCAL_DATE_TIME) in AvroRowDeserializationSchema.",,dwysakowicz,jark,libenchao,lzljs3620320,Paul Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14645,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 24 06:33:59 UTC 2020,,,,,,,,,,"0|z0diu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/20 05:39;Paul Lin;Working on the fix. Please assign the issue to me.;;;","10/May/20 13:28;Paul Lin;[~jark] May I ask your opinion on this issue? There's some similar issue reported recently, like FLINK-17478.;;;","12/May/20 06:33;Paul Lin;cc [~dwysakowicz] [~twalthr] ;;;","12/May/20 06:44;dwysakowicz;Hi [~Paul Lin] what you are suggesting would fix this particular case. However this is only patching for that particular case (probably worth doing anyway). The problem how I see it is that the type returned by the source/deserializationschema does not correspond to what is actually supported/expected. 

The source that uses {{AvroRowSerializationSchema}} should return {{DataTypes.Timestamp().bridgedTo(long.class)}} but it returns {{DataTypes.Timestamp()}} which defaults to {{LocalDateTime}} as a bridging class. In other words the {{AvroRowDeSerialization}}/source that uses it, completely ignores the bridging class mechanism.;;;","12/May/20 07:18;Paul Lin;Regarding the bridging class mechanism, I agree that it should be fixed. I've previously run into a related issue, FLINK-16693 FYI.  IMHO, the root cause is that currently `DescriptorProperties` does not carry the information about the conversion class of DataType, so even if the source declares `DataTypes.Timestamp().bridgedTo(long.class)`, the TableSourceFactory would get `DataTypes.Timestamp()`. 

 

But I think the bridging class mechanism is not closely related in this case. Because now the valid conversion classes of Timestamp are LocalDateTime and java.sql.Timestamp but not long, so we still need AvroRow(De)SerializationSchema to do the LocalDateTime/java.sql.Timestamp to/from long conversion.  What do you think? [~dwysakowicz];;;","12/May/20 07:42;dwysakowicz;Good point that {{Timestamp}} type does not support {{long}} as the conversion class.  Actually I should've said that the {{AvroRowSerializationSchema}} in current implementation should return {{DataTypes.Timestamp().bridgedTo(java.sql.Timestamp.class)}}.

I don't think the problem lays in {{DescriptorProperties}}. What the {{TableSourceFactory}} receives is a requested logical schema (that's the case e.g. from DDL). It should be the responsibility of the source+schema to enrich that with the bridging classes the schema actually produces for those logical types. By the way I think this mechanism will be completely revamped in FLIP-95

I don't think adding a conversion between  {{LocalDateTime/java.sql.Timestamp}}, {{long }} is correct. There is a reason why {{TIMESTAMP}} does not have that conversion. The correct type in this case would be {{TIMESTAMP_WITH_LOCAL_TIME_ZONE}}.;;;","12/May/20 08:14;Paul Lin;Maybe I could add more background information. Avro requires long-based timestamps for serialization, but the query result would be Timestamp backed by LocalDateTime or java.sql.Timestamp, so we need to convert them to long in AvroRowSerializationSchema. That's the way it always has been doing.

The problem is that previous it only needs to deal with java.sql.Timestamp (see [https://github.com/apache/flink/blob/master/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowSerializationSchema.java#L251]), but after FLINK-14645 it also needs to deal with LocalDateTime.

The conversion I proposed is not for the generation use case of SQL data types, but just for the serialization of Avro, to bridge between the physical classes of the SQL data types and the classes that a specific serializer requires. That's why the conversion happens in AvroRow(De)SerializationSchema. ;;;","12/May/20 08:30;Paul Lin;{{> AvroRowSerializationSchema}} in current implementation should return {{DataTypes.Timestamp().bridgedTo(java.sql.Timestamp.class)}}.

If the bridging class machanism is respected, forcing  AvroRowSerializationSchema to use java.sql.Timestamp as the physical class may fix the problem for now, but I think java.sql.Timestamp would be deprecated in the future (for the precision reason), and finally we have to support LocalDateTime.;;;","12/May/20 08:37;lzljs3620320;Thanks [~dwysakowicz] and [~Paul Lin] for your discussion.

[~Paul Lin] 's explanation sounds good to me. Parquet also support long-based timestamps. And we should convert this long to LocalDateTime/TimestampData in avro schema before return to planner.

[~libenchao] is doing new FLIP-95 Avro schema based on RowData(set TimestampData for timestamp type) (FLINK-17526), FYI.;;;","12/May/20 09:22;dwysakowicz;First of all lets use the full type names as we have three different timestamp types:
* {{TIMESTAMP_WITH_LOCAL_TIME}} - this has an {{Instant}} semantics and thus can be mapped to {{long}}
* {{TIMESTAMP_WITHOUT_TIME_ZONE/TIMESTAMP}} - this has  a {{LocalDateTime}} semantics and *can not* be mapped to {{long}}
* {{TIMESTAMP_WITH_TIME_ZONE}} - this has an {{OffsetDateTime}} semantics and *can not* be mapped to {{long}}

I think it's vital to properly support this types in sources. This is very nicely explained in this document: https://docs.google.com/document/d/1gNRww9mZJcHvUDCXklzjFEQGpefsuR_akCDfWsdE35Q
This effort is already being implemented into multiple hadoop formats see e.g. https://issues.apache.org/jira/browse/AVRO-2328 

With this introduction I agree we will need to support mapping of e.g. {{long, logicalType: local-timestamp-micros}} to {{LocalDateTime}} or {{long, logicalType: timestamp-micros}} to {{Instant}}.
My argument is we do not need to support all the conversion between all the possible conversion classes. The source just tells which conversion class does it use for a particular logical type.

Example:
(long-term} If a column was created with a DDL: {{colName TIMESTAMP(3)}} than the source should imply the field in avro is of type {{long, logicalType: local-timestamp-micros}} and it should produce {{LocalDateTime}}. It does not need to be able to produce also the {{java.sql.Timestamp}}.

Currently it uses the {{java.sql.Timestamp}} conversion class for a {{TIMESTAMP(3)}} and in my opinion the problem is this information flow is not respected/inverted.

As a last comment, as I already said in my first reply its probably worth adding a case:
{code}
case LONG:
	if (info == Types.SQL_TIMESTAMP) {
		return convertToTimestamp(object);
	}
	if (info == Types.LOCAL_DATE_TIME) {
		return convertToTimestamp(object);
	}
	return object;
{code}
or at least it will not harm. I still think this is -not a correct long-term solution but- just a patch for the current mishandling of the conversion classes, which should not be necessary.;;;","12/May/20 10:19;Paul Lin;[~lzljs3620320] Thanks a lot for the information!

[~dwysakowicz] Thanks a lot for the very detailed explanation!  Now I understand your point. I think with the new Blink planner Timestamp will always be bridged to LocalDateTime (correct me if I'm wrong), and we keep java.sql.Timestamp just for compatibility with the old planner. When fully migrated to the new planner, we can safely clean up the codes for java.sql.Timestamp.;;;","29/May/20 06:24;Paul Lin;Seems that we have reached a consensus. Could you assign this to me, so that I can create a PR? [~dwysakowicz] [~lzljs3620320];;;","29/May/20 13:21;jark;What's the purpose of this issue? Does it just want to fix for 1.10.x? 
I have this question because we introduced the new {{AvroRowDataDeserializationSchema}} which should already fix this problem.;;;","30/May/20 03:37;Paul Lin;[~jark] I checked AvroRowDataDeserializationSchema, and yes, the problem should be fixed in it. Is AvroRow(De)SerializationSchema going to be deprecated in 1.11? In that case, this fix should only apply to 1.10.x.;;;","24/Sep/20 06:33;Paul Lin;Should be fixed by `a479aee9131bebc67c1eb512542eb7c8f1e54233`.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink sql client not able to read parquet hive table because  `HiveMapredSplitReader` not supports name mapping reading for parquet format.,FLINK-17086,13297589,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,leiwangouc,leiwangouc,10/Apr/20 09:53,21/May/20 08:23,13/Jul/23 08:07,21/May/20 08:23,1.10.0,,,,,,,,,Connectors / Hive,,,,,0,,,,,"When writing hive table with parquet format, flink sql client not able to read it correctly because HiveMapredSplitReader not supports name mapping reading for parquet format.

[http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/fink-sql-client-not-able-to-read-parquet-format-table-td34119.html]",,leiwangouc,leonard,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16802,FLINK-17474,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 08:23:28 UTC 2020,,,,,,,,,,"0|z0dhww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/20 05:37;lzljs3620320;CC: [~lirui];;;","13/Apr/20 06:11;lirui;Hi [~leiwangouc], thanks for reporting the issue. Let me try to understand it. So given the same underlying parquet file, the column order defined in DDL doesn't matter in Hive but matters in Flink. For example, you can either {{CREATE TABLE `robotparquet`(  `robotid` int,  `robottime` bigint )}}, or {{CREATE TABLE `robotparquet`(  `robottime` bigint,   `robotid` int)}} in Hive, and both tables will return the correct data for columns {{robottime}} and {{robotid}}. But you cannot do the same in Flink. Is that right?;;;","13/Apr/20 12:19;leiwangouc;Hi  [~lirui]， Your understanding is right. 

Hive client will work well under both ddl statement.

Flink SQL client only work  under one ddl statement.  Under another there's error: 

SQL statement. Reason:
java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.LongWritable

 

Also take attentin the way the parquet file is written. 

I write a class called RobotData and there only two fields：robotId， robotTime  and using StreamingFileSink to write to hdfs： 

StreamingFileSink
 .forBulkFormat(new Path(""hdfs://namenode:8020/user/abc/parquet""),
 ParquetAvroWriters.forReflectRecord(RobotData.class)).build();;;;","14/Apr/20 07:26;lirui;Hey [~leiwangouc], thanks for the clarifications. I think FLINK-16802 will help fix the problem. I'll submit a PR for that ticket shortly.;;;","24/Apr/20 03:39;lirui;Hi [~leiwangouc], FLINK-16802 has been fixed and you can try whether that fixes the issue.;;;","30/Apr/20 04:35;leiwangouc;Hi [~lirui]， I build package from the latest code from flink github and test it .  There's new error:  

 

select * from robotparquet

e SQL statement. Reason:
org.apache.flink.shaded.org.apache.parquet.io.InvalidRecordException: robottime not found in message com.geekplus.robotdata.parser.RobotUploadDataTest {
 required int32 robotId;
 required int64 robotTime;
}

Seems it is a case sensitive issue. 

The parquet data is written by java. The field name is case sensitive.

But hive is case insensitive.

 ;;;","30/Apr/20 06:45;lirui;[~leiwangouc], the latest code by default uses vectorized reader for parquet tables, and I think the vectorized reader is case-sensitive at the moment. You can set {{table.exec.hive.fallback-mapred-reader=true}} to fall back to the MR reader and have a try.;;;","30/Apr/20 07:00;lzljs3620320;vectorized reader is case sensitive.

[~leiwangouc] It is a good topic to support case insensitive.;;;","30/Apr/20 08:09;leiwangouc;[~lirui]   Add  table.exec.hive.fallback-mapred-reader: true    in  conf/flink-conf.yaml and tested it again.

It is correct now.  flink sql client works under both ddl statement. 

Although i don't know how ""table.exec.hive.fallback-mapred-reader: true""  affect it.  

 

 ;;;","30/Apr/20 08:41;lirui;[~leiwangouc] Glad to know it worked.

For Orc and Parquet tables, we have vectorized and non-vectorized readers. Setting ""table.exec.hive.fallback-mapred-reader: true"" will force use the non-vectorized reader.
In general, non-vectorized reader provides better compatibility with Hive, but is less performant than the vectorized one. So I suggest use it only as a workaround when the vectorized reader doesn't meet your needs. We'll make the vectorized reader case-insensitive too in the future.;;;","30/Apr/20 08:45;lzljs3620320;Create FLINK-17474 for tracking this case insensitive. FYI;;;","19/May/20 08:35;lzljs3620320;FLINK-17474 will be fixed in 1.11;;;","21/May/20 08:23;lzljs3620320;Hi [~leiwangouc], related issues have been fixed, you can re-try Flink 1.11. Close this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utils.CollectHelper is possible to throw NPE if the sink is up but not initialized when the job fails,FLINK-17080,13297553,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,10/Apr/20 07:23,15/Apr/20 07:24,13/Jul/23 08:07,15/Apr/20 07:24,1.11.0,,,,,1.11.0,,,,API / DataSet,,,,,0,pull-request-available,,,,"When the sink using {{Utils.CollectHelper}} is up but not initialized and the job fails due to other operators, it is possible that the {{close()}} method of {{Utils.CollectHelper}} is called when {{open()}} is not called.

Currently {{Utils.CollectHelper}} does not deal with this case and will throw NPE.",,jark,TsReaper,,,,,,,,,,,,,,,,,,,,"wuchong commented on pull request #11688: [FLINK-17080] Fix possible NPE in Utils.CollectHelper
URL: https://github.com/apache/flink/pull/11688
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/20 07:24;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16636,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 07:24:38 UTC 2020,,,,,,,,,,"0|z0dhow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/20 07:24;jark;Fixed in master (1.11.0): 0fa08304a5bb59744aa378d297e66df180e12519;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slow checkpoint cleanup causing OOMs,FLINK-17073,13297397,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,trohrmann,trohrmann,09/Apr/20 14:24,28/May/21 07:00,13/Jul/23 08:07,14/Oct/20 14:27,1.10.0,1.11.0,1.7.3,1.8.0,1.9.0,1.12.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"A user reported that he sees a decline in checkpoint cleanup speed when upgrading from Flink 1.7.2 to 1.10.0. The result is that a lot of cleanup tasks are waiting in the execution queue occupying memory. Ultimately, the JM process dies with an OOM.

Compared to Flink 1.7.2, we introduced a dedicated {{ioExecutor}} which is used by the {{HighAvailabilityServices}} (FLINK-11851). Before, we use the {{AkkaRpcService}} thread pool which was a {{ForkJoinPool}} with a max parallelism of 64. Now it is a {{FixedThreadPool}} with as many threads as CPU cores. This change might have caused the decline in completed checkpoint discard throughput. This suspicion needs to be validated before trying to fix it!

[1] https://lists.apache.org/thread.html/r390e5d775878918edca0b6c9f18de96f828c266a888e34ed30ce8494%40%3Cuser.flink.apache.org%3E",,dian.fu,echauchot,felixzheng,godfreyhe,kezhuw,klion26,liyu,Paul Lin,pnowojski,roman,SleePy,stevenz3wu,trohrmann,wind_ljy,yunta,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20992,,,,FLINK-17248,FLINK-17860,FLINK-17421,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 14 14:27:58 UTC 2020,,,,,,,,,,"0|z0dgq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/20 12:54;chesnay;Seems like the underlying issue is that we aren't limiting the number of tasks that can be queued up (which I _think_ would implicitly slow down checkpointing since it would delay the completion of a pending checkpoint). Ignoring potential architectural problems, this should make the system way more resilient to these kind of issues than an increase of the pool size would.;;;","15/Apr/20 13:39;trohrmann;I agree, but this would require bigger architectural changes. In the meantime I would suggest to make the number of IO threads configurable for the user. That way, users can work around this problem until the proper fix has been put in place.;;;","15/Apr/20 14:06;trohrmann;Maybe we could create a related issue which introduces the configuration option and keep this one for the proper fix which might entail to throttle the checkpoint throughput based on the cleanup backlog.;;;","15/Apr/20 14:50;yunta;I prefer to the configuration solution to keep the behavior the same as before. Current Flink architecture cannot totally prevent this problem if the speed of creating checkpoint larger than the speed of deleting previous checkpoints. Increase the pool size could not prevent this but only mitigate the possibility.;;;","27/Apr/20 16:55;pnowojski;I haven't analysed the issue, so I'm not sure if indeed backpressuring is the right thing to do, but assuming that is the case, it's not a bug fix, but an improvement/new feature, so I've created another ticket for that FLINK-17421.

Btw, IMO FLINK-17248 is a duplicate of this ticket, as it's fixing regression reported in this issue.;;;","28/Apr/20 08:16;trohrmann;I would like to keep this ticket and FLINK-17248 separate. The reason is that I believe that the latter will just mitigate the problem and the true bug is that we don't limit the number of concurrent checkpoint clean up tasks. In that sense, it has always been broken. Moreover, this ticket is not about a regression we want to fix. The underlying problem only surfaced due to changing the executor.

What we can do is to say that FLINK-17421 will fix this bug here. Hence, once FLINK-17421 is closed, we can close this bug issue.;;;","04/May/20 08:28;pnowojski;As it's a rare issue that in this particular case will have a workaround (increasing number of threads in the thread pool), I'm reducing the priority. Also because of other independent efforts in the {{CheckpointCoordinator}} code, it's unlikely the bug fix will be back-ported to previous releases.;;;","15/May/20 13:17;echauchot;Hi, I just started contributing to Flink. I'd like to take a look at this subject as it can be a good introduction to Flink architecture IMHO. As the temporary workaround discussed above has been implemented [here|[https://github.com/apache/flink/pull/11957]] maybe it is time to tackle the above subject. One thing I wonder is: if we want to limit the number of CompletedCheckpoints submitted to the IOExecutor for cleaning, what happens if 

_ZooKeeperCompletedCheckpointStore_  tries to submit a new CompletedCheckpoint when the limit has already been reached ? Shall it delay the submission waiting for the current number of submitted tasks to decrease?;;;","15/May/20 13:52;trohrmann;Hi [~echauchot], I think what needs to happen is that we backpressure the whole checkpointing mechanism if the cleanup cannot keep up with it. This means that we don't trigger new checkpoints. This is far from trivial to realize, though, and on top of it, it needs to be properly designed. I'm not entirely sure whether this is really a good task to get started with.;;;","15/May/20 14:02;echauchot;ok, fair enough, I'll pick another one.;;;","26/May/20 11:48;echauchot;Anyway I'd like to take part to the design discussions regarding checkpoint backpressure in order to learn about these topics.;;;","24/Jun/20 09:03;echauchot;[~trohrmann]  I'm thinking about this issue, I'll start a first design doc for the checkpoint backpressure system. I'll send it here and on the ML.;;;","25/Jun/20 07:31;trohrmann;Yes, your help is highly appreciated. Given that this is not a trivial problem I would suggest to sync with [~pnowojski] and [~SleePy] about the next steps.;;;","25/Jun/20 07:48;echauchot;[~trohrmann], I wrote [this|[https://s.apache.org/checkpoint-backpressure]] FLIP style design document for checkpoint backpressure. Can you tell me what you think? Also I don't have the rights to create FLIP design documents in flink confluence workspace so I did the FLIP in a google doc. Can you give me the rights?;;;","25/Jun/20 08:03;echauchot;I'm always glad to help ! No problem about syncing with [~pnowojski] and [~SleePy]. Not an trivial problem indeed, but I figured out that it could be a good way to learn quite a lot about Flink internals :);;;","03/Jul/20 07:29;pnowojski;Hey [~echauchot], I'm OoO since last two weeks and will be back on July 13th. Can we sync offline on this issue?;;;","05/Jul/20 16:46;SleePy;Hi [~echauchot], thanks for doing so much. I left a couple of comments in design doc. The second proposal seems to be a reliable and light solution :);;;","06/Jul/20 08:46;echauchot;[~pnowojski] no problem. Sorry for bothering you during your days off :) . [~SleePy] thanks for reviewing this doc, I'll look at your comments.;;;","20/Jul/20 15:17;echauchot;[~SleePy] can you assign this ticket to me please ?;;;","21/Jul/20 01:56;SleePy;[~echauchot], sorry I don't have the authorization of issue assignment. [~pnowojski], could you help to assign the ticket to him?;;;","21/Jul/20 04:55;zjwang;I have assigned it to [~echauchot];;;","21/Jul/20 09:07;echauchot;thanks guys !;;;","21/Jul/20 15:32;roman;I think an alternative (or complementary) temporary solution is to use a bounded queue when creating ioExecutor.

This way, we solve this issue and also possible others, which we don't account for in design doc.;;;","21/Jul/20 16:48;roman;As for the long-term solution, I'd propose the following:
 # Extract *ZooKeeperCompletedCheckpointStore.tryRemoveCompletedCheckpoint* (along with *executor*) to a new class, e.g. *CheckpointCleaner* that maintains a queue of checkpoints to remove
 # On removal completion, it calls *CheckpointCoordinator*.timer.execute(*executeQueuedRequest*)
 # In *CheckpointRequestDecider.chooseRequestToExecute*, check *CheckpointCleaner.numberOfCheckpointsToRemove* and return if it's greater than the threshold (see below) 
 # *CheckpointCleaner* reports **this count in a thread-safe manner (performance isn't an issue here)

 

This way, we ensure these properties:
 # if checkpoint can't be started, it's queued (if the queue has space) but not started
 # once a subsumed checkpoint is removed, we check the queue, and if possible, start the checkpoint 

It's possible that we check the queue twice (1st in chooseRequestToExecute, 2nd in CheckpointCleaner), but that's OK, we'll just execute the next request if possible.

 

Regarding adding an additional configuration parameter for the threshold, I don't see much value in it. Conceptually, we don't want to proceed as long as there are checkpoints to remove from the previous completion.

So we can use max-concurrent-checkpoints as a threshold (maybe multiplied by some constant factor to account for spikes and savepoints). 

 

What do you think [~echauchot], [~SleePy]?;;;","24/Jul/20 20:31;echauchot;Hi [~roman_khachatryan],

thanks for the suggestions! Overall this is what I intended to do modulo these minor things:

In 1. I meant to use the _CompletedCheckpoints_ dequeue to keep track of the checkpoints to clean and avoid adding a new queue

In 2. Yes indeed, it needs to call _executeQueuedRequest,_ but it needs also not to call it when the previous checkpoint cleaning is not done (still need to figure out how to sync cleaning with work in CheckpointCoordinator) so that checkpoint cleaning becomes part of the checkpoint process and not a side fire-and-forget process. This behavior will be configurable to avoid lowering checkpoint rate when CP cleaning rate is not a problem. Once cleaning is part of the standard checkpointing process, checking in flight checkpoints will tell how many potential cleaning checkpoints there are and if there are too much, drop any new CP trigger request.

In 3. yes that is what I meant in ""drop any new CP trigger request"" above.

In 4.  I'm not clear yet about concurrency in checkpointing.  ;;;","27/Jul/20 07:59;SleePy;To [~roman_khachatryan], thanks for nice suggestions!

{quote}I think an alternative (or complementary) temporary solution is to use a bounded queue when creating ioExecutor.{quote}
I'm not a fan of this temporary solution. We have to consider how to treat the invoker which launches asynchronous IO operations through {{ioExecutor}} if the queue is full. Make them failed or wait till there is some space available? I'm afraid it's not a small work to review all the places calls {{ioExecutor}}. If we want a temporary solution, maybe we could just increase the thread count. 

Regarding to the long-term solution. Actually Etienne and me have not discuss many of the implementation details. I just gave some suggestions to make sure it's in the right direction. It's cool to have your detailed suggestions. It may help a lot for the contributor who is not familiar with this part. I just thought we don't have to discuss too much details here. It might be better to give contributor more free space. We could pay more attention on code review to guarantee it's correct and reasonable.

BTW, just a tiny suggestion, code refactoring is not necessary, we should focus on solving the issue first. After that, we could consider if we could do some refactoring to make the codes more readable or elegant. 

To [~echauchot], besides the implementation, is there any question about the plan? Please feel free to ask anything that you don't understand. ;;;","27/Jul/20 13:37;roman;Thanks [~echauchot], [~SleePy]!

[~SleePy], sure, I just shared my view of how it can be implemented.

[~echauchot], 

for (1) I don't think having a separate queue is an issue, rather the opposite (the class + thread manages its own work queue).

for (2) I think checking the aforementioned queue size (+ number of deletions in progress) is enough, isn't it?;;;","28/Jul/20 08:12;SleePy;BTW [~echauchot], before writing any codes, it would be great to write an implementation plan first. That's a better place to discuss implementation detail.
I heard some other guys are also interested in this issue. It would be helpful fo them to understand what is happening. Besides that, there would be some other PRs on {{CheckpointCoordinator}} at the same time. We have to make sure there would be no big conflict between these changes.;;;","28/Jul/20 09:57;echauchot;[~SleePy] sure, I'll update the google doc to add impl plan.;;;","28/Jul/20 14:56;echauchot;[~roman_khachatryan] 

When [~SleePy] and I discussed in [the deisgn doc|https://docs.google.com/document/d/1q0y0aWlJMoUWNW7jjsM8uWfHsy2dM6YmmcmhpQzgLMA/edit?usp=sharing], the idea was to wait until last checkpoint was cleaned before accepting another (that is what we called make cleaning part of checkpoint processing). Thus, checking only existing number of pending checkpoints was enough (no need for a new queue) to foresee an flood of checkpoints to clean. 

But the solution you propose (managing the queue of the checkpoints to clean and monitor its size) seems even simpler to me: it avoids having to sync normal checkpointing and checkpoint cleaning:

As you said, when we chose a checkpoint trigger request to execute (*CheckpointRequestDecider.chooseRequestToExecute*), we can drop new checkpoint requests when there are too many checkpoints to clean and thus regulate the whole checkpointing system. Syncing cleaning and checkpointing might not be necessary for this regulation, you're right.

If you don't mind, I'll go for this implementation proposal in the design doc.

[~roman_khachatryan] thanks anyway for the suggestions and please take a look at the design doc where we will have the impl discussions;;;","28/Jul/20 15:27;roman;Thanks for your analysis [~echauchot].
Sure, go ahead!;;;","31/Jul/20 14:15;echauchot;[~roman_khachatryan] [~SleePy] I just submitted the PR: [https://github.com/apache/flink/pull/13040] also please read the design doc's implementation plan comments for an explanation of the impl choices (responsibilities, interfaces, threshold, resume event ...)

 ;;;","03/Aug/20 12:54;SleePy;[~echauchot], sorry for the late reply. Thanks for pushing this!

I'm OK with [~roman_khachatryan]'s plan. It's simpler to implement in some aspects indeed. In my plan, we have to consider how to avoid synchronous cleaning which you mentioned. Because in the near future, {{CheckpointCoordinator}} would be no big lock anymore. 

{quote}...we can drop new checkpoint requests when there are too many checkpoints to clean...{quote}
I think we should take care of the cleaning for both successful checkpoint and failed checkpoint. 

I have left some comments in the doc.;;;","03/Aug/20 16:13;roman;Thanks for the PR [~echauchot] ,

I found that it significantly diverges both from [your design doc|#comment-17144726] and [my proposal above|#comment-17162168],
in that it doesn't have CheckpointCleaner and it doesn't trigger checkpoint request upon discard completion.

 ;;;","04/Aug/20 08:09;echauchot;Hi Roman, thanks for your feedback. Yes indeed it diverges in some points because while coding I figured out better responsibilities, coupling etc... The reasons are explained in the comments in the doc. Please take a look at them and tell me what you think.;;;","04/Aug/20 08:49;roman;Hi Etienne, 

You're right, I missed option 3.b in your design, which I think is implemented in the PR.

However, I'm not sure that we can use it. I've commented in the design doc, please take a look.;;;","11/Aug/20 13:55;echauchot;[@ifndef-SleePy|https://github.com/ifndef-SleePy] [@rkhachatryan|https://github.com/rkhachatryan] I applied all your comments of the design doc to the PR PTAL;;;","13/Aug/20 14:31;echauchot;Hi all, 

I'll be off for 2 weeks starting tonight, so I'll need to wait until my return to tackle these subjects.;;;","17/Sep/20 09:09;echauchot;Hi, I addressed all the comments in the PR and added an integration test. Is someone available for finishing review/merging while Roman is on vacation ?;;;","17/Sep/20 09:17;pnowojski;Hi [~echauchot], [sorry for the delay|https://twitter.com/schneems/status/1191844272682786822]. 

[~roman_khachatryan] should be back on Monday, can we postpone the review until then? I think it would be the most efficient to wait for him to finish this off. I can ping [~roman_khachatryan] to take a look at it as soon as he is back online.;;;","17/Sep/20 12:41;echauchot;Ah, sure !  I did not notice, he'll be back that soon.

Thanks.;;;","14/Oct/20 14:27;pnowojski;Merged to master as 81c7fa6f90..318c7b6659;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ERROR at teardown of TableConfigTests.test_get_set_decimal_context,FLINK-17068,13297334,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dian.fu,rmetzger,rmetzger,09/Apr/20 09:41,19/Apr/20 11:13,13/Jul/23 08:07,19/Apr/20 11:13,1.11.0,,,,,1.11.0,,,,API / Python,Tests,,,,0,pull-request-available,test-stability,,,"CI run: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7243&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=14487301-07d2-5d56-5690-6dfab9ffd4d9

{code}
2020-04-09T00:34:15.9084299Z ==================================== ERRORS ====================================
2020-04-09T00:34:15.9085728Z ______ ERROR at teardown of TableConfigTests.test_get_set_decimal_context ______
2020-04-09T00:34:15.9086216Z 
2020-04-09T00:34:15.9086725Z self = <contextlib._GeneratorContextManager object at 0x7f8d978989b0>
2020-04-09T00:34:15.9087144Z 
2020-04-09T00:34:15.9087457Z     def __enter__(self):
2020-04-09T00:34:15.9087787Z         try:
2020-04-09T00:34:15.9091929Z >           return next(self.gen)
2020-04-09T00:34:15.9092634Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9092863Z 
2020-04-09T00:34:15.9093134Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9093548Z __ ERROR at setup of TableConfigTests.test_get_set_idle_state_retention_time ___
2020-04-09T00:34:15.9093803Z 
2020-04-09T00:34:15.9094082Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c3f3da0>
2020-04-09T00:34:15.9094313Z 
2020-04-09T00:34:15.9094502Z     def __enter__(self):
2020-04-09T00:34:15.9094862Z         try:
2020-04-09T00:34:15.9095088Z >           return next(self.gen)
2020-04-09T00:34:15.9095707Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9095913Z 
2020-04-09T00:34:15.9096203Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9096818Z _ ERROR at teardown of TableConfigTests.test_get_set_idle_state_retention_time _
2020-04-09T00:34:15.9100686Z 
2020-04-09T00:34:15.9101687Z self = <contextlib._GeneratorContextManager object at 0x7f8d978d83c8>
2020-04-09T00:34:15.9102005Z 
2020-04-09T00:34:15.9102193Z     def __enter__(self):
2020-04-09T00:34:15.9102415Z         try:
2020-04-09T00:34:15.9102741Z >           return next(self.gen)
2020-04-09T00:34:15.9103144Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9103367Z 
2020-04-09T00:34:15.9103786Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9104185Z ________ ERROR at setup of TableConfigTests.test_get_set_local_timezone ________
2020-04-09T00:34:15.9104999Z 
2020-04-09T00:34:15.9105287Z self = <contextlib._GeneratorContextManager object at 0x7f8d979345f8>
2020-04-09T00:34:15.9105531Z 
2020-04-09T00:34:15.9105707Z     def __enter__(self):
2020-04-09T00:34:15.9105924Z         try:
2020-04-09T00:34:15.9106138Z >           return next(self.gen)
2020-04-09T00:34:15.9106555Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9106858Z 
2020-04-09T00:34:15.9107159Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9107675Z ______ ERROR at teardown of TableConfigTests.test_get_set_local_timezone _______
2020-04-09T00:34:15.9107983Z 
2020-04-09T00:34:15.9108350Z self = <contextlib._GeneratorContextManager object at 0x7f8d981f8240>
2020-04-09T00:34:15.9108699Z 
2020-04-09T00:34:15.9108983Z     def __enter__(self):
2020-04-09T00:34:15.9109311Z         try:
2020-04-09T00:34:15.9109566Z >           return next(self.gen)
2020-04-09T00:34:15.9109872Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9110082Z 
2020-04-09T00:34:15.9110349Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9111098Z __ ERROR at setup of TableConfigTests.test_get_set_max_generated_code_length ___
2020-04-09T00:34:15.9111479Z 
2020-04-09T00:34:15.9111740Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c3380f0>
2020-04-09T00:34:15.9112010Z 
2020-04-09T00:34:15.9112297Z     def __enter__(self):
2020-04-09T00:34:15.9112571Z         try:
2020-04-09T00:34:15.9112803Z >           return next(self.gen)
2020-04-09T00:34:15.9113114Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9113353Z 
2020-04-09T00:34:15.9113737Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9114282Z _ ERROR at teardown of TableConfigTests.test_get_set_max_generated_code_length _
2020-04-09T00:34:15.9114652Z 
2020-04-09T00:34:15.9114929Z self = <contextlib._GeneratorContextManager object at 0x7f8d9783b550>
2020-04-09T00:34:15.9115169Z 
2020-04-09T00:34:15.9115460Z     def __enter__(self):
2020-04-09T00:34:15.9115756Z         try:
2020-04-09T00:34:15.9115989Z >           return next(self.gen)
2020-04-09T00:34:15.9116279Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9116579Z 
2020-04-09T00:34:15.9116944Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9117387Z __________ ERROR at setup of TableConfigTests.test_get_set_null_check __________
2020-04-09T00:34:15.9117640Z 
2020-04-09T00:34:15.9117908Z self = <contextlib._GeneratorContextManager object at 0x7f8d9823a390>
2020-04-09T00:34:15.9118258Z 
2020-04-09T00:34:15.9118519Z     def __enter__(self):
2020-04-09T00:34:15.9118827Z         try:
2020-04-09T00:34:15.9119083Z >           return next(self.gen)
2020-04-09T00:34:15.9119531Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9119819Z 
2020-04-09T00:34:15.9120164Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9120827Z ________ ERROR at teardown of TableConfigTests.test_get_set_null_check _________
2020-04-09T00:34:15.9121505Z 
2020-04-09T00:34:15.9121923Z self = <contextlib._GeneratorContextManager object at 0x7f8d7a6ba978>
2020-04-09T00:34:15.9122165Z 
2020-04-09T00:34:15.9122343Z     def __enter__(self):
2020-04-09T00:34:15.9122554Z         try:
2020-04-09T00:34:15.9122787Z >           return next(self.gen)
2020-04-09T00:34:15.9123242Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9123458Z 
2020-04-09T00:34:15.9123726Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9124304Z _________ ERROR at setup of TableConfigTests.test_get_set_sql_dialect __________
2020-04-09T00:34:15.9124642Z 
2020-04-09T00:34:15.9124899Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c3739e8>
2020-04-09T00:34:15.9125262Z 
2020-04-09T00:34:15.9125514Z     def __enter__(self):
2020-04-09T00:34:15.9125854Z         try:
2020-04-09T00:34:15.9126071Z >           return next(self.gen)
2020-04-09T00:34:15.9126572Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9126890Z 
2020-04-09T00:34:15.9127300Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9127871Z ________ ERROR at teardown of TableConfigTests.test_get_set_sql_dialect ________
2020-04-09T00:34:15.9128278Z 
2020-04-09T00:34:15.9128544Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c365748>
2020-04-09T00:34:15.9128851Z 
2020-04-09T00:34:15.9129126Z     def __enter__(self):
2020-04-09T00:34:15.9129350Z         try:
2020-04-09T00:34:15.9129595Z >           return next(self.gen)
2020-04-09T00:34:15.9130021Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9130228Z 
2020-04-09T00:34:15.9130741Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9131424Z _______ ERROR at setup of TableConfigCompletenessTests.test_completeness _______
2020-04-09T00:34:15.9131820Z 
2020-04-09T00:34:15.9132103Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c3f3908>
2020-04-09T00:34:15.9132486Z 
2020-04-09T00:34:15.9132681Z     def __enter__(self):
2020-04-09T00:34:15.9132892Z         try:
2020-04-09T00:34:15.9133104Z >           return next(self.gen)
2020-04-09T00:34:15.9133409Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9133617Z 
2020-04-09T00:34:15.9134006Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9134674Z _____ ERROR at teardown of TableConfigCompletenessTests.test_completeness ______
2020-04-09T00:34:15.9135059Z 
2020-04-09T00:34:15.9135426Z self = <contextlib._GeneratorContextManager object at 0x7f8d978d8b00>
2020-04-09T00:34:15.9135726Z 
2020-04-09T00:34:15.9136010Z     def __enter__(self):
2020-04-09T00:34:15.9136212Z         try:
2020-04-09T00:34:15.9136440Z >           return next(self.gen)
2020-04-09T00:34:15.9136871Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9137101Z 
2020-04-09T00:34:15.9137350Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9137907Z _ ERROR at setup of StreamTableEnvironmentTests.test_create_table_environment __
2020-04-09T00:34:15.9138167Z 
2020-04-09T00:34:15.9138570Z self = <contextlib._GeneratorContextManager object at 0x7f8d97907630>
2020-04-09T00:34:15.9138852Z 
2020-04-09T00:34:15.9139045Z     def __enter__(self):
2020-04-09T00:34:15.9139244Z         try:
2020-04-09T00:34:15.9139611Z >           return next(self.gen)
2020-04-09T00:34:15.9140012Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9140236Z 
2020-04-09T00:34:15.9140665Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9141269Z _ ERROR at teardown of StreamTableEnvironmentTests.test_create_table_environment _
2020-04-09T00:34:15.9141527Z 
2020-04-09T00:34:15.9141801Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c63ab70>
2020-04-09T00:34:15.9142029Z 
2020-04-09T00:34:15.9142291Z     def __enter__(self):
2020-04-09T00:34:15.9142610Z         try:
2020-04-09T00:34:15.9142973Z >           return next(self.gen)
2020-04-09T00:34:15.9143456Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9143677Z 
2020-04-09T00:34:15.9143923Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9144502Z _ ERROR at setup of StreamTableEnvironmentTests.test_create_table_environment_with_blink_planner _
2020-04-09T00:34:15.9145034Z 
2020-04-09T00:34:15.9145370Z self = <contextlib._GeneratorContextManager object at 0x7f8d97898828>
2020-04-09T00:34:15.9145662Z 
2020-04-09T00:34:15.9145892Z     def __enter__(self):
2020-04-09T00:34:15.9146159Z         try:
2020-04-09T00:34:15.9146495Z >           return next(self.gen)
2020-04-09T00:34:15.9146948Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9147256Z 
2020-04-09T00:34:15.9147684Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9148149Z _ ERROR at teardown of StreamTableEnvironmentTests.test_create_table_environment_with_blink_planner _
2020-04-09T00:34:15.9148649Z 
2020-04-09T00:34:15.9149059Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c5fada0>
2020-04-09T00:34:15.9149316Z 
2020-04-09T00:34:15.9149536Z     def __enter__(self):
2020-04-09T00:34:15.9149834Z         try:
2020-04-09T00:34:15.9150154Z >           return next(self.gen)
2020-04-09T00:34:15.9150714Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9151092Z 
2020-04-09T00:34:15.9151384Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9151980Z __________ ERROR at setup of StreamTableEnvironmentTests.test_explain __________
2020-04-09T00:34:15.9152334Z 
2020-04-09T00:34:15.9152722Z self = <contextlib._GeneratorContextManager object at 0x7f8d9783b2e8>
2020-04-09T00:34:15.9153030Z 
2020-04-09T00:34:15.9153212Z     def __enter__(self):
2020-04-09T00:34:15.9153430Z         try:
2020-04-09T00:34:15.9153660Z >           return next(self.gen)
2020-04-09T00:34:15.9154078Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9154293Z 
2020-04-09T00:34:15.9154803Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9155329Z ________ ERROR at teardown of StreamTableEnvironmentTests.test_explain _________
2020-04-09T00:34:15.9155614Z 
2020-04-09T00:34:15.9156000Z self = <contextlib._GeneratorContextManager object at 0x7f8d98192cc0>
2020-04-09T00:34:15.9156358Z 
2020-04-09T00:34:15.9156575Z     def __enter__(self):
2020-04-09T00:34:15.9156787Z         try:
2020-04-09T00:34:15.9157013Z >           return next(self.gen)
2020-04-09T00:34:15.9157317Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9157521Z 
2020-04-09T00:34:15.9157891Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9159267Z ___ ERROR at setup of StreamTableEnvironmentTests.test_explain_with_extended ___
2020-04-09T00:34:15.9159634Z 
2020-04-09T00:34:15.9159931Z self = <contextlib._GeneratorContextManager object at 0x7f8d977e1278>
2020-04-09T00:34:15.9160247Z 
2020-04-09T00:34:15.9160605Z     def __enter__(self):
2020-04-09T00:34:15.9160835Z         try:
2020-04-09T00:34:15.9161155Z >           return next(self.gen)
2020-04-09T00:34:15.9161505Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9161758Z 
2020-04-09T00:34:15.9162024Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9163041Z _ ERROR at teardown of StreamTableEnvironmentTests.test_explain_with_extended __
2020-04-09T00:34:15.9163619Z 
2020-04-09T00:34:15.9164120Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c3f3e80>
2020-04-09T00:34:15.9164634Z 
2020-04-09T00:34:15.9164967Z     def __enter__(self):
2020-04-09T00:34:15.9165322Z         try:
2020-04-09T00:34:15.9165716Z >           return next(self.gen)
2020-04-09T00:34:15.9166262Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9166640Z 
2020-04-09T00:34:15.9167091Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
{code}",,dian.fu,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,"dianfu commented on pull request #11805: [FLINK-17068][python][tests] Ensure the permission of scripts set correctly before executing the Python tests
URL: https://github.com/apache/flink/pull/11805
 
 
   
   ## What is the purpose of the change
   
   *This pull request add the executable permission for the Python scripts before executing the Python tests.*
   
   ## Brief change log
   
     - *Ensure the permission of scripts set correctly in `dev/lint-python.sh`*
   
   ## Verifying this change
   
   Verify manually in my local environment.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Apr/20 04:29;githubbot;600","dianfu commented on pull request #11805: [FLINK-17068][python][tests] Ensure the permission of scripts set correctly before executing the Python tests
URL: https://github.com/apache/flink/pull/11805
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Apr/20 11:13;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-17207,,,,,,,,,,,,,,,,FLINK-17223,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 19 11:13:55 UTC 2020,,,,,,,,,,"0|z0dgc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/20 10:15;dian.fu;[~rmetzger] Thanks for reporting this issue. Will take a look at ASAP.;;;","09/Apr/20 10:38;dian.fu;I found logs like the following:
{code}
PermissionError: [Errno 13] Permission denied: '/__w/1/s/build-target/./bin/pyflink-gateway-server.sh'
{code}

[~rmetzger] It seems that there are some accidental environment problems, e.g. the permission of pyflink-gateway-server.sh is not set correctly. Could you help to check if this is the case? Besides, this problem happens randomly, e.g. the latest tests such as [https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/7263/logs/148] was successful.

Feel free to ping me if there is anything I can do for this issue.;;;","09/Apr/20 15:18;rmetzger;Thanks a lot for looking into the issue. 
I also observed quite a few successful builds by now. Let's see how often and on which builders the issue occurs. Maybe that helps already narrowing down the cause.;;;","13/Apr/20 09:49;pnowojski;Another instance: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7380&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=14487301-07d2-5d56-5690-6dfab9ffd4d9;;;","14/Apr/20 11:21;dian.fu;[~rmetzger] I observe another instance which maybe helpful shared by Chesnay:  [https://dev.azure.com/chesnay/0f3463e8-185e-423b-aa88-6cc39182caea/_apis/build/builds/205/logs/116].

It failed during preparing the environment when running the Python tests with the following errors: 
{code:java}
Could not install packages due to an EnvironmentError: [Errno 28] No space left on device{code}
For the failures in this JIRA, I also suspect that it failed because the disk is full as it failed during installing the PyFlink package(when copying files from pyflink.zip to the installation directory).;;;","19/Apr/20 04:36;dian.fu;Just make it clear: this issue is because the permission of the Python scripts are not correct. I can reproduce the same error message after changing the permission of `pyflink-gateway-server.sh` to a wrong value `444` in my local environment. Have submitted a PR which ensures the permissions of the Python scripts are correct before executing the Python tests.

Regarding to the `No space left on device` error, it's not related this problem and it currently only occurs in the private azure pipeline AFAIK. I have created a ticket FLINK-17220 to track it.;;;","19/Apr/20 11:13;dian.fu;Merged to master via 6e613398ef5bb9654fe8595fb9ff09c062389bb4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update pyarrow version bounds less than 0.14.0,FLINK-17066,13297305,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,09/Apr/20 08:26,09/Apr/20 14:31,13/Jul/23 08:07,09/Apr/20 14:31,1.10.0,,,,,1.10.1,,,,API / Python,,,,,0,pull-request-available,,,,"We need to update pyarrow version bounds less than 0.14.0 in PyFlink 1.10. The bug[1] comes from the dependency of beam 2.15 which has been resolved in beam 2.17.

 [1] https://issues.apache.org/jira/browse/BEAM-8368",,dian.fu,hequn8128,hxbks2ks,,,,,,,,,,,,,,,,,,,"HuangXingBo commented on pull request #11690: [FLINK-17066][python] Update pyarrow version bounds less than 0.14.0
URL: https://github.com/apache/flink/pull/11690
 
 
   ## What is the purpose of the change
   
   *This pull request will update pyarrow version bounds less than 0.14.0*
   
   
   ## Brief change log
   
     - *set pyarrow version in setup.py*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/20 10:38;githubbot;600","hequn8128 commented on pull request #11690: [FLINK-17066][python] Update pyarrow version bounds less than 0.14.0
URL: https://github.com/apache/flink/pull/11690
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/20 14:28;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 14:31:46 UTC 2020,,,,,,,,,,"0|z0dg5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/20 14:31;hequn8128;The problem does not exist in 1.11.0 since pyflink depends on beam-2.19.0 in 1.11.0. ;;;","09/Apr/20 14:31;hequn8128;Fixed in 1.10.1 via 2833504f830ddb7b1172ae57428865fcb5cc9a83 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve literals conversion in ExpressionConverter,FLINK-17064,13297293,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,09/Apr/20 07:53,15/Apr/20 16:19,13/Jul/23 08:07,15/Apr/20 16:19,,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"There are couple of issues with the {{ExpressionResolver}} and literals conversion:
1. There is a lot of code duplication
2. Precision of certain types might get lost e.g. BINARY, CHAR",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,"dawidwys commented on pull request #11694: [FLINK-17064][table-planner] Improve literals conversion in ExpressionConverter
URL: https://github.com/apache/flink/pull/11694
 
 
   ## What is the purpose of the change
   
   It simplifies conversion of literals in ExpressionConverter. It also fixes certain mappings from Flink's type system to Calcite's which were losing precision.
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   Extended ExpressionConverterTest
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/20 17:46;githubbot;600","dawidwys commented on pull request #11694: [FLINK-17064][table-planner] Improve literals conversion in ExpressionConverter
URL: https://github.com/apache/flink/pull/11694
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/20 16:06;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16379,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 16:19:56 UTC 2020,,,,,,,,,,"0|z0dg34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/20 16:19;dwysakowicz;Fixed in 96d4dfe6bd28c08c635ddd8e9718da7ad71b1e15 b3292cbac125fe0db87e2c5c567ee0d5a510e9a4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the conversion from Java row type to Python row type,FLINK-17062,13297280,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,09/Apr/20 07:08,09/Apr/20 11:45,13/Jul/23 08:07,09/Apr/20 11:45,1.9.0,,,,,1.10.1,1.11.0,1.9.3,,API / Python,,,,,0,pull-request-available,,,,It iterate over the result of FieldsDataType.getFieldDataTypes when converting Java row type to Python row type. The result is non-deterministic as the result of FieldsDataType.getFieldDataTypes is of type map.,,dian.fu,f.pompermaier,,,,,,,,,,,,,,,,,,,,"dianfu commented on pull request #11680: [FLINK-17062][python] Fix the conversion from Java row type to Python row type
URL: https://github.com/apache/flink/pull/11680
 
 
   
   ## What is the purpose of the change
   
   *This pull request fix the conversion from Java row type to Python row type.*
   
   ## Brief change log
   
     - *Fix the conversion from Java row type to Python row type*
   
   ## Verifying this change
   
   This change is a trivial rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/20 07:11;githubbot;600","dianfu commented on pull request #11680: [FLINK-17062][python] Fix the conversion from Java row type to Python row type
URL: https://github.com/apache/flink/pull/11680
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/20 11:39;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 11:45:37 UTC 2020,,,,,,,,,,"0|z0dg08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/20 08:33;f.pompermaier;Maybe a stupid question: would it be possible (in the future of course) to use Arrow as Row type both for Java and Python so you don't have to pay the cost of conversion? Also a frontend consuming the data could benefit from using already existing arrow-js libraries;;;","09/Apr/20 09:03;dian.fu;[~f.pompermaier] Thanks a lot for the suggestions!

The conversion here means the conversion between the Java data types and Python data types, not means the conversion between Java objects and Python objects. This is needed because:
 - Python type to Java type: the result type of Python UDF is needed to be converted to Java data type to make sure that it could fit into the existing type system of the table module, e.g. the type inference, etc.
 - Java type to Python type: it's currently only used to retrieve the schema of a Table (via Table.get_schema().get_field_data_types()). For example, users may check the schema of a table.

Regarding to the Python/Java object conversion, you are right and it has already used Arrow as the data exchange format between the Java process and Python process for [vectorized Python UDF|https://cwiki.apache.org/confluence/display/FLINK/FLIP-97%3A+Support+Scalar+Vectorized+Python+UDF+in+PyFlink](which takes pandas.Series as the input and output).;;;","09/Apr/20 11:45;dian.fu;Merged via
 * master: 36e597277014f41b4eca1dae14e3d247e8ab17e5
 * release-1.10: 3ea800042dc6ae818748eea216218befd2b380fa
 * release-1.9: f497a51ae226d92a4364bf7c518ace0253ba3963;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""{flink}"" isn't being replaced by the Apache Flink trademark in Stateful Functions docs",FLINK-17060,13297243,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Authuir,tzulitai,tzulitai,09/Apr/20 00:54,20/Apr/20 08:47,13/Jul/23 08:07,20/Apr/20 08:47,statefun-2.0.0,,,,,statefun-2.0.1,statefun-2.1.0,,,Documentation,Stateful Functions,,,,0,pull-request-available,,,,See https://ci.apache.org/projects/flink/flink-statefun-docs-master/sdk/modules.html#embedded-module,,Authuir,tzulitai,,,,,,,,,,,,,,,,,,,,"authuir commented on pull request #100: [FLINK-17060][docs] Replace ""{flink}"" with the Apache Flink trademark
URL: https://github.com/apache/flink-statefun/pull/100
 
 
   Replace ""{flink}"" with the Apache Flink trademark
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Apr/20 10:24;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 20 08:47:04 UTC 2020,,,,,,,,,,"0|z0dfs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/20 10:24;Authuir;PR created: [https://github.com/apache/flink-statefun/pull/100];;;","20/Apr/20 08:47;tzulitai;Fixed.

* statefun/master - ccc8afa129fa16b8f29cbeb161f88d5d93309692
* statefun/release-2.0 - 753f774632614ad6bb5e38d8a82a268a43e21073;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMH main() methods call unrelated benchmarks,FLINK-17056,13297123,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,08/Apr/20 16:24,09/Apr/20 07:49,13/Jul/23 08:07,09/Apr/20 07:49,1.10.0,,,,,1.11.0,,,,Benchmarks,,,,,0,,,,,"Each benchmark class is accompanied by an according {{public static main (String[] args)}} method which should run all benchmarks in that class. However, it just uses the class' simple name in a regexp like {{"".*<name>.*""}} and may thus also match further classes that were not intended to run. An example for this is the {{StreamNetworkThroughputBenchmarkExecutor}} which also runs benchmarks from {{DataSkewStreamNetworkThroughputBenchmarkExecutor}}. Using the canonical name instead fixes that behaviour.",,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 07:49:45 UTC 2020,,,,,,,,,,"0|z0df1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/20 07:49;nkruber;merged to master via https://github.com/dataArtisans/flink-benchmarks/commit/b5ace15ad74d4699ab7f4176515b332a9307db66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskMailboxProcessorTest.testIdleTime() unstable,FLINK-17054,13297092,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,rmetzger,rmetzger,08/Apr/20 14:00,08/Apr/20 14:26,13/Jul/23 08:07,08/Apr/20 14:25,,,,,,1.11.0,,,,Runtime / Task,,,,,0,test-stability,,,,"CI run: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7205&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=d2c1c472-9d7b-5913-b8e4-461f3092fb7a

{code}
[ERROR] Failures: 
[ERROR]   TaskMailboxProcessorTest.testIdleTime:283 
Expected: a value equal to or greater than <10L>
     but: <9L> was less than <10L>
{code}",,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16864,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 14:25:58 UTC 2020,,,,,,,,,,"0|z0deug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/20 14:08;pnowojski;[~wenlong.lwl], could you take a look ASAP?

If the failure will not be rare, we will have to revert the original commit or disable the test.;;;","08/Apr/20 14:25;pnowojski;reverted d69d3c0589 on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RecordWriterTest.testClearBuffersAfterInterruptDuringBlockingBufferRequest fails,FLINK-17053,13297084,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,rmetzger,rmetzger,08/Apr/20 13:37,08/Apr/20 14:26,13/Jul/23 08:07,08/Apr/20 14:25,,,,,,1.11.0,,,,Runtime / Network,,,,,0,test-stability,,,,"CI run: https://travis-ci.org/github/apache/flink/jobs/672458481
{code}
[ERROR] Tests run: 12, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.378 s <<< FAILURE! - in org.apache.flink.runtime.io.network.api.writer.BroadcastRecordWriterTest
[ERROR] testClearBuffersAfterInterruptDuringBlockingBufferRequest(org.apache.flink.runtime.io.network.api.writer.BroadcastRecordWriterTest)  Time elapsed: 1.197 s  <<< FAILURE!
Wanted but not invoked:
bufferProvider.requestBufferBuilderBlocking();
-> at org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.testClearBuffersAfterInterruptDuringBlockingBufferRequest(RecordWriterTest.java:211)
However, there were exactly 2 interactions with this mock:
bufferProvider.requestBufferBuilder();
-> at org.apache.flink.runtime.io.network.api.writer.RecordWriterTest$RecyclingPartitionWriter.tryGetBufferBuilder(RecordWriterTest.java:726)
bufferProvider.requestBufferBuilder();
-> at org.apache.flink.runtime.io.network.api.writer.RecordWriterTest$RecyclingPartitionWriter.tryGetBufferBuilder(RecordWriterTest.java:726)
	at org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.testClearBuffersAfterInterruptDuringBlockingBufferRequest(RecordWriterTest.java:211)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
{code}
",,aljoscha,liyu,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16864,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 14:25:45 UTC 2020,,,,,,,,,,"0|z0deso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/20 13:37;rmetzger;This error was also reported here: FLINK-14295;;;","08/Apr/20 14:08;aljoscha;Another occurrence: https://dev.azure.com/aljoschakrettek/Flink/_build/results?buildId=85&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=6545c5fa-bc4d-5447-1a0b-b3260f46cc6b&l=6734;;;","08/Apr/20 14:25;pnowojski;SNAFU, bad mockito test exploded. After [~wenlong.lwl] change, either requestBufferBuilderBlocking or requestBufferBuilder can be called inter-exchangeably. Test has to be rewritten to drop the mockito usage.

I'm reverting FLINK-16864 for now.;;;","08/Apr/20 14:25;pnowojski;reverted d69d3c0589 on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If multiple views are based on tables not defined in the yaml config file, dropping one of the views will throw exception",FLINK-17045,13296995,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,TsReaper,TsReaper,08/Apr/20 06:56,01/Jul/20 14:35,13/Jul/23 08:07,01/Jul/20 14:35,1.11.0,,,,,1.11.0,,,,Table SQL / Client,,,,,0,,,,,"Add the following test in {{LocalExecutorITCase}} and run it
{code:java}
@Test
public void testBuggyViews() throws Exception {
    final Executor executor = createDefaultExecutor(clusterClient);
    final SessionContext session = new SessionContext(""test-session"", new Environment());
    String sessionId = executor.openSession(session);

    final String ddlTemplate = ""create table %s(\n"" +
        ""  a int,\n"" +
        ""  b bigint,\n"" +
        ""  c varchar\n"" +
        "") with (\n"" +
        ""  'connector.type'='filesystem',\n"" +
        ""  'format.type'='csv',\n"" +
        ""  'connector.path'='xxx'\n"" +
        "")\n"";

    executor.useCatalog(sessionId, ""catalog1"");
    executor.createTable(sessionId, String.format(ddlTemplate, ""MyTable1""));
    executor.createTable(sessionId, String.format(ddlTemplate, ""MyTable2""));

    executor.addView(sessionId, ""AdditionalView1"", ""SELECT * FROM MyTable1"");
    executor.addView(sessionId, ""AdditionalView2"", ""SELECT * FROM MyTable2"");

    // exception thrown here
    executor.removeView(sessionId, ""AdditionalView1"");

    executor.closeSession(sessionId);
}
{code}
The following exception is thrown
{code:java}
org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context.

	at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:767)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.removeView(LocalExecutor.java:320)
	at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testBuggyViews(LocalExecutorITCase.java:163)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Invalid view 'AdditionalView2' with query:
SELECT * FROM MyTable2
Cause: SQL validation failed. From line 1, column 15 to line 1, column 22: Object 'MyTable2' not found
	at org.apache.flink.table.client.gateway.local.ExecutionContext.registerView(ExecutionContext.java:682)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$9(ExecutionContext.java:602)
	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:597)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:508)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:164)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:121)
	at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:756)
	... 39 more
{code}
This is caused by not adding the {{SessionState}} in {{LocalExecutor#removeView}} when creating the new builder.
{code:java}
if (newEnv.getTables().remove(name) != null) {
   // Renew the ExecutionContext.
   this.contextMap.put(
         sessionId,
         createExecutionContextBuilder(context.getOriginalSessionContext())
               // should be: .env(newEnv).sessionState(getExecutionContext(sessionId).getSessionState()).build());
               .env(newEnv).build());
}{code}",,godfreyhe,jark,leonard,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17113,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 14:34:56 UTC 2020,,,,,,,,,,"0|z0de8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/20 14:34;leonard;The function executor#addView()  and executor#addView() has been removed in FLINK-17113, and the problem has gone away in latest code as following: 

 
{code:java}
@Test
// tests passed
public void testBuggyViews() throws Exception {
   final Executor executor = createDefaultExecutor(clusterClient);
   final SessionContext session = new SessionContext(""test-session"", new Environment());
   String sessionId = executor.openSession(session);

   final String ddlTemplate = ""create table %s(\n"" +
      ""  a int,\n"" +
      ""  b bigint,\n"" +
      ""  c varchar\n"" +
      "") with (\n"" +
      ""  'connector.type'='filesystem',\n"" +
      ""  'format.type'='csv',\n"" +
      ""  'connector.path'='xxx'\n"" +
      "")\n"";

   executor.useCatalog(sessionId, ""catalog1"");
   executor.createTable(sessionId, String.format(ddlTemplate, ""MyTable1""));
   executor.createTable(sessionId, String.format(ddlTemplate, ""MyTable2""));

   executor.executeSql(sessionId, ""CREATE  VIEW AdditionalView1 AS SELECT * FROM MyTable1"");
   executor.executeSql(sessionId, ""CREATE  VIEW AdditionalView2 AS SELECT * FROM MyTable2"");

   // exception thrown here
   executor.executeSql(sessionId, ""DROP  VIEW AdditionalView1"");

   executor.closeSession(sessionId);
}
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointWriterITCase broken,FLINK-17040,13296963,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kevin.cyj,wenlong.lwl,wenlong.lwl,08/Apr/20 01:56,08/Apr/20 09:42,13/Jul/23 08:07,08/Apr/20 07:03,,,,,,1.11.0,,,,Runtime / Task,Tests,,,,0,pull-request-available,,,,"I think it is because of the change of flink-16537 which create partition writer in beforeInvoke [~zhijiang]

Caused by: java.lang.UnsupportedOperationException: This method should never be called
	at org.apache.flink.state.api.runtime.SavepointEnvironment.getAllWriters(SavepointEnvironment.java:242)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:439)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:433)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:454)
	at org.apache.flink.state.api.output.BoundedOneInputStreamTaskRunner.mapPartition(BoundedOneInputStreamTaskRunner.java:76)",,kevin.cyj,klion26,rmetzger,wenlong.lwl,zjwang,,,,,,,,,,,,,,,,,"wsry commented on pull request #11665: [FLINK-17040][tests] Fix SavepointWriterITCase failure because of UnsupportedOperationException
URL: https://github.com/apache/flink/pull/11665
 
 
   ## What is the purpose of the change
   
   Fix SavepointWriterITCase failure because of UnsupportedOperationException. As figured out in FLINK-17040, the problem is caused by FLINK-16537.
   
   
   ## Brief change log
   
     - Implement SavepointEnvironment#getAllWriters and return null instead of throwing UnsupportedOperationException.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Apr/20 03:50;githubbot;600","zhijiangW commented on pull request #11665: [FLINK-17040][tests] Fix SavepointWriterITCase failure because of UnsupportedOperationException
URL: https://github.com/apache/flink/pull/11665
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Apr/20 06:57;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,FLINK-17046,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 07:22:53 UTC 2020,,,,,,,,,,"0|z0de1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/20 03:13;zjwang;Thanks for reporting this [~wenlong.lwl]!

It is indeed caused by FLINK-16537. But it is strange that the Azure shows success before the respective PR merged, and actually the Azure did not execute any tests in that PR before.

The fix should be easy to only return empty list in SavepointEnvironment#getAllWriters, instead of throwing UnsupportedOperationException in some dummy methods. The root cause for Azure should be investigated further afterwards.;;;","08/Apr/20 06:14;rmetzger;Thanks a lot for looking into this issue. 
Regarding Azure: The build for the pull request failed because the git checkout failed. Sadly, flinkbot showed ""SUCCESS"", even though the build was not successful. I will look into it.
The Travis build failed in the library (so it correctly found the issue). I'm pretty sure Azure would have found the issue as well, if the checkout would have worked.;;;","08/Apr/20 07:02;kevin.cyj;Fixed via 5303a215213335c6574e83911ee49bceac851d25 on master.;;;","08/Apr/20 07:22;zjwang;Thanks for the explanations [~rmetzger]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HiveTableSourceTest fails with: ""Timeout waiting for HMS to start""",FLINK-17031,13296768,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,rmetzger,rmetzger,07/Apr/20 07:54,29/Apr/21 12:35,13/Jul/23 08:07,29/Apr/21 12:35,1.11.0,,,,,,,,,Connectors / Hive,,,,,0,stale-major,test-stability,,,"CI: https://travis-ci.org/github/apache/flink/jobs/671775522?utm_medium=notification&utm_source=slack

{code}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 92.236 s <<< FAILURE! - in org.apache.flink.connectors.hive.HiveTableSourceTest
[ERROR] org.apache.flink.connectors.hive.HiveTableSourceTest  Time elapsed: 92.236 s  <<< ERROR!
java.util.concurrent.TimeoutException: Timeout waiting for HMS to start
	at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.waitForHMSStart(FlinkStandaloneHiveRunner.java:438)
	at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.startHMS(FlinkStandaloneHiveRunner.java:418)
	at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.access$400(FlinkStandaloneHiveRunner.java:92)
	at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner$1.before(FlinkStandaloneHiveRunner.java:118)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,lirui,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 12:35:54 UTC 2021,,,,,,,,,,"0|z0dcu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/20 13:36;rmetzger;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=617&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa;;;","22/Apr/21 11:23;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 12:35;lirui;Closing this one as we no longer use standalone HMS for the tests;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NPE from NullAwareMapIterator,FLINK-17015,13296743,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jark,jark,07/Apr/20 06:19,08/Oct/21 11:01,13/Jul/23 08:07,02/Jun/20 08:33,,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When using Heap statebackend, the underlying {{org.apache.flink.runtime.state.heap.HeapMapState#iterator}} may return a null iterator. It results in the {{NullAwareMapIterator}} holds a null iterator and throws NPE in the following {{NullAwareMapIterator#hasNext}} invocking. ",,eastcirclek,jark,klion26,libenchao,liyu,tartarus,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17610,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17610,"07/Apr/20 06:17;jark;92164295_3052056384855585_3776552648744894464_o.jpg;https://issues.apache.org/jira/secure/attachment/12999201/92164295_3052056384855585_3776552648744894464_o.jpg",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 02:32:06 UTC 2020,,,,,,,,,,"0|z0dcoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 12:05;liyu;[~jark] is this a critical bug that need to be fixed in 1.11.0? Asking since I'm considering whether the dependent issue FLINK-17610 could be deferred to 1.12. Thanks.;;;","18/May/20 12:58;jark;I think we should fix this one in 1.11.0. But we can fix it in table side (with a lot of null condition). If FLINK-17610 is planned in 1.11, that would be great. ;;;","18/May/20 13:28;liyu;Thanks for the confirmation [~jark].

Since FLINK-17610 is an improvement rather than feature, plus the fact that it blocks the bug here that we'd like to fix in 1.11.0, I suggest we still keep it for 1.11.0.

[~pnowojski] [~zjwang] Please let me know if any different idea. Thanks.;;;","19/May/20 02:32;zjwang;If I understand correctly, FLINK-17610 can also be treated as a solution for this bug somehow, so +1 from my side to make it for 1.11.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Streaming File Sink s3 end-to-end test fails with ""Output hash mismatch""",FLINK-17010,13296537,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,aljoscha,rmetzger,rmetzger,06/Apr/20 15:04,08/Apr/20 14:18,13/Jul/23 08:07,07/Apr/20 18:37,1.11.0,,,,,1.11.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,test-stability,,,"CI: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7099&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
2020-04-06T13:17:38.2460013Z Digest: sha256:a61ed0bca213081b64be94c5e1b402ea58bc549f457c2682a86704dd55231e09
2020-04-06T13:17:38.2475230Z Status: Downloaded newer image for stedolan/jq:latest
2020-04-06T13:18:00.4459693Z Number of produced values 13124/60000
2020-04-06T13:18:25.3214772Z Number of produced values 18300/60000
2020-04-06T13:19:06.9767370Z Number of produced values 45366/60000
2020-04-06T13:20:01.2846102Z Number of produced values 60000/60000
2020-04-06T13:20:02.5940091Z Cancelling job ff95cd4fd52d10b6540c03cf72b33111.
2020-04-06T13:20:03.7862792Z Cancelled job ff95cd4fd52d10b6540c03cf72b33111.
2020-04-06T13:20:03.8343709Z Waiting for job (ff95cd4fd52d10b6540c03cf72b33111) to reach terminal state CANCELED ...
2020-04-06T13:20:05.8474817Z Job (ff95cd4fd52d10b6540c03cf72b33111) reached terminal state CANCELED
2020-04-06T13:20:08.6987955Z FAIL File Streaming Sink: Output hash mismatch.  Got 61bb5f161b859759a9829516d96e2bbc, expected 6727342fdd3aae2129e61fc8f433fb6f.
2020-04-06T13:20:08.6989364Z head hexdump of actual:
2020-04-06T13:20:08.7288989Z 0000000   C   o   m   p   l   e   t   e   d       2   .   0       K   i
2020-04-06T13:20:08.7289917Z 0000010   B   /   3   4   0   .   7       K   i   B       (   5   .   3
2020-04-06T13:20:08.7293001Z 0000020       K   i   B   /   s   )       w   i   t   h       1   1   0
2020-04-06T13:20:08.7298661Z 0000030       f   i   l   e   (   s   )       r   e   m   a   i   n   i
2020-04-06T13:20:08.7299371Z 0000040   n   g  \r   d   o   w   n   l   o   a   d   :       s   3   :
2020-04-06T13:20:08.7301377Z 0000050   /   /   f   l   i   n   k   -   i   n   t   e   g   r   a   t
2020-04-06T13:20:08.7302336Z 0000060   i   o   n   -   t   e   s   t   s   /   t   e   m   p   /   t
2020-04-06T13:20:08.7303021Z 0000070   e   s   t   _   s   t   r   e   a   m   i   n   g   _   f   i
2020-04-06T13:20:08.7303968Z 0000080   l   e   _   s   i   n   k   -   7   b   0   7   7   2   1   2
2020-04-06T13:20:08.7304790Z 0000090   -   d   9   f   8   -   4   0   d   8   -   9   d   0   a   -
2020-04-06T13:20:08.7305297Z 00000a0   f   f   9   f   7   e   9   6   d   7   b   d   /   0   /   p
2020-04-06T13:20:08.7306285Z 00000b0   a   r   t   -   2   -   1       t   o       h   o   s   t   d
2020-04-06T13:20:08.7307138Z 00000c0   i   r   /   t   e   m   p   -   t   e   s   t   -   d   i   r
2020-04-06T13:20:08.7307891Z 00000d0   e   c   t   o   r   y   -   3   5   0   6   5   0   6   7   2
2020-04-06T13:20:08.7308402Z 00000e0   8   9   /   t   e   m   p   /   t   e   s   t   _   s   t   r
2020-04-06T13:20:08.7308870Z 00000f0   e   a   m   i   n   g   _   f   i   l   e   _   s   i   n   k
2020-04-06T13:20:08.7309579Z 0000100   -   7   b   0   7   7   2   1   2   -   d   9   f   8   -   4
2020-04-06T13:20:08.7310295Z 0000110   0   d   8   -   9   d   0   a   -   f   f   9   f   7   e   9
2020-04-06T13:20:08.7311022Z 0000120   6   d   7   b   d   /   0   /   p   a   r   t   -   2   -   1
2020-04-06T13:20:08.7311537Z 0000130  \n   C   o   m   p   l   e   t   e   d       2   .   0       K
2020-04-06T13:20:08.7312010Z 0000140   i   B   /   3   4   0   .   7       K   i   B       (   5   .
2020-04-06T13:20:08.7312461Z 0000150   3       K   i   B   /   s   )       w   i   t   h       1   0
2020-04-06T13:20:08.7312930Z 0000160   9       f   i   l   e   (   s   )       r   e   m   a   i   n
2020-04-06T13:20:08.7313393Z 0000170   i   n   g  \r   C   o   m   p   l   e   t   e   d       4   .
2020-04-06T13:20:08.7313844Z 0000180   4       K   i   B   /   3   4   0   .   7       K   i   B    
2020-04-06T13:20:08.7314332Z 0000190   (   9   .   8       K   i   B   /   s   )       w   i   t   h
2020-04-06T13:20:08.7314785Z 00001a0       1   0   9       f   i   l   e   (   s   )       r   e   m
2020-04-06T13:20:08.7315236Z 00001b0   a   i   n   i   n   g  \r   d   o   w   n   l   o   a   d   :
2020-04-06T13:20:08.7315957Z 00001c0       s   3   :   /   /   f   l   i   n   k   -   i   n   t   e
2020-04-06T13:20:08.7316672Z 00001d0   g   r   a   t   i   o   n   -   t   e   s   t   s   /   t   e
2020-04-06T13:20:08.7317163Z 00001e0   m   p   /   t   e   s   t   _   s   t   r   e   a   m   i   n
2020-04-06T13:20:08.7317869Z 00001f0   g   _   f   i   l   e   _   s   i   n   k   -   7   b   0   7
2020-04-06T13:20:08.7318579Z 0000200   7   2   1   2   -   d   9   f   8   -   4   0   d   8   -   9
2020-04-06T13:20:08.7319283Z 0000210   d   0   a   -   f   f   9   f   7   e   9   6   d   7   b   d
2020-04-06T13:20:08.7320032Z 0000220   /   0   /   p   a   r   t   -   2   -   0       t   o       h
2020-04-06T13:20:08.7320747Z 0000230   o   s   t   d   i   r   /   t   e   m   p   -   t   e   s   t
2020-04-06T13:20:08.7321447Z 0000240   -   d   i   r   e   c   t   o   r   y   -   3   5   0   6   5
2020-04-06T13:20:08.7321955Z 0000250   0   6   7   2   8   9   /   t   e   m   p   /   t   e   s   t
2020-04-06T13:20:08.7322758Z 0000260   _   s   t   r   e   a   m   i   n   g   _   f   i   l   e   _
2020-04-06T13:20:08.7323476Z 0000270   s   i   n   k   -   7   b   0   7   7   2   1   2   -   d   9
2020-04-06T13:20:08.7324210Z 0000280   f   8   -   4   0   d   8   -   9   d   0   a   -   f   f   9
2020-04-06T13:20:08.7324690Z 0000290   f   7   e   9   6   d   7   b   d   /   0   /   p   a   r   t
2020-04-06T13:20:08.7325360Z 00002a0   -   2   -   0  \n   C   o   m   p   l   e   t   e   d       4
2020-04-06T13:20:08.7325861Z 00002b0   .   4       K   i   B   /   3   4   0   .   7       K   i   B
2020-04-06T13:20:08.7326493Z 00002c0       (   9   .   8       K   i   B   /   s   )       w   i   t
2020-04-06T13:20:08.7327076Z 00002d0   h       1   0   8       f   i   l   e   (   s   )       r   e
2020-04-06T13:20:08.7327550Z 00002e0   m   a   i   n   i   n   g  \r   C   o   m   p   l   e   t   e
2020-04-06T13:20:08.7328001Z 00002f0   d       8   .   1       K   i   B   /   3   4   0   .   7    
2020-04-06T13:20:08.7328465Z 0000300   K   i   B       (   1   7   .   6       K   i   B   /   s   )
2020-04-06T13:20:08.7328933Z 0000310       w   i   t   h       1   0   8       f   i   l   e   (   s
2020-04-06T13:20:08.7329401Z 0000320   )       r   e   m   a   i   n   i   n   g  \r   d   o   w   n
2020-04-06T13:20:08.7329889Z 0000330   l   o   a   d   :       s   3   :   /   /   f   l   i   n   k
2020-04-06T13:20:08.7330629Z 0000340   -   i   n   t   e   g   r   a   t   i   o   n   -   t   e   s
2020-04-06T13:20:08.7331129Z 0000350   t   s   /   t   e   m   p   /   t   e   s   t   _   s   t   r
2020-04-06T13:20:08.7331608Z 0000360   e   a   m   i   n   g   _   f   i   l   e   _   s   i   n   k
2020-04-06T13:20:08.7332294Z 0000370   -   7   b   0   7   7   2   1   2   -   d   9   f   8   -   4
2020-04-06T13:20:08.7333106Z 0000380   0   d   8   -   9   d   0   a   -   f   f   9   f   7   e   9
2020-04-06T13:20:08.7333822Z 0000390   6   d   7   b   d   /   1   /   p   a   r   t   -   2   -   1
2020-04-06T13:20:08.7334310Z 00003a0   2       t   o       h   o   s   t   d   i   r   /   t   e   m
2020-04-06T13:20:08.7334982Z 00003b0   p   -   t   e   s   t   -   d   i   r   e   c   t   o   r   y
2020-04-06T13:20:08.7335680Z 00003c0   -   3   5   0   6   5   0   6   7   2   8   9   /   t   e   m
2020-04-06T13:20:08.7336186Z 00003d0   p   /   t   e   s   t   _   s   t   r   e   a   m   i   n   g
2020-04-06T13:20:08.7336967Z 00003e0   _   f   i   l   e   _   s   i   n   k   -   7   b   0   7   7
2020-04-06T13:20:08.7338105Z 00003f0   2   1   2   -   d   9   f   8   -   4   0   d   8   -   9   d
2020-04-06T13:20:08.7338895Z 0000400   0   a   -   f   f   9   f   7   e   9   6   d   7   b   d   /
2020-04-06T13:20:08.7339640Z 0000410   1   /   p   a   r   t   -   2   -   1   2  \n   C   o   m   p
2020-04-06T13:20:08.7340137Z 0000420   l   e   t   e   d       8   .   1       K   i   B   /   3   4
2020-04-06T13:20:08.7340611Z 0000430   0   .   7       K   i   B       (   1   7   .   6       K   i
2020-04-06T13:20:08.7341079Z 0000440   B   /   s   )       w   i   t   h       1   0   7       f   i
2020-04-06T13:20:08.7341534Z 0000450   l   e   (   s   )       r   e   m   a   i   n   i   n   g  \r
2020-04-06T13:20:08.7341997Z 0000460   C   o   m   p   l   e   t   e   d       1   1   .   1       K
2020-04-06T13:20:08.7342446Z 0000470   i   B   /   3   4   0   .   7       K   i   B       (   2   2
2020-04-06T13:20:08.7342912Z 0000480   .   9       K   i   B   /   s   )       w   i   t   h       1
2020-04-06T13:20:08.7343382Z 0000490   0   7       f   i   l   e   (   s   )       r   e   m   a   i
2020-04-06T13:20:08.7343869Z 00004a0   n   i   n   g  \r   d   o   w   n   l   o   a   d   :       s
2020-04-06T13:20:08.7344593Z 00004b0   3   :   /   /   f   l   i   n   k   -   i   n   t   e   g   r
2020-04-06T13:20:08.7345322Z 00004c0   a   t   i   o   n   -   t   e   s   t   s   /   t   e   m   p
2020-04-06T13:20:08.7345834Z 00004d0   /   t   e   s   t   _   s   t   r   e   a   m   i   n   g   _
2020-04-06T13:20:08.7346517Z 00004e0   f   i   l   e   _   s   i   n   k   -   7   b   0   7   7   2
2020-04-06T13:20:08.7347225Z 00004f0   1   2   -   d   9   f   8   -   4   0   d   8   -   9   d   0
2020-04-06T13:20:08.7348173Z 0000500   a   -   f   f   9   f   7   e   9   6   d   7   b   d   /   0
2020-04-06T13:20:08.7348985Z 0000510   /   p   a   r   t   -   2   -   1   0       t   o       h   o
2020-04-06T13:20:08.7349719Z 0000520   s   t   d   i   r   /   t   e   m   p   -   t   e   s   t   -
2020-04-06T13:20:08.7350442Z 0000530   d   i   r   e   c   t   o   r   y   -   3   5   0   6   5   0
2020-04-06T13:20:08.7351115Z 0000540   6   7   2   8   9   /   t   e   m   p   /   t   e   s   t   _
2020-04-06T13:20:08.7352710Z 0000550   s   t   r   e   a   m   i   n   g   _   f   i   l   e   _   s
2020-04-06T13:20:08.7353469Z 0000560   i   n   k   -   7   b   0   7   7   2   1   2   -   d   9   f
2020-04-06T13:20:08.7354013Z 0000570   8   -   4   0   d   8   -   9   d   0   a   -   f   f   9   f
2020-04-06T13:20:08.7360064Z 0000580   7   e   9   6   d   7   b   d   /   0   /   p   a   r   t   -
2020-04-06T13:20:08.7360656Z 0000590   2   -   1   0  \n   C   o   m   p   l   e   t   e   d       1
2020-04-06T13:20:08.7361032Z 00005a0   1   .   1       K   i   B   /   3   4   0   .   7       K   i
2020-04-06T13:20:08.7361380Z 00005b0   B       (   2   2   .   9       K   i   B   /   s   )       w
2020-04-06T13:20:08.7361741Z 00005c0   i   t   h       1   0   6       f   i   l   e   (   s   )    
2020-04-06T13:20:08.7362090Z 00005d0   r   e   m   a   i   n   i   n   g  \r   C   o   m   p   l   e
2020-04-06T13:20:08.7362599Z 00005e0   t   e   d       1   4   .   1       K   i   B   /   3   4   0
2020-04-06T13:20:08.7362959Z 00005f0   .   7       K   i   B       (   2   8   .   9       K   i   B
2020-04-06T13:20:08.7363310Z 0000600   /   s   )       w   i   t   h       1   0   6       f   i   l
2020-04-06T13:20:08.7363671Z 0000610   e   (   s   )       r   e   m   a   i   n   i   n   g  \r   d
2020-04-06T13:20:08.7364043Z 0000620   o   w   n   l   o   a   d   :       s   3   :   /   /   f   l
2020-04-06T13:20:08.7364639Z 0000630   i   n   k   -   i   n   t   e   g   r   a   t   i   o   n   -
2020-04-06T13:20:08.7364990Z 0000640   t   e   s   t   s   /   t   e   m   p   /   t   e   s   t   _
2020-04-06T13:20:08.7365351Z 0000650   s   t   r   e   a   m   i   n   g   _   f   i   l   e   _   s
2020-04-06T13:20:08.7365884Z 0000660   i   n   k   -   7   b   0   7   7   2   1   2   -   d   9   f
2020-04-06T13:20:08.7366403Z 0000670   8   -   4   0   d   8   -   9   d   0   a   -   f   f   9   f
2020-04-06T13:20:08.7366931Z 0000680   7   e   9   6   d   7   b   d   /   0   /   p   a   r   t   -
2020-04-06T13:20:08.7367456Z 0000690   2   -   9       t   o       h   o   s   t   d   i   r   /   t
2020-04-06T13:20:08.7367990Z 00006a0   e   m   p   -   t   e   s   t   -   d   i   r   e   c   t   o
2020-04-06T13:20:08.7368507Z 00006b0   r   y   -   3   5   0   6   5   0   6   7   2   8   9   /   t
2020-04-06T13:20:08.7368870Z 00006c0   e   m   p   /   t   e   s   t   _   s   t   r   e   a   m   i
2020-04-06T13:20:08.7369399Z 00006d0   n   g   _   f   i   l   e   _   s   i   n   k   -   7   b   0
2020-04-06T13:20:08.7369919Z 00006e0   7   7   2   1   2   -   d   9   f   8   -   4   0   d   8   -
2020-04-06T13:20:08.7370453Z 00006f0   9   d   0   a   -   f   f   9   f   7   e   9   6   d   7   b
2020-04-06T13:20:08.7370969Z 0000700   d   /   0   /   p   a   r   t   -   2   -   9  \n   C   o   m
2020-04-06T13:20:08.7371334Z 0000710   p   l   e   t   e   d       1   4   .   1       K   i   B   /
2020-04-06T13:20:08.7371679Z 0000720   3   4   0   .   7       K   i   B       (   2   8   .   9    
2020-04-06T13:20:08.7372043Z 0000730   K   i   B   /   s   )       w   i   t   h       1   0   5    
2020-04-06T13:20:08.7372406Z 0000740   f   i   l   e   (   s   )       r   e   m   a   i   n   i   n
2020-04-06T13:20:08.7372751Z 0000750   g  \r   C   o   m   p   l   e   t   e   d       1   5   .   1
2020-04-06T13:20:08.7373109Z 0000760       K   i   B   /   3   4   0   .   7       K   i   B       (
2020-04-06T13:20:08.7373603Z 0000770   3   0   .   2       K   i   B   /   s   )       w   i   t   h
2020-04-06T13:20:08.7373961Z 0000780       1   0   5       f   i   l   e   (   s   )       r   e   m
2020-04-06T13:20:08.7374320Z 0000790   a   i   n   i   n   g  \r   C   o   m   p   l   e   t   e   d
2020-04-06T13:20:08.7374664Z 00007a0       1   7   .   5       K   i   B   /   3   4   0   .   7    
2020-04-06T13:20:08.7375021Z 00007b0   K   i   B       (   3   4   .   9       K   i   B   /   s   )
2020-04-06T13:20:08.7375363Z 00007c0       w   i   t   h       1   0   5       f   i   l   e   (   s
2020-04-06T13:20:08.7375855Z 00007d0   )       r   e   m   a   i   n   i   n   g  \r   d   o   w   n
2020-04-06T13:20:08.7376278Z 00007e0   l   o   a   d   :       s   3   :   /   /   f   l   i   n   k
2020-04-06T13:20:08.7376875Z 00007f0   -   i   n   t   e   g   r   a   t   i   o   n   -   t   e   s
2020-04-06T13:20:08.7377242Z 0000800   t   s   /   t   e   m   p   /   t   e   s   t   _   s   t   r
2020-04-06T13:20:08.7377588Z 0000810   e   a   m   i   n   g   _   f   i   l   e   _   s   i   n   k
2020-04-06T13:20:08.7378125Z 0000820   -   7   b   0   7   7   2   1   2   -   d   9   f   8   -   4
2020-04-06T13:20:08.7378641Z 0000830   0   d   8   -   9   d   0   a   -   f   f   9   f   7   e   9
2020-04-06T13:20:08.7379170Z 0000840   6   d   7   b   d   /   0   /   p   a   r   t   -   2   -   1
2020-04-06T13:20:08.7379517Z 0000850   3       t   o       h   o   s   t   d   i   r   /   t   e   m
2020-04-06T13:20:08.7380042Z 0000860   p   -   t   e   s   t   -   d   i   r   e   c   t   o   r   y
2020-04-06T13:20:08.7380579Z 0000870   -   3   5   0   6   5   0   6   7   2   8   9   /   t   e   m
2020-04-06T13:20:08.7380931Z 0000880   p   /   t   e   s   t   _   s   t   r   e   a   m   i   n   g
2020-04-06T13:20:08.7381460Z 0000890   _   f   i   l   e   _   s   i   n   k   -   7   b   0   7   7
2020-04-06T13:20:08.7381975Z 00008a0   2   1   2   -   d   9   f   8   -   4   0   d   8   -   9   d
2020-04-06T13:20:08.7382506Z 00008b0   0   a   -   f   f   9   f   7   e   9   6   d   7   b   d   /
2020-04-06T13:20:08.7383035Z 00008c0   0   /   p   a   r   t   -   2   -   1   3  \n   C   o   m   p
2020-04-06T13:20:08.7383382Z 00008d0   l   e   t   e   d       1   7   .   5       K   i   B   /   3
2020-04-06T13:20:08.7383740Z 00008e0   4   0   .   7       K   i   B       (   3   4   .   9       K
2020-04-06T13:20:08.7384084Z 00008f0   i   B   /   s   )       w   i   t   h       1   0   4       f
2020-04-06T13:20:08.7384443Z 0000900   i   l   e   (   s   )       r   e   m   a   i   n   i   n   g
2020-04-06T13:20:08.7384817Z 0000910  \r   d   o   w   n   l   o   a   d   :       s   3   :   /   /
2020-04-06T13:20:08.7385378Z 0000920   f   l   i   n   k   -   i   n   t   e   g   r   a   t   i   o
2020-04-06T13:20:08.7385915Z 0000930   n   -   t   e   s   t   s   /   t   e   m   p   /   t   e   s
2020-04-06T13:20:08.7387030Z 0000940   t   _   s   t   r   e   a   m   i   n   g   _   f   i   l   e
2020-04-06T13:20:08.7387666Z 0000950   _   s   i   n   k   -   7   b   0   7   7   2   1   2   -   d
2020-04-06T13:20:08.7388178Z 0000960   9   f   8   -   4   0   d   8   -   9   d   0   a   -   f   f
2020-04-06T13:20:08.7388542Z 0000970   9   f   7   e   9   6   d   7   b   d   /   1   /   p   a   r
2020-04-06T13:20:08.7389057Z 0000980   t   -   2   -   1       t   o       h   o   s   t   d   i   r
2020-04-06T13:20:08.7389585Z 0000990   /   t   e   m   p   -   t   e   s   t   -   d   i   r   e   c
2020-04-06T13:20:08.7390117Z 00009a0   t   o   r   y   -   3   5   0   6   5   0   6   7   2   8   9
2020-04-06T13:20:08.7390476Z 00009b0   /   t   e   m   p   /   t   e   s   t   _   s   t   r   e   a
2020-04-06T13:20:08.7391006Z 00009c0   m   i   n   g   _   f   i   l   e   _   s   i   n   k   -   7
2020-04-06T13:20:08.7391591Z 00009d0   b   0   7   7   2   1   2   -   d   9   f   8   -   4   0   d
2020-04-06T13:20:08.7392129Z 00009e0   8   -   9   d   0   a   -   f   f   9   f   7   e   9   6   d
2020-04-06T13:20:08.7392662Z 00009f0   7   b   d   /   1   /   p   a   r   t   -   2   -   1  \n   C
2020-04-06T13:20:08.7393012Z 0000a00   o   m   p   l   e   t   e   d       1   7   .   5       K   i
2020-04-06T13:20:08.7393372Z 0000a10   B   /   3   4   0   .   7       K   i   B       (   3   4   .
2020-04-06T13:20:08.7393716Z 0000a20   9       K   i   B   /   s   )       w   i   t   h       1   0
2020-04-06T13:20:08.7394073Z 0000a30   3       f   i   l   e   (   s   )       r   e   m   a   i   n
2020-04-06T13:20:08.7394418Z 0000a40   i   n   g  \r   C   o   m   p   l   e   t   e   d       2   1
2020-04-06T13:20:08.7394895Z 0000a50   .   3       K   i   B   /   3   4   0   .   7       K   i   B
2020-04-06T13:20:08.7395257Z 0000a60       (   4   1   .   9       K   i   B   /   s   )       w   i
2020-04-06T13:20:08.7395655Z 0000a70   t   h       1   0   3       f   i   l   e   (   s   )       r
2020-04-06T13:20:08.7396013Z 0000a80   e   m   a   i   n   i   n   g  \r   d   o   w   n   l   o   a
2020-04-06T13:20:08.7396587Z 0000a90   d   :       s   3   :   /   /   f   l   i   n   k   -   i   n
2020-04-06T13:20:08.7397265Z 0000aa0   t   e   g   r   a   t   i   o   n   -   t   e   s   t   s   /
2020-04-06T13:20:08.7397945Z 0000ab0   t   e   m   p   /   t   e   s   t   _   s   t   r   e   a   m
2020-04-06T13:20:08.7398579Z 0000ac0   i   n   g   _   f   i   l   e   _   s   i   n   k   -   7   b
2020-04-06T13:20:08.7399155Z 0000ad0   0   7   7   2   1   2   -   d   9   f   8   -   4   0   d   8
2020-04-06T13:20:08.7399709Z 0000ae0   -   9   d   0   a   -   f   f   9   f   7   e   9   6   d   7
2020-04-06T13:20:08.7400276Z 0000af0   b   d   /   0   /   p   a   r   t   -   2   -   8       t   o
2020-04-06T13:20:08.7400835Z 0000b00       h   o   s   t   d   i   r   /   t   e   m   p   -   t   e
2020-04-06T13:20:08.7401405Z 0000b10   s   t   -   d   i   r   e   c   t   o   r   y   -   3   5   0
2020-04-06T13:20:08.7401876Z 0000b20   6   5   0   6   7   2   8   9   /   t   e   m   p   /   t   e
2020-04-06T13:20:08.7402338Z 0000b30   s   t   _   s   t   r   e   a   m   i   n   g   _   f   i   l
2020-04-06T13:20:08.7402888Z 0000b40   e   _   s   i   n   k   -   7   b   0   7   7   2   1   2   -
2020-04-06T13:20:08.7403408Z 0000b50   d   9   f   8   -   4   0   d   8   -   9   d   0   a   -   f
2020-04-06T13:20:08.7403772Z 0000b60   f   9   f   7   e   9   6   d   7   b   d   /   0   /   p   a
2020-04-06T13:20:08.7404284Z 0000b70   r   t   -   2   -   8  \n   C   o   m   p   l   e   t   e   d
2020-04-06T13:20:08.7404647Z 0000b80       2   1   .   3       K   i   B   /   3   4   0   .   7    
2020-04-06T13:20:08.7405008Z 0000b90   K   i   B       (   4   1   .   9       K   i   B   /   s   )
2020-04-06T13:20:08.7405358Z 0000ba0       w   i   t   h       1   0   2       f   i   l   e   (   s
2020-04-06T13:20:08.7405721Z 0000bb0   )       r   e   m   a   i   n   i   n   g  \r   C   o   m   p
2020-04-06T13:20:08.7406065Z 0000bc0   l   e   t   e   d       2   5   .   0       K   i   B   /   3
2020-04-06T13:20:08.7406673Z 0000bd0   4   0   .   7       K   i   B       (   4   8   .   9       K
2020-04-06T13:20:08.7407014Z 0000be0   i   B   /   s   )       w   i   t   h       1   0   2       f
2020-04-06T13:20:08.7407371Z 0000bf0   i   l   e   (   s   )       r   e   m   a   i   n   i   n   g
2020-04-06T13:20:08.7407757Z 0000c00  \r   d   o   w   n   l   o   a   d   :       s   3   :   /   /
2020-04-06T13:20:08.7408367Z 0000c10   f   l   i   n   k   -   i   n   t   e   g   r   a   t   i   o
2020-04-06T13:20:08.7408893Z 0000c20   n   -   t   e   s   t   s   /   t   e   m   p   /   t   e   s
2020-04-06T13:20:08.7409226Z 0000c30   t   _   s   t   r   e   a   m   i   n   g   _   f   i   l   e
2020-04-06T13:20:08.7409750Z 0000c40   _   s   i   n   k   -   7   b   0   7   7   2   1   2   -   d
2020-04-06T13:20:08.7410268Z 0000c50   9   f   8   -   4   0   d   8   -   9   d   0   a   -   f   f
2020-04-06T13:20:08.7410630Z 0000c60   9   f   7   e   9   6   d   7   b   d   /   0   /   p   a   r
2020-04-06T13:20:08.7411158Z 0000c70   t   -   2   -   1   1       t   o       h   o   s   t   d   i
2020-04-06T13:20:08.7411673Z 0000c80   r   /   t   e   m   p   -   t   e   s   t   -   d   i   r   e
2020-04-06T13:20:08.7412206Z 0000c90   c   t   o   r   y   -   3   5   0   6   5   0   6   7   2   8
2020-04-06T13:20:08.7412553Z 0000ca0   9   /   t   e   m   p   /   t   e   s   t   _   s   t   r   e
2020-04-06T13:20:08.7413082Z 0000cb0   a   m   i   n   g   _   f   i   l   e   _   s   i   n   k   -
2020-04-06T13:20:08.7413613Z 0000cc0   7   b   0   7   7   2   1   2   -   d   9   f   8   -   4   0
2020-04-06T13:20:08.7414131Z 0000cd0   d   8   -   9   d   0   a   -   f   f   9   f   7   e   9   6
2020-04-06T13:20:08.7414815Z 0000ce0   d   7   b   d   /   0   /   p   a   r   t   -   2   -   1   1
2020-04-06T13:20:08.7415245Z 0000cf0  \n   C   o   m   p   l   e   t   e   d       2   5   .   0    
2020-04-06T13:20:08.7415604Z 0000d00   K   i   B   /   3   4   0   .   7       K   i   B       (   4
2020-04-06T13:20:08.7415949Z 0000d10   8   .   9       K   i   B   /   s   )       w   i   t   h    
2020-04-06T13:20:08.7416309Z 0000d20   1   0   1       f   i   l   e   (   s   )       r   e   m   a
2020-04-06T13:20:08.7416668Z 0000d30   i   n   i   n   g  \r   C   o   m   p   l   e   t   e   d    
2020-04-06T13:20:08.7417013Z 0000d40   2   8   .   6       K   i   B   /   3   4   0   .   7       K
2020-04-06T13:20:08.7417370Z 0000d50   i   B       (   5   4   .   4       K   i   B   /   s   )    
2020-04-06T13:20:08.7417714Z 0000d60   w   i   t   h       1   0   1       f   i   l   e   (   s   )
2020-04-06T13:20:08.7418076Z 0000d70       r   e   m   a   i   n   i   n   g  \r   d   o   w   n   l
2020-04-06T13:20:08.7418648Z 0000d80   o   a   d   :       s   3   :   /   /   f   l   i   n   k   -
2020-04-06T13:20:08.7419198Z 0000d90   i   n   t   e   g   r   a   t   i   o   n   -   t   e   s   t
2020-04-06T13:20:08.7419567Z 0000da0   s   /   t   e   m   p   /   t   e   s   t   _   s   t   r   e
2020-04-06T13:20:08.7420079Z 0000db0   a   m   i   n   g   _   f   i   l   e   _   s   i   n   k   -
2020-04-06T13:20:08.7420614Z 0000dc0   7   b   0   7   7   2   1   2   -   d   9   f   8   -   4   0
2020-04-06T13:20:08.7421247Z 0000dd0   d   8   -   9   d   0   a   -   f   f   9   f   7   e   9   6
2020-04-06T13:20:08.7421942Z 0000de0   d   7   b   d   /   1   /   p   a   r   t   -   2   -   1   3
2020-04-06T13:20:08.7422343Z 0000df0       t   o       h   o   s   t   d   i   r   /   t   e   m   p
2020-04-06T13:20:08.7422945Z 0000e00   -   t   e   s   t   -   d   i   r   e   c   t   o   r   y   -
2020-04-06T13:20:08.7423359Z 0000e10   3   5   0   6   5   0   6   7   2   8   9   /   t   e   m   p
2020-04-06T13:20:08.7423759Z 0000e20   /   t   e   s   t   _   s   t   r   e   a   m   i   n   g   _
2020-04-06T13:20:08.7424370Z 0000e30   f   i   l   e   _   s   i   n   k   -   7   b   0   7   7   2
2020-04-06T13:20:08.7424968Z 0000e40   1   2   -   d   9   f   8   -   4   0   d   8   -   9   d   0
2020-04-06T13:20:08.7425577Z 0000e50   a   -   f   f   9   f   7   e   9   6   d   7   b   d   /   1
2020-04-06T13:20:08.7426180Z 0000e60   /   p   a   r   t   -   2   -   1   3  \n                    
2020-04-06T13:20:08.7426460Z 0000e6b
2020-04-06T13:20:13.1615895Z rm: cannot remove '/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-shaded-netty-tcnative-static-*.jar': No such file or directory
2020-04-06T13:20:13.3069822Z 5c0f753ccbc8092ee92b422b9efc0e2f8b4895f4512cd28ab6e59b995cabacad
2020-04-06T13:20:13.3541676Z 5c0f753ccbc8092ee92b422b9efc0e2f8b4895f4512cd28ab6e59b995cabacad
2020-04-06T13:20:13.3570646Z [FAIL] Test script contains errors.
2020-04-06T13:20:13.3577141Z Checking of logs skipped.
2020-04-06T13:20:13.3577527Z 
2020-04-06T13:20:13.3578665Z [FAIL] 'Streaming File Sink s3 end-to-end test' failed after 4 minutes and 38 seconds! Test exited with exit code 1
{code}",,aljoscha,leonard,rmetzger,zjwang,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11662: [FLINK-17010][hotfix][e2e] Disable broken 'Streaming File Sink s3 e2e
URL: https://github.com/apache/flink/pull/11662
 
 
   This test is currently broken. There's a blocker for it.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Apr/20 16:57;githubbot;600","aljoscha commented on pull request #11662: [FLINK-17010][hotfix][e2e] Disable broken 'Streaming File Sink s3 e2e
URL: https://github.com/apache/flink/pull/11662
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Apr/20 18:38;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15772,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 14:18:11 UTC 2020,,,,,,,,,,"0|z0dbo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/20 04:41;rmetzger;Another case on Travis: https://travis-ci.org/github/apache/flink/jobs/671655595?utm_medium=notification&utm_source=slack;;;","07/Apr/20 04:45;rmetzger;I think this test is permanently broken. Upgrading to blocker: https://travis-ci.org/github/apache/flink/jobs/671655602?utm_medium=notification&utm_source=slack;;;","07/Apr/20 04:47;rmetzger;Azure nightlies: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7123&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","07/Apr/20 07:57;rmetzger;I'm currently testing whether reverting https://github.com/georyan/flink/commit/1a06dd82a06d2fb80b4a878e1bc90eda72d1c307 solves the problem: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=281&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16;;;","07/Apr/20 08:53;rmetzger;Test run with secrets set: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=282&view=results;;;","07/Apr/20 17:05;rmetzger;Reverting does not solve the problem. I suggest to disable the test.;;;","07/Apr/20 17:37;aljoscha;This one is the culprit: [https://github.com/apache/flink/commit/5f2f7d637d8073b1034f8c1124f52604745936eb]

I don't yet know why, though.
 ;;;","07/Apr/20 18:37;aljoscha;The reason is that the test redirects stdout to get the sorted results:
{code}
get_complete_result > ""${TEST_DATA_DIR}/complete_result""
{code}

[5f2f7d637d8073b1034f8c1124f52604745936eb|https://github.com/apache/flink/commit/5f2f7d637d8073b1034f8c1124f52604745936eb] removed the {{--quiet}} parameter from the s3 fetch command, which changed stdout.

You can see that additional line in the log output even:
{code}
2020-04-06T13:20:08.7299371Z 0000040   n   g  \r   d   o   w   n   l   o   a   d   :       s   3   :
{code}

The {{s3}} command is printing which file it is downloading.

By the way, this failure is deterministic, so every commit since [5f2f7d637d8073b1034f8c1124f52604745936eb|https://github.com/apache/flink/commit/5f2f7d637d8073b1034f8c1124f52604745936eb]  (yesterday) was committed even while the build was failing.
;;;","07/Apr/20 18:37;aljoscha;master: a9f3e2ffa17967c965a7d5b2cc91abe0e8a61ef8;;;","07/Apr/20 18:41;rmetzger;Thanks a lot!;;;","08/Apr/20 03:00;zjwang;The same issue still happened in [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7167&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5];;;","08/Apr/20 03:04;zjwang;Another case [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7155&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5];;;","08/Apr/20 06:21;rmetzger;These two builds do not include Aljoschas fix.
The first build that contains his fix is ""20200407.12"", the two builds you are linking are ""20200407.11"" and ""20200407.10"".;;;","08/Apr/20 14:18;zjwang;Great, this issue seems resolved. Sorry for misleading.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle null value for HADOOP_CONF_DIR/HADOOP_HOME env in AbstractKubernetesParameters#getLocalHadoopConfigurationDirectory,FLINK-17008,13296503,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,felixzheng,felixzheng,felixzheng,06/Apr/20 12:04,07/Apr/21 07:15,13/Jul/23 08:07,07/Apr/21 07:15,,,,,,,,,,Deployment / Kubernetes,,,,,0,,,,,"{{System.getenv(Constants.ENV_HADOOP_CONF_DIR)}} or {{System.getenv(Constants.HADOOP_HOME)}} could return null value if they are not set in the System environments, it's a minor improvement to take these situations into consideration.",,felixzheng,tison,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20214,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 11:01:50 UTC 2020,,,,,,,,,,"0|z0dbgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/20 11:00;chesnay;What are you proposing? StringUtils#is(Not)Blank already handle null.;;;","09/Apr/20 11:19;felixzheng;Hi, [~chesnay].

If both *{color:#172b4d}ENV_HADOOP_CONF_DIR{color}* and {color:#172b4d}*ENV_HADOOP_HOME* are unset, then the {{possibleHadoopConfPaths}} would contain two elements that it shouldn't have: {{null/etc/hadoop}} and {{null/conf}}, though it does not affect the functionality, users will see the warning log noise from {{HadoopConfMountDecorator}} saying that{color}
{quote}{color:#172b4d}Found 0 files in directory null/etc/hadoop, skip to mount the Hadoop Configuration ConfigMap.{color}
{quote}
{quote}{color:#172b4d}Found 0 files in directory null/etc/hadoop, skip to create the Hadoop Configuration ConfigMap.{color}
{quote}
{color:#172b4d}It's my mistake when introducing the {{HadoopConfMountDecorator}}, given that the incorrect warnings could confuse users, this ticket proposes to fix it.{color};;;","11/May/20 01:59;tison;Sounds reasonable. It would be more expressive in the PR I think.;;;","06/Nov/20 11:01;trohrmann;[~felixzheng] any update on the progress of this improvement?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-runtime tests are crashing the JVM on Java11 because of PowerMock,FLINK-16981,13296168,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,rmetzger,rmetzger,04/Apr/20 08:30,09/Apr/20 18:32,13/Jul/23 08:07,09/Apr/20 18:32,1.10.0,,,,,1.10.1,1.11.0,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,"Nightly travis run: https://travis-ci.org/github/apache/flink/jobs/670686286?utm_medium=notification&utm_source=slack

{code}
22:11:49.063 [INFO] Reactor Summary:
22:11:49.063 [INFO] 
22:11:49.068 [INFO] flink-annotations .................................. SUCCESS [  4.733 s]
22:11:49.069 [INFO] flink-metrics ...................................... SUCCESS [  0.250 s]
22:11:49.069 [INFO] flink-metrics-core ................................. SUCCESS [  3.012 s]
22:11:49.069 [INFO] flink-core ......................................... SUCCESS [01:34 min]
22:11:49.069 [INFO] flink-java ......................................... SUCCESS [ 30.494 s]
22:11:49.074 [INFO] flink-runtime ...................................... FAILURE [25:01 min]
22:11:49.074 [INFO] flink-scala ........................................ SKIPPED
22:11:49.074 [INFO] flink-optimizer .................................... SKIPPED
22:11:49.074 [INFO] flink-clients ...................................... SKIPPED
22:11:49.074 [INFO] flink-streaming-java ............................... SKIPPED
22:11:49.074 [INFO] flink-test-utils ................................... SKIPPED
22:11:49.074 [INFO] flink-runtime-web .................................. SKIPPED
22:11:49.074 [INFO] flink-statebackend-rocksdb ......................... SKIPPED
22:11:49.074 [INFO] flink-streaming-scala .............................. SKIPPED
22:11:49.074 [INFO] flink-scala-shell .................................. SKIPPED
22:11:49.074 [INFO] ------------------------------------------------------------------------
22:11:49.074 [INFO] BUILD FAILURE
22:11:49.074 [INFO] ------------------------------------------------------------------------
22:11:49.074 [INFO] Total time: 27:20 min
22:11:49.077 [INFO] Finished at: 2020-04-03T22:11:49+00:00
22:11:49.355 [INFO] Final Memory: 97M/330M
22:11:49.355 [INFO] ------------------------------------------------------------------------
22:11:49.361 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (default-test) on project flink-runtime_2.11: There are test failures.
22:11:49.362 [ERROR] 
22:11:49.362 [ERROR] Please refer to /home/travis/build/apache/flink/flink-runtime/target/surefire-reports for the individual test results.
22:11:49.362 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
22:11:49.362 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
22:11:49.362 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-runtime && /usr/local/lib/jvm/openjdk11/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-runtime/target/surefire/surefirebooter5965056229397858556.jar /home/travis/build/apache/flink/flink-runtime/target/surefire 2020-04-03T21-44-37_853-jvmRun1 surefire4393983892864834687tmp surefire_47412704678292479303842tmp
22:11:49.362 [ERROR] Error occurred in starting fork, check output in log
22:11:49.362 [ERROR] Process Exit Code: 239
22:11:49.362 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
22:11:49.362 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-runtime && /usr/local/lib/jvm/openjdk11/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-runtime/target/surefire/surefirebooter5965056229397858556.jar /home/travis/build/apache/flink/flink-runtime/target/surefire 2020-04-03T21-44-37_853-jvmRun1 surefire4393983892864834687tmp surefire_47412704678292479303842tmp
22:11:49.362 [ERROR] Error occurred in starting fork, check output in log
22:11:49.362 [ERROR] Process Exit Code: 239
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
22:11:49.362 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
22:11:49.362 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
22:11:49.362 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
22:11:49.362 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
22:11:49.363 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
22:11:49.363 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
22:11:49.363 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
22:11:49.363 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
22:11:49.363 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
22:11:49.363 [ERROR] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
22:11:49.363 [ERROR] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
22:11:49.363 [ERROR] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
22:11:49.363 [ERROR] at java.base/java.lang.reflect.Method.invoke(Method.java:566)
22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
22:11:49.363 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
22:11:49.363 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-runtime && /usr/local/lib/jvm/openjdk11/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-runtime/target/surefire/surefirebooter5965056229397858556.jar /home/travis/build/apache/flink/flink-runtime/target/surefire 2020-04-03T21-44-37_853-jvmRun1 surefire4393983892864834687tmp surefire_47412704678292479303842tmp
22:11:49.363 [ERROR] Error occurred in starting fork, check output in log
22:11:49.363 [ERROR] Process Exit Code: 239
22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)
22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)
22:11:49.363 [ERROR] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
22:11:49.363 [ERROR] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
22:11:49.363 [ERROR] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
22:11:49.363 [ERROR] at java.base/java.lang.Thread.run(Thread.java:834)
22:11:49.363 [ERROR] -> [Help 1]
22:11:49.363 [ERROR] 
22:11:49.363 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
22:11:49.363 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
22:11:49.363 [ERROR] 
22:11:49.363 [ERROR] For more information about the errors and possible solutions, please read the following articles:
22:11:49.363 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
22:11:49.363 [ERROR] 
22:11:49.363 [ERROR] After correcting the problems, you can resume the build with the command
22:11:49.364 [ERROR]   mvn <goals> -rf :flink-runtime_2.11
{code}",,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11633: [FLINK-16981][tests] Add global PowerMock exclusions
URL: https://github.com/apache/flink/pull/11633
 
 
   Potential fix for PowerMock LinkageError issues.
   It is well known that certain classes should be explicitly ignored, but specifying them on each test is tedious, easily forgotten and led to inconsistent configurations.
   
   This PR makes use of module-wide excludes through a configuration file.
   A project wide ignore would of course be awesome, but it appears this isn't possible. By virtue of us usually not not adding more PowerMock-base tests this should work _fine_ though.
   
   The effectiveness can be easily checked by running `RocksDBAsyncSnapshotTest`; with the configuration file no error will be logged, without a LinkageError will appear.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Apr/20 12:33;githubbot;600","zentol commented on pull request #11633: [FLINK-16981][tests] Add global PowerMock exclusions
URL: https://github.com/apache/flink/pull/11633
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Apr/20 10:08;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 18:32:25 UTC 2020,,,,,,,,,,"0|z0d9xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/20 08:31;rmetzger;For those looking into this issue, the log above also contains this:

{code}
log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.
log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by 
log4j:ERROR [jdk.internal.loader.ClassLoaders$AppClassLoader@4b85612c] whereas object of type 
log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.powermock.core.classloader.javassist.JavassistMockClassLoader@635eaaf1].
log4j:ERROR Could not instantiate appender named ""file"".
log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.
log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by 
log4j:ERROR [jdk.internal.loader.ClassLoaders$AppClassLoader@4b85612c] whereas object of type 
log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.powermock.core.classloader.javassist.JavassistMockClassLoader@635eaaf1].
log4j:ERROR Could not instantiate appender named ""console"".
log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.
log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by 
log4j:ERROR [jdk.internal.loader.ClassLoaders$AppClassLoader@4b85612c] whereas object of type 
log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.powermock.core.classloader.javassist.JavassistMockClassLoader@635eaaf1].
log4j:ERROR Could not instantiate appender named ""console"".
log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.
log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by 
log4j:ERROR [jdk.internal.loader.ClassLoaders$AppClassLoader@4b85612c] whereas object of type 
log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.powermock.core.classloader.javassist.JavassistMockClassLoader@635eaaf1].
log4j:ERROR Could not instantiate appender named ""console"".
log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.
log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by 
log4j:ERROR [jdk.internal.loader.ClassLoaders$AppClassLoader@4b85612c] whereas object of type 
log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.powermock.core.classloader.javassist.JavassistMockClassLoader@635eaaf1].
log4j:ERROR Could not instantiate appender named ""console"".
{code};;;","06/Apr/20 11:23;chesnay;Improved powermock exclusions merged:
master: bf671fb3d2a05f3f684d0acbea049d04cd014b17
1.10: 6bd76c5ab976a80b77c4551e78cee4392d6e96b2

Let's see if that resolves the issue.;;;","09/Apr/20 11:02;chesnay;[~rmetzger] It seems likely that the underlying cause is similar to FLINK-16973; what do you think about making this JIRa only about the Powermock problem (and closing it)?;;;","09/Apr/20 18:32;rmetzger;Yep, closing it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python UDF doesn't work with protobuf 3.6.1,FLINK-16980,13296167,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,04/Apr/20 08:30,04/Apr/20 12:22,13/Jul/23 08:07,04/Apr/20 12:08,1.10.0,,,,,1.10.1,1.11.0,,,API / Python,,,,,0,pull-request-available,,,,PyFlink UDF execution module is not compatible with protobuf 3.6.1 because it uses a newer interface to access the enum value defined in proto model. We need to fix this.,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,"HuangXingBo commented on pull request #11631: [FLINK-16980][python] Fix PyFlink UDF execution module is not compatible with protobuf 3.6.1
URL: https://github.com/apache/flink/pull/11631
 
 
   ## What is the purpose of the change
   
   *This pull request will Fix PyFlink UDF execution module is not compatible with protobuf 3.6.1*
   
   ## Brief change log
   
     - *Use a compatible way of enum in protobuf*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Apr/20 09:18;githubbot;600","HuangXingBo commented on pull request #11632: [FLINK-16980][python] Fix PyFlink UDF execution module is not compatible with protobuf 3.6.1
URL: https://github.com/apache/flink/pull/11632
 
 
   ## What is the purpose of the change
   
   *This pull request will Fix PyFlink UDF execution module is not compatible with protobuf 3.6.1*
   
   ## Brief change log
   
     - *Use a compatible way of enum in protobuf*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Apr/20 10:02;githubbot;600","dianfu commented on pull request #11631: [FLINK-16980][python] Fix Python UDF to work with protobuf 3.6.1
URL: https://github.com/apache/flink/pull/11631
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Apr/20 12:04;githubbot;600","dianfu commented on pull request #11632: [FLINK-16980][python] Fix Python UDF to work with protobuf 3.6.1
URL: https://github.com/apache/flink/pull/11632
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Apr/20 12:06;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 04 12:08:15 UTC 2020,,,,,,,,,,"0|z0d9x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/20 12:08;dian.fu;Merged in
- master via d859ff3cc933490584446223589f5a8798c0c18b
- release-1.10 via 4af03a69d55b3ed1c5bdd21a852a76e8ac35697c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-sql-connector-hive-1.2.2_2.11 doesn't compile on JDK11,FLINK-16979,13296165,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,rmetzger,rmetzger,04/Apr/20 08:24,05/Apr/20 18:08,13/Jul/23 08:07,05/Apr/20 18:08,1.11.0,,,,,1.11.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"Both the jdk11 compile and e2e test failed in this nightly: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7052&view=logs&j=946871de-358d-5815-3994-8175615bc253&t=a4536961-0635-5533-730b-7dc5e128220e

{code}
[ERROR] Failed to execute goal on project flink-sql-connector-hive-1.2.2_2.11: Could not resolve dependencies for project org.apache.flink:flink-sql-connector-hive-1.2.2_2.11:jar:1.11-SNAPSHOT: Could not find artifact jdk.tools:jdk.tools:jar:1.6 at specified path /usr/lib/jvm/adoptopenjdk-11-hotspot-amd64/../lib/tools.jar -> [Help 1]
{code}
",,rmetzger,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11635: [FLINK-16979][hive][build] Exclude jdk.tools
URL: https://github.com/apache/flink/pull/11635
 
 
   `jdk.tools` isn't available on Java 9+. As a system dependency explicitly depending on it is usually unnecessary, so it is safe to exclude in general.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Apr/20 09:57;githubbot;600","zentol commented on pull request #11635: [FLINK-16979][hive][build] Exclude jdk.tools
URL: https://github.com/apache/flink/pull/11635
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Apr/20 18:08;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 05 18:08:31 UTC 2020,,,,,,,,,,"0|z0d9wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/20 08:37;rmetzger;Same error on travis https://travis-ci.org/github/apache/flink/jobs/670584782?utm_medium=notification&utm_source=slack;;;","05/Apr/20 07:05;rmetzger;error persists in nightly runs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7069&view=logs&j=946871de-358d-5815-3994-8175615bc253&t=a4536961-0635-5533-730b-7dc5e128220e;;;","05/Apr/20 18:08;chesnay;master: 105379f8e36aec83de8bef92003d4c686023e7ab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update chinese documentation for ListCheckpointed deprecation,FLINK-16976,13296062,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzhanhua,aljoscha,aljoscha,03/Apr/20 16:05,29/Jun/20 05:53,13/Jul/23 08:07,29/Jun/20 05:53,,,,,,1.12.0,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,"The change for the english documentation is in https://github.com/apache/flink/commit/10aadfc6906a1629f7e60eacf087e351ba40d517

The original Jira issue is FLINK-6258.",,aljoscha,jark,klion26,rmetzger,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-6258,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 29 05:53:30 UTC 2020,,,,,,,,,,"0|z0d99s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/20 13:41;zhangzhanhua;I can help to update chinese documentation ;;;","17/Apr/20 09:29;aljoscha;also cc [~jark];;;","17/Apr/20 12:08;jark;Assigned to you [~zhangzhanhua];;;","19/Apr/20 16:00;zhangzhanhua;pushed to my fork: https://github.com/zzh1985/flink , there is an open pull request existed for 
[FLINK-16101]  https://github.com/apache/flink/pull/11664 . Do I need to start a new pull request?;;;","09/Jun/20 08:40;zhangzhanhua;[~jark] This issue can not be handle because the corresponding English doc already be modifyed many times. Any suggestion?;;;","09/Jun/20 11:19;jark;[~zhangzhanhua], you can just update Chinese docs according to this commit: https://github.com/apache/flink/commit/10aadfc6906a1629f7e60eacf087e351ba40d517
Remove the parts of ListCheckpointed.;;;","29/Jun/20 05:53;rmetzger;Merged to master in 41daafd074f6df6529ef4e830dde67295c29a8fa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Various builds failing with ""Corrupted STDOUT by directly writing""",FLINK-16973,13296031,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,pnowojski,pnowojski,03/Apr/20 13:53,26/Mar/21 03:14,13/Jul/23 08:07,26/Mar/21 03:13,1.11.0,,,,,1.11.0,,,,Build System,Build System / Azure Pipelines,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7028&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=f66652e3-384e-5b25-be29-abfea69ea8da (kafka/gelly)
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7021&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=40015e30-d9f1-555e-929f-497bfa903ca8 (libraries)

{noformat}
[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /__w/3/s/flink-connectors/flink-connector-kafka/target/surefire-reports/2020-04-03T11-40-23_195-jvmRun1.dumpstream
{noformat}

followed by:
{noformat}

[ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:245)
[ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
[ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
[ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
[ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
[ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
[ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
[ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
[ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
[ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
[ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
[ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
[ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
[ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
[ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{noformat}
",,liyu,maguowei,pnowojski,rmetzger,trohrmann,yunta,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11638: [FLINK-16973][tests] Add tooling for collecting jvm crash files
URL: https://github.com/apache/flink/pull/11638
 
 
   ## What is the purpose of the change
   
   A lot of tests are failing because of crashed JVMs.
   This PR will add some tooling to search the Flink directory for JVM crash files.
   
   
   
   ## Verifying this change
   
   This change has been tested here: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7072&view=results
   In this test, the ""core"" profile fails with an artificially induced JVM crash. A coredump + some debugging files are attached.
   There was a [previous discussion](https://github.com/apache/flink/pull/11372#discussion_r392367035) about this change. [I tried](https://github.com/rmetzger/flink/commit/cb886dccda18fcb8536ac47dc447d134bb89602c) the proposed approach as well, but files were not properly written into the specified directory.
    
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Apr/20 13:55;githubbot;600","rmetzger commented on pull request #11638: [FLINK-16973][tests] Add tooling for collecting jvm crash files
URL: https://github.com/apache/flink/pull/11638
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Apr/20 11:37;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,FLINK-21900,,,,,,,,,,,,FLINK-17082,,,,,,,,,,,"09/Apr/20 10:07;rmetzger;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12999421/screenshot-1.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 03:13:24 UTC 2021,,,,,,,,,,"0|z0d92w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/20 13:57;pnowojski;Another instance (core)
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7029&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=fb588352-ef18-568d-b447-699986250ccb;;;","07/Apr/20 11:37;rmetzger;Added debug tooling in https://github.com/apache/flink/commit/aad3f7d5dec93455025a0e3e61411a15a7f716f7;;;","09/Apr/20 10:02;rmetzger;Another case, with debug info :) 
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7246&view=logs&j=56781494-ebb0-5eae-f732-b9c397ec6ede&t=1c77081b-ed07-545b-edcc-d6440ebe1964;;;","09/Apr/20 10:08;rmetzger;{code}
# Created at 2020-04-09T02:43:38.684
Corrupted STDOUT by directly writing to native stream in forked JVM 1. Stream 'java.lang.OutOfMemoryError: Java heap space'.
java.lang.IllegalArgumentException: Stream stdin corrupted. Expected comma after third character in command 'java.lang.OutOfMemoryError: Java heap space'.
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$OperationalData.<init>(ForkClient.java:507)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:210)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:177)
	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88)
	at java.lang.Thread.run(Thread.java:748)


# Created at 2020-04-09T02:43:38.684
Corrupted STDOUT by directly writing to native stream in forked JVM 1. Stream 'Dumping heap to java_pid30594.hprof ...'.
java.lang.IllegalArgumentException: Stream stdin corrupted. Expected comma after third character in command 'Dumping heap to java_pid30594.hprof ...'.
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$OperationalData.<init>(ForkClient.java:507)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:210)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:177)
	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88)
	at java.lang.Thread.run(Thread.java:748)


# Created at 2020-04-09T02:44:19.167
Corrupted STDOUT by directly writing to native stream in forked JVM 1. Stream 'Heap dump file created [3274182887 bytes in 40.483 secs]'.
java.lang.IllegalArgumentException: Stream stdin corrupted. Expected comma after third character in command 'Heap dump file created [3274182887 bytes in 40.483 secs]'.
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$OperationalData.<init>(ForkClient.java:507)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:210)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:177)
	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88)
	at java.lang.Thread.run(Thread.java:748)
{code}

The heapdump says:
{code}
pool-1-thread-1"" prio=5 tid=26 RUNNABLE
    at java.lang.OutOfMemoryError.<init>(OutOfMemoryError.java:48)
    at org.mockito.internal.invocation.DefaultInvocationFactory.createInvocation(DefaultInvocationFactory.java:36)
       Local Variable: org.mockito.internal.debugging.LocationImpl#2806340
       Local Variable: java.lang.Object[]#8425684
       Local Variable: java.lang.reflect.Method#13
       Local Variable: org.mockito.internal.invocation.RealMethod$IsIllegal#1
       Local Variable: org.apache.flink.table.client.gateway.Executor$MockitoMock$138782620#1
       Local Variable: org.mockito.internal.creation.settings.CreationSettings#1
    at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:63)
    at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49)
    at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptAbstract(MockMethodInterceptor.java:128)
    at org.apache.flink.table.client.gateway.Executor$MockitoMock$138782620.retrieveResultChanges(<unknown string>)
    at org.apache.flink.table.client.cli.CliTableauResultView.printStreamResults(CliTableauResultView.java:207)
       Local Variable: java.lang.String#1661
       Local Variable: int[]#4885
    at org.apache.flink.table.client.cli.CliTableauResultView.lambda$displayStreamResults$0(CliTableauResultView.java:88)
       Local Variable: java.util.concurrent.atomic.AtomicInteger#1
       Local Variable: org.apache.flink.table.client.cli.CliTableauResultView#2
    at org.apache.flink.table.client.cli.CliTableauResultView$$Lambda$18.run(<unknown string>)
       Local Variable: org.apache.flink.table.client.cli.CliTableauResultView$$Lambda$18#1
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
       Local Variable: java.util.concurrent.Executors$RunnableAdapter#2
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
       Local Variable: java.util.concurrent.ThreadPoolExecutor#3
       Local Variable: java.util.concurrent.FutureTask#2
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
       Local Variable: java.util.concurrent.ThreadPoolExecutor$Worker#2
    at java.lang.Thread.run(Thread.java:748)
{code}

 !screenshot-1.png! 

Heapdump is available here: https://artprodsu6weu.artifacts.visualstudio.com/A03e2a4fd-f647-46c5-a324-527d2c2984ce/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/artifact/cGlwZWxpbmVhcnRpZmFjdDovL3JtZXR6Z2VyL3Byb2plY3RJZC81YmQzZWYwYS00MzU5LTQxYWYtYWJjYS04MTFiMDQwOThkMmUvYnVpbGRJZC83MjQ2L2FydGlmYWN0TmFtZS9sb2dzLWNyb25faGFkb29wMjQxLWxpYnJhcmllcw2/content?format=file&subPath=%2F20200409.1.tar.gz;;;","09/Apr/20 10:15;rmetzger;I will wait for a few more occurrences, or somebody willing to look deeper into heapdumps :) ;;;","09/Apr/20 11:27;chesnay;Possibly related: https://github.com/mockito/mockito/issues/1614;;;","09/Apr/20 11:31;chesnay;If this is indeed it then we might have a problem. While they added an API to clear these mocks in 2.25.0 that we could call at the end of all tests in {{TestLogger}}, I do not know what effect this might have on running tests.;;;","09/Apr/20 14:17;trohrmann;Well it would be a good reason to finally get rid of Mockito ;-);;;","09/Apr/20 18:27;rmetzger;You guys amaze me again and again. I usually look a bit into the problems (assuming it is not helpful), then somebody looks over my analysis with a good explanation :) 
I think [~pnowojski] would also like to get rid of Mockito quite badly :) ;;;","10/Apr/20 08:30;chesnay;If I'm reading this correctly there are 2.7 million mock invocations flying around for the SQL client {{Executor}}. There are only a few mocks of this class, but maybe it is being called a ridiculous number of times, with mockito capturing all calls?

I've opened FLINK-17082 to remove the mocks in question, let's see if that fixes things.;;;","28/Apr/20 17:00;chesnay;[~rmetzger] haven't seen this in a while; I suppose we can close it?;;;","28/Apr/20 18:52;rmetzger;Yes, I agree. Closing ticket.;;;","22/Mar/21 06:07;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15137&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=8738


{code:java}
# Created at 2021-03-21T23:17:41.677
Corrupted STDOUT by directly writing to native stream in forked JVM 1. Stream 'java.lang.OutOfMemoryError: Java heap space'.
java.lang.IllegalArgumentException: Stream stdin corrupted. Expected comma after third character in command 'java.lang.OutOfMemoryError: Java heap space'.
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$OperationalData.<init>(ForkClient.java:507)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:210)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:177)
	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88)
	at java.lang.Thread.run(Thread.java:748)


# Created at 2021-03-21T23:17:41.677
Corrupted STDOUT by directly writing to native stream in forked JVM 1. Stream 'Dumping heap to java_pid15966.hprof ...'.
java.lang.IllegalArgumentException: Stream stdin corrupted. Expected comma after third character in command 'Dumping heap to java_pid15966.hprof ...'.
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$OperationalData.<init>(ForkClient.java:507)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:210)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:177)
	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88)
	at java.lang.Thread.run(Thread.java:748)


# Created at 2021-03-21T23:17:41.703
Corrupted STDOUT by directly writing to native stream in forked JVM 1. Stream 'Heap dump file created [2833993 bytes in 0.027 secs]'.
java.lang.IllegalArgumentException: Stream stdin corrupted. Expected comma after third character in command 'Heap dump file created [2833993 bytes in 0.027 secs]'.
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$OperationalData.<init>(ForkClient.java:507)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:210)
	at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:177)
	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88)
	at java.lang.Thread.run(Thread.java:748)



{code}
;;;","22/Mar/21 06:19;maguowei;another case:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15103&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=8826;;;","26/Mar/21 03:13;yunta;[~maguowei] Closing this ticket as the new core dump problem should be the same as FLINK-21929.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Walkthrough Table Java nightly end-to-end test' [FAIL] Test script contains errors,FLINK-16959,13295963,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,pnowojski,pnowojski,03/Apr/20 07:24,27/Apr/20 10:19,13/Jul/23 08:07,27/Apr/20 10:19,,,,,,1.11.0,,,,Build System / Azure Pipelines,Table SQL / API,Table SQL / Planner,Tests,,0,test-stability,,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7009&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334


{noformat}
java.lang.AbstractMethodError: Method org/apache/flink/walkthrough/common/table/SpendReportTableSink.consumeDataSet(Lorg/apache/flink/api/java/DataSet;)Lorg/apache/flink/api/java/operators/DataSink; is abstract
	at org.apache.flink.walkthrough.common.table.SpendReportTableSink.consumeDataSet(SpendReportTableSink.java) ~[?:?]
	at org.apache.flink.table.api.internal.BatchTableEnvImpl.writeToSink(BatchTableEnvImpl.scala:133) ~[flink-table_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.table.api.internal.TableEnvImpl.insertInto(TableEnvImpl.scala:732) ~[flink-table_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.table.api.internal.TableEnvImpl.insertInto(TableEnvImpl.scala:675) ~[flink-table_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.table.api.internal.TableImpl.insertInto(TableImpl.java:409) ~[flink-table_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.walkthrough.SpendReport.main(SpendReport.java:41) ~[?:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_242]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_242]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:142) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:662) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:210) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:893) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:966) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_242]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_242]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:966) [flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]

[FAIL] Test script contains errors.

{noformat}
",,lzljs3620320,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16535,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 10:19:13 UTC 2020,,,,,,,,,,"0|z0d8ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/20 07:31;pnowojski;Reverted 9ffa85aaa1b9025e3d2becdc084a02aa8440d4d9 as 6a42bf6e72;;;","07/Apr/20 02:50;lzljs3620320;CC: [~godfreyhe];;;","09/Apr/20 09:44;rmetzger;After Piotr's revert, it seems that something for FLINK-16535 has been merged again, re-enabling this bug again.
Should we revert again?
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7246&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334;;;","09/Apr/20 09:50;rmetzger;I don't understand why the CI run in the pull request passed? https://github.com/apache/flink/pull/11657;;;","09/Apr/20 09:56;rmetzger;Actually the build adding the commit is passing: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7182&view=results;;;","09/Apr/20 09:56;rmetzger;This might be a problem of the CI setup;;;","09/Apr/20 09:58;rmetzger;I'm assigning the issue to myself to investigate.;;;","27/Apr/20 10:19;rmetzger;Closing. Issue has not appeared again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassRelocator uses JDK 11 APIs,FLINK-16954,13295950,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,aljoscha,chesnay,chesnay,03/Apr/20 06:47,06/Apr/20 11:15,13/Jul/23 08:07,06/Apr/20 11:15,1.9.3,,,,,1.9.3,,,,API / Type Serialization System,,,,,0,pull-request-available,,,,"Usage of JDK 11 APIs in 0bc3d69e036ede36d11daa65bf6541e5c3a209c3.

{code:java}
06:35:03.332 [ERROR] /home/travis/build/flink-ci/flink-mirror/flink-core/src/test/java/org/apache/flink/api/common/typeutils/ClassRelocator.java:[21,38] package jdk.internal.org.objectweb.asm does not exist
06:35:03.332 [ERROR] /home/travis/build/flink-ci/flink-mirror/flink-core/src/test/java/org/apache/flink/api/common/typeutils/ClassRelocator.java:[22,38] package jdk.internal.org.objectweb.asm does not exist
06:35:03.332 [ERROR] /home/travis/build/flink-ci/flink-mirror/flink-core/src/test/java/org/apache/flink/api/common/typeutils/ClassRelocator.java:[23,46] package jdk.internal.org.objectweb.asm.commons does not exist
06:35:03.332 [ERROR] /home/travis/build/flink-ci/flink-mirror/flink-core/src/test/java/org/apache/flink/api/common/typeutils/ClassRelocator.java:[24,46] package jdk.internal.org.objectweb.asm.commons does not exist
06:35:03.332 [ERROR] /home/travis/build/flink-ci/flink-mirror/flink-core/src/test/java/org/apache/flink/api/common/typeutils/ClassRelocator.java:[158,81] cannot find symbol
06:35:03.332 [ERROR] symbol:   class ClassReader
06:35:03.332 [ERROR] location: class org.apache.flink.api.common.typeutils.ClassRelocator.ClassRemapper
06:35:03.332 [ERROR] /home/travis/build/flink-ci/flink-mirror/flink-core/src/test/java/org/apache/flink/api/common/typeutils/ClassRelocator.java:[158,32] cannot find symbol
06:35:03.332 [ERROR] symbol:   class ClassWriter
06:35:03.332 [ERROR] location: class org.apache.flink.api.common.typeutils.ClassRelocator.ClassRemapper
06:35:03.332 [ERROR] /home/travis/build/flink-ci/flink-mirror/flink-core/src/test/java/org/apache/flink/api/common/typeutils/ClassRelocator.java:[165,32] cannot find symbol
06:35:03.334 [ERROR] symbol:   class ClassReader
06:35:03.334 [ERROR] location: class org.apache.flink.api.common.typeutils.ClassRelocator.ClassRemapper
{code}",,aljoscha,klion26,tzulitai,,,,,,,,,,,,,,,,,,,"aljoscha commented on pull request #11625: [FLINK-16954] Don't use JDK11 APIs in ClassRelocator
URL: https://github.com/apache/flink/pull/11625
 
 
   We instead use asm-7, which is normally not available in the Flink 1.9.x
   line, as a test dependency.
   
   cc @zentol 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Apr/20 08:19;githubbot;600","aljoscha commented on pull request #11625: [FLINK-16954] Don't use JDK11 APIs in ClassRelocator
URL: https://github.com/apache/flink/pull/11625
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Apr/20 11:15;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,FLINK-11767,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 06 11:15:35 UTC 2020,,,,,,,,,,"0|z0d8kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/20 06:48;chesnay;ping [~aljoscha] [~igal];;;","03/Apr/20 07:51;aljoscha;It seems we need the {{asm-commons}} package which is not available in the shaded asm6 release. Our shaded asm7 release includes that, though.;;;","03/Apr/20 07:53;tzulitai;[~aljoscha] That was actually the reason why the new TypeSerializer upgrade test base was not merged back then for 1.9.0, and postponed for 1.10.0.
I think we shouldn't have backported those tests to release-1.9?;;;","03/Apr/20 10:55;aljoscha;But that would mean we only check serializer compatibility from 1.10 onwards, right? We already have a bit of a gap because we don't test from 1.8.;;;","03/Apr/20 10:58;tzulitai;[~aljoscha] true. Having just looked at your PR, +1 to go with your approach.;;;","06/Apr/20 11:15;aljoscha;release-1.9: 85f69d4a2256e3759a23ae2ebb1f0d354811d1d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArtifactResolutionException: Could not transfer artifact.  Entry [...] has not been leased from this pool,FLINK-16947,13295827,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,pnowojski,pnowojski,02/Apr/20 15:44,28/May/21 08:59,13/Jul/23 08:07,18/Feb/21 19:39,,,,,,1.13.0,,,,Build System / Azure Pipelines,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6982&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5
Build of flink-metrics-availability-test failed with:
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (end-to-end-tests) on project flink-metrics-availability-test: Unable to generate classpath: org.apache.maven.artifact.resolver.ArtifactResolutionException: Could not transfer artifact org.apache.maven.surefire:surefire-grouper:jar:2.22.1 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): Entry [id:13][route:{s}->https://maven-central-eu.storage-download.googleapis.com:443][state:null] has not been leased from this pool
[ERROR] org.apache.maven.surefire:surefire-grouper:jar:2.22.1
[ERROR] 
[ERROR] from the specified remote repositories:
[ERROR] google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/, releases=true, snapshots=false),
[ERROR] apache.snapshots (https://repository.apache.org/snapshots, releases=false, snapshots=true)
[ERROR] Path to dependency:
[ERROR] 1) dummy:dummy:jar:1.0
[ERROR] 2) org.apache.maven.surefire:surefire-junit47:jar:2.22.1
[ERROR] 3) org.apache.maven.surefire:common-junit48:jar:2.22.1
[ERROR] 4) org.apache.maven.surefire:surefire-grouper:jar:2.22.1
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :flink-metrics-availability-test

{noformat}
",,dian.fu,dwysakowicz,godfreyhe,hxbks2ks,lhotari,maguowei,mapohl,rmetzger,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 07 10:21:42 UTC 2021,,,,,,,,,,"0|z0d7tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/20 19:00;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6999&view=results;;;","08/Apr/20 06:37;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7179&view=logs&j=7bafe89a-737e-5a81-708c-24b72a2345fc&t=8f0197c1-92aa-5b5f-4284-1ae542d75a1e
;;;","09/Apr/20 10:12;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7246&view=logs&j=d7c8e8bb-fe77-5ff6-da77-1ab29e1d1ea5&t=e55b7fb2-dc3a-5c43-de38-fa1358e27ddb;;;","11/Apr/20 16:43;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7348&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","14/Apr/20 15:24;pnowojski;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7447&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=c9d840c6-cbef-5643-7ca0-8d92b566a661;;;","20/Apr/20 05:43;rmetzger;This also happens with the maven mirror in the same data center:
""[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (default-test) on project flink-metrics-core: Unable to generate classpath: org.apache.maven.artifact.resolver.ArtifactResolutionException: Could not transfer artifact org.apache.maven.surefire:surefire-grouper:jar:2.22.1 from/to alicloud-mvn-mirror (http://mavenmirror.alicloud.dak8s.net:8888/repository/maven-central/): Entry [id:11][route:{}->http://mavenmirror.alicloud.dak8s.net:8888][state:null] has not been leased from this pool""

It is probably a bug in the http wagon connector in maven.

https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7739&view=logs&j=8f07b2a4-809b-5b4b-72cb-057a2a4800e5&t=94d9ac6d-1d28-5d1b-a2e7-5d8447626b00
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7739&view=logs&j=ac1ca734-b7f5-591e-e57d-c64fab7b62ae&t=809b7cd3-8495-55cd-c799-281d673c530f;;;","20/Apr/20 08:41;chesnay;Maven 3.2.5 uses wagon-http 2.8 which uses apache httpClient 4.3.2 which has a related issue HTTPCLIENT-1453.

It would be good to understand though why we aren't seeing this Travis.;;;","20/Apr/20 18:05;rmetzger;That is indeed a very good observation. The Alibaba machines are probably running in a somewhat unstable network environment.
I suspect the rate-limiting of the network (the machines are capped at 100MBit/s) to cause this. When I download stuff, the variability of download rates is crazy (sometimes wget reports 150 kb/s for a few minutes, then it jumps to 11 MB/s and finishes in a few seconds).
Other indications for network issues: 
- the downloading of the maven cache (probably also because of a library that is not able to deal with the broken network) FLINK-16411
- the conda download in the python tests (FLINK-17188)
- maven 3.2.5 having issues downloading from google / apache maven central mirrors.

I'm wondering whether we should build our own maven version with a newer http client version (if we want to get rid of this problem as well);;;","21/Apr/20 06:08;rmetzger;Mh, this is a case (against the google mirror) from a machine hosted by Microsoft: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7815&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","23/Apr/20 09:53;rmetzger;Another case on a MSFT hosted instance: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7891&view=logs&j=611400c5-3e21-5738-5497-354ca409d74b&t=e5de9f9d-ffb6-5f0e-0f2f-1a2a35ec8a6a;;;","27/Apr/20 06:09;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=273&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=662e1729-9aac-55e2-8268-b965b3860e1f;;;","27/Apr/20 12:04;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=294&view=logs&j=1454c523-5777-5d91-a870-f026a11d0383&t=160c3171-f94a-5870-7346-5c8980c235f3;;;","27/Apr/20 13:57;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=301&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa;;;","28/Apr/20 07:41;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=338&view=logs&j=0cc9eec4-8a2b-5ab4-c983-cd86f93e4003&t=361d2e73-bdce-5cb5-9c30-a4e220be55d5;;;","28/Apr/20 07:42;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=338&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=6f8201e9-1579-595a-9d2b-7158b26b4c57;;;","04/May/20 15:14;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=558&view=logs&j=1454c523-5777-5d91-a870-f026a11d0383&t=160c3171-f94a-5870-7346-5c8980c235f3;;;","04/May/20 15:24;rmetzger;MSFT hosted: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7944&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=2f5b54d0-1d28-5b01-d344-aa50ffe0cdf8;;;","05/May/20 06:10;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=585&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=d4549d78-6fab-5c0c-bdb9-abaafb66ea8b
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=585&view=logs&j=ac1ca734-b7f5-591e-e57d-c64fab7b62ae&t=809b7cd3-8495-55cd-c799-281d673c530f;;;","05/May/20 06:38;rmetzger;We are getting an exception relating to connection pooling, even though we pass {{-Dmaven.wagon.http.pool=false}} to Maven. I have launched 12 builds overnight, with that flag removed. 

**{{-Dmaven.wagon.http.pool=false}} REMOVED:**
Out of the 132 jobs, I had the following issues:
- 2x ""Read timed out""
- 4x ""Premature end of Content-Length delimited message body""
- 1x unrelated: ""MVN exited with EXIT CODE: 143."" 

**{{-Dmaven.wagon.http.pool=false}} SET:**
Out of the 132 jobs, I had the following issues:
- 3x ""Premature end of Content-Length delimited message body""
- 2x ""has not been leased from this pool""
- 16x unrelated: ""MVN exited with EXIT CODE: 143."" (while having BUILD SUCCESS)
- 1x unrelated ""ZooKeeperHighAvailabilityITCase#testRestoreBehaviourWithFaultyStateHandles""


I'm going to run 132 jobs with {{-Dmaven.wagon.http.pool=false}} set (our normal build), and I'm going to run 132 jobs with {{-Dhttp.keepAlive=false}}.;;;","05/May/20 09:26;rmetzger;{quote}Maven 3.2.5 uses wagon-http 2.8 which uses apache httpClient 4.3.2 which has a related issue HTTPCLIENT-1453.{quote}

I was able to run Maven 3.2.5 with the wagon-http-3.1.0 dependency: https://github.com/rmetzger/flink-ci/commit/6b11569f6bc87765d1b521ee7b2013f72b8bb0ee
It uses HttpClient 4.5.5: https://issues.apache.org/jira/browse/WAGON-513.
I will execute 12 tests with the modified Maven 3.2.5 version and report back.;;;","05/May/20 13:41;rmetzger;**maven 3.2.5 with maven 3.5.4 http code**
Out of the 132 jobs, I had the following issues:
- 1x ""Premature end of Content-Length delimited message body""
- 3x unrelated ""RemoteInputChannelTest.testConcurrentOnSenderBacklogAndRecycle"", ""Timeout waiting for HMS to start"", ""RocksDBWriteBatchPerformanceTest""

It seems that the custom maven approach is promising, but I can't rule out that the network issues are not permanent. I will conduct further tests.;;;","06/May/20 12:44;rmetzger;*maven 3.2.5 with maven 3.5.4 http code*
Out of 156 jobs:
- 4x ""Read timed out""
- 8x ""Premature end of Content-Length delimited message body""
- 7x unrelated: Pipeline download failure, exit 137 by docker, exit 143

The 137 errors by docker were caused by the Linux OOM killer. Running 10 builders per machine is a bit too many :) Reduced to 8 per machine.;;;","07/May/20 14:34;rmetzger;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=739&view=logs&j=16ccbdb7-2a3e-53da-36eb-fb718edc424a&t=cf61ce33-6fba-5fbe-2c0c-e41c4013e891;;;","09/May/20 06:53;rmetzger;""read timeout"" with vanilla mvn 325: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=844&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=17a7e096-e650-5b91-858e-3d426f9eeb2f;;;","10/May/20 07:22;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=903&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa;;;","13/May/20 07:29;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1106&view=logs&j=1454c523-5777-5d91-a870-f026a11d0383&t=160c3171-f94a-5870-7346-5c8980c235f3;;;","14/May/20 14:38;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1216&view=logs&j=f2bf5708-0b51-5b18-cc70-e832adfd38b6&t=a5e6a0c3-8d06-5970-7a20-2edf0e8ef868;;;","18/May/20 06:32;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1634&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa;;;","25/May/20 05:45;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2061&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=6f8201e9-1579-595a-9d2b-7158b26b4c57;;;","10/Jun/20 02:32;wanglijie;Another instance: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3088&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf];;;","06/Jul/20 03:33;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4243&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=b4cd3436-dbe8-556d-3bca-42f92c3cbf2f];;;","14/Jul/20 03:15;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4464&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=e5682198-9e22-5770-69f6-7551182edea8;;;","16/Jul/20 06:24;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4543&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53;;;","22/Jul/20 02:08;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4698&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=6f8201e9-1579-595a-9d2b-7158b26b4c57;;;","25/Jul/20 04:18;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4900&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab];;;","28/Jul/20 01:39;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4938&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=8560c56f-9ec1-5c40-4ff5-9d3eaaaa882d];;;","29/Jul/20 01:51;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4975&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab;;;","02/Aug/20 11:05;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5094&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=39a61cac-5c62-532f-d2c1-dea450a66708;;;","09/Aug/20 03:16;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5300&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=662e1729-9aac-55e2-8268-b965b3860e1f];;;","13/Aug/20 01:19;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5471&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=6f8201e9-1579-595a-9d2b-7158b26b4c57];;;","14/Aug/20 02:06;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5515&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa];;;","15/Aug/20 02:02;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5542&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=f2460a7f-3a8f-5c32-3bac-d566722e4e62];;;","22/Aug/20 02:47;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5781&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=61b612c5-1c7d-5289-27c2-71f332ae98d7;;;","23/Aug/20 01:30;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5789&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=8560c56f-9ec1-5c40-4ff5-9d3eaaaa882d];;;","06/Sep/20 13:17;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6241&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=6f8201e9-1579-595a-9d2b-7158b26b4c57;;;","17/Sep/20 02:32;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6568&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=02d88c1a-f1b3-5a8c-4b4a-cf43c70f99e1;;;","06/Nov/20 09:08;godfreyhe;https://dev.azure.com/godfreyhe/c147b7ad-1708-46c3-9021-cc523e50c4d5/_apis/build/builds/64/logs/90;;;","11/Nov/20 07:21;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9437&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","16/Nov/20 19:14;rmetzger;I removed the fix for 1.12 marker. This is an infrastructure issue that we can not solve since we are using such an old Maven version / or unreliable network infrastructure.;;;","01/Dec/20 13:57;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=121&view=logs&j=9401bf33-03c4-5a24-83fe-e51d75db73ef&t=72901ab2-7cd0-57be-82b1-bca51de20fba;;;","02/Dec/20 16:12;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=124&view=logs&j=9401bf33-03c4-5a24-83fe-e51d75db73ef&t=72901ab2-7cd0-57be-82b1-bca51de20fba;;;","03/Dec/20 08:03;mapohl;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10450&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=b4cd3436-dbe8-556d-3bca-42f92c3cbf2f];;;","05/Dec/20 03:21;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10537&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","05/Dec/20 03:22;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10536&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=662e1729-9aac-55e2-8268-b965b3860e1f;;;","13/Dec/20 02:39;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10825&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=f5211ead-5e53-5af8-f827-4dbf08df26bb;;;","14/Dec/20 01:37;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10832&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa;;;","17/Dec/20 01:47;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10945&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=662e1729-9aac-55e2-8268-b965b3860e1f;;;","18/Dec/20 02:19;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10989&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=02d88c1a-f1b3-5a8c-4b4a-cf43c70f99e1;;;","18/Dec/20 03:21;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10990&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab;;;","07/Jan/21 16:38;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=165&view=logs&j=9401bf33-03c4-5a24-83fe-e51d75db73ef&t=72901ab2-7cd0-57be-82b1-bca51de20fba;;;","08/Jan/21 07:24;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=170&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=ebffacc3-9693-5a7f-11b7-68b343827cf3&l=31055;;;","21/Jan/21 07:54;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12303&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","25/Jan/21 02:46;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12414&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7;;;","05/Feb/21 07:49;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12961&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=b4cd3436-dbe8-556d-3bca-42f92c3cbf2f;;;","05/Feb/21 07:55;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12959&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53;;;","08/Feb/21 00:40;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13030&view=logs&j=af96ba69-1e60-500c-b8d1-43a47801b668&t=5af1cce8-b79c-5474-167c-f4e5395858de;;;","08/Feb/21 04:00;maguowei;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13066&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53]

 ;;;","10/Feb/21 14:20;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13179&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=b4cd3436-dbe8-556d-3bca-42f92c3cbf2f;;;","11/Feb/21 07:05;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13220&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=ea63c80c-957f-50d1-8f67-3671c14686b9

It happens quite a bit recently. Do we have any idea how we can improve the situation here?;;;","11/Feb/21 07:17;rmetzger;My best bet at this point would be using the latest maven version ... but that's not really possible due to shading (unless we use the ""invoke maven n times approach"" ... but even then, I'm concerned that the dependency resolution might differ a bit across versions)

I've invested quite some time into this issue in April 2020: 
- I manually bumped the http connector version of Maven 3.2.5 (without success).
- I've set up a maven mirror in Alibaba Cloud (where the builders are running) .. I can also see the errors on the server side. I even opened a ticket at sonatype (which provides the maven mirror) ... without success.
;;;","12/Feb/21 07:14;dwysakowicz;Thanks for the update [~rmetzger] and the efforts!;;;","12/Feb/21 07:14;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13264&view=logs&j=946871de-358d-5815-3994-8175615bc253&t=e0240c62-4570-5d1c-51af-dd63d2093da1;;;","12/Feb/21 07:15;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13266&view=logs&j=3e60b793-4158-5027-ac6d-4cdc51dffe1e&t=d5ed4970-7667-5f7e-2ece-62e410f74748;;;","16/Feb/21 04:38;lhotari;I came across a similar issue in Apache Pulsar's build. The bug causing the failures where the error message is ""Could not transfer artifact groupId:artifactId:1.2.3 from/to central ([https://repo1.maven.org/maven2]): Entry [id:1280][route:\\{s}->https://repo1.maven.org:443][state:null] has not been leased from this pool"" is HTTPCORE-634 . It has been fixed in HttpCore 4.4.14 . This fix isn't in a released version of Maven or the Maven Wagon Http library. I have created a PR [https://github.com/apache/maven-wagon/pull/76] to get the changes first in Maven Wagon.

The immediate workaround is to replace the existing MAVEN_HOME/lib/wagon-http-*-shaded.jar file with [https://jitpack.io/com/github/lhotari/maven-wagon/wagon-http/5ff79d284/wagon-http-5ff79d284-shaded.jar] (compiled version of maven-wagon PR 76). This seems to work at least in Maven 3.6.3 .

Simple bash script to replace the Maven built-in wagon-http version with the patched one: 
{code:java}
#!/bin/bash
MAVEN_HOME=$(mvn -v |grep 'Maven home:' | awk '{ print $3 }')
if [ -d ""$MAVEN_HOME"" ]; then
  cd ""$MAVEN_HOME/lib""
  rm wagon-http-*-shaded.jar
  curl -O https://jitpack.io/com/github/lhotari/maven-wagon/wagon-http/5ff79d284/wagon-http-5ff79d284-shaded.jar
fi {code}
I also tried to use the build / extensions / extension element to override the built-in wagon-http version, but in my quick experiment that didn't seem to work.

I hope this helps!
 ;;;","17/Feb/21 13:01;lhotari;I have reported WAGON-607 to upgrade HttpCore to 4.4.14 .;;;","17/Feb/21 13:57;rmetzger;Awesome, thanks a lot for your comment! I'll try to get that updated jar into our build docker image!;;;","18/Feb/21 19:39;rmetzger;Maybe a fix, merged to master only for now: https://github.com/apache/flink/commit/3279c85233a7d696b344d64d804fe473599bdd01;;;","11/Mar/21 15:06;lhotari;[~rmetzger] Has the change fixed this issue? btw. I created [https://github.com/rmetzger/flink-ci/pull/3] since there's now an official release of Maven Wagon 3.4.3 which includes the fix.;;;","29/Mar/21 03:15;maguowei;just for reporting on brach 1.12
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15603&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=e5682198-9e22-5770-69f6-7551182edea8&l=6847;;;","30/Mar/21 09:05;rmetzger;Thanks [~lhotari] for the PR! ;;;","07/Apr/21 10:20;rmetzger;Fixed in release-1.12 https://github.com/apache/flink/commit/46b4644a2a5d473b01dfb4e2a6e4b1ec3cc483d1;;;","07/Apr/21 10:21;rmetzger;Updated to the latest docker image in master in https://github.com/apache/flink/commit/99b1c4294f9b32e60dff109d1bd16681d4a47c0b (now using the official Maven Wagon release! Thanks again [~lhotari]!);;;"
Compile error in. DumpCompiledPlanTest and PreviewPlanDumpTest ,FLINK-16944,13295742,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,pnowojski,pnowojski,02/Apr/20 09:20,10/May/20 11:55,13/Jul/23 08:07,02/Apr/20 10:14,1.11.0,,,,,1.10.1,1.11.0,,,API / DataStream,Runtime / Configuration,,,,0,test-stability,,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6967&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=4716f636-db2d-5472-7d55-b6120a857b87

{noformat}
[ERROR] /__w/3/s/flink-tests/src/test/java/org/apache/flink/test/optimizer/jsonplan/PreviewPlanDumpTest.java:[110,63] method getPipelineFromProgram in enum org.apache.flink.client.program.PackagedProgramUtils cannot be applied to given types;
  required: org.apache.flink.client.program.PackagedProgram,org.apache.flink.configuration.Configuration,int,boolean
  found: org.apache.flink.client.program.PackagedProgram,int,boolean
  reason: actual and formal argument lists differ in length
[ERROR] /__w/3/s/flink-tests/src/test/java/org/apache/flink/test/optimizer/jsonplan/DumpCompiledPlanTest.java:[107,63] method getPipelineFromProgram in enum org.apache.flink.client.program.PackagedProgramUtils cannot be applied to given types;
  required: org.apache.flink.client.program.PackagedProgram,org.apache.flink.configuration.Configuration,int,boolean
  found: org.apache.flink.client.program.PackagedProgram,int,boolean
  reason: actual and formal argument lists differ in length
{noformat}
",,dian.fu,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16560,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 10 11:55:26 UTC 2020,,,,,,,,,,"0|z0d7ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/20 10:14;chesnay;master: 89522f9fbf5974ba1acee0ec3cb7ee626d7a91c0;;;","10/May/20 11:55;dian.fu;Add fix version of ""1.10.1"" as I noticed that it has also been backport to 1.10.1 via 1e217d4027db49220d2c0b515cbc5c95a1ff39f3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid creating currentRegion HashSet with manually set initialCapacity,FLINK-16940,13295730,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gjy,gjy,gjy,02/Apr/20 08:32,02/Apr/20 15:05,13/Jul/23 08:07,02/Apr/20 15:05,1.11.0,,,,,1.11.0,,,,Runtime / REST,,,,,0,pull-request-available,,,,"The {{currentRegion}} HashSet in {{PipelinedRegionComputeUtil}} is created with an initialCapacity of 1. This is wrong because when we add the first element, the sets capacity will be already increased. From the style guidelines:
{quote}
Set the initial capacity for a collection only if there is a good proven reason for that, otherwise do not clutter the code. In case of Maps it can be even deluding because the Map’s load factor effectively reduces the capacity.
{quote}
https://flink.apache.org/contributing/code-style-and-quality-java.html",,gjy,,,,,,,,,,,,,,,,,,,,,"GJL commented on pull request #11613: [FLINK-16940][runtime] Create currentRegion HashSet with default capacity
URL: https://github.com/apache/flink/pull/11613
 
 
   ## What is the purpose of the change
   
   *Create currentRegion HashSet with default capacity*
   
   
   ## Brief change log
   
     - *See commit*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 08:35;githubbot;600","GJL commented on pull request #11613: [FLINK-16940][runtime] Create currentRegion HashSet with default capacity
URL: https://github.com/apache/flink/pull/11613
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 15:05;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 15:05:47 UTC 2020,,,,,,,,,,"0|z0d780:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/20 15:05;gjy;master: f1c91cca3f66bd5f91446a58ce1b2d4c6015af4a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManagerMessageParameters#taskManagerIdParameter is not declared final,FLINK-16939,13295729,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gjy,gjy,gjy,02/Apr/20 08:18,02/Apr/20 15:03,13/Jul/23 08:07,02/Apr/20 15:03,1.11.0,,,,,1.11.0,,,,Runtime / REST,,,,,0,pull-request-available,,,,The field {{TaskManagerMessageParameters#taskManagerIdParameter}} is not declared final but it should be.,,gjy,,,,,,,,,,,,,,,,,,,,,"GJL commented on pull request #11612: [FLINK-16939][rest] Declare taskManagerIdParameter final
URL: https://github.com/apache/flink/pull/11612
 
 
   ## What is the purpose of the change
   
   *Declare `taskManagerIdParameter` final*
   
   
   ## Brief change log
   
     - *See commit*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 08:24;githubbot;600","GJL commented on pull request #11612: [FLINK-16939][rest] Declare taskManagerIdParameter final
URL: https://github.com/apache/flink/pull/11612
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 15:02;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 15:03:31 UTC 2020,,,,,,,,,,"0|z0d77s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/20 15:03;gjy;master: 3fa4a7e5ae9befc5a01df4202942090b63b60a69;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unnecessary entries in the distribution jar NOTICE,FLINK-16932,13295673,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dian.fu,dian.fu,dian.fu,02/Apr/20 02:01,02/Apr/20 17:07,13/Jul/23 08:07,02/Apr/20 17:07,,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"The following packages which are declared in the NOTICE to be bundled in the jar of statefun-flink-distribution are not actually bundled in it:
 - org.apache.commons:commons-math3:3.5
 - commons-cli:commons-cli:jar:1.3.1
 - commons-collections:commons-collections:3.2.2
 - org.apache.commons:commons-compress:1.18
 - com.google.code.findbugs:jsr305:1.3.9
 - org.objenesis:objenesis:2.1
 - com.esotericsoftware.kryo:kryo:2.24.0
 - com.esotericsoftware.minlog:minlog:1.2

This was introduced after https://issues.apache.org/jira/browse/FLINK-16892. 

We should remove these entries in the NOTICE.",,dian.fu,sewen,,,,,,,,,,,,,,,,,,,,"dianfu commented on pull request #92: [FLINK-16932][legal] Remove unnecessary entries in the distribution jar NOTICE
URL: https://github.com/apache/flink-statefun/pull/92
 
 
   The following packages which are declared in the NOTICE to be bundled in the jar of statefun-flink-distribution are not actually bundled in it:
   
   org.apache.commons:commons-math3:3.5
   commons-cli:commons-cli:jar:1.3.1
   commons-collections:commons-collections:3.2.2
   org.apache.commons:commons-compress:1.18
   com.google.code.findbugs:jsr305:1.3.9
   org.objenesis:objenesis:2.1
   com.esotericsoftware.kryo:kryo:2.24.0
   com.esotericsoftware.minlog:minlog:1.2
   This was introduced after https://issues.apache.org/jira/browse/FLINK-16892. 
   
   We should remove these entries in the NOTICE.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 02:38;githubbot;600","asfgit commented on pull request #92: [FLINK-16932][legal] Remove unnecessary entries in the distribution jar NOTICE
URL: https://github.com/apache/flink-statefun/pull/92
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 17:02;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 17:07:16 UTC 2020,,,,,,,,,,"0|z0d6vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/20 17:07;sewen;Fixed in
  - 2.0 via 02dba2ffc1cc94cc3ef7582b4e6ce697d7bcc027
  - master via fcb1604e8d1c83ba802c3a6b59745e96ca7ca91b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stateful Function jobs fail to restore after losing task slots using the new Flink scheduler,FLINK-16927,13295571,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,igal,tzulitai,tzulitai,01/Apr/20 16:03,02/Apr/20 17:06,13/Jul/23 08:07,02/Apr/20 17:06,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"The following exception is being thrown indefinitely once a worker is lost:
{code}
org.apache.flink.util.FlinkException: Multi task slot is not local and, thus, does not fulfill the co-location constraint.
{code}

This is caused by FLINK-16139, which is fixed in Flink 1.10.1.

To unblock the release for Stateful Functions 2.0.0, which depends on Flink 1.10.0,
for StateFun 2.0.0 we can use the old legacy scheduler for the time being.",,sewen,tzulitai,,,,,,,,,,,,,,,,,,,,"igalshilman commented on pull request #91: [FLINK-16927][core] Fallback to the legacy scheduler.
URL: https://github.com/apache/flink-statefun/pull/91
 
 
   This PR switches to the legacy scheduler until `FLINK-16139` would be resolved.
   See the relevant JIRA issues for details.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 22:09;githubbot;600","asfgit commented on pull request #91: [FLINK-16927][core] Fallback to the legacy scheduler.
URL: https://github.com/apache/flink-statefun/pull/91
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 17:02;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16928,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 17:06:37 UTC 2020,,,,,,,,,,"0|z0d68o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/20 17:06;sewen;Fixed in
  - 2.0 via 
      - 7779d8690b073acd93172f2bf5d98572f9b2b17c 
      - 2529d272d29db12fe9835e3c8260a661ae011e28

  - master via
      - 8e462ee8a26d5f1beb5b4ef0d0bf70715551d172
      - 99a654242b2a249a065ac71b985f5339f963fd1e
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Values present at flink-conf.yaml are not respected via the StreamPlanEnvironment ,FLINK-16926,13295552,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igal,igal,igal,01/Apr/20 14:54,02/Apr/20 17:05,13/Jul/23 08:07,02/Apr/20 17:05,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"Values present at flink-conf.yaml are not respected via the StreamPlanEnvironment,

this happens when using a JobClusterEntrypoint, and seems to be caused by https://issues.apache.org/jira/browse/FLINK-16560

 ",,igal,sewen,,,,,,,,,,,,,,,,,,,,"igalshilman commented on pull request #90:  [FLINK-16926][core] Reconfigure StreamExecutionEnvironment
URL: https://github.com/apache/flink-statefun/pull/90
 
 
   ### This PR adds a workaround for FLINK-16560.
   
   When starting a statefun Job via `StatefulFunctionsClusterEntryPoint` (an adopted version of a
   `JobClusterEntryPoint`) the provided ExecutionEnvironment is initialized with an empty configuration. As a result the ExecutionEnvironment might be missing important configuration options set at `flink-conf.yaml`.
   
   The workaround is to use the configuration obtained from `GlobalConfiguration.loadConfiguration()` to re-configure the execution environment provided.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 20:44;githubbot;600","asfgit commented on pull request #90:  [FLINK-16926][core] Reconfigure StreamExecutionEnvironment
URL: https://github.com/apache/flink-statefun/pull/90
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 17:02;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 17:05:05 UTC 2020,,,,,,,,,,"0|z0d64g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/20 17:05;sewen;Fixed in
  - 2.0 via e3ee2b6c0798478545a8c87c3b918ff59a56799d
  - master via 809db8b804025acdc3150633d8b96015b4350ec0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DecimalData.toUnscaledBytes should be consistent with BigDecimla.unscaledValue.toByteArray,FLINK-16922,13295528,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,lzljs3620320,lzljs3620320,01/Apr/20 12:40,16/Oct/20 10:52,13/Jul/23 08:07,20/May/20 14:24,,,,,,1.11.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"In Decimal:
{code:java}
public byte[] toUnscaledBytes() {
   if (!isCompact()) {
      return toBigDecimal().unscaledValue().toByteArray();
   }

   // big endian; consistent with BigInteger.toByteArray()
   byte[] bytes = new byte[8];
   long l = longVal;
   for (int i = 0; i < 8; i++) {
      bytes[7 - i] = (byte) l;
      l >>>= 8;
   }
   return bytes;
}
{code}
When is compact, it will return fix 8 length byte array.

This should not happen, it brings an incompatible byte array.",,jark,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16996,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 20 14:24:22 UTC 2020,,,,,,,,,,"0|z0d5z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/20 14:24;jark;master (1.12.0): 34671add8a435ee4431f4c1c4da37a8e078b7a8a
1.11.0: f7356560145f2bb862d1608264de3cf476f4abba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""kubernetes session test"" is unstable",FLINK-16921,13295523,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,rmetzger,rmetzger,01/Apr/20 12:06,11/May/20 19:42,13/Jul/23 08:07,11/May/20 19:42,1.11.0,,,,,1.11.0,,,,Deployment / Kubernetes,Tests,,,,0,pull-request-available,test-stability,,,"CI: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6915&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

I assume some services didn't come up?
{code}
Caused by: java.util.concurrent.CompletionException: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /10.1.0.4:30095
{code}

Full log
{code}
2020-04-01T09:13:59.0673858Z Successfully built ba628fa7af0d
2020-04-01T09:13:59.0726818Z Successfully tagged test_kubernetes_session:latest
2020-04-01T09:13:59.2547709Z clusterrolebinding.rbac.authorization.k8s.io/flink-role-binding-default created
2020-04-01T09:14:00.0586087Z 2020-04-01 09:14:00,055 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost
2020-04-01T09:14:00.0608876Z 2020-04-01 09:14:00,060 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
2020-04-01T09:14:00.0611236Z 2020-04-01 09:14:00,060 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.heap.size, 1024m
2020-04-01T09:14:00.0613869Z 2020-04-01 09:14:00,061 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
2020-04-01T09:14:00.0616344Z 2020-04-01 09:14:00,061 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
2020-04-01T09:14:00.0619384Z 2020-04-01 09:14:00,061 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 1
2020-04-01T09:14:00.0624467Z 2020-04-01 09:14:00,062 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2020-04-01T09:14:00.9838038Z 2020-04-01 09:14:00,983 INFO  org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
2020-04-01T09:14:00.9922554Z 2020-04-01 09:14:00,991 INFO  org.apache.flink.kubernetes.utils.KubernetesUtils            [] - Kubernetes deployment requires a fixed port. Configuration blob.server.port will be set to 6124
2020-04-01T09:14:00.9927409Z 2020-04-01 09:14:00,992 INFO  org.apache.flink.kubernetes.utils.KubernetesUtils            [] - Kubernetes deployment requires a fixed port. Configuration taskmanager.rpc.port will be set to 6122
2020-04-01T09:14:01.0587014Z 2020-04-01 09:14:01,058 WARN  org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator [] - Found 0 files in directory null/etc/hadoop, skip to mount the Hadoop Configuration ConfigMap.
2020-04-01T09:14:01.0592498Z 2020-04-01 09:14:01,059 WARN  org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator [] - Found 0 files in directory null/etc/hadoop, skip to create the Hadoop Configuration ConfigMap.
2020-04-01T09:14:01.8684880Z 2020-04-01 09:14:01,868 INFO  org.apache.flink.kubernetes.KubernetesClusterDescriptor      [] - Create flink session cluster flink-native-k8s-session-1 successfully, JobManager Web Interface: http://10.1.0.4:30095
2020-04-01T09:14:03.2952029Z Executing WordCount example with default input data set.
2020-04-01T09:14:03.2955365Z Use --input to specify file input.
2020-04-01T09:15:31.5606577Z 
2020-04-01T09:15:31.5610358Z ------------------------------------------------------------
2020-04-01T09:15:31.5610913Z  The program finished with the following exception:
2020-04-01T09:15:31.5611114Z 
2020-04-01T09:15:31.5611772Z org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-04-01T09:15:31.5617073Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
2020-04-01T09:15:31.5629389Z 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
2020-04-01T09:15:31.5665671Z 	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:143)
2020-04-01T09:15:31.5667385Z 	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:659)
2020-04-01T09:15:31.5668074Z 	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:210)
2020-04-01T09:15:31.5668537Z 	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:890)
2020-04-01T09:15:31.5670148Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:963)
2020-04-01T09:15:31.5670582Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-04-01T09:15:31.5671095Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-04-01T09:15:31.5671703Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
2020-04-01T09:15:31.5672240Z 	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
2020-04-01T09:15:31.5672717Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:963)
2020-04-01T09:15:31.5673439Z Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-04-01T09:15:31.5674090Z 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:276)
2020-04-01T09:15:31.5674589Z 	at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:969)
2020-04-01T09:15:31.5675154Z 	at org.apache.flink.client.program.ContextEnvironment.executeAsync(ContextEnvironment.java:85)
2020-04-01T09:15:31.5675704Z 	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:54)
2020-04-01T09:15:31.5676202Z 	at org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:93)
2020-04-01T09:15:31.5676842Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-04-01T09:15:31.5677375Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-04-01T09:15:31.5677862Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-04-01T09:15:31.5678271Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-04-01T09:15:31.5678675Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
2020-04-01T09:15:31.5679004Z 	... 11 more
2020-04-01T09:15:31.5679444Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-04-01T09:15:31.5679993Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-04-01T09:15:31.5680572Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-04-01T09:15:31.5681024Z 	at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:964)
2020-04-01T09:15:31.5681359Z 	... 19 more
2020-04-01T09:15:31.5681666Z Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-04-01T09:15:31.5682177Z 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:359)
2020-04-01T09:15:31.5682684Z 	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
2020-04-01T09:15:31.5683158Z 	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
2020-04-01T09:15:31.5683794Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-04-01T09:15:31.5684288Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-04-01T09:15:31.5684849Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:287)
2020-04-01T09:15:31.5685376Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-04-01T09:15:31.5685893Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-04-01T09:15:31.5686503Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-04-01T09:15:31.5687195Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-04-01T09:15:31.5688323Z 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$1(RestClient.java:342)
2020-04-01T09:15:31.5688868Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:500)
2020-04-01T09:15:31.5689573Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:493)
2020-04-01T09:15:31.5690170Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:472)
2020-04-01T09:15:31.5690741Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:413)
2020-04-01T09:15:31.5691310Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:538)
2020-04-01T09:15:31.5691854Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:531)
2020-04-01T09:15:31.5692416Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:111)
2020-04-01T09:15:31.5693043Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:323)
2020-04-01T09:15:31.5693711Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
2020-04-01T09:15:31.5694327Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685)
2020-04-01T09:15:31.5695684Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632)
2020-04-01T09:15:31.5696348Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549)
2020-04-01T09:15:31.5696964Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511)
2020-04-01T09:15:31.5697862Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
2020-04-01T09:15:31.5698916Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2020-04-01T09:15:31.5700726Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:15:31.5701197Z Caused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete the operation. Number of retries has been exhausted.
2020-04-01T09:15:31.5701923Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:284)
2020-04-01T09:15:31.5702265Z 	... 21 more
2020-04-01T09:15:31.5702770Z Caused by: java.util.concurrent.CompletionException: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /10.1.0.4:30095
2020-04-01T09:15:31.5703655Z 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2020-04-01T09:15:31.5704266Z 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2020-04-01T09:15:31.5704736Z 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:957)
2020-04-01T09:15:31.5705194Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
2020-04-01T09:15:31.5705520Z 	... 19 more
2020-04-01T09:15:31.5705931Z Caused by: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /10.1.0.4:30095
2020-04-01T09:15:31.5706441Z Caused by: java.net.ConnectException: Connection refused
2020-04-01T09:15:31.5706773Z 	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2020-04-01T09:15:31.5707130Z 	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
2020-04-01T09:15:31.5707650Z 	at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2020-04-01T09:15:31.5708275Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:336)
2020-04-01T09:15:31.5708890Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685)
2020-04-01T09:15:31.5709552Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632)
2020-04-01T09:15:31.5710126Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549)
2020-04-01T09:15:31.5710661Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511)
2020-04-01T09:15:31.5711214Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
2020-04-01T09:15:31.5711842Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2020-04-01T09:15:31.5712263Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:15:31.6868426Z error: filespec must match the canonical format: [[namespace/]pod:]file/path
2020-04-01T09:15:31.6921059Z sort: cannot read: '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-37181306061/out/wc_out*': No such file or directory
2020-04-01T09:15:31.6929489Z FAIL WordCount: Output hash mismatch.  Got d41d8cd98f00b204e9800998ecf8427e, expected e682ec6622b5e83f2eb614617d5ab2cf.
2020-04-01T09:15:31.6930063Z head hexdump of actual:
2020-04-01T09:15:31.6944131Z head: cannot open '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-37181306061/out/wc_out*' for reading: No such file or directory
2020-04-01T09:15:31.7805615Z deployment.apps ""flink-native-k8s-session-1"" deleted
2020-04-01T09:15:31.8726196Z clusterrolebinding.rbac.authorization.k8s.io ""flink-role-binding-default"" deleted
2020-04-01T09:15:31.8752534Z Stopping minikube ...
2020-04-01T09:15:31.9416557Z * Stopping ""minikube"" in none ...
2020-04-01T09:15:52.2656905Z * Node """" stopped.
2020-04-01T09:15:52.2698505Z [FAIL] Test script contains errors.
2020-04-01T09:15:52.2708182Z Checking for errors...
2020-04-01T09:15:52.2952618Z Found error in log files:
2020-04-01T09:15:52.2985933Z 2020-04-01 09:14:02,666 INFO  org.apache.flink.client.cli.CliFrontend                      [] - --------------------------------------------------------------------------------
2020-04-01T09:15:52.2987997Z 2020-04-01 09:14:02,681 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Starting Command Line Client (Version: 1.11-SNAPSHOT, Rev:f97b1b5, Date:01.04.2020 @ 08:00:52 UTC)
2020-04-01T09:15:52.2989547Z 2020-04-01 09:14:02,681 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  OS current user: vsts
2020-04-01T09:15:52.2990871Z 2020-04-01 09:14:03,057 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Current Hadoop/Kerberos user: vsts
2020-04-01T09:15:52.2992274Z 2020-04-01 09:14:03,057 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  JVM: OpenJDK 64-Bit Server VM - Azul Systems, Inc. - 1.8/25.242-b20
2020-04-01T09:15:52.2993652Z 2020-04-01 09:14:03,057 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Maximum heap size: 1545 MiBytes
2020-04-01T09:15:52.2995282Z 2020-04-01 09:14:03,057 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  JAVA_HOME: /usr/lib/jvm/zulu-8-azure-amd64
2020-04-01T09:15:52.2996609Z 2020-04-01 09:14:03,058 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Hadoop version: 2.8.3
2020-04-01T09:15:52.2997906Z 2020-04-01 09:14:03,058 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  JVM Options:
2020-04-01T09:15:52.2999338Z 2020-04-01 09:14:03,058 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dlog.file=/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/flink-vsts-client-fv-az510.log
2020-04-01T09:15:52.3001044Z 2020-04-01 09:14:03,058 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dlog4j.configuration=file:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/conf/log4j-cli.properties
2020-04-01T09:15:52.3003113Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dlog4j.configurationFile=file:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/conf/log4j-cli.properties
2020-04-01T09:15:52.3004826Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dlogback.configurationFile=file:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/conf/logback.xml
2020-04-01T09:15:52.3008292Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Program Arguments:
2020-04-01T09:15:52.3009068Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     run
2020-04-01T09:15:52.3009777Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -e
2020-04-01T09:15:52.3010534Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     kubernetes-session
2020-04-01T09:15:52.3011388Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dkubernetes.cluster-id=flink-native-k8s-session-1
2020-04-01T09:15:52.3012467Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/examples/batch/WordCount.jar
2020-04-01T09:15:52.3013382Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     --output
2020-04-01T09:15:52.3014112Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     /tmp/wc_out
2020-04-01T09:15:52.3017725Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Classpath: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-shaded-zookeeper-3.4.10.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-table_2.11-1.11-SNAPSHOT.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-table-blink_2.11-1.11-SNAPSHOT.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/log4j-1.2-api-2.12.1.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/log4j-api-2.12.1.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/log4j-core-2.12.1.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/log4j-slf4j-impl-2.12.1.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-dist_2.11-1.11-SNAPSHOT.jar:::
2020-04-01T09:15:52.3020796Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] - --------------------------------------------------------------------------------
2020-04-01T09:15:52.3021831Z 2020-04-01 09:14:03,063 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost
2020-04-01T09:15:52.3023145Z 2020-04-01 09:14:03,063 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
2020-04-01T09:15:52.3024241Z 2020-04-01 09:14:03,063 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.heap.size, 1024m
2020-04-01T09:15:52.3025360Z 2020-04-01 09:14:03,063 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
2020-04-01T09:15:52.3026692Z 2020-04-01 09:14:03,063 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
2020-04-01T09:15:52.3027826Z 2020-04-01 09:14:03,063 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 1
2020-04-01T09:15:52.3028887Z 2020-04-01 09:14:03,064 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2020-04-01T09:15:52.3030033Z 2020-04-01 09:14:03,177 WARN  org.apache.flink.runtime.util.HadoopUtils                    [] - Could not find Hadoop configuration via any of the supported methods (Flink configuration, environment variables).
2020-04-01T09:15:52.3031153Z 2020-04-01 09:14:03,212 INFO  org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop user set to vsts (auth:SIMPLE), credentials check status: true
2020-04-01T09:15:52.3032320Z 2020-04-01 09:14:03,220 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-7188653955626299344.conf.
2020-04-01T09:15:52.3033214Z 2020-04-01 09:14:03,234 INFO  org.apache.flink.client.cli.CliFrontend                      [] - Running 'run' command.
2020-04-01T09:15:52.3034045Z 2020-04-01 09:14:03,237 INFO  org.apache.flink.client.cli.CliFrontend                      [] - Building program from JAR file
2020-04-01T09:15:52.3034933Z 2020-04-01 09:14:03,261 INFO  org.apache.flink.client.ClientUtils                          [] - Starting program (detached: false)
2020-04-01T09:15:52.3035967Z 2020-04-01 09:14:03,347 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost
2020-04-01T09:15:52.3037022Z 2020-04-01 09:14:03,347 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
2020-04-01T09:15:52.3038072Z 2020-04-01 09:14:03,347 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.heap.size, 1024m
2020-04-01T09:15:52.3039296Z 2020-04-01 09:14:03,347 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
2020-04-01T09:15:52.3041949Z 2020-04-01 09:14:03,347 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
2020-04-01T09:15:52.3043066Z 2020-04-01 09:14:03,347 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 1
2020-04-01T09:15:52.3044219Z 2020-04-01 09:14:03,348 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2020-04-01T09:15:52.3045394Z 2020-04-01 09:14:03,368 INFO  org.apache.flink.api.java.ExecutionEnvironment               [] - The job has 0 registered types and 0 default Kryo serializers
2020-04-01T09:15:52.3046417Z 2020-04-01 09:14:03,461 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost
2020-04-01T09:15:52.3049389Z 2020-04-01 09:14:03,461 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
2020-04-01T09:15:52.3050540Z 2020-04-01 09:14:03,461 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.heap.size, 1024m
2020-04-01T09:15:52.3051673Z 2020-04-01 09:14:03,461 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
2020-04-01T09:15:52.3052795Z 2020-04-01 09:14:03,461 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
2020-04-01T09:15:52.3053986Z 2020-04-01 09:14:03,461 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 1
2020-04-01T09:15:52.3055413Z 2020-04-01 09:14:03,462 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2020-04-01T09:15:52.3056750Z 2020-04-01 09:14:04,660 INFO  org.apache.flink.kubernetes.KubernetesClusterDescriptor      [] - Retrieve flink cluster flink-native-k8s-session-1 successfully, JobManager Web Interface: http://10.1.0.4:30095
2020-04-01T09:15:52.3057873Z 2020-04-01 09:15:31,546 WARN  org.apache.flink.runtime.rest.RestClient                     [] - Rest endpoint shutdown failed.
2020-04-01T09:15:52.3058365Z java.util.concurrent.TimeoutException: null
2020-04-01T09:15:52.3058810Z 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784) ~[?:1.8.0_242]
2020-04-01T09:15:52.3059467Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928) ~[?:1.8.0_242]
2020-04-01T09:15:52.3060544Z 	at org.apache.flink.runtime.rest.RestClient.shutdown(RestClient.java:164) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3061513Z 	at org.apache.flink.client.program.rest.RestClusterClient.close(RestClusterClient.java:230) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3062683Z 	at org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.lambda$execute$1(AbstractSessionClusterExecutor.java:70) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3063436Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]
2020-04-01T09:15:52.3064136Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]
2020-04-01T09:15:52.3064698Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]
2020-04-01T09:15:52.3065240Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]
2020-04-01T09:15:52.3066208Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:287) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3066967Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]
2020-04-01T09:15:52.3067535Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]
2020-04-01T09:15:52.3068095Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]
2020-04-01T09:15:52.3068638Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]
2020-04-01T09:15:52.3069566Z 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$1(RestClient.java:342) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3070612Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:500) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3071704Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:493) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3072819Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:472) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3074044Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:413) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3075163Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:538) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3076270Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:531) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3077570Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:111) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3078751Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:323) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3079959Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3081073Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3082179Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3083283Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3084307Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3085519Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3086646Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3187864Z 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
2020-04-01T09:15:52.3189080Z 2020-04-01 09:15:31,555 ERROR org.apache.flink.client.cli.CliFrontend                      [] - Error while running the command.
2020-04-01T09:15:52.3189903Z org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-04-01T09:15:52.3191267Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3192332Z 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3193324Z 	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:143) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3194243Z 	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:659) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3195410Z 	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:210) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3198847Z 	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:890) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3200010Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:963) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3200697Z 	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_242]
2020-04-01T09:15:52.3201100Z 	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_242]
2020-04-01T09:15:52.3201940Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) [flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
2020-04-01T09:15:52.3202983Z 	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3203930Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:963) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3204809Z Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-04-01T09:15:52.3205945Z 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3206942Z 	at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:969) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3208379Z 	at org.apache.flink.client.program.ContextEnvironment.executeAsync(ContextEnvironment.java:85) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3209348Z 	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:54) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3209966Z 	at org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:93) ~[?:?]
2020-04-01T09:15:52.3210430Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_242]
2020-04-01T09:15:52.3210892Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_242]
2020-04-01T09:15:52.3211450Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]
2020-04-01T09:15:52.3211928Z 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]
2020-04-01T09:15:52.3212768Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3213234Z 	... 11 more
2020-04-01T09:15:52.3213655Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-04-01T09:15:52.3214257Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_242]
2020-04-01T09:15:52.3214762Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_242]
2020-04-01T09:15:52.3215792Z 	at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:964) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3216980Z 	at org.apache.flink.client.program.ContextEnvironment.executeAsync(ContextEnvironment.java:85) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3218146Z 	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:54) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3218898Z 	at org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:93) ~[?:?]
2020-04-01T09:15:52.3219352Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_242]
2020-04-01T09:15:52.3219842Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_242]
2020-04-01T09:15:52.3220414Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]
2020-04-01T09:15:52.3221028Z 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]
2020-04-01T09:15:52.3221872Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3222323Z 	... 11 more
2020-04-01T09:15:52.3222672Z Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-04-01T09:15:52.3223573Z 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:359) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3224254Z 	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) ~[?:1.8.0_242]
2020-04-01T09:15:52.3224957Z 	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866) ~[?:1.8.0_242]
2020-04-01T09:15:52.3225655Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]
2020-04-01T09:15:52.3226387Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]
2020-04-01T09:15:52.3227451Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:287) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3228386Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]
2020-04-01T09:15:52.3228959Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]
2020-04-01T09:15:52.3229505Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]
2020-04-01T09:15:52.3230062Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]
2020-04-01T09:15:52.3230970Z 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$1(RestClient.java:342) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3232014Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:500) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3233131Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:493) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3234243Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:472) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3235329Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:413) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3236416Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:538) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3237484Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:531) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3238684Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:111) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3239858Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:323) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3241058Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3242193Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3243300Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3244396Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3245562Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3246695Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3248100Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3248952Z 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242]
2020-04-01T09:15:52.3249625Z Caused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete the operation. Number of retries has been exhausted.
2020-04-01T09:15:52.3250701Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:284) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3251409Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]
2020-04-01T09:15:52.3251984Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]
2020-04-01T09:15:52.3252687Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]
2020-04-01T09:15:52.3253248Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]
2020-04-01T09:15:52.3254138Z 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$1(RestClient.java:342) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3255329Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:500) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3256458Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:493) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3257610Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:472) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3258847Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:413) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3265212Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:538) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3266873Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:531) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3268469Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:111) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3269732Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:323) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3271223Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3272704Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3274160Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3275736Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3277139Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3278277Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3279634Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3280542Z 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242]
2020-04-01T09:15:52.3281187Z Caused by: java.util.concurrent.CompletionException: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /10.1.0.4:30095
2020-04-01T09:15:52.3281922Z 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_242]
2020-04-01T09:15:52.3282620Z 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_242]
2020-04-01T09:15:52.3283173Z 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:957) ~[?:1.8.0_242]
2020-04-01T09:15:52.3283721Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940) ~[?:1.8.0_242]
2020-04-01T09:15:52.3284395Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]
2020-04-01T09:15:52.3284966Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]
2020-04-01T09:15:52.3285962Z 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$1(RestClient.java:342) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3288624Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:500) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3289888Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:493) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3292603Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:472) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3294053Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:413) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3295689Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:538) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3297327Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:531) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3298930Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:111) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3300350Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:323) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3301801Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3303015Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3304138Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3305238Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3306280Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3307360Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3311829Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3312625Z 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242]
2020-04-01T09:15:52.3313166Z Caused by: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /10.1.0.4:30095
2020-04-01T09:15:52.3313713Z Caused by: java.net.ConnectException: Connection refused
2020-04-01T09:15:52.3314200Z 	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_242]
2020-04-01T09:15:52.3314670Z 	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_242]
2020-04-01T09:15:52.3315824Z 	at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3317043Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:336) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3318317Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3319437Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3320530Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3321573Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3322667Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3323770Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3324462Z 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242]
2020-04-01T09:15:52.3324746Z Checking for exceptions...
2020-04-01T09:15:52.3325554Z Found exception in log files:
2020-04-01T09:15:52.3326685Z 2020-04-01 09:14:02,666 INFO  org.apache.flink.client.cli.CliFrontend                      [] - --------------------------------------------------------------------------------
2020-04-01T09:15:52.3328690Z 2020-04-01 09:14:02,681 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Starting Command Line Client (Version: 1.11-SNAPSHOT, Rev:f97b1b5, Date:01.04.2020 @ 08:00:52 UTC)
2020-04-01T09:15:52.3330018Z 2020-04-01 09:14:02,681 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  OS current user: vsts
2020-04-01T09:15:52.3331320Z 2020-04-01 09:14:03,057 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Current Hadoop/Kerberos user: vsts
2020-04-01T09:15:52.3332433Z 2020-04-01 09:14:03,057 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  JVM: OpenJDK 64-Bit Server VM - Azul Systems, Inc. - 1.8/25.242-b20
2020-04-01T09:15:52.3333528Z 2020-04-01 09:14:03,057 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Maximum heap size: 1545 MiBytes
2020-04-01T09:15:52.3334583Z 2020-04-01 09:14:03,057 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  JAVA_HOME: /usr/lib/jvm/zulu-8-azure-amd64
2020-04-01T09:15:52.3335779Z 2020-04-01 09:14:03,058 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Hadoop version: 2.8.3
2020-04-01T09:15:52.3336775Z 2020-04-01 09:14:03,058 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  JVM Options:
2020-04-01T09:15:52.3338100Z 2020-04-01 09:14:03,058 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dlog.file=/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/flink-vsts-client-fv-az510.log
2020-04-01T09:15:52.3339830Z 2020-04-01 09:14:03,058 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dlog4j.configuration=file:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/conf/log4j-cli.properties
2020-04-01T09:15:52.3341541Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dlog4j.configurationFile=file:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/conf/log4j-cli.properties
2020-04-01T09:15:52.3343299Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dlogback.configurationFile=file:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/conf/logback.xml
2020-04-01T09:15:52.3344445Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Program Arguments:
2020-04-01T09:15:52.3345528Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     run
2020-04-01T09:15:52.3346389Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -e
2020-04-01T09:15:52.3347283Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     kubernetes-session
2020-04-01T09:15:52.3348540Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dkubernetes.cluster-id=flink-native-k8s-session-1
2020-04-01T09:15:52.3349699Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/examples/batch/WordCount.jar
2020-04-01T09:15:52.3350737Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     --output
2020-04-01T09:15:52.3351626Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     /tmp/wc_out
2020-04-01T09:15:52.3361051Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Classpath: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-shaded-zookeeper-3.4.10.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-table_2.11-1.11-SNAPSHOT.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-table-blink_2.11-1.11-SNAPSHOT.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/log4j-1.2-api-2.12.1.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/log4j-api-2.12.1.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/log4j-core-2.12.1.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/log4j-slf4j-impl-2.12.1.jar:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-dist_2.11-1.11-SNAPSHOT.jar:::
2020-04-01T09:15:52.3364232Z 2020-04-01 09:14:03,059 INFO  org.apache.flink.client.cli.CliFrontend                      [] - --------------------------------------------------------------------------------
2020-04-01T09:15:52.3365566Z 2020-04-01 09:14:03,063 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost
2020-04-01T09:15:52.3366807Z 2020-04-01 09:14:03,063 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
2020-04-01T09:15:52.3368271Z 2020-04-01 09:14:03,063 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.heap.size, 1024m
2020-04-01T09:15:52.3369634Z 2020-04-01 09:14:03,063 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
2020-04-01T09:15:52.3370825Z 2020-04-01 09:14:03,063 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
2020-04-01T09:15:52.3372118Z 2020-04-01 09:14:03,063 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 1
2020-04-01T09:15:52.3373353Z 2020-04-01 09:14:03,064 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2020-04-01T09:15:52.3374802Z 2020-04-01 09:14:03,177 WARN  org.apache.flink.runtime.util.HadoopUtils                    [] - Could not find Hadoop configuration via any of the supported methods (Flink configuration, environment variables).
2020-04-01T09:15:52.3376309Z 2020-04-01 09:14:03,212 INFO  org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop user set to vsts (auth:SIMPLE), credentials check status: true
2020-04-01T09:15:52.3377729Z 2020-04-01 09:14:03,220 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-7188653955626299344.conf.
2020-04-01T09:15:52.3378950Z 2020-04-01 09:14:03,234 INFO  org.apache.flink.client.cli.CliFrontend                      [] - Running 'run' command.
2020-04-01T09:15:52.3379870Z 2020-04-01 09:14:03,237 INFO  org.apache.flink.client.cli.CliFrontend                      [] - Building program from JAR file
2020-04-01T09:15:52.3380849Z 2020-04-01 09:14:03,261 INFO  org.apache.flink.client.ClientUtils                          [] - Starting program (detached: false)
2020-04-01T09:15:52.3381930Z 2020-04-01 09:14:03,347 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost
2020-04-01T09:15:52.3383077Z 2020-04-01 09:14:03,347 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
2020-04-01T09:15:52.3384370Z 2020-04-01 09:14:03,347 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.heap.size, 1024m
2020-04-01T09:15:52.3385900Z 2020-04-01 09:14:03,347 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
2020-04-01T09:15:52.3387186Z 2020-04-01 09:14:03,347 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
2020-04-01T09:15:52.3388807Z 2020-04-01 09:14:03,347 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 1
2020-04-01T09:15:52.3390194Z 2020-04-01 09:14:03,348 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2020-04-01T09:15:52.3391542Z 2020-04-01 09:14:03,368 INFO  org.apache.flink.api.java.ExecutionEnvironment               [] - The job has 0 registered types and 0 default Kryo serializers
2020-04-01T09:15:52.3392828Z 2020-04-01 09:14:03,461 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost
2020-04-01T09:15:52.3393971Z 2020-04-01 09:14:03,461 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
2020-04-01T09:15:52.3395271Z 2020-04-01 09:14:03,461 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.heap.size, 1024m
2020-04-01T09:15:52.3396555Z 2020-04-01 09:14:03,461 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
2020-04-01T09:15:52.3398209Z 2020-04-01 09:14:03,461 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
2020-04-01T09:15:52.3399361Z 2020-04-01 09:14:03,461 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 1
2020-04-01T09:15:52.3400532Z 2020-04-01 09:14:03,462 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2020-04-01T09:15:52.3401870Z 2020-04-01 09:14:04,660 INFO  org.apache.flink.kubernetes.KubernetesClusterDescriptor      [] - Retrieve flink cluster flink-native-k8s-session-1 successfully, JobManager Web Interface: http://10.1.0.4:30095
2020-04-01T09:15:52.3403009Z 2020-04-01 09:15:31,546 WARN  org.apache.flink.runtime.rest.RestClient                     [] - Rest endpoint shutdown failed.
2020-04-01T09:15:52.3403583Z java.util.concurrent.TimeoutException: null
2020-04-01T09:15:52.3404081Z 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784) ~[?:1.8.0_242]
2020-04-01T09:15:52.3404660Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928) ~[?:1.8.0_242]
2020-04-01T09:15:52.3405761Z 	at org.apache.flink.runtime.rest.RestClient.shutdown(RestClient.java:164) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3406940Z 	at org.apache.flink.client.program.rest.RestClusterClient.close(RestClusterClient.java:230) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3409294Z 	at org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.lambda$execute$1(AbstractSessionClusterExecutor.java:70) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3410159Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]
2020-04-01T09:15:52.3410814Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]
2020-04-01T09:15:52.3411424Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]
2020-04-01T09:15:52.3412195Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]
2020-04-01T09:15:52.3413269Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:287) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3414024Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]
2020-04-01T09:15:52.3414663Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]
2020-04-01T09:15:52.3415283Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]
2020-04-01T09:15:52.3415891Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]
2020-04-01T09:15:52.3416902Z 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$1(RestClient.java:342) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3418264Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:500) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3419529Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:493) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3420893Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:472) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3422099Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:413) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3423435Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:538) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3424622Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:531) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3425999Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:111) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3427512Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:323) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3429041Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3430416Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3431633Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3432844Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3433968Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3435325Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3438421Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3440525Z 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
2020-04-01T09:15:52.3441936Z 2020-04-01 09:15:31,555 ERROR org.apache.flink.client.cli.CliFrontend                      [] - Error while running the command.
2020-04-01T09:15:52.3442974Z org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-04-01T09:15:52.3444417Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3445820Z 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3447222Z 	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:143) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3448568Z 	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:659) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3449585Z 	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:210) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3450619Z 	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:890) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3451668Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:963) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3452297Z 	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_242]
2020-04-01T09:15:52.3452790Z 	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_242]
2020-04-01T09:15:52.3453709Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) [flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
2020-04-01T09:15:52.3455135Z 	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3456472Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:963) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3457371Z Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-04-01T09:15:52.3458731Z 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3459788Z 	at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:969) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3460900Z 	at org.apache.flink.client.program.ContextEnvironment.executeAsync(ContextEnvironment.java:85) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3462004Z 	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:54) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3462709Z 	at org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:93) ~[?:?]
2020-04-01T09:15:52.3463368Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_242]
2020-04-01T09:15:52.3463940Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_242]
2020-04-01T09:15:52.3464579Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]
2020-04-01T09:15:52.3465327Z 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]
2020-04-01T09:15:52.3466338Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3466952Z 	... 11 more
2020-04-01T09:15:52.3467434Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-04-01T09:15:52.3468465Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_242]
2020-04-01T09:15:52.3469056Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_242]
2020-04-01T09:15:52.3470049Z 	at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:964) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3471154Z 	at org.apache.flink.client.program.ContextEnvironment.executeAsync(ContextEnvironment.java:85) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3472261Z 	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:54) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3472979Z 	at org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:93) ~[?:?]
2020-04-01T09:15:52.3473500Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_242]
2020-04-01T09:15:52.3474037Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_242]
2020-04-01T09:15:52.3474646Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]
2020-04-01T09:15:52.3475335Z 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]
2020-04-01T09:15:52.3476317Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3477038Z 	... 11 more
2020-04-01T09:15:52.3477420Z Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-04-01T09:15:52.3478522Z 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:359) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3482227Z 	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) ~[?:1.8.0_242]
2020-04-01T09:15:52.3482954Z 	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866) ~[?:1.8.0_242]
2020-04-01T09:15:52.3483494Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]
2020-04-01T09:15:52.3485230Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]
2020-04-01T09:15:52.3486484Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:287) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3487452Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]
2020-04-01T09:15:52.3488172Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]
2020-04-01T09:15:52.3488823Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]
2020-04-01T09:15:52.3489353Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]
2020-04-01T09:15:52.3490255Z 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$1(RestClient.java:342) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3491238Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:500) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3492283Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:493) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3493697Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:472) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3494905Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:413) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3496161Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:538) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3497231Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:531) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3498387Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:111) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3499488Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:323) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3500681Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3501760Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3502984Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3504078Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3505121Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3506222Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3507551Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3508111Z 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242]
2020-04-01T09:15:52.3509743Z Caused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete the operation. Number of retries has been exhausted.
2020-04-01T09:15:52.3510944Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:284) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3511681Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]
2020-04-01T09:15:52.3512360Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]
2020-04-01T09:15:52.3512896Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]
2020-04-01T09:15:52.3513423Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]
2020-04-01T09:15:52.3514270Z 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$1(RestClient.java:342) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3515389Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:500) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3516479Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:493) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3517595Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:472) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3518789Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:413) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3519950Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:538) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3520969Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:531) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3521976Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:111) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3523062Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:323) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3524214Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3525427Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3526519Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3527981Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3529281Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3530553Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3531723Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3532670Z 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242]
2020-04-01T09:15:52.3533291Z Caused by: java.util.concurrent.CompletionException: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /10.1.0.4:30095
2020-04-01T09:15:52.3534014Z 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_242]
2020-04-01T09:15:52.3534558Z 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_242]
2020-04-01T09:15:52.3535104Z 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:957) ~[?:1.8.0_242]
2020-04-01T09:15:52.3535656Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940) ~[?:1.8.0_242]
2020-04-01T09:15:52.3536195Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]
2020-04-01T09:15:52.3536752Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]
2020-04-01T09:15:52.3537746Z 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$1(RestClient.java:342) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3538735Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:500) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3539769Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:493) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3540803Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:472) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3541944Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:413) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3543105Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:538) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3544172Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:531) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3545249Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:111) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3546404Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:323) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3547716Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3548788Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3549808Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3550843Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3551818Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3552932Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3553987Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3554697Z 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242]
2020-04-01T09:15:52.3555491Z Caused by: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /10.1.0.4:30095
2020-04-01T09:15:52.3556071Z Caused by: java.net.ConnectException: Connection refused
2020-04-01T09:15:52.3556462Z 	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_242]
2020-04-01T09:15:52.3556945Z 	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_242]
2020-04-01T09:15:52.3557986Z 	at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3559245Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:336) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3560565Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3561808Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3562902Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3563948Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3565340Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3566520Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-04-01T09:15:52.3567401Z 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242]
2020-04-01T09:15:52.3567942Z Checking for non-empty .out files...
2020-04-01T09:15:52.3568852Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directory
2020-04-01T09:15:52.3570198Z No non-empty .out files.
2020-04-01T09:15:52.3570364Z 
2020-04-01T09:15:52.3571052Z [FAIL] 'Run kubernetes session test' failed after 3 minutes and 15 seconds! Test exited with exit code 1 and the logs contained errors, exceptions or non-empty .out files
{code}",,dian.fu,felixzheng,rmetzger,roman,wangyang0918,zjwang,,,,,,,,,,,,,,,,"wangyang0918 commented on pull request #11619: [FLINK-16921][e2e] Make Kubernetes e2e test stable
URL: https://github.com/apache/flink/pull/11619
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Use the following optimization to make this K8s e2e test more stable both in azure/travis and local.
   
   * Remove start/stop code for non-linux environment, let the user manually start/stop minikube. Since we will encounter various problem in local environment(e.g. Mac), including disk problem, network problem, docker daemon, driver setup(virtualbox, etc.). 
   * Print more debug information so that when the test failed it is easier to investigate
   * Explicitly wait for the dispatcher running and then submit the job to the existing cluster
   
   
   ## Brief change log
   * Do not start/stop minikube in non-linux environment for k8s e2e tests
   * Print more information for debugging when Kubernetes e2e tests failed
   * Wait for rest endpoint up and then submit Flink job to existing Kubernetes session
   
   
   ## Verifying this change
   
   * Run in local environment(Mac) more than 10 times, all should pass
   * Run in azure more that 3 times, all should pass
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 11:26;githubbot;600","rmetzger commented on pull request #11619: [FLINK-16921][e2e] Make Kubernetes e2e test stable
URL: https://github.com/apache/flink/pull/11619
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Apr/20 08:35;githubbot;600","wangyang0918 commented on pull request #11630: [FLINK-16921][e2e] Describe all resources and show pods logs before cleanup when failed
URL: https://github.com/apache/flink/pull/11630
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The pods may be pending because of not enough resources, disk pressure, or other problems. Then wait_rest_endpoint_up will timeout. Describing all resources will help to debug these problems.
   
   We still have some failed instances and can not reproduce in the local environment(Mac/Linux). Open this PR to run e2e tests more times to find the root cause.
   
   ## Brief change log
   * Describe all resources so that we could find more information about why the K8s e2e tests failed
   * Debug log could not show up in sometimes, so move `debug_and_show_logs` before `cleanup`
   
   
   ## Verifying this change
   
   * Run e2e tests more times, K8s related tests should pass
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Apr/20 04:36;githubbot;600","rmetzger commented on pull request #11630: [FLINK-16921][e2e] Describe all resources and show pods logs before cleanup when failed
URL: https://github.com/apache/flink/pull/11630
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Apr/20 08:02;githubbot;600","rmetzger commented on pull request #11641: [FLINK-16921][e2e] Disable K8s tests until instability is resolved
URL: https://github.com/apache/flink/pull/11641
 
 
   ## What is the purpose of the change
   
   Disable an unstable e2e test.
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Apr/20 17:48;githubbot;600","rmetzger commented on pull request #11641: [FLINK-16921][e2e] Disable K8s tests until instability is resolved
URL: https://github.com/apache/flink/pull/11641
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Apr/20 12:25;githubbot;600","wangyang0918 commented on pull request #11710: [FLINK-16921][e2e] Use kubectl instead of curl to wait for rest endpoint up
URL: https://github.com/apache/flink/pull/11710
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   We find that the curl may block in azure pipeline environment for K8s nodeport address. So this PR use the kubectl wait and kubectl logs to wait for the rest endpoint up.
   
   
   ## Brief change log
   
   * Add `wait_rest_endpoint_up_k8s` implemented by `kubectl wait`
   * Enable k8s related tests
   
   
   ## Verifying this change
   
   * Run in my azure pipeline, ten continuous tests passed
   
   * https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=40&view=results
   * https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=41&view=results
   * https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=42&view=results
   * https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=43&view=results
   * https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=44&view=results
   * https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=45&view=results
   * https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=46&view=results
   * https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=47&view=results
   * https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=48&view=results
   * https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=49&view=results
   
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Apr/20 02:26;githubbot;600","rmetzger commented on pull request #11710: [FLINK-16921][e2e] Use kubectl instead of curl to wait for rest endpoint up
URL: https://github.com/apache/flink/pull/11710
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/20 14:53;githubbot;600",,,0,4800,,,0,4800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 19:42:18 UTC 2020,,,,,,,,,,"0|z0d5y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/20 14:36;wangyang0918;I will take a look and try to fix it.;;;","02/Apr/20 01:19;felixzheng;Though I haven't inspected the details, it may be related to [FLINK-16194|[https://github.com/apache/flink/pull/11233]]

We left an open question in that PR and that time we plans to filing another follow-up ticket to solve it:

_For the new design, we don't listen to the *ADD* event when creating the rest Service. The previous design assumes that the Service is ready once the client receives the *ADD* event. However, this is incorrect, no matter for the LB or the NodePort type. We plan to open another issue to further discuss and then fix this problem._

If this is the case, we can help address this problem.

 ;;;","02/Apr/20 08:44;wangyang0918;I have run many times of kubernetes session tests on my local machine(Mac). And only reproduce once when the machine disk is in high disk pressure. So the jobmanager is not launched successfully and blocking at pending state. After a while, the flink client failed with timeout exception. I think we will have some similar problem in the azure environment. Also there could be other reasons that the jobmanager pod does not start in time.

 

I will attach a PR with the following optimization to make this test more stable both in azure/travis and local.
 * Remove start/stop code for non-linux environment, let the user manually start/stop minikube. Since we will encounter various problem in local environment(e.g. Mac), including disk problem, network problem, docker daemon, driver setup(virtualbox, etc.). 
 * Print more debug information so that when the test failed it is easier to investigate
 * Explicitly wait for the dispatcher running and then submit the job to the existing cluster;;;","03/Apr/20 08:35;rmetzger;merged to master in 0d20a6d5b7dc4c2ea0567981ed8b08d278763d00;;;","03/Apr/20 08:36;rmetzger;Thanks a lot for your contribution!;;;","03/Apr/20 19:18;rmetzger;The test failed again: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7046&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","04/Apr/20 08:02;rmetzger;Merged another debugging PR d2d91b62d0bd07f7127c90daa42a5f46745865f9.
This time, I'll keep the ticket open.;;;","05/Apr/20 06:59;rmetzger;It seems that this test is now causing the e2e tests to timeout: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7063&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
2020-04-04T13:20:52.0754138Z Successfully built f903ec8214d7
2020-04-04T13:20:52.0783611Z Successfully tagged test_kubernetes_session:latest
2020-04-04T13:20:52.3071468Z clusterrolebinding.rbac.authorization.k8s.io/flink-role-binding-default created
2020-04-04T13:20:55.5979382Z 2020-04-04 13:20:53,217 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost
2020-04-04T13:20:55.5980850Z 2020-04-04 13:20:53,221 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
2020-04-04T13:20:55.5982902Z 2020-04-04 13:20:53,222 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.heap.size, 1024m
2020-04-04T13:20:55.5984206Z 2020-04-04 13:20:53,222 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
2020-04-04T13:20:55.5985556Z 2020-04-04 13:20:53,222 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
2020-04-04T13:20:55.5986683Z 2020-04-04 13:20:53,222 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 1
2020-04-04T13:20:55.5988046Z 2020-04-04 13:20:53,222 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2020-04-04T13:20:55.5989439Z 2020-04-04 13:20:54,363 INFO  org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
2020-04-04T13:20:55.5990749Z 2020-04-04 13:20:54,373 INFO  org.apache.flink.kubernetes.utils.KubernetesUtils            [] - Kubernetes deployment requires a fixed port. Configuration blob.server.port will be set to 6124
2020-04-04T13:20:55.5992462Z 2020-04-04 13:20:54,373 INFO  org.apache.flink.kubernetes.utils.KubernetesUtils            [] - Kubernetes deployment requires a fixed port. Configuration taskmanager.rpc.port will be set to 6122
2020-04-04T13:20:55.5993732Z 2020-04-04 13:20:54,451 WARN  org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator [] - Found 0 files in directory null/etc/hadoop, skip to mount the Hadoop Configuration ConfigMap.
2020-04-04T13:20:55.5995006Z 2020-04-04 13:20:54,451 WARN  org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator [] - Found 0 files in directory null/etc/hadoop, skip to create the Hadoop Configuration ConfigMap.
2020-04-04T13:20:55.5996642Z 2020-04-04 13:20:55,439 INFO  org.apache.flink.kubernetes.KubernetesClusterDescriptor      [] - Create flink session cluster flink-native-k8s-session-1 successfully, JobManager Web Interface: http://10.1.0.4:30480
2020-04-04T16:06:50.4761095Z ##[error]The operation was canceled.
2020-04-04T16:06:50.4780542Z ##[section]Finishing: Run e2e tests
{code}
If you look at the timestamps at the beginning of the log, you see that the test gets stuck till the operation gets cancelled.;;;","05/Apr/20 07:03;rmetzger;The nightly e2e tests had the same issue: 
- https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7069&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5
- https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7069&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=375367d9-d72e-5c21-3be0-b45149130f6b
- https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7069&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334
- https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7069&view=logs&j=7bafe89a-737e-5a81-708c-24b72a2345fc&t=8f0197c1-92aa-5b5f-4284-1ae542d75a1e


Upgrading this to a blocker.;;;","05/Apr/20 07:07;rmetzger;We should consider disabling the test until it is stable again.;;;","05/Apr/20 17:49;rmetzger;E2e tests are failing quite reliably with this: https://github.com/apache/flink/pull/11641;;;","06/Apr/20 11:28;rmetzger;More failures: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7086&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","06/Apr/20 11:44;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7091&view=results
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7085&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=08866332-78f7-59e4-4f7e-49a56faa3179
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7063&view=results
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7052&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=375367d9-d72e-5c21-3be0-b45149130f6b;;;","06/Apr/20 12:26;rmetzger;As of https://github.com/apache/flink/commit/d2be6aef18b3e1afbbf1d3f4ed0694460ed870b6, this test has been disabled. Please re-enable it in the PR fixing the issues.;;;","07/Apr/20 04:44;rmetzger;I only disabled it on Azure, not in Travis' splits.
Here's a travis failure https://travis-ci.org/github/apache/flink/jobs/671655599?utm_medium=notification&utm_source=slack :) ;;;","07/Apr/20 06:38;wangyang0918;I am just wondering why the test could block at {{wait_rest_endpoint_up}}. Maybe it has some relation with the {{curl}}. I will dig into and find the root cause.;;;","08/Apr/20 03:46;wangyang0918;I have add an explicit timeout(3s) for {{curl}} in {{wait_rest_endpoint}}. And run it in my azure for 5 times, the K8s related tests could always pass.

[https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=6&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=2f5b54d0-1d28-5b01-d344-aa50ffe0cdf8]

[https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=5&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=2f5b54d0-1d28-5b01-d344-aa50ffe0cdf8]

[https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=4&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=2f5b54d0-1d28-5b01-d344-aa50ffe0cdf8]

[https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=3&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=2f5b54d0-1d28-5b01-d344-aa50ffe0cdf8]

[https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=1&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=2f5b54d0-1d28-5b01-d344-aa50ffe0cdf8]

 

I will run 5 more times and try to get the conclusion.;;;","08/Apr/20 08:53;rmetzger;Thanks a lot. The JVM crash in the first run is quite interesting. Maybe I should add the tooling for collecting crashreports also to e2e tests :)
You probably noticed that the build takes a lot of time (2hrs+) That's probably because the maven artifacts are hosted in Europe. You can either move your azure account to europe, or push to ""master"" once. This will create a cache file containing all the maven dependencies.
These test branches won't need to download all the dependencies, speeding up the build.

I agree with the approach of having 10 passed builds to verify the fix.;;;","09/Apr/20 05:17;wangyang0918;Unfortunately, adding the connection timeout for {{curl}} still could not solve this problem. I will try to use other ways to make sure the jobmanager is launched successfully, for example, use {{kubectl log}} and grep the logs ""Rest endpoint listening at"".

 

[https://dev.azure.com/wangyang0918/Flink/_build/results?buildId=10&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=2f5b54d0-1d28-5b01-d344-aa50ffe0cdf8];;;","10/Apr/20 04:18;zjwang;Another instance: [https://travis-ci.org/github/apache/flink/builds/672997960] for  kubernetes session test failure.

It only indicates ""

Error from server (BadRequest): container ""flink-task-manager"" in pod ""flink-task-manager-67959f4985-fd6tr"" is terminated

"" in the log, not sure the root cause is the same.;;;","10/Apr/20 06:42;wangyang0918;[~zjwang] Thanks for you providing the information. This case failed because ""Dispatcher REST endpoint has not started within a timeout of 20 sec"". I am trying to refactor the {{wait_rest_endpoint_up}} on K8s to make the tests more stable. When it could pass in continuous 10 cases, i think it is stable and will upload a new PR.;;;","12/Apr/20 16:01;zjwang;Another instance for this test failure, but not given specific reason, I guess not finish within expected time. The log is [https://api.travis-ci.org/v3/job/673786844/log.txt];;;","15/Apr/20 01:05;dian.fu;Another instance : [https://api.travis-ci.org/v3/job/674870394/log.txt]

It contains the following logs:
{code}
2020-04-14 21:19:53,388 INFO org.apache.flink.kubernetes.KubernetesClusterDescriptor [] - Create flink session cluster flink-native-k8s-session-1 successfully, JobManager Web Interface: http://10.20.0.226:31902 Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Waiting for Dispatcher REST endpoint to come up... Dispatcher REST endpoint has not started within a timeout of 20 sec
{code}

and 
{code}
2020-04-14 21:20:13,816 ERROR [flink-rest-server-netty-worker-thread-2] org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler [] - Exception occurred in REST handler: Cannot connect to ResourceManager right now. Please try to refresh. 2020-04-14 21:20:14,992 ERROR [flink-rest-server-netty-worker-thread-1] org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler [] - Exception occurred in REST handler: Cannot connect to ResourceManager right now. Please try to refresh. 2020-04-14 21:20:16,013 ERROR [flink-rest-server-netty-worker-thread-2] org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler [] - Exception occurred in REST handler: Cannot connect to ResourceManager right now. Please try to refresh.
{code};;;","15/Apr/20 03:48;wangyang0918;[~dian.fu] [~zjwang] Thanks for sharing more instances. I have already filed a PR to fix this problem. [https://github.com/apache/flink/pull/11710];;;","17/Apr/20 14:54;rmetzger;Resolved in https://github.com/apache/flink/commit/b7f16a406c46f27de785fdd7e831a3b3f6f2a97b

Thanks a lot!;;;","11/May/20 11:43;roman;Another instance (with [changes|https://github.com/apache/flink/pull/11710/commits] merged):

[https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/944/logs/87]

 

Reopening.;;;","11/May/20 12:43;wangyang0918;[~roman_khachatryan] It is another issue because of fabric8 kubernetes-client compatibility with jdk 8u252. Azure pipeline has upgraded the jdk version recently. Please refer FLINK-17416 for more information.;;;","11/May/20 19:42;roman;Thanks for the clarification [~fly_in_gis];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""TPC-DS end-to-end test (Blink planner)"" gets stuck",FLINK-16917,13295486,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,rmetzger,rmetzger,01/Apr/20 09:48,22/Jun/21 14:05,13/Jul/23 08:07,02/Apr/20 11:20,,,,,,1.11.0,,,,Runtime / Task,Tests,,,,0,pull-request-available,test-stability,,,"The message you see from the CI system is
{code}
##[error]The job running on agent Hosted Agent ran longer than the maximum time of 240 minutes. For more information, see https://go.microsoft.com/fwlink/?linkid=2077134
{code}

Example: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6899&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee

The end of the log file looks as follows:

{code}
2020-03-31T23:00:40.5416207Z [INFO]Run TPC-DS query 97 success.
2020-03-31T23:00:40.5439265Z [INFO]Run TPC-DS query 98 ...
2020-03-31T23:00:40.8269500Z Job has been submitted with JobID eec4759ae6d585ee9f8d9f84f1793c0e
2020-03-31T23:01:33.4757621Z Program execution finished
2020-03-31T23:01:33.4758328Z Job with JobID eec4759ae6d585ee9f8d9f84f1793c0e has finished.
2020-03-31T23:01:33.4758880Z Job Runtime: 51093 ms
2020-03-31T23:01:33.4759057Z 
2020-03-31T23:01:33.4760999Z [INFO]Run TPC-DS query 98 success.
2020-03-31T23:01:33.4761612Z [INFO]Run TPC-DS query 99 ...
2020-03-31T23:01:33.7297686Z Job has been submitted with JobID f47efc4194df2e0ead677fff239f3dfd
2020-03-31T23:01:50.0037484Z ##[error]The operation was canceled.
2020-03-31T23:01:50.0091655Z ##[section]Finishing: Run e2e tests
{code}

Notice the time difference between ""Job has been submitted"" and ""The operation was canceled."". There was nothing happening for 20 minutes.
",,AHeise,begginghard,danny0405,godfreyhe,jark,leonard,pnowojski,rmetzger,,,,,,,,,,,,,,"AHeise commented on pull request #11611: [FLINK-16917][runtime] Revert FLINK-16245
URL: https://github.com/apache/flink/pull/11611
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Reverts FLINK-16245 to improve/stabilize table API e2e tests.
   
   ## Brief change log
   
   ## Verifying this change
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 07:37;githubbot;600","rmetzger commented on pull request #11611: [FLINK-16917][runtime] Revert FLINK-16245
URL: https://github.com/apache/flink/pull/11611
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 11:11;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,FLINK-16910,,,,,,,,,,,,,,FLINK-16795,,,,,,,,,"02/Apr/20 06:30;rmetzger;Screenshot 2020-04-02 08.12.01.png;https://issues.apache.org/jira/secure/attachment/12998550/Screenshot+2020-04-02+08.12.01.png","02/Apr/20 06:30;rmetzger;Screenshot 2020-04-02 08.24.28.png;https://issues.apache.org/jira/secure/attachment/12998551/Screenshot+2020-04-02+08.24.28.png","02/Apr/20 01:32;ykt836;image-2020-04-02-09-32-52-979.png;https://issues.apache.org/jira/secure/attachment/12998540/image-2020-04-02-09-32-52-979.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 16 21:29:28 UTC 2020,,,,,,,,,,"0|z0d5ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/20 09:55;rmetzger;Maybe the script should be changed to expect an output after at least 5 minutes. Otherwise, fail the test and print the Flink logs for further debugging.
;;;","01/Apr/20 10:05;jark;cc [~danny0405] [~godfreyhe];;;","01/Apr/20 10:22;danny0405;Thanks for reporting this, would try a test on my MAC soon ~;;;","01/Apr/20 10:31;rmetzger;The error does not happen all the time;;;","01/Apr/20 12:13;rmetzger;Maybe the test is actually failing consistently. This might be related: FLINK-16906;;;","01/Apr/20 17:26;rmetzger;Another case: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6927&view=results;;;","02/Apr/20 01:33;ykt836;I think the root cause is tpc-ds tests have slowed down a lot. It only took us <20 min to run, but now it takes ~2 hour. 

I checked the master test history, FLINK-16245 seems to be the root casue.

You can see until FLINK-16767, the performance seems to be still normal. 

cc [~AHeise] [~chesnay]

!image-2020-04-02-09-32-52-979.png!;;;","02/Apr/20 02:26;danny0405;It seems that after FLINK-16411[1], the tests take about 5h to finish, before that, we usually finish the whole tests within 3h.

But it's interesting that the FLINK-16411 test itself take 3 hours, but following that a typo fix can not finish already.

[1]  https://dev.azure.com/rmetzger/Flink/_build?definitionId=4&_a=summaryhttps://dev.azure.com/rmetzger/Flink/_build?definitionId=4&_a=summary
[2] https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6894;;;","02/Apr/20 06:31;rmetzger;Thank you for looking into this ticket. 

[~ykt836] It is interesting how the runtime of the test has changed! I believe the issue was introduced much earlier than FLINK-16245.

My analysis points at FLINK-14338 to be the root cause.

[~danny0405] You need to look at this Pipeline: https://dev.azure.com/rmetzger/Flink/_build?definitionId=8&_a=summary, the other one you posted is just from the pull requests, thus it is not in order of the pushes to master.

Why do I believe FLINK-14338 is the root cause?

Once FLINK-14338 got merged, the e2e tests started failing with ""flink-table-planner contains unwanted dependency org.apiguardian.api"" (reported and fixed in FLINK-16878). Since then, the ""TPC-DS end-to-end test (Blink planner)"" was not executed anymore (because it runs after the dependency check).
Once FLINK-16878 got resolved, the TPC-DS e2e test started timing out.

I visualized this for you:
 !Screenshot 2020-04-02 08.12.01.png! 

Once FLINK-16878 got resolved, the TPC-DS e2e test started timing out (see the 4hrs duration):
 !Screenshot 2020-04-02 08.24.28.png!  

;;;","02/Apr/20 06:44;arvid;Hi [~ykt836] , FLINK-16245 introduced a safety net around the user code classloader, to ensure user classes are not accessible after a job terminates. My main expectation would be that either stuff runs or it breaks, but having a much longer e2e does not immediately sense.

Two options where I could see it happen:
 * Classloading is much slower now because of indirection. The safety net is also not parallel (but afaik none of our classloaders are). I severely doubt that this would increase e2e times just for this one test suite.
 * Some cleanup code needs new classes and is invoked after classloader is closed. That part then runs in timeouts. Did you observe any timeout warnings in the log?

@all, did you also observe increased times for other e2e tests?;;;","02/Apr/20 06:48;ykt836;[~rmetzger]  [~AHeise]

You can check out  *#20200331.7 FLINK-16767[hive] Failed to read Hive table with RegexSerDe* , which committed after calcite upgrade. 

It's true that the e2e test failed in that pipeline, but the tpc-ds e2e test actually succeeded before ""Dependency shading of table modules test"" fails:

2020-03-31T11:56:37.8941780Z [PASS] 'TPC-DS end-to-end test (Blink planner)' passed after *19 minutes* and 33 seconds! Test exited with exit code 0.

 

And then if you check out the next run: ""*#20200331.8 FLINK-16245[tests] Adjust BatchFineGrainedRecoveryITCase*"", you will see tpc-ds tests was already very slow:

2020-03-31T13:35:03.4588365Z [PASS] 'TPC-DS end-to-end test (Blink planner)' passed after *86 minutes* and 17 seconds! Test exited with exit code 0.

 ;;;","02/Apr/20 06:53;pnowojski;Let's revert FLINK-16245 as that's relatively simple PR and see if this solves the problem.;;;","02/Apr/20 06:55;rmetzger;[~AHeise] Kurt is relating to the high build times of ""#20200331.8 [FLINK-16245][tests] Adjust BatchFineGrainedRecoveryITCase"" [1].
I have analyzed the e2e build time of ""#20200331.15 [hotfix] Fix typo in org.apache.flink.sql.parser.ddl.SqlCreateTable"" [2].
Column J shows the durations of the e2e tests: https://docs.google.com/spreadsheets/d/1VpA3wsOY88ezY8qTUFl8iztPc58mRa-1pIncIBAbUL8/edit#gid=117660203 As you can see in K, the e2e tests are not slower because of that (except for ""TPC-DS end-to-end test (Blink planner)"", which doesn't finish :) )


[1] https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6877&view=result
[2] https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6899&view=results


;;;","02/Apr/20 06:57;rmetzger;[~ykt836] You are right! My analysis is wrong.

For some reason I thought that ""Dependency shading of table modules test"" runs before the TPC-DS test :( 

I agree with Piotr to revert the commit.;;;","02/Apr/20 06:58;arvid;:+1 to reverting FLINK-16245, I guess we need to understand the issue better before reapplying the patch.;;;","02/Apr/20 06:59;ykt836;And I quickly went through other e2e tests between #20200331.7 and #20200331.8, it's strange only tpc-h and tpc-ds tests are slowed down.**;;;","02/Apr/20 07:03;rmetzger;I would like to mention that the e2e test run of the pull request adding FLINK-16245 [1] showed the issue that we are discussing here!
I know the Azure tests are not the most stable one, but they are sometimes showing real issues :) 

[1] https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6813&view=logs&j=e428c73e-5bb8-51ab-b1f4-da736192251d&t=fb85e9b1-9460-5259-9251-5d7b922523ea;;;","02/Apr/20 07:09;arvid;I'm assuming that something in Table API is leaking the classloader and hence we only see the issue there. [~chesnay] , seems like failing fast would have been the better option after all.;;;","02/Apr/20 07:20;rmetzger;Who's going to revert?;;;","02/Apr/20 08:10;rmetzger;[~AHeise] is. I've assinged you to the ticket.;;;","02/Apr/20 11:12;rmetzger;I merged the revert: c79c26a1089187c50bbd39d53bcfa47b509f47e7.

I guess we can close this ticket, as the issue will be tracked in FLINK-16245 ?;;;","16/Aug/20 21:29;chesnay;A fix for the orc issues has been merged to master for 1.12, along with an option to disable FLINK-16245 in case there are similar instances in the future:

6c130daaf59e77b343d1c947822ea0573738a204

fce82d7f56d5da3d3bf9ea6b66888d1350eb172f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The logic of NullableSerializer#copy is wrong,FLINK-16916,13295485,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,klion26,klion26,klion26,01/Apr/20 09:45,02/Apr/20 17:01,13/Jul/23 08:07,02/Apr/20 17:01,1.10.0,1.8.3,1.9.2,,,1.10.1,1.11.0,1.8.4,1.9.3,API / Type Serialization System,,,,,0,pull-request-available,,,,"When debugging the problem reported by FLINK-16724, Found that the logic of {{NullableSerializer#copy}} is wrong. currently, the logic is such as below:
{code:java}
public void copy(DataInputView source, DataOutputView target) throws IOException {
   boolean isNull = source.readBoolean();
   target.writeBoolean(isNull);
   if (isNull) {
      target.write(padding);
   }
   else {
      originalSerializer.copy(source, target);
   }
}

{code}
we forgot to skip {{paddings.length}} bytes when if the {{padding}}'s length is not 0.

We can correct the logic such as below 
{code:java}
public void copy(DataInputView source, DataOutputView target) throws IOException {
   boolean isNull = deserializeNull(source); // this will skip the padding values.
   target.writeBoolean(isNull);
   if (isNull) {
      target.write(padding);
   }
   else {
      originalSerializer.copy(source, target);
   }
}
{code}",,aljoscha,klion26,,,,,,,,,,,,,,,,,,,,"klion26 commented on pull request #11614: [FLINK-16916][serialization] Fix the logic of NullableSerializer#copy
URL: https://github.com/apache/flink/pull/11614
 
 
   ## What is the purpose of the change
   
   Fix the wrong logic of `NullableSerializer#copy`, 
   currently, the logic is such as below:
   ```
   public void copy(DataInputView source, DataOutputView target) throws IOException {
      boolean isNull = source.readBoolean();
      target.writeBoolean(isNull);
      if (isNull) {
         target.write(padding);
      }
      else {
         originalSerializer.copy(source, target);
      }
   }
   ```
   we forgot to skip paddings.length bytes when if the padding's length is not 0.
   
   We can correct the logic such as below 
   ```
   public void copy(DataInputView source, DataOutputView target) throws IOException {
      boolean isNull = deserializeNull(source); // this will skip the padding values.
      target.writeBoolean(isNull);
      if (isNull) {
         target.write(padding);
      }
      else {
         originalSerializer.copy(source, target);
      }
   }
   ```
   
   
   ## Verifying this change
   Harden the test of `NullalbeSerialierTest#testSerializedCopyAsSequence` by update the test data.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (yes)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 08:39;githubbot;600","aljoscha commented on pull request #11614: [FLINK-16916][serialization] Fix the logic of NullableSerializer#copy
URL: https://github.com/apache/flink/pull/11614
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 16:45;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 17:01:57 UTC 2020,,,,,,,,,,"0|z0d5pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/20 09:48;klion26;[~aljoscha] , what do you think about this, please assign this to me if it is reasonable.
As this can lead to wrong answer when using this serializer, so mark it as BLOCKer, please downgrade if the priority set wrong.;;;","02/Apr/20 07:36;aljoscha;Good analysis!;;;","02/Apr/20 17:01;aljoscha;master: d6d7158d0a205e7d4524ae39c5cd7512d7c0ab38
release-1.10: 5c89c61c051c2f5728d8a5bfb7d833158f9eabb7
release-1.9: 6c4112ee955171331f464ffd8c13324adc83f06e
release-1.8: 8e97e1137fc85db2dbef6d50b687332f0c013ce5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadableConfigToConfigurationAdapter#getEnum throws UnsupportedOperationException,FLINK-16913,13295466,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,felixzheng,felixzheng,01/Apr/20 08:50,07/Apr/20 14:14,13/Jul/23 08:07,07/Apr/20 11:54,1.10.0,,,,,1.10.1,1.11.0,,,Runtime / Configuration,,,,,0,pull-request-available,,,,"Steps to reproduce the issue:
 # Set flink-conf.yaml
 ** state.backend: rocksdb
 ** state.checkpoints.dir: hdfs:///flink-checkpoints
 ** state.savepoints.dir: hdfs:///flink-checkpoints
 # Start a Kubernetes session cluster
 # Submit a job to the session cluster, unfortunately a UnsupportedOperationException occurs.

{code:java}
 The program finished with the following exception:org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: The adapter does not support this method
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:143)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:659)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:210)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:890)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:963)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:963)
Caused by: java.lang.UnsupportedOperationException: The adapter does not support this method
	at org.apache.flink.configuration.ReadableConfigToConfigurationAdapter.getEnum(ReadableConfigToConfigurationAdapter.java:258)
	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.<init>(RocksDBStateBackend.java:336)
	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.configure(RocksDBStateBackend.java:394)
	at org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory.createFromConfig(RocksDBStateBackendFactory.java:47)
	at org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory.createFromConfig(RocksDBStateBackendFactory.java:32)
	at org.apache.flink.runtime.state.StateBackendLoader.loadStateBackendFromConfig(StateBackendLoader.java:154)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.loadStateBackend(StreamExecutionEnvironment.java:792)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.configure(StreamExecutionEnvironment.java:761)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.<init>(StreamExecutionEnvironment.java:217)
	at org.apache.flink.client.program.StreamContextEnvironment.<init>(StreamContextEnvironment.java:53)
	at org.apache.flink.client.program.StreamContextEnvironment.lambda$setAsContext$2(StreamContextEnvironment.java:103)
	at java.util.Optional.map(Optional.java:215)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getExecutionEnvironment(StreamExecutionEnvironment.java:1882)
	at org.apache.flink.streaming.examples.socket.SocketWindowWordCount.main(SocketWindowWordCount.java:62)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
	... 8 more
{code}
I am wondering why we introduce {{ReadableConfigToConfigurationAdapter}} to wrap the {{Configuration}} but leave many of the getter methods in it to throw UnsupportedOperationException that causes potential problems.",,dwysakowicz,felixzheng,klion26,kyledong,liyu,lsy,,,,,,,,,,,,,,,,"dawidwys commented on pull request #11622: [FLINK-16913][configuration, statebackend] Migrated StateBackends to use ReadableConfig instead of Configuration
URL: https://github.com/apache/flink/pull/11622
 
 
   ## What is the purpose of the change
   
   StateBackendFactories do not need a full read and write access to the
   Configuration object. It's sufficient to have read only access. Moreover
   the ReadableConfig is a lightweight interface that can be implemented in
   other ways, not just through the Configuration. Lastly we exposed this
   lightweight interface as a configuration entry point for
   ExecutionEnvironments. This change will make it possible to pass the
   ReadableConfig directly to the StateBackendFactories without fragile
   adapters.
   
   
   ## Brief change log
   
     - Replaced Configuration with ReadableConfig for StateBackends configuration
     - Changed type of `TIMER_SERVICE_FACTORY` option
     - Removed `ReadableConfigToConfigurationAdapter.java`
   
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (**yes** / no)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 15:46;githubbot;600","dawidwys commented on pull request #11637: [FLINK-16913][configuration] Support more getters in the ReadableConfigToConfigurationAdapter
URL: https://github.com/apache/flink/pull/11637
 
 
   ## What is the purpose of the change
   
   Support all getters in ReadableConfigToConfigurationAdapter. It fixes configuring RocksDBStateBackend from flink-conf.yaml. In 1.11, we fix the issue by using the `ReadableConfig` directly for configuration. In 1.10.1 we don't want to change `PublicEvolving` interface.
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Apr/20 11:53;githubbot;600","dawidwys commented on pull request #11622: [FLINK-16913][configuration, statebackend] Migrated StateBackends to use ReadableConfig instead of Configuration
URL: https://github.com/apache/flink/pull/11622
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Apr/20 17:31;githubbot;600","dawidwys commented on pull request #11637: [FLINK-16913][configuration] Support more getters in the ReadableConfigToConfigurationAdapter
URL: https://github.com/apache/flink/pull/11637
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Apr/20 11:46;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/20 08:46;felixzheng;image-2020-04-01-16-46-13-122.png;https://issues.apache.org/jira/secure/attachment/12998422/image-2020-04-01-16-46-13-122.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 07 14:14:22 UTC 2020,,,,,,,,,,"0|z0d5lc:",9223372036854775807,"Starting from 1.11 the StateBackendFactory#createFromConfig interface now takes ReadableConfig instead of Configuration. A configuration class is still a valid argument to that method, as it implements the ReadableConfig interface. Implementors of custom StateBackends should adjust their implementations.",,,,,,,,,,,,,,,,,,,"07/Apr/20 11:50;dwysakowicz;Hi [~felixzheng] thank you for creating the issue. The reason for introducing the {{ReadableConfigToConfigurationAdapter}} was to support configuring state backends from flink-conf.yaml on the client side without changing interfaces such as {{StateBackendFactory}}. I wanted to implement the minimal set of getters to support all the {{StateBackendFactories}}. It turned out I missed some of the options. Moreover I agree with you that not supporting all the getters is fragile.

In master branch I removed the class and changed all the affected classes to use the {{ReadableConfig}} instead. In 1.10.1 I took a slightly different approach not to change any interfaces in a minor release. In 1.10.1 I implemented the affected getters.;;;","07/Apr/20 11:51;dwysakowicz;Implemented in:
master: bb46756b84940a6134910e74406bfaff4f2f37e9
1.10.1: 23e984d6e7ae5983762d2be5b9884b4c81311f75;;;","07/Apr/20 14:14;felixzheng;Thanks for the quick fix and the explanation [~dwysakowicz].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PrestoS3FileSystemITCase#testSimpleFileWriteAndRead fails on checkPathExistence,FLINK-16911,13295461,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,pnowojski,pnowojski,01/Apr/20 08:29,22/May/20 10:29,13/Jul/23 08:07,22/May/20 10:29,,,,,,1.11.0,,,,FileSystems,Tests,,,,0,test-stability,,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6901&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=4460a45f-967f-53b9-4464-d878fd31f3f7

{noformat}
[ERROR] Tests run: 7, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 32.365 s <<< FAILURE! - in org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase
[ERROR] testSimpleFileWriteAndRead[Scheme = s3p](org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase)  Time elapsed: 13.992 s  <<< FAILURE!
java.lang.AssertionError: expected:<true> but was:<false>

[ERROR] org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase  Time elapsed: 14.415 s  <<< FAILURE!
java.lang.AssertionError

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   PrestoS3FileSystemITCase>AbstractHadoopFileSystemITTest.teardown:149->AbstractHadoopFileSystemITTest.cleanupDirectoryWithRetry:162
[ERROR]   PrestoS3FileSystemITCase>AbstractHadoopFileSystemITTest.testSimpleFileWriteAndRead:83->AbstractHadoopFileSystemITTest.checkPathExistence:62 expected:<true> but was:<false>
[INFO] 

{noformat}
",,aljoscha,dian.fu,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 10:29:37 UTC 2020,,,,,,,,,,"0|z0d5k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/20 12:41;rmetzger;Another case: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7056&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=d26b3528-38b0-53d2-05f7-37557c2405e4;;;","04/Apr/20 12:43;rmetzger;[~pnowojski] Note: You've assigned this ticket to the ""Connectors / FileSystem"" component. I think ""FileSystem"" is the right component for everything that relates to the file systems implementations itself.;;;","07/Apr/20 16:48;rmetzger;another case: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7149&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=d26b3528-38b0-53d2-05f7-37557c2405e4;;;","08/May/20 01:46;dian.fu;Another instance: https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/765/logs/104;;;","19/May/20 07:28;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1742&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","22/May/20 10:19;aljoscha;I believe this is a timeout problem, the {{deadline}} is set in {{@BeforeClass}}, so it's shared for all the tests, maybe 30s is a bit low for that.;;;","22/May/20 10:29;aljoscha;release-1.11: 801ce9271f757b90a41de477c8903deaa4c37170
master: 006ab7abbe1b3dea415505903714650b2abb1189

I increased the timeout, please re-open if it happens again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kinesis connector NOTICE should have contents of AWS KPL's THIRD_PARTY_NOTICES file manually merged in,FLINK-16901,13295379,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,liyu,tzulitai,tzulitai,31/Mar/20 23:28,01/May/20 13:54,13/Jul/23 08:07,01/May/20 13:54,1.10.0,,,,,1.10.1,1.11.0,,,Connectors / Kinesis,,,,,0,legal,pull-request-available,,,"The Flink Kinesis connector artifact bundles AWS KPL's [THIRD_PARTY_NOTICES|https://github.com/awslabs/amazon-kinesis-producer/blob/master/THIRD_PARTY_NOTICES] file under the {{META-INF}} folder.

The contents of this should be manually merged into the artifact's own NOTICE file, and the {{THIRD_PARTY_NOTICES}} file itself excluded.

---

Since Stateful Functions' {{statefun-flink-distribution}} bundles the Flink Kinesis connector, the {{THIRD_PARTY_NOTICES}} is bundled there as well. For now, since we're already about to release Stateful Functions, we'll have to apply these changes downstream in {{statefun-flink-distribution}}'s NOTICE file.

Once this is fixed upstream in the Flink Kinesis connector's NOTICE file, we can revert the changes in StateFun.",,liyu,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 13:54:50 UTC 2020,,,,,,,,,,"0|z0d520:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/May/20 04:38;liyu;Merged into release-1.10 via 54a1dbfb28df21b2e1170b95290f3dacdbfee9c5;;;","01/May/20 13:52;liyu;Checked and confirmed master branch also has the same issue, and merged the fix into master via 261e72119b69c4fc3e22d9bcdec50f6ca2fdc2e9;;;","01/May/20 13:54;liyu;Change the issue type to `Bug` since the previous behavior didn't follow our [licensing policy|https://cwiki.apache.org/confluence/display/FLINK/Licensing].;;;","01/May/20 13:54;liyu;Closing the issue since all work done.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing jquery license file in Stateful Functions,FLINK-16893,13295338,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sewen,sewen,31/Mar/20 19:16,01/Apr/20 02:58,13/Jul/23 08:07,01/Apr/20 02:58,,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,,,,,"jquery is part of the source release, because it is bundled as part of the docs system.
We need to have its MIT license file in the source release.",,sewen,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 02:58:49 UTC 2020,,,,,,,,,,"0|z0d4t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/20 02:58;tzulitai;Fixed.

master - fda6b3ea805cbd390abd53cdeef7a3de515f06bf
release-2.0 - c3b889ab3d87ecf9bd6412a0e2057765993bbee4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
statefun-flink-distribution bundles many unwanted dependencies,FLINK-16892,13295337,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,sewen,sewen,31/Mar/20 19:14,01/Apr/20 02:57,13/Jul/23 08:07,01/Apr/20 02:57,,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,,,,,"Because the ""provided"" scope in Maven does not work well with transitive dependencies, many transitive dependencies are interpreted as ""compile"" scope and included in the shaded jar.

Currently only ""flink-runtime"" and ""flink-streaming-java"" are overridden as provided in ""statefun-flink-distribution"". We need to add all Flink other dependent Flink modules directly as provided.",,sewen,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 02:57:43 UTC 2020,,,,,,,,,,"0|z0d4sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/20 02:57;tzulitai;Fixed.

master: d059466ea27e22b5d13ebf6d21f4432d5aabd25f
release-2.0: ab494030a120af07fc4011074fe7918e7dac50ea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Re-add jquery license file under ""/licenses""",FLINK-16888,13295303,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,sewen,sewen,31/Mar/20 16:15,01/Apr/20 07:47,13/Jul/23 08:07,01/Apr/20 07:47,1.10.1,1.11.0,,,,1.10.1,1.11.0,,,Build System,,,,,0,pull-request-available,,,,"The license file for jquery was removed together with the license files for the other components from the old web UI.

However, jquery is also used in the docs and through that part of the source release. We hence need to add the license file back.",,liyu,sewen,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11585: [FLINK-16888][legal] Add jquery license
URL: https://github.com/apache/flink/pull/11585
 
 
   Adds the jquery license since it is used for the documentation.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 16:39;githubbot;600","zentol commented on pull request #11585: [FLINK-16888][legal] Add jquery license
URL: https://github.com/apache/flink/pull/11585
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 07:46;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,FLINK-16331,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 07:47:40 UTC 2020,,,,,,,,,,"0|z0d4lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/20 07:47;chesnay;master: bb195633f3d1194f63a91d918581a51681440b28
1.10: a3de1a3a65a8bbba47af3246c3f46e2aa9b23e82 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL hive-connector wilcard excludes don't work on maven 3.1.X,FLINK-16885,13295240,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,chesnay,chesnay,31/Mar/20 12:11,03/Apr/20 03:34,13/Jul/23 08:07,03/Apr/20 03:34,1.11.0,,,,,1.11.0,,,,Build System,Connectors / Hive,,,,0,pull-request-available,,,,"The sql-connector-hive modules added in FLINK-16455 use wildcards imports to exclude all transitive dependencies from hive.

This is a maven 3.2.1+ feature. This may imply that Flink cannot be properly built anymore with maven 3.1 .",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #11620: [FLINK-16885][hive] Remove wilcard excludes that don't work on maven 3.1.X
URL: https://github.com/apache/flink/pull/11620
 
 
   
   ## What is the purpose of the change
   
   The sql-connector-hive modules added in FLINK-16455 use wildcards imports to exclude all transitive dependencies from hive.
   
   This is a maven 3.2.1+ feature. This may imply that Flink cannot be properly built anymore with maven 3.1 .
   
   ## Brief change log
   
   Just remove the exclusions.
   The shade-plugin will remove all transitive dependencies since promoteTransitiveDependencies is false.
   
   All declared dependencies are included in the artifactSet.
   
   ## Verifying this change
   
   This change is a trivial rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 12:30;githubbot;600","JingsongLi commented on pull request #11620: [FLINK-16885][hive] Remove wilcard excludes that don't work on maven 3.1.X
URL: https://github.com/apache/flink/pull/11620
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Apr/20 03:33;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 03:34:26 UTC 2020,,,,,,,,,,"0|z0d47c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/20 11:13;lzljs3620320;Sorry for this, Hi [~chesnay], do you have suggestion for excluding all transitive dependencies? Or exclude all one by one.;;;","02/Apr/20 11:18;chesnay;You can just remove the exclusions.
The shade-plugin will remove all transitive dependencies since {{promoteTransitiveDependencies}} is false.

This will work so long as all declared dependencies are included in the {{artifactSet}}.;;;","02/Apr/20 12:27;lzljs3620320;Thanks [~chesnay], got it.;;;","03/Apr/20 03:34;lzljs3620320;master: 2b94ca60ca437df98d0efeb7b9a43ecdf216825b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-table-planner contains unwanted dependency org.apiguardian.api,FLINK-16878,13295170,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,danny0405,rmetzger,rmetzger,31/Mar/20 06:57,02/Apr/20 10:36,13/Jul/23 08:07,31/Mar/20 13:06,1.11.0,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,"CI run: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6856&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
==============================================================================
Running 'Dependency shading of table modules test'
==============================================================================
TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-57663957727
Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
Success: There are no unwanted dependencies in the /home/vsts/work/1/s/flink-end-to-end-tests/../flink-table/flink-table-api-java/target/flink-table-api-java-1.11-SNAPSHOT.jar jar.
Success: There are no unwanted dependencies in the /home/vsts/work/1/s/flink-end-to-end-tests/../flink-table/flink-table-api-scala/target/flink-table-api-scala_2.11-1.11-SNAPSHOT.jar jar.
Success: There are no unwanted dependencies in the /home/vsts/work/1/s/flink-end-to-end-tests/../flink-table/flink-table-api-java-bridge/target/flink-table-api-java-bridge_2.11-1.11-SNAPSHOT.jar jar.
Success: There are no unwanted dependencies in the /home/vsts/work/1/s/flink-end-to-end-tests/../flink-table/flink-table-api-scala-bridge/target/flink-table-api-scala-bridge_2.11-1.11-SNAPSHOT.jar jar.
Failure: There are unwanted dependencies in the /home/vsts/work/1/s/flink-end-to-end-tests/../flink-table/flink-table-planner/target/flink-table-planner_2.11-1.11-SNAPSHOT.jar jar:       -> org.apiguardian.api                                not found
      -> org.apiguardian.api                                not found
      -> org.apiguardian.api                                not found
      -> org.apiguardian.api                                not found
      -> org.apache.commons.io.input                        not found
[FAIL] Test script contains errors.
Checking for errors...
No errors in log files.
Checking for exceptions...
No exceptions in log files.
Checking for non-empty .out files...
grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directory
No non-empty .out files.

[FAIL] 'Dependency shading of table modules test' failed after 0 minutes and 14 seconds! Test exited with exit code 1

{code}",,jark,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,"wuchong commented on pull request #11575: [FLINK-16878][e2e][table] Fix dependency shading of table modules test failure after …
URL: https://github.com/apache/flink/pull/11575
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 13:06;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14338,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 13:06:46 UTC 2020,,,,,,,,,,"0|z0d3rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/20 07:02;jark;Sorry [~rmetzger], will fix it soon. ;;;","31/Mar/20 07:05;rmetzger;Cool, thanks a lot :) ;;;","31/Mar/20 13:06;jark;Fixed in master (1.11.0): 451975853cb0a1999ed3dbb57f4190521f1e0d5c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to push filter into OrcTableSource when upgrading to 1.9.2,FLINK-16860,13294920,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,nikobearrr,nikobearrr,30/Mar/20 07:53,17/Apr/20 02:16,13/Jul/23 08:07,02/Apr/20 07:10,1.10.0,1.9.2,,,,1.10.1,1.9.3,,,Connectors / ORC,Table SQL / API,,,,0,pull-request-available,,,,"We have a batch job which we currently have on a flink cluster running 1.8.2
 The job runs fine. We wanted to upgrade to flink 1.10, but that yielded errors, so we started downgrading until we found that the issue is in flink 1.9.2

The job on 1.9.2 fails with:
{code:java}
Caused by: org.apache.flink.table.api.TableException: Failed to push filter into table source! table source with pushdown capability must override and change explainSource() API to explain the pushdown applied!{code}
Which is not happening on flink 1.8.2. You can check the logs for the exactly same job, just running on different cluster versions: [^flink-1.8.2.txt] [^flink-1.9.2.txt]

 

I tried to narrow it down and it seems that this exception has been added in FLINK-12399 and there was a small discussion regarding the exception: [https://github.com/apache/flink/pull/8468#discussion_r329876088]

Our code looks something like this:

 
{code:java}
String tempTableName = ""tempTable"";
String sql = SqlBuilder.buildSql(tempTableName);
BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);
OrcTableSource orcTableSource = OrcTableSource.builder()
 .path(hdfsFolder, true)
 .forOrcSchema(ORC.getSchema())
 .withConfiguration(config)
 .build();
tableEnv.registerTableSource(tempTableName, orcTableSource);
Table tempTable = tableEnv.sqlQuery(sql);
return tableEnv.toDataSet(tempTable, Row.class); 
{code}
Where the sql build is nothing more than
{code:java}
SELECT * FROM table WHERE id IN (1,2,3) AND mid IN(4,5,6){code}
 ","flink 1.8.2

flink 1.9.2",aljoscha,jark,leonard,lzljs3620320,nikobearrr,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #11594: [FLINK-16860][orc] Distinguish empty filters from no pushed down in expainSource
URL: https://github.com/apache/flink/pull/11594
 
 
   
   ## What is the purpose of the change
   
   After FLINK-12399. there are bugs in OrcTableSource.
   
   The case is there are some filters to be pushed down to source, but source can not consume them. We need distinguish empty filters from no pushed down in expainSource.
   
   ## Brief change log
   
   Return ""null"" if no filter pushed down, return ""true"" there is filter pushed down.
   
   ## Verifying this change
   
   `OrcTableSourceTest.testUnsupportedPredOnly`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 09:25;githubbot;600","JingsongLi commented on pull request #11596: [FLINK-16860][orc] Distinguish empty filters from no pushed down in expainSource
URL: https://github.com/apache/flink/pull/11596
 
 
   ## What is the purpose of the change
   
   After FLINK-12399. there are bugs in OrcTableSource.
   
   The case is there are some filters to be pushed down to source, but source can not consume them. We need distinguish empty filters from no pushed down in expainSource.
   
   ## Brief change log
   
   Return ""null"" if no filter pushed down, return ""true"" there is filter pushed down.
   
   ## Verifying this change
   
   `OrcTableSourceTest.testUnsupportedPredOnly`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 09:33;githubbot;600","JingsongLi commented on pull request #11597: [FLINK-16860][orc] Distinguish empty filters from no pushed down in expainSource
URL: https://github.com/apache/flink/pull/11597
 
 
   ## What is the purpose of the change
   
   After FLINK-12399. there are bugs in OrcTableSource.
   
   The case is there are some filters to be pushed down to source, but source can not consume them. We need distinguish empty filters from no pushed down in expainSource.
   
   ## Brief change log
   
   Return ""null"" if no filter pushed down, return ""true"" there is filter pushed down.
   
   ## Verifying this change
   
   `OrcTableSourceTest.testUnsupportedPredOnly`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 09:33;githubbot;600","JingsongLi commented on pull request #11594: [FLINK-16860][orc] Distinguish empty filters from no pushed down in expainSource
URL: https://github.com/apache/flink/pull/11594
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 07:09;githubbot;600","JingsongLi commented on pull request #11596: [FLINK-16860][orc] Distinguish empty filters from no pushed down in expainSource
URL: https://github.com/apache/flink/pull/11596
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 07:09;githubbot;600","JingsongLi commented on pull request #11597: [FLINK-16860][orc] Distinguish empty filters from no pushed down in expainSource
URL: https://github.com/apache/flink/pull/11597
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 07:09;githubbot;600",,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,FLINK-12399,,,,,,,,,,,,,,,,,"30/Mar/20 07:52;nikobearrr;flink-1.8.2.txt;https://issues.apache.org/jira/secure/attachment/12998191/flink-1.8.2.txt","30/Mar/20 07:52;nikobearrr;flink-1.9.2.txt;https://issues.apache.org/jira/secure/attachment/12998192/flink-1.9.2.txt",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 07:10:53 UTC 2020,,,,,,,,,,"0|z0d28o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/20 08:36;aljoscha;[~twalthr] or [~jark], could you please take a look at this?;;;","01/Apr/20 08:45;lzljs3620320;Ah... Yes, after FLINK-12399. there are bugs in {{OrcTableSource}}.

The case is there are some filters to be pushed down to source, but source can not consume them. We need distinguish empty filters from no pushed down in {{expainSource}}.;;;","01/Apr/20 09:34;lzljs3620320;Hi [~nikobearrr] you can try [https://github.com/apache/flink/pull/11597];;;","01/Apr/20 10:49;nikobearrr;Hi [~lzljs3620320] is there a docker build I can run it from? ;;;","01/Apr/20 12:07;lzljs3620320;[~nikobearrr] currently no...;;;","02/Apr/20 07:10;lzljs3620320;master: c8a23c74e618b752bbdc58dca62d997ddd303d40

release-1.10: 9d21a4a5539e6fe0b253def255d6641335da18df

release-1.9: 78e2c0de6ef21e497e82de4a180759a960d11eab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
taskmanager.sh not working on Mac,FLINK-16856,13294894,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,wangyang0918,wangyang0918,30/Mar/20 05:23,08/Apr/20 09:03,13/Jul/23 08:07,08/Apr/20 09:03,1.11.0,,,,,1.11.0,,,,Deployment / Scripts,,,,,0,pull-request-available,,,,"When we start the standalone cluster, the following exception will show up. The root cause is an invalid argument of head in {{taskmanager.sh}}. [https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/bin/taskmanager.sh#L52]

 

cc [~chesnay]

 
{code:java}
wangyang-pc:build-target danrtsey.wy$ ./bin/start-cluster.sh
Starting cluster.
Starting standalonesession daemon on host wangyang-pc.
head: illegal line count -- -2
Starting taskexecutor daemon on host wangyang-pc.
{code}",,guoyangze,wangyang0918,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11563: [FLINK-16856][scripts] Manually calculate lines of logging output
URL: https://github.com/apache/flink/pull/11563
 
 
   'head' with a negative '-n' argument isn't standardized, so we now calculate the number of lines to treat as logging output manually.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Mar/20 08:51;githubbot;600","zentol commented on pull request #11563: [FLINK-16856][scripts] Manually calculate lines of logging output
URL: https://github.com/apache/flink/pull/11563
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Apr/20 09:02;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,FLINK-16798,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 09:03:24 UTC 2020,,,,,,,,,,"0|z0d22w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/20 07:32;chesnay;Can you tell me what system you are running on? This works fine for me and we haven't seen issues on CI.

https://linux.die.net/man/1/head

{code}
-n, --lines=[-]K
    print the first K lines instead of the first 10; with the leading '-', print all but the last K lines of each file 
{code};;;","30/Mar/20 07:52;wangyang0918;Aha, i get your point. However, i am afraid it could not work on Mac.;;;","08/Apr/20 09:03;chesnay;master: 86e9ec070ba28be6f71624a2039214177f7e135d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct dependency versions in the NOTICE file of module statefun-ridesharing-example-simulator,FLINK-16854,13294877,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hequn8128,hequn8128,hequn8128,30/Mar/20 01:49,30/Mar/20 03:44,13/Jul/23 08:07,30/Mar/20 03:44,,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"There are some dependencies with a wrong version in the NOTICE file:
{code}
com.google.code.findbugs:jsr305:3.0.2:compile (Remove compile)
org.hibernate.validator:hibernate-validator:6.0.17 (Version should be 6.0.17.Final)
org.jboss.logging:jboss-logging:3.3.2  (Version should be 3.3.2.Final)
{code}",,hequn8128,tzulitai,,,,,,,,,,,,,,,,,,,,"hequn8128 commented on pull request #81: [FLINK-16854][legal] Correct dependency versions in the NOTICE file of module statefun-ridesharing-example-simulator
URL: https://github.com/apache/flink-statefun/pull/81
 
 
   There are some dependencies with a wrong version in the NOTICE file:
   ```
   com.google.code.findbugs:jsr305:3.0.2:compile (Remove compile)
   org.hibernate.validator:hibernate-validator:6.0.17 (Version should be 6.0.17.Final)
   org.jboss.logging:jboss-logging:3.3.2  (Version should be 3.3.2.Final)
   ```
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Mar/20 02:04;githubbot;600","tzulitai commented on pull request #81: [FLINK-16854][legal] Correct dependency versions in the NOTICE file of module statefun-ridesharing-example-simulator
URL: https://github.com/apache/flink-statefun/pull/81
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Mar/20 02:50;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 03:44:26 UTC 2020,,,,,,,,,,"0|z0d1z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/20 03:44;tzulitai;Fixed.

master: 6a14369c205c45f97fbb61ae926052cc24a2e1f4
release-2.0: 110f69051c106edb27a530765fe3e8ebaba7e435;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the version of protobuf-java from 3.8.0 to 3.7.1 in the NOTICE file of module statefun-flink-distribution,FLINK-16853,13294876,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hequn8128,hequn8128,hequn8128,30/Mar/20 01:46,30/Mar/20 03:44,13/Jul/23 08:07,30/Mar/20 03:44,,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"In the NOTICE file, the version of protobuf-java should be 3.7.1",,hequn8128,tzulitai,,,,,,,,,,,,,,,,,,,,"hequn8128 commented on pull request #80: [FLINK-16853][legal] Update the version of protobuf-java from 3.8.0 to 3.7.1 in the NOTICE file of module statefun-flink-distribution
URL: https://github.com/apache/flink-statefun/pull/80
 
 
   In the NOTICE file, the version of protobuf-java should be 3.7.1.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Mar/20 01:59;githubbot;600","tzulitai commented on pull request #80: [FLINK-16853][legal] Update the version of protobuf-java from 3.8.0 to 3.7.1 in the NOTICE file of module statefun-flink-distribution
URL: https://github.com/apache/flink-statefun/pull/80
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Mar/20 02:50;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 03:44:54 UTC 2020,,,,,,,,,,"0|z0d1yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/20 03:44;tzulitai;Fixed.

master: b103bdc02b00a875f06d21399dcbb2808b62757c
release-2.0: c4cc44aec267ca56ee55ef7607c2415df8bb4826;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python SDK distribution is missing LICENSE and NOTICE files,FLINK-16843,13294704,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,28/Mar/20 10:16,28/Mar/20 17:19,13/Jul/23 08:07,28/Mar/20 17:19,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,,,,,"The Python SDK distributions for Stateful Functions do not bundle any LICENSE or NOTICE files.

This should be fixed, as these are required to be included in all ASF-released distributions.",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 28 17:19:45 UTC 2020,,,,,,,,,,"0|z0d0wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/20 17:19;tzulitai;Fixed.

master - a06db812d3f4c153886df97e31d96d9534b1ca67
release-2.0: c0934bbe53723d4e955d32bfc7da73f92681cffd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ridesharing example simulator built artifact is missing NOTICE / LICENSE for bundled dependencies,FLINK-16842,13294653,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,28/Mar/20 08:53,28/Mar/20 17:20,13/Jul/23 08:07,28/Mar/20 17:20,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,,,,,"The {{statefun-ridesharing-example-simulator}} artifact bundles {{spring-boot}} as a dependency, which in turn pulls in some other dependencies that are non-ASLv2.

We should add NOTICE / LICENSE files to the built artifact for those.",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 28 17:20:18 UTC 2020,,,,,,,,,,"0|z0d0lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/20 17:20;tzulitai;Fixed.

master - 6f6476cb99f9e70925c2b483c69eaca18fb52cc0
release-2.0 - 2b77e200df3595101deba6a6f7c2d95f02f3735f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stateful Function artifacts jars should not bundle proto sources,FLINK-16841,13294650,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,28/Mar/20 08:44,28/Mar/20 17:19,13/Jul/23 08:07,28/Mar/20 17:19,statefun-2.0.0,,,,,statefun-2.0.0,,,,Build System,Stateful Functions,,,,0,,,,,"These protobuf definition files are being bundled in built artifacts:
{code}
google/protobuf/any.proto
google/protobuf/api.proto
google/protobuf/descriptor.proto
google/protobuf/duration.proto
google/protobuf/empty.proto
google/protobuf/field_mask.proto
google/protobuf/source_context.proto
google/protobuf/struct.proto
google/protobuf/timestamp.proto
google/protobuf/type.proto
google/protobuf/wrappers.proto
{code}

This is caused by the {{addProtoSources}} configuration of the {{protoc-jar-maven-plugin}}.

We should remove those, because:
- Bundling those will require licensing acknowledgement to Protobuf in our artifacts.
- Those definition files are not used directly by Stateful Functions at all.",,lsy,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 28 17:19:16 UTC 2020,,,,,,,,,,"0|z0d0ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/20 17:19;tzulitai;Fixed -

master: e72eb732ba6403ab915ab73570a9d96431254fa9
release-2.0: 621a6eefbd3a06321521cea9b456d6be39a26cbd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stateful Functions Quickstart archetype Dockerfile should reference a specific version tag,FLINK-16838,13294595,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tzulitai,tzulitai,tzulitai,28/Mar/20 03:48,28/Mar/20 17:21,13/Jul/23 08:07,28/Mar/20 17:21,,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"Currently, the quickstart archetype provides a skeleton Dockerfile that always builds on top of the latest image:
{code}
FROM statefun
{code}

While it happens to work for the first ever release since the {{latest}} tag will (coincidentally) point to the correct version,
once we have multiple releases this will no longer be correct.",,tzulitai,,,,,,,,,,,,,,,,,,,,,"tzulitai commented on pull request #78: [FLINK-16838] Change base image name and apply versioning
URL: https://github.com/apache/flink-statefun/pull/78
 
 
   This PR has the following end-goal in mind:
   - Examples / E2E tests / quickstart archetype built or run from snapshot versions (i.e. from `master` or `release-2.0` branches) should always run against a locally-built StateFun base image, built from the source of said snapshot version.
   - Released examples / E2E tests / quickstart archetypes, should not require users to locally build a StateFun image, assuming that we will have official images published to Docker Hub.
   
   This PR does a few things to accomplish that:
   
   1. Let the image build script `tools/docker/build-distribution.sh` build images tagged with the current source version. i.e.,
   `2.1-SNAPSHOT` when built from the snapshot `master` branch, `2.0-SNAPSHOT` when built from the snapshot `release-2.0` branch, or
   simply `2.0.0` if built from a officially released source distribution
   
   2. Update Dockerfiles of all examples / E2E tests / quickstart archetype to use a specific version tag. This has the effect that, if those were built from a released distribution, the image may be pulled directly from Docker Hub (when we publish the images after the release). Otherwise, if those were built from a snapshot version, they would use a locally-built snapshot image.
   
   On the side, this PR also uses the opportunity to rename the image name from `statefun` to `flink-statefun`, with the following reasoning:
   - It would be more consistent with the naming convention of other distributions, like the Python SDK
   - Lets the image name to show association with Flink
   
   ---
   
   ## Verifying
   
   I verified the changes by:
   - Running all the examples that used images built from StateFun base image, and all ran without problems
   - Run all end-to-end tests, using `mvn clean verify -Prun-e2e-tests`
   - Created new project from quickstart archetype - the generated skeleton project has correct Dockerfiles
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Mar/20 07:46;githubbot;600","tzulitai commented on pull request #78: [FLINK-16838] Change base image name and apply versioning
URL: https://github.com/apache/flink-statefun/pull/78
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Mar/20 15:15;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 28 17:21:02 UTC 2020,,,,,,,,,,"0|z0d08g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/20 17:21;tzulitai;Fixed.

master via 4750c144bbe6bd76a075c0d69c402785286eedb5
release-2.0 via 1a98294f6ba1713b11c43ac3eb534974439ae56d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Losing leadership does not clear rpc connection in JobManagerLeaderListener,FLINK-16836,13294511,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,27/Mar/20 16:44,02/Apr/20 09:01,13/Jul/23 08:07,02/Apr/20 09:01,1.11.0,,,,,1.10.1,1.11.0,1.9.3,,Runtime / Coordination,,,,,0,pull-request-available,,,,"When losing the leadership the {{JobManagerLeaderListener}} closes the current {{rpcConnection}} but does not clear the field. This can lead to a failure of {{JobManagerLeaderListener#reconnect}} if this method is called after the {{JobMaster}} has lost its leadership.

I propose to clear the field so that {{RegisteredRpcConnection#tryReconnect}} won't be called on a closed rpc connection.",,liyu,trohrmann,,,,,,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #11552: [FLINK-16836] Clear rpcConnection field in JobManagerLeaderListener when target loses leadership
URL: https://github.com/apache/flink/pull/11552
 
 
   ## What is the purpose of the change
   
   Clearing the rpcConnection field in the JobManagerLeaderListener when target loses leadership
   prevents that we try to reconnect to the target in case JobLeaderService.reconnect(JobID) is
   called.
   
   ## Verifying this change
   
   Added `JobLeaderServiceTest#doesNotReconnectAfterTargetLostLeadership`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/20 17:12;githubbot;600","tillrohrmann commented on pull request #11552: [FLINK-16836] Clear rpcConnection field in JobManagerLeaderListener when target loses leadership
URL: https://github.com/apache/flink/pull/11552
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 10:50;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14316,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 09:01:15 UTC 2020,,,,,,,,,,"0|z0czps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/20 10:50;trohrmann;Fixed via 7ff9ce16607fe345fa41d0c92eb956df4b31e067;;;","02/Apr/20 09:01;trohrmann;Fixed via

1.10.1: dede2ae753806214aac0c3552a7f2909ea881a51
1.9.3: fa2d98f2d7a8d31d26a82e0017204001bef56d61;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Examples cannot be run from IDE,FLINK-16834,13294505,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,27/Mar/20 16:20,02/Apr/20 07:37,13/Jul/23 08:07,01/Apr/20 10:06,1.11.0,,,,,1.11.0,,,,Client / Job Submission,Examples,,,,0,pull-request-available,,,,"Due to removing the dependency {{flink-clients}} from {{flink-streaming-java}}, the examples can no longer be executed from the IDE. The problem is that the {{flink-clients}} dependency is missing.

In order to solve this problem, we need to add the {{flink-clients}} dependency to all modules which need it and previously obtained it transitively from {{flink-streaming-java}}.",,aljoscha,alpinegizmo,trohrmann,,,,,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #11551: [FLINK-16834] Add flink-clients dependency to all example modules 
URL: https://github.com/apache/flink/pull/11551
 
 
   ## What is the purpose of the change
   
   This commit adds a flink-clients dependency to all example modules. That
   way they become executable from the IDE again.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/20 16:37;githubbot;600","tillrohrmann commented on pull request #11551: [FLINK-16834] Add flink-clients dependency to all example modules 
URL: https://github.com/apache/flink/pull/11551
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 10:05;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-16915,,,,,,,,FLINK-15090,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 10:06:04 UTC 2020,,,,,,,,,,"0|z0czog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/20 12:52;trohrmann;Escalating this issue since also quickstarts and potentially also the walkthroughs might be affected.;;;","31/Mar/20 13:06;alpinegizmo;[~trohrmann] I'm wondering if https://stackoverflow.com/questions/60938250/why-does-running-the-example-flink-app-throw-this-error is related to this.;;;","01/Apr/20 09:57;trohrmann;Thanks for this pointer [~alpinegizmo]. I think the problem of the SO question is not caused by this problem but because we set the Flink dependencies to {{provided}}. There is an easy solution to this problem by activating the {{add-dependencies-for-IDEA}} profile.;;;","01/Apr/20 10:06;trohrmann;Fixed via

da734c371b2b3ca62b6c091e8bda5986e13dc33d
3ce6ecd0e776f16e98beeda5aed0ea52de223645
da46ab619ecf663ce0d5e0e28d127cd53a41bc00;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamExecTemporalSort should require a distribution trait in StreamExecTemporalSortRule,FLINK-16827,13294460,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,libenchao,robinjun,robinjun,27/Mar/20 13:24,24/Jul/20 04:42,13/Jul/23 08:07,13/Jul/20 15:21,1.9.1,,,,,1.11.2,1.12.0,,,Table SQL / Planner,Table SQL / Runtime,,,,0,pull-request-available,,,,"flink reads kafka data and sorts by time field. In the case of multiple concurrency, it throws the following null pointer exception. One concurrent processing is normal.

!image-2020-03-27-21-22-21-122.png!

 

!image-2020-03-27-21-22-44-191.png!

 

 

 ","flink on yarn

!image-2020-03-27-21-23-13-648.png!",aljoscha,dian.fu,godfreyhe,jark,leonard,libenchao,robinjun,,,,,,,,,,,,,,,"libenchao commented on pull request #11643: [FLINK-16827][table-planner-blink] StreamExecTemporalSort should requ…
URL: https://github.com/apache/flink/pull/11643
 
 
   …ire a distribution trait in StreamExecTemporalSortRule.
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   This pr adds FlinkRelDistribution to StreamExecTemporalSortRule.
   
   ## Brief change log
   
   This pr adds FlinkRelDistribution to StreamExecTemporalSortRule.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   - TemporalSortITCase.testEventTimeOrderByWithParallelInput
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Apr/20 08:26;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/20 13:22;robinjun;image-2020-03-27-21-22-21-122.png;https://issues.apache.org/jira/secure/attachment/12998002/image-2020-03-27-21-22-21-122.png","27/Mar/20 13:22;robinjun;image-2020-03-27-21-22-44-191.png;https://issues.apache.org/jira/secure/attachment/12998001/image-2020-03-27-21-22-44-191.png","27/Mar/20 13:23;robinjun;image-2020-03-27-21-23-13-648.png;https://issues.apache.org/jira/secure/attachment/12998000/image-2020-03-27-21-23-13-648.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 17 02:30:34 UTC 2020,,,,,,,,,,"0|z0czeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/20 03:22;libenchao;[~robinjun] Yes, you are right, we've spotted this bug too. It's because in `StreamExecTemporalSortRule`, we didn't add `FlinkRelDistribution`.;;;","01/Apr/20 08:32;aljoscha;[~jark] Fyi, the component was wrongly assigned initially.;;;","01/Apr/20 10:40;jark;[~libenchao], do you want to fix this?;;;","02/Apr/20 01:40;libenchao;[~jark] Sure, I'd like to fix this.;;;","02/Apr/20 04:01;jark;Assigned to you [~libenchao]. I also updated the title. ;;;","13/Jul/20 15:21;libenchao;Fixed via

66353f27c4c6481443d1f04a8f23e7f98dd7beda (1.12.0)

076a474cee465d3fc3267e2d2367bfdb59fce1d4 (1.11.2);;;","17/Jul/20 01:57;docete;[~jark] [~libenchao] I think this bugfix should port to 1.11, or the TemporalSort in 1.11 is not available for production environments.;;;","17/Jul/20 02:30;libenchao;[~docete] Thanks for the reminder, I'll raise another pr against 1.11;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PrometheusReporterEndToEndITCase should rely on path returned by DownloadCache,FLINK-16825,13294431,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,chesnay,chesnay,27/Mar/20 11:11,27/Mar/20 11:16,13/Jul/23 08:07,27/Mar/20 11:16,1.10.0,,,,,1.10.1,1.11.0,,,Runtime / Metrics,Tests,,,,0,,,,,"The {{PrometheusReporterEndToEndITCas}} uses the {{DownloadCache#getOrDownload}} to download a file, but ignores the returned {{Path}} and simply assumes the file name.
This assumption can fail if there was an error during the download, where the cache appends an attempt index.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 11:16:52 UTC 2020,,,,,,,,,,"0|z0cz80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/20 11:16;chesnay;master: 42dd176e170d2a4343d6e40cc4f15d0fb379d43c 
1.10: 451b7acabb4f345ee46d0b5cf9d0feef2b1ca1b1 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The functioin TIMESTAMPDIFF doesn't perform expected result,FLINK-16823,13294360,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,docete,adam_deng,adam_deng,27/Mar/20 05:56,18/Apr/20 13:03,13/Jul/23 08:07,18/Apr/20 13:03,1.10.0,1.9.1,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"For example,

In mysql bellow sql get result 6, but in flink the output is 5

SELECT timestampdiff (MONTH, TIMESTAMP '2019-09-01 00:00:00',TIMESTAMP '2020-03-01 00:00:00' )

 

!image-2020-03-27-13-50-51-955.png!

 

 ",,adam_deng,jark,lsy,,,,,,,,,,,,,,,,,,,"docete commented on pull request #11573: [FLINK-16823][table-planner-blink] Fix TIMESTAMPDIFF/TIMESTAMPADD yie…
URL: https://github.com/apache/flink/pull/11573
 
 
   …ld incorrect results
   
   ## What is the purpose of the change
   
   Because CALCITE-3881, TIMESTAMPDIFF/TIMESTAMPADD yields incorrect results. This PR is a temporary solution to fix this before CALCITE-3881 is fixed.
   
   ## Brief change log
   - dfd2474 implements addMonths/subtractMonths in flink side.
   
   ## Verifying this change
   This change added tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (**yes** / no)
     - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 07:16;githubbot;600","KurtYoung commented on pull request #11573: [FLINK-16823][table-planner-blink] Fix TIMESTAMPDIFF/TIMESTAMPADD yie…
URL: https://github.com/apache/flink/pull/11573
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Apr/20 13:03;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-3881,,,,,,,,,"27/Mar/20 05:50;adam_deng;image-2020-03-27-13-50-51-955.png;https://issues.apache.org/jira/secure/attachment/12997957/image-2020-03-27-13-50-51-955.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 13:03:49 UTC 2020,,,,,,,,,,"0|z0cys8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/20 07:07;ykt836;Yes, this is a bug. [~docete] could you take a look at this?;;;","27/Mar/20 08:02;docete;OK, i will take a look at it.;;;","27/Mar/20 08:46;docete;Flink use Calcite's SqlFunctions#subtractMonths to find the number of month between two dates/timestamps.

There may be a bug in the algorithm of SqlFunctions#subtractMonths. 

Will dig deeper to find the root cause and fix.

 ;;;","27/Mar/20 12:10;docete;The root cause is CALCITE-3881, will fix on calcite side.;;;","27/Mar/20 12:23;docete;[~danny0405] What the plan of the upgrading of Calcite? Should we also upgrade calcite-avatica?;;;","31/Mar/20 07:18;docete;After talk with [~danny0405], we decide to introduce addMonths/subtractMonths as a temporary solution on Flink side to fix this corner case before CALCITE-3881 is fixed.;;;","18/Apr/20 13:03;ykt836;master: 591527798bf0d7f2a3884934b6260af9b84f3bf2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The config set by SET command does not work,FLINK-16822,13294358,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,godfreyhe,godfreyhe,godfreyhe,27/Mar/20 05:31,20/Apr/20 16:16,13/Jul/23 08:07,16/Apr/20 05:58,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / Client,,,,,0,pull-request-available,,,,"Users can add or change the properties for execution behavior through SET command in SQL client CLI, e.g. {{SET execution.parallelism=10}}, {{SET table.optimizer.join-reorder-enabled=true}}. But the {{table.xx}} config can't change the TableEnvironment behavior, because the property set from CLI does not be set into TableEnvironment's table config.",,aljoscha,godfreyhe,jark,JaryZhen,,,,,,,,,,,,,,,,,,"godfreyhe commented on pull request #11544: [FLINK-16822] [sql-client] `table.xx` property set from CLI should also be set into SessionState's TableConfig
URL: https://github.com/apache/flink/pull/11544
 
 
   
   
   ## What is the purpose of the change
   
   *The config set by SET command does not work, the reason is `table.xx` property set from CLI does not be set into SessionState's TableConfig.*
   
   
   ## Brief change log
   
     - *Update table config of SessionState when calling setSessionProperty method*
   
   
   ## Verifying this change
   
   
   This change added tests and can be verified as follows:
   
     - *Added testSetSessionProperties in LocalExecutorITCase to verify the bug*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/20 05:43;githubbot;600","wuchong commented on pull request #11544: [FLINK-16822] [sql-client] `table.xx` property set from CLI should also be set into TableEnvironment's TableConfig
URL: https://github.com/apache/flink/pull/11544
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/20 14:23;githubbot;600","godfreyhe commented on pull request #11763: [FLINK-16822] [sql-client] `table.xx` property set from CLI should also be set into TableEnvironment's TableConfig
URL: https://github.com/apache/flink/pull/11763
 
 
   
   ## What is the purpose of the change
   
   *The config set by SET command does not work, the reason is table.xx property set from CLI does not be set into TableEnvironment's TableConfig.*
   
   
   ## Brief change log
   
     - *move TableConfig out side of SessionState, and create TableConfig when initializing TableEnvironment*
   
   
   ## Verifying this change
   
   
   
   This change added tests and can be verified as follows:
   
     - *Added testSetSessionProperties in LocalExecutorITCase to verify the bug*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/20 02:27;githubbot;600","wuchong commented on pull request #11763: [FLINK-16822] [sql-client] `table.xx` property set from CLI should also be set into TableEnvironment's TableConfig
URL: https://github.com/apache/flink/pull/11763
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/20 05:57;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 14:25:10 UTC 2020,,,,,,,,,,"0|z0cyrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/20 14:25;jark;Fixed in
 - master (1.11.0): 297f0a2c3dfae4c763eba0a1fed49f1570f37f45
 - 1.10.1: 08d941df9ecdd645bc37697449509b6a456c2161;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Run Kubernetes test failed with invalid named ""minikube""",FLINK-16821,13294350,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,zjwang,zjwang,27/Mar/20 04:21,11/Jun/20 03:14,13/Jul/23 08:07,11/Jun/20 03:14,1.10.0,1.11.0,,,,1.10.1,1.11.0,,,Deployment / Kubernetes,Tests,,,,0,pull-request-available,test-stability,,,"This is the test run [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6702&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]

Log output
{code:java}
2020-03-27T00:07:38.9666021Z Running 'Run Kubernetes test'
2020-03-27T00:07:38.9666656Z ==============================================================================
2020-03-27T00:07:38.9677101Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-38967103614
2020-03-27T00:07:41.7529865Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-03-27T00:07:41.7721475Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-03-27T00:07:41.8208394Z Docker version 19.03.8, build afacb8b7f0
2020-03-27T00:07:42.4793914Z docker-compose version 1.25.4, build 8d51620a
2020-03-27T00:07:42.5359301Z Installing minikube ...
2020-03-27T00:07:42.5494076Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-03-27T00:07:42.5494729Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-03-27T00:07:42.5498136Z 
2020-03-27T00:07:42.6214887Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-03-27T00:07:43.3467750Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-03-27T00:07:43.3469636Z 100 52.0M  100 52.0M    0     0  65.2M      0 --:--:-- --:--:-- --:--:-- 65.2M
2020-03-27T00:07:43.4262625Z * There is no local cluster named ""minikube""
2020-03-27T00:07:43.4264438Z   - To fix this, run: minikube start
2020-03-27T00:07:43.4282404Z Starting minikube ...
2020-03-27T00:07:43.7749694Z * minikube v1.9.0 on Ubuntu 16.04
2020-03-27T00:07:43.7761742Z * Using the none driver based on user configuration
2020-03-27T00:07:43.7762229Z X The none driver requires conntrack to be installed for kubernetes version 1.18.0
2020-03-27T00:07:43.8202161Z * There is no local cluster named ""minikube""
2020-03-27T00:07:43.8203353Z   - To fix this, run: minikube start
2020-03-27T00:07:43.8568899Z * There is no local cluster named ""minikube""
2020-03-27T00:07:43.8570685Z   - To fix this, run: minikube start
2020-03-27T00:07:43.8583793Z Command: start_kubernetes_if_not_running failed. Retrying...
2020-03-27T00:07:48.9017252Z * There is no local cluster named ""minikube""
2020-03-27T00:07:48.9019347Z   - To fix this, run: minikube start
2020-03-27T00:07:48.9031515Z Starting minikube ...
2020-03-27T00:07:49.0612601Z * minikube v1.9.0 on Ubuntu 16.04
2020-03-27T00:07:49.0616688Z * Using the none driver based on user configuration
2020-03-27T00:07:49.0620173Z X The none driver requires conntrack to be installed for kubernetes version 1.18.0
2020-03-27T00:07:49.1040676Z * There is no local cluster named ""minikube""
2020-03-27T00:07:49.1042353Z   - To fix this, run: minikube start
2020-03-27T00:07:49.1453522Z * There is no local cluster named ""minikube""
2020-03-27T00:07:49.1454594Z   - To fix this, run: minikube start
2020-03-27T00:07:49.1468436Z Command: start_kubernetes_if_not_running failed. Retrying...
2020-03-27T00:07:54.1907713Z * There is no local cluster named ""minikube""
2020-03-27T00:07:54.1909876Z   - To fix this, run: minikube start
2020-03-27T00:07:54.1921479Z Starting minikube ...
2020-03-27T00:07:54.3388738Z * minikube v1.9.0 on Ubuntu 16.04
2020-03-27T00:07:54.3395499Z * Using the none driver based on user configuration
2020-03-27T00:07:54.3396443Z X The none driver requires conntrack to be installed for kubernetes version 1.18.0
2020-03-27T00:07:54.3824399Z * There is no local cluster named ""minikube""
2020-03-27T00:07:54.3837652Z   - To fix this, run: minikube start
2020-03-27T00:07:54.4203902Z * There is no local cluster named ""minikube""
2020-03-27T00:07:54.4204895Z   - To fix this, run: minikube start
2020-03-27T00:07:54.4217866Z Command: start_kubernetes_if_not_running failed. Retrying...
2020-03-27T00:07:59.4235917Z Command: start_kubernetes_if_not_running failed 3 times.
2020-03-27T00:07:59.4236459Z Could not start minikube. Aborting...
2020-03-27T00:07:59.8439850Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2020-03-27T00:07:59.8939088Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2020-03-27T00:07:59.9515679Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2020-03-27T00:07:59.9528463Z Stopping minikube ...
2020-03-27T00:07:59.9921558Z * There is no local cluster named ""minikube""
2020-03-27T00:07:59.9922957Z   - To fix this, run: minikube start
2020-03-27T00:07:59.9943342Z Command: sudo minikube stop failed. Retrying...
2020-03-27T00:08:05.0475257Z * There is no local cluster named ""minikube""
2020-03-27T00:08:05.0476544Z   - To fix this, run: minikube start
2020-03-27T00:08:05.0498749Z Command: sudo minikube stop failed. Retrying...
2020-03-27T00:08:10.1843339Z * There is no local cluster named ""minikube""
2020-03-27T00:08:10.1846448Z   - To fix this, run: minikube start
2020-03-27T00:08:10.1890972Z Command: sudo minikube stop failed. Retrying...
2020-03-27T00:08:15.1900926Z Command: sudo minikube stop failed 3 times.
2020-03-27T00:08:15.1906577Z Could not stop minikube. Aborting...
2020-03-27T00:08:15.1907434Z [FAIL] Test script contains errors.
2020-03-27T00:08:15.1915373Z Checking for errors...
2020-03-27T00:08:15.2133082Z No errors in log files.
2020-03-27T00:08:15.2133514Z Checking for exceptions...
2020-03-27T00:08:15.2390795Z No exceptions in log files.
2020-03-27T00:08:15.2392029Z Checking for non-empty .out files...
2020-03-27T00:08:15.2412061Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directory
2020-03-27T00:08:15.2415311Z No non-empty .out files.
2020-03-27T00:08:15.2415821Z 
2020-03-27T00:08:15.2416806Z [FAIL] 'Run Kubernetes test' failed after 0 minutes and 34 seconds! Test exited with exit code 1
2020-03-27T00:08:15.2417355Z 
2020-03-27T00:08:15.2454057Z cp: cannot stat '/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*': No such file or directory
2020-03-27T00:08:15.2459692Z Published e2e logs into debug logs artifact:
2020-03-27T00:08:15.2489410Z COMPRESSING build artifacts.
2020-03-27T00:08:15.2519389Z tar: Removing leading `/' from member names
2020-03-27T00:08:15.2528098Z /home/vsts/work/1/s/flink-end-to-end-tests/artifacts/
2020-03-27T00:08:15.2529353Z /home/vsts/work/1/s/flink-end-to-end-tests/artifacts/e2e-flink-logs/
2020-03-27T00:08:15.5819526Z No taskexecutor daemon to stop on host fv-az678.
2020-03-27T00:08:15.8011570Z No standalonesession daemon to stop on host fv-az678.
2020-03-27T00:08:16.2280113Z 
2020-03-27T00:08:16.2409524Z ##[error]Bash exited with code '1'.
2020-03-27T00:08:16.2456126Z ##[section]Finishing: Run e2e tests{code}",,liyu,pnowojski,rmetzger,wangyang0918,zjwang,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11546: [FLINK-16821][e2e] Use constant minikube version instead of latest
URL: https://github.com/apache/flink/pull/11546
 
 
   
   ## What is the purpose of the change
   
   The Kubernetes e2e test suddenly started failing. 
   The cause was that there's a new minikube version, which has a changed behavior. 
   
   
   ## Brief change log
   
   - Use a fixed minikube version instead of latest
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/20 08:08;githubbot;600","rmetzger commented on pull request #11546: [FLINK-16821][e2e] Use constant minikube version instead of latest
URL: https://github.com/apache/flink/pull/11546
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/20 09:26;githubbot;600","rmetzger commented on pull request #11581: [FLINK-16821][e2e] Use constant minikube version instead of latest
URL: https://github.com/apache/flink/pull/11581
 
 
   This is a backport of https://github.com/apache/flink/pull/11546 to `release-1.10`.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 13:50;githubbot;600","rmetzger commented on pull request #11581: [FLINK-16821][e2e] Use constant minikube version instead of latest
URL: https://github.com/apache/flink/pull/11581
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 15:47;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 03:13:53 UTC 2020,,,,,,,,,,"0|z0cyq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/20 04:25;zjwang;Another instance [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6705&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5];;;","27/Mar/20 04:26;zjwang;Another instance [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6708&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5];;;","27/Mar/20 04:31;zjwang;Another instance [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6709&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5];;;","27/Mar/20 07:20;rmetzger;I will look into this!

The first commit that caused the problem is obviously from me: https://github.com/flink-ci/flink-mirror/commit/6a0805141088ba5dd2773aa138af7bc43bbfa432;;;","27/Mar/20 07:32;rmetzger;I believe the minikube version changed from minikube v1.8.2 to v1.9.0 (we are just downloading the latest)

I will validate and open a PR.;;;","27/Mar/20 08:26;wangyang0918;I think [~rmetzger] is right. We need to use a fixed stable version instead latest. Since minikube may have some incompatible version upgrade.;;;","27/Mar/20 09:26;rmetzger;Resolved in b752bfbe0073bb446de702f902b90564f41953cb;;;","28/Mar/20 13:39;zjwang;Thanks for solving it [~rmetzger]!

I guess it is already needed for release-1.10?  Another instance found in release-1.10 : [https://travis-ci.org/github/apache/flink/builds/667815122?utm_medium=notification&utm_source=slack];;;","31/Mar/20 09:21;pnowojski;Yes, another instance for 1.10:
https://api.travis-ci.org/v3/job/668871738/log.txt

[~rmetzger] how difficult would it be to port it to 1.10?;;;","31/Mar/20 11:40;rmetzger;Very easy. I will open a PR today.;;;","31/Mar/20 15:35;liyu;Also noticed the failure in release-1.10 crone build and just approved the PR for release-1.10, and from the crone job history it also started to fail from 4 days ago. Thanks for the fix [~rmetzger]!;;;","31/Mar/20 15:48;rmetzger;Resolved in 1.10: 722c053e65d477f52cf422162a5f763e93cd0aa7;;;","10/Jun/20 16:05;zjwang;The same issue seems happen again in release-1.11 branch:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3157&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5];;;","10/Jun/20 16:55;rmetzger;I believe the error you've reported is tracked under FLINK-18239 (and I just merged a fix);;;","11/Jun/20 03:06;wangyang0918;[~zjwang] Do you still find some failure after FLINK-18239 merged? The root cause is azure pipeline suddenly provided a minikube with version v1.11.0 yesterday, which could not be used because conntrack is not installed correctly.;;;","11/Jun/20 03:13;zjwang;Thanks for the feedback. I have not found other failures after FLINK-18239 and I believe this issue should be fixed as well. Close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Got KryoException while using UDAF in flink1.9,FLINK-16819,13294347,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,dixingxing@yeah.net,dixingxing@yeah.net,27/Mar/20 04:01,18/Apr/20 15:32,13/Jul/23 08:07,18/Apr/20 14:20,1.9.1,,,,,1.9.2,,,,API / Type Serialization System,Table SQL / Planner,,,,0,,,,,"Recently,  we are trying to upgrade online *sql jobs* from flink1.7 to flink1.9 , most jobs works fine, but some jobs got  KryoExceptions. 

We found that UDAF will trigger this exception, btw ,we are using blink planner.

*Here is the full stack traces:*
 2020-03-27 11:46:55
 com.esotericsoftware.kryo.KryoException: java.lang.IndexOutOfBoundsException: Index: 104, Size: 2
 Serialization trace:
 seed (java.util.Random)
 gen (com.tdunning.math.stats.AVLTreeDigest)
     at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
     at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)
     at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679)
     at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
     at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)
     at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761)
     at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:346)
     at org.apache.flink.util.InstantiationUtil.deserializeFromByteArray(InstantiationUtil.java:536)
     at org.apache.flink.table.dataformat.BinaryGeneric.getJavaObjectFromBinaryGeneric(BinaryGeneric.java:86)
     at org.apache.flink.table.dataformat.DataFormatConverters$GenericConverter.toExternalImpl(DataFormatConverters.java:628)
     at org.apache.flink.table.dataformat.DataFormatConverters$GenericConverter.toExternalImpl(DataFormatConverters.java:633)
     at org.apache.flink.table.dataformat.DataFormatConverters$DataFormatConverter.toExternal(DataFormatConverters.java:320)
     at org.apache.flink.table.dataformat.DataFormatConverters$PojoConverter.toExternalImpl(DataFormatConverters.java:1293)
     at org.apache.flink.table.dataformat.DataFormatConverters$PojoConverter.toExternalImpl(DataFormatConverters.java:1257)
     at org.apache.flink.table.dataformat.DataFormatConverters$DataFormatConverter.toExternal(DataFormatConverters.java:302)
     at GroupAggsHandler$71.setAccumulators(Unknown Source)
     at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:151)
     at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
     at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:85)
     at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processElement(StreamOneInputProcessor.java:164)
     at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:143)
     at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:279)
     at org.apache.flink.streaming.runtime.tasks.StreamTask.run(StreamTask.java:301)
     at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:406)
     at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)
     at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)
     at java.lang.Thread.run(Thread.java:748)
 Caused by: java.lang.IndexOutOfBoundsException: Index: 104, Size: 2
     at java.util.ArrayList.rangeCheck(ArrayList.java:657)
     at java.util.ArrayList.get(ArrayList.java:433)
     at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:42)
     at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:805)
     at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:677)
     at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
     ... 26 more","Flink1.9.1

Apache hadoop 2.7.2",dixingxing@yeah.net,jark,libenchao,neighborhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 15:32:35 UTC 2020,,,,,,,,,,"0|z0cypc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/20 04:40;neighborhood;[~jark] hi Jark, this is the bug(or not) I mentioned  before, actually we found that there are several UDAFs encountering Kyro serialization exception everytime we try to migrate to Flink 1.9.1 from Flink 1.7.2. We also tried  to register our own specialized serializer for these UDAFs, but unforunately, it didn't work. we are look forward to the solution or tips to fix this kind of problem cuz our users cannot their jobs on Flink 1.9  due to this problem. ANY opinion is welcomed.

Tkx;;;","27/Mar/20 04:42;neighborhood;also I can make sure that it happens when the job tries to do checkpointing;;;","27/Mar/20 05:30;jark;Hi [~neighborhood], this should be caused by FLINK-13702 and is fixed in 1.9.2. Did you tried 1.9.2 or 1.10?;;;","18/Apr/20 14:17;dixingxing@yeah.net;Hi [~jark] , the problem has been resolved by upgrading to 1.9.2, thanks a lot.;;;","18/Apr/20 14:20;dixingxing@yeah.net;the problem resolved by upgrading to 1.9.2;;;","18/Apr/20 14:30;jark;Glad to hear that [~dixingxing@yeah.net]!;;;","18/Apr/20 15:04;neighborhood;[~jark]  These jobs which I mentioned aboved has been running correctly for a few day after upgrading to 1.9.2. Your advice did help us to fix this problem

Salute!;;;","18/Apr/20 15:32;jark;Glad to hear that too [~neighborhood]! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StringUtils.arrayToString() doesn't convert array of byte array correctly,FLINK-16817,13294325,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,phoenixjiangnan,phoenixjiangnan,phoenixjiangnan,26/Mar/20 23:51,07/Apr/20 20:59,13/Jul/23 08:07,07/Apr/20 20:59,,,,,,1.11.0,,,,API / Core,,,,,0,,,,,,,phoenixjiangnan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 07 20:57:57 UTC 2020,,,,,,,,,,"0|z0cykg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/20 20:57;phoenixjiangnan;master: 989bc02518d1be05d4f2260d9c4a67098df19063;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
planner doesn't parse timestamp and date array correctly,FLINK-16816,13294319,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,phoenixjiangnan,phoenixjiangnan,26/Mar/20 22:49,03/Jun/20 06:55,13/Jul/23 08:07,03/Jun/20 06:54,,,,,,,,,,Table SQL / Planner,Table SQL / Runtime,,,,0,pull-request-available,,,,"planner doesn't parse timestamp and date array correctly.

 

Repro: 

In a input format (like JBDCInputFormat)'s \{{nextRecord(Row)}} API
 # when setting a timestamp datum as java.sql.Timestamp/Date, it works fine
 # when setting an array of timestamp datums as java.sql.Timestamp[]/Date[], it breaks and below is the strack trace

 
{code:java}
/Caused by: java.lang.ClassCastException: java.sql.Timestamp cannot be cast to java.time.LocalDateTime
	at org.apache.flink.table.dataformat.DataFormatConverters$LocalDateTimeConverter.toInternalImpl(DataFormatConverters.java:748)
	at org.apache.flink.table.dataformat.DataFormatConverters$ObjectArrayConverter.toBinaryArray(DataFormatConverters.java:1110)
	at org.apache.flink.table.dataformat.DataFormatConverters$ObjectArrayConverter.toInternalImpl(DataFormatConverters.java:1093)
	at org.apache.flink.table.dataformat.DataFormatConverters$ObjectArrayConverter.toInternalImpl(DataFormatConverters.java:1068)
	at org.apache.flink.table.dataformat.DataFormatConverters$DataFormatConverter.toInternal(DataFormatConverters.java:344)
	at org.apache.flink.table.dataformat.DataFormatConverters$RowConverter.toInternalImpl(DataFormatConverters.java:1377)
	at org.apache.flink.table.dataformat.DataFormatConverters$RowConverter.toInternalImpl(DataFormatConverters.java:1365)
	at org.apache.flink.table.dataformat.DataFormatConverters$DataFormatConverter.toInternal(DataFormatConverters.java:344)
	at SourceConversion$1.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:714)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:689)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:669)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:93)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:208)
{code}

seems that planner runtime handles java.sql.Timetamp in these two cases differently",,jark,leonard,libenchao,lsy,phoenixjiangnan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16820,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 06:54:46 UTC 2020,,,,,,,,,,"0|z0cyj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/20 09:45;leonard;This bug has gone away,  after we imported RowData, this bug fixed because we convert Timestamp/Date to TimestampData/Int as internal data structure rather than LocalDatetime/LocalDate. I open the PR only add unit test cases to cover it.;;;","29/May/20 12:41;jark;Thanks [~Leonard Xu].;;;","03/Jun/20 06:54;jark;This has already been fixed by FLINK-16820.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 JDBCInputFormat doesn't correctly map Short,FLINK-16813,13294297,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,phoenixjiangnan,phoenixjiangnan,phoenixjiangnan,26/Mar/20 20:32,16/Apr/20 02:57,13/Jul/23 08:07,16/Apr/20 02:57,1.10.0,,,,,1.11.0,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,"currently when JDBCInputFormat converts a JDBC result set row to Flink Row, it doesn't check the type returned from jdbc result set.

Short from jdbc result set actually returns an Integer

 ",,jark,phoenixjiangnan,,,,,,,,,,,,,,,,,,,,"bowenli86 commented on pull request #11538: [FLINK-16813][jdbc]  JDBCInputFormat doesn't correctly map Short
URL: https://github.com/apache/flink/pull/11538
 
 
   ## What is the purpose of the change
   
   commit 1: fix bug that JDBCInputFormat doesn't correctly map Short
   commit 2: add e2e tests for reading from postgres with JDBCTableSource and PostgresCatalog
   
   this PR depends on https://github.com/apache/flink/pull/11537
   
   ## Brief change log
   
   
   ## Verifying this change
   UT and IT added
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 21:46;githubbot;600","bowenli86 commented on pull request #11538: [FLINK-16813][jdbc]  JDBCInputFormat doesn't correctly map Short
URL: https://github.com/apache/flink/pull/11538
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Apr/20 22:04;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 02:57:13 UTC 2020,,,,,,,,,,"0|z0cye8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/20 02:57;phoenixjiangnan;master: 3bfeea1aed10586376835ed68dd0d31bdafe5d0f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"StreamingKafkaITCase fails with ""Could not instantiate instance using default factory.""",FLINK-16805,13294168,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,rmetzger,rmetzger,26/Mar/20 10:45,26/Mar/20 13:58,13/Jul/23 08:07,26/Mar/20 13:58,1.11.0,,,,,1.11.0,,,,Connectors / Kafka,Tests,,,,0,pull-request-available,test-stability,,,"CI: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6654&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=27d1d645-cbce-54e2-51c4-d8b45fe24607

{code}
2020-03-26T08:17:42.8881925Z [INFO]  T E S T S
2020-03-26T08:17:42.8882791Z [INFO] -------------------------------------------------------
2020-03-26T08:17:43.6840472Z [INFO] Running org.apache.flink.tests.util.kafka.StreamingKafkaITCase
2020-03-26T08:17:43.6933052Z [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 0.006 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.StreamingKafkaITCase
2020-03-26T08:17:43.6934567Z [ERROR] testKafka[0: kafka-version:0.10.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 0.004 s  <<< ERROR!
2020-03-26T08:17:43.6935170Z java.lang.RuntimeException: Could not instantiate instance using default factory.
2020-03-26T08:17:43.6935702Z 	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.<init>(StreamingKafkaITCase.java:72)
2020-03-26T08:17:43.6936024Z 
2020-03-26T08:17:43.6936691Z [ERROR] testKafka[1: kafka-version:0.11.0.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 0 s  <<< ERROR!
2020-03-26T08:17:43.6937288Z java.lang.RuntimeException: Could not instantiate instance using default factory.
2020-03-26T08:17:43.6937789Z 	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.<init>(StreamingKafkaITCase.java:72)
2020-03-26T08:17:43.6938113Z 
2020-03-26T08:17:43.6938890Z [ERROR] testKafka[2: kafka-version:2.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 0 s  <<< ERROR!
2020-03-26T08:17:43.6939646Z java.lang.RuntimeException: Could not instantiate instance using default factory.
2020-03-26T08:17:43.6940153Z 	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.<init>(StreamingKafkaITCase.java:72)
2020-03-26T08:17:43.6940485Z 
2020-03-26T08:17:44.0270048Z [INFO] 
2020-03-26T08:17:44.0270457Z [INFO] Results:
2020-03-26T08:17:44.0270649Z [INFO] 
2020-03-26T08:17:44.0270863Z [ERROR] Errors: 
2020-03-26T08:17:44.0271847Z [ERROR]   StreamingKafkaITCase.<init>:72 Â» Runtime Could not instantiate instance using ...
2020-03-26T08:17:44.0272651Z [ERROR]   StreamingKafkaITCase.<init>:72 Â» Runtime Could not instantiate instance using ...
2020-03-26T08:17:44.0273487Z [ERROR]   StreamingKafkaITCase.<init>:72 Â» Runtime Could not instantiate instance using ...
2020-03-26T08:17:44.0274218Z [INFO] 
2020-03-26T08:17:44.0274517Z [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0
{code}",,rmetzger,,,,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11532: [FLINK-16805][e2e] Remove pre-commit profile activation
URL: https://github.com/apache/flink/pull/11532
 
 
   ## What is the purpose of the change
   
   Remove the pre-commit profile from the global PROFILE variable.
   
   ## Verifying this change
   
   I verified the change here: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=243&view=logs&j=764762df-f65b-572b-3d5c-65518c777be4&t=764762df-f65b-572b-3d5c-65518c777be4
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 13:56;githubbot;600","rmetzger commented on pull request #11532: [FLINK-16805][e2e] Remove pre-commit profile activation
URL: https://github.com/apache/flink/pull/11532
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 13:57;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16778,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 13:58:32 UTC 2020,,,,,,,,,,"0|z0cxlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/20 10:50;rmetzger;Another case: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6657&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=27d1d645-cbce-54e2-51c4-d8b45fe24607;;;","26/Mar/20 10:50;rmetzger;The problem is reproducible on in IntelliJ on master.

{code}
java.lang.RuntimeException: Could not instantiate instance using default factory.

	at org.apache.flink.tests.util.util.FactoryUtils.lambda$loadAndInvokeFactory$0(FactoryUtils.java:61)
	at java.util.Optional.orElseThrow(Optional.java:290)
	at org.apache.flink.tests.util.util.FactoryUtils.loadAndInvokeFactory(FactoryUtils.java:61)
	at org.apache.flink.tests.util.flink.FlinkResource.get(FlinkResource.java:75)
	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.<init>(StreamingKafkaITCase.java:72)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
{code}

Upgrading this to a blocker issue.;;;","26/Mar/20 10:54;chesnay;You can probably reproduce it in IntelliJ since youu didn't set the {{distDir}} property there, so it is expected to fail.;;;","26/Mar/20 10:56;rmetzger;If that's the case, then we should improve the error reporting :);;;","26/Mar/20 11:01;chesnay;I'm aware of that, in the meantime you can enable logging to get this info.;;;","26/Mar/20 11:06;chesnay;Unfortunately we don't have the logs for the test execution, so I can't tell what the problem is.

We know that the distDir property is correctly set on azure, otherwise the other e2e tests wouldn't have run so far.;;;","26/Mar/20 12:31;rmetzger;I believe this commit introduced the issue: https://github.com/flink-ci/flink-mirror/commit/dee9156360122209c6d7b703bff5be882d1a6389;;;","26/Mar/20 12:36;chesnay;By setting {{-Dpre-commit-tests}} we have enabled this test in all test runs. If there is any that doesnt that distDir, that might explain it.;;;","26/Mar/20 12:39;rmetzger;We can either revert, or I try to fix the issue quickly. When passing {{-DdistDir}} and {{-DmoduleDir}} the test passes locally.
I guess something is wrong on AZP;;;","26/Mar/20 12:45;chesnay;You only need to revert the pre-commit thing (it is redundant anyway), the hadoop category is fine.;;;","26/Mar/20 12:48;chesnay;If we pass distDir in other places we're likely just running the test multiple times.;;;","26/Mar/20 12:51;rmetzger;I pushed a commit removing the pre commit profile to ci.;;;","26/Mar/20 13:58;rmetzger;Resolved in cd5e3f47c4877f2d271aa69989e664a45f2fab5a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logs from BashJavaUtils are not properly preserved and passed into TM logs.,FLINK-16798,13294112,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,xtsong,xtsong,26/Mar/20 07:48,30/Nov/21 20:37,13/Jul/23 08:07,27/Mar/20 11:18,1.11.0,,,,,1.11.0,,,,Deployment / Scripts,,,,,0,pull-request-available,,,,"With FLINK-15519, in the TM start-up scripts, we have captured logs from {{BashJavaUtils}} and passed into the TM JVM process via environment variable. These logs will be merged with other TM logs, writing to same places respecting user's log configurations.

This effort was broken in FLINK-15727, where the outputs from {{BashJavaUtils}}  are thrown away, except for the result JVM parameters and dynamic configurations",,xtsong,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11519: [FLINK-16798] Fix TM inherit logs from BashJavaUtils.
URL: https://github.com/apache/flink/pull/11519
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/20 09:45;githubbot;600","zentol commented on pull request #11520: [FLINK-16798][scripts] Properly forward BashJavaUtils logging output
URL: https://github.com/apache/flink/pull/11520
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/20 11:18;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,FLINK-16856,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 11:18:34 UTC 2020,,,,,,,,,,"0|z0cxcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/20 07:54;xtsong;cc [~guoyangze] [~chesnay];;;","27/Mar/20 11:18;chesnay;master: fbfb639e500e7cfa06485f1328671cfe645dfd76;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix The Bug of Python UDTF in SQL Query,FLINK-16796,13294109,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,26/Mar/20 07:34,31/Mar/20 02:26,13/Jul/23 08:07,31/Mar/20 02:26,,,,,,1.11.0,,,,API / Python,,,,,0,pull-request-available,,,,"When executes Python UDTF in sql query, it will cause some problem.",,hequn8128,hxbks2ks,,,,,,,,,,,,,,,,,,,,"HuangXingBo commented on pull request #11521: [FLINK-16796][python] Fix The Bug of Python UDTF in SQL Query
URL: https://github.com/apache/flink/pull/11521
 
 
   ## What is the purpose of the change
   
   *This pull request will fix the bug of Python UDTF in SQL Query*
   
   ## Brief change log
   
     - *Add logic to collect RexFieldAccess in UDTF*
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Add test_table_function_with_sql_query in test_udtf.py*
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 08:18;githubbot;600","hequn8128 commented on pull request #11521: [FLINK-16796][python] Fix The Bug of Python UDTF in SQL Query
URL: https://github.com/apache/flink/pull/11521
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 02:26;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 02:26:23 UTC 2020,,,,,,,,,,"0|z0cxbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/20 02:26;hequn8128;Fixed in 1.11.0 via 52315e79b439eda4f6cd7836051c1791382a4a7d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
End to end tests timeout on Azure,FLINK-16795,13294106,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,rmetzger,rmetzger,26/Mar/20 07:07,27/Jul/20 08:06,13/Jul/23 08:07,27/Jul/20 08:06,1.11.0,1.12.0,,,,1.12.0,,,,Build System / Azure Pipelines,Tests,,,,0,pull-request-available,,,,"Example: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6650&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179 or https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6637&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}##[error]The job running on agent Azure Pipelines 6 ran longer than the maximum time of 200 minutes. For more information, see https://go.microsoft.com/fwlink/?linkid=2077134
{code}
and {code}##[error]The operation was canceled.{code}
",,becket_qin,dian.fu,dwysakowicz,pnowojski,rmetzger,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11522: [FLINK-16795][AZP] Increase e2e test timeout by 20min
URL: https://github.com/apache/flink/pull/11522
 
 
   ## What is the purpose of the change
   
   We recently saw that e2e tests were timing out. This change increases the timeout by 20 minutes
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 08:29;githubbot;600","rmetzger commented on pull request #11522: [FLINK-16795][AZP] Increase e2e test timeout by 20min
URL: https://github.com/apache/flink/pull/11522
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 10:35;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16917,,,,,,,,,,,"15/Jun/20 14:45;becket_qin;image.png;https://issues.apache.org/jira/secure/attachment/13005720/image.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 27 08:06:59 UTC 2020,,,,,,,,,,"0|z0cxb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/20 07:56;rmetzger;The time limit is something we can configure. The question is: why are we running into these timeouts.

I analyzed the durations of the tests. It seems that there's a lot of variance in their durations: https://docs.google.com/spreadsheets/d/1VpA3wsOY88ezY8qTUFl8iztPc58mRa-1pIncIBAbUL8/edit#gid=117660203

Python script to extract the execution times:
{code:python}
import sys
import re
pattern = re.compile(""^.*'([^']+)'.* ([0-9]+) minutes.* ([0-9]+) seconds.*$"")
with open(sys.argv[1]) as f:
	for line in f:
	    if ""Test exited with exit code"" in line:
	    	result = pattern.match(line)
	    	test_name = result.group(1)
	    	seconds = int(result.group(2)) * 60 + int(result.group(3))
	    	print(""{};{}"".format(seconds,test_name))
{code}

160 minutes seems to be normal for the e2e tests. The build times can be between 20-30 minutes.
I will increase the limit by 20 minutes for now.;;;","26/Mar/20 10:36;rmetzger;Resolved in e0517e01ebfb247b1918fb88fe6eb588a2a4a4a7

Please reopen this ticket if we hit the issue again (and we will);;;","19/May/20 19:26;rmetzger;Reopening: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1834&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","19/May/20 19:31;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1813&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","20/May/20 09:53;rmetzger;bq. Reopening: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1834&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

Heavy hitter e2e's:
20 minutes: ""Running Kerberized YARN per-job on Docker test (default input)"" (usually 10 minutes)
19 minutes: ""TPC-DS end-to-end test (Blink planner)"" (expected runtime)
9 minutes. ""PyFlink end-to-end test"" (new test)

these two tests have been added as well: (4,5 minutes each)
Running Kerberized YARN application on Docker test (custom fs plugin)			
Running Kerberized YARN application on Docker test (default input)	

Summary of why this failed: We added ~20 minutes of additional tests (since my last runtime analysis) & the ""Running Kerberized YARN per-job on Docker test"" took 10 more minutes.;;;","20/May/20 09:57;rmetzger;bq. Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1813&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
2020-05-19T13:52:36.3671337Z ==============================================================================
2020-05-19T13:52:36.3672676Z Running 'Wordcount on Docker test (custom fs plugin)'
2020-05-19T13:52:36.3673075Z ==============================================================================
2020-05-19T13:52:36.3690425Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-36368466956
2020-05-19T13:52:36.5392828Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT
2020-05-19T13:52:36.5542369Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT
2020-05-19T13:52:37.3563181Z Docker version 19.03.9, build 9d988398e7
2020-05-19T13:52:37.9807947Z docker-compose version 1.25.4, build 8d51620a
2020-05-19T13:52:38.0441455Z Starting fileserver for Flink distribution
2020-05-19T13:52:38.0443400Z ~/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin ~/work/1/s
2020-05-19T13:52:51.3219872Z ~/work/1/s
2020-05-19T13:52:51.3222306Z ~/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-36368466956 ~/work/1/s
2020-05-19T13:52:51.3236345Z Preparing Dockeriles
2020-05-19T13:52:51.3536035Z Cloning into 'flink-docker'...
2020-05-19T13:55:01.5771005Z fatal: unable to access 'https://github.com/apache/flink-docker.git/': Failed to connect to github.com port 443: Connection timed out
2020-05-19T13:55:01.5788886Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/common_docker.sh: line 49: cd: flink-docker: No such file or directory
2020-05-19T13:55:01.5793574Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/common_docker.sh: line 50: ./add-custom.sh: No such file or directory
2020-05-19T13:55:01.5795243Z Building images
2020-05-19T13:55:01.6515325Z unable to prepare context: path ""dev/test_docker_embedded_job-debian"" not found
2020-05-19T13:55:01.6555918Z ~/work/1/s
2020-05-19T17:00:40.6850549Z ##[error]The operation was canceled.
2020-05-19T17:00:40.6891233Z ##[section]Finishing: Run e2e tests
{code}

Summary: The ""Wordcount on Docker test (custom fs plugin)"" failed cloning ""flink-docker"" and stalled ""forever"". This is probably related to the recent changes from FLINK-17656. [~chesnay] can you harden the test to properly fail if the clone fails?;;;","20/May/20 18:10;rmetzger;The reason for the slow Kerberized YARN test seems that downloading 12.5 MB of ubuntu packages lasted 10 minutes.

I will close this issue again.;;;","05/Jun/20 06:55;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2758&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334;;;","05/Jun/20 07:29;rmetzger;Reopening ticket because of https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2773&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","05/Jun/20 17:54;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2825&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","11/Jun/20 06:56;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3223&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","11/Jun/20 18:45;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3292&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","11/Jun/20 18:48;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3286&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","15/Jun/20 08:42;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3481&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","15/Jun/20 09:45;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3483&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=332da49d-5837-4578-b629-3ec794fb1e0e;;;","15/Jun/20 14:45;becket_qin;I had a timeout case in my CI tests and found that in my case the problem was that the build time of Flink took over 1h20m, which cause the entire CI tests timed out.
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3474&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]

The build time of every single module took much longer than a normal run in my case, as shown in the attached image.;;;","15/Jun/20 14:55;pnowojski;I've seen the similar thing as [~becket_qin] reported above, but I don't have any numbers to back it up - the build itself was taking long time. Is it another issue then you are looking at [~rmetzger]?;;;","15/Jun/20 15:17;rmetzger;[~becket_qin] for the link you posted, the build time seems to be fine with ~28 minutes. Do you have a link to the build with took +60 minutes?;;;","15/Jun/20 18:51;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3519&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","16/Jun/20 01:22;becket_qin;[~rmetzger] Ah, I pasted a wrong link. Here is the one. 

[https://dev.azure.com/becketqin/apache-flink/_build/results?buildId=18&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16];;;","16/Jun/20 05:58;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3542&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","16/Jun/20 06:01;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3542&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3542&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3542&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=375367d9-d72e-5c21-3be0-b45149130f6b
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3541&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","16/Jun/20 06:50;rmetzger;I guess all these new failures are caused by FLINK-18311. 
;;;","16/Jun/20 06:53;rmetzger;[~becket_qin] thanks a lot for posting the updated link here. From my analysis so far, I assume this has been caused by slow download speeds from the Azure VM.
Our current CI setup has the download logs disabled. If we such issues more frequently, I'll enable the logs again to get a glimpse what's going on;;;","16/Jun/20 15:37;pnowojski;Suddenly this is failing almost every build right now.;;;","16/Jun/20 17:40;rmetzger;The end to end tests were broken since yesterday afternoon (CEST), due to the StreamingKafkaITCase / FLINK-18311. 
The first builds that are not affected by this anymore should come in soon.;;;","17/Jun/20 06:58;rmetzger;This is a real e2e timeout now: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3657&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","17/Jun/20 09:44;rmetzger;Based on this analysis, I propose to give the e2e tests 20 minutes more: https://docs.google.com/spreadsheets/d/1fOexjfZmBKjn480jgb9LQPgebIJHtQOb9F03Ua1kRDs/edit#gid=180188288

The last failure was 7 seconds too slow. 20 minutes should give us enough headroom for test time variances.
Generally, I should add some tooling for tracking the e2e durations better.;;;","17/Jun/20 12:33;rmetzger;Merged to master (1.12) in https://github.com/apache/flink/commit/2150533ac0b2a6cc00238041853bbb6ebf22cee9.

Merged to release-1.11 in: 6a5213ee605297687ec4b8a2f872df05cd4921a3;;;","18/Jun/20 12:53;rmetzger;As a follow up to these issues, I filed a ticket for centrally tracking the E2E test durations so that we can see performance regressions etc: https://issues.apache.org/jira/browse/FLINK-18366;;;","08/Jul/20 18:01;dwysakowicz;New instances:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4334&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4333&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4340&view=results

Just an observation that when this happens I can not see any logs in the e2e view. (maybe its just my thing?);;;","09/Jul/20 01:48;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4350&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4350&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4350&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179] 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4350&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6];;;","09/Jul/20 13:10;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4354&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4354&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee

That becomes quite prevalent. ;;;","09/Jul/20 13:17;chesnay;These failures could be caused by FLINK-18533. I will disable part of FLINK-17075 later today.;;;","09/Jul/20 13:38;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4362&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee;;;","27/Jul/20 08:06;rmetzger;I'm closing this ticket. Afaik the E2e tests are currently not timing out.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix pyarrow version incompatible problem,FLINK-16786,13294062,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rongr,hequn8128,hequn8128,26/Mar/20 01:39,27/Mar/20 02:34,13/Jul/23 08:07,26/Mar/20 15:51,,,,,,1.11.0,,,,API / Python,,,,,0,pull-request-available,,,,"As reported in FLINK-16483, we should make the version of pyarrow consistent between pyflink and beam. Other dependencies should also be checked. ",,dian.fu,hequn8128,,,,,,,,,,,,,,,,,,,,"walterddr commented on pull request #11517: [FLINK-16786][hotfix] fix pyarrow version in setup.py
URL: https://github.com/apache/flink/pull/11517
 
 
   ## What is the purpose of the change
   
   Fix incorrectly configured pyarrow version
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): yes (pyarrow)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 03:51;githubbot;600","hequn8128 commented on pull request #11517: [FLINK-16786][hotfix] fix pyarrow version in setup.py
URL: https://github.com/apache/flink/pull/11517
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 15:51;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16483,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 15:51:36 UTC 2020,,,,,,,,,,"0|z0cx1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/20 15:51;hequn8128;Fixed in 1.11.0 via 7ce9d7c96fbdc6a871e720ca8b2f9f461b3971c7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when filtering by decimal column,FLINK-16771,13293773,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,25/Mar/20 09:55,17/Apr/20 04:10,13/Jul/23 08:07,17/Apr/20 04:10,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"The following SQL can trigger the issue:
{code}
create table foo (d decimal(15,8));
insert into foo values (cast('123.123' as decimal(15,8)));
select * from foo where d>cast('123456789.123' as decimal(15,8));
{code}",,jark,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11511: [FLINK-16771][table-planner-blink] NPE when filtering by decimal column
URL: https://github.com/apache/flink/pull/11511
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix NPE when comparing decimal column with null values.
   
   
   ## Brief change log
   
     - Fix `GenerateUtils::generateLiteral` for decimal type
     - Add test case
   
   
   ## Verifying this change
   
   Added test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 13:14;githubbot;600","JingsongLi commented on pull request #11511: [FLINK-16771][table-planner-blink] NPE when filtering by decimal column
URL: https://github.com/apache/flink/pull/11511
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Apr/20 07:47;githubbot;600","lirui-apache commented on pull request #11736: [FLINK-16771][table-planner-blink] NPE when filtering by decimal column
URL: https://github.com/apache/flink/pull/11736
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix NPE when comparing decimal column with null values.
   
   
   ## Brief change log
   
     - Fix `GenerateUtils::generateLiteral` for decimal type
     - Add test case
   
   
   ## Verifying this change
   
   Added test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Apr/20 11:52;githubbot;600","JingsongLi commented on pull request #11736: [FLINK-16771][table-planner-blink] NPE when filtering by decimal column
URL: https://github.com/apache/flink/pull/11736
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/20 04:09;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 04:10:52 UTC 2020,,,,,,,,,,"0|z0cvtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/20 09:59;lirui;I think the root cause is {{cast('123456789.123' as decimal(15,8))}} returns null and the generated code doesn't properly handle this.;;;","14/Apr/20 07:48;lzljs3620320;master: 3c0d5ff29ffd045f055f724642534699abddbcae;;;","14/Apr/20 07:49;lzljs3620320;Hi [~lirui], you can create a PR for release-1.10;;;","17/Apr/20 04:10;lzljs3620320;release-1.10: 47d6781de53a0cc237f457ae4f9c74c25ea590c6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resuming Externalized Checkpoint (rocks, incremental, scale up) end-to-end test fails with no such file",FLINK-16770,13293757,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,zjwang,zjwang,25/Mar/20 08:38,11/May/20 08:59,13/Jul/23 08:07,11/May/20 08:59,1.11.0,,,,,1.11.0,,,,Runtime / Checkpointing,Tests,,,,0,pull-request-available,test-stability,,,"The log : [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6603&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]

 

There was also the similar problem in https://issues.apache.org/jira/browse/FLINK-16561, but for the case of no parallelism change. And this case is for scaling up. Not quite sure whether the root cause is the same one.
{code:java}
2020-03-25T06:50:31.3894841Z Running 'Resuming Externalized Checkpoint (rocks, incremental, scale up) end-to-end test'
2020-03-25T06:50:31.3895308Z ==============================================================================
2020-03-25T06:50:31.3907274Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-31390197304
2020-03-25T06:50:31.5500274Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-03-25T06:50:31.6354639Z Starting cluster.
2020-03-25T06:50:31.8871932Z Starting standalonesession daemon on host fv-az655.
2020-03-25T06:50:33.5021784Z Starting taskexecutor daemon on host fv-az655.
2020-03-25T06:50:33.5152274Z Waiting for Dispatcher REST endpoint to come up...
2020-03-25T06:50:34.5498116Z Waiting for Dispatcher REST endpoint to come up...
2020-03-25T06:50:35.6031346Z Waiting for Dispatcher REST endpoint to come up...
2020-03-25T06:50:36.9848425Z Waiting for Dispatcher REST endpoint to come up...
2020-03-25T06:50:38.0283377Z Dispatcher REST endpoint is up.
2020-03-25T06:50:38.0285490Z Running externalized checkpoints test, with ORIGINAL_DOP=2 NEW_DOP=4 and STATE_BACKEND_TYPE=rocks STATE_BACKEND_FILE_ASYNC=true STATE_BACKEND_ROCKSDB_INCREMENTAL=true SIMULATE_FAILURE=false ...
2020-03-25T06:50:46.1754645Z Job (b8cb04e4b1e730585bc616aa352866d0) is running.
2020-03-25T06:50:46.1758132Z Waiting for job (b8cb04e4b1e730585bc616aa352866d0) to have at least 1 completed checkpoints ...
2020-03-25T06:50:46.3478276Z Waiting for job to process up to 200 records, current progress: 173 records ...
2020-03-25T06:50:49.6332988Z Cancelling job b8cb04e4b1e730585bc616aa352866d0.
2020-03-25T06:50:50.4875673Z Cancelled job b8cb04e4b1e730585bc616aa352866d0.
2020-03-25T06:50:50.5468230Z ls: cannot access '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-31390197304/externalized-chckpt-e2e-backend-dir/b8cb04e4b1e730585bc616aa352866d0/chk-[1-9]*/_metadata': No such file or directory
2020-03-25T06:50:50.5606260Z Restoring job with externalized checkpoint at . ...
2020-03-25T06:50:58.4728245Z 
2020-03-25T06:50:58.4732663Z ------------------------------------------------------------
2020-03-25T06:50:58.4735785Z  The program finished with the following exception:
2020-03-25T06:50:58.4737759Z 
2020-03-25T06:50:58.4742666Z org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-03-25T06:50:58.4746274Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
2020-03-25T06:50:58.4749954Z 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
2020-03-25T06:50:58.4752753Z 	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:142)
2020-03-25T06:50:58.4755400Z 	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:659)
2020-03-25T06:50:58.4757862Z 	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:210)
2020-03-25T06:50:58.4760282Z 	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:890)
2020-03-25T06:50:58.4763591Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:963)
2020-03-25T06:50:58.4764274Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-03-25T06:50:58.4764809Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-03-25T06:50:58.4765434Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
2020-03-25T06:50:58.4766180Z 	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
2020-03-25T06:50:58.4773549Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:963)
2020-03-25T06:50:58.4774502Z Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-03-25T06:50:58.4775382Z 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:276)
2020-03-25T06:50:58.4776163Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1741)
2020-03-25T06:50:58.4777706Z 	at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:90)
2020-03-25T06:50:58.4778334Z 	at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:58)
2020-03-25T06:50:58.4779007Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1620)
2020-03-25T06:50:58.4779654Z 	at org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.main(DataStreamAllroundTestProgram.java:215)
2020-03-25T06:50:58.4780371Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-25T06:50:58.4784367Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-25T06:50:58.4785063Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-25T06:50:58.4785557Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-25T06:50:58.4786204Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
2020-03-25T06:50:58.4786547Z 	... 11 more
2020-03-25T06:50:58.4787007Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-03-25T06:50:58.4787717Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-03-25T06:50:58.4788203Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-03-25T06:50:58.4788835Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1736)
2020-03-25T06:50:58.4789362Z 	... 20 more
2020-03-25T06:50:58.4789720Z Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-03-25T06:50:58.4790467Z 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:359)
2020-03-25T06:50:58.4791087Z 	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
2020-03-25T06:50:58.4791650Z 	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
2020-03-25T06:50:58.4792560Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-03-25T06:50:58.4793617Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-03-25T06:50:58.4794496Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:274)
2020-03-25T06:50:58.4795255Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-03-25T06:50:58.4796264Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-03-25T06:50:58.4796867Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-03-25T06:50:58.4797439Z 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
2020-03-25T06:50:58.4798000Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
2020-03-25T06:50:58.4798589Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-03-25T06:50:58.4799162Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-03-25T06:50:58.4799727Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-25T06:50:58.4800210Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-25T06:50:58.4800767Z Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
2020-03-25T06:50:58.4801351Z org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.
2020-03-25T06:50:58.4801938Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$3(Dispatcher.java:336)
2020-03-25T06:50:58.4803660Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2020-03-25T06:50:58.4804555Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2020-03-25T06:50:58.4805235Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-03-25T06:50:58.4805839Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-03-25T06:50:58.4806515Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-03-25T06:50:58.4807184Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-03-25T06:50:58.4807807Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-03-25T06:50:58.4808417Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-03-25T06:50:58.4809055Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-03-25T06:50:58.4809783Z Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
2020-03-25T06:50:58.4810756Z 	at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36)
2020-03-25T06:50:58.4811444Z 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2020-03-25T06:50:58.4811937Z 	... 6 more
2020-03-25T06:50:58.4812414Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
2020-03-25T06:50:58.4813330Z 	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:152)
2020-03-25T06:50:58.4814154Z 	at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:84)
2020-03-25T06:50:58.4814846Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:379)
2020-03-25T06:50:58.4815622Z 	at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34)
2020-03-25T06:50:58.4816074Z 	... 7 more
2020-03-25T06:50:58.4816924Z Caused by: java.io.IOException: Cannot access file system for checkpoint/savepoint path 'file://.'.
2020-03-25T06:50:58.4817673Z 	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorage.resolveCheckpointPointer(AbstractFsCheckpointStorage.java:233)
2020-03-25T06:50:58.4818450Z 	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorage.resolveCheckpoint(AbstractFsCheckpointStorage.java:110)
2020-03-25T06:50:58.4819276Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1312)
2020-03-25T06:50:58.4819943Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.tryRestoreExecutionGraphFromSavepoint(SchedulerBase.java:314)
2020-03-25T06:50:58.4820633Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:247)
2020-03-25T06:50:58.4821258Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:223)
2020-03-25T06:50:58.4821862Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:118)
2020-03-25T06:50:58.4822505Z 	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:103)
2020-03-25T06:50:58.4823115Z 	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:282)
2020-03-25T06:50:58.4823665Z 	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:270)
2020-03-25T06:50:58.4824485Z 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)
2020-03-25T06:50:58.4825597Z 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)
2020-03-25T06:50:58.4826400Z 	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:146)
2020-03-25T06:50:58.4826919Z 	... 10 more
2020-03-25T06:50:58.4829018Z Caused by: java.io.IOException: Found local file path with authority '.' in path 'file://.'. Hint: Did you forget a slash? (correct path would be 'file:///.')
2020-03-25T06:50:58.4829875Z 	at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:441)
2020-03-25T06:50:58.4830364Z 	at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:389)
2020-03-25T06:50:58.4830807Z 	at org.apache.flink.core.fs.Path.getFileSystem(Path.java:292)
2020-03-25T06:50:58.4831408Z 	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorage.resolveCheckpointPointer(AbstractFsCheckpointStorage.java:230)
2020-03-25T06:50:58.4832021Z 	... 22 more
2020-03-25T06:50:58.4832151Z 
2020-03-25T06:50:58.4832356Z End of exception on server side>]
2020-03-25T06:50:58.4832720Z 	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:390)
2020-03-25T06:50:58.4833238Z 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:374)
2020-03-25T06:50:58.4833884Z 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
2020-03-25T06:50:58.4834376Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
2020-03-25T06:50:58.4834724Z 	... 4 more
2020-03-25T06:50:58.5042321Z Resuming from externalized checkpoint job could not be started.
2020-03-25T06:50:58.5044210Z [FAIL] Test script contains errors.
2020-03-25T06:50:58.5052826Z Checking of logs skipped.
2020-03-25T06:50:58.5053164Z 
2020-03-25T06:50:58.5054116Z [FAIL] 'Resuming Externalized Checkpoint (rocks, incremental, scale up) end-to-end test' failed after 0 minutes and 27 seconds! Test exited with exit code 1
2020-03-25T06:50:58.5054639Z 
2020-03-25T06:50:58.8067813Z Stopping taskexecutor daemon (pid: 86888) on host fv-az655.
2020-03-25T06:50:59.0257270Z Stopping standalonesession daemon (pid: 86603) on host fv-az655.
2020-03-25T06:50:59.4920994Z 
2020-03-25T06:50:59.5000014Z ##[error]Bash exited with code '1'.
2020-03-25T06:50:59.5015374Z ##[section]Finishing: Run e2e tests
{code}",,aljoscha,klion26,pnowojski,rmetzger,roman,sewen,SleePy,trohrmann,ym,yunta,zjwang,,,,,,,,,,,"ifndef-SleePy commented on pull request #11627: [FLINK-16770][e2e] Make the checkpoint resuming e2e case pass by increasing the…
URL: https://github.com/apache/flink/pull/11627
 
 
   … retained checkpoints number
   
   ## What is the purpose of the change
   
   * This is a quick fixing of FLINK-16770
   * It could avoid testing failure caused by changing of FLINK-14971
   * This hotfix should be reverted when the final solution of FLINK-16770 completes
   
   ## Brief change log
   
   * Increase the number of retained checkpoints to avoid all checkpoints are subsumed when a race condition of `CheckpointCoordinator` happens
   
   ## Verifying this change
   
   * This change is already covered by existing tests, such as *test_resume_externalized_checkpoints.sh* and *test_resume_savepoint.sh*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Apr/20 09:33;githubbot;600","rmetzger commented on pull request #11627: [FLINK-16770][e2e] Make the checkpoint resuming e2e case pass by increasing the…
URL: https://github.com/apache/flink/pull/11627
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Apr/20 07:09;githubbot;600","Myasuka commented on pull request #11742: [FLINK-16770][checkpoint] Avoid dispose checkpoint during async finalization of pendingcheckpoint
URL: https://github.com/apache/flink/pull/11742
 
 
   ## What is the purpose of the change
   
   This is a quick hot-fix of FLINK-16770, which would not remove checkpoint by mistake but would let checkpoint could complete after job switched to cancelling status. However, this should not introduce harmful influence.
   
   ## Brief change log
   
   Refactor the logic of `PendingCheckpoint#dispose` and `PendingCheckpoint#finalizeCheckpoint` so that they are exclusive from each other. 
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Refactor `PendingCheckpoint#testAbortingDuringAsyncFinalization` to verify not discard checkpoint when already in async phase of finalize checkpoint.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers:no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Apr/20 18:11;githubbot;600","ifndef-SleePy commented on pull request #11781: [FLINK-16770][checkpointing] Revert commits of FLINK-16945 and FLINK-14971
URL: https://github.com/apache/flink/pull/11781
 
 
   ## What is the purpose of the change
   
   * This PR reverts the commits of FLINK-16945 and FLINK-14971, because we can't find a simple and clean solution of FLINK-16770.
   * We decide to revert the commits which cause the inconsistent state of `CompletedCheckpointStore`. So the releasing could get rid of blocking by this.
   
   ## Brief change log
   
   * Reverts commits of https://github.com/apache/flink/pull/11648, https://github.com/apache/flink/pull/11627 and https://github.com/apache/flink/pull/11347
   
   ## Verifying this change
   
   * This change is already covered by existing tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/20 14:05;githubbot;600","pnowojski commented on pull request #11781: [FLINK-16770][checkpointing] Revert commits of FLINK-16945 and FLINK-14971
URL: https://github.com/apache/flink/pull/11781
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/20 09:17;githubbot;600",,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,FLINK-16423,,,,,,FLINK-14971,,,,,,FLINK-16561,,FLINK-16931,FLINK-17094,FLINK-17006,FLINK-17140,,FLINK-16423,,,,"01/Apr/20 07:01;rmetzger;e2e-output.log;https://issues.apache.org/jira/secure/attachment/12998410/e2e-output.log","01/Apr/20 07:00;rmetzger;flink-vsts-standalonesession-0-fv-az53.log;https://issues.apache.org/jira/secure/attachment/12998408/flink-vsts-standalonesession-0-fv-az53.log","16/Apr/20 03:24;yunta;image-2020-04-16-11-24-54-549.png;https://issues.apache.org/jira/secure/attachment/13000075/image-2020-04-16-11-24-54-549.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 08:59:12 UTC 2020,,,,,,,,,,"0|z0cvq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/20 10:32;yunta;I think this issue is the same root cause as FLINK-16561 , and cannot come to an idea why this could happen if we retain checkpoint and at least one checkpoint completed. If there any place to find the cluster logs in detail? It seems it has uploaded logs failed.;;;","29/Mar/20 08:34;zjwang;Another instance [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6788&view=logs&j=7bafe89a-737e-5a81-708c-24b72a2345fc&t=8f0197c1-92aa-5b5f-4284-1ae542d75a1e];;;","29/Mar/20 08:38;zjwang;Another instance [https://travis-ci.org/apache/flink/builds/668073755?utm_source=slack&utm_medium=notification];;;","31/Mar/20 13:08;pnowojski;another instance: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6882&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","31/Mar/20 18:03;rmetzger;Another instance: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6892&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

Assigning myself to address this ...;;;","01/Apr/20 07:00;rmetzger;I have understood the following so far:
- the test is searching for the checkpoint directory, but no checkpoint exists
- It seems that the checkpoint N does not get retained, if N+1 gets triggered and the job gets cancelled immediately thereafter.
The job has checkpoint retention on cancellation enabled.

Proof:
{code}
$ cat flink-vsts-standalonesession-0-fv-az53.log | grep ""CheckpointCoo\|job.lastCheckpointExternalPath\|switched from state""
2020-04-01 06:30:18,805 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job General purpose test job (c34d2f91cf100e020226725452b5000a) switched from state CREATED to RUNNING.
localhost.jobmanager.General purpose test job.lastCheckpointExternalPath: n/a
2020-04-01 06:30:19,571 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: EventSource -> Timestamps/Watermarks (1/4) of job c34d2f91cf100e020226725452b5000a is not in state RUNNING but DEPLOYING instead. Aborting checkpoint.
localhost.jobmanager.General purpose test job.lastCheckpointExternalPath: n/a
2020-04-01 06:30:20,597 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 @ 1585722620570 for job c34d2f91cf100e020226725452b5000a.
2020-04-01 06:30:21,170 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 1 for job c34d2f91cf100e020226725452b5000a (158574 bytes in 597 ms).
localhost.jobmanager.General purpose test job.lastCheckpointExternalPath: file:/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-1
2020-04-01 06:30:21,570 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 2 @ 1585722621570 for job c34d2f91cf100e020226725452b5000a.
2020-04-01 06:30:21,689 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 2 for job c34d2f91cf100e020226725452b5000a (274341 bytes in 113 ms).
localhost.jobmanager.General purpose test job.lastCheckpointExternalPath: file:/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-2
2020-04-01 06:30:22,571 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 3 @ 1585722622570 for job c34d2f91cf100e020226725452b5000a.
2020-04-01 06:30:22,689 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 3 for job c34d2f91cf100e020226725452b5000a (326291 bytes in 118 ms).
localhost.jobmanager.General purpose test job.lastCheckpointExternalPath: file:/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-3
2020-04-01 06:30:23,571 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 4 @ 1585722623570 for job c34d2f91cf100e020226725452b5000a.
2020-04-01 06:30:23,650 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 4 for job c34d2f91cf100e020226725452b5000a (341697 bytes in 78 ms).
localhost.jobmanager.General purpose test job.lastCheckpointExternalPath: file:/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-4
2020-04-01 06:30:24,570 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 5 @ 1585722624570 for job c34d2f91cf100e020226725452b5000a.
2020-04-01 06:30:24,643 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 5 for job c34d2f91cf100e020226725452b5000a (345026 bytes in 72 ms).
localhost.jobmanager.General purpose test job.lastCheckpointExternalPath: file:/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5
2020-04-01 06:30:25,571 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 6 @ 1585722625570 for job c34d2f91cf100e020226725452b5000a.
2020-04-01 06:30:25,659 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 6 for job c34d2f91cf100e020226725452b5000a (347049 bytes in 88 ms).
localhost.jobmanager.General purpose test job.lastCheckpointExternalPath: file:/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-6
2020-04-01 06:30:26,571 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 7 @ 1585722626570 for job c34d2f91cf100e020226725452b5000a.
2020-04-01 06:30:26,633 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 7 for job c34d2f91cf100e020226725452b5000a (349427 bytes in 60 ms).
localhost.jobmanager.General purpose test job.lastCheckpointExternalPath: file:/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-7
2020-04-01 06:30:27,570 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 8 @ 1585722627570 for job c34d2f91cf100e020226725452b5000a.
2020-04-01 06:30:27,677 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job General purpose test job (c34d2f91cf100e020226725452b5000a) switched from state RUNNING to CANCELLING.
2020-04-01 06:30:28,131 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job General purpose test job (c34d2f91cf100e020226725452b5000a) switched from state CANCELLING to CANCELED.
2020-04-01 06:30:28,131 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job c34d2f91cf100e020226725452b5000a.
2020-04-01 06:30:28,131 INFO  org.apache.flink.runtime.checkpoint.CompletedCheckpoint      [] - Checkpoint with ID 8 at 'file:/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-8' not discarded.
2020-04-01 06:30:30,758 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Starting job 597e0c903b4fe886a0dfab962a9bcf93 from savepoint file://. ()
{code}

Before cancellation, this is the content of the checkpoint dir:
{code}
2020-04-01T06:30:25.4552903Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir
2020-04-01T06:30:25.4554109Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a
2020-04-01T06:30:25.4555100Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/shared
2020-04-01T06:30:25.4556094Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/taskowned
2020-04-01T06:30:25.4557084Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5
2020-04-01T06:30:25.4558165Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/2ea88999-1416-4880-9556-8e09e160bb41
2020-04-01T06:30:25.4559321Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/b2bb185d-dab4-430e-990a-295eb905d97d
2020-04-01T06:30:25.4560483Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/24013bb5-c371-44ff-83ab-dde53a8cf1cd
2020-04-01T06:30:25.4561549Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/_metadata
2020-04-01T06:30:25.4563080Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/6047540d-e336-4ed1-ab67-80ee222135c3
2020-04-01T06:30:25.4564570Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/51c50ce7-909b-4525-a098-cabd5e0377d9
2020-04-01T06:30:25.4565810Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/c818c3cd-e91d-454f-99ce-6c61c6f55aa7
2020-04-01T06:30:25.4567043Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/b8c8a828-2a7e-4197-a413-3b1a777b2788
2020-04-01T06:30:25.4568272Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/faafce1d-f281-4b76-a11b-167b7fb1e22e
2020-04-01T06:30:25.4569619Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/1dd494fa-3887-4d02-9567-79ffeb79afe6
2020-04-01T06:30:25.4570991Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/e11d0e0e-cb1e-44b3-a593-fc96cf9173bb
2020-04-01T06:30:25.4572877Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/b2ee6c2a-d980-470b-83a1-eddb8fb83a81
2020-04-01T06:30:25.4574130Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/a2c87162-f591-4a51-9caf-7594c7f110d7
2020-04-01T06:30:25.4575357Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/7e6efd4c-9dee-478c-8ff3-218fe461030b
2020-04-01T06:30:25.4576568Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/e2dbe0f9-8120-4fca-8833-e53b5e8ec1d1
2020-04-01T06:30:25.4589585Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/6408dffc-99ed-43f0-afbd-5d3836af58a5
2020-04-01T06:30:25.4591144Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/6b27e2f2-cdcc-417e-91e2-6bbf5dfcaeab
2020-04-01T06:30:25.4592357Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/c47b20ed-e29a-4a78-a9ad-8e30a80765b0
2020-04-01T06:30:25.4593849Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/140167a5-8d86-4a8a-97cc-91a8c272ed0a
2020-04-01T06:30:25.4595121Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/050b237c-2a3c-4dec-a242-8fe93558d459
2020-04-01T06:30:25.4596528Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/c60c4f87-f437-466b-801a-a290b0e07f7c
2020-04-01T06:30:25.4597721Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/21f99bc1-21c1-44bf-91c9-312790051844
2020-04-01T06:30:25.4598900Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/a4c763a3-bf51-4d67-88de-377019cd489a
2020-04-01T06:30:25.4600099Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/91cfe5fd-51ef-46cd-ba15-fada90d1ced5
2020-04-01T06:30:25.4601551Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/b58caf97-a2ca-47d6-bd32-a7e005e54d01
2020-04-01T06:30:25.4602772Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/fa2d79e2-f6da-4ec7-9450-8d9eaba4a689
2020-04-01T06:30:25.4604562Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/37e56b80-4772-40fb-a605-9427ec310106
2020-04-01T06:30:25.4605790Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/6f93dfc5-5440-44f6-aade-6e68542cbb14
2020-04-01T06:30:25.4607168Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/d2dadd16-eda7-47e2-b31f-babf62fb59a5
2020-04-01T06:30:25.4608385Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/6117955e-6513-4a8e-8e24-dfab248491fd
2020-04-01T06:30:25.4609791Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/0f038eb9-1c9a-45b8-8e67-ffdac782a7e3
2020-04-01T06:30:25.4611002Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/183e1715-3c31-4946-9153-7ce4450e056c
2020-04-01T06:30:25.4612183Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/6f819cea-cb94-42ef-819c-9f2ef21e145f
2020-04-01T06:30:25.4613496Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/8fce4fc7-20e3-4e79-a329-42b173cea697
2020-04-01T06:30:25.4614687Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/4f2e6e30-1f14-4ac6-a956-8de22d168bd9
2020-04-01T06:30:25.4618606Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/61d27b66-7897-43b9-ad40-440ca5fafaf3
2020-04-01T06:30:25.4619845Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/chk-5/723d4610-7004-4b52-96b6-15af5b311287
{code}

After cancellation:
{code}
2020-04-01T06:30:27.8809772Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir
2020-04-01T06:30:27.8811029Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a
2020-04-01T06:30:27.8812272Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/shared
2020-04-01T06:30:27.8813488Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-08993937085/externalized-chckpt-e2e-backend-dir/c34d2f91cf100e020226725452b5000a/taskowned
{code}

I believe there's something wrong in the checkpoint coordinator
;;;","01/Apr/20 07:08;yunta;[~rmetzger] I have reproduced this with additional logs in my [private branch |https://github.com/Myasuka/flink/tree/travis-fix-bug] and personal azure pipeline https://myasuka.visualstudio.com/flink/_build/results?buildId=10&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=2f5b54d0-1d28-5b01-d344-aa50ffe0cdf8 . 
From the addtional logs, I have figured out why this could happen:
{{CheckpointCoordinator}} drop the pending checkpoint-8 when cancelling the job in {{CheckpointCoordinator#stopCheckpointScheduler}}, however, chk-8 has just been asynchronously added to checkpoint store successfully during {{PendingCheckpoint#finalizeCheckpoint}}. On the other hand, once chk-8 is added to checkpoint store, chk-7 will be removed in checkpoint store. That's why we could see logs:

{code:java}
Checkpoint with ID 8 at 'file:/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-07881636808/externalized-chckpt-e2e-backend-dir/0329a0facde65e8a8432124ce5db8e3c/chk-8' not discarded.
{code}

In the end, chk-8 is deleted when we stop the scheduler and chk-7 is delete when chk-8 is successfully added to checkpoint store.

I am not sure whether you have ever did some work to figure out the root cause, please assign this ticket to me if you don't mind and not doing so much work ever.
;;;","01/Apr/20 07:16;rmetzger;I did not know that you are working on this ticket as well. To avoid duplicate work in the future, it would be nice if you could assign yourself / or write a comment if you are working on something.

I will assign both tickets to you.;;;","01/Apr/20 09:14;SleePy;Hi [~yunta], thanks for the analysis. I have a question that if chk-8 is dropped when cancelling the job, the chk-7 would not be subsumed since the finalization of chk-8 would not finish after adding to checkpoint store asynchronously. It would check the discarding state before doing subsuming. 

Although I haven't check the testing case carefully, I guess this might be relevant with FLINK-14971 which make the threading model here asynchronous. There is a small possibility that a checkpoint is discarded but it could be added into checkpoint store successfully. Because currently the cancellation and the manipulation on checkpoint store are in different threads. There is no a big lock for everything as before. Do you think it could cause this failure?;;;","01/Apr/20 17:54;yunta;[~SleePy] Yes, this bug is introduced from FLINK-14971 and I could reproduce it locally with unit test. There exists a logic competition between {{PendingCheckpoint#dispose}} and {{PendingCheckpoint#finalizeCheckpoint}}, current {{operationLock}} can only ensure the async phase to delete this pending checkpoint and adding completed checkpoint would not happen at the same time. However, this cannot ensure the pending checkpoint would not be firstly added to checkpoint store and then dropped. 

One quick fix would add atomic boolean to share among these two async operations, once this pending checkpoint is added to checkpoint store, it would not be dropped anymore asynchronously. However, this could lead something misleading: if this pending checkpoint is added to checkpoint store successfully asynchronously but tagged as disposed in the main thread. Although we could avoid to drop this in the async phase of  {{PendingCheckpoint#dispose}}, checkpoint coordinator would not treat this pending checkpoint as successful and would not display in the checkpoint web UI. But luckily, we could ensure at least no data will be deleted by mistake, job could still failover by recovering from latest completed checkpoint.

Another solution needs to compare and set some atomic variable in the main thread when {{PendingCheckpoint#dispose}} and share that when we try to add checkpoint store. If we firstly arrive to add checkpoint to store, we would not let main thread to tag that pending checkpoint as discarded. On the other hand, if we firstly arrive to tag this pending checkpoint would be discarded, we would not try to add to checkpoint store. I think this could be really light-weight and non-blocking, but it would introduce some extra CAS work in the main thread. What do you think of this ? [~pnowojski];;;","02/Apr/20 05:43;SleePy;Hi [~yunta], thanks for the response. If I understand correctly, there is an inconsistent state of {{CompletedCheckpointStore}} while stopping a checkpoint which is doing asynchronous finalization.

There are two strategies here,
 1. The checkpoint which is doing finalization could be aborted when {{CheckpointCoordinator}} is being shut down or periodic scheduler is being stopped. This is the choice of current implementation. However we didn't handle the {{CompletedCheckpointStore}} well. For example it might be better that reverting the state of {{CompletedCheckpointStore}} when the {{PendingCheckpoint}} finds the discarding after asynchronous finalization. But I think it's not easy to do so. Because there might be a subsuming operation during {{CompletedCheckpointStore#addCheckpoint}}.
 2. The checkpoint which is doing finalization could NOT be aborted when {{CheckpointCoordinator}} is being shut down or period scheduler is being stopped. I personally prefer this solution, because it could simply the concurrent conflict scenario and it's much easier to implement. I think introducing an atomic boolean might not be enough. It's better to rethink the relationship between {{PendingCheckpoint#abort}} and {{PendingCheckpoint#finalizeCheckpoint}}. And we also need to rewrite a part of error handling of the finalization.

BTW, [~yunta] could you share the unit test case which could reproduce the scenario locally? I want to verify my assumption and solution. The original e2e test case is not stable.;;;","02/Apr/20 09:08;yunta;As I said above, I prefer to let {{PendingCheckpoint#dispose}} and {{PendingCheckpoint#finalizeCheckpoint}} to share the same variable so that who enters first would not allow another enter in. However, after I think a bit more, some logic still changed. I will give explanations below:

Previously, the job status and checkpoint status looks like below:
{code:bash}
RUNNING --> pending chk-8 --> FAILING (discard pending chk-8) --> try to complete chk-8, but already discarded --> RESTARTING (from chk-7)
{code}
However, after FLINK-14971, things changed:
{code:java}
main thread: RUNNING --> pending chk-8 --> FAILING (discard pending chk-8) ----> RESTARTING (no checkpoint existed)
                                       |
async-IO thread:                       |---> finalizing chk-8 --> chk-8 added, chk-7 subsumed
{code}
If we just introduce a light-weight shared variable, things could be like:
{code:bash}
main thread: RUNNING --> pending chk-8 --> FAILING (try to discard pending chk-8) ----> RESTARTING
                                       |           ||  
                                       |        share variable
                                       |           ||
async-IO thread:                       |---> try to finalize chk-8 --> if finalize first enter, chk-8 added and chk-7 subsumed
{code}
we might get result as:
{code:bash}
RUNNING --> pending chk-8 --> FAILING (try to discard pending chk-8) --> completed chk-8 --> RESTARTING
{code}
or
{code:bash}
RUNNING --> pending chk-8 --> FAILING (try to discard pending chk-8) --> RESTARTING --> completed chk-8
{code}
As you can see, job FAILING which lead to {{CheckpointCoordinator#stopCheckpointScheduler}} would not have the top priority. A strict sync lock between {{CheckpointCoordinator#stopCheckpointScheduler}} and {{PendingCheckpoint#finalizeCheckpoint}} might not help as the async IO phase would subsume checkpoint when adding completed checkpoint. Thus, I currently prefer to change the logic of {{CompletedCheckpointStore}}, it would not subsume checkpoint within itself but only executed when we call it outside from checkpoint coordinator.

 The new work flow looks like below:
{code:bash}
main thread: RUNNING --> pending chk-8 --> FAILING (tag pending chk-8 as discarded, cancel async io finalizingFuture) --> RESTARTING from chk-7
                                       |                                                 
                                       |                                                  
                                       |                                                
async-IO thread:                       |---> try to finalize chk-8 --> if not tagged as discarded, chk-8 added, or canceled to delete chk-8
{code}
 or
{code:bash}
main thread: RUNNING --> pending chk-8 -->                              ----> complete chk-8, subsume chk-7 in store --> FAILING --> RESTARTING from chk-8
                                       |                                  |  
                                       |                                  |   
                                       |                                  | 
async-IO thread:                       |---> try to finalize chk-8 --> if not tagged as discarded, chk-8 added
{code};;;","03/Apr/20 09:39;SleePy;After a short discussion with [~yunta] offline, we reached agreement of the possible solution. [~yunta] will continue working on it.

Besides that, we think it's better to quickly fix the failed case first. So other guys could avoid suffering from this unstable failure. I have created a PR to try to resolve the failed case in a work-around way. [~yunta] could you take a look is there anything missing?;;;","03/Apr/20 12:59;pnowojski;Thanks [~yunta] for analysing the issue and [~SleePy] for confirming it.;;;","06/Apr/20 07:10;rmetzger;Temporary hotfix merged in dd9f9bf040cb82ed7e18c9fdf7c7e1ca6f43f896. Thank you guys for working on this!;;;","06/Apr/20 09:11;SleePy;Thanks [~rmetzger] for manually verifying and merging the PR. ;;;","08/Apr/20 10:09;rmetzger;Is this end to end test failing because of this issue FLINK-16423?;;;","08/Apr/20 11:00;SleePy;To [~rmetzger], I think FLINK-16423 and this ticket fail in same scenario. To be short, the atomicity of finalizing a checkpoint is broken.
I wrote a comment in FLINK-16423.;;;","14/Apr/20 07:00;aljoscha;Another occurrence of the general problem: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7418&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=643a4312-2b8f-5e76-5975-7bbc0942470d&l=3814;;;","14/Apr/20 09:12;SleePy;[~aljoscha], the uploading to transfer.sh failed, I can't confirm the root cause. It might be the same reason. [~yunta], do you need some help?;;;","14/Apr/20 13:17;rmetzger;You don't need to rely on transfer.sh anymore. We are storing the logs also in Azure Pipelines.
Each build as a set of artifacts associated with it. In case of the build Aljoscha posted, you need to download the file with the ""-tests"" suffix: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7418&view=artifacts&type=publishedArtifacts;;;","14/Apr/20 14:00;yunta;[~SleePy] After looking at logs of [~aljoscha] 's instance, I think this should be the same cause.

The job cancelled and did not know checkpoint-1 has been completed:
{code:bash}
05:25:23,676 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 @1586841923675 for job e2e741b6dcacdac50afd2830c8a6892d.
05:25:23,824 [Source: Custom Source (1/1)] WARN  org.apache.flink.runtime.taskmanager.Task   [] - Source: Custom Source (1/1) (73b5d00dc26989a5c2226f19ee14ea94) switched from RUNNING to FAILED.
{code}

But it would then recover form the checkpoint-1, which means the checkpoint store already contain that checkpoint-1 which is added in the async-io thread.
 {code:bash}
05:25:23,835 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Tumbling Window Test (e2e741b6dcacdac50afd2830c8a6892d) switched from state RESTARTING to RUNNING.
05:25:23,835 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Restoring job e2e741b6dcacdac50afd2830c8a6892d from latest valid checkpoint: Checkpoint 1 @ 1586841923675 for e2e741b6dcacdac50afd2830c8a6892d.
{code}

BTW, somehow I cannot receive any notification from JIRA recently, and I have to check issues manually periodically, sorry for late reply.;;;","14/Apr/20 15:08;SleePy;Thanks [~rmetzger] for reminding.
[~yunta] good job, please give me a feedback if you need any help.;;;","16/Apr/20 03:29;yunta;Update some progress:

After discussed with [~pnowojski] and [~SleePy] offline. We have reached the agreement that the idea of ""The checkpoint which is doing finalization could NOT be aborted when {{CheckpointCoordinator}} is being shut down or period scheduler is being stopped"", which would lead to ""checkpoint could complete after job is not in RUNNING status"", would violate the EXACTLY_ONCE semantics.

Explanation:

If chk-3 is already completed, and chk-4 is in the async phase which would create _metadata in the HDFS. The job switches to FAILING, and it restart from chk-3 in checkpoint store. After that, the async thread add the chk-4 into the checkpoint store. Then when it comes back to the main thread, we would treat it as completed, notify chk-4 to all tasks. If we use two-phase commit for those tasks, the data between chk-3 and chk-4 would be processed twice.

 

Below is the current logic after FLINK-14971 :

!image-2020-04-16-11-24-54-549.png!

If we still want the 100% non-blocking for all checkpoint threads, we need to split the logic of adding new checkpoint and subsuming old checkpoint to make adding could be reverted without deleting old checkpoints by mistake. However, this would change a lot and needs to be reviewed carefully with a design doc. Since Flink-1.11 future freeze time is close, we might look at another solution which could be much simpler. This idea would ""revert"" some logic introduced in FLINK-14971 to make some blocking actions, for example: share a lock among aborting and completing pending checkpoint in Flink-1.10 did. This could be treated as a compromise to not 100% non-blocking but satisfy EXACTLY_ONCE semantics based on previous commits in FLINK-14971. We currently prefer to the latter one which could be done in a short time, and ask [~SleePy] for help as he is more familiar with previous commits introduced in FLINK-14971.;;;","16/Apr/20 14:17;sewen;Thank you all for the great discussion and analysis. I would like to add a few points and suggestions, from the way I understand the problem:
h2. There are are two main issues:

*(1) Missing ownership in the multi-threaded system. Meaning: Who owns the ""Pending Checkpoint during Finalization""?*
 - It is owned by the CheckpointCoordinator (who aborts it when shutting down)
 - It is also owned by the I/O Thread or the Completed Checkpoint Store who writes it to ZooKeeper (or similar system).

*(2) No Shared Ground Truth between the Checkpoint Coordinator and the JobMaster*
 - When a checkpoint is finalized, that decision is not consistently visible to the JM.
 - The JM only sees the result once it is in ZK, which is an asynchronous operation
 - That causes the final issue described here: possibility that the JM starts from an earlier checkpoint, if a restart happens while the async writing to ZK still happens.
 - NOTE: It is fine to ignore a checkpoint that was completed, if we did not send ""notification complete"" and we are sure it will always be ignored. That would be as if the checkpoint never completed.
 - NOTE: It is not fine to ignore it and start from an earlier checkpoint if it will get committed later. That is the bug to prevent.

h2. Two steps to a cleaner solution

*(1) When the checkpoint is ready (all tasks acked, metadata written out), Checkpoint Coordinator transfers ownership to the CompletedCheckpointStore.*
 - That means the Checkpoint is removed from the ""Pending Checkpoints"" map and added to the CompletedCheckpointStore in one call in the main thread. If this is in one call, it is atomic against other modifications (cancellation, disposing checkpoints). Because the checkpoint is removed from the ""Pending Checkpoints"" map (not owned by the coordinator any more) it will not get cancelled during shutdown of the coordinator.

    ==> This is a very simple change

 

*(2) The addition to the CompletedCheckpointStore must be constant time and executed in the main thread*
 - That means that the CompletedCheckpointStore would put the Completed Checkpoint into a local list and then kick off the asynchronous request to add it to ZK.
 - If the JM looks up the latest checkpoint, it refers to that local list. That way all local components refer to the same status and do not exchange status asynchronously via an external system (ZK).

==> The change is that the CompletedCheckpointStore would not always repopulate itself from ZK upon ""restore checkpoint"", but keep the local state and only repopulate itself when the master gains leader status (and clears itself when leader status is lost).

==> This is a slightly more complex change, but not too big.
h2. Distributed Races and Corner Cases

I think this is an existing corner case issue, that existed before, but is now more likely due to this bug.

It exists, because JM failover can happen concurrently with ZK updates.
 * Once the call to add the checkpoint to ZK is sent off, the checkpoint might or might not get added to ZK (which is the distributed ground truth).
 * During that time, we cannot restore at all.
 ** If the JM already restored form the checkpoint, it sends ""restore state"" to the tasks, which is equivalent to ""notify checkpoint complete"" and materializes external side effects. If the addition to ZK then fails and the JM fails and another JM becomes leader, it will restore from an earlier checkpoint
 ** If the JM restores from an earlier checkpoint during that time, and then the ZK call completes, we have duplicate side effects.
 * In both cases we get fractured consistency or duplicate side effects

 

I see three possible solutions, which are not easy or not great

*(a) We cannot restore during the period where the checkpoint is in ""uncertain if committed"" state*
 * The CompletedCheckpointStore would need to keep the Checkpoint in a ""uncertain"" list initially, until the I/O executor call returns from adding the Checkpoint to ZK.
 * When asking the CompletedCheckpointStore for the latest checkpoint, it returns a CompletableFuture.
 * While the latest checkpoint is in ""uncertain"" state, the future cannot be completed. It completes after the ZK command completes (usually few 100ms). Restore operations would need to wait during that time.
 * There is a separate issue FLINK-16931 where ""loading metadata"" for the latest completed checkpoint can take long (seconds), because it is an I/O operations. This sounds like a similar issue, but I fear that the solution is more complex that anticipated in that issue.

*(b) Operators never commit side-effects are during restore.*
 * That would be a change of the current contract, and many operators would need to be adjusted.
 * It would be safe to restore already from the new checkpoint that is not yet committed to ZK, because the restore never creates side effects.
 * The side effects would only be committed after the ZK write is done (notify checkpoint complete), or after another later checkpoint is written to ZK (later checkpoint commits side effect for both the previous checkpoint and itself).
 * Downside: During failover, it means that side effects get committed later, because they are not committed during ""restore"" but only as part of the next completed checkpoint.

*(c) We actually make the ""add to ZK"" command blocking/synchronous for now*
 * This is how it used to be. It does not fully fix the issue, but makes it a bit more unlikely to happen.

 * 
 ** It prevents the situation where the local JM would restore that new checkpoint and then later the ZK write fails, which most likely the more common part of the issue.
 ** The JM could still lose leadership and another JM takes over, restoring an earlier checkpoint while the ZK commit goes through after that. This should be super rare, might even be impossible due to the way the ZK client works.

 * This blocks the RPC thread, which is bad. It can in the worst case lead to system crashes, if it blocks for too long.
 * We can mitigate this a bit, by running the actual I/O in a separate thread and aborting it with a timeout. Then, double checking ZK whether the update did go through or not.

==> It might be worth doing this while we have no better solution.;;;","24/Apr/20 09:28;rmetzger;Another case: https://api.travis-ci.org/v3/job/678609505/log.txt;;;","24/Apr/20 12:59;pnowojski;[~rmetzger] how could it happen? I've reverted the cause of this bug over a week ago:
https://issues.apache.org/jira/browse/FLINK-14971
Or do we have to revert even more code [~SleePy]? [~rmetzger] what was the commit that failed there? Did it include the reverting commits:
{code}
ffa4475685 [8 days ago] (apache/master) Revert ""[FLINK-14971][checkpointing] Handle ack/declined message of checkpoint"" [ifndef-SleePy]
2813be7a3e [8 days ago] Revert ""[FLINK-14971][checkpointing] Introduce main thread executor in CheckpointCoordinator to execute all non-IO operations instead of the timer thread"" [ifndef-SleePy]
e217a8c666 [8 days ago] Revert ""[hotfix] Harden ResumeCheckpointManuallyITCase"" [ifndef-SleePy]
6adbe94b0e [8 days ago] Revert ""[FLINK-14971][checkpointing] Make CompletedCheckpointStore thread-safe to avoid synchronization outside"" [ifndef-SleePy]
214896aeee [8 days ago] Revert ""[FLINK-14971][checkpointing] Remove coordinator-wide lock of CheckpointCoordinator"" [ifndef-SleePy]
5d5a29bbe1 [8 days ago] Revert ""[FLINK-14971][checkpointing] Remove lock of PendingCheckpoint and introduce IO lock for PendingCheckpoint"" [ifndef-SleePy]
cfe5a27dd4 [8 days ago] Revert ""[hotfix] Make the checkpoint resuming e2e case pass by increasing the retained checkpoints number"" [ifndef-SleePy]
d3fa2e5224 [8 days ago] Revert ""[FLINK-16945][checkpointing] Execute CheckpointFailureManager.FailJobCallback directly in main thread executor"" [ifndef-SleePy]
{code}
?;;;","24/Apr/20 13:09;rmetzger;I didn't realize that you've reverted the commits potentially causing this. The run is based on this commit https://github.com/apache/flink/commit/008e0afb3c62c059dcdf2c58a43cdd2e2d283512 (this is the run: https://travis-ci.org/github/apache/flink/jobs/678609505)
Can somebody review and approve this PR? It would help us debug this e2e test in the future. In current master, it is impossible to debug the e2e test: https://github.com/apache/flink/pull/11831;;;","24/Apr/20 14:05;pnowojski;Definitely your PR includes the reverted commits, so there is something to investigate here. Either there is another error with similar symptoms, or we haven't reverted enough commits. [~SleePy] [~yunta] what do you think has happened?

For one thing I forgot to mention in this ticket that I've reverted the commits. We also need to clean up the tickets for this issue. I wanted to close this bug, but we were discussing solutions here, but I guess that was a mistake - after reverting the commits and re-opening the original issue, we should migrated discussions there. So let's keep this ticket open for the investigation of your most recent report [~rmetzger].;;;","25/Apr/20 07:11;SleePy;Technically speaking, the scenario we discussed here should not happen with the reverted commits. The finalization of checkpoint is reverted to be executed synchronously and wrapped in the coordinator-wide lock. There shouldn't be race condition at all. On the other hand, the earlier commits of the refactoring are merged over 3 months ago. So to answer the question of [~pnowojski], I think we have reverted enough commits.

I have noticed that there are some logs:
{quote}kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]
 Killed TM @
{quote}
It seems that there is no TM process at some time. I guess it's not a normal scenario. The {{ha_tm_watchdog}} in common_ha.sh should start a new TM before killing an old one in this case. What if there is no TM process at all? Exited or killed unexpectedly? I'm not sure. I think there will be no enough TM to finish the testing case. Because the {{ha_tm_watchdog}} only starts a new TM if there are enough TMs,
{quote}local MISSING_TMS=$((EXPECTED_TMS-RUNNING_TMS))
 if [ ${MISSING_TMS} -eq 0 ]; then

    start a new TM only if we have exactly the expected number
    ""$FLINK_DIR""/bin/taskmanager.sh start > /dev/null
 fi
{quote}
I guess the failure cause is another one, maybe it's relevant to the ""no TM process"". But I can't tell what really happened in this case without any other logs. Is there any way we could find the JM logs? [~rmetzger];;;","27/Apr/20 07:15;rmetzger;No, there is no way to get the logs. The load-upload functionality is never executed, because the script hangs forever in an error case.
The VM executing the test has been destroyed.

We need to merge my PR, wait for the issue to appear again and then look at the logs.;;;","27/Apr/20 07:28;rmetzger;[~SleePy] thanks again for the analysis of the log output. I have filed a new ticket for the failure, as it is quite likely that this issue is not related to the broken checkpoint coordinator, but rather to a problem with the test scripts, or the TaskManagers. Let's track the analysis of this failure pattern in FLINK-17404.;;;","11/May/20 08:59;pnowojski;As the cause of this failure has been reverted some time ago, I'm closing this ticket.

Let's move the discussion about the long term fix to FLINK-14971  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart hangs,FLINK-16768,13293736,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,zjwang,zjwang,25/Mar/20 07:01,15/Sep/20 07:43,13/Jul/23 08:07,01/Sep/20 13:43,1.10.0,1.11.0,1.12.0,,,1.11.3,1.12.0,,,FileSystems,Tests,,,,0,pull-request-available,test-stability,,,"Logs: [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6584&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=d26b3528-38b0-53d2-05f7-37557c2405e4]
{code:java}
2020-03-24T15:52:18.9196862Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fd36c00b800 nid=0xc21 runnable [0x00007fd3743ce000]
2020-03-24T15:52:18.9197235Z    java.lang.Thread.State: RUNNABLE
2020-03-24T15:52:18.9197536Z 	at java.net.SocketInputStream.socketRead0(Native Method)
2020-03-24T15:52:18.9197931Z 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
2020-03-24T15:52:18.9198340Z 	at java.net.SocketInputStream.read(SocketInputStream.java:171)
2020-03-24T15:52:18.9198749Z 	at java.net.SocketInputStream.read(SocketInputStream.java:141)
2020-03-24T15:52:18.9199171Z 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)
2020-03-24T15:52:18.9199840Z 	at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:593)
2020-03-24T15:52:18.9200265Z 	at sun.security.ssl.InputRecord.read(InputRecord.java:532)
2020-03-24T15:52:18.9200663Z 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975)
2020-03-24T15:52:18.9201213Z 	- locked <0x00000000927583d8> (a java.lang.Object)
2020-03-24T15:52:18.9201589Z 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933)
2020-03-24T15:52:18.9202026Z 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)
2020-03-24T15:52:18.9202583Z 	- locked <0x0000000092758c00> (a sun.security.ssl.AppInputStream)
2020-03-24T15:52:18.9203029Z 	at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)
2020-03-24T15:52:18.9203558Z 	at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)
2020-03-24T15:52:18.9204121Z 	at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)
2020-03-24T15:52:18.9204626Z 	at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)
2020-03-24T15:52:18.9205121Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9205679Z 	at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
2020-03-24T15:52:18.9206164Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9206786Z 	at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)
2020-03-24T15:52:18.9207361Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9207839Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9208327Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9208809Z 	at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
2020-03-24T15:52:18.9209273Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9210003Z 	at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)
2020-03-24T15:52:18.9210658Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9211154Z 	at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$read$3(S3AInputStream.java:445)
2020-03-24T15:52:18.9211631Z 	at org.apache.hadoop.fs.s3a.S3AInputStream$$Lambda$42/1936375962.execute(Unknown Source)
2020-03-24T15:52:18.9212044Z 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
2020-03-24T15:52:18.9212553Z 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260)
2020-03-24T15:52:18.9212972Z 	at org.apache.hadoop.fs.s3a.Invoker$$Lambda$23/1457226878.execute(Unknown Source)
2020-03-24T15:52:18.9213408Z 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)
2020-03-24T15:52:18.9213866Z 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:256)
2020-03-24T15:52:18.9214273Z 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:231)
2020-03-24T15:52:18.9214701Z 	at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:441)
2020-03-24T15:52:18.9215443Z 	- locked <0x00000000926e88b0> (a org.apache.hadoop.fs.s3a.S3AInputStream)
2020-03-24T15:52:18.9215852Z 	at java.io.DataInputStream.read(DataInputStream.java:149)
2020-03-24T15:52:18.9216305Z 	at org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.read(HadoopDataInputStream.java:94)
2020-03-24T15:52:18.9216781Z 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
2020-03-24T15:52:18.9217187Z 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
2020-03-24T15:52:18.9217571Z 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
2020-03-24T15:52:18.9218108Z 	- locked <0x00000000926ea000> (a java.io.InputStreamReader)
2020-03-24T15:52:18.9218475Z 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
2020-03-24T15:52:18.9218876Z 	at java.io.BufferedReader.fill(BufferedReader.java:161)
2020-03-24T15:52:18.9219261Z 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
2020-03-24T15:52:18.9219890Z 	- locked <0x00000000926ea000> (a java.io.InputStreamReader)
2020-03-24T15:52:18.9220256Z 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
2020-03-24T15:52:18.9220914Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getContentsOfFile(HadoopS3RecoverableWriterITCase.java:423)
2020-03-24T15:52:18.9221704Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:411)
2020-03-24T15:52:18.9222457Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)
2020-03-24T15:52:18.9223222Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:330)
2020-03-24T15:52:18.9223817Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-24T15:52:18.9224232Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-24T15:52:18.9224729Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-24T15:52:18.9225160Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-24T15:52:18.9225675Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-03-24T15:52:18.9226171Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-03-24T15:52:18.9226682Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-03-24T15:52:18.9227187Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-03-24T15:52:18.9227661Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-03-24T15:52:18.9228145Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-03-24T15:52:18.9228718Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-03-24T15:52:18.9229112Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-03-24T15:52:18.9229582Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-03-24T15:52:18.9230029Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-03-24T15:52:18.9230525Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-03-24T15:52:18.9230963Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-03-24T15:52:18.9231546Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-03-24T15:52:18.9231999Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-03-24T15:52:18.9232432Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-03-24T15:52:18.9232862Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-03-24T15:52:18.9233307Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-03-24T15:52:18.9233833Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-03-24T15:52:18.9234284Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-03-24T15:52:18.9234700Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-03-24T15:52:18.9235076Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-03-24T15:52:18.9235599Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-03-24T15:52:18.9236124Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-03-24T15:52:18.9236648Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-03-24T15:52:18.9237167Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-03-24T15:52:18.9237688Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-03-24T15:52:18.9238244Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-03-24T15:52:18.9238745Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-03-24T15:52:18.9239202Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-03-24T15:52:18.9239549Z 
2020-03-24T15:52:18.9239794Z ""VM Thread"" os_prio=0 tid=0x00007fd36c260800 nid=0xc58 runnable {code}
 ",,dian.fu,gaoyunhaii,maguowei,rmetzger,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18374,,,,,,,,,,,,,,,,FLINK-17730,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 07:43:03 UTC 2020,,,,,,,,,,"0|z0cvlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 12:00;rmetzger;{code}
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.awaitPendingPartUploadToComplete(RecoverableMultiPartUploadImpl.java:224)
	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.awaitPendingPartsUpload(RecoverableMultiPartUploadImpl.java:215)
	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.snapshotAndGetRecoverable(RecoverableMultiPartUploadImpl.java:151)
	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.snapshotAndGetCommitter(RecoverableMultiPartUploadImpl.java:123)
	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.snapshotAndGetCommitter(RecoverableMultiPartUploadImpl.java:56)
	at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.closeForCommit(S3RecoverableFsDataOutputStream.java:167)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:409)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:337)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2798&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","09/Jun/20 06:42;rmetzger;	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:344)

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2947&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","10/Jun/20 06:47;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3111&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","10/Jun/20 12:12;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3145&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","10/Jun/20 18:48;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3182&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","12/Jun/20 06:02;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3342&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407;;;","15/Jun/20 06:38;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3435&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=603cb7fd-6f38-5c99-efca-877e1439232f;;;","16/Jun/20 19:28;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3611&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","08/Jul/20 02:15;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4314&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c];;;","09/Jul/20 01:43;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4351&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=e4f347ab-2a29-5d7c-3685-b0fcd2b6b051];;;","28/Jul/20 01:55;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4938&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8];;;","28/Jul/20 01:58;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4911&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=603cb7fd-6f38-5c99-efca-877e1439232f];;;","03/Aug/20 03:00;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5103&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","04/Aug/20 01:40;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5140&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=603cb7fd-6f38-5c99-efca-877e1439232f;;;","10/Aug/20 02:12;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5321&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=e4f347ab-2a29-5d7c-3685-b0fcd2b6b051];;;","20/Aug/20 01:37;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5720&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361;;;","21/Aug/20 09:31;gaoyunhaii;Hi, I read the pasted logs, it seems that failures here are also caused by ""Process produced no output for 900 seconds."", which seems to be duplicate with FLINK-18374 . I think we could mark this or 18374 as duplicated and only have one left.;;;","23/Aug/20 02:54;maguowei;After analyzing three fail cases I think that the `testRecoverWithStateWithMultiPart` is not cause of the failure. The test fails might be related to the monitor mechanism of the watchdog.

What I found is that
 # At the time there indeed is no output in the standoutput but the `testRecoverWithStateWithMultiPart` is just started or finished. At least one test process still worked.
 # There is one test process finished quickly because there are not many heavy tests for it.

My gut feeling is that maybe we could let watchdog monitor the mvn-log or rebalance the tests. I am not the expert of the building system so maybe [~rmetzger] could give some advice for this. :)

==============Following are three cases I analysis =================================

Following is the cases one
 # [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5720&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361 |https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5720&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361]
 # Logs-cron_scala212-connectors-1597869093
 # The watch dog kills the test at 2020-08-19 21:20:06.
 # From the mvn-2.log we can see that the test finished at 21:05:01,065
 # From the mvn-1.log we can see that it still worked at 21:14:29,149.  From the mvn-1.log testRecoverWithStateWithMultiPart finished at 21:14:29,149.

 

Following is the case two
 # [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5321&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=e4f347ab-2a29-5d7c-3685-b0fcd2b6b051]
 # Logs-cron_scala212-connectors-1597869093
 # The watch dog kills the test process at 2020-08-09 21:18:39
 # From the mvn-2.log we can see that the test finished at 21:05:01,06
 # From the mvn-1.log we can see that the test still worked at 21:14:29,149.From the mvn-1.log testRecoverWithStateWithMultiPart finished at 21:14:29,149

Following is the case three
 # [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5140&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=603cb7fd-6f38-5c99-efca-877e1439232f]
 # The watch dog kills the test processes at 2020-08-03 21:23:49
 # Logs-cron_jdk11-connectors-1596489841
 # From the mvn-1.log we can see that the test finished at 21:08:36,968
 # From the mvn-1.log we can see that the test still worked at 21:18:23,548.;;;","25/Aug/20 08:15;rmetzger;Thanks a lot for analyzing this failure. If it helps making this test more robust, we can monitor the mvn-log file for updates as well with the watchdog. I'll assign the ticket to myself to address this.;;;","01/Sep/20 13:43;rmetzger;Changed watchdog monitoring in https://github.com/apache/flink/commit/79e74f5ea1c8fe33bee1a5a142ce113d49ebae8e.;;;","13/Sep/20 10:13;dian.fu;Another instance on 1.11:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6473&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=2f99feaa-7a9b-5916-4c1c-5e61f395079e

[~rmetzger] Should we also cherry-pick the watchdog changes to 1.11?;;;","14/Sep/20 11:09;rmetzger;I agree that it is a good idea to cherry-pick the change. Master has been stable for 2 weeks with my change.
Sadly, cherry-picking is not possible without huge conflicts. I'm currently testing a poor-man's backport for 1.11 in my Azure account.;;;","14/Sep/20 12:30;dian.fu;Thanks a lot [~rmetzger]. (y)

Another instance on 1.11:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6476&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=2f99feaa-7a9b-5916-4c1c-5e61f395079e;;;","15/Sep/20 07:43;rmetzger;Backport to release-1.11: https://github.com/apache/flink/commit/408729731440d67b1d6399019bacc91c2a06fcc2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to read Hive table with RegexSerDe,FLINK-16767,13293718,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,25/Mar/20 05:32,14/Apr/20 15:04,13/Jul/23 08:07,14/Apr/20 07:53,1.10.0,,,,,1.10.1,1.11.0,,,Connectors / Hive,,,,,1,pull-request-available,,,,,,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11504: [FLINK-16767][hive] Failed to read Hive table with RegexSerDe
URL: https://github.com/apache/flink/pull/11504
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix reading Hive tables with RegexSerDe.
   RegexSerDe initialization fails with NPE. It's because we don't properly generate the table properties.
   
   
   ## Brief change log
   
     - Create table properties by calling Hive util method
     - Keep the table properties in `HiveTablePartition` and use it to initialize SerDe
     - Add test case
   
   
   ## Verifying this change
   
   Existing and added test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 07:43;githubbot;600","JingsongLi commented on pull request #11504: [FLINK-16767][hive] Failed to read Hive table with RegexSerDe
URL: https://github.com/apache/flink/pull/11504
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 09:04;githubbot;600","lirui-apache commented on pull request #11579: [FLINK-16767][hive] Failed to read Hive table with RegexSerDe
URL: https://github.com/apache/flink/pull/11579
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix reading Hive tables with RegexSerDe.
   RegexSerDe initialization fails with NPE. It's because we don't properly generate the table properties.
   
   
   ## Brief change log
   
     - Create table properties by calling Hive util method
     - Keep the table properties in HiveTablePartition and use it to initialize SerDe
     - Add test case
   
   
   ## Verifying this change
   
   Existing and added test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 10:00;githubbot;600","JingsongLi commented on pull request #11579: [FLINK-16767][hive] Failed to read Hive table with RegexSerDe
URL: https://github.com/apache/flink/pull/11579
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Apr/20 07:53;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,FLINK-16882,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 15:04:20 UTC 2020,,,,,,,,,,"0|z0cvhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/20 07:53;lzljs3620320;master: 1444fd68115594b872201242886b4d789f4b26a5

release-1.10: d2b05d50fc9134919a4a74992cbc64b21b8feed0;;;","14/Apr/20 13:23;ykt836;It is possible that this is the reason of FLINK-17138?;;;","14/Apr/20 15:04;lirui;It doesn't seem to be related -- the test failure happens when trying to get PK of the table. Actually I can't reproduce the issue locally. So need some time to investigate.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveModuleTest failed to compile on release-1.10,FLINK-16759,13293693,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,dian.fu,dian.fu,25/Mar/20 02:17,25/Mar/20 10:22,13/Jul/23 08:07,25/Mar/20 03:02,1.10.1,,,,,1.10.1,,,,Build System,Connectors / Hive,,,,0,pull-request-available,,,,"The cron task of release-1.10 failed to compile with the following exception:
{code}
23:36:45.190 [ERROR] /home/travis/build/apache/flink/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java:[158,45] constructor HiveModule in class org.apache.flink.table.module.hive.HiveModule cannot be applied to given types;
 required: java.lang.String
 found: no arguments
 reason: actual and formal argument lists differ in length
{code}

instance: [https://api.travis-ci.org/v3/job/666450476/log.txt]",,dian.fu,lzljs3620320,,,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #11503: [FLINK-16759][hive] HiveModuleTest failed to compile on release-1.10
URL: https://github.com/apache/flink/pull/11503
 
 
   
   ## What is the purpose of the change
   
   Fix `HiveModuleTest.testConstantReturnValue`
   
   ## Verifying this change
   
   This change is a trivial rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 02:32;githubbot;600","JingsongLi commented on pull request #11503: [FLINK-16759][hive] HiveModuleTest failed to compile on release-1.10
URL: https://github.com/apache/flink/pull/11503
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 03:02;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 03:02:58 UTC 2020,,,,,,,,,,"0|z0cvbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/20 02:19;dian.fu;cc [~lirui] [~lzljs3620320];;;","25/Mar/20 02:29;lzljs3620320;[~dian.fu] Sorry, I will fix this soon.;;;","25/Mar/20 03:02;lzljs3620320;release-1.10: 572717dbc35369d29441c2ad4eb0ba73aa803ee0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ridesharing example doesn't start,FLINK-16752,13293585,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igal,igal,igal,24/Mar/20 15:47,25/Mar/20 04:29,13/Jul/23 08:07,25/Mar/20 04:29,,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"The ridesharing simulator doesn't start, after changing the parent from spring-boot to statefun.",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,"igalshilman commented on pull request #70: [FLINK-16752] Fix Ridesharing example
URL: https://github.com/apache/flink-statefun/pull/70
 
 
   Chaning the maven parent from spring-boot to statefun-parent
   had missed a default configuration of the spring-boot-maven-plugin
   that triggers the repackage goal.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Mar/20 18:53;githubbot;600","tzulitai commented on pull request #70: [FLINK-16752] Fix Ridesharing example
URL: https://github.com/apache/flink-statefun/pull/70
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 04:18;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 04:29:21 UTC 2020,,,,,,,,,,"0|z0cuo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/20 04:29;tzulitai;Fixed.

master - 67b75ebd5dcd9d2d3b585395bad735a58eceb844
release-2.0 - 61d660971a0049af97f17993c50cd7ca9e7b1c40;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OrcSplitReaderUtil::logicalTypeToOrcType fails to create decimal type with precision < 10,FLINK-16740,13293450,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,24/Mar/20 06:14,24/Mar/20 12:45,13/Jul/23 08:07,24/Mar/20 12:44,1.10.0,,,,,1.10.1,1.11.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11492: [FLINK-16740][orc] OrcSplitReaderUtil::logicalTypeToOrcType fails to …
URL: https://github.com/apache/flink/pull/11492
 
 
   …create decimal type with precision < 10
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   `TypeDescription::DEFAULT_SCALE` is 10. `OrcSplitReaderUtil::logicalTypeToOrcType` sets precision before it sets scale. So if we create a decimal type with precision<10, it'll cause an exception.
   
   
   ## Brief change log
   
     - Sets scale first in OrcSplitReaderUtil::logicalTypeToOrcType.
     - Add test for decimal type.
   
   
   ## Verifying this change
   
   Added test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Mar/20 06:21;githubbot;600","JingsongLi commented on pull request #11492: [FLINK-16740][orc] OrcSplitReaderUtil::logicalTypeToOrcType fails to …
URL: https://github.com/apache/flink/pull/11492
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Mar/20 12:38;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 12:44:39 UTC 2020,,,,,,,,,,"0|z0cttc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/20 12:44;lzljs3620320;release-1.10: 123943c819a0408941f8d437bad46a7f68f00d5e

master: d6a0e4a6f390a22f8a54db4469d0ce64a8e5d045;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to call Hive UDF with constant return value,FLINK-16732,13293432,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,24/Mar/20 03:02,24/Mar/20 12:45,13/Jul/23 08:07,24/Mar/20 12:45,1.10.0,,,,,1.10.1,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"The following test fails:
{code}
	@Test
	public void test() throws Exception {
		tableEnv.unloadModule(""core"");
		tableEnv.loadModule(""hive"", new HiveModule());
		tableEnv.loadModule(""core"", CoreModule.INSTANCE);
		System.out.println(TableUtils.collectToList(tableEnv.sqlQuery(""select str_to_map('a:1,b:2,c:3',',',':')"")));
	}
{code}
With exception:
{noformat}
Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveJavaObject(WritableStringObjectInspector.java:46)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveJavaObject(WritableStringObjectInspector.java:26)
	at org.apache.flink.table.functions.hive.conversion.HiveInspectors.toFlinkObject(HiveInspectors.java:277)
	at org.apache.flink.table.functions.hive.conversion.HiveInspectors.toFlinkObject(HiveInspectors.java:319)
	at org.apache.flink.table.functions.hive.HiveGenericUDF.evalInternal(HiveGenericUDF.java:86)
	at org.apache.flink.table.functions.hive.HiveScalarFunction.eval(HiveScalarFunction.java:100)
......
{noformat}",,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11494: [FLINK-16732][hive] Failed to call Hive UDF with constant return value
URL: https://github.com/apache/flink/pull/11494
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix the issue that `str_to_map` fails if it returns a constant value.
   
   
   ## Brief change log
   
     - If a Hive UDF returns a constant value, i.e. the result OI is a constant OI, just use the constant writable value from the OI.
     - Add test case for `str_to_map`
   
   
   ## Verifying this change
   
   Existing and added test cases.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Mar/20 07:10;githubbot;600","JingsongLi commented on pull request #11494: [FLINK-16732][hive] Failed to call Hive UDF with constant return value
URL: https://github.com/apache/flink/pull/11494
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Mar/20 12:40;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 12:45:17 UTC 2020,,,,,,,,,,"0|z0ctpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/20 12:45;lzljs3620320;master: b7a83ff45ef0bf29c03616f542ff6d1abf22e2e2

release-1.10: f9f63a4c11cf13f56ad06d9da6231d711477f28e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix cast exception when having time point literal as parameters,FLINK-16727,13293298,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,Matrix42,Matrix42,23/Mar/20 12:50,30/Mar/20 06:23,13/Jul/23 08:07,30/Mar/20 06:23,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"I defined as ScalarFunction as follow:

 
{code:java}
public class DateFunc extends ScalarFunction {


    public String eval(Date date) {
      return date.toString();
    }

    @Override
    public TypeInformation<?> getResultType(Class<?>[] signature) {
        return Types.STRING;
    }

   @Override
   public TypeInformation<?>[] getParameterTypes(Class<?>[] signature) {
      return new TypeInformation[]{Types.INT};
   }
}
{code}
I ues it in sql: `select func(DATE '2020-11-12') as a from source` , Flink throws 'cannot cast 2020-11-12 as class java.time.LocalDate '

 

The full code is in the [^Flinktest.zip] Main class is com.lorinda.template.TestDateFunction","[^Flinktest.zip]",jark,libenchao,lsy,Matrix42,,,,,,,,,,,,,,,,,,"wuchong commented on pull request #11550: [FLINK-16727][table-planner-blink] Fix cast exception when having time point literal as parameters
URL: https://github.com/apache/flink/pull/11550
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix cast exception when having time point literal as parameters. For example,  the query `select func(DATE '2020-11-12') as a from source` will throw:
   
   ```
   java.lang.AssertionError: cannot cast 2020-03-27 as class java.time.LocalDate
   
   	at org.apache.calcite.sql.SqlLiteral.getValueAs(SqlLiteral.java:351)
   	at org.apache.calcite.sql.SqlCallBinding.getOperandLiteralValue(SqlCallBinding.java:217)
   	at org.apache.flink.table.planner.functions.utils.ScalarSqlFunction$$anon$1$$anonfun$1.apply(ScalarSqlFunction.scala:90)
   	at org.apache.flink.table.planner.functions.utils.ScalarSqlFunction$$anon$1$$anonfun$1.apply(ScalarSqlFunction.scala:86)
   ```
   
   The root cause is the literal parameter extraction in `ScalarSqlFunction#SqlReturnTypeInference`, however the extracted literal parameter is never used... So we can simply remove them. 
   
   ## Brief change log
   
   - remove the literal paramter extraction logic
   
   ## Verifying this change
   
   - add test in `UserDefinedScalarFunctionTest#testLiteralTemporalParameters`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/20 14:38;githubbot;600","wuchong commented on pull request #11550: [FLINK-16727][table-planner-blink] Fix cast exception when having time point literal as parameters
URL: https://github.com/apache/flink/pull/11550
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Mar/20 06:17;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/20 12:49;Matrix42;Flinktest.zip;https://issues.apache.org/jira/secure/attachment/12997433/Flinktest.zip",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 06:23:59 UTC 2020,,,,,,,,,,"0|z0csvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/20 02:30;jark;Could you show the full exception stack?;;;","25/Mar/20 08:55;Matrix42;Exception in thread ""main"" java.lang.AssertionError: cannot cast 2020-11-12 as class java.time.LocalDate
 at org.apache.calcite.sql.SqlLiteral.getValueAs(SqlLiteral.java:351)
 at org.apache.calcite.sql.SqlCallBinding.getOperandLiteralValue(SqlCallBinding.java:217)
 at org.apache.flink.table.planner.functions.utils.ScalarSqlFunction$$anon$1$$anonfun$1.apply(ScalarSqlFunction.scala:92)
 at org.apache.flink.table.planner.functions.utils.ScalarSqlFunction$$anon$1$$anonfun$1.apply(ScalarSqlFunction.scala:88)
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
 at scala.collection.immutable.Range.foreach(Range.scala:160)
 at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
 at scala.collection.AbstractTraversable.map(Traversable.scala:104)
 at org.apache.flink.table.planner.functions.utils.ScalarSqlFunction$$anon$1.inferReturnType(ScalarSqlFunction.scala:88)
 at org.apache.calcite.sql.SqlOperator.inferReturnType(SqlOperator.java:470)
 at org.apache.calcite.sql.SqlOperator.validateOperands(SqlOperator.java:437)
 at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:303)
 at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:219)
 at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5599)
 at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5586)
 at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
 at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1690)
 at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1675)
 at org.apache.calcite.sql.SqlAsOperator.deriveType(SqlAsOperator.java:133)
 at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5599)
 at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5586)
 at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
 at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1690)
 at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1675)
 at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:478)
 at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4104)
 at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3388)
 at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
 at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
 at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1007)
 at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:967)
 at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:216)
 at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:942)
 at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:649)
 at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:125)
 at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:104)
 at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:127)
 at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:85)
 at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:464)
 at com.lorinda.template.TestDateFunction.main(TestDateFunction.java:41)

Process finished with exit code 1;;;","25/Mar/20 11:10;jark;Thanks [~Matrix42], I think this is bug and should be fixed. I will take this issue. ;;;","30/Mar/20 06:23;jark;Fixed in 
 - master (1.11.0): f765ad09ae2b2aa478c887b988e11e92a8b730bd
 - 1.10.1: f619b77a182cdfb9653caf12e37d32f9949cbdd7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KvStateServerHandlerTest leaks Netty ByteBufs,FLINK-16718,13293255,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gjy,gjy,gjy,23/Mar/20 09:16,23/Mar/20 09:34,13/Jul/23 08:07,23/Mar/20 09:34,1.10.0,1.11.0,,,,1.10.1,1.11.0,,,Runtime / Queryable State,Tests,,,,0,pull-request-available,,,,The {{KvStateServerHandlerTest}} leaks Netty {{ByteBuf}} instances.,,gjy,,,,,,,,,,,,,,,,,,,,,"GJL commented on pull request #11453: [FLINK-16718][tests] Fix ByteBuf leak in KvStateServerHandlerTest
URL: https://github.com/apache/flink/pull/11453
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Mar/20 09:25;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13553,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 09:34:01 UTC 2020,,,,,,,,,,"0|z0csm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/20 09:34;gjy;1.10: 00f27c6b42cc7db29d5a71ad9872ef3ee27272e0
master: 50ee6554dffee784f6dbfeaba7b18a18bdba5659;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet columnar row reader read footer from wrong end,FLINK-16711,13293216,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,23/Mar/20 02:50,23/Mar/20 04:57,13/Jul/23 08:07,23/Mar/20 04:57,,,,,,1.11.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,"{code:java}
readFooter(conf, path, range(splitStart, splitLength))
{code}
Should be:
{code:java}
readFooter(conf, path, range(splitStart, splitStart + splitLength))
{code}
 ",,godfreyhe,lzljs3620320,,,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #11484: [FLINK-16711][parquet] Parquet columnar row reader read footer from wrong end
URL: https://github.com/apache/flink/pull/11484
 
 
   
   ## What is the purpose of the change
   
   Fix `ParquetColumnarRowSplitReader` read footer bug
   
   ## Brief change log
   
   `readFooter(conf, path, range(splitStart, splitLength))`
   Should be:
   `readFooter(conf, path, range(splitStart, splitStart + splitLength))`
   
   ## Verifying this change
   
   - Can not re-produce in test.
   - Manually test.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Mar/20 02:53;githubbot;600","JingsongLi commented on pull request #11484: [FLINK-16711][parquet] Parquet columnar row reader read footer from wrong end
URL: https://github.com/apache/flink/pull/11484
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Mar/20 04:57;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 04:57:47 UTC 2020,,,,,,,,,,"0|z0csdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/20 04:57;lzljs3620320;Master: 816bff8db8ff34caf1ee37ac3dd4897835e744dc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log Upload blocks Main Thread in TaskExecutor,FLINK-16710,13293164,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangsan,gjy,gjy,22/Mar/20 12:22,03/Apr/20 19:38,13/Jul/23 08:07,03/Apr/20 19:38,1.10.0,,,,,1.11.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Uploading logs to the BlobServer blocks the TaskExecutor's main thread. We should introduce an IO thread pool that carries out file system accesses (listing files in a directory, checking if file exists, uploading files).

Affected RPCs:
 * {{TaskExecutor#requestLogList(Time)}}
 * {{TaskExecutor#requestFileUploadByName(String, Time)}}
 * {{TaskExecutor#requestFileUploadByType(FileType, Time)}}",,felixzheng,gjy,uce,wangsan,wind_ljy,zhuzh,,,,,,,,,,,,,,,,"jrthe42 commented on pull request #11571: [FLINK-16710][runtime] Log Upload blocks Main Thread in TaskExecutor
URL: https://github.com/apache/flink/pull/11571
 
 
   ## What is the purpose of the change
   
   TaskExecutors upload logs to BlobServer in main thread, which may cause main thread blocked. This pr intraduce a separate IO thread pool to carries out file system accesses (listing files in a directory, checking if file exists, uploading files).
   
   
   ## Brief change log
     - *TaskExecutors upload logs to BlobServer using a separate IO thread pool*
   
   ## Verifying this change
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 03:15;githubbot;600","GJL commented on pull request #11571: [FLINK-16710][runtime] Log Upload blocks Main Thread in TaskExecutor
URL: https://github.com/apache/flink/pull/11571
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Apr/20 19:36;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16302,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 19:38:03 UTC 2020,,,,,,,,,,"0|z0cs20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/20 03:26;wangsan;Hi [~gjy], I can help with this issue if no one is working on this.;;;","30/Mar/20 07:34;gjy;[~wangsan] You can work on this, and I have assigned the issue to you. Let me know if you have questions.;;;","03/Apr/20 19:38;gjy;master: dbf0c4c5914d11b6c1209f089ed014db8cd733cb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix potential memory leak of rest server when using session/standalone cluster for version 1.8,FLINK-16707,13293144,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiangbo,jiangbo,jiangbo,22/Mar/20 08:02,24/Mar/20 15:32,13/Jul/23 08:07,24/Mar/20 15:31,1.8.3,,,,,1.8.4,,,,Runtime / REST,,,,,0,pull-request-available,,,,"This issue has been fixed, but not in version 1.8. We are using version 1.8, so we hope to fix this problem in version 1.8 as well

https://issues.apache.org/jira/browse/FLINK-14139",,jiangbo,trohrmann,,,,,,,,,,,,,,,,,,,,"lijiangbo commented on pull request #11477: [FLINK-16707]Fix potential memory leak of rest server when using sess…
URL: https://github.com/apache/flink/pull/11477
 
 
   This issue has been fixed, but I hope to fix it in version 1.8 as well
   [https://github.com/apache/flink/pull/9750](https://github.com/apache/flink/pull/9750)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Mar/20 08:15;githubbot;600","tillrohrmann commented on pull request #11477: [FLINK-16707]Fix potential memory leak of rest server when using sess…
URL: https://github.com/apache/flink/pull/11477
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Mar/20 15:32;githubbot;600",,,,,,,,3600,2400,1200,33%,3600,2400,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 15:31:42 UTC 2020,,,,,,,,,,"0|z0crxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/20 15:31;trohrmann;Fixed via be4cb1a39ba6ec4b23f860f13ae18c47a8b69f81;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalExecutor tears down MiniCluster before client can retrieve JobResult,FLINK-16705,13293100,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mxm,mxm,mxm,21/Mar/20 13:17,03/Apr/20 11:19,13/Jul/23 08:07,03/Apr/20 11:19,1.10.0,,,,,1.10.1,1.11.0,,,Client / Job Submission,,,,,0,pull-request-available,,,,"There is a race condition in {{LocalExecutor}} between (a) shutting down the cluster when the job has finished and (b) the client which retrieves the result of the job execution.

This was observed in Beam, running a large test suite with the Flink Runner.

We should make sure the job result retrieval and the cluster shutdown do not interfere.",,eleanore0102,leonard,liyu,mxm,nikobearrr,tison,trohrmann,,,,,,,,,,,,,,,"mxm commented on pull request #11473: [FLINK-16705] Ensure MiniCluster shutdown does not interfere with JobResult retrieval
URL: https://github.com/apache/flink/pull/11473
 
 
   ## What is the purpose of the change
   
   There is a race condition in `LocalExecutor` between (a) shutting down the
   cluster when the job has finished and (b) the client which retrieves the result
   of the job execution.
   
   This was observed in Beam, running a large test suite with the Flink Runner.
   
   ## Brief change log
   
   We should make sure the job result retrieval and the cluster shutdown do not
   interfere. This adds a PerJobMiniClusterClient which guarantees that.
   
   
   ## Verifying this change
   
   This change added tests, see `PerJobMiniClusterClientTest` and `LocalExecutorITCase`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (JavaDocs)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Mar/20 13:34;githubbot;600","tillrohrmann commented on pull request #11473: [FLINK-16705] Ensure MiniCluster shutdown does not interfere with JobResult retrieval
URL: https://github.com/apache/flink/pull/11473
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Apr/20 11:18;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,BEAM-9631,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 11:19:45 UTC 2020,,,,,,,,,,"0|z0crns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/20 13:39;tison;It seems the problem is that we want to support {{executeAsync}} while keep {{JobClient}} ""stateless"" so that we don't have to manage resource. Thus, we add a terminate future in {{LocalExecutor}} to terminate the cluster when the job finished, which, as described here, possibly race with a separated result request.

My opinion would be still we(and the user) manage {{JobClient}} lifecycle so that we ensure the result delivered in {{execute}} (blocking version). Alternative is we make something like {{MiniDispatcher}} to special handle it.

What do you think? cc [~kkl0u] [~aljoscha].;;;","21/Mar/20 13:45;tison;Ref

https://github.com/apache/flink/commit/c927e176f35b9db0f9daec7b67ccdb41d8e7d9f5
https://github.com/apache/flink/commit/41176ba0189ef2782449a1b7d4d074adc8a4af6f;;;","21/Mar/20 14:35;mxm;Hi [~tison]. Not sure if you saw the PR already. I've opened it shortly after opening this issue. I went for a per-job mini cluster client which handles the shutdown and caches the result. This seemed like the best solution to me and avoids altering any of the JobClient logic.;;;","21/Mar/20 14:37;tison;Hi [~mxm]! Yes I checked the PR. It is the case where I mentioned as ""special handling"". And it should work.;;;","03/Apr/20 11:19;trohrmann;Fixed via

1.11.0: db81417b8616850aa36b34fecafcc275db2cb10a
1.10.1: 128f5f7bd1da9ae50973115b38f4def3adc331e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AkkaRpcActor state machine does not record transition to terminating state.,FLINK-16703,13293054,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,trohrmann,dchmelev,dchmelev,21/Mar/20 01:19,31/Mar/20 10:46,13/Jul/23 08:07,31/Mar/20 10:46,1.10.0,1.11.0,1.8.0,1.9.0,2.0.0,1.10.1,1.11.0,1.9.3,,Runtime / Coordination,,,,,0,pull-request-available,,,,"As part of FLINK-11551, the state machine of AkkaRpcActor has been updated to include 'terminating' and 'terminated' states. However, when actor termination request is handled, the resulting 'terminating' state is not updated by the FSM.

[https://github.com/apache/flink/blame/master/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActor.java#L175]

As a side-effect, {{isRunning()}} predicate can return that the actor is still running after terminate was initiated and to still handle messages.

I believe the fix is trivial and the private field {{state}} should be updated with the return value of the call to {{state.terminate()}}.

Feel free to adjust the priority accordingly.",,dchmelev,trohrmann,,,,,,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #11549: [FLINK-16703][rpc] Set AkkaRpcActor state to TERMINATING when terminating
URL: https://github.com/apache/flink/pull/11549
 
 
   ## What is the purpose of the change
   
   This commit fixes a bug where we did not update the state of the AkkaRpcActor
   in case of terminating it. Moreover, this commit fixes the problem that the
   onStop action could have been called multiple times.
   
   ## Verifying this change
   
   - Added `AkkaRpcActorTest#callsOnStopOnlyOnce`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/20 13:38;githubbot;600","tillrohrmann commented on pull request #11549: [FLINK-16703][rpc] Set AkkaRpcActor state to TERMINATING when terminating
URL: https://github.com/apache/flink/pull/11549
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 10:41;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 10:46:07 UTC 2020,,,,,,,,,,"0|z0crdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/20 11:05;trohrmann;Yes, you are right [~dchmelev]. Thanks a lot for reporting this issue.;;;","31/Mar/20 10:46;trohrmann;Fixed via

1.11.0: 1e0db6425f18b8d69b69176b9cb75e8b000bf768
1.10.1: 8773895b6b7cf75ee642ebd1496c0566a0ddaa40
1.9.3: b71c4002d3942f48807d2b5c0ac64ee3e55b2f59;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoint trigger documentation is insufficient,FLINK-16696,13292877,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,roman,roman,20/Mar/20 09:27,08/Apr/20 21:35,13/Jul/23 08:07,08/Apr/20 21:35,1.9.0,,,,,1.10.1,1.11.0,1.9.3,,Documentation,Runtime / REST,,,,0,pull-request-available,,,,"Rest api docs doesn't provide enough details about /jobs/:jobid/savepoints/:triggerid endpoint (how to get completed savepoint location):

[https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/rest_api.html#api]

 

Javadoc covers it better:

[https://ci.apache.org/projects/flink/flink-docs-release-1.10/api/java/index.html?org/apache/flink/runtime/rest/handler/job/savepoints/SavepointHandlers.html]

 

[http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Savepoint-Location-from-Flink-REST-API-td33808.html]

 ",,alangford,klion26,roman,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11658: [FLINK-16696][rest] Properly document async responses 
URL: https://github.com/apache/flink/pull/11658
 
 
   Fixes an issue where the REST API status response for asynchronous operations wasn't documented properly.
   
   The async operation result has a generic parameter, which obviously doesn't work with the REST API generation since it relies on classes. As a result the generator always considered the inner (generic) result as an `Object`, basically not documenting anything.
   
   This PR provides a stop-gap solution by hard-coding additional logic for `AsynchronousOperationStatusMessageHeaders`.
   I exposed the inner class by making `AsynchronousOperationStatusMessageHeaders#getValueClass` public, generate a separate schema for it, and overwrite the corresponding property entry of the outer schema.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Apr/20 13:39;githubbot;600","zentol commented on pull request #11658: [FLINK-16696][rest] Properly document async responses 
URL: https://github.com/apache/flink/pull/11658
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Apr/20 20:44;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 21:35:39 UTC 2020,,,,,,,,,,"0|z0cqag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/20 17:58;alangford;The comments in the Javadocs are inconsistent with the behavior of the API:

[https://ci.apache.org/projects/flink/flink-docs-release-1.10/api/java/index.html?org/apache/flink/runtime/rest/handler/job/savepoints/SavepointHandlers.html]

The Javadoc claims that the response JSON will be of the form (note that the second key of the object is ""savepoint""):

{
 {{  ""status"": {}}
 {{    ""id"": ""completed""}}
 {{  },}}
 {{  ""savepoint"": {}}
 {{    ...}}
        }
 {{}}}

However, the API actually returns ""operation"" where the javadocs say ""savepoint"":

{
 {{  ""status"": {}}
 {{    ""id"": ""completed""}}
 {{  },}}
 {{  ""operation"": {}}
 {{    ...}}
        }
 {{}}}

The REST API docs actually have this correct. So the javadocs need to be corrected AND the REST API docs need to be given more detail as stated in the original description.;;;","08/Apr/20 21:35;chesnay;master:
2795af3e8ebf3c085b901feacc58417ac7a197bd
4e51682dc18baad424d880f5c34ab5d65cd8f802
1.10:
27b057176b9ad576ab6451022ae478b6346e7d0d
b2122fc0302dfa8532acc43b096007c9fb9cc70d
1.9:
709556445f5f77ec6f28217b7c5ed5eb63505996
66d3adfe76edbc836786f7f076295cd8fbb10561;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resuming Externalized Checkpoint end-to-end test failed on travis,FLINK-16694,13292874,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,pnowojski,pnowojski,20/Mar/20 09:25,14/Sep/20 02:51,13/Jul/23 08:07,09/Jun/20 12:23,1.9.2,,,,,1.12.0,,,,Runtime / Metrics,Tests,,,,0,pull-request-available,test-stability,,,"Running 'Resuming Externalized Checkpoint (rocks, incremental, scale down) end-to-end test' failed on travis (release-1.9 branch) with the error:

{code:java}
The job exceeded the maximum log length, and has been terminated.
{code}

https://api.travis-ci.org/v3/job/664469537/log.txt
https://travis-ci.org/github/apache/flink/builds/664469494

This error is probably because of metrics logging and it's masking some other underlying issue, potentially FLINK-16695 as they happened in the same build.
",,guoyangze,klion26,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16695,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 14 02:51:31 UTC 2020,,,,,,,,,,"0|z0cq9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/20 09:29;pnowojski;another instance in the same build ( https://api.travis-ci.org/v3/job/664469529/log.txt )
;;;","27/May/20 13:57;chesnay;We now only print the first 500 lines of each log to prevent the build from terminating.
I will close the issue for now; let's revisit it if/once it occurs the next time.

master: a5ed8860cb977a330555b3883ae0099a8d229b02
1.11: f7db8b351a86247519c72406009bd1f48608c02e 
1.10: 32c9731b245448dac349a0f5870e9db2fd88aa11
1.9: e6f2a25f03b8f5949855807cdf90f5960e0207af ;;;","02/Jun/20 06:24;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2524&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8

In this run, the test stalled forever:
{code}
2020-06-01T20:45:27.8122406Z Job (255b5425f819c9ab8619fd158c547919) is running.
2020-06-01T20:45:27.8129405Z Waiting for job (255b5425f819c9ab8619fd158c547919) to have at least 1 completed checkpoints ...
2020-06-02T00:00:24.2059925Z ##[error]The operation was canceled.
{code}

I propose to run it with {{run_test_with_timeout}} so that we get some logs.;;;","03/Jun/20 05:37;rmetzger;It happened again, on the Hadoop 3.1.3 profile: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2587&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8

Most likely it's the same issue as FLINK-17404.;;;","09/Jun/20 12:23;trohrmann;Merged fix which runs the test with a 15 minute timeout via fa70cef5e145454221caee201146030a924e5cb8;;;","14/Sep/20 02:51;guoyangze;[~chesnay] Hi, I just wonder does it make more sense to print the last 500 lines? The clue is more likely to be there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingFileSink builder does not work with Scala,FLINK-16684,13292710,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,19/Mar/20 13:51,19/Mar/20 18:55,13/Jul/23 08:07,19/Mar/20 16:39,1.10.0,,,,,1.10.1,1.11.0,,,API / Scala,Connectors / FileSystem,,,,0,pull-request-available,,,,"The {{StreamingFileSink}} builders don't work with Scala as they lose with every {{with*}} method more type information. For example, the following example does not compile:

{code}
val sink = StreamingFileSink.forRowFormat(new Path(""s3a://123""), new Encoder[String] {
      override def encode(element: String, stream: OutputStream): Unit = ???
    }).withRollingPolicy(
      DefaultRollingPolicy.builder()
        .withRolloverInterval(TimeUnit.MINUTES.toMinutes(5))
        .withInactivityInterval(TimeUnit.MINUTES.toMinutes(5))
        .withMaxPartSize(128 * 1024 * 1024)
        .build()
    ).withBucketAssigner(
      new BucketAssigner[String, String] {
        override def getBucketId(element: String, context: BucketAssigner.Context): String = ???
        override def getSerializer: SimpleVersionedSerializer[String] = ???
      }
    ).build();
{code}

The problem seems to be that Scala does type inference slightly differently than Java. I believe that the unspecified {{RowFormatBuilder}} type cannot be properly resolved.",,gaoyunhaii,maguowei,trohrmann,,,,,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #11454: [FLINK-16684] Fix StreamingFileSink builder compilation for Scala
URL: https://github.com/apache/flink/pull/11454
 
 
   ## What is the purpose of the change
   
   This commit introduces a new type name for the row and bulk format
   StreamingFileSink builders in order to solve the compilation problem
   of Scala when using generic types with the self-type idiom.
   
   ## Verifying this change
   
   Added `StreamingFileSinkTest.testStreamingFileSinkRowFormatBuilderCompiles` and `StreamingFileSinkTest.testStreamingFileSinkBulkFormatBuilderCompiles`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 14:13;githubbot;600","tillrohrmann commented on pull request #11454: [FLINK-16684] Fix StreamingFileSink builder compilation for Scala
URL: https://github.com/apache/flink/pull/11454
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 16:34;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,FLINK-16616,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 16:39:45 UTC 2020,,,,,,,,,,"0|z0cp9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/20 16:39;trohrmann;Fixed via

1.11.0: 067c044cca59f87110f14e021a8fd04d58d0abd6
1.10.1: 06859cc2579ee4b0b0a12ee5529d4d12ea6e20aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hbase Connector Doc Error,FLINK-16678,13292665,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sulei,sulei,sulei,19/Mar/20 09:48,13/Apr/21 20:41,13/Jul/23 08:07,19/Mar/20 11:06,1.10.0,,,,,1.11.0,,,,Documentation,,,,,0,documentation,pull-request-available,,,"!截屏2020-03-1917.41.57.png!

this word should be queried.",,jark,sulei,,,,,,,,,,,,,,,,,,,,"ThunderSuuuuu commented on pull request #11452: [FLINK-16678][Document/HBase Connector]fix the word error
URL: https://github.com/apache/flink/pull/11452
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 10:02;githubbot;600","wuchong commented on pull request #11452: [FLINK-16678][Document/HBase Connector]fix the word error
URL: https://github.com/apache/flink/pull/11452
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 11:05;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/20 09:43;sulei;截屏2020-03-1917.41.57.png;https://issues.apache.org/jira/secure/attachment/12997098/%E6%88%AA%E5%B1%8F2020-03-1917.41.57.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 11:06:03 UTC 2020,,,,,,,,,,"0|z0cozc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/20 11:06;jark;Fixed in master (1.11.0): 4649ddd27e90b16d00076299e814a3d0541eb099;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_pipeline_from_invalid_json is failing Azure,FLINK-16676,13292644,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hequn8128,pnowojski,pnowojski,19/Mar/20 08:27,19/Mar/20 14:56,13/Jul/23 08:07,19/Mar/20 14:42,1.11.0,,,,,1.11.0,,,,API / Python,,,,,0,pull-request-available,test-stability,,,"Seems like all recent builds are failing:
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6376&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=165fcdc0-86f6-5c68-d844-b25f91686149
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6375&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=165fcdc0-86f6-5c68-d844-b25f91686149
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6372&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=165fcdc0-86f6-5c68-d844-b25f91686149

{code:java}
=================================== FAILURES ===================================
____________ ValidationPipelineTest.test_pipeline_from_invalid_json ____________

self = <pyflink.ml.tests.test_pipeline.ValidationPipelineTest testMethod=test_pipeline_from_invalid_json>

    def test_pipeline_from_invalid_json(self):
        invalid_json = '[a:aa]'
    
        # load json
        p = Pipeline()
        with self.assertRaises(RuntimeError) as context:
            p.load_json(invalid_json)
        exception_str = str(context.exception)
    
        # only assert key error message as the whole message is very long.
        self.assertTrue(
            'Cannot load the JSON as either a Java Pipeline or a Python Pipeline.'
            in exception_str)
        self.assertTrue(

{code}
",,hequn8128,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,"hequn8128 commented on pull request #11450: [FLINK-16676][python][test] Fix ValidationPipelineTest.test_pipeline_from_invalid_json failure on Azure
URL: https://github.com/apache/flink/pull/11450
 
 
   Do not review now. Add some log to see the detailed error message. 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 09:15;githubbot;600","hequn8128 commented on pull request #11450: [FLINK-16676][python][test] Fix ValidationPipelineTest.test_pipeline_from_invalid_json failure on Azure
URL: https://github.com/apache/flink/pull/11450
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 14:41;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 14:56:07 UTC 2020,,,,,,,,,,"0|z0couo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/20 08:58;hequn8128;Looking into the problem now.;;;","19/Mar/20 11:12;hequn8128;The problem is the detailed error message would be different in different environments. 
In travis:
{code}
An error occurred while calling o0.loadJson.
{code}
but in Azure:
{code}
An error occurred while calling o1095.loadJson
{code}

To solve the problem, we can check the general error message instead of the very detailed one in the test.;;;","19/Mar/20 14:42;hequn8128;Resolved in 1.11.0 via b9943dc47761ef8ad2c620350125351309047052

[~pnowojski] Thanks a lot for reporting the issue. :);;;","19/Mar/20 14:56;pnowojski;Thanks for fixing it quickly :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableEnvironmentITCase. testClearOperation fails on travis nightly build,FLINK-16675,13292636,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,liyu,liyu,19/Mar/20 06:52,02/Apr/20 10:42,13/Jul/23 08:07,24/Mar/20 10:14,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / API,,,,,0,pull-request-available,test-stability,,,"Observed below error in release-1.10 crone build:
{code}
00:39:02.215 [ERROR] testClearOperation[batch](org.apache.flink.table.api.TableEnvironmentITCase)  Time elapsed: 0.081 s  <<< ERROR!
org.apache.flink.api.common.InvalidProgramException: The implementation of the CollectionInputFormat is not serializable. The object probably contains or references non serializable fields.
	at org.apache.flink.table.api.TableEnvironmentITCase.testClearOperation(TableEnvironmentITCase.scala:154)
Caused by: java.lang.RuntimeException: Row arity of from does not match serializers.
	at org.apache.flink.table.api.TableEnvironmentITCase.testClearOperation(TableEnvironmentITCase.scala:154)
{code}
https://api.travis-ci.org/v3/job/664048405/log.txt",,jark,lirui,liyu,lzljs3620320,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11449: [FLINK-16675][test] Fix TableEnvironmentITCase.testClearOperation fai…
URL: https://github.com/apache/flink/pull/11449
 
 
   …lure
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix TableEnvironmentITCase.testClearOperation test failure
   
   
   ## Brief change log
   
     - Reset the collection data in `TestCollectionTableFactory`
   
   
   ## Verifying this change
   
   CRON test
   
   ## Does this pull request potentially affect one of the following parts:
   
     NA
   
   ## Documentation
   
    NA
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 09:08;githubbot;600","lirui-apache commented on pull request #11451: [FLINK-16675][test] Fix TableEnvironmentITCase.testClearOperation fai…
URL: https://github.com/apache/flink/pull/11451
 
 
   …lure
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix TableEnvironmentITCase.testClearOperation test failure
   
   
   ## Brief change log
   
     - Reset the collection data in `TestCollectionTableFactory`
   
   
   ## Verifying this change
   
   CRON test
   
   ## Does this pull request potentially affect one of the following parts:
   
   NA
   
   ## Documentation
   
   NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 09:58;githubbot;600","JingsongLi commented on pull request #11449: [FLINK-16675][test] Fix TableEnvironmentITCase.testClearOperation fai…
URL: https://github.com/apache/flink/pull/11449
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 12:31;githubbot;600","JingsongLi commented on pull request #11451: [FLINK-16675][test] Fix TableEnvironmentITCase.testClearOperation fai…
URL: https://github.com/apache/flink/pull/11451
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 12:32;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,FLINK-16433,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 10:14:10 UTC 2020,,,,,,,,,,"0|z0cosw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/20 06:57;liyu;From the commit history, this should be a regression of FLINK-16433.
[~lirui] Could you take a look here? Thanks.;;;","19/Mar/20 09:15;lirui;I can't reproduce the failure by running this specific case. So I suspect it's due to test fork reuse. I added some reset logic in the PR. Is there a way to verify the change, like manually trigger the cron job?;;;","19/Mar/20 12:33;lzljs3620320;release-1.10: e9c0dd2511adc1d09559434864ed268352a355d1

master: e19bc924a7a6c7e4522b850e2020e2dc4847da57

Let's observe whether it has been fixed.;;;","24/Mar/20 10:14;liyu;The issue disappeared from recent nightly builds thus closing it. Thanks [~lirui] and [~lzljs3620320] for the efforts!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to set DataStreamSource parallelism to default (-1),FLINK-16664,13292586,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,18/Mar/20 23:25,19/Mar/20 11:49,13/Jul/23 08:07,19/Mar/20 11:49,1.10.0,,,,,1.10.1,1.11.0,,,API / DataStream,,,,,0,pull-request-available,,,,"A hotfix part of FLINK-14405 actually breaks setting the parallelism to its default value for datastream sources, i.e. using value {{-1}}. This is because of a small typo: instead of
{code:java}
OperatorValidationUtils.validateParallelism(parallelism, isParallel);  {code}
this is called in org.apache.flink.streaming.api.datastream.DataStreamSource#setParallelism:
{code:java}
OperatorValidationUtils.validateMaxParallelism(parallelism, isParallel); {code}",,nkruber,trohrmann,,,,,,,,,,,,,,,,,,,,"NicoK commented on pull request #11446: [FLINK-16664][stream] fix wrong check for DataStreamSource#setParallelism()
URL: https://github.com/apache/flink/pull/11446
 
 
   ## What is the purpose of the change
   
   This fixes a regression in the DataStream API for being unable to call
   ```
   DataStreamSource#setParallelism(-1)
   ```
   which should set the cluster/job default. It was introduced by https://github.com/apache/flink/commit/4a5e8c083969cad215819ad6a3a0d2919e8c2d5e.
   
   @xintongsong actually, I'm also a bit unsure on some other things from that commit, like `OperatorValidationUtils#validateMaxParallelism(int, boolean)` using an upper bound of `Integer.MAX_VALUE` instead of `Transformation#UPPER_BOUND_MAX_PARALLELISM`. In the end, it is not too much of a problem since this is actually checked twice in `SingleOutputStreamOperator#setMaxParallelism`, once there and a second time in the transformation which uses the correct upper bound. In the end, that part was only copied from the old code which did not have an upper bound check there and therefore, I did not change this here either.
   
   When merging, please also apply to release-1.10
   
   ## Brief change log
   
   - change `DataStreamSource#setParallelism()` to call `OperatorValidationUtils.validateParallelism` instead of `OperatorValidationUtils.validateMaxParallelism`
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/20 23:43;githubbot;600","tillrohrmann commented on pull request #11446: [FLINK-16664][stream] fix wrong check for DataStreamSource#setParallelism()
URL: https://github.com/apache/flink/pull/11446
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 11:48;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 11:49:27 UTC 2020,,,,,,,,,,"0|z0cohs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/20 11:49;trohrmann;Fixed via

1.11.0: a56738fdaf4ebf89784a31fe1317fd4cd97d4de2
1.10.1: 68abb11c23527c4053ee2a50d1d4621fa8ecdccc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docs version 1.10 missing from version picker dropdown,FLINK-16663,13292519,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,sewen,sewen,18/Mar/20 16:11,25/Mar/20 09:19,13/Jul/23 08:07,25/Mar/20 09:19,1.11.0,,,,,1.11.0,,,,Documentation,,,,,0,pull-request-available,,,,"When going to the latest master docs [1], the documentation version 1.10 is not shown in the ""Pick Docs Version"" dropdown menu.

[1] https://ci.apache.org/projects/flink/flink-docs-master/",,liyu,sewen,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11506: [FLINK-16663][docs] Interpret versions as strings
URL: https://github.com/apache/flink/pull/11506
 
 
   Fixes an issue in our documentation setup where the `previous_version` keys were interpreted as floats, resulting in a clash between `1.1` and `1.10`. With this PR these versions are interpreted as strings, preventing this.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 08:19;githubbot;600","zentol commented on pull request #11506: [FLINK-16663][docs] Interpret versions as strings
URL: https://github.com/apache/flink/pull/11506
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 09:18;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 09:19:02 UTC 2020,,,,,,,,,,"0|z0co2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/20 16:22;chesnay;Given that the picked order is broken (1.1 being at the very top) my guess is that yaml interprets {{1.1}} and {{1.10}} in the [config|https://github.com/apache/flink/blob/master/docs/_config.yml#L66] as floats, cuts of the trailing zero and treats them both as {{1.1}} .;;;","19/Mar/20 10:40;sewen;Ah, a classic. I guess putting this in quotes might help?;;;","19/Mar/20 11:30;chesnay;I would think so, yes.;;;","25/Mar/20 09:19;chesnay;master: e5bca025adafad03d5d83fe7b3c6aa673f613fdb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blink Planner failed to generate JobGraph for POJO DataStream converting to Table (Cannot determine simple type name),FLINK-16662,13292496,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zhanglibing1990@126.com,chenxyz,chenxyz,18/Mar/20 14:51,19/Nov/20 04:24,13/Jul/23 08:07,22/Apr/20 16:56,1.10.0,,,,,1.10.1,1.11.0,,,Client / Job Submission,Table SQL / Planner,,,,0,pull-request-available,,,,"When using Blink Palnner to convert a POJO DataStream to a Table, Blink will generate and compile the SourceConversion$1 code. If the Jar task is submitted to Flink, since the UserCodeClassLoader is not used when generating the JobGraph, the ClassLoader(AppClassLoader) of the compiled code cannot load the POJO class in the Jar package, so the following error will be reported:

 
{code:java}
Caused by: org.codehaus.commons.compiler.CompileException: Line 27, Column 174: Cannot determine simple type name ""net""Caused by: org.codehaus.commons.compiler.CompileException: Line 27, Column 174: Cannot determine simple type name ""net"" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6746) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6507) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6520) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6520) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6520) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6520) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6520) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6520) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6520) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6520) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6486) at org.codehaus.janino.UnitCompiler.access$13800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$21$1.visitReferenceType(UnitCompiler.java:6394) at org.codehaus.janino.UnitCompiler$21$1.visitReferenceType(UnitCompiler.java:6389) at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3917) at org.codehaus.janino.UnitCompiler$21.visitType(UnitCompiler.java:6389) at org.codehaus.janino.UnitCompiler$21.visitType(UnitCompiler.java:6382) at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3916) at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6382) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7009) at org.codehaus.janino.UnitCompiler.access$15200(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$21$2.visitCast(UnitCompiler.java:6425) at org.codehaus.janino.UnitCompiler$21$2.visitCast(UnitCompiler.java:6403) at org.codehaus.janino.Java$Cast.accept(Java.java:4887) at org.codehaus.janino.UnitCompiler$21.visitRvalue(UnitCompiler.java:6403) at org.codehaus.janino.UnitCompiler$21.visitRvalue(UnitCompiler.java:6382) at org.codehaus.janino.Java$Rvalue.accept(Java.java:4105) at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6382) at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9150) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9036) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8938) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5060) at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4421) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4394) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5019) at org.codehaus.janino.UnitCompiler.access$8600(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$16.visitCast(UnitCompiler.java:4416) at org.codehaus.janino.UnitCompiler$16.visitCast(UnitCompiler.java:4394) at org.codehaus.janino.Java$Cast.accept(Java.java:4887) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5019) at org.codehaus.janino.UnitCompiler.access$8600(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$16.visitCast(UnitCompiler.java:4416) at org.codehaus.janino.UnitCompiler$16.visitCast(UnitCompiler.java:4394) at org.codehaus.janino.Java$Cast.accept(Java.java:4887) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2580) at org.codehaus.janino.UnitCompiler.access$2700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1503) at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3511) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78) ... 20 more


// generate class
/* 1 */
/* 2 */      public class SourceConversion$1 extends org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        private transient org.apache.flink.table.dataformat.DataFormatConverters.PojoConverter converter$0;
/* 7 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 8 */
/* 9 */        public SourceConversion$1(
/* 10 */            Object[] references,
/* 11 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 12 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 13 */            org.apache.flink.streaming.api.operators.Output output) throws Exception {
/* 14 */          this.references = references;
/* 15 */          converter$0 = (((org.apache.flink.table.dataformat.DataFormatConverters.PojoConverter) references[0]));
/* 16 */          this.setup(task, config, output);
/* 17 */        }
/* 18 */
/* 19 */        @Override
/* 20 */        public void open() throws Exception {
/* 21 */          super.open();
/* 22 */          
/* 23 */        }
/* 24 */
/* 25 */        @Override
/* 26 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 27 */          org.apache.flink.table.dataformat.BaseRow in1 = (org.apache.flink.table.dataformat.BaseRow) (org.apache.flink.table.dataformat.BaseRow) converter$0.toInternal((net.xxxxxxxxxx.Student) element.getValue());
/* 28 */          
/* 29 */          
/* 30 */          
/* 31 */          output.collect(outElement.replace(in1));
/* 32 */        }
/* 33 */
/* 34 */        
/* 35 */
/* 36 */        @Override
/* 37 */        public void close() throws Exception {
/* 38 */           super.close();
/* 39 */          
/* 40 */        }
/* 41 */
/* 42 */        
/* 43 */      }
/* 44 */    {code}

I think like generating Pipeline (StreamGraph), UserCodeClassLoader should be used when generating JobGraph.
The test code is as follows:

 
{code:java}
public class App {

    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        EnvironmentSettings envSet = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, envSet);

        env.enableCheckpointing(2 * 60 * 1000);

        TableConfig config = tableEnv.getConfig();
        config.setIdleStateRetentionTime(Time.hours(24),
                Time.hours(25));

        DataStreamSource<Student> source = env.addSource(new SourceFunction<Student>() {
            @Override
            public void run(SourceContext<Student> ctx) throws Exception {
                ctx.collect(new Student(1, ""Tom""));
            }

            @Override
            public void cancel() {

            }
        });

        tableEnv.createTemporaryView(""student"", source, ""id, name"");
        Table table = tableEnv.sqlQuery(""select id, name from student"");

        CsvTableSink sink = new CsvTableSink(""/data/student"", "","", 10, FileSystem.WriteMode.OVERWRITE);

        String[] fieldNames = {""id"", ""name""};
        TypeInformation[] fieldTypes = {Types.INT, Types.STRING};
        tableEnv.registerTableSink(""student_sink"", fieldNames, fieldTypes, sink);
        table.insertInto(""student_sink"");

        env.execute(""Test_Jar"");
    }

    @Getter
    @Setter
    @NoArgsConstructor
    @AllArgsConstructor
    public static class Student {
        private Integer id;
        private String name;
    }
}{code}
To reproduce this bug, the following conditions must be met:

1. Convert POJO DataStream to Table
2. Enables Checkpoint, StreamingJobGraphGenerator#preValidate() will check whether Checkpoint is enabled
3. The program is packaged into a Jar and submitted to Flink, or invoke PackagedProgramUtils.createJobGraph to create JobGraph by the Jar Program directly",,aljoscha,chenxyz,csbliss,dwysakowicz,godfreyhe,jark,kkl0u,libenchao,liyu,nkruber,tliao,twalthr,xiemeilong,zhanglibing1990@126.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16585,,,FLINK-17171,,FLINK-17108,FLINK-17561,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 05:18:25 UTC 2020,,,,,,,,,,"0|z0cnxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/20 12:03;zhanglibing1990@126.com;Hi，[~chenxyz] I have also encountered this problem，I found when enable the checkpoint in blink plan mode，the StreamingJobGraphGenerator#preValidate() method will to check whether every operator in streamGraph doesn't implement the InputSelectable interface in the checkpointing opened state.

I think you are right  about UserCodeClassLoader should be used when generating JobGraph, because the OperatorFactory generate operatorClass  instance use dynamic compile and it depends on the UserCodeClassLoader.

I fixed this bug by expanding the scope of using UserCodeClassLoader in PackagedProgramUtils class as follows：

 
{code:java}
public static JobGraph createJobGraph(
   PackagedProgram packagedProgram,
   Configuration configuration,
   int defaultParallelism,
   @Nullable JobID jobID,
   boolean suppressOutput) throws ProgramInvocationException {
   final ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
   final JobGraph jobGraph;
   try {
      Thread.currentThread().setContextClassLoader(packagedProgram.getUserCodeClassLoader());
      final Pipeline pipeline = getPipelineFromProgram(packagedProgram, defaultParallelism, suppressOutput);
      jobGraph = FlinkPipelineTranslationUtil.getJobGraph(pipeline, configuration, defaultParallelism);
      if (jobID != null) {
         jobGraph.setJobID(jobID);
      }
      jobGraph.addJars(packagedProgram.getJobJarAndDependencies());
      jobGraph.setClasspaths(packagedProgram.getClasspaths());
      jobGraph.setSavepointRestoreSettings(packagedProgram.getSavepointSettings());
   } finally {
      Thread.currentThread().setContextClassLoader(contextClassLoader);
   }
   return jobGraph;
}
{code}
and remove the the contextClassLoader logic in getPipelineFromProgram() method:

 
{code:java}
public static Pipeline getPipelineFromProgram(
   PackagedProgram prog,
   int parallelism,
   boolean suppressOutput) throws CompilerException, ProgramInvocationException {
   // temporary hack to support the optimizer plan preview
   OptimizerPlanEnvironment env = new OptimizerPlanEnvironment();
   if (parallelism > 0) {
      env.setParallelism(parallelism);
   }
   return env.getPipeline(prog, suppressOutput);
}{code}
@[~jark] please assign to me, and i will fix it. thanks 

 ;;;","24/Mar/20 03:52;csbliss;[~zhanglibing1990@126.com]

similar issue: https://issues.apache.org/jira/browse/FLINK-16585

Also, add some unit tests for this issue;;;","15/Apr/20 22:31;tliao;Hey, any updates on this? I've also hit this issue when trying to switch from the old planner to the blink planner on 1.10. As a data point, this is my setup:
 # UDFs being shipped.
 # Checkpointing enabled.
 # Table is being created through a sql query that references a UDF.
 # The table is converted to a retract stream.;;;","16/Apr/20 05:06;ykt836;[~zhanglibing1990@126.com] I've assigned this to you. ;;;","16/Apr/20 07:13;jark;cc [~aljoscha];;;","16/Apr/20 09:12;aljoscha;What's the proposed fix?;;;","16/Apr/20 11:48;jark;[~aljoscha], the proposed fix is in the first comment. Use the user classloader in PackagedProgram as the thread context classloader before {{getPipelineFromProgram}}. ;;;","16/Apr/20 12:48;kkl0u;Thanks for reporting and debugging all this. This existed also in the 1.10, right? 
If this is the case, then apart from the bug itself, this also shows that our tests should be also extended :(

Probably adding an end-to-end test?;;;","16/Apr/20 13:00;jark;Hi [~zhanglibing1990@126.com], do you have time to fix this? As we are going to create 1.10.1 RC and this is a blocker, we may need to have a fix soon. ;;;","16/Apr/20 13:08;zhanglibing1990@126.com;HI [~jark] I will fix it as soon as possible.;;;","17/Apr/20 14:07;liyu;No mean to push but any estimated time for the fix [~zhanglibing1990@126.com]? Thanks.;;;","20/Apr/20 20:06;kkl0u;Hi [~zhanglibing1990@126.com], and thanks for working on this. From the discussion, I was left with the impression that we are going to either add a test in the PRs for this issue, or, at least, create a new JIRA that will target adding a test for these cases.

The PR has no test and there is also no other JIRA for this. Could you please create one so that we do not forget about it?;;;","20/Apr/20 20:16;kkl0u;[~NicoK] could you please verify that this PR solves the issue? Given that there is no test coverage, we need manual verification.;;;","21/Apr/20 18:10;zhanglibing1990@126.com;[~kkl0u]  I did solve the problem with this method, and verified the issue in my standalone cluster. but unfortunately the issue will only recur when submit the job to the cluster, and I don't know whether it need a end-to-end test to verify it,or what's your opinion on this pr?

In the meantime， I have created a Jira for this PR to add some tests in the future FLINK-17310.

thanks ;;;","22/Apr/20 16:56;kkl0u;Merged on master with 19d9cee49bce02dd0cf232cfba003936b3b2f68e
and on release-1.10 with ae39868c850b5fbc08d5161b664b702e2a9f25ba;;;","23/Apr/20 18:10;chenxyz;Hi,[~kkl0u], [~zhanglibing1990@126.com], thx for fix. I'm reporter of this issue.I think I can add a test. The root cause is current classloader (AppClassLoader, jvm default) cannot find user class, but find in UserCodeClassLoader. so we can generate job graph from a blink jar, which contains user code (udf, customer pojos,...).The test class execute in AppClassLoader (default), user class load in UserCodeClassLoader. It can be recured locally.
 I submited PR for the test[11889|[https://github.com/apache/flink/pull/11889]], [11890|[https://github.com/apache/flink/pull/11890]];;;","24/Apr/20 03:22;jark;Hi [~chenxyz], I think a better test is to add a real user case in end-to-end tests. A simple idea is that we can extend {{StreamSQLTestProgram}} a bit, to register a UDF which uses user defined POJO class, and call the UDF in the SQL pipeline. I guess this can reproduce this problem. ;;;","24/Apr/20 03:23;jark;Btw, your approach is very similar to an end-to-end test. ;;;","24/Apr/20 04:12;chenxyz;[~jark] thx, you're right, end-to-end test is a better way for running in flink cluster and testing the whole lifecycle;;;","13/May/20 05:18;xiemeilong;[~zhanglibing1990@126.com] After upgrade to 1.10.1, a similar issue occurred.
{code:java}
Caused by: org.apache.flink.table.api.ValidationException: Given parameters of function 'generateDecoder' do not match any signature. 
Actual: (com.yunmo.iot.schema.RecordFormat, com.yunmo.iot.schema.Schema) 
Expected: (com.yunmo.iot.schema.RecordFormat, com.yunmo.iot.schema.Schema)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BytesColumnVector should init buffer in Hive 3.x,FLINK-16652,13292452,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lirui,lirui,18/Mar/20 12:14,23/Mar/20 06:01,13/Jul/23 08:07,23/Mar/20 06:01,1.10.0,,,,,1.10.1,1.11.0,,,Connectors / Hive,Connectors / ORC,,,,0,pull-request-available,,,,The failed test is {{TableEnvHiveConnectorTest#testDifferentFormats}} when hive 3.x.,,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #11485: [FLINK-16652][orc] BytesColumnVector should init buffer in Hive 3.x
URL: https://github.com/apache/flink/pull/11485
 
 
   
   ## What is the purpose of the change
   
   BytesColumnVector should init buffer before fill in Hive 3.x.
   Otherwise there is a NPE.
   
   ## Brief change log
   
   For `BytesColumnVector`, `initBuffer` before `init`.
   
   ## Verifying this change
   
   `TableEnvHiveConnectorTest#testDifferentFormats` when hive 3.x
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Mar/20 03:32;githubbot;600","JingsongLi commented on pull request #11485: [FLINK-16652][orc] BytesColumnVector should init buffer in Hive 3.x
URL: https://github.com/apache/flink/pull/11485
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Mar/20 05:56;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 06:01:02 UTC 2020,,,,,,,,,,"0|z0cno0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/20 06:01;lzljs3620320;release-1.10: 7a01a1d08a1a5a781d0daa8f6aa0b3b63f67246e

master: cdcc25cee9c5702c94376c3843def6c59849ec96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Miss file extension when inserting to hive table with compression,FLINK-16647,13292398,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,18/Mar/20 08:36,27/Mar/20 03:07,13/Jul/23 08:07,27/Mar/20 03:07,1.10.0,,,,,1.10.1,1.11.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,"When {{hive.exec.compress.output}} is on, we write into Hive tables with a compression codec. But we don't append a proper extension to the resulting files, which means these files can't be consumed later on.",,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11440: [FLINK-16647][table-runtime-blink][hive] Miss file extension when ins…
URL: https://github.com/apache/flink/pull/11440
 
 
   …erting to hive table with compression
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   To add proper file extension when compression is enabled.
   
   When `hive.exec.compress.output` is on, we write into Hive tables with a compression codec. But we don't append a proper extension to the resulting files, which means these files can't be consumed later on.
   
   
   ## Brief change log
   
     - Support setting file extension in `FileSystemOutputFormat`, and use the extension in `PartitionTempFileManager`.
     - Decide file extension by calling Hive util method.
     - Refactor code.
     - Add test case.
   
   
   ## Verifying this change
   
   New test case.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/20 12:05;githubbot;600","lirui-apache commented on pull request #11505: [FLINK-16647][table-runtime-blink][hive] Miss file extension when inserting to hive table with compression
URL: https://github.com/apache/flink/pull/11505
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   To add proper file extension when compression is enabled.
   
   When `hive.exec.compress.output` is on, we write into Hive tables with a compression codec. But we don't append a proper extension to the resulting files, which means these files can't be consumed later on.
   
   
   ## Brief change log
   
     - Support setting file extension in `FileSystemOutputFormat`, and use the extension in `PartitionTempFileManager`.
     - Decide file extension by calling Hive util method.
     - Refactor code.
     - Add test case.
   
   
   ## Verifying this change
   
   New test case.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 08:15;githubbot;600","JingsongLi commented on pull request #11440: [FLINK-16647][table-runtime-blink][hive] Miss file extension when ins…
URL: https://github.com/apache/flink/pull/11440
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/20 03:04;githubbot;600","JingsongLi commented on pull request #11505: [FLINK-16647][table-runtime-blink][hive] Miss file extension when inserting to hive table with compression
URL: https://github.com/apache/flink/pull/11505
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/20 03:05;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 03:07:04 UTC 2020,,,,,,,,,,"0|z0cnc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/20 03:07;lzljs3620320;release-1.10:

d33c0eb7e880f91774dbed263ffcaf9c83b23e13

fd9d68f2c6e32894f6ac846f899c466b3f07d972

6758ee33ef7fd87d53e97459d293fa38700c9085

master:

d33c0eb7e880f91774dbed263ffcaf9c83b23e13

fd9d68f2c6e32894f6ac846f899c466b3f07d972

6758ee33ef7fd87d53e97459d293fa38700c9085;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink read orc file throw a NullPointerException,FLINK-16646,13292383,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangjun,zhangjun,zhangjun,18/Mar/20 07:49,31/Mar/20 06:48,13/Jul/23 08:07,19/Mar/20 09:29,1.10.0,,,,,1.10.1,1.11.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,18/Mar/20 00:00,0,pull-request-available,,,,"When I use OrcRowInputFormat to read multiple orc files, the system throws one NullPointerException .

the code like this

 
{code:java}
StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment();
environment.setParallelism(1);
String path = ""file://tmp/dir"";
String schema = ..... ;
OrcRowInputFormat orcRowInputFormat = new OrcRowInputFormat(
   path,
   schema,
   new org.apache.hadoop.conf.Configuration());
DataStream dataStream  =environment.createInput(orcRowInputFormat);
dataStream.writeAsText(""file:///tmp/aaa"", FileSystem.WriteMode.OVERWRITE);
environment.execute();
{code}
 

the exception is 

 
{code:java}
Caused by: java.lang.NullPointerExceptionCaused by: java.lang.NullPointerException at org.apache.flink.orc.shim.OrcShimV200.computeProjectionMask(OrcShimV200.java:188) at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:120) at org.apache.flink.orc.OrcSplitReader.<init>(OrcSplitReader.java:73) at org.apache.flink.orc.OrcRowSplitReader.<init>(OrcRowSplitReader.java:50) at org.apache.flink.orc.OrcRowInputFormat.open(OrcRowInputFormat.java:102) at org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator$SplitReader.run(ContinuousFileReaderOperator.java:315)
{code}
 

 ",,lzljs3620320,zhangjun,,,,,,,,,,,,,,,,,,,,"zhangjun888 commented on pull request #11434: [FLINK-16646]flink read orc file throw a NullPointerException
URL: https://github.com/apache/flink/pull/11434
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   fix the bug when read multiple orc file throw a NullPointerException
   
   
   ## Brief change log
   
   *(for example:)*
     - set the schema null  on method OrcInputFormat#closeInputFormat instead of method close
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/20 08:05;githubbot;600","JingsongLi commented on pull request #11434:  [FLINK-16646][orc] Flink read orc file throw a NullPointerException after re-open
URL: https://github.com/apache/flink/pull/11434
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 03:26;githubbot;600","zhangjun888 commented on pull request #11447: [FLINK-16646][orc] Flink read orc file throw a NullPointerException after re-open
URL: https://github.com/apache/flink/pull/11447
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   fix the bug when read multiple orc file throw a NullPointerException
   
   
   ## Brief change log
   
   *(for example:)*
     - set the schema null on method OrcInputFormat#closeInputFormat instead of method close
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 06:26;githubbot;600","JingsongLi commented on pull request #11447: [FLINK-16646][orc] Flink read orc file throw a NullPointerException after re-open
URL: https://github.com/apache/flink/pull/11447
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 09:28;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,FLINK-16870,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 09:29:06 UTC 2020,,,,,,,,,,"0|z0cn8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/20 08:19;zhangjun;ContinuousFileReaderOperator use OrcRowInputFormat to read orc files and use the same orc schema, but when there are multiple files in the path, the system reads a file(split) and will call the close method of OrcRowInputFormat to set the schema to null, and then read the second orc file,but the schema is null now, the system throws a java.lang.NullPointerException, so we move set the schema to null into the closeInputFormat method.

hi, [~lzljs3620320] I submit a PR to solve the bug ;;;","18/Mar/20 08:42;lzljs3620320;[~zhangjun] (y) Thanks for the reporting. You are right.;;;","19/Mar/20 03:23;zhangjun;hi,[~lzljs3620320] ,is it necessary to add a pr to release-1.10 ?;;;","19/Mar/20 03:26;lzljs3620320;[~zhangjun] Yes, it is good to do.;;;","19/Mar/20 06:34;zhangjun;hi,[~lzljs3620320], I submit a PR to release-1.10  . [https://github.com/apache/flink/pull/11447];;;","19/Mar/20 09:29;lzljs3620320;release-1.10: d2213bfa81d9b77c6ff1361c31314e606b042c4a

master: 1276507fa68880866f227dcf5f607ffb0ca9ffdf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink checkStateMappingCompleteness doesn't include UserDefinedOperatorIDs,FLINK-16638,13292310,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,edu05,basharaj,basharaj,18/Mar/20 00:52,07/May/20 15:55,13/Jul/23 08:07,07/May/20 15:55,1.10.0,1.9.1,,,,1.11.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"[StateAssignmentOperation.checkStateMappingCompleteness|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StateAssignmentOperation.java#L555] doesn't check for UserDefinedOperatorIDs (specified using setUidHash), causing the exception:

{code}
 java.lang.IllegalStateException: There is no operator for the state {}
{code}
to be thrown when a savepoint can't be mapped to an ExecutionJobVertex, even when the operator hash is explicitly specified.

I believe this logic should be extended to also include UserDefinedOperatorIDs as so:

{code:java}
for (ExecutionJobVertex executionJobVertex : tasks) {
  allOperatorIDs.addAll(executionJobVertex.getOperatorIDs());
  allOperatorIDs.addAll(executionJobVertex.getUserDefinedOperatorIDs());
}
{code}",,basharaj,edu05,klion26,pnowojski,roman,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 15:55:06 UTC 2020,,,,,,,,,,"0|z0cmsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/20 14:40;roman;Thanks for reporting this issue [~basharaj],

 

You're right the allOperatorIDs contains non-user-defined IDs.

However, I see that the other side of the check is also using non-user-defined IDs (StateAssignmentOperation#operatorStates). So the code looks fine to me.

 

Probably, the snapshot data is corrupted. The log message you provided looks strange: it should contain the operator ID that can't be mapped. In your case it's just {}.

 

Can you please post a full message with a stacktrace?;;;","20/Mar/20 16:40;basharaj;Hi [~roman_khachatryan], sorry I didn't post the full log message, the error is:
{code:java}
 java.lang.IllegalStateException: There is no operator for the state 6463bd1ad519d1e0c283c83f761989c1
	at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.checkStateMappingCompleteness(StateAssignmentOperation.java:567)
	at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignStates(StateAssignmentOperation.java:79)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedState(CheckpointCoordinator.java:1078)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1143)
	at org.apache.flink.runtime.scheduler.LegacyScheduler.tryRestoreExecutionGraphFromSavepoint(LegacyScheduler.java:237)
	at org.apache.flink.runtime.scheduler.LegacyScheduler.createAndRestoreExecutionGraph(LegacyScheduler.java:196)
	at org.apache.flink.runtime.scheduler.LegacyScheduler.<init>(LegacyScheduler.java:176)
	at org.apache.flink.runtime.scheduler.LegacySchedulerFactory.createInstance(LegacySchedulerFactory.java:70)
	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:275)
	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:265)
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)
	at org.apache.flink.runtime.jobmaster.JobManagerRunner.<init>(JobManagerRunner.java:146)
{code}
I still don't see how the code is right. Operator hash 6463bd1ad519d1e0c283c83f761989c1 is in the save point so it's in _operatorStates_. When I set that hash directly on the operator using setUidHash the code is not adding it (through a call to getUserDefinedOperatorIDs) to `allOperatorIDs` and this condition will always be true
{code:java}
			if (!allOperatorIDs.contains(operatorGroupStateEntry.getKey())) {
{code}
Throwing the exception above.;;;","23/Mar/20 09:22;roman;Unfortunately, I can't follow the stacktrace (e.g. LegacyScheduler).

Are you sure your version is 1.10?;;;","23/Mar/20 16:28;basharaj;[~roman_khachatryan] sorry this was against 1.9.1, I updated the Jira. ;;;","24/Mar/20 10:22;roman;Thanks, [~basharaj].

I think you are right, this is a bug.;;;","25/Mar/20 08:00;zjwang;[~basharaj], do you want to contribute the PR for this bug? If so, i can assign this ticket for you. :);;;","25/Mar/20 16:37;basharaj;[~zjwang] sure I can. ;;;","05/Apr/20 10:21;pnowojski;[~basharaj] thanks for reporting and volunteering :) Just a quick question what's the timeline that you could provide a fix for this? ;;;","05/Apr/20 17:52;basharaj;[~pnowojski]I I can submit a fix by end of this week, would that be OK? ;;;","06/Apr/20 09:08;pnowojski;Sure, thanks!;;;","20/Apr/20 16:24;pnowojski;Hey [~basharaj], could we get another estimate when could you work on this ticket? Sorry for putting a pressure, but we would like it to be fixed for 1.11 release. If you can not work on it in the next week or so, someone might need to take it over.;;;","20/Apr/20 16:34;basharaj;[~pnowojski] sorry about that. Unfortunately due to work and personal reasons I couldn't get a chance to work on this. Please feel free to assign it to someone else to meet the 1.11 release schedule.;;;","21/Apr/20 07:11;pnowojski;Thanks for the update [~basharaj]. No problem. I have unassigned you for a time being. I'm not sure exactly when someone from our side would be able to pick it up, so if you find some time feel free to come back to this issue.;;;","26/Apr/20 15:48;edu05;Hi [~pnowojski] and [~basharaj] I volunteer to fix this one, Bashar already offered most of the ""just do it"" solution. I also suggest this opportunity to clean up the code.

[JobVertex|https://github.com/apache/flink/blob/a9f8a0b4481719fb511436a61e36cb0e26559c79/flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph/JobVertex.java#L59] has a field called _idAlternatives_ that is only used in a test and in a separate function that is also only invoked from a unit test (i.e. it can be deleted).

It also has fields _operatorIDs_ and _operatorIdsAlternatives_, their getters are called in similar situations, _operatorIdsAlternatives_'s getter has an extra invocation in [ExecutionJobVertex::includeAlternativeOperatorIDs|https://github.com/apache/flink/blob/a9f8a0b4481719fb511436a61e36cb0e26559c79/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionJobVertex.java#L667] that is only making up for a previous call to get only the _operatorIDs_. It seems like both concepts are tightly related and should be encapsulated in a single entity. Not surprisingly the bug we're tackling here is due to retrieving one set of ids while forgetting the other set of ids.

 

There are 6 production code invocations to [ExecutionJobVertex::getOperatorIDs|https://github.com/apache/flink/blob/a9f8a0b4481719fb511436a61e36cb0e26559c79/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionJobVertex.java#L296]:
 - The call in [Checkpoints::loadAndValidateCheckpoint|#L140] is soon followed by the previously mentioned _ExecutionJobVertex::includeAlternativeOperatorIDs_. Reinforcing the argument that both are related.

 * The call in [StateAssignmentOperation::assignStates|https://github.com/apache/flink/blob/a9f8a0b4481719fb511436a61e36cb0e26559c79/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StateAssignmentOperation.java#L89] is directly followed to call to retrieve the operator id alternatives, also reinforcing the argument.
 * The call in [StateAssignmentOperation::checkStateMappingCompleteness|https://github.com/apache/flink/blob/a9f8a0b4481719fb511436a61e36cb0e26559c79/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StateAssignmentOperation.java#L558] is the one with the bug!

 - The other 3 in [StateAssignmentOperation::assignTaskStateToExecutionJobVertices|https://github.com/apache/flink/blob/a9f8a0b4481719fb511436a61e36cb0e26559c79/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StateAssignmentOperation.java#L217], [StateAssignmentOperation::assignAttemptState|https://github.com/apache/flink/blob/a9f8a0b4481719fb511436a61e36cb0e26559c79/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StateAssignmentOperation.java#L118], and [PendingCheckpoint::acknowledgeTask|https://github.com/apache/flink/blob/a9f8a0b4481719fb511436a61e36cb0e26559c79/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/PendingCheckpoint.java#L374] are not followed by a retrieval of the alternative ids nor do they need one. Why these don't need to is puzzling me and they're the only reason not to encapsulate both ids together. If someone could explain this we might be able to leave the code pretty tidy!

I offer myself volunteer for either the quick dirty fix or the longer cleaner solution. The first should take me a couple of days whereas the second should take me ~4 days (considering I've already done a lot of the ground work locally). I'm open to suggestions.

 

 ;;;","27/Apr/20 08:50;roman;Hi [~edu05],

Thanks a lot for offering your help!

 

Can you please describe the second approach that you mentioned?;;;","27/Apr/20 11:26;edu05;Hi [~roman_khachatryan], sure, the idea is to avoid future similar bugs by forcing developers to access a single list of (operatorId, userDefinedOperatorId) pairs. Using only one set of ids or both is then a conscious choice of the developer. This can be achieved with a new class OperatorIdPair that will hold both ids.

I've pushed the putative changes to [https://github.com/edu05/flink/tree/FLINK-16638-PREP] , the fix is also included, developers are forced to make a conscious decision as which operator ids to use and presents the chance to remove some code (look at //COULD THIS IF BLOCK BE REMOVED IF THE ALTERNATIVE IDS HAD BEEN ADDED TO THE MAPPING FROM THE START? comment).

My question in my previous post was around how come there are 3 out of 6 invocations that DO need the user defined operator id, whereas the other 3 do NOT.

 

I hope this helps illustrate, let me know your thoughts!;;;","28/Apr/20 08:03;roman;Thanks Eduardo, I agree OperatorID pair makes sense.

Can you please create a PR?

We can also fix the issues you mentioned there.;;;","28/Apr/20 11:26;edu05;[~roman_khachatryan] Sure will! Expect one in one or two days :);;;","30/Apr/20 10:12;edu05;Hi [~roman_khachatryan] I've submitted a PR, it's on the last stage of the Azure build (e2e tests). Let me know if I can be of any help with the review.;;;","30/Apr/20 17:29;roman;Hi [~edu05],

I'll take a look.

Thanks!;;;","07/May/20 15:55;pnowojski;Thanks for the contribution [~edu05] and for reporting [~basharaj].

merged commit 0114338 into apache:master ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incompatible okio dependency in flink-metrics-influxdb module,FLINK-16635,13292164,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,17/Mar/20 11:05,01/Apr/20 10:22,13/Jul/23 08:07,18/Mar/20 15:53,1.10.0,,,,,1.10.1,1.11.0,,,Runtime / Metrics,,,,,0,pull-request-available,,,,"With FLINK-12147 we bumped {{influxdb-java}} from version {{2.14}} to {{2.16}}. At the same time we fix the okio dependency to version {{1.14.0}}. Since {{influxdb-java}} transitive dependency {{converter-moshi:jar:2.6.1}} requires {{moshi:jar:1.8.0}} which requires {{okio:jar:1.16.0}}, the influxdb metric reporter fails as described [here|https://stackoverflow.com/q/60667654/4815083]. We should fix this incompatibility by removing the dependency management entry for okio.",,trohrmann,,,,,,,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #11428: [FLINK-16635] Remove pinned dependency for okio and okhttp from flink-metrics-influxdb
URL: https://github.com/apache/flink/pull/11428
 
 
   ## What is the purpose of the change
   
   With FLINK-12147 we bumped the influxdb-java version from 2.14 to 2.16. At the same
   time we still have okio and okhttp fixed to an incompatible version. This commit
   removes the dependency management entries for these dependencies so that the
   influxdb reporter bundles the correct dependencies.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/20 11:15;githubbot;600","tillrohrmann commented on pull request #11428: [FLINK-16635] Remove pinned dependency for okio and okhttp from flink-metrics-influxdb
URL: https://github.com/apache/flink/pull/11428
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/20 15:50;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-16918,,,,,,,,FLINK-12147,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 15:53:23 UTC 2020,,,,,,,,,,"0|z0clw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/20 15:53;trohrmann;Fixed via

1.11.0:
b4c0f2db5ecc8a0677763c1f822d5fdc46a52597
405cf8f429c6a5031c21597abe9193bedcb8e15b

1.10.1:
2fc3667b1ea514bfeb67556fcb5832f0de87261d
ac33f1a9c690e7183ac2c0d340d5568cca1ba40e
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI builds without S3 credentials fail,FLINK-16633,13292138,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,17/Mar/20 09:15,20/Mar/20 10:46,13/Jul/23 08:07,20/Mar/20 10:46,1.11.0,,,,,1.11.0,,,,Build System / Azure Pipelines,FileSystems,,,,0,pull-request-available,,,,"Pull request builds on Azure don't have access to the S3 credentials (through environment variables).
They fail with the following errors:

{code}
2020-03-17T02:42:24.4186165Z [ERROR] Tests run: 7, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 0.645 s <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase
2020-03-17T02:42:24.4187582Z [ERROR] testResumeAfterCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase)  Time elapsed: 0.11 s  <<< ERROR!
2020-03-17T02:42:24.4188593Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4189408Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4190152Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.prepare(HadoopS3RecoverableWriterExceptionITCase.java:120)
2020-03-17T02:42:24.4190798Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4191529Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4192258Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.prepare(HadoopS3RecoverableWriterExceptionITCase.java:120)
2020-03-17T02:42:24.4192689Z 
2020-03-17T02:42:24.4193119Z [ERROR] testResumeAfterCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase)  Time elapsed: 0.11 s  <<< ERROR!
2020-03-17T02:42:24.4193704Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4194461Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4195299Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanup(HadoopS3RecoverableWriterExceptionITCase.java:130)
2020-03-17T02:42:24.4195945Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4196588Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4197307Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanup(HadoopS3RecoverableWriterExceptionITCase.java:130)
2020-03-17T02:42:24.4197697Z 
2020-03-17T02:42:24.4198135Z [ERROR] testResumeWithWrongOffset(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4198791Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4199755Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4200466Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.prepare(HadoopS3RecoverableWriterExceptionITCase.java:120)
2020-03-17T02:42:24.4201169Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4201834Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4202578Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.prepare(HadoopS3RecoverableWriterExceptionITCase.java:120)
2020-03-17T02:42:24.4203010Z 
2020-03-17T02:42:24.4203579Z [ERROR] testResumeWithWrongOffset(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:24.4204201Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4204884Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4205769Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanup(HadoopS3RecoverableWriterExceptionITCase.java:130)
2020-03-17T02:42:24.4206387Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4207038Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4207731Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanup(HadoopS3RecoverableWriterExceptionITCase.java:130)
2020-03-17T02:42:24.4208128Z 
2020-03-17T02:42:24.4208574Z [ERROR] testExceptionWritingAfterCloseForCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4209199Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4209846Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4210561Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.prepare(HadoopS3RecoverableWriterExceptionITCase.java:120)
2020-03-17T02:42:24.4211262Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4211909Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4212667Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.prepare(HadoopS3RecoverableWriterExceptionITCase.java:120)
2020-03-17T02:42:24.4213270Z 
2020-03-17T02:42:24.4213976Z [ERROR] testExceptionWritingAfterCloseForCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:24.4215187Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4216174Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4217335Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanup(HadoopS3RecoverableWriterExceptionITCase.java:130)
2020-03-17T02:42:24.4218162Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4218811Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4219751Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanup(HadoopS3RecoverableWriterExceptionITCase.java:130)
2020-03-17T02:42:24.4220133Z 
2020-03-17T02:42:24.4220524Z [ERROR] org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:24.4221167Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4221776Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4222539Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanUp(HadoopS3RecoverableWriterExceptionITCase.java:109)
2020-03-17T02:42:24.4223186Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4223818Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)
2020-03-17T02:42:24.4224649Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanUp(HadoopS3RecoverableWriterExceptionITCase.java:109)
2020-03-17T02:42:24.4225138Z 
2020-03-17T02:42:24.4244002Z [ERROR] Tests run: 16, Failures: 0, Errors: 16, Skipped: 0, Time elapsed: 0.651 s <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase
2020-03-17T02:42:24.4245484Z [ERROR] testMkdirsReturnsTrueWhenCreatingDirectory(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.106 s  <<< ERROR!
2020-03-17T02:42:24.4246427Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4247410Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4248029Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4248676Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4249045Z 
2020-03-17T02:42:24.4249509Z [ERROR] testMkdirsReturnsTrueWhenCreatingDirectory(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.106 s  <<< ERROR!
2020-03-17T02:42:24.4250207Z java.lang.NullPointerException
2020-03-17T02:42:24.4250371Z 
2020-03-17T02:42:24.4250811Z [ERROR] testMkdirsFailsWithExistingParentFile(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4251528Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4252098Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4252769Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4253384Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4253748Z 
2020-03-17T02:42:24.4254188Z [ERROR] testMkdirsFailsWithExistingParentFile(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4254773Z java.lang.NullPointerException
2020-03-17T02:42:24.4254946Z 
2020-03-17T02:42:24.4255377Z [ERROR] testMkdirsReturnsTrueForExistingDirectory(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4255992Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4256640Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4257238Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4258010Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4258381Z 
2020-03-17T02:42:24.4258794Z [ERROR] testMkdirsReturnsTrueForExistingDirectory(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4259265Z java.lang.NullPointerException
2020-03-17T02:42:24.4259476Z 
2020-03-17T02:42:24.4260069Z [ERROR] testPathAndScheme(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4260921Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4261716Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4262355Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4262963Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4263450Z 
2020-03-17T02:42:24.4263844Z [ERROR] testPathAndScheme(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4264260Z java.lang.NullPointerException
2020-03-17T02:42:24.4264413Z 
2020-03-17T02:42:24.4265143Z [ERROR] testHomeAndWorkDir(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4266031Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4267039Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4267981Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4268921Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4269308Z 
2020-03-17T02:42:24.4269695Z [ERROR] testHomeAndWorkDir(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4270133Z java.lang.NullPointerException
2020-03-17T02:42:24.4270288Z 
2020-03-17T02:42:24.4270655Z [ERROR] testFileSystemKind(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4271303Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4271841Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4272728Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4273341Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4273698Z 
2020-03-17T02:42:24.4274064Z [ERROR] testFileSystemKind(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4274498Z java.lang.NullPointerException
2020-03-17T02:42:24.4274765Z 
2020-03-17T02:42:24.4275176Z [ERROR] testMkdirsFailsForExistingFile(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4275736Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4276290Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4276995Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4277768Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4278112Z 
2020-03-17T02:42:24.4278526Z [ERROR] testMkdirsFailsForExistingFile(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4278971Z java.lang.NullPointerException
2020-03-17T02:42:24.4279123Z 
2020-03-17T02:42:24.4279553Z [ERROR] testMkdirsCreatesParentDirectories(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4280192Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4280730Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4281383Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:24.4281969Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)
2020-03-17T02:42:24.4282377Z 
2020-03-17T02:42:24.4282797Z [ERROR] testMkdirsCreatesParentDirectories(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:24.4283358Z java.lang.NullPointerException
2020-03-17T02:42:24.4283511Z 
2020-03-17T02:42:24.6650046Z [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase
2020-03-17T02:42:24.9808490Z [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase
2020-03-17T02:42:25.2595666Z [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.593 s <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase
2020-03-17T02:42:25.2596561Z [ERROR] org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase  Time elapsed: 0.593 s  <<< ERROR!
2020-03-17T02:42:25.2597100Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.2597675Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.setup(HadoopS3FileSystemITCase.java:55)
2020-03-17T02:42:25.2598249Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.2598841Z 	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.setup(HadoopS3FileSystemITCase.java:55)
2020-03-17T02:42:25.2599168Z 
2020-03-17T02:42:25.6533694Z [ERROR] Tests run: 27, Failures: 0, Errors: 27, Skipped: 0, Time elapsed: 0.668 s <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase
2020-03-17T02:42:25.6534735Z [ERROR] testCloseWithNoData(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.108 s  <<< ERROR!
2020-03-17T02:42:25.6535359Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6535930Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6536575Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6537219Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6537792Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6538384Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6539009Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6539642Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6540175Z 
2020-03-17T02:42:25.6540552Z [ERROR] testCloseWithNoData(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.109 s  <<< ERROR!
2020-03-17T02:42:25.6541479Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6542046Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6542681Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6543303Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6543886Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6544668Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6545075Z 
2020-03-17T02:42:25.6545467Z [ERROR] testCommitAfterPersist(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6546001Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6546671Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6547289Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6547916Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6548470Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6549054Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6549675Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6550350Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6550693Z 
2020-03-17T02:42:25.6551139Z [ERROR] testCommitAfterPersist(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6551684Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6552224Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6552850Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6553467Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6554044Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6554948Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6555325Z 
2020-03-17T02:42:25.6555730Z [ERROR] testRecoverWithEmptyState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6556276Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6556825Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6557467Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6558074Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6558750Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6559330Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6559962Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6560614Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6560961Z 
2020-03-17T02:42:25.6561429Z [ERROR] testRecoverWithEmptyState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6561994Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6562533Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6563183Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6563849Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6564454Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6565279Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6565667Z 
2020-03-17T02:42:25.6566084Z [ERROR] testRecoverFromIntermWithoutAdditionalState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6566671Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6567202Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6567836Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6568450Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6569032Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6569621Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6570675Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6571362Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6571694Z 
2020-03-17T02:42:25.6572114Z [ERROR] testRecoverFromIntermWithoutAdditionalState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6572712Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6573269Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6573904Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6574650Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6575262Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6575918Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6576389Z 
2020-03-17T02:42:25.6576854Z [ERROR] testCallingDeleteObjectTwiceDoesNotThroughException(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6577466Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6578013Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6578636Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6579255Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6579817Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6580473Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6581140Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6581834Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6582161Z 
2020-03-17T02:42:25.6582616Z [ERROR] testCallingDeleteObjectTwiceDoesNotThroughException(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6583207Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6583764Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6584393Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6585170Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6585780Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6586416Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6586789Z 
2020-03-17T02:42:25.6587189Z [ERROR] testCommitAfterNormalClose(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6587737Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6588271Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6588902Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6589516Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6590121Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6590703Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6591389Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6591996Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6592343Z 
2020-03-17T02:42:25.6592733Z [ERROR] testCommitAfterNormalClose(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6593375Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6593904Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6594693Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6595335Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6595920Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6596553Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6596930Z 
2020-03-17T02:42:25.6597326Z [ERROR] testRecoverWithStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6597899Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6598574Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6599207Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6599814Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6600443Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6601091Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6601701Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6602336Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6602667Z 
2020-03-17T02:42:25.6603066Z [ERROR] testRecoverWithStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6603646Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6604188Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6604982Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6605599Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6606174Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6606826Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6607207Z 
2020-03-17T02:42:25.6607663Z [ERROR] testRecoverFromIntermWithoutAdditionalStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:25.6608277Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6608827Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6609437Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6610075Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6610733Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6611378Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6611992Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6612600Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6612925Z 
2020-03-17T02:42:25.6613378Z [ERROR] testRecoverFromIntermWithoutAdditionalStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6613979Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6614617Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6615320Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6616048Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6616634Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6617264Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6617636Z 
2020-03-17T02:42:25.6618025Z [ERROR] testRecoverWithState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:25.6618565Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6619104Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6619746Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6620404Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6620980Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6621597Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6622229Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6622827Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6623172Z 
2020-03-17T02:42:25.6623550Z [ERROR] testRecoverWithState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:25.6624108Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6624780Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6625433Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6626040Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6626633Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6627265Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6627725Z 
2020-03-17T02:42:25.6628117Z [ERROR] testCleanupRecoverableState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6628686Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6629217Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6629848Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6630504Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6631134Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6631732Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6632346Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6633035Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6633373Z 
2020-03-17T02:42:25.6633768Z [ERROR] testCleanupRecoverableState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6634337Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6635068Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6635709Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6636335Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6636913Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6637578Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6638019Z 
2020-03-17T02:42:25.6638426Z [ERROR] testCommitAfterRecovery(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6638976Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6639529Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6640196Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6640822Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6641442Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6642030Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6642665Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6643276Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6643613Z 
2020-03-17T02:42:25.6644010Z [ERROR] testCommitAfterRecovery(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6644720Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6645275Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6645932Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6646533Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6647134Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6647762Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6648139Z 
2020-03-17T02:42:25.6648547Z [ERROR] testRecoverAfterMultiplePersistsState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:25.6649134Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6649666Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6650421Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6651076Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6651658Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6652239Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6652879Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6653483Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6653826Z 
2020-03-17T02:42:25.6654233Z [ERROR] testRecoverAfterMultiplePersistsState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.001 s  <<< ERROR!
2020-03-17T02:42:25.6654916Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6655451Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6656110Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6656709Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6657292Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6657950Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6658328Z 
2020-03-17T02:42:25.6658760Z [ERROR] testRecoverAfterMultiplePersistsStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6659377Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6659905Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6660590Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6661269Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6661911Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6662500Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6663118Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)
2020-03-17T02:42:25.6663741Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)
2020-03-17T02:42:25.6664076Z 
2020-03-17T02:42:25.6664607Z [ERROR] testRecoverAfterMultiplePersistsStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6665213Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6665753Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6666406Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6667094Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6667679Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6668327Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)
2020-03-17T02:42:25.6668690Z 
2020-03-17T02:42:25.6669029Z [ERROR] org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase  Time elapsed: 0.002 s  <<< ERROR!
2020-03-17T02:42:25.6669521Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6670091Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6670733Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanUp(HadoopS3RecoverableWriterITCase.java:123)
2020-03-17T02:42:25.6671361Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T02:42:25.6671927Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)
2020-03-17T02:42:25.6672555Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanUp(HadoopS3RecoverableWriterITCase.java:123){code}

Also, the end 2 end tests are failing:
{code}
2020-03-17T03:46:29.1079571Z 
2020-03-17T03:46:29.1084340Z org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-03-17T03:46:29.1085345Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
2020-03-17T03:46:29.1086049Z 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
2020-03-17T03:46:29.1086659Z 	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:142)
2020-03-17T03:46:29.1088954Z 	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664)
2020-03-17T03:46:29.1092911Z 	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
2020-03-17T03:46:29.1093723Z 	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895)
2020-03-17T03:46:29.1095120Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)
2020-03-17T03:46:29.1095752Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-03-17T03:46:29.1096250Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-03-17T03:46:29.1096804Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
2020-03-17T03:46:29.1097517Z 	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
2020-03-17T03:46:29.1098167Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)
2020-03-17T03:46:29.1098932Z Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-03-17T03:46:29.1099822Z 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:199)
2020-03-17T03:46:29.1100632Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1741)
2020-03-17T03:46:29.1101367Z 	at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:90)
2020-03-17T03:46:29.1102043Z 	at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:58)
2020-03-17T03:46:29.1102869Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1620)
2020-03-17T03:46:29.1103466Z 	at StreamingFileSinkProgram.main(StreamingFileSinkProgram.java:77)
2020-03-17T03:46:29.1103948Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-17T03:46:29.1104447Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-17T03:46:29.1105005Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-17T03:46:29.1105525Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-17T03:46:29.1106030Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
2020-03-17T03:46:29.1106468Z 	... 11 more
2020-03-17T03:46:29.1106966Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-03-17T03:46:29.1107615Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-03-17T03:46:29.1108421Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-03-17T03:46:29.1109196Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1736)
2020-03-17T03:46:29.1109706Z 	... 20 more
2020-03-17T03:46:29.1110142Z Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-03-17T03:46:29.1110767Z 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:359)
2020-03-17T03:46:29.1111370Z 	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
2020-03-17T03:46:29.1111969Z 	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
2020-03-17T03:46:29.1112556Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-03-17T03:46:29.1113126Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-03-17T03:46:29.1114124Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:274)
2020-03-17T03:46:29.1114670Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-03-17T03:46:29.1115820Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-03-17T03:46:29.1116298Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-03-17T03:46:29.1116730Z 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
2020-03-17T03:46:29.1117188Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
2020-03-17T03:46:29.1117642Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-03-17T03:46:29.1118099Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-03-17T03:46:29.1118537Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-17T03:46:29.1118915Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-17T03:46:29.1119564Z Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
2020-03-17T03:46:29.1120091Z org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.
2020-03-17T03:46:29.1120568Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$3(Dispatcher.java:336)
2020-03-17T03:46:29.1121079Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2020-03-17T03:46:29.1122414Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2020-03-17T03:46:29.1122937Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-03-17T03:46:29.1123372Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-03-17T03:46:29.1123965Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-03-17T03:46:29.1124442Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-03-17T03:46:29.1124852Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-03-17T03:46:29.1125279Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-03-17T03:46:29.1125691Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-03-17T03:46:29.1126205Z Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
2020-03-17T03:46:29.1126730Z 	at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36)
2020-03-17T03:46:29.1127210Z 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2020-03-17T03:46:29.1127512Z 	... 6 more
2020-03-17T03:46:29.1127830Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
2020-03-17T03:46:29.1128573Z 	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:152)
2020-03-17T03:46:29.1129303Z 	at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:84)
2020-03-17T03:46:29.1129881Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:379)
2020-03-17T03:46:29.1130394Z 	at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34)
2020-03-17T03:46:29.1130710Z 	... 7 more
2020-03-17T03:46:29.1131072Z Caused by: org.apache.flink.util.FlinkRuntimeException: Failed to create checkpoint storage at checkpoint coordinator side.
2020-03-17T03:46:29.1131593Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.<init>(CheckpointCoordinator.java:301)
2020-03-17T03:46:29.1132112Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.<init>(CheckpointCoordinator.java:220)
2020-03-17T03:46:29.1132772Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.enableCheckpointing(ExecutionGraph.java:490)
2020-03-17T03:46:29.1133357Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:338)
2020-03-17T03:46:29.1133916Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.createExecutionGraph(SchedulerBase.java:263)
2020-03-17T03:46:29.1134458Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:235)
2020-03-17T03:46:29.1134986Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:223)
2020-03-17T03:46:29.1135581Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:118)
2020-03-17T03:46:29.1136099Z 	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:103)
2020-03-17T03:46:29.1136589Z 	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:281)
2020-03-17T03:46:29.1137028Z 	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:269)
2020-03-17T03:46:29.1137573Z 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)
2020-03-17T03:46:29.1138251Z 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)
2020-03-17T03:46:29.1138826Z 	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:146)
2020-03-17T03:46:29.1139163Z 	... 10 more
2020-03-17T03:46:29.1139470Z Caused by: java.io.IOException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T03:46:29.1140143Z 	at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:163)
2020-03-17T03:46:29.1140672Z 	at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:61)
2020-03-17T03:46:29.1141170Z 	at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:468)
2020-03-17T03:46:29.1141618Z 	at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:389)
2020-03-17T03:46:29.1142008Z 	at org.apache.flink.core.fs.Path.getFileSystem(Path.java:298)
2020-03-17T03:46:29.1142644Z 	at org.apache.flink.runtime.state.memory.MemoryBackendCheckpointStorage.<init>(MemoryBackendCheckpointStorage.java:85)
2020-03-17T03:46:29.1143267Z 	at org.apache.flink.runtime.state.memory.MemoryStateBackend.createCheckpointStorage(MemoryStateBackend.java:295)
2020-03-17T03:46:29.1144068Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.<init>(CheckpointCoordinator.java:298)
2020-03-17T03:46:29.1144448Z 	... 23 more
2020-03-17T03:46:29.1144843Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
2020-03-17T03:46:29.1145321Z 	at java.util.Objects.requireNonNull(Objects.java:228)
2020-03-17T03:46:29.1145777Z 	at org.apache.hadoop.fs.s3native.S3xLoginHelper.buildFSURI(S3xLoginHelper.java:69)
2020-03-17T03:46:29.1146355Z 	at org.apache.hadoop.fs.s3a.S3AFileSystem.setUri(S3AFileSystem.java:467)
2020-03-17T03:46:29.1147156Z 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:234)
2020-03-17T03:46:29.1148468Z 	at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:126)
2020-03-17T03:46:29.1148901Z 	... 30 more
2020-03-17T03:46:29.1149032Z 
2020-03-17T03:46:29.1149237Z End of exception on server side>]
2020-03-17T03:46:29.1149603Z 	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:390)
2020-03-17T03:46:29.1150123Z 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:374)
2020-03-17T03:46:29.1150761Z 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
2020-03-17T03:46:29.1151980Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
2020-03-17T03:46:29.1156319Z 	... 4 more
2020-03-17T03:46:29.1288717Z Job could not be submitted.
2020-03-17T03:46:29.8870683Z 
2020-03-17T03:46:30.6358869Z rm: cannot remove '/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-shaded-netty-tcnative-static-*.jar': No such file or directory
2020-03-17T03:46:30.8096233Z ad0803bd62b2c6245dcfc51f4a5345bf69f56d60c1bc4827792dda44f72164f0
2020-03-17T03:46:30.8578725Z ad0803bd62b2c6245dcfc51f4a5345bf69f56d60c1bc4827792dda44f72164f0
2020-03-17T03:46:30.8607224Z [FAIL] Test script contains errors.
2020-03-17T03:46:30.8614963Z Checking of logs skipped.
2020-03-17T03:46:30.8615321Z 
2020-03-17T03:46:30.8616405Z [FAIL] 'Streaming File Sink s3 end-to-end test' failed after 0 minutes and 42 seconds! Test exited with exit code 1
{code}

The YARN tests are also affected
{code}
[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 1.156 s <<< FAILURE! - in org.apache.flink.yarn.YarnFileStageTestS3ITCase
[ERROR] testRecursiveUploadForYarnS3a(org.apache.flink.yarn.YarnFileStageTestS3ITCase)  Time elapsed: 0.592 s  <<< ERROR!
java.io.IOException: Cannot instantiate file system for URI: s3a://$(IT_CASE_S3_BUCKET)/temp/tests-e694ca1f-5578-4b67-b936-7db933e35da0
	at org.apache.flink.yarn.YarnFileStageTestS3ITCase.testRecursiveUploadForYarn(YarnFileStageTestS3ITCase.java:159)
	at org.apache.flink.yarn.YarnFileStageTestS3ITCase.testRecursiveUploadForYarnS3a(YarnFileStageTestS3ITCase.java:199)
Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
	at org.apache.flink.yarn.YarnFileStageTestS3ITCase.testRecursiveUploadForYarn(YarnFileStageTestS3ITCase.java:159)
	at org.apache.flink.yarn.YarnFileStageTestS3ITCase.testRecursiveUploadForYarnS3a(YarnFileStageTestS3ITCase.java:199)
{code}",,rmetzger,,,,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11430: [FLINK-16633][AZP] Fix builds without s3 credentials
URL: https://github.com/apache/flink/pull/11430
 
 
   ## What is the purpose of the change
   
   Problem: Builds on Azure where failing if S3 credentials were not provided, because the syntax in the pipeline definition was incorrect.
   This mostly affected PR builds, because they don't have access to the credentials.
   
   ## Brief change log
   
     - Use a way to access secret variables that leads to an empty value (instead of the variable name). Also, extend the check in the unit tests to ""variable empty"" instead of just ""variable not set"".
   
   ## Verifying this change
   
   I triggered a build [with](https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=178&view=results) and [without](https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=176&view=results) credentials + there is the test run of this PR.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (**yes** / no / don't know)
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/20 17:10;githubbot;600","rmetzger commented on pull request #11430: [FLINK-16633][AZP] Fix builds without s3 credentials
URL: https://github.com/apache/flink/pull/11430
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Mar/20 10:46;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-16644,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 20 10:46:48 UTC 2020,,,,,,,,,,"0|z0clq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/20 10:46;rmetzger;Resolved in master in b226cbd1f595b405f5da376fa3d22e1850fbf4e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SqlDateTimeUtils#toSqlTimestamp(String, String) may yield incorrect result",FLINK-16632,13292137,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,docete,lzljs3620320,lzljs3620320,17/Mar/20 09:14,08/Apr/20 03:13,13/Jul/23 08:07,08/Apr/20 03:13,,,,,,1.10.1,1.11.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Legacy planner support SQL: ""CAST('1999-9-10' AS TIMESTAMP)"".

Blink planner loose this support after timestamp precision support.",,jark,kezhuw,libenchao,lzljs3620320,wuyanzu,,,,,,,,,,,,,,,,,"docete commented on pull request #11654: [FLINK-16632][table-planner-blink] Fix SqlDateTimeUtils#toSqlTimestam…
URL: https://github.com/apache/flink/pull/11654
 
 
   …p(String, String) may yield incorrect result
   
   
   ## What is the purpose of the change
   
   `SqlDateTimeUtils#toSqlTimestamp(String, String)` may yield incorrect result. Which cause:
   
   1) cast STRING to TIMESTAMP yields incompatible result. 
   The original result comes from `DateTimeUtils#timestampStringToUnixDate` which supports special cases like '1999-9-10 05:20:10' or '1999-9-10'.
   
   2) TO_TIMESTAMP yields incorrect result.
   The original result comes from `SqlDateTimeUtils#toTimestamp(String, String, TimeZone)` which follows rules of completion of java.text.SimpleDateFormat
   
   This PR fix that.
   
   ## Brief change log
   - 11a9a2f Fix SqlDateTimeUtils#toSqlTimestamp(String, String) may yield incorrect result
   
   ## Verifying this change
   
   This change added tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (**yes** / no)
     - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Apr/20 08:54;githubbot;600","wuchong commented on pull request #11654: [FLINK-16632][table-planner-blink] Fix SqlDateTimeUtils#toSqlTimestam…
URL: https://github.com/apache/flink/pull/11654
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Apr/20 03:12;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/20 07:39;wuyanzu;image-2020-04-03-15-39-35-702.png;https://issues.apache.org/jira/secure/attachment/12998708/image-2020-04-03-15-39-35-702.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 03:13:45 UTC 2020,,,,,,,,,,"0|z0clq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/20 07:57;wuyanzu;!image-2020-04-03-15-39-35-702.png!

I found a bug related to this one, TO_TIMESTAMP('2020040315','yyyyMMddHH'), it will be regarded as a date format, and erase the hour field to '00'

Maybe we should check  whether the format only contain the date field ('y','M','d') when the dateStr length <= 10;;;;","05/Apr/20 12:57;ykt836;[~docete] Could you take a look?;;;","07/Apr/20 08:21;docete;SqlDateTimeUtils#toSqlTimestamp(String, String) may yield incorrect result. Which cause:

1) cast STRING to TIMESTAMP yields incompatible result. 

The original result comes from DateTimeUtils#timestampStringToUnixDate which supports special cases 

like '1999-9-10 05:20:10' or '1999-9-10'.

2) TO_TIMESTAMP yields incorrect result.

The original result comes from SqlDateTimeUtils#toTimestamp(String, String, TimeZone) which follows rules of completion of java.text.SimpleDateFormat

Will file a PR to fix this;;;","08/Apr/20 03:13;jark;Fixed in 

 - master(1.11.0): dd6d40fd82b5ac945a95783f1a50cd35fdeadbb1

 - 1.10.1: 1c5b9b0940a73afd5fd809db085470040899a2f1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming bucketing end-to-end test output hash mismatch,FLINK-16629,13292119,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,pnowojski,pnowojski,17/Mar/20 07:32,25/Mar/20 07:54,13/Jul/23 08:07,25/Mar/20 07:54,1.11.0,,,,,1.11.0,,,,API / DataStream,Tests,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6298/logs/722

Some of the output mismatch failures were reported in another ticket: https://issues.apache.org/jira/browse/FLINK-16227

{code}
2020-03-17T02:04:19.9176915Z Number of produced values 30618/60000
2020-03-17T02:04:19.9202731Z Truncating buckets
2020-03-17T02:04:25.0504959Z Truncating buckets
2020-03-17T02:04:30.1731295Z Truncating buckets
2020-03-17T02:04:35.3190114Z Truncating buckets
2020-03-17T02:04:40.4723887Z Truncating buckets
2020-03-17T02:04:45.5984655Z Truncating buckets
2020-03-17T02:04:50.7185356Z Truncating buckets
2020-03-17T02:04:55.8627129Z Truncating buckets
2020-03-17T02:05:01.0715985Z Number of produced values 74008/60000
2020-03-17T02:05:02.3976850Z Cancelling job dba2fdb79579158295db27d0214fc2ff.
2020-03-17T02:05:03.4633541Z Cancelled job dba2fdb79579158295db27d0214fc2ff.
2020-03-17T02:05:03.4738270Z Waiting for job (dba2fdb79579158295db27d0214fc2ff) to reach terminal state CANCELED ...
2020-03-17T02:05:03.5149228Z Job (dba2fdb79579158295db27d0214fc2ff) reached terminal state CANCELED
2020-03-17T02:05:03.5150587Z Job dba2fdb79579158295db27d0214fc2ff was cancelled, time to verify
2020-03-17T02:05:03.5590118Z FAIL Bucketing Sink: Output hash mismatch.  Got c3787e7a52d913675e620837a7531742, expected 01aba5ff77a0ef5e5cf6a727c248bdc3.
2020-03-17T02:05:03.5591888Z head hexdump of actual:
2020-03-17T02:05:03.5989908Z 0000000   (   7   ,   1   0   ,   0   ,   S   o   m   e       p   a   y
2020-03-17T02:05:03.5991252Z 0000010   l   o   a   d   .   .   .   )  \n   (   7   ,   1   0   ,   1
2020-03-17T02:05:03.5991923Z 0000020   ,   S   o   m   e       p   a   y   l   o   a   d   .   .   .
2020-03-17T02:05:03.5993055Z 0000030   )  \n   (   7   ,   1   0   ,   2   ,   S   o   m   e       p
2020-03-17T02:05:03.5993690Z 0000040   a   y   l   o   a   d   .   .   .   )  \n   (   7   ,   1   0
2020-03-17T02:05:03.5994332Z 0000050   ,   3   ,   S   o   m   e       p   a   y   l   o   a   d   .
2020-03-17T02:05:03.5994967Z 0000060   .   .   )  \n   (   7   ,   1   0   ,   4   ,   S   o   m   e
2020-03-17T02:05:03.5995744Z 0000070       p   a   y   l   o   a   d   .   .   .   )  \n   (   7   ,
2020-03-17T02:05:03.5996359Z 0000080   1   0   ,   5   ,   S   o   m   e       p   a   y   l   o   a
2020-03-17T02:05:03.5997133Z 0000090   d   .   .   .   )  \n   (   7   ,   1   0   ,   6   ,   S   o
2020-03-17T02:05:03.5997704Z 00000a0   m   e       p   a   y   l   o   a   d   .   .   .   )  \n   (
2020-03-17T02:05:03.5998295Z 00000b0   7   ,   1   0   ,   7   ,   S   o   m   e       p   a   y   l
2020-03-17T02:05:03.5999087Z 00000c0   o   a   d   .   .   .   )  \n   (   7   ,   1   0   ,   8   ,
2020-03-17T02:05:03.6000243Z 00000d0   S   o   m   e       p   a   y   l   o   a   d   .   .   .   )
2020-03-17T02:05:03.6000880Z 00000e0  \n   (   7   ,   1   0   ,   9   ,   S   o   m   e       p   a
2020-03-17T02:05:03.6001494Z 00000f0   y   l   o   a   d   .   .   .   )  \n                        
2020-03-17T02:05:03.6001999Z 00000fa
2020-03-17T02:05:03.9875220Z Stopping taskexecutor daemon (pid: 49278) on host fv-az668.
2020-03-17T02:05:04.2569285Z Stopping standalonesession daemon (pid: 46323) on host fv-az668.
2020-03-17T02:05:04.7664418Z Stopping taskexecutor daemon (pid: 46615) on host fv-az668.
2020-03-17T02:05:04.7674722Z Skipping taskexecutor daemon (pid: 47009), because it is not running anymore on fv-az668.
2020-03-17T02:05:04.7687383Z Skipping taskexecutor daemon (pid: 47299), because it is not running anymore on fv-az668.
2020-03-17T02:05:04.7689091Z Skipping taskexecutor daemon (pid: 47619), because it is not running anymore on fv-az668.
2020-03-17T02:05:04.7690289Z Stopping taskexecutor daemon (pid: 48538) on host fv-az668.
2020-03-17T02:05:04.7691796Z Stopping taskexecutor daemon (pid: 48988) on host fv-az668.
2020-03-17T02:05:04.7692365Z [FAIL] Test script contains errors.
2020-03-17T02:05:04.7713750Z Checking of logs skipped.
2020-03-17T02:05:04.7714249Z 
2020-03-17T02:05:04.7715316Z [FAIL] 'Streaming bucketing end-to-end test' failed after 2 minutes and 43 seconds! Test exited with exit code 1
{code}",,kkl0u,rmetzger,zjwang,,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11464: [FLINK-16629] Ignore Streaming bucketing end-to-end test with Hadoop 2.4.1
URL: https://github.com/apache/flink/pull/11464
 
 
   ## What is the purpose of the change
   
   The stream bucketing end to end test does not work on Hadoop 2.4.1 because it is lacking the `truncate()` method of Hadoop --> We need to exclude it.
   
   
   
   
   ## Verifying this change
   
   Validated here: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=208&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Mar/20 10:41;githubbot;600","rmetzger commented on pull request #11464: [FLINK-16629] Ignore Streaming bucketing end-to-end test with Hadoop 2.4.1
URL: https://github.com/apache/flink/pull/11464
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 07:54;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16227,,FLINK-16616,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 07:54:21 UTC 2020,,,,,,,,,,"0|z0clm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/20 09:38;kkl0u;[~pnowojski] I think that based on the discussion [here|https://lists.apache.org/list.html?dev@flink.apache.org:lte=1M:drop%20bucketing%20sink] the plan is to remove the {{BucketingSink}} so I am not sure if it is worth spending cycles on fixing this. What do you think?;;;","17/Mar/20 09:40;rmetzger;How about disabling the bucketing sink test when running in the hadoop 2.4.1 profile? (the test passes with other hadoop versions);;;","17/Mar/20 10:33;kkl0u;[~rmetzger] Only in 2.4.1? This seems to make things even weirder. Can this be that something was fixed in Hadoop and fixes also our instability?;;;","17/Mar/20 12:20;rmetzger;We could go through old builds to see around which change the error started occurring.

I suspect the recent work on the file system plugins might have changed something non-deterministic?;;;","17/Mar/20 13:14;kkl0u;If this is not a big deal for you, then I think that this could make sense even if we are planning to remove the bucketing sink. What do you think?;;;","17/Mar/20 17:14;rmetzger;Uff, I'm getting the impression that the last few days I'm doing more harm than good :/ 
While checking if the test also fails regularly on Travis, I realized that we are not running the end to end tests with Hadoop 2.4.1 there.
This issue has probably been in the nightly end to end tests ever since I enabled them in Azure (when I enabled nightly tests, the e2e tests were still failing).

Maybe the issue is just that in Hadoop 2.4.1 the {{truncate()}} call is not available?

I still vote to disable the test on HD 2.4.1;;;","19/Mar/20 11:09;kkl0u;Yes, I think that given that we are probably going to deprecate the sink, we can disable the test. ;;;","19/Mar/20 12:18;rmetzger;Okay, I will create a PR;;;","24/Mar/20 05:51;zjwang;Another instance :[https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6546&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=375367d9-d72e-5c21-3be0-b45149130f6b]

 [https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6546/logs/679]

 ;;;","25/Mar/20 06:29;zjwang;Another instance [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6591&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=68a897ab-3047-5660-245a-cce8f83859f6];;;","25/Mar/20 07:54;rmetzger;Test disabled for Hadoop 2.4.1 profile in 811c3be9beee6fd3d1c2482e70f1f57a67dc769c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent REST handler from being closed more than once,FLINK-16626,13292106,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kyledong,chaiyq,chaiyq,17/Mar/20 05:01,12/Apr/20 10:28,13/Jul/23 08:07,12/Apr/20 10:28,1.10.0,,,,,1.10.1,1.11.0,,,Deployment / YARN,,,,,0,pull-request-available,,,,"In Flink 1.10.0 release, job cancellation can be problematic, as users frequently experience java.util.concurrent.TimeoutException at the client side, because the REST endpoint closes pre-maturely before sending out the response, this is because the jobCancellationHandler is incorrectly reused and closed twice.
 
When executing the following command to stop a flink job with yarn per-job mode, the client keeps retrying untill timeout (1 minutes）and exit with failure. But the job stops successfully.
 Command :
{noformat}
flink cancel $jobId yid appId
{noformat}
 The exception on the client side is :
{quote}{quote} 
{quote}
2020-03-17 12:32:13,709 DEBUG org.apache.flink.runtime.rest.RestClient - Sending request of class class org.apache.flink.runtime.rest.messages.EmptyRequestBody to ocdt31.aicloud.local:51231/v1/jobs/cc61033484d4c0e7a27a8a2a36f4de7a?mode=cancel
 ...
 2020-03-17 12:33:11,065 DEBUG org.apache.flink.runtime.rest.RestClient - Sending request of class class org.apache.flink.runtime.rest.messages.EmptyRequestBody to ocdt31.aicloud.local:51231/v1/jobs/cc61033484d4c0e7a27a8a2a36f4de7a?mode=cancel
 2020-03-17 12:33:14,070 DEBUG org.apache.flink.runtime.rest.RestClient - Shutting down rest endpoint.
 2020-03-17 12:33:14,070 DEBUG org.apache.flink.runtime.rest.RestClient - Sending request of class class org.apache.flink.runtime.rest.messages.EmptyRequestBody to ocdt31.aicloud.local:51231/v1/jobs/cc61033484d4c0e7a27a8a2a36f4de7a?mode=cancel
 2020-03-17 12:33:14,077 DEBUG org.apache.flink.shaded.netty4.io.netty.buffer.PoolThreadCache - Freed 2 thread-local buffer(s) from thread: flink-rest-client-netty-thread-1
 2020-03-17 12:33:14,080 DEBUG org.apache.flink.runtime.rest.RestClient - Rest endpoint shutdown complete.
 2020-03-17 12:33:14,083 ERROR org.apache.flink.client.cli.CliFrontend - Error while running the command.
 org.apache.flink.util.FlinkException: Could not cancel job cc61033484d4c0e7a27a8a2a36f4de7a.
 at org.apache.flink.client.cli.CliFrontend.lambda$cancel$7(CliFrontend.java:545)
 at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:843)
 at org.apache.flink.client.cli.CliFrontend.cancel(CliFrontend.java:538)
 at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:904)
 at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754)
 at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
 at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)
 Caused by: java.util.concurrent.TimeoutException
 at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771)
 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
 at org.apache.flink.client.cli.CliFrontend.lambda$cancel$7(CliFrontend.java:543)
 ... 9 more

------------------------------------------------------------
 The program finished with the following exception:

org.apache.flink.util.FlinkException: Could not cancel job cc61033484d4c0e7a27a8a2a36f4de7a.
 at org.apache.flink.client.cli.CliFrontend.lambda$cancel$7(CliFrontend.java:545)
 at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:843)
 at org.apache.flink.client.cli.CliFrontend.cancel(CliFrontend.java:538)
 at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:904)
 at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754)
 at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
 at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)
 Caused by: java.util.concurrent.TimeoutException
 at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771)
 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
 at org.apache.flink.client.cli.CliFrontend.lambda$cancel$7(CliFrontend.java:543)
 ... 9 more
{quote}
Actually, the job was cancelled. But the server also prints some exception:

 
{quote}2020-03-17 12:25:13,754 ERROR [flink-akka.actor.default-dispatcher-17] org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:766) - Failed to submit a listener notification task. Event loop shut down? java.util.concurrent.RejectedExecutionException: event executor terminated at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:855) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:340) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:333) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:766) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:764) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:421) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:149) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30) at org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse(HandlerUtils.java:224) at org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse(HandlerUtils.java:176) at org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse(HandlerUtils.java:91) at org.apache.flink.runtime.rest.handler.AbstractRestHandler.lambda$respondToRequest$0(AbstractRestHandler.java:78) at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:656) at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:632) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:874) at akka.dispatch.OnComplete.internal(Future.scala:264) at akka.dispatch.OnComplete.internal(Future.scala:261) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74) at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21) at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436) at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{quote}",,aljoscha,chaiyq,felixzheng,gyfora,klion26,kyledong,leishuiyu,liyu,tartarus,tison,trohrmann,wangyang0918,,,,,,,,,,"kylemeow commented on pull request #11639: [FLINK-16626][runtime] Prevent REST handler from being closed more than once
URL: https://github.com/apache/flink/pull/11639
 
 
   ## What is the purpose of the change
   
   In Flink 1.10.0 release, job cancellation can be problematic, as users frequently experience *java.util.concurrent.TimeoutException* at the client side, because the REST endpoint closes pre-maturely before sending out the response. 
   
   After discussion with the community and research, it is shown that there are two issues to address:
   1. AbstractHandler and its subclasses can be closed more than once (whether intentionally or unintentionally), so this might lead to unexpected behavior like exceptions, especially when interacting with external systems, or unintended deregistration of Phaser in the handler instance which causes early shutdown of the cluster. 
   2. In WebMonitorEndpoint class, the same jobCancelTerminationHandler instance has been registered twice, thus during handler closure process, *closeAsync* method is called twice, therefore, the cluster pre-maturely entered internalShutdown process, leaving unfinished responses behind.
   
   ## Brief change log
   
   - Added an AtomicBoolean field to prevent closeAsync method of one handler instance from being called multiple times.
   - Added a new legacyJobCancelTerminationHandler to prevent reuse of existing jobCancelTerminationHandler handler instance.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   YARNJobCancellationITCase
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: yes
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Apr/20 14:51;githubbot;600","zentol commented on pull request #11639: [FLINK-16626][runtime] Prevent REST handler from being closed more than once
URL: https://github.com/apache/flink/pull/11639
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Apr/20 10:28;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-16637,,,,,,,,,,,,,,FLINK-16637,,,,,,,,,,,"28/Mar/20 04:45;tison;jobmanager.log;https://issues.apache.org/jira/secure/attachment/12998058/jobmanager.log","28/Mar/20 04:45;tison;patch.diff;https://issues.apache.org/jira/secure/attachment/12998059/patch.diff",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 12 10:28:49 UTC 2020,,,,,,,,,,"0|z0clj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/20 05:09;chaiyq;In my opinion, when the client call an cancel command, the server will run into the following logic :
 # call the JobCancellationHandler to handle the request
 # response to the client

Codes as follows:
{code:java}
// @Override
protected CompletableFuture<Void> respondToRequest(ChannelHandlerContext ctx, HttpRequest httpRequest, HandlerRequest<R, M> handlerRequest, T gateway) {
   CompletableFuture<P> response;

   try {
      response = handleRequest(handlerRequest, gateway);
   } catch (RestHandlerException e) {
      response = FutureUtils.completedExceptionally(e);
   }

   return response.thenAccept(resp -> HandlerUtils.sendResponse(ctx, httpRequest, resp, messageHeaders.getResponseStatusCode(), responseHeaders));
}
{code}
But in the handleRequest function of `JobCancellationHandler`, the cluster will stop and call to shutdown the RestServerEndpoint in which the EventLoopGroup also will be shutdown.

 

Then when calling `HandlerUtils.sendResponse`  to response to the client,  exception occurs.

 ;;;","17/Mar/20 08:18;wangyang0918;I think not only {{flink cancel}}, other operations may come across the similar problems. Since the dispatcher may exit before the Flink client has received the response.;;;","18/Mar/20 02:23;chaiyq; Maybe you'ra right. As far as i see, *flink stop* is going to be removed .Only *flink cancel* comes into this logic. Also there could be some other situation i haven't noticed.;;;","23/Mar/20 10:27;tartarus;We encountered the same problem. So I solved it temporarily and did not wait for the result of the cancel and returned directly.
{code:java}
//代码占位符
return CompletableFuture.completedFuture(EmptyResponseBody.getInstance());
//	return terminationFuture.handle(
//		(Acknowledge ack, Throwable throwable) -> {
//			if (throwable != null) {
//			Throwable error = ExceptionUtils.stripCompletionException(throwable);
//				if (error instanceof TimeoutException) {
//					throw new CompletionException(
//						new RestHandlerException(
//							""Job cancellation timed out."",
//							HttpResponseStatus.REQUEST_TIMEOUT, error));
//				} else if (error instanceof FlinkJobNotFoundException) {
//					throw new CompletionException(
//						new RestHandlerException(
//							""Job could not be found."",
//							HttpResponseStatus.NOT_FOUND, error));
//				} else {
//					throw new CompletionException(
//						new RestHandlerException(
//							""Job cancellation failed: "" + error.getMessage(),
//							HttpResponseStatus.INTERNAL_SERVER_ERROR, error));
//				}
//			} else {
//				return EmptyResponseBody.getInstance();
//			}
//		});
{code}
But this is equivalent to changed the semantics of cancel!!!
Equivalent to just sending a signal to JM.
Do you have any good suggestions?
;;;","28/Mar/20 01:18;tison;Could you reproduce that issue and share TRACE level JM log?

Some important trace of {{AbstractHandler}} is required to locate the root cause.;;;","28/Mar/20 02:20;tison;I've submitted a local YARN testing log, which indicates that ""finalizeRequestProcessing"" happens after cluster shutdown. Will investigate further later.;;;","28/Mar/20 02:26;tison;For coming developer: you can duplicate {{YARNITCase}}, insert a cancel command, and add log for locally debugging.;;;","28/Mar/20 04:19;tison;I found that {{org.apache.flink.runtime.rest.handler.InFlightRequestTracker#awaitAsync()}} is called more than once which cause {{Phaser}} synchronization meaningless. Further debugging...;;;","28/Mar/20 05:01;tison;link to the analysis

https://issues.apache.org/jira/browse/FLINK-16637?focusedCommentId=17063164&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17063164
https://issues.apache.org/jira/browse/FLINK-16637?focusedCommentId=17069225&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17069225
https://issues.apache.org/jira/browse/FLINK-16637?focusedCommentId=17069226&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17069226
https://issues.apache.org/jira/browse/FLINK-16637?focusedCommentId=17069227&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17069227

and closed FLINK-16637 as duplication;;;","28/Mar/20 09:52;tartarus;This is  JM logs,but It's a surface phenomenon.
{code:java}
2020-03-22 17:16:14,770 ERROR org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.rejectedExecution[flink-akka.actor.default-dispatcher-20]  - Failed to submit a listener notification task. Event loop shut down?
java.util.concurrent.RejectedExecutionException: event executor terminated
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:855)
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:340)
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:333)
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:766)
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:764)
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:421)
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:149)
	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
	at org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse(HandlerUtils.java:224)
	at org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse(HandlerUtils.java:176)
	at org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse(HandlerUtils.java:91)
	at org.apache.flink.runtime.rest.handler.AbstractRestHandler.lambda$respondToRequest$0(AbstractRestHandler.java:78)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:656)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:632)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:874)
	at akka.dispatch.OnComplete.internal(Future.scala:264)
	at akka.dispatch.OnComplete.internal(Future.scala:261)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}
;;;","30/Mar/20 06:38;wangyang0918; [~tison] Thanks a lot for your analysis. I think you are right. However, i have checked the YARN code, even in the yarn-3.x, the proxy still could not support {{PATCH}} request. And the hadoop community does not have a clear plan to fix it.

Because of this, we still can not remove the {{YarnCancelJobTerminationHeaders}}, i suggest to create a new {{JobCancellationHandler}} object for yarn-cancel. It should fix this problem.

 

>> Side output

Currently, the {{YarnCancelJobTerminationHeaders}} is only used via webui cancelation. The {{flink cancel}} is using {{JobCancellationHeaders}} and will directly talk to jobmanager(not using the YARN application proxy). For other deployment(standalone, K8s), they are also using {{YarnCancelJobTerminationHeaders}} for the webui cancelation. It is maybe a small trick for the name.;;;","30/Mar/20 07:21;wangyang0918;[~tison] The reason why {{InFlightRequestTracker#awaitAsync}} is called twice in your analysis is that we are using a same {{jobCancelTerminationHandler}} for two different handlers, {{JobCancelTerminationHandler}} and {{YarnCancelJobTerminationHeaders}}.;;;","01/Apr/20 02:26;kyledong;Hi [~tison], I am quite interested in solving this issue and have been following this for a while, so I guess I can help fix this bug : );;;","01/Apr/20 02:34;tison;[~kyledong] Thanks for your interest. I've assigned to you.

[~liyu] I raise this issue as blocker for 1.10.1 since it broke user interface({{flink cancel}}) and it's hopeful we fix it in days. Comment if you have any concern.;;;","01/Apr/20 02:54;liyu;Thanks for the note [~tison], will watch this one.

And thanks for standing up [~kyledong], no mean to push but hopefully we could get a thorough fix soon (smile).;;;","01/Apr/20 06:27;kyledong;Thanks [~liyu]  [~tison] for watching this issue, and I will try my best to fix and test it thoroughly : );;;","12/Apr/20 10:28;chesnay;master: a882498f1dba79f34b329323c3faf0374b74bd82

1.10: 593214993e07a939c96b6bc19e16c7daa110e39d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automatic Any unpacking is broken with type deduction.,FLINK-16617,13291955,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igal,igal,igal,16/Mar/20 11:57,17/Mar/20 06:11,13/Jul/23 08:07,17/Mar/20 06:11,,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"Automatic Any unpacking is broken when a function is annotated with type hints.

 ",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,"igalshilman commented on pull request #60: [FLINK-16617][py] Use Protobuf DESCRIPTOR instead of class
URL: https://github.com/apache/flink-statefun/pull/60
 
 
   The `any.Is()` method requires a Protobuf descriptor,
   instead of a class, this PR fixes it.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/20 12:09;githubbot;600","tzulitai commented on pull request #60: [FLINK-16617][py] Use Protobuf DESCRIPTOR instead of class
URL: https://github.com/apache/flink-statefun/pull/60
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/20 06:11;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 17 06:11:56 UTC 2020,,,,,,,,,,"0|z0cklk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/20 06:11;tzulitai;Fixed in master via fc5276f713009ce2747dc264ee4b4d66fd3753de.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
statefun-quickstart generates invalid pom.xml,FLINK-16603,13291803,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tzulitai,igal,igal,15/Mar/20 12:38,17/Mar/20 09:56,13/Jul/23 08:07,17/Mar/20 09:56,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"Generating a stateful functions via the mvn archetype statefun-quickstart

doesn't generate a valid pom.xml,

steps to reproduce:

1)
{code:java}
 mvn archetype:generate                    \
  -DarchetypeGroupId=org.apache.flink \
  -DarchetypeArtifactId=statefun-quickstart \
  -DarchetypeVersion=1.1-SNAPSHOT 

{code}
2) 
{code:java}
cd <generated project>
mvn clean install {code}",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,"tzulitai commented on pull request #59: [FLINK-16603] Fix quickstart Maven archetype
URL: https://github.com/apache/flink-statefun/pull/59
 
 
   The quickstart Maven archetype was not working due to a few issues:
   
   - the archetype `pom.xml` not being included for filtering, causing the `@project.version@` string not being replaced with the project version.
   - the archetype `pom.xml` did not define `modelVersion`
   
   ---
   
   ### Verifying
   
   An integration-test phase test has been added to verify that projects generated with the archetype can be compiled.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/20 08:02;githubbot;600","tzulitai commented on pull request #59: [FLINK-16603] Fix quickstart Maven archetype
URL: https://github.com/apache/flink-statefun/pull/59
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/20 09:55;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 17 09:56:15 UTC 2020,,,,,,,,,,"0|z0cjns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/20 09:56;tzulitai;Fixed in master via 39f9916a8569ecb9e3a4e295d245f22cfcfd5b6e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Respect the rest.bind-port for the Kubernetes setup,FLINK-16600,13291772,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,felixzheng,felixzheng,felixzheng,15/Mar/20 05:57,18/Apr/20 15:35,13/Jul/23 08:07,18/Apr/20 15:35,1.10.0,,,,,1.11.0,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"Our current logic only takes care of {{RestOptions.PORT}} but not {{RestOptions.BIND_PORT}}, which is a bug.

For example, when one sets the {{RestOptions.BIND_PORT}} to a value other than {{RestOptions.PORT}}, jobs could not be submitted to the existing session cluster deployed via the kubernetes-session.sh.",,felixzheng,tison,,,,,,,,,,,,,,,,,,,,"zhengcanbin commented on pull request #11705: [FLINK-16600][k8s] Fix not respecting the RestOptions.BIND_PORT for the Kubernetes setup
URL: https://github.com/apache/flink/pull/11705
 
 
   ## What is the purpose of the change
   
   Our current logic only takes care of RestOptions.PORT but not RestOptions.BIND_PORT, which is a bug; for example, when one sets the RestOptions.BIND_PORT to a value different from RestOptions.PORT, jobs could not be submitted to the existing session cluster deployed via the kubernetes-session.sh.
   
   This PR fixes the bug.
   
   
   ## Brief change log
   
   Set the RestOptions.BIND_PORT as the target port of the external Service.
   
   
   ## Verifying this change
   
   This change can be tested via the existing unit tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Apr/20 02:52;githubbot;600","TisonKun commented on pull request #11705: [FLINK-16600][k8s] Fix not respecting the RestOptions.BIND_PORT for the Kubernetes setup
URL: https://github.com/apache/flink/pull/11705
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Apr/20 15:35;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 15:35:57 UTC 2020,,,,,,,,,,"0|z0cjgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/20 12:32;felixzheng;Hi [~tison] [~trohrmann]. Could you help assign this ticket to me?;;;","18/Apr/20 15:35;tison;master(1.11) via 7a7aaec9745461da9590c61f6ba75be64388d397;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Respect the rest port exposed by Service in Fabric8FlinkKubeClient#getRestEndpoint,FLINK-16598,13291765,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,felixzheng,felixzheng,felixzheng,15/Mar/20 03:51,16/Apr/20 06:09,13/Jul/23 08:07,16/Apr/20 06:09,1.10.0,,,,,1.11.0,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"Steps to reproduce the problem:
 # Start a Kubernetes session cluster, by default, the rest port exposed by the rest Service is 8081.
 # Submit a job to the session cluster, but specify a different rest port via -Drest.port=8082.

As we can see, the job could never be submitted to the existing session cluster since we retrieve the port of the Endpoint from the Flink configuration object instead of the created Service.",,felixzheng,tison,,,,,,,,,,,,,,,,,,,,"zhengcanbin commented on pull request #11715: [FLINK-16598][k8s] Respect the rest-port exposed by the external Service when retrieving Endpoint
URL: https://github.com/apache/flink/pull/11715
 
 
   ## What is the purpose of the change
   
   Currently, we parse the `rest.port` from the Flink `Configuration` when a user submits his jobs to an existing native Kubernetes session cluster, this is definitely wrong since we do not respect the real port exposed by the external Service created when deploying the session cluster. This PR will fix this problem.
   
   
   ## Brief change log
   
     - *Refactor the logic on retrieving the rest-port in the method of `Fabric8FlinkKubeClient#getRestEndpoint`*
     - *Introduce a new test class `KubernetesClientTestBase` which provides some tools for the Service*
   
   
   ## Verifying this change
   
   This change added unit tests and can be verified as follows:
   
     - *Start a native Kubernetes session cluster, by default, the rest port exposed by the rest Service is 8081*
     - *Then submit a job to the session cluster, but specify a different rest port via -Drest.port=8082*
     - *As expected, the job is successfully submitted to the session cluster*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Apr/20 08:44;githubbot;600","TisonKun commented on pull request #11715: [FLINK-16598][k8s] Respect the rest-port exposed by the external Service when retrieving Endpoint
URL: https://github.com/apache/flink/pull/11715
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/20 03:20;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 06:09:59 UTC 2020,,,,,,,,,,"0|z0cjfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/20 12:33;felixzheng;Hi [~tison] [~trohrmann]. Could you help assign this ticket to me?;;;","16/Apr/20 06:09;tison;master(1.11) via 135d2e2c8c493d023395c0146989c759ccb1a8d5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-zh Doc show a wrong Email address,FLINK-16591,13291673,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,forideal,forideal,forideal,14/Mar/20 03:57,01/Apr/20 10:43,13/Jul/23 08:07,01/Apr/20 10:43,,,,,,1.11.0,,,,chinese-translation,Project Website,,,,0,pull-request-available,,,,"The Link is https://flink.apache.org/zh/community.html
发送一封不包含任何内容或主题的邮件到 subscribe-listname@flink.apache.org（替换 listname 为 dev, user, user-zh 等等）

The right Email pattern is listname-subscribe@flink.apache.org.",,forideal,jark,tison,,,,,,,,,,,,,,,,,,,"blacksunshine commented on pull request #319: [FLINK-16591][chinese-translation] Flink-zh Doc show a wrong Email ad…
URL: https://github.com/apache/flink-web/pull/319
 
 
   Flink-zh Doc show a wrong Email address
   
   The Link is https://flink.apache.org/zh/community.html
   发送一封不包含任何内容或主题的邮件到 subscribe-listname@flink.apache.org（替换 listname 为 dev, user, user-zh 等等）
   
   The right Email pattern is listname-subscribe@flink.apache.org.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 09:21;githubbot;600","wuchong commented on pull request #319: [FLINK-16591][chinese-translation] Flink-zh Doc show a wrong Email ad…
URL: https://github.com/apache/flink-web/pull/319
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 10:42;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 10:43:18 UTC 2020,,,,,,,,,,"0|z0ciuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/20 04:02;forideal;Hi [~tison] Can you assign this issue to me ? Thank you！;;;","14/Mar/20 04:50;tison;Yes of course! Please mark the issue as ""In Progress"" by clicking ""Start Progress"" button above when you start progress.;;;","01/Apr/20 10:43;jark;Fixed in flink-web: https://github.com/apache/flink-web/commit/7fd3cf3d1fbb2040a7792eb0ee8f5cb1de05be21;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-oss-fs-hadoop: Not all dependencies in NOTICE file are bundled,FLINK-16590,13291591,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,gjy,gjy,13/Mar/20 15:58,07/Apr/20 10:46,13/Jul/23 08:07,07/Apr/20 10:45,1.11.0,,,,,1.11.0,,,,Connectors / FileSystem,Release System,,,,0,pull-request-available,,,,NOTICE file in flink-oss-fs-hadoop lists {{org.apache.commons:commons-compress}} as a bundled dependency which is not correct. There are likely other dependencies that are wrongly listed in the NOTICE file.,,aljoscha,gjy,,,,,,,,,,,,,,,,,,,,"aljoscha commented on pull request #11644: [FLINK-16590] flink-oss-fs-hadoop: remove erroneous license references
URL: https://github.com/apache/flink/pull/11644
 
 
   I checked the output of the maven shade plugin on `flink-oss-fs-hadoop`, combined that with `flink-fs-hadoop-shaded`, sorted, cleaned it up. Then took this as the basis and cleaned up the `NOTICE` file and licenses.
   
   @zentol Could you please review this?
   
   
   cc @GJL 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Apr/20 09:00;githubbot;600","aljoscha commented on pull request #11644: [FLINK-16590] flink-oss-fs-hadoop: remove erroneous license references
URL: https://github.com/apache/flink/pull/11644
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Apr/20 10:46;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,FLINK-15835,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 07 10:45:42 UTC 2020,,,,,,,,,,"0|z0cico:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/20 10:45;aljoscha;master: 2e74f25a8465c97924c367efc683fa6e1b6c5a0f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Table SQL fails/crashes with big queries with lots of fields,FLINK-16589,13291536,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,libenchao,StarGhost,StarGhost,13/Mar/20 11:21,16/Jun/21 07:28,13/Jul/23 08:07,19/Jun/20 08:20,1.10.0,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Hi,

My use case is a streaming application with a few streaming tables.

I was trying to build a SELECT query (and registering it as a temporary view) with about 200 fields/expressions out of another streaming table. The application is successfully submitted to Flink cluster. However the worker processes keep crashing, with the exception as quoted below. 

It clearly mentioned in the log that this is a bug, so I fire this ticket. By the way, if I lower the number of fields down to 100 then it works nicely.

Please advice.

Thanks a lot for all the efforts bring Flink up. It is really amazing!
{code:java}
java.lang.RuntimeException: Could not instantiate generated class 'GroupAggsHandler$9687'    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:57)    at org.apache.flink.table.runtime.operators.aggregate.MiniBatchGroupAggFunction.open(MiniBatchGroupAggFunction.java:136)    at org.apache.flink.table.runtime.operators.bundle.AbstractMapBundleOperator.open(AbstractMapBundleOperator.java:84)    at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1007)    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)    at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)    at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68)    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78)    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:52)    ... 10 moreCaused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66)    ... 12 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81)    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)    ... 15 moreCaused by: org.codehaus.janino.InternalCompilerException: Compiling ""GroupAggsHandler$9687"": Code of method ""retract(Lorg/apache/flink/table/dataformat/BaseRow;)V"" of class ""GroupAggsHandler$9687"" grows beyond 64 KB    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78)    ... 21 moreCaused by: org.codehaus.janino.InternalCompilerException: Code of method ""retract(Lorg/apache/flink/table/dataformat/BaseRow;)V"" of class ""GroupAggsHandler$9687"" grows beyond 64 KB    at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1009)    at org.codehaus.janino.CodeContext.write(CodeContext.java:901)    at org.codehaus.janino.CodeContext.writeShort(CodeContext.java:1026)    at org.codehaus.janino.UnitCompiler.writeConstantLongInfo(UnitCompiler.java:12274)    at org.codehaus.janino.UnitCompiler.pushConstant(UnitCompiler.java:10679)    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4936)    at org.codehaus.janino.UnitCompiler.access$8400(UnitCompiler.java:215)    at org.codehaus.janino.UnitCompiler$16.visitUnaryOperation(UnitCompiler.java:4414)    at org.codehaus.janino.UnitCompiler$16.visitUnaryOperation(UnitCompiler.java:4394)    at org.codehaus.janino.Java$UnaryOperation.accept(Java.java:4719)    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394)    at org.codehaus.janino.UnitCompiler.fakeCompile(UnitCompiler.java:3719)    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5569)    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2580)    at org.codehaus.janino.UnitCompiler.access$2700(UnitCompiler.java:215)    at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1503)    at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1487)    at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3511)    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)    ... 28 more
{code}",,danny0405,godfreyhe,jark,kezhuw,leonard,libenchao,lzljs3620320,Matrix42,StarGhost,,,,,,,,,,,,,"libenchao commented on pull request #11512: [FLINK-16589][table-planner-blink] Split code for AggsHandlerCodeGene…
URL: https://github.com/apache/flink/pull/11512
 
 
   …rator
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Split code for AggsHandlerCodeGenerator
   
   ## Brief change log
   
   Split code for AggsHandlerCodeGenerator
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   - AggrateITCase.testSplitAggsHandler
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know) 
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 15:43;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23007,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 08:19:47 UTC 2020,,,,,,,,,,"0|z0ci0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/20 12:02;ykt836;The root cause is here:
Code of method ""retract(Lorg/apache/flink/table/dataformat/BaseRow;)V"" of class ""GroupAggsHandler$9687"" grows beyond 64 KB 
We didn't do proper code split when generated code is too long. 

cc [~jark];;;","13/Mar/20 13:52;lzljs3620320;If just too many fields in ""select *"" with ""group by"", it can be resolved, just like `ProjectionCodeGenerator`, we can extract its ""loop"" code generation.;;;","13/Mar/20 13:55;libenchao;[~ykt836] We did the code split only for CalcCodeGenerator in https://issues.apache.org/jira/browse/FLINK-15430.  

If we need to split for {{GroupAggsHandler}} too, I can help to do that.;;;","13/Mar/20 15:42;jark;Yes. We didn't split code for GroupAggsHandler, but that can be done as [~libenchao] said, even though it is a complete solution... 

Assigned this issue to you [~libenchao].;;;","15/Mar/20 02:15;libenchao;[~jark] Thanks for the assignment.

[~StarGhost] Could you show us the query which produces this exception? As you described above, you just did a SELECT, and then the exception is threw? Since the exception says the generated aggregation code is too long, there should be some aggregation in your query.;;;","15/Mar/20 02:29;StarGhost;[~libenchao]: please see below an excerpt. The actual query is actually twice as long (which generates the exception above). Yes there is an aggregation, but very simple one.
{code:java}
SELECT
        id AS id,
	LAST_VALUE(platform_domain) AS platform_domain,
	MAX(now) AS now,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 0, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 1, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_WTD0,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 1, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 2, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_WTD1,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 2, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 3, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_WTD2,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 3, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 4, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_WTD3,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 4, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 5, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_WTD4,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 5, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 6, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_WTD5,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 6, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 7, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_WTD6,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 0, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 1, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H0,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 1, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 2, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H1,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 2, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 3, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H2,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 3, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 4, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H3,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 4, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 5, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H4,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 5, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 6, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H5,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 6, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 7, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H6,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 7, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 8, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H7,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 8, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 9, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H8,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 9, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 10, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H9,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 10, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 11, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H10,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 11, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 12, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H11,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 12, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 13, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H12,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 13, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 14, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H13,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 14, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 15, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H14,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 15, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 16, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H15,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 16, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 17, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H16,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 17, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 18, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H17,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 18, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 19, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H18,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 19, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 20, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H19,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 20, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 21, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H20,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 21, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 22, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H21,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 22, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 23, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H22,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 23, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 24, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_today_H23,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 0, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 1, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD0,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 1, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 2, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD1,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 2, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 3, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD2,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 3, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 4, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD3,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 4, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 5, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD4,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 5, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 6, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD5,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 6, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 7, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD6,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 7, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 8, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD7,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 8, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 9, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD8,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 9, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 10, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD9,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 10, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 11, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD10,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 11, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 12, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD11,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 12, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 13, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD12,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 13, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 14, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD13,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 14, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 15, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD14,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 15, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 16, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD15,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 16, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 17, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD16,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 17, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 18, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD17,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 18, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 19, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD18,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 19, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 20, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD19,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 20, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 21, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD20,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 21, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 22, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD21,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 22, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 23, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD22,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 23, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 24, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD23,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 24, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 25, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD24,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 25, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 26, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD25,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 26, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 27, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD26,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 27, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 28, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD27,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 28, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 29, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD28,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 29, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 30, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD29,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 30, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 31, now))) AND pws_status = 'delivered', revenue, 0)) AS nmv_trend_MTD30,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 0, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 1, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_WTD0,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 1, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 2, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_WTD1,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 2, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 3, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_WTD2,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 3, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 4, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_WTD3,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 4, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 5, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_WTD4,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 5, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 6, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_WTD5,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 6, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -MOD(CAST(DAYOFWEEK(now) AS INT) + 5, 7) + 7, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_WTD6,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 0, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 1, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD0,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 1, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 2, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD1,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 2, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 3, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD2,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 3, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 4, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD3,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 4, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 5, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD4,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 5, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 6, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD5,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 6, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 7, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD6,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 7, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 8, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD7,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 8, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 9, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD8,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 9, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 10, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD9,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 10, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 11, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD10,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 11, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 12, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD11,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 12, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 13, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD12,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 13, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 14, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD13,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 14, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 15, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD14,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 15, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 16, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD15,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 16, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 17, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD16,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 17, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 18, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD17,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 18, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 19, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD18,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 19, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 20, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD19,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 20, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 21, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD20,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 21, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 22, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD21,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 22, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 23, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD22,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 23, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 24, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD23,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 24, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 25, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD24,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 25, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 26, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD25,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 26, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 27, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD26,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 27, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 28, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD27,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 28, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 29, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD28,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 29, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 30, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD29,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 30, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(DAY, -CAST(DAYOFMONTH(now) AS INT) + 1 + 31, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_MTD30,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 0, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 1, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H0,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 1, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 2, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H1,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 2, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 3, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H2,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 3, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 4, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H3,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 4, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 5, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H4,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 5, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 6, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H5,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 6, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 7, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H6,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 7, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 8, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H7,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 8, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 9, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H8,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 9, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 10, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H9,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 10, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 11, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H10,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 11, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 12, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H11,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 12, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 13, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H12,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 13, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 14, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H13,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 14, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 15, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H14,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 15, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 16, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H15,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 16, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 17, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H16,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 17, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 18, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H17,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 18, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 19, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H18,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 19, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 20, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H19,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 20, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 21, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H20,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 21, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 22, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H21,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 22, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 23, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H22,
	SUM(IF((time_slot BETWEEN TIMESTAMPADD(HOUR, 23, now) AND TIMESTAMPADD(SECOND, -1, TIMESTAMPADD(HOUR, 24, now))) AND pws_status = 'shipped', revenue, 0)) AS shipped_revenue_trend_today_H23
FROM
	shop_items_stats
GROUP BY
	id
{code};;;","15/Mar/20 02:39;libenchao;[~StarGhost] Thanks for the example. I'll fix this in a few days.;;;","02/Jun/20 08:19;danny0405;cc [~jark];;;","19/Jun/20 08:19;libenchao;Fixed via

* release-1.11: 334b7b8b00885135b0682756f970b4e440f0f189
* master: 99fca58fe60199b93f962e5fbf0cb0314b9d3b99;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Table program cannot be compiled.Cannot determine simple type name ""com""",FLINK-16585,13291497,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,hiliuxg,hiliuxg,13/Mar/20 06:51,19/Nov/20 05:43,13/Jul/23 08:07,19/Nov/20 04:24,1.10.0,,,,,,,,,Table SQL / Planner,,,,,0,,,,,"I user blink planner  and   try to register my scalar function and enable checkpointing  ,then I got some errors like this .

My code is in the attachments

------------------------------------------------------------------------------------

org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68) at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78) at org.apache.flink.table.runtime.generated.GeneratedClass.getClass(GeneratedClass.java:96) at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.getStreamOperatorClass(CodeGenOperatorFactory.java:62) at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.preValidate(StreamingJobGraphGenerator.java:214) at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:149) at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:104) at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:777) at org.apache.flink.streaming.api.graph.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:52) at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:43) at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:57) at org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils$JarHandlerContext.toJobGraph(JarHandlerUtils.java:128) at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$getJobGraphAsync$6(JarRunHandler.java:138) at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66) ... 16 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81) at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ... 19 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 8, Column 30: Cannot determine simple type name ""com"" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6746) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6507) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6520) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6520) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6520) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6520) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6486) at org.codehaus.janino.UnitCompiler.access$13800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$21$1.visitReferenceType(UnitCompiler.java:6394) at org.codehaus.janino.UnitCompiler$21$1.visitReferenceType(UnitCompiler.java:6389) at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3917) at org.codehaus.janino.UnitCompiler$21.visitType(UnitCompiler.java:6389) at org.codehaus.janino.UnitCompiler$21.visitType(UnitCompiler.java:6382) at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3916) at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6382) at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$24.getType(UnitCompiler.java:8184) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6786) at org.codehaus.janino.UnitCompiler.access$14300(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$21$2$1.visitFieldAccess(UnitCompiler.java:6412) at org.codehaus.janino.UnitCompiler$21$2$1.visitFieldAccess(UnitCompiler.java:6407) at org.codehaus.janino.Java$FieldAccess.accept(Java.java:4299) at org.codehaus.janino.UnitCompiler$21$2.visitLvalue(UnitCompiler.java:6407) at org.codehaus.janino.UnitCompiler$21$2.visitLvalue(UnitCompiler.java:6403) at org.codehaus.janino.Java$Lvalue.accept(Java.java:4137) at org.codehaus.janino.UnitCompiler$21.visitRvalue(UnitCompiler.java:6403) at org.codehaus.janino.UnitCompiler$21.visitRvalue(UnitCompiler.java:6382) at org.codehaus.janino.Java$Rvalue.accept(Java.java:4105) at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6382) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6768) at org.codehaus.janino.UnitCompiler.access$14100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$21$2$1.visitAmbiguousName(UnitCompiler.java:6410) at org.codehaus.janino.UnitCompiler$21$2$1.visitAmbiguousName(UnitCompiler.java:6407) at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4213) at org.codehaus.janino.UnitCompiler$21$2.visitLvalue(UnitCompiler.java:6407) at org.codehaus.janino.UnitCompiler$21$2.visitLvalue(UnitCompiler.java:6403) at org.codehaus.janino.Java$Lvalue.accept(Java.java:4137) at org.codehaus.janino.UnitCompiler$21.visitRvalue(UnitCompiler.java:6403) at org.codehaus.janino.UnitCompiler$21.visitRvalue(UnitCompiler.java:6382) at org.codehaus.janino.Java$Rvalue.accept(Java.java:4105) at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6382) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8939) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5060) at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4421) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4394) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3781) at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3760) at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3732) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3732) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2871) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78) ... 25 more
 
 ","cluster : standalone

java version : openjdk 1.8.0_181",csbliss,fsk119,godfreyhe,hiliuxg,jark,libenchao,RocMarshal,xiemeilong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16662,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/20 06:49;hiliuxg;flink110.zip;https://issues.apache.org/jira/secure/attachment/12996623/flink110.zip",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 19 04:23:30 UTC 2020,,,,,,,,,,"0|z0chrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/20 06:32;csbliss;I haven't been able to reproduce this issue. Can you be more specific?;;;","13/Apr/20 03:30;xiemeilong;I have the same issue on cluster mode, but not on local mode. ;;;","17/Nov/20 02:43;jark;cc [~fsk119];;;","19/Nov/20 04:23;fsk119;I have reproduced the bug in release-1.10.0 and find it works well in release-1.10.1. I think the bug has been fixed by [FLINK-16662|https://issues.apache.org/jira/browse/FLINK-16662]. 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid classloader during pipeline creation,FLINK-16583,13291492,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,twalthr,zjwang,zjwang,13/Mar/20 06:25,19/Mar/20 10:42,13/Jul/23 08:07,19/Mar/20 10:42,,,,,,1.11.0,,,,Connectors / Kafka,Table SQL / Client,Tests,,,0,test-stability,,,,"The end-to-end test {{SQLClientKafkaITCase.testKafka}} failed with
{code:java}
18:13:02.425 [ERROR] testKafka[0: kafka-version:0.10 kafka-sql-version:.*kafka-0.10.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase)  Time elapsed: 32.246 s  <<< ERROR!
java.io.IOException: 
Process execution failed due error. Error output:Mar 12, 2020 6:11:46 PM org.jline.utils.Log logr
WARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster.
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:131)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178)

	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178)
	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)

18:13:02.425 [ERROR] testKafka[1: kafka-version:0.11 kafka-sql-version:.*kafka-0.11.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase)  Time elapsed: 34.539 s  <<< ERROR!
java.io.IOException: 
Process execution failed due error. Error output:Mar 12, 2020 6:12:21 PM org.jline.utils.Log logr
WARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster.
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:131)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178)

	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178)
	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)

{code}
[https://api.travis-ci.org/v3/job/661535183/log.txt]",,hequn8128,jark,twalthr,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 10:42:29 UTC 2020,,,,,,,,,,"0|z0chqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/20 01:49;hequn8128;Another instance: https://api.travis-ci.org/v3/job/661966609/log.txt;;;","18/Mar/20 10:47;twalthr;I investigated this issue. It is a bug in the classloading when creating a pipeline.;;;","18/Mar/20 15:45;pnowojski;Another instance: https://api.travis-ci.org/v3/job/663507744/log.txt;;;","18/Mar/20 16:01;twalthr;I asked [~jark] to review a fix: https://github.com/apache/flink/pull/11438;;;","19/Mar/20 10:42;twalthr;It was a classloading bug that was introduced in the last release while updating the code for the new Pipeline abstractions.;;;","19/Mar/20 10:42;twalthr;Fixed in 1.11.0: 931d3a26e800af3271851a313e1d9bf5647fa468;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyBufferPoolTest may have warns on NettyBuffer leak ,FLINK-16582,13291491,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,13/Mar/20 05:57,22/Nov/22 08:29,13/Jul/23 08:07,22/Nov/22 08:29,1.15.3,1.16.0,1.17.0,,,1.15.4,1.16.1,1.17.0,,Runtime / Network,Tests,,,,0,pull-request-available,,,,"{code:java}
4749 [Flink Netty Client (50072) Thread 0] ERROR
org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector [] - LEAK:
ByteBuf.release() was not called before it's garbage-collected. See
https://netty.io/wiki/reference-counted-objects.html for more information.
Recent access records: 
Created at:
	org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:349)
	org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
	org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178)
	org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.buffer(AbstractByteBufAllocator.java:115)
	org.apache.flink.runtime.io.network.netty.NettyBufferPoolTest.testNoHeapAllocations(NettyBufferPoolTest.java:38)
	sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.lang.reflect.Method.invoke(Method.java:498)
	org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	org.junit.runners.Suite.runChild(Suite.java:128)
	org.junit.runners.Suite.runChild(Suite.java:27)
	org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)


Test ignored.


Process finished with exit code 0
{code}
We should released the allocated buffers in the tests.",,AHeise,gaoyunhaii,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 22 08:29:46 UTC 2022,,,,,,,,,,"0|z0chqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/20 09:05;arvid;[~gaoyunhaii] is there still some progress on this ticket?;;;","07/Dec/20 09:21;gaoyunhaii;Hi [~AHeise], very sorry for the long delay, I'll open a PR for this issue soon~;;;","16/Apr/21 10:58;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:56;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","02/Jun/21 23:29;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","11/Jun/21 11:00;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","22/Nov/22 08:29;gaoyunhaii;Merged on master via 88a161101bb33b4c088325788bd11d41f9369355

Merged on release-1.16 via  d8f7777f3216101fd31dbcdc4a725d2e7ead4113

Merged on release-1.15 via 28a877b8880e271123ce674e8669dfe6c6954f1f

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support state ttl for Mini-Batch deduplication using StateTtlConfig,FLINK-16581,13291489,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,lzljs3620320,lzljs3620320,13/Mar/20 05:45,20/Apr/20 16:16,13/Jul/23 08:07,17/Apr/20 06:27,1.10.0,1.9.2,,,,1.11.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"This lead to OOM with long running streaming job.

We should check all unbounded operations, should not lack state TTL",,felixzheng,godfreyhe,jark,klion26,libenchao,lsy,lzljs3620320,,,,,,,,,,,,,,,"wuchong commented on pull request #11482: [FLINK-16581][table] Minibatch deduplication lack state TTL bug fix
URL: https://github.com/apache/flink/pull/11482
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/20 06:26;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,FLINK-16949,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 06:27:04 UTC 2020,,,,,,,,,,"0|z0chq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/20 05:45;lzljs3620320;CC: [~jark];;;","13/Mar/20 06:04;lsy;[~lzljs3620320] I want to fix this bug;;;","13/Mar/20 06:08;lzljs3620320;Assigned to you. [~lsy];;;","23/Mar/20 15:07;jark;Hi [~lsy], thanks for the contribution. I glanced the pull request you submitted. But I would like to discuss the approach here. 

Currently, there are 2 ways to cleanup states. 

1) registering a processing-time timer, and cleanup entries when the timer is callback.
  - pros: can cleanup multiple states at the same time (state consistent)
  - cons: timer space depends on the key size, which may lead to OOM (heap timer). 
  - used in Group Aggregation, Over Aggregateion, TopN

2) using the {{StateTtlConfig}} provided by DataStream [1].
  - pros: decouple the logic of state ttl with the record processing, easy to program (take a look at old planner NonWindowJoin which bundles ttl timestamp with records in MapState).
  - cons: can't cleanup multiple states at the same time.
  - useed in Sream-Stream Joins.

Personally, I perfer using {{StateTtlConfig}} which leverage the ability of DataStream and not inventing the same thing. Besides, it can help to improve the readability of codes (reduce bugs). What do you think [~lzljs3620320] [~lsy]?

[1]: https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/state/state.html#state-time-to-live-ttl


;;;","24/Mar/20 02:20;lzljs3620320;Hi [~jark], although
 # Look at {{KeyedProcessFunctionWithCleanupState}} , we already have many operators that use processing-time timer to cleanup state.
 # Now default timer is rocksDb? In FLINK-15637

I prefer using StateTtlConfig too, It should be unified to DataStream. But why we not use it in 1.9? Can you explain more about ""can't cleanup multiple states at the same time.""?;;;","24/Mar/20 13:48;jark;Hi [~lzljs3620320], that's true there is a lot of operators using timers. That's because {{StateTtlConfig}} is introduced in recent releases and we don't have much time to refactor the existing operators. The reason why we use  {{StateTtlConfig}} is because it simplify the implementatiion A LOT. 

> can't cleanup multiple states at the same time
For example, in COUNT DISTINCT, there are 2 states, the {{MapState}} stores all distinct values, the {{count}} store the size of the MapState. If we use {{StateTtlConfig}}, some entries of MapState may be retired, but {{count}} is not. If a retired value comes in, the {{count}} value gets larger by mistake. If we use timer, MapState and count will be reset together. 
But I think that's not a big problem, because the result is anyway not correct once ttl happens. 

in {{RetractableTopNFunction}}, there will be multiple states, the {{dataState}} which stores all input data, the {{treeMap}} stores the TopN element in order. ;;;","25/Mar/20 02:22;lzljs3620320;I got your concern, but I think this is not a big problem too. We can use StateTtlConfig.;;;","25/Mar/20 06:32;lsy;[~jark] I agree with you, use StateTtlConfig can simplify the implementation a lot, thanks for your guidance, I will modify the code.

> can't cleanup multiple states at the same time

For this problem, I think user should discard the expired data in SQL, otherwise, the result will be affected!;;;","05/Apr/20 13:05;jark;I removed fix versions for 1.9 and 1.10, because it is a new feature and breaks the state compatibility. ;;;","17/Apr/20 06:27;jark;Resolved in master (1.11.0): 3f6080f4e9bdeb564c32a1bbbd5f577b0fbfe873;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception will be thrown when computing columnInterval relmetadata in some case,FLINK-16577,13291466,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,godfreyhe,icshuo,icshuo,13/Mar/20 02:33,05/Jun/20 03:07,13/Jul/23 08:07,05/Jun/20 03:07,1.10.0,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Consider the following SQL

 
{code:java}
// a: INT, c: LONG
SELECT 
    c, SUM(a) 
FROM T 
WHERE a > 0.1 AND a < 1 
GROUP BY c{code}
 

Here the sql type of 0.1 is Decimal and 1 is Integer, and they are both in NUMERIC type family, and do not trigger type coercion, so the plan is:
{code:java}
FlinkLogicalAggregate(group=[{0}], EXPR$1=[SUM($1)])
+- FlinkLogicalCalc(select=[c, a], where=[AND(>(a, 0.1:DECIMAL(2, 1)), <(a, 1))])
   +- FlinkLogicalTableSourceScan(table=[[...]], fields=[a, b, c])
{code}
When we calculate the filtered column interval of calc, it'll lead to validation exception of `FiniteValueInterval`:

!image-2020-03-13-10-32-35-375.png!",,godfreyhe,icshuo,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/20 02:32;icshuo;image-2020-03-13-10-32-35-375.png;https://issues.apache.org/jira/secure/attachment/12996605/image-2020-03-13-10-32-35-375.png","13/Mar/20 02:38;icshuo;image-2020-03-13-10-38-17-001.png;https://issues.apache.org/jira/secure/attachment/12996607/image-2020-03-13-10-38-17-001.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 03:07:33 UTC 2020,,,,,,,,,,"0|z0chkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/20 02:35;icshuo;cc [~godfreyhe], [~jark];;;","13/Mar/20 02:38;icshuo;I think one possible solution is when we get value from RexLiteral using FlinkRelOptUtil#getLiteralValue(..), we do not get value by its `concrete type`, but by its `broad type`. e.g., for RexLiteral(1), now we get ""Integer(1)"", after modifying, it'll be 'BigDecimal(1)'.;;;","13/Mar/20 03:10;jark;A simple way is always using BigDecimal for numerics? ;;;","13/Mar/20 03:31;godfreyhe;as [~icshuo] [~jark] said, maybe using `broad type` is sample way. i will take a look at this;;;","05/Jun/20 03:07;jark;- master (1.12.0): 564b652c128f311883c799779701421752eb99f8
- 1.11.0: a3a34b7d6b0e2f8c9ff4974c9947604fac8f8872;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State inconsistency on restore with memory state backends,FLINK-16576,13291427,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,klion26,nkruber,nkruber,12/Mar/20 21:39,16/Apr/20 07:46,13/Jul/23 08:07,16/Apr/20 07:46,1.10.0,1.9.2,,,,1.10.1,1.11.0,1.9.3,,Runtime / State Backends,,,,,0,pull-request-available,,,,"I occasionally see a few state inconsistencies with the {{TopSpeedWindowing}} example in Flink. Restore would fail with either of these causes, but only for the memory state backends and only with some combinations of parallelism I took the savepoint with and parallelism I restore the job with:
{code:java}
java.lang.IllegalArgumentException: KeyGroupRange{startKeyGroup=64, endKeyGroup=95} does not contain key group 97 {code}
or
{code:java}
java.lang.NullPointerException
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readKeyGroupStateData(HeapRestoreOperation.java:280) {code}
or
{code:java}
java.io.IOException: Corrupt stream, found tag: 8
	at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:217) {code}
 

I managed to make it reproducible in a test that I quickly hacked together in [https://github.com/NicoK/flink/blob/state.corruption.debug/flink-examples/flink-examples-streaming/src/test/java/org/apache/flink/streaming/test/examples/windowing/TopSpeedWindowingSavepointRestoreITCase.java] (please checkout the whole repository since I had to change some dependencies).

In a bit more detail, this is what I discovered before, also with a manual savepoint on S3:

Savepoint that was taken with parallelism 2 (p=2) and shows the restore failure in three different ways (all running in Flink 1.10.0; but I also see it in Flink 1.9):
 * first of all, if I try to restore with p=2, everything is fine
 * if I restore with p=4 I get an exception like the one mentioned above:
{code:java}
2020-03-11 15:53:35,149 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (3/4) (2ecdb03905cc8a376d43b086925452a6) switched from RUNNING to FAILED.
java.lang.Exception: Exception while creating StreamOperatorStateContext.
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for EvictingWindowOperator_90bea66de1c231edf33913ecd54406c1_(3/4) from any of the 1 provided restore options.
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)
	... 9 more
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:116)
	at org.apache.flink.runtime.state.filesystem.FsStateBackend.createKeyedStateBackend(FsStateBackend.java:529)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)
	... 11 more
Caused by: java.lang.IllegalArgumentException: KeyGroupRange{startKeyGroup=64, endKeyGroup=95} does not contain key group 97
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:161)
	at org.apache.flink.runtime.state.heap.HeapPriorityQueueSet.globalKeyGroupToLocalIndex(HeapPriorityQueueSet.java:158)
	at org.apache.flink.runtime.state.heap.HeapPriorityQueueSet.getDedupMapForKeyGroup(HeapPriorityQueueSet.java:147)
	at org.apache.flink.runtime.state.heap.HeapPriorityQueueSet.getDedupMapForElement(HeapPriorityQueueSet.java:154)
	at org.apache.flink.runtime.state.heap.HeapPriorityQueueSet.add(HeapPriorityQueueSet.java:121)
	at org.apache.flink.runtime.state.heap.HeapPriorityQueueSnapshotRestoreWrapper.lambda$keyGroupReader$0(HeapPriorityQueueSnapshotRestoreWrapper.java:85)
	at org.apache.flink.runtime.state.KeyGroupPartitioner$PartitioningResultKeyGroupReader.readMappingsInKeyGroup(KeyGroupPartitioner.java:298)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readKeyGroupStateData(HeapRestoreOperation.java:293)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readStateHandleStateData(HeapRestoreOperation.java:254)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:153)
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:114)
	... 15 more
{code}

 * if I restore with p=3, I get the following exception instead:
{code:java}
2020-03-11 21:40:28,390 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (3/3) (2fb8acde321f6cbfec56c300153d6dea) switched from RUNNING to FAILED.
java.lang.Exception: Exception while creating StreamOperatorStateContext.
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for EvictingWindowOperator_90bea66de1c231edf33913ecd54406c1_(3/3) from any of the 1 provided restore options.
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)
	... 9 more
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:116)
	at org.apache.flink.runtime.state.filesystem.FsStateBackend.createKeyedStateBackend(FsStateBackend.java:529)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)
	... 11 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readKeyGroupStateData(HeapRestoreOperation.java:280)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readStateHandleStateData(HeapRestoreOperation.java:254)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:153)
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:114)
	... 15 more
{code}

While the latter error somewhat indicates that the savepoint may be corrupt (or that the reader/deserializer messed up), it remains a mystery to me why I can successfully restore with p=2.
 * occasionally, for p=4, I also see this exception instead:

{code:java}
10:23:12,046 WARN  org.apache.flink.streaming.api.operators.BackendRestorerProcedure  - Exception while restoring keyed state backend for EvictingWindowOperator_90bea66de1c231edf33913ecd54406c1_(3/4) from alternative (1/1), will retry while more alternatives are available.
org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:116)
	at org.apache.flink.runtime.state.filesystem.FsStateBackend.createKeyedStateBackend(FsStateBackend.java:529)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Corrupt stream, found tag: 8
	at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:217)
	at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:46)
	at org.apache.flink.api.common.typeutils.base.ListSerializer.deserialize(ListSerializer.java:133)
	at org.apache.flink.api.common.typeutils.base.ListSerializer.deserialize(ListSerializer.java:42)
	at org.apache.flink.runtime.state.heap.StateTableByKeyGroupReaders.lambda$createV2PlusReader$0(StateTableByKeyGroupReaders.java:77)
	at org.apache.flink.runtime.state.KeyGroupPartitioner$PartitioningResultKeyGroupReader.readMappingsInKeyGroup(KeyGroupPartitioner.java:297)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readKeyGroupStateData(HeapRestoreOperation.java:295)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readStateHandleStateData(HeapRestoreOperation.java:256)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:155)
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:114)
	... 15 more{code}
 

I narrowed the cause down to the state access inside {{DeltaTrigger}}: Removing the cleanup in {{org.apache.flink.streaming.api.windowing.triggers.DeltaTrigger#clear()}} did not change anything but removing state access in its {{onElement}} (pictured below) seems to resolve the bug:
{code:java}
@Override
public TriggerResult onElement(T element, long timestamp, W window, TriggerContext ctx) throws Exception {
   ValueState<T> lastElementState = ctx.getPartitionedState(stateDesc);
   if (lastElementState.value() == null) {
      lastElementState.update(element);
      return TriggerResult.CONTINUE;
   }
   if (deltaFunction.getDelta(lastElementState.value(), element) > this.threshold) {
      lastElementState.update(element);
      return TriggerResult.FIRE;
   }
   return TriggerResult.CONTINUE;
} {code}",,aljoscha,dian.fu,fwollsch,gyfora,klion26,liyu,nkruber,wind_ljy,ym,,,,,,,,,,,,,"klion26 commented on pull request #11555: [FLINK-16576][state backends] Correct the logic of KeyGroupStateHandle#getIntersection
URL: https://github.com/apache/flink/pull/11555
 
 
   
   
   ## What is the purpose of the change
   
   This pr will return null if the handler's key-group does not have any intersection with the given key-group range.
   
   
   ## Verifying this change
   
   
   This change added tests and can be verified as follows:
   
   - `KeyGroupsStateHandleTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Mar/20 16:31;githubbot;600","carp84 commented on pull request #11555: [FLINK-16576][state backends] Correct the logic of KeyGroupStateHandle#getIntersection
URL: https://github.com/apache/flink/pull/11555
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/20 12:23;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 07:46:29 UTC 2020,,,,,,,,,,"0|z0chc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/20 09:40;aljoscha;You are talking about heap-based state backends that do backup to a DFS? Not the actual {{MemoryStateBackend}}, right?;;;","13/Mar/20 09:43;nkruber;Both actually: {{MemoryStateBackend}} as well as {{FsStateBackend}} as you can see in my test.

So far, I did not see this for {{RocksDBStateBackend}}.;;;","28/Mar/20 12:44;klion26;The reason why the restore failed here because of that {{the *mapping of stateId and metaInfo is wrong*}}.

The mapping is wrong because we registered some metaInfos that do not belong to current subtask. 
{code:java}
// HeapRestoreOperation#restore
createOrCheckStateForMetaInfo(restoredMetaInfos, kvStatesById); // will register the metainfo

readStateHandleStateData(
   fsDataInputStream,
   inView,
   keyGroupsStateHandle.getGroupRangeOffsets(),
   kvStatesById, restoredMetaInfos.size(),
   serializationProxy.getReadVersion(),
   serializationProxy.isUsingKeyGroupCompression());


private void createOrCheckStateForMetaInfo(
   List<StateMetaInfoSnapshot> restoredMetaInfo,
   Map<Integer, StateMetaInfoSnapshot> kvStatesById) {

   for (StateMetaInfoSnapshot metaInfoSnapshot : restoredMetaInfo) {
      final StateSnapshotRestore registeredState;

      ......

      if (registeredState == null) {
         kvStatesById.put(kvStatesById.size(), metaInfoSnapshot); // constructing the mapping between stateId and metaInfo, even if the current statehandle does not belong to the current subtask
      }
   }
}
{code}
from the code above we can see, we'll always register the metainfo even if the current state handle does not belong to ourselves(the KeyGroupStateHandle will contain metaInfo, EMPTY_KEYGROUP, empty offsets and the stateHandle data). after the registered the wrong metainfo, then the *{{mapping of stateId and metaInfo becomes wrong(when constructing the mapping, we assume that all the handles belong to the current subtask).}}* {{(RocksDBStateBackend does not construct such mapping, so would not encounter such error).}}

{{For the solution here, I want to filter out the stateHandles out when assigning state to subtask in }}{{StateAssignmentOperation}}.{{ }};;;","29/Mar/20 10:58;liyu;Thanks for the analysis [~klion26]. Checking the commit history, this seems to be a long existing problem.

Since the root cause is incorrect mapping of {{stateId}} and {{metaInfo}}, I suggest we add some checking in {{HeapRestoreOperation}} to reinforce the logic. Meantime, I think adding the filter in {{KeyGroupsStateHandle#getIntersection}} also makes sense - to improve at both the dispatcher and the receiver side.;;;","15/Apr/20 12:28;liyu;Fixed in master via:
0a4870d9ae5f71a243eb351816562cb706af3e9c
792dbf0b72ca8a7ac2cb00d4da3f84c7bfe1b59d

Now working on release-1.10 and release-1.9;;;","16/Apr/20 07:43;liyu;Merged in release-1.10 via:
1e9d72dbb353988a426f9da7e0f6efca06c44e62
ac2be600330ba1316e766d7da816b8078d7294bf

Merged in release-1.9 via
bb6de2ddb37287da0def0f9d81dbc4792512e97a
e95eae275f21796f9983c4d8b0bc4be1a0483e94;;;","16/Apr/20 07:46;liyu;Closing since issue fixed in all related branches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis consumer does not properly shutdown RecordFetcher threads,FLINK-16573,13291304,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,12/Mar/20 12:24,16/Mar/20 11:50,13/Jul/23 08:07,16/Mar/20 11:50,1.10.0,1.8.3,1.9.2,,,1.10.1,1.11.0,1.9.3,,Connectors / Kinesis,,,,,0,pull-request-available,,,,"The threads may not shut down correctly because they do not check for the running flag in the inner loops. The threads also do not get interrupted because they are not connected to the main task thread.

These threads keep lingering around after the job has shut down:

{noformat}
Thread 23168: (state = BLOCKED)
 - java.lang.Object.wait(long) @bci=0 (Compiled frame; information may be imprecise)
 - org.apache.flink.streaming.connectors.kinesis.util.RecordEmitter.emitRecords() @bci=140, line=209 (Compiled frame)
 - org.apache.flink.streaming.connectors.kinesis.util.RecordEmitter.run() @bci=18, line=177 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)
{noformat}",,mxm,thw,,,,,,,,,,,,,,,,,,,,"mxm commented on pull request #11394: [FLINK-16573] Ensure Kinesis RecordFetcher threads shutdown on cancel
URL: https://github.com/apache/flink/pull/11394
 
 
   
   
   ## What is the purpose of the change
   
   Ensure that the Kinesis RecordFetcher threads shut down on cancel.
   
   The threads may not shut down correctly because they do not check for the
   running flag in the inner loops. The threads also do not get interrupted because
   they are not connected to the main task thread.
   
   These threads keep lingering around after the job has shut down:
   
   ```
   Thread 23168: (state = BLOCKED)
    - java.lang.Object.wait(long) @bci=0 (Compiled frame; information may be imprecise)
    - org.apache.flink.streaming.connectors.kinesis.util.RecordEmitter.emitRecords() @bci=140, line=209 (Compiled frame)
    - org.apache.flink.streaming.connectors.kinesis.util.RecordEmitter.run() @bci=18, line=177 (Interpreted frame)
    - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)
   ```
   
   ## Brief change log
   
   - Check for the `running` flag in the inner loops which showed up in the stack traces we took
   
   ## Verifying this change
   
   I'm going to verify this change on a cluster.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Mar/20 12:31;githubbot;600","mxm commented on pull request #11394: [FLINK-16573] Ensure Kinesis RecordFetcher threads shutdown on cancel
URL: https://github.com/apache/flink/pull/11394
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/20 11:44;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,FLINK-16393,,,,,,,,,,,,,,,,,,,,,,,FLINK-16510,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 11:50:32 UTC 2020,,,,,,,,,,"0|z0cgkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/20 12:39;mxm;Should add that this eventually causes the meta space to run out after many job restarts because the lingering threads prevent old classloaders from becoming garbage collected.;;;","12/Mar/20 20:02;thw;FYI for 1.11 this issue would only appear when event time alignment / source sync is enabled as the record emitter thread won't be created otherwise (after FLINK-16393);;;","16/Mar/20 11:50;mxm;Backported also to {{release-1.9}} and {{release-1.10}} branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckPubSubEmulatorTest is flaky on Azure,FLINK-16572,13291302,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,aljoscha,aljoscha,12/Mar/20 12:19,15/Jul/20 08:04,13/Jul/23 08:07,11/Jun/20 07:57,1.11.0,,,,,1.11.1,1.12.0,,,Connectors / Google Cloud PubSub,Tests,,,,0,pull-request-available,test-stability,,,Log: https://dev.azure.com/aljoschakrettek/Flink/_build/results?buildId=56&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=ce095137-3e3b-5f73-4b79-c42d3d5f8283&l=7842,,aljoscha,dian.fu,jark,kevin.cyj,klion26,rmetzger,sewen,Xeli,yunta,zjwang,,,,,,,,,,,,"Xeli commented on pull request #11810: [FLINK-16572] [pubsub,e2e] CheckPubSubEmulatorTest is flaky on Azure
URL: https://github.com/apache/flink/pull/11810
 
 
   I was unable to consistently reproduce the failing test (see jira ticket for logs). This MR fixes a bug where it would try to send an acknowledge request without any acknowledge id. After that fix, I did not see the test failing anymore.
   
   I do not think this should fix it entirely but I cannot see it failing anymore. I'd like to add this fix and see if the problem arises again and if so with what kind of exceptions.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Apr/20 10:13;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18187,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 07:57:56 UTC 2020,,,,,,,,,,"0|z0cgkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/20 06:38;zjwang;Another same failure case in [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6259&view=results];;;","21/Mar/20 16:05;rmetzger;Another case https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6470&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
2020-03-21T15:25:36.8453336Z -------------------------------------------------------
2020-03-21T15:25:36.8453640Z  T E S T S
2020-03-21T15:25:36.8454096Z -------------------------------------------------------
2020-03-21T15:25:37.3421468Z Running org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest
2020-03-21T15:26:37.2300415Z Mar 21, 2020 3:26:37 PM io.grpc.internal.ManagedChannelImpl$NameResolverListenerImpl onError
2020-03-21T15:26:37.2303138Z WARNING: [io.grpc.internal.ManagedChannelImpl-13] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host unknown-host-to-force-sink-crash, cause=java.lang.RuntimeException: java.net.UnknownHostException: unknown-host-to-force-sink-crash: Name or service not known
2020-03-21T15:26:37.2324733Z 	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:399)
2020-03-21T15:26:37.2325645Z 	at io.grpc.internal.DnsNameResolver$Resolve.resolveInternal(DnsNameResolver.java:269)
2020-03-21T15:26:37.2326461Z 	at io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:225)
2020-03-21T15:26:37.2327184Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-03-21T15:26:37.2327804Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-21T15:26:37.2328265Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-21T15:26:37.2329218Z Caused by: java.net.UnknownHostException: unknown-host-to-force-sink-crash: Name or service not known
2020-03-21T15:26:37.2329956Z 	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
2020-03-21T15:26:37.2330387Z 	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)
2020-03-21T15:26:37.2330885Z 	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324)
2020-03-21T15:26:37.2331343Z 	at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
2020-03-21T15:26:37.2331784Z 	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
2020-03-21T15:26:37.2332206Z 	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
2020-03-21T15:26:37.2332722Z 	at io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:624)
2020-03-21T15:26:37.2333472Z 	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:367)
2020-03-21T15:26:37.2333910Z 	... 5 more
2020-03-21T15:26:37.2334077Z }
2020-03-21T15:26:38.1504278Z Mar 21, 2020 3:26:38 PM io.grpc.internal.ManagedChannelImpl$NameResolverListenerImpl onError
2020-03-21T15:26:38.1507500Z WARNING: [io.grpc.internal.ManagedChannelImpl-13] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host unknown-host-to-force-sink-crash, cause=java.lang.RuntimeException: java.net.UnknownHostException: unknown-host-to-force-sink-crash
2020-03-21T15:26:38.1508477Z 	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:399)
2020-03-21T15:26:38.1509060Z 	at io.grpc.internal.DnsNameResolver$Resolve.resolveInternal(DnsNameResolver.java:269)
2020-03-21T15:26:38.1509534Z 	at io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:225)
2020-03-21T15:26:38.1509972Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-03-21T15:26:38.1510447Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-21T15:26:38.1510835Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-21T15:26:38.1511426Z Caused by: java.net.UnknownHostException: unknown-host-to-force-sink-crash
2020-03-21T15:26:38.1511843Z 	at java.net.InetAddress.getAllByName0(InetAddress.java:1281)
2020-03-21T15:26:38.1512212Z 	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
2020-03-21T15:26:38.1512597Z 	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
2020-03-21T15:26:38.1513476Z 	at io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:624)
2020-03-21T15:26:38.1513965Z 	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:367)
2020-03-21T15:26:38.1514371Z 	... 5 more
2020-03-21T15:26:38.1514512Z }
2020-03-21T15:26:40.1434924Z Mar 21, 2020 3:26:40 PM io.grpc.internal.ManagedChannelImpl$NameResolverListenerImpl onError
2020-03-21T15:26:40.1446572Z WARNING: [io.grpc.internal.ManagedChannelImpl-13] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host unknown-host-to-force-sink-crash, cause=java.lang.RuntimeException: java.net.UnknownHostException: unknown-host-to-force-sink-crash
2020-03-21T15:26:40.1448017Z 	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:399)
2020-03-21T15:26:40.1448791Z 	at io.grpc.internal.DnsNameResolver$Resolve.resolveInternal(DnsNameResolver.java:269)
2020-03-21T15:26:40.1449477Z 	at io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:225)
2020-03-21T15:26:40.1450223Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-03-21T15:26:40.1450923Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-21T15:26:40.1451536Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-21T15:26:40.1452489Z Caused by: java.net.UnknownHostException: unknown-host-to-force-sink-crash
2020-03-21T15:26:40.1453264Z 	at java.net.InetAddress.getAllByName0(InetAddress.java:1281)
2020-03-21T15:26:40.1454046Z 	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
2020-03-21T15:26:40.1454736Z 	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
2020-03-21T15:26:40.1455399Z 	at io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:624)
2020-03-21T15:26:40.1456116Z 	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:367)
2020-03-21T15:26:40.1456640Z 	... 5 more
2020-03-21T15:26:40.1457027Z }
2020-03-21T15:26:42.4973775Z Mar 21, 2020 3:26:42 PM io.grpc.internal.ManagedChannelImpl$NameResolverListenerImpl onError
2020-03-21T15:26:42.4976899Z WARNING: [io.grpc.internal.ManagedChannelImpl-13] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host unknown-host-to-force-sink-crash, cause=java.lang.RuntimeException: java.net.UnknownHostException: unknown-host-to-force-sink-crash
2020-03-21T15:26:42.4978041Z 	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:399)
2020-03-21T15:26:42.4978647Z 	at io.grpc.internal.DnsNameResolver$Resolve.resolveInternal(DnsNameResolver.java:269)
2020-03-21T15:26:42.4979594Z 	at io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:225)
2020-03-21T15:26:42.4980353Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-03-21T15:26:42.4980966Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-21T15:26:42.4981453Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-21T15:26:42.4982342Z Caused by: java.net.UnknownHostException: unknown-host-to-force-sink-crash
2020-03-21T15:26:42.4983115Z 	at java.net.InetAddress.getAllByName0(InetAddress.java:1281)
2020-03-21T15:26:42.4983795Z 	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
2020-03-21T15:26:42.4984350Z 	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
2020-03-21T15:26:42.4985107Z 	at io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:624)
2020-03-21T15:26:42.4985769Z 	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:367)
2020-03-21T15:26:42.4986244Z 	... 5 more
2020-03-21T15:26:42.4986522Z }
2020-03-21T15:26:46.3164354Z Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 68.973 sec
2020-03-21T15:26:46.3213049Z Running org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSourceTest
2020-03-21T15:26:49.1114675Z Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.786 sec
2020-03-21T15:26:49.1115485Z Running org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest
2020-03-21T15:27:02.7388370Z Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 13.625 sec <<< FAILURE!
2020-03-21T15:27:02.7389823Z testPull(org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest)  Time elapsed: 11.075 sec  <<< ERROR!
2020-03-21T15:27:02.7390637Z com.google.api.gax.rpc.InvalidArgumentException: io.grpc.StatusRuntimeException: INVALID_ARGUMENT: No ack ids specified.
2020-03-21T15:27:02.7391396Z 	at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:49)
2020-03-21T15:27:02.7392080Z 	at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:72)
2020-03-21T15:27:02.7392745Z 	at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:60)
2020-03-21T15:27:02.7393481Z 	at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:97)
2020-03-21T15:27:02.7394181Z 	at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:68)
2020-03-21T15:27:02.7394751Z 	at com.google.common.util.concurrent.Futures$4.run(Futures.java:1123)
2020-03-21T15:27:02.7395406Z 	at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:435)
2020-03-21T15:27:02.7396110Z 	at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:900)
2020-03-21T15:27:02.7396772Z 	at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:811)
2020-03-21T15:27:02.7397436Z 	at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:675)
2020-03-21T15:27:02.7398055Z 	at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:507)
2020-03-21T15:27:02.7398667Z 	at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:482)
2020-03-21T15:27:02.7399331Z 	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2020-03-21T15:27:02.7400002Z 	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2020-03-21T15:27:02.7400722Z 	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2020-03-21T15:27:02.7401484Z 	at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:694)
2020-03-21T15:27:02.7402329Z 	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2020-03-21T15:27:02.7403055Z 	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2020-03-21T15:27:02.7403824Z 	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2020-03-21T15:27:02.7404624Z 	at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2020-03-21T15:27:02.7405414Z 	at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2020-03-21T15:27:02.7406021Z 	at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2020-03-21T15:27:02.7406637Z 	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2020-03-21T15:27:02.7407339Z 	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2020-03-21T15:27:02.7408561Z 	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2020-03-21T15:27:02.7409244Z 	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2020-03-21T15:27:02.7409756Z 	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2020-03-21T15:27:02.7410311Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-03-21T15:27:02.7410882Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-21T15:27:02.7411351Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-21T15:27:02.7412020Z 	Suppressed: com.google.api.gax.rpc.AsyncTaskException: Asynchronous task failed
2020-03-21T15:27:02.7412597Z 		at com.google.api.gax.rpc.ApiExceptions.callAndTranslateApiException(ApiExceptions.java:57)
2020-03-21T15:27:02.7413259Z 		at com.google.api.gax.rpc.UnaryCallable.call(UnaryCallable.java:112)
2020-03-21T15:27:02.7413935Z 		at org.apache.flink.streaming.connectors.gcp.pubsub.emulator.PubsubHelper.pullMessages(PubsubHelper.java:196)
2020-03-21T15:27:02.7414632Z 		at org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest.testPull(CheckPubSubEmulatorTest.java:77)
2020-03-21T15:27:02.7415221Z 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-21T15:27:02.7415735Z 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-21T15:27:02.7416301Z 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-21T15:27:02.7416831Z 		at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-21T15:27:02.7417369Z 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-03-21T15:27:02.7417953Z 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-03-21T15:27:02.7418550Z 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-03-21T15:27:02.7419220Z 		at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-03-21T15:27:02.7420430Z 		at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-03-21T15:27:02.7420983Z 		at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-03-21T15:27:02.7421551Z 		at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-03-21T15:27:02.7422093Z 		at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-03-21T15:27:02.7422595Z 		at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-03-21T15:27:02.7423109Z 		at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-03-21T15:27:02.7423625Z 		at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-03-21T15:27:02.7424139Z 		at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-03-21T15:27:02.7424672Z 		at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-03-21T15:27:02.7425238Z 		at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-03-21T15:27:02.7425742Z 		at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-03-21T15:27:02.7426301Z 		at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
2020-03-21T15:27:02.7427817Z 		at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
2020-03-21T15:27:02.7428659Z 		at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
2020-03-21T15:27:02.7429266Z 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-21T15:27:02.7429863Z 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-21T15:27:02.7430514Z 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-21T15:27:02.7431436Z 		at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-21T15:27:02.7431980Z 		at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
2020-03-21T15:27:02.7432626Z 		at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
2020-03-21T15:27:02.7433265Z 		at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
2020-03-21T15:27:02.7433859Z 		at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
2020-03-21T15:27:02.7434444Z 		at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2020-03-21T15:27:02.7435007Z Caused by: io.grpc.StatusRuntimeException: INVALID_ARGUMENT: No ack ids specified.
2020-03-21T15:27:02.7435669Z 	at io.grpc.Status.asRuntimeException(Status.java:530)
2020-03-21T15:27:02.7436040Z 	... 19 more
2020-03-21T15:27:02.7436267Z 
2020-03-21T15:27:03.0906019Z 
2020-03-21T15:27:03.0915949Z Results :
2020-03-21T15:27:03.0916490Z 
2020-03-21T15:27:03.0916722Z Tests in error: 
2020-03-21T15:27:03.0917318Z   testPull(org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest): io.grpc.StatusRuntimeException: INVALID_ARGUMENT: No ack ids specified.
2020-03-21T15:27:03.0917832Z 
2020-03-21T15:27:03.0918108Z Tests run: 5, Failures: 0, Errors: 1, Skipped: 0
2020-03-21T15:27:03.0918308Z 
2020-03-21T15:27:03.0972026Z [INFO] ------------------------------------------------------------------------
2020-03-21T15:27:03.0972770Z [INFO] BUILD FAILURE
2020-03-21T15:27:03.0973568Z [INFO] ------------------------------------------------------------------------
2020-03-21T15:27:03.0976421Z [INFO] Total time:  01:40 min
2020-03-21T15:27:03.0977164Z [INFO] Finished at: 2020-03-21T15:27:03Z
2020-03-21T15:27:03.0977923Z [INFO] ------------------------------------------------------------------------
2020-03-21T15:27:03.0997643Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.12.4:test (default) on project flink-connector-gcp-pubsub-emulator-tests: There are test failures.
{code};;;","25/Mar/20 12:50;rmetzger;Another case: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6611&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","03/Apr/20 14:26;pnowojski;Another instance:
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7015&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","04/Apr/20 08:33;rmetzger;Another case: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7050&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","06/Apr/20 09:05;rmetzger;[~Xeli] do you have time to look into this?;;;","06/Apr/20 09:24;Xeli;Sure, I'll have a look at it this week! (I cant assign it to myself but feel free to :));;;","06/Apr/20 09:31;rmetzger;Awesome! I assigned you to the ticket. Thanks a lot.;;;","15/Apr/20 05:50;dian.fu;Another instance: [https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/7489/logs/21];;;","21/Apr/20 08:25;rmetzger;[~Xeli] Thanks a lot for providing a fix for the test instability.

I merged it to master in https://github.com/apache/flink/commit/a0f349f8ce97e5272479751039c0c762f4ee53bf;;;","24/Apr/20 14:19;rmetzger;Mh, it seems that there are still instabilities: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=121&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}

	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest.testPull(CheckPubSubEmulatorTest.java:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
{code};;;","28/Apr/20 07:01;rmetzger;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=338&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=375367d9-d72e-5c21-3be0-b45149130f6b;;;","30/Apr/20 07:11;rmetzger;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=413&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","30/Apr/20 07:11;rmetzger;[~Xeli] could you take a look at these test failures?;;;","07/May/20 12:05;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=731&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","07/May/20 12:21;Xeli;Yeah I'll have another look, reproducing these locally is quite difficult though, is it oke if I trigger the build on azure a couple of times with debug statements? I think I can trigger it by just doing dummy commits on a MR right?;;;","07/May/20 12:50;rmetzger;Thanks a lot!
Yes, debugging these rarely occurring issues is quite annoying.

You can open a draft PR and push dummy commits to trigger CI. Note that we use ""flinkbot"", which takes up to 30 minutes for picking up a commit, and does some magic with branches to ""flink-ci/flink"".

This issue is occurring quite rarely (once a week, which is probably 1/350 builds fail with this). Maybe it would make more sense to carefully add some debug statements specifically to this test (only in the test code, we don't want to release testing code in the upcoming Flink 1.11 :) ). If that helps, I'm willing to consider & merge :) ;;;","11/May/20 13:01;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=965&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","13/May/20 07:30;rmetzger;{code}
testPull(org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest)  Time elapsed: 11.079 sec  <<< FAILURE!
java.lang.AssertionError: expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest.testPull(CheckPubSubEmulatorTest.java:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1107&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","13/May/20 19:18;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1159&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","14/May/20 17:21;sewen;One more instance:
https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/9/logs/152;;;","15/May/20 09:13;rmetzger;Another instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1372&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","18/May/20 06:28;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1654&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","19/May/20 19:37;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1780&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8;;;","19/May/20 19:37;rmetzger;[~Xeli] can you take a look at this test? If not, we need to find somebody else to look into it, or disable it.;;;","20/May/20 06:55;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1874&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","20/May/20 17:10;sewen;One more instance: https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/26/logs/102;;;","23/May/20 03:29;yunta;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2040&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","23/May/20 11:59;Xeli;Apologies for the delay [~rmetzger], I had some struggles getting flink to built locally for some reason, which doesn't help in quickly adding some changes.. :P

 

I've added a retry in the test to see if after 60s it does find the missing message: [https://github.com/apache/flink/pull/12301]

Not something we should want in permanent testing code, but this should give some information to see if the publishing part or the pulling part is going wrong.;;;","25/May/20 06:24;rmetzger;Thanks a lot. Merged as part of https://github.com/apache/flink/commit/50253c6b89e3c92cac23edda6556770a63643c90;;;","26/May/20 07:35;rmetzger;The error is still happening: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2162&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","26/May/20 08:28;Xeli;Could it be that this was run on an older code base? The stacktrace ([https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2162&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5&l=6715)] refers to:
{code:java}
at org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest.testPull(CheckPubSubEmulatorTest.java:78)
{code}
That line used to have the assert but not anymore: 

[https://github.com/apache/flink/blob/master/flink-end-to-end-tests/flink-connector-gcp-pubsub-emulator-tests/src/test/java/org/apache/flink/streaming/connectors/gcp/pubsub/CheckPubSubEmulatorTest.java#L78];;;","26/May/20 08:30;rmetzger;Oh yes, you are right. That run was from the ""release-1.11"" branch, not master. I merged your quickfix only to master.;;;","28/May/20 02:24;klion26;seems another instance [https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/2296/logs/133] ;;;","28/May/20 07:09;rmetzger;This failure is from a pull request which is based on 1.11-SNAPSHOT: https://github.com/flink-ci/flink/blob/ci_12361_956eb8c52c2e828b82a04f8cd41d984cc7a4d9bf/pom.xml. We have not merged the workaround to 1.11-SNAPSHOT.;;;","28/May/20 11:43;rmetzger;This failure really is from master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2342&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5 (and the line is the right one).;;;","03/Jun/20 05:35;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2591&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","03/Jun/20 13:47;rmetzger;[~Xeli] I opened another pull request to add some more debugging to the connector test. Could you help reviewing? https://github.com/apache/flink/pull/12467;;;","04/Jun/20 15:19;rmetzger;Added more debugging in https://github.com/apache/flink/commit/52861e30b65d6b50f24028e31a972d68a9a9f9f8;;;","05/Jun/20 11:26;jark;Another instance: https://dev.azure.com/imjark/34b0e969-d005-4c82-913f-430c2250ce26/_apis/build/builds/142/logs/155;;;","05/Jun/20 11:37;rmetzger;(For the future, can you post the link to the run itself, instead a deep link into the log. Otherwise, it is difficult to find out the version / branch / time etc.)
For this run, I assume it is a release-1.11 run (or it doesn't contain my changes from yesterday yet);;;","08/Jun/20 06:09;rmetzger;Failure on master with the new debugging info: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2885&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5
I'll take a look today ...;;;","08/Jun/20 10:18;rmetzger;I opened another PR to stabilize the test: https://github.com/apache/flink/pull/12524;;;","08/Jun/20 13:08;rmetzger;Merged https://github.com/apache/flink/commit/0358292606481d4545ac372f2db2634a9a25922e;;;","11/Jun/20 07:57;rmetzger;Closing ticket for now. Let's reopen it if the issue surfaces again.

If it doesn't for a few more days, I will push my latest change to 1.11 as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Get the API error of the StreamQueryConfig on Page ""Query Configuration""",FLINK-16567,13291264,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csbliss,csbliss,csbliss,12/Mar/20 09:13,24/Mar/20 13:51,13/Jul/23 08:07,24/Mar/20 04:07,1.10.0,1.9.0,1.9.1,1.9.2,,1.10.1,,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,"{code:java}
// code placeholder
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

// obtain query configuration from TableEnvironment
StreamQueryConfig qConfig = tableEnv.queryConfig();
// set query parameters
qConfig.withIdleStateRetentionTime(Time.hours(12), Time.hours(24));
{code}
On page ""query_configuration.md"", tableEnv.queryConfig() 、withIdleStateRetentionTime() does not exist",,csbliss,jark,lzljs3620320,,,,,,,,,,,,,,,,,,,"manbuyun commented on pull request #11410: [FLINK-16567][Documentation] Get the API error of the StreamQueryConf…
URL: https://github.com/apache/flink/pull/11410
 
 
   On page ""query_configuration.md"" and ""query_configuration.zh.md"", tableEnv.queryConfig() 、withIdleStateRetentionTime() does not exist.
   This pull request fix related api operations involving StreamQueryConfig
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Mar/20 13:47;githubbot;600","JingsongLi commented on pull request #11410: [FLINK-16567][Documentation] Get the API error of the StreamQueryConf…
URL: https://github.com/apache/flink/pull/11410
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Mar/20 04:05;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 13:51:06 UTC 2020,,,,,,,,,,"0|z0cgc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/20 09:22;csbliss;Can I have this issue?;;;","12/Mar/20 09:52;ykt836;[~csbliss] thanks for reporting this, I've assigned to you.;;;","15/Mar/20 12:48;csbliss;Hi [~ykt836]

StreamQueryConfig is deprecated and replaced by TableConfig in the future. But our documents and flink api are not up-to-date, which is not friendly to users.

I think we should add api support for TableConfig, such as Table#insertInto、StreamTableEnvironment# toAppendStream;;;","18/Mar/20 15:14;jark;There is an earlier issue for this: FLINK-13691. Please also consider Timo's reminder there. ;;;","24/Mar/20 04:07;lzljs3620320;Master: 9c351b79af07a13ffc964da388309adaedc94a89

release-1.10: 6c5621eadded6ab9d0a963a22e52e0d308c307a6;;;","24/Mar/20 04:08;lzljs3620320;[~jark] This one is just update documentation, currently we have wrong doc, can not work. Remove StreamQueryConfig should in FLINK-13691;;;","24/Mar/20 13:51;jark;Thanks [~lzljs3620320]. Got it. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forward Configuration in PackagedProgramUtils#getPipelineFromProgram,FLINK-16560,13291222,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,aljoscha,zhuzh,zhuzh,12/Mar/20 03:56,02/Apr/20 09:23,13/Jul/23 08:07,02/Apr/20 08:14,1.10.0,,,,,1.10.1,1.11.0,,,API / DataStream,Runtime / Configuration,,,,0,pull-request-available,,,,"PackagedProgramUtils#createJobGraph(...) is used to generate JobGraph in k8s job mode.
The problem is that the configuration field of StreamExecutionEnvironment is a newly created one when building the job program. This is because StreamPlanEnvironment ctor will base on the no param version ctor of StreamExecutionEnvironment.

This may lead to an unexpected result when invoking StreamExecutionEnvironment#configure(...) which relies on the configuration. Many configurations in the flink conf file will not be respected, like pipeline.time-characteristic, pipeline.operator-chaining, execution.buffer-timeout, and state backend configs.",,aljoscha,dwysakowicz,kezhuw,kkl0u,klion26,liyu,zhuzh,,,,,,,,,,,,,,,"aljoscha commented on pull request #11607: [FLINK-16560] Forward Configuration in PackagedProgramUtils#getPipelineFromProgram
URL: https://github.com/apache/flink/pull/11607
 
 
   Before, when using PackagedProgramUtils (for example in the standalone cluster entrypoint or the web ui) the Flink Configuration would not be applied to the execution environment.
   
   This also adds a test that verifies that we forward configuration.
   
   ## Brief change log
   
     - change `OptimizerPlanEnvironment` and `StreamPlanEnvironment` constructors to accept a `Configuration` and forward to the correct super constructor.
   
   ## Verifying this change
   
    - see the newly added test
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: indirectly, because we now correctly forward also deployment options
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 16:03;githubbot;600","aljoscha commented on pull request #11608: [FLINK-16560] Forward Configuration in PackagedProgramUtils#getPipelineFromProgram
URL: https://github.com/apache/flink/pull/11608
 
 
   Before, when using PackagedProgramUtils (for example in the standalone cluster entrypoint or the web ui) the Flink Configuration would not be applied to the execution environment.
   
   This also adds a test that verifies that we forward configuration.
   
   This is a version of #11607 for Flink 1.10. There are slight differences because the plan environments were changed. Also the tests are in a different place because in Flink 1.10 the dependencies between `flink-clients` and `flink-streaming-java` are reversed.
   
   ## Brief change log
   
     - change `OptimizerPlanEnvironment` and `StreamPlanEnvironment` constructors to accept a `Configuration` and forward to the correct super constructor.
   
   ## Verifying this change
   
    - see the newly added test
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: indirectly, because we now correctly forward also deployment options
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 16:04;githubbot;600","aljoscha commented on pull request #11607: [FLINK-16560] Forward Configuration in PackagedProgramUtils#getPipelineFromProgram
URL: https://github.com/apache/flink/pull/11607
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 08:08;githubbot;600","aljoscha commented on pull request #11608: [FLINK-16560] Forward Configuration in PackagedProgramUtils#getPipelineFromProgram
URL: https://github.com/apache/flink/pull/11608
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 08:14;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,FLINK-16944,,,,,,FLINK-16658,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 08:14:56 UTC 2020,,,,,,,,,,"0|z0cg2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/20 03:56;zhuzh;cc [~aljoscha] [~klion26];;;","12/Mar/20 10:40;aljoscha;We should fix this, yes. ;;;","01/Apr/20 03:56;zhuzh;[~aljoscha] do you think this is a blocker issue to fix in 1.10.1?;;;","01/Apr/20 08:59;aljoscha;I think we don't need to fix it in 1.10.1, since the ""buggy"" behaviour is the same as before Flink 1.10, where we didn't read the Configuration at all in the environments. [~kkloudas] what do you think?;;;","01/Apr/20 12:53;aljoscha;Ah, come to think about this again, I think it would be good to fix in 1.10.1.;;;","01/Apr/20 13:33;kkl0u;I think that this is resolved by https://issues.apache.org/jira/browse/FLINK-16658 but for the 1.11. For 1.10.1 there should be a separate fix.;;;","02/Apr/20 08:14;aljoscha;master: a24734ea339872763306b44770678c4ace6a369f
release-1.10: bf3c84370748e444257b49c6cdd52663f7379436;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot create Hive avro table in test,FLINK-16559,13291215,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,12/Mar/20 02:56,09/Jun/20 16:13,13/Jul/23 08:07,09/Jun/20 04:17,1.10.0,,,,,1.11.0,,,,Connectors / Hive,Tests,,,,0,pull-request-available,,,,"Trying to create a Hive avro table will hit the following exception:
{noformat}
Caused by: java.lang.NoSuchMethodError: org.apache.avro.Schema$Field.<init>(Ljava/lang/String;Lorg/apache/avro/Schema;Ljava/lang/String;Lorg/codehaus/jackson/JsonNode;)V
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:76)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:61)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:170)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:114)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:83)
	at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:449)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:436)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:641)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:624)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:831)
......
{noformat}",,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15547,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 04:17:03 UTC 2020,,,,,,,,,,"0|z0cg14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/20 03:01;lirui;This is due to avro conflict between hive-exec and flink-shaded-hadoop-2-uber. In flink-shaded-hadoop-2-uber {{org/codehaus/jackson/JsonNode}} is relocated. Therefore if we load {{Schema$Field}} from flink-shaded-hadoop-2-uber, we'll get this NoSuchMethodError.;;;","19/May/20 09:49;lzljs3620320;It should can be used with HADOOP_CLASSPATH. And we can consider shading also?;;;","19/May/20 10:02;lzljs3620320;At-least, we should document this.;;;","09/Jun/20 04:17;lzljs3620320;master: fd9214e7754455ab4733fe52daeb4e0b292e583f

release-1.11: c98122c29ef875d7fbc2b85fc9496bcfa11e0363;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopS3* tests fail with NullPointerException exceptions,FLINK-16550,13291096,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,rmetzger,rmetzger,11/Mar/20 14:33,22/Jun/21 14:04,13/Jul/23 08:07,12/Mar/20 12:45,1.11.0,,,,,1.10.1,1.11.0,,,FileSystems,,,,,0,pull-request-available,test-stability,,,"Logs: https://travis-ci.org/github/apache/flink/jobs/660975486?utm_medium=notification

All subsequent builds failed as well. It is likely that this commit / FLINK-16014 introduced the issue, as these tests depend on S3 credentials to be available.

{code}
09:38:48.022 [INFO] -------------------------------------------------------
09:38:48.025 [INFO]  T E S T S
09:38:48.026 [INFO] -------------------------------------------------------
09:38:48.657 [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase
09:38:48.669 [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase
09:38:54.541 [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 5.88 s <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase
09:38:54.542 [ERROR] testResumeAfterCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase)  Time elapsed: 3.592 s  <<< ERROR!
java.lang.Exception: Unexpected exception, expected<java.io.IOException> but was<java.lang.NullPointerException>
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:162)

09:38:54.542 [ERROR] testResumeWithWrongOffset(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase)  Time elapsed: 0.24 s  <<< ERROR!
java.lang.Exception: Unexpected exception, expected<java.io.IOException> but was<java.lang.NullPointerException>
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:182)

09:38:54.542 [ERROR] testExceptionWritingAfterCloseForCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase)  Time elapsed: 0.448 s  <<< ERROR!
java.lang.Exception: Unexpected exception, expected<java.io.IOException> but was<java.lang.NullPointerException>
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testExceptionWritingAfterCloseForCommit(HadoopS3RecoverableWriterExceptionITCase.java:144)

09:38:55.173 [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase
09:38:58.737 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.066 s <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase
09:38:58.737 [ERROR] testDirectoryListing(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase)  Time elapsed: 3.448 s  <<< ERROR!
java.io.FileNotFoundException: No such file or directory: s3://[secure]/temp/tests-f37db36e-c116-4c58-a16b-8ca241baae4b/testdir

09:38:59.447 [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase
09:39:01.791 [ERROR] Tests run: 13, Failures: 0, Errors: 13, Skipped: 0, Time elapsed: 6.611 s <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase
09:39:01.797 [ERROR] testCloseWithNoData(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 2.394 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCloseWithNoData(HadoopS3RecoverableWriterITCase.java:186)

09:39:01.798 [ERROR] testCommitAfterPersist(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.191 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCommitAfterPersist(HadoopS3RecoverableWriterITCase.java:208)

09:39:01.799 [ERROR] testRecoverWithEmptyState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.235 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithSmallData(HadoopS3RecoverableWriterITCase.java:352)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithEmptyState(HadoopS3RecoverableWriterITCase.java:302)

09:39:01.799 [ERROR] testRecoverFromIntermWithoutAdditionalState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.181 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithSmallData(HadoopS3RecoverableWriterITCase.java:352)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalState(HadoopS3RecoverableWriterITCase.java:316)

09:39:01.799 [ERROR] testCallingDeleteObjectTwiceDoesNotThroughException(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.181 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCallingDeleteObjectTwiceDoesNotThroughException(HadoopS3RecoverableWriterITCase.java:245)

09:39:01.801 [ERROR] testCommitAfterNormalClose(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.174 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCommitAfterNormalClose(HadoopS3RecoverableWriterITCase.java:196)

09:39:01.802 [ERROR] testRecoverWithStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.338 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:330)

09:39:01.803 [ERROR] testRecoverFromIntermWithoutAdditionalStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.486 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:337)

09:39:01.810 [ERROR] testRecoverWithState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.199 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithSmallData(HadoopS3RecoverableWriterITCase.java:352)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithState(HadoopS3RecoverableWriterITCase.java:309)

09:39:01.810 [ERROR] testCleanupRecoverableState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.202 s  <<< ERROR!
java.lang.Exception: Unexpected exception, expected<java.io.FileNotFoundException> but was<java.lang.NullPointerException>
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCleanupRecoverableState(HadoopS3RecoverableWriterITCase.java:223)

09:39:01.810 [ERROR] testCommitAfterRecovery(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.26 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCommitAfterRecovery(HadoopS3RecoverableWriterITCase.java:270)

09:39:01.810 [ERROR] testRecoverAfterMultiplePersistsState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.165 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithSmallData(HadoopS3RecoverableWriterITCase.java:352)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsState(HadoopS3RecoverableWriterITCase.java:323)

09:39:01.810 [ERROR] testRecoverAfterMultiplePersistsStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase)  Time elapsed: 0.735 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:344)

09:39:14.711 [WARNING] Tests run: 8, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 15.262 s - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase
09:39:15.047 [INFO] 
09:39:15.047 [INFO] Results:
09:39:15.047 [INFO] 
09:39:15.047 [ERROR] Errors: 
09:39:15.047 [ERROR]   HadoopS3FileSystemITCase>AbstractHadoopFileSystemITTest.testDirectoryListing:127 Â» FileNotFound
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterExceptionITCase.testExceptionWritingAfterCloseForCommit Â» 
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit Â»  Unexpected e...
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset Â»  Unexpect...
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testCallingDeleteObjectTwiceDoesNotThroughException:245 Â» NullPointer
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testCleanupRecoverableState Â»  Unexpected exce...
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testCloseWithNoData:186 Â» NullPointer
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testCommitAfterNormalClose:196 Â» NullPointer
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testCommitAfterPersist:208 Â» NullPointer
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testCommitAfterRecovery:270 Â» NullPointer
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsState:323->testResumeAfterMultiplePersistWithSmallData:352->testResumeAfterMultiplePersist:384 Â» NullPointer
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsStateWithMultiPart:344->testResumeAfterMultiplePersistWithMultiPartUploads:364->testResumeAfterMultiplePersist:384 Â» NullPointer
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalState:316->testResumeAfterMultiplePersistWithSmallData:352->testResumeAfterMultiplePersist:384 Â» NullPointer
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalStateWithMultiPart:337->testResumeAfterMultiplePersistWithMultiPartUploads:364->testResumeAfterMultiplePersist:384 Â» NullPointer
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testRecoverWithEmptyState:302->testResumeAfterMultiplePersistWithSmallData:352->testResumeAfterMultiplePersist:384 Â» NullPointer
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testRecoverWithState:309->testResumeAfterMultiplePersistWithSmallData:352->testResumeAfterMultiplePersist:384 Â» NullPointer
09:39:15.047 [ERROR]   HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart:330->testResumeAfterMultiplePersistWithMultiPartUploads:364->testResumeAfterMultiplePersist:384 Â» NullPointer
09:39:15.047 [INFO] 
09:39:15.047 [ERROR] Tests run: 26, Failures: 0, Errors: 17, Skipped: 2
{code}",,arvid heise,nkruber,rmetzger,,,,,,,,,,,,,,,,,,,"AHeise commented on pull request #11384: [FLINK-16550][s3] Force namespace parsing in XmlResponsesSaxParser.
URL: https://github.com/apache/flink/pull/11384
 
 
   
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   This commit explicitly enables Sax' NAMESPACES_FEATURE (default true) in JAXP (default false), such that multi part uploads to S3 actually work.
   
   ## Brief change log
   
   Explicitly enables Sax' NAMESPACES_FEATURE (default true) in JAXP (default false).
   
   ## Verifying this change
   
   Covered by e2e only executed on nightly.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (**yes** / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/20 21:39;githubbot;600","rmetzger commented on pull request #11384: [FLINK-16550][s3] Force namespace parsing in XmlResponsesSaxParser.
URL: https://github.com/apache/flink/pull/11384
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Mar/20 12:45;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,FLINK-16564,,,,,,FLINK-16014,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 16:01:47 UTC 2020,,,,,,,,,,"0|z0cfao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/20 20:48;chesnay;ping [~AHeise];;;","11/Mar/20 22:30;nkruber;This actually also showed up in an end-to-end Flink cluster setup where I couldn't download my savepoint from s3 anymore and this showed up on job submission:

{code}
 The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
        at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)
        at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:199)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1741)
        at org.apache.flink.streaming.api.environment.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:94)
        at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:63)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1620)
        at org.apache.flink.streaming.examples.windowing.TopSpeedWindowing.main(TopSpeedWindowing.java:96)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
        ... 8 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1736)
        ... 17 more
Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:359)
        at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
        at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
        at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:274)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
        at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$3(Dispatcher.java:336)
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
        at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
        ... 6 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
        at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:152)
        at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:84)
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:379)
        at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34)
        ... 7 more
Caused by: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory '<path>' on file system 's3'.
        at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorage.resolveCheckpointPointer(AbstractFsCheckpointStorage.java:243)
        at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorage.resolveCheckpoint(AbstractFsCheckpointStorage.java:110)
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1152)
        at org.apache.flink.runtime.scheduler.SchedulerBase.tryRestoreExecutionGraphFromSavepoint(SchedulerBase.java:307)
        at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:240)
        at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:216)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:120)
        at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:105)
        at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:278)
        at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:266)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)
        at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:146)
        ... 10 more

End of exception on server side>]
        at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:390)
        at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:374)
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
        ... 4 more
{code}

The proposed patch fixes that scenario as well.;;;","11/Mar/20 22:42;rmetzger;Thanks for providing a patch Arvid. I assigned you to the ticket.;;;","12/Mar/20 12:45;rmetzger;Resolved in 62abfa97a497f03d8701f4acdbdd9217f0d33a06;;;","12/Mar/20 16:00;arvid;Should also be backported to 1.10.;;;","12/Mar/20 16:01;rmetzger;Done in 15a149ebc4042c5f623c88c1afddb826bc4d09e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document of table.exec.shuffle-mode is incorrect,FLINK-16541,13291051,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,11/Mar/20 10:51,16/Mar/20 03:31,13/Jul/23 08:07,16/Mar/20 03:31,1.10.0,,,,,1.10.1,1.11.0,,,Documentation,,,,,0,pull-request-available,,,,"Document of table.exec.shuffle-mode is incorrect. Currently the document states that
{code:java}
Sets exec shuffle mode. Only batch or pipeline can be set.
{code}
But ""pipeline"" is incorrect, the correct configuration is ""pipeline{color:#FF0000}*d*{color}"". Using pipeline instead of pipelined will throw the following exception.
{code:java}
java.lang.IllegalArgumentException: table.exec.shuffle-mode can only be set to BATCH or PIPELINED
	at org.apache.flink.table.planner.utils.ExecutorUtils.isShuffleModeAllBatch(ExecutorUtils.java:98)
	at org.apache.flink.table.planner.utils.ExecutorUtils.setBatchProperties(ExecutorUtils.java:69)
	at org.apache.flink.table.planner.delegation.BatchExecutor.getStreamGraph(BatchExecutor.java:50)
	at com.ververica.flink.table.gateway.context.ExecutionContext.createPipeline(ExecutionContext.java:272)
	at com.ververica.flink.table.gateway.operation.SelectOperation.executeQueryInternal(SelectOperation.java:246){code}",,godfreyhe,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,"TsReaper commented on pull request #11386: [FLINK-16541][doc] Fix document of table.exec.shuffle-mode
URL: https://github.com/apache/flink/pull/11386
 
 
   ## What is the purpose of the change
   
   This PR fixes the document of the `table.exec.shuffle-mode` table config.
   
   ## Brief change log
   
    - Change `pipeline` option to `pipelined` in the document.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Mar/20 02:05;githubbot;600","JingsongLi commented on pull request #11386: [FLINK-16541][doc] Fix document of table.exec.shuffle-mode
URL: https://github.com/apache/flink/pull/11386
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/20 02:37;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 03:31:19 UTC 2020,,,,,,,,,,"0|z0cf0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/20 03:31;lzljs3620320;master: 328193db25a089b321b80730e21bb122763565eb

release-1.10: b5ef72f0a86f552e832fb1f97a4f2c110fcf5b07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql client set param error,FLINK-16539,13291020,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zhangjun,zhangjun,zhangjun,11/Mar/20 08:50,26/May/20 18:58,13/Jul/23 08:07,12/Mar/20 13:46,1.10.0,,,,,1.11.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"When setting int type parameters in sql client, such as:
set execution.parallelism = 10;
The system threw an exception: 
{code:java}
//
Caused by: org.apache.flink.table.api.ValidationException: Property 'parallelism' must be a integer value but was: 10 at org.apache.flink.table.descriptors.DescriptorProperties.validateComparable(DescriptorProperties.java:1572) at org.apache.flink.table.descriptors.DescriptorProperties.validateInt(DescriptorProperties.java:944) at org.apache.flink.table.descriptors.DescriptorProperties.validateInt(DescriptorProperties.java:937) at org.apache.flink.table.client.config.entries.ExecutionEntry.validate(ExecutionEntry.java:140) at org.apache.flink.table.client.config.entries.ConfigEntry.<init>(ConfigEntry.java:39) ... 11 more
{code}",,godfreyhe,jark,zhangjun,,,,,,,,,,,,,,,,,,,"zhangjun888 commented on pull request #11392: [FLINK-16539]fix sql client set int param error
URL: https://github.com/apache/flink/pull/11392
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   fix sql client set int param error
   
   
   ## Brief change log
   
     - Remove spaces before and after attribute value
   
   ## Verifying this change
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Mar/20 08:37;githubbot;600","KurtYoung commented on pull request #11392: [FLINK-16539]fix sql client set int param error
URL: https://github.com/apache/flink/pull/11392
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Mar/20 13:45;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 13:46:12 UTC 2020,,,,,,,,,,"0|z0cets:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/20 08:58;ykt836;Looks like we forgot to trim the value somewhere. [~zhangjun] would you like to fix this?;;;","11/Mar/20 09:20;godfreyhe;you are right [~ykt836], the `DescriptorProperties` does not trim the string value when doing the validation.;;;","12/Mar/20 08:43;zhangjun;If the attribute value is an int type with spaces in front, the system does not remove the spaces. When using Integer.valueOf () function to verify it, an error will be thrown, we just remove the spaces before and after;;;","12/Mar/20 08:58;zhangjun;I found that the system did a trim operation when parsing the command line, but did not do a trim operation on the value. Besides, I don’t know if the trim will affect other commands except the set command,so I add the trim to the callSet method. [~ykt836];;;","12/Mar/20 13:46;ykt836;merge to master: 625da7b51004bbda8cd97c46208766ba98c63288;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shouldn't trim whitespaces in Path,FLINK-16532,13290975,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,11/Mar/20 03:49,23/Mar/20 02:48,13/Jul/23 08:07,23/Mar/20 02:48,1.10.0,,,,,1.11.0,,,,API / Core,,,,,0,pull-request-available,,,,"The following test fails:
{code}
create table dest (x int) partitioned by (p string);
insert into dest select 1,' ';
{code}
With error:
{noformat}
Caused by: MetaException(message:Invalid partition key & values; keys [p, ], values [])
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partition_result$get_partition_resultStandardScheme.read(ThriftHiveMetastore.java)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partition_result$get_partition_resultStandardScheme.read(ThriftHiveMetastore.java)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partition_result.read(ThriftHiveMetastore.java)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_partition(ThriftHiveMetastore.java:2204)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_partition(ThriftHiveMetastore.java:2189)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getPartition(HiveMetaStoreClient.java:1307)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:169)
	at com.sun.proxy.$Proxy33.getPartition(Unknown Source)
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.getPartition(HiveMetastoreClientWrapper.java:127)
	at org.apache.flink.connectors.hive.HiveTableMetaStoreFactory$HiveTableMetaStore.getPartition(HiveTableMetaStoreFactory.java:88)
	at org.apache.flink.table.filesystem.PartitionLoader.loadPartition(PartitionLoader.java:69)
	at org.apache.flink.table.filesystem.FileSystemCommitter.commitSingleCheckpoint(FileSystemCommitter.java:111)
	at org.apache.flink.table.filesystem.FileSystemCommitter.commitUpToCheckpoint(FileSystemCommitter.java:99)
	at org.apache.flink.table.filesystem.FileSystemOutputFormat.finalizeGlobal(FileSystemOutputFormat.java:90)
	... 34 more
{noformat}",,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11393: [FLINK-16532][core][hive] Cannot insert white space characters as par…
URL: https://github.com/apache/flink/pull/11393
 
 
   …tition value for Hive table
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   To fix the issue that Flink doesn't support trailing whitespaces for partition values, which is allowed in Hive.
   
   
   ## Brief change log
   
     - Don't trim whitespaces in Flink `Path`.
     - Update related tests for `Path`.
     - Add test case for Hive connector.
   
   
   ## Verifying this change
   
   Existing and new test case.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Mar/20 11:32;githubbot;600","JingsongLi commented on pull request #11393: [FLINK-16532][core] Shouldn't trim whitespaces in Path
URL: https://github.com/apache/flink/pull/11393
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Mar/20 02:47;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-1640,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 02:48:28 UTC 2020,,,,,,,,,,"0|z0cejs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/20 02:48;lzljs3620320;Master: 044733db92cc3fa825923e88a51416aa8d9476c6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Properties in version field are not resolved,FLINK-16527,13290949,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,nobe0716,nobe0716,11/Mar/20 00:38,08/Apr/20 21:32,13/Jul/23 08:07,25/Mar/20 13:57,shaded-10.0,,,,,shaded-11.0,,,,BuildSystem / Shaded,,,,,0,pull-request-available,,,,"https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/
several pom of above path seem to have wrong property value,
`hadoop.version` with `2.4.1`

https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.6.5-7.0/flink-shaded-hadoop-2-uber-2.6.5-7.0.pom
https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.6.5-8.0/flink-shaded-hadoop-2-uber-2.6.5-8.0.pom
https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.6.5-9.0/flink-shaded-hadoop-2-uber-2.6.5-9.0.pom
https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.7.5-7.0/flink-shaded-hadoop-2-uber-2.7.5-7.0.pom
https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.7.5-8.0/flink-shaded-hadoop-2-uber-2.7.5-8.0.pom
https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.7.5-9.0/flink-shaded-hadoop-2-uber-2.7.5-9.0.pom
https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-7.0/flink-shaded-hadoop-2-uber-2.8.3-7.0.pom
https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-8.0/flink-shaded-hadoop-2-uber-2.8.3-8.0.pom
https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-9.0/flink-shaded-hadoop-2-uber-2.8.3-9.0.pom

https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/hive/#depedencies
Documentation recommended `2.6.5-8.0` that cannot be built.

It makes below errors.

{code:groovy}
Warning:<i><b>project ':streaming-flink': Unable to build Scala project configuration</b>
Details: org.gradle.api.internal.artifacts.ivyservice.DefaultLenientConfiguration$ArtifactResolveException: Could not resolve all files for configuration ':streaming-flink:compileClasspath'.
Caused by: org.gradle.internal.resolve.ModuleVersionResolveException: Could not resolve org.apache.flink:flink-shaded-hadoop-2-uber:2.7.5-7.0.
Required by:
    project :streaming-flink
Caused by: org.gradle.internal.resolve.ModuleVersionResolveException: Could not resolve org.apache.flink:flink-shaded-hadoop-2-uber:2.7.5-7.0.
Caused by: org.gradle.api.internal.artifacts.ivyservice.ivyresolve.parser.MetaDataParseException: inconsistent module metadata found. Descriptor: org.apache.flink:flink-shaded-hadoop-2-uber:2.4.1-7.0 Errors: bad version: expected='2.7.5-7.0' found='2.4.1-7.0'</i>
{code}",,nobe0716,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #82: [FLINK-16527][build] Resolve properties in version field
URL: https://github.com/apache/flink-shaded/pull/82
 
 
   During the build process we rely on the `shade-plugin` to replace references to properties (e.g., `${scala.binary.version}`) by their actual value.
   This is important because the property _field_ cannot be changed during the build. If you set a property on the command-line the _field_ is not updated, only the in-memory analogue is.
   Hence any references to the property after the build process are problematic because you can have a value mismatch between the value set in the field vs the one set at build-time.
   
   Turns out the shade-plugin does not resolve properties in the `<version>` field, resulting in an inconsistency which some build systems outright reject.
   
   We now additionally use the `flatten-plugin` to resolve _all_ properties.
   The primary purpose of this plugin is to create fully self-contained poms without any references to parents and what-not; since we only care about properties we run it in it's `resolveCiFriendliesOnly` mode, which is the least invasive mode of operation.
   (For us) it only resolves properties and copies the `license` field into the root pom (which is fine, but unfortunate).
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/20 10:12;githubbot;600","zentol commented on pull request #82: [FLINK-16527][build] Resolve properties in version field
URL: https://github.com/apache/flink-shaded/pull/82
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 13:57;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-17059,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 13:57:15 UTC 2020,,,,,,,,,,"0|z0cee0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/20 00:52;nobe0716;I downloaded full contents of https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.7.5-7.0/ and change hadoop.version in .pom file to 2.7.5, then build success;;;","25/Mar/20 13:57;chesnay;shaded-master: d898878f7f5abe03bd25fbc52ce820c7f31d5791;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix exception when computed column expression references a keyword column name,FLINK-16526,13290873,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,liuyufei,liuyufei,10/Mar/20 17:19,13/Mar/20 07:29,13/Jul/23 08:07,12/Mar/20 06:38,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / API,,,,,0,pull-request-available,,,,"{code:sql}
json_row          ROW<`timestamp` BIGINT>,
`timestamp`   AS `json_row`.`timestamp`
{code}
It translate to ""SELECT json_row.timestamp FROM __temp_table__""
Throws exception ""Encountered "". timestamp"" at line 1, column 157. Was expecting one of:...""",,danny0405,jark,libenchao,liuyufei,,,,,,,,,,,,,,,,,,"wuchong commented on pull request #11380: [FLINK-16526][table-planner-blink] Fix exception when computed column expression references a keyword column name
URL: https://github.com/apache/flink/pull/11380
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix parse exception when computed column expression references a keyword column name, e.g. 
   
   ```sql
   json_row          ROW<`timestamp` BIGINT>,
   `timestamp`   AS `json_row`.`timestamp`
   ```
   
   ## Brief change log
   
   - Calcite does't quote `SqlIdentifier` in `SqlNode#toString`, so we can't use this method to get a quoted string.
   - quote `SqlIdentifier` by ourselves.
   
   ## Verifying this change
   
   - Added a test in `TableScanTest` to cover this case.
   - Update `CatalogTableITCase#testInsertSourceTableWithUserDefinedFuncField` to cover this case.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/20 14:25;githubbot;600","wuchong commented on pull request #11380: [FLINK-16526][table-planner-blink] Fix exception when computed column expression references a keyword column name
URL: https://github.com/apache/flink/pull/11380
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Mar/20 06:35;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 06:38:50 UTC 2020,,,,,,,,,,"0|z0cdx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/20 02:05;ykt836;cc [~danny0405];;;","11/Mar/20 03:14;danny0405;Thanks, [~liuyufei], you should try

{code:sql}
`timestamp`   AS json_row['timestamp']
{code}

instead.
;;;","11/Mar/20 14:04;jark;This is a bug, I will fix it. ;;;","12/Mar/20 06:38;jark;Fixed in
 - master (1.11.0) : d723d0012c6b5e41e9d56784bc424e3942747225
 - 1.10.1: 884edd6dec549450ac44eb80d83de85eb50dc11b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TwoPhaseCommitSinkFunction subtask logs misleading name,FLINK-16525,13290838,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fpaul,fpaul,fpaul,10/Mar/20 15:22,13/Apr/21 20:40,13/Jul/23 08:07,11/Mar/20 07:04,,,,,,1.11.0,,,,Runtime / Task,,,,,0,pull-request-available,,,,"The current name() function in TwoPhaseCommitSinkFunction tries to describe the currently running subtask with its class name, the index of the subtask and the number of parallel subtasks.

Since the starting index of the subtask is 0, and the starting number for the parallelism is 1, it could lead to the following log message.
{code:java}
15:59:41,448 INFO  org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction  - FlinkKafkaProducer 0/1 - checkpoint 1 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1583852371370} from checkpoint 1
{code}
Although only one subtask is running it describes the subtask as 0/1 which might indicate more than one subtask.

I would suggest incrementing the first number after the class name by 1 to better indicate how many subtasks are running.

 

 ",,fpaul,pnowojski,wind_ljy,,,,,,,,,,,,,,,,,,,"fapaul commented on pull request #11371: [FLINK-16525][task] Increment subtask id by 1 to display subtask name
URL: https://github.com/apache/flink/pull/11371
 
 
   ## What is the purpose of the change
   
   Improve displayed name for TwoPhaseCommitSink subtask instances.
   
   ## Brief change log
   
   Increment the subtask index before displaying it to better match the parallelism which index starts at 1.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Mar/20 15:57;githubbot;600","pnowojski commented on pull request #11371: [FLINK-16525][task] Increment subtask id by 1 to display subtask name
URL: https://github.com/apache/flink/pull/11371
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/20 07:03;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 11 07:04:25 UTC 2020,,,,,,,,,,"0|z0cdpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/20 07:04;pnowojski;merged commit 642f096 into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointCoordinatorFailureTest logs LinkageErrors,FLINK-16519,13290702,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,godfreyhe,godfreyhe,godfreyhe,10/Mar/20 03:03,11/Mar/20 10:42,13/Jul/23 08:07,11/Mar/20 10:42,1.11.0,,,,,1.11.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"This issue is in https://travis-ci.org/apache/flink/jobs/660152153?utm_medium=notification&utm_source=slack

Log output

{code:java}
2020-03-09 15:52:14,550 main ERROR Could not reconfigure JMX java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/javassist/JavassistMockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
	at org.powermock.core.classloader.javassist.JavassistMockClassLoader.loadUnmockedClass(JavassistMockClassLoader.java:90)
	at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:104)
	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)
	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:165)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:141)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:590)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:349)
	at org.apache.flink.util.TestLogger.<init>(TestLogger.java:36)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.<init>(CheckpointCoordinatorFailureTest.java:55)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTestInstance(PowerMockJUnit44RunnerDelegateImpl.java:197)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTest(PowerMockJUnit44RunnerDelegateImpl.java:182)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:204)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:160)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:134)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:34)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:44)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:136)
	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:117)
	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:57)
	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}
",,godfreyhe,rmetzger,sewen,,,,,,,,,,,,,,,,,,,"godfreyhe commented on pull request #11366: [FLINK-16519] [checkpointing] Remove PowerMockito to avoid LinkageError in CheckpointCoordinatorFailureTest
URL: https://github.com/apache/flink/pull/11366
 
 
   
   
   ## What is the purpose of the change
   
   *CheckpointCoordinatorFailureTest does not depend on PowerMockito any more, remove it to avoid LinkageError*
   
   
   ## Brief change log
   
     - *Remove PowerMockito from CheckpointCoordinatorFailureTest*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Mar/20 11:51;githubbot;600","zentol commented on pull request #11366: [FLINK-16519] [checkpointing] Remove PowerMockito to avoid LinkageError in CheckpointCoordinatorFailureTest
URL: https://github.com/apache/flink/pull/11366
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/20 10:42;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16476,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 11 10:42:36 UTC 2020,,,,,,,,,,"0|z0ccv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/20 08:02;sewen;[~chesnay] Do we need to define ""exclusions"" for the log4j2 classes in the {{@Powermock}} annotation?
Like we have in some other tests:
{code}
@PowerMockIgnore({""java.*"", ""javax.*"", ""org.slf4j.*"", ""org.apache.log4j.*""})
{code};;;","10/Mar/20 08:19;godfreyhe;In fact, I found this test class does not depend on {{PowerMock}} any more. So I think removing {{PowerMock}} is the simplest approach to fix this issue.;;;","10/Mar/20 08:19;godfreyhe;could I take this ticket ?;;;","10/Mar/20 11:25;sewen;Sure, go ahead!;;;","11/Mar/20 09:33;chesnay;[~sewen] The exclusion for {{javax.*}} should have been sufficient.;;;","11/Mar/20 10:42;chesnay;master: 610c9fb405615cf3eae4b082cbbc6c81f06d0028;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stateful Function's KafkaSinkProvider should use `setProperty` instead of `put` for resolving client properties,FLINK-16518,13290700,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,10/Mar/20 02:39,10/Mar/20 08:13,13/Jul/23 08:07,10/Mar/20 08:09,,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"The {{put}} method is strongly discourage to be used on {{Properties}} as a bad practice, since it allows putting non-string values.

This has already caused a bug, where a long was put into the properties, while Kafka was expecting an integer:
{code}
org.apache.kafka.common.config.ConfigException: Invalid value 100000 for configuration transaction.timeout.ms: Expected value to be a 32-bit integer, but it was a java.lang.Long
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:669)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:471)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:464)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:62)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:75)
	at org.apache.kafka.clients.producer.ProducerConfig.<init>(ProducerConfig.java:396)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:326)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:298)
	at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.<init>(FlinkKafkaInternalProducer.java:76)
{code}",,tzulitai,,,,,,,,,,,,,,,,,,,,,"tzulitai commented on pull request #55:  [FLINK-16518] [kafka] Set client properties as strings in KafkaSinkProvider
URL: https://github.com/apache/flink-statefun/pull/55
 
 
   The put method is strongly discourage to be used on `Properties` as a bad practice, since it allows putting non-string values.
   
   This has already caused a bug, where a long was put into the properties, while Kafka was expecting an integer:
   ```
   org.apache.kafka.common.config.ConfigException: Invalid value 100000 for configuration transaction.timeout.ms: Expected value to be a 32-bit integer, but it was a java.lang.Long
   	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:669)
   	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:471)
   	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:464)
   	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:62)
   	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:75)
   	at org.apache.kafka.clients.producer.ProducerConfig.<init>(ProducerConfig.java:396)
   	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:326)
   	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:298)
   	at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.<init>(FlinkKafkaInternalProducer.java:76)
   ```
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Mar/20 03:01;githubbot;600","tzulitai commented on pull request #55:  [FLINK-16518] [kafka] Set client properties as strings in KafkaSinkProvider
URL: https://github.com/apache/flink-statefun/pull/55
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Mar/20 08:09;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 10 08:09:59 UTC 2020,,,,,,,,,,"0|z0ccuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/20 08:09;tzulitai;Fixed in master via ab491a80db6e0ac5bb44d7a19f99d8bb88f7a1e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task manager safeguard shutdown may not be reliable,FLINK-16510,13290629,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,09/Mar/20 20:02,07/Aug/20 10:30,13/Jul/23 08:07,07/Aug/20 10:30,1.10.1,1.11.1,1.12.0,,,1.10.2,1.11.2,1.12.0,,Runtime / Task,,,,,0,pull-request-available,,,,"The {{JvmShutdownSafeguard}} does not always succeed but can hang when multiple threads attempt to shutdown the JVM. Apparently mixing {{System.exit()}} with ShutdownHooks and forcefully terminating the JVM via {{Runtime.halt()}} does not play together well:

{noformat}
""Jvm Terminator"" #22 daemon prio=5 os_prio=0 tid=0x00007fb8e82f2800 nid=0x5a96 runnable [0x00007fb35cffb000]
   java.lang.Thread.State: RUNNABLE
	at java.lang.Shutdown.$$YJP$$halt0(Native Method)
	at java.lang.Shutdown.halt0(Shutdown.java)
	at java.lang.Shutdown.halt(Shutdown.java:139)
	- locked <0x000000047ed67638> (a java.lang.Shutdown$Lock)
	at java.lang.Runtime.halt(Runtime.java:276)
	at org.apache.flink.runtime.util.JvmShutdownSafeguard$DelayedTerminator.run(JvmShutdownSafeguard.java:86)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

""FlinkCompletableFutureDelayScheduler-thread-1"" #18154 daemon prio=5 os_prio=0 tid=0x00007fb708a7d000 nid=0x5a8a waiting for monitor entry [0x00007fb289d49000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at java.lang.Shutdown.halt(Shutdown.java:139)
	- waiting to lock <0x000000047ed67638> (a java.lang.Shutdown$Lock)
	at java.lang.Shutdown.exit(Shutdown.java:213)
	- locked <0x000000047edb7348> (a java.lang.Class for java.lang.Shutdown)
	at java.lang.Runtime.exit(Runtime.java:110)
	at java.lang.System.exit(System.java:973)
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.terminateJVM(TaskManagerRunner.java:266)
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.lambda$onFatalError$1(TaskManagerRunner.java:260)
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$$Lambda$27464/1464672548.accept(Unknown Source)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:943)
	at org.apache.flink.runtime.concurrent.DirectExecutorService.execute(DirectExecutorService.java:211)
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$orTimeout$11(FutureUtils.java:361)
	at org.apache.flink.runtime.concurrent.FutureUtils$$Lambda$27435/159015392.run(Unknown Source)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- <0x00000006d5e56bd0> (a java.util.concurrent.ThreadPoolExecutor$Worker)
{noformat}

Note that under this condition the JVM should terminate but it still hangs. Sometimes it quits after several minutes.",,aljoscha,mxm,rmetzger,sewen,thw,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16511,,FLINK-17470,FLINK-11103,FLINK-16573,,,,,,,"28/Jul/20 19:58;mxm;command.txt;https://issues.apache.org/jira/secure/attachment/13008622/command.txt","23/Jul/20 17:31;mxm;stack2-1.txt;https://issues.apache.org/jira/secure/attachment/13008296/stack2-1.txt","28/Jul/20 19:57;mxm;stack3-mixed.txt;https://issues.apache.org/jira/secure/attachment/13008620/stack3-mixed.txt","28/Jul/20 19:58;mxm;stack3.txt;https://issues.apache.org/jira/secure/attachment/13008621/stack3.txt",,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 07 10:30:36 UTC 2020,,,,,,,,,,"0|z0ccew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/20 10:45;sewen;Do you know what exactly is happening there? It does not look like a deadlock.
Is the {{java.lang.Shutdown.$$YJP$$halt0}} call simply stuck?;;;","10/Mar/20 11:15;mxm;Yes, indeed no deadlock. The thread which calls halt0 is in RUNNABLE state. Unfortunately, I have not been able to find out what it is doing but I suspect that multiple threads triggering the shutdown causes the JVM to choke.

I'll try to use strace next time I'm seeing this. ;;;","12/Mar/20 12:38;mxm;We haven't seen this particular problem after we replaced all graceful shutdowns with hard exists. However, we've seen task managers freezing. Looks like this is caused by lack of metaspace (we restrict it to 2GB). The meta space fills up after many restarts due to lingering threads which hold on to the classloader. ;;;","12/Mar/20 13:57;sewen;Have you looked into ""application""/""per job"" deployment? That eliminates all dynamic class loading in the first place.
Using ""sessions"" mode for only a single job may be a non-ideal setup.;;;","12/Mar/20 15:43;mxm;I'm constrained by what Lyft's k8s operator offers. At this point it brings up a ""session"" cluster and submits the job against it. I agree that this is suboptimal. Note that we would still have the problem of lingering threads like in FLINK-16573 which could eat up memory, even though meta space would not be affected.;;;","08/Jul/20 17:54;thw;We are not able to reliably run our applications on k8s when pods get stuck during termination on a fatal task manager error. When pods don't exit our infrastructure cannot replace the task manager and applications cannot recover. We have seen this issue many times and we were able to reproduce it with benchmarks that produce intermittent OOMs. Based on the analysis from [~mxm] we have applied this change to our fork:

[https://github.com/lyft/flink/commit/4787e4d638c5b299164b85e7e492967bf573c400]

We would like to address this issue upstream though. When a fatal error occurs, the process should safely terminate. Triggering shutdown hooks is unlikely to succeed. It is important that we get a fresh TM deployed to allow for job recovery and forward progress (avoid extended downtime and need for manual intervention).

Do you see any downside using the hard stop instead of System.exit?

Currently, there are multiple occurrences of System.exit - for everything that aims to ""exitOnFatalError"" it would be nice to centralize. ;;;","09/Jul/20 10:13;sewen;The current approach in Flink is the following:

  - fatal errors initiall call System.exit(), which allows shutdown hooks to run and to clean up stuff (local files)
  - one of the Shutdown hooks has a timer that calls Runtime.halt() if the JVM is not terminated after 10 seconds.

The change you proposed would mean that Flink never attempts to clean up.

For some reason, the shutdown hooks seem to not work reliably on some JVM / Kernel versions. Could you confirm whether you might be affected by the same issue as FLINK-17470 ?

If yes, could we try and find another way to go for the ""graceful exit first, hard exit if unsuccessful"" approach?

;;;","09/Jul/20 11:31;mxm;If you look at the stacktrace in the description, you can see that the presence of a graceful shutdown including shutdown hooks can block a forceful shutdown via {{System.getRuntime().halt()}}. We had applied the fix that Thomas linked to our Flink 1.8 version, once we moved to Flink 1.10, we hadn't reapplied the fix and were running into this issue again. Hence, we reverted back to always using Runtime#halt(). It is crucial for us that the shutdown always succeeds, no matter what, otherwise the hosting K8s pod won't terminate and we are in an idle state.

FLINK-17470 is unrelated because we do not use the bash scripts to stop the node. Rather, the problem is that the TaskManager does not terminate itself during fatal errors, OOM in our case. I noticed that there is a configuration value {{https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html#taskmanager-jvm-exit-on-oom}} but I don't think it is effective because an OOM can potentially be thrown at non Task-related places where it won't lead to a forceful shutdown.

Would it make sense to have something like {{taskmanager.jvm-exit-on-error}} which always forcefully exit on fatal errors if set to {{true}}?
;;;","09/Jul/20 11:31;mxm;As for the Java version, we are using the following which does look like the ""newest"" 1.8 version:
{noformat}
openjdk version ""1.8.0_252""
OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)
OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)
{noformat};;;","13/Jul/20 17:39;chesnay;[~mxm] Do you know what other threads have been doing when the JVM was stuck? I found a [similar case|https://bugs.eclipse.org/bugs/show_bug.cgi?id=348487] with a thread being stuck within Runtime#halt, due to a non-daemon thread lingering around. Various shutdown hooks we have create non-daemon threads via ShutdownHookUtil#addShutdownHook; maybe one of them got stuck? This would also explain while calling Runtime#halt immediately fixes the issue; the shutdown hooks are never started.;;;","23/Jul/20 17:38;mxm;[~chesnay] Here a stack dump from the stuck task manager: [^stack2-1.txt]. 

Note the common pattern that after an OOM, all the threads get stuck in native methods like {{sun.misc.Unsafe.defineAnonymousClass}} or {{java.lang.Class.getDeclaredConstructors0}}. There does not seem a graceful way to get out of this situation other than immediately halting the JVM.

I would like to introduce an option to always forcefully terminate the task manager in case of unrecoverable errors. That's what we have been doing and it's the only way we have been able to reliably operate Flink in our k8s deployments in case of OOM errors. ;;;","24/Jul/20 09:31;chesnay;Maybe we can store all shutdown hook threads that we create in {{org.apache.flink.util.ShutdownHookUtil#addShutdownHook}}, and trigger an interrupt before calling {{Runtime#halt}} in the backup shutdown hook?
Hell, maybe we could even iterate over all threads and interrupt everything that isn't the halting shutdown hook.

While we could _maybe_ fix shutdown hooks getting stuck by removing all usages of anonymous classes, this seems like quite the effort since we'd have to cover _all_ code-paths...;;;","24/Jul/20 10:36;trohrmann;Thanks for providing the stack trace [~mxm]. I assume that this problem is somewhat reproducible, right? I am asking because it would be helpful to obtain the mixed stack trace via {{jstack -m}} which contains the JVM threads and also the C/C++ threads. Moreover, in order to better understand the problem, it would also be helpful to see the exact command with which you start the JVM process.

I agree that an easy solution would be to make Flink's exit behaviour configurable (per default {{System.exit()}} but if users wish then to use {{Runtime.halt()}}). However, before doing this, I would really like to better understand why the system gets stuck in your case because {{Runtime.halt()}} is actually being called in your case:

{code}
Thread 18575: (state = BLOCKED)
 - java.lang.Shutdown.$$YJP$$halt0(int) @bci=0 (Interpreted frame)
 - java.lang.Shutdown.halt0(int) @bci=14 (Interpreted frame)
 - java.lang.Shutdown.halt(int) @bci=30, line=139 (Interpreted frame)
 - java.lang.Runtime.halt(int) @bci=57, line=275 (Interpreted frame)
 - org.apache.flink.runtime.util.JvmShutdownSafeguard$DelayedTerminator.run() @bci=59, line=86 (Interpreted frame)
 - java.lang.Thread.run() @bci=49, line=748 (Compiled frame)
{code}

The problem seems to be that all Java threads are currently being {{BLOCKED}}. Maybe it's because a safepoint has been reached and there is some problem coming back from it. The following JVM options could help with the debugging:

* {{-XX:+PrintGCApplicationStoppedTime}} – this will actually report pause time for all safepoints (GC related or not). Unfortunately output from this option lacks timestamps, but it is still useful to narrow down problem to safepoints.
* {{-XX:+PrintSafepointStatistics –XX:PrintSafepointStatisticsCount=1}} – this two options will force JVM to report reason and timings after each safepoint (it will be reported to stdout, not GC log).

Reference: http://blog.ragozin.info/2012/10/safepoints-in-hotspot-jvm.html;;;","24/Jul/20 10:40;trohrmann;I'm posting this for reference: There seems to be a problem in the JDK 12 where the JVM might deadlock if it starts a concurrent G1 cycle while shutting down: https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8210706.;;;","24/Jul/20 11:17;chesnay;The GC one can't be fixed in any case though, right?;;;","24/Jul/20 16:10;trohrmann;Jup, if it is a GC bug, then this is a JVM problem. Moreover, this problem should then also occur when only using {{Runtime.halt()}}. But maybe it is more likely to be triggered when running additional shut down hooks which create some lambdas. But let's first try to figure out whether it is really this problem.;;;","24/Jul/20 16:20;thw;I think there are 2 interests involved here:

1. How to ensure that the task manager JVM gets safely terminated on a fatal error, regardless of the behavior of user code and other dependencies
2. Why execution of shutdown hooks can cause the JVM to get stuck

We at Lyft need the option to safely terminate the VMs so that pods are replaced immediately. We are not interested in executing of shutdown hooks when the JVM is already in a problematic state.;;;","27/Jul/20 07:51;trohrmann;I think it is a reasonable request to be able to safely terminate a Flink process in case of fatal errors [~thw]. However, I'm not entirely sure whether we can already jump to the conclusion that it is caused by running the shutdown hooks. If we want to ensure that 1. works, then I believe we should also try to understand why the JVM got stuck in the first place (e.g. it could be caused by a garbage collector which could make {{Runtime.halt}} also susceptible).

What wouldn't hurt is to already centralize the fatal error handling so that one could make it more easily configurable.;;;","28/Jul/20 09:13;trohrmann;In order to point this out because it might have gotten lost in the discussion: Per default Flink will call {{Runtime.halt}} via the {{JvmShutdownSafeguard.DelayedTerminator}} which is also being executed in the stack trace Max posted. According to the JavaDocs of this method, it will also interrupt possibly running shut down hooks.;;;","28/Jul/20 09:24;chesnay;[~trohrmann] I cannot find a mention of interrupts in the JavaDocs of {{Runtime#halt}}.;;;","28/Jul/20 09:42;trohrmann;The JavaDoc says ""If the shutdown sequence has already been initiated then this method does not wait for any running shutdown hooks or finalizers to finish their work."".;;;","28/Jul/20 20:06;mxm;Here is the requested information (without changing the JVM arguments):

{{jstack -F -m 1}}:  [^stack3-mixed.txt] (same error without -F)
{{jstack -F 1}}:  [^stack3.txt]
Command:  [^command.txt] 

Interestingly, the originally reported behavior of hanging in the shutdown hooks is not visible in the stack trace. Still, the problem is not reproducible if {{halt}} will be immediately called on fatal errors without running shutdown hooks.;;;","29/Jul/20 08:36;trohrmann;Thanks [~mxm]. It looks indeed as if the JVM is doing some Garbage collection:

{code}
0x00007ff70aacdf93	_ZN9CodeCache8blobs_doEP15CodeBlobClosure + 0x93
0x00007ff70ac2c82d	_ZN15G1RootProcessor17process_all_rootsEP10OopClosureP10CLDClosureP15CodeBlobClosure + 0xdd
0x00007ff70ac2305c	_ZN11G1MarkSweep17mark_sweep_phase3Ev + 0xbc
0x00007ff70ac231f4	_ZN11G1MarkSweep19invoke_at_safepointEP18ReferenceProcessorb + 0xf4
0x00007ff70ac06d08	_ZN15G1CollectedHeap13do_collectionEbbm.part.286 + 0x558
0x00007ff70ac07735	_ZN15G1CollectedHeap25satisfy_failed_allocationEmhPb + 0x75
0x00007ff70b13b46f	_ZN25VM_G1CollectForAllocation4doitEv + 0x7f
0x00007ff70b13a446	_ZN12VM_Operation8evaluateEv + 0x46
0x00007ff70b1386f5	_ZN8VMThread18evaluate_operationEP12VM_Operation + 0xe5
0x00007ff70b138fce	_ZN8VMThread4loopEv + 0x3be
0x00007ff70b139428	_ZN8VMThread3runEv + 0xb8
0x00007ff70af6ae62	_ZL10java_startP6Thread + 0x102
{code}

It does not seem blocked though from the stack trace. If the problem is reproducible, does it also occur if you change G1 to parallel GC via replacing {{-XX:+UseG1GC}} with {{-XX:+UseParallelGC}}?;;;","29/Jul/20 08:37;sewen;This looks consistent with Till's analysis of GC-related deadlocks. If that is the case, then calling {{Runtime.halt()}} has a lot lower risk of running into that JVM bug, but not zero risk.

I any case, we can try to make the exit behavior configurable:
  - default is exit() with delayed halt()
  - users can configure that halt() is called directly

A bit tricky is that we somehow need to push this configuration option through to many places, like when the UncaughtExceptionHandlers for service threads are created.
;;;","29/Jul/20 08:43;trohrmann;The stack trace is indeed interesting. I cannot see any activity from the JVM to shut itself down. Did the logs contain some informative logging statements?;;;","29/Jul/20 12:03;mxm;{quote}
does it also occur if you change G1 to parallel GC via replacing -XX:+UseG1GC with -XX:+UseParallelGC?
{quote}

[~trohrmann] I'll check!

{quote}Did the logs contain some informative logging statements?
{quote}
[~trohrmann] Frankly, I haven't been able yet to reproduce the freeze when I have the safepoint JVM arguments set you suggested. It could be a Heisenberg effect because the args effect the runtime behavior.
{quote}I any case, we can try to make the exit behavior configurable:
 - default is exit() with delayed halt()
 - users can configure that halt() is called directly

A bit tricky is that we somehow need to push this configuration option through to many places, like when the UncaughtExceptionHandlers for service threads are created.
{quote}

[~sewen] That's exactly what I had in mind. I'll try to see whether we can do this in the least obtrusive way. If I'm not mistaken {{GlobalConfiguration}} is generally accessible everywhere. If we go that route, we would just have to ensure that we only load the configuration value once to construct the exception handler.;;;","29/Jul/20 12:19;trohrmann;I think {{GlobalConfiguration}} is more of utility class to load a {{Configuration}}. It is not recommended to do this at arbitrary places because one might miss overridden configuration values which have been passed in as dynamic properties, for example.

Maybe we could leverage {{Thread.setDefaultUncaughtExceptionHandler}} for this purpose. We only have to make sure that we clear all the explicitly configured {{UncaughtExceptionHandlers}} which are not configurable wrt their exit behaviour.;;;","30/Jul/20 08:22;mxm;[~trohrmann] I think it is safe to say that the problem is not reproducible with G1 being disabled. I've run the test for more than 20 hours. Usually the problem would occurs within 30 minutes, or even faster with more memory pressure.

Global state can be dangerous if it is mutable (which the config shouldn't be). I wasn't aware of properties which modify the config but since those are also global I don't see a reason why we wouldn't be able to resolve them in GlobalConfiguration. 

Preferably, we should resolve the config only once per task manager and initialize the exception handler at the same place, or pass through the config value. I'll have a look.;;;","30/Jul/20 17:34;mxm;I ran another test with the existing {{taskmanager.jvm-exit-on-oom}} option enabled which does a halt in the task thread on OOM errors. So far it looks like the pods get killed as expected. Apparently, the odds are low that the OOM errors occurs outside the Task thread. Still, it is possible that we get the error elsewhere in the TaskManager. I wonder whether we should deprecate this option in favor of a more general {{taskmanager.jvm-exit-on-fatal-error}} which would include OOM errors? The alternative would be to add it as an additional option.;;;","31/Jul/20 07:20;trohrmann;I think the current contract is that we terminate the Flink process when encountering a fatal error. {{taskmanager.jvm-exit-on-oom}} basically says whether an OOM originating from the user code should be considered a fatal error or not. Hence, I am not entirely sure what the meaning of {{taskmanager.jvm-exit-on-fatal-error}} would be.

What I would suggest is to make the exit behaviour configurable. One could introduce {{cluster.clean-up-on-fatal-error}}/{{cluster.fatal-error-behaviour}}/{{cluster.halt-jvm-on-fatal-exit}} to make the behaviour controllable.;;;","31/Jul/20 09:11;mxm;It might be helpful to define ""fatal error"". An OOM error can be a fatal error because we are not guaranteed to be able to recover from it. We currently do not treat OOM errors as fatal, except when it's thrown from the Task thread when {{taskmanager.jvm-exit-on-oom}} is set to true. In this case we do not stick to the regular {{System.exit()}} routine but we issue a {{Runtime.halt()}}. In the recent tests I ran, this behavior prevented the problem reported here. I guess the configuration option has value on its own and we should not touch it for now.

I'll proceed with the solution discussed here, i.e. adding an option to configure forceful exists instead of the default graceful exit.;;;","07/Aug/20 10:30;trohrmann;Fixed via

master: 182be5145b5b45cfedc65abca5dfe723651aa38e
1.11.2: 6782254aadad50d2879a626eb39efeafc93fae13
1.10.2: 8c134867dfab5c330da9458c3db9a54ed324ccab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive get primary key should not throw exception when Invalid method name,FLINK-16500,13290469,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,09/Mar/20 07:25,09/Mar/20 09:31,13/Jul/23 08:07,09/Mar/20 09:31,,,,,,1.11.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"We can use hive 2.2 dependencies to support hive 2.0.0 metastore server.

Consider HiveShim2.2 and 2.1, only primary key is not compatible. We can catch this non-compatible invalid method name exception, and return none.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #11352: [FLINK-16500][hive] Hive get primary key should not throw exception when Invalid method name
URL: https://github.com/apache/flink/pull/11352
 
 
   
   ## What is the purpose of the change
   
   We can use hive 2.2 dependencies to support hive 2.0.0 metastore server.
   
   Consider HiveShim2.2 and 2.1, only primary key is not compatible. We can catch this non-compatible invalid method name exception, and return none.
   
   ## Brief change log
   
   Return none instead of throwing exception when Invalid method name `HiveShimV210.getPrimaryKey`
   
   ## Verifying this change
   
   Manually testing.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/20 07:30;githubbot;600","JingsongLi commented on pull request #11352: [FLINK-16500][hive] Hive get primary key should not throw exception when Invalid method name
URL: https://github.com/apache/flink/pull/11352
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/20 09:31;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 09 09:31:35 UTC 2020,,,,,,,,,,"0|z0cbfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/20 09:31;lzljs3620320;Master: 2d08e102f40ca644ee5c84c38ec0bacd586ee288;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CorrelateTest.testCorrelatePythonTableFunction test fails,FLINK-16477,13290266,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hequn8128,rmetzger,rmetzger,07/Mar/20 07:41,11/Mar/20 13:27,13/Jul/23 08:07,11/Mar/20 13:27,1.11.0,,,,,1.11.0,,,,Table SQL / Planner,Tests,,,,0,test-stability,,,,"Build: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6038&view=logs&j=d47ab8d2-10c7-5d9e-8178-ef06a797a0d8&t=9a1abf5f-7cf4-58c3-bb2a-282a64aebb1f

logs
{code}
2020-03-07T00:35:53.9694141Z [ERROR] Failures: 
2020-03-07T00:35:53.9695913Z [ERROR]   CorrelateTest.testCorrelatePythonTableFunction:127 planBefore expected:<...PythonTableFunction$[8c725e0df55184e548883d4b29709539]($0, $1)], rowType=[...> but was:<...PythonTableFunction$[201b6fc5cb9d565450dafbaaf1a440ab]($0, $1)], rowType=[...>
2020-03-07T00:35:53.9698944Z [ERROR]   CorrelateTest.testCorrelatePythonTableFunction:188 planBefore expected:<...PythonTableFunction$[8c725e0df55184e548883d4b29709539]($0, $1)], rowType=[...> but was:<...PythonTableFunction$[201b6fc5cb9d565450dafbaaf1a440ab]($0, $1)], rowType=[...>
2020-03-07T00:35:53.9700745Z [INFO] 
2020-03-07T00:35:53.9701456Z [ERROR] Tests run: 4329, Failures: 2, Errors: 0, Skipped: 14
{code}",,godfreyhe,hequn8128,rmetzger,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16476,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 11 13:25:47 UTC 2020,,,,,,,,,,"0|z0ca68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/20 09:15;rmetzger;Also happened on the travis nightly run [https://travis-ci.org/apache/flink/jobs/659806496?utm_medium=notification&utm_source=slack];;;","10/Mar/20 06:25;zjwang;Another failure instance [https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6095/logs/213];;;","11/Mar/20 04:48;zjwang;Another instance [https://api.travis-ci.org/v3/job/660629180/log.txt]

[https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6146/logs/362];;;","11/Mar/20 10:01;godfreyhe;cc [~hxbks2ks] [~hequn8128];;;","11/Mar/20 11:47;hequn8128;I think it is the same problem as it is described in FLINK-13318. Will create a hotfix and add the SerialVersionUID for the UDTF. Thank you all for reporting and providing information for the problem.;;;","11/Mar/20 13:25;hequn8128;Fixed in 1.11.0 via 5fa42593b79b57874237a784e8bce1d3ad632b60;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SelectivityEstimatorTest logs LinkageErrors,FLINK-16476,13290264,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,godfreyhe,rmetzger,rmetzger,07/Mar/20 07:37,03/Apr/20 03:54,13/Jul/23 08:07,03/Apr/20 03:54,1.11.0,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,"This is the test run https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6038&view=logs&j=d47ab8d2-10c7-5d9e-8178-ef06a797a0d8&t=9a1abf5f-7cf4-58c3-bb2a-282a64aebb1f

Log output
{code}
2020-03-07T00:35:20.1270791Z [INFO] Running org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest
2020-03-07T00:35:21.6473057Z [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.408 s - in org.apache.flink.table.planner.plan.utils.FlinkRexUtilTest
2020-03-07T00:35:21.6541713Z [INFO] Running org.apache.flink.table.planner.plan.metadata.FlinkRelMdNonCumulativeCostTest
2020-03-07T00:35:21.7294613Z [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.073 s - in org.apache.flink.table.planner.plan.metadata.FlinkRelMdNonCumulativeCostTest
2020-03-07T00:35:21.7309958Z [INFO] Running org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest
2020-03-07T00:35:23.7443246Z ScriptEngineManager providers.next(): javax.script.ScriptEngineFactory: Provider jdk.nashorn.api.scripting.NashornScriptEngineFactory not a subtype
2020-03-07T00:35:23.8260013Z 2020-03-07 00:35:23,819 main ERROR Could not reconfigure JMX java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/javassist/JavassistMockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
2020-03-07T00:35:23.8262329Z 	at java.lang.ClassLoader.defineClass1(Native Method)
2020-03-07T00:35:23.8263241Z 	at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
2020-03-07T00:35:23.8264629Z 	at org.powermock.core.classloader.javassist.JavassistMockClassLoader.loadUnmockedClass(JavassistMockClassLoader.java:90)
2020-03-07T00:35:23.8266241Z 	at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:104)
2020-03-07T00:35:23.8267808Z 	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)
2020-03-07T00:35:23.8269485Z 	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)
2020-03-07T00:35:23.8270900Z 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
2020-03-07T00:35:23.8272000Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
2020-03-07T00:35:23.8273779Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
2020-03-07T00:35:23.8275087Z 	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:165)
2020-03-07T00:35:23.8276515Z 	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:141)
2020-03-07T00:35:23.8278036Z 	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:590)
2020-03-07T00:35:23.8279741Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
2020-03-07T00:35:23.8281190Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
2020-03-07T00:35:23.8282440Z 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
2020-03-07T00:35:23.8283717Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
2020-03-07T00:35:23.8285186Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
2020-03-07T00:35:23.8286575Z 	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
2020-03-07T00:35:23.8287933Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
2020-03-07T00:35:23.8289393Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
2020-03-07T00:35:23.8290816Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
2020-03-07T00:35:23.8292179Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
2020-03-07T00:35:23.8293304Z 	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
2020-03-07T00:35:23.8294485Z 	at org.apache.calcite.util.trace.CalciteTrace.getParserTracer(CalciteTrace.java:111)
2020-03-07T00:35:23.8295692Z 	at org.apache.calcite.util.trace.CalciteTrace.<clinit>(CalciteTrace.java:56)
2020-03-07T00:35:23.8296925Z 	at org.apache.calcite.sql.parser.SqlParserUtil.<clinit>(SqlParserUtil.java:73)
2020-03-07T00:35:23.8298190Z 	at org.apache.calcite.sql.SqlCollation.<init>(SqlCollation.java:86)
2020-03-07T00:35:23.8299369Z 	at org.apache.calcite.sql.SqlCollation.<init>(SqlCollation.java:106)
2020-03-07T00:35:23.8300623Z 	at org.apache.calcite.sql.SqlCollation.<clinit>(SqlCollation.java:36)
2020-03-07T00:35:23.8301809Z 	at org.apache.calcite.sql.type.SqlTypeUtil.addCharsetAndCollation(SqlTypeUtil.java:1109)
2020-03-07T00:35:23.8303177Z 	at org.apache.calcite.sql.type.SqlTypeFactoryImpl.createSqlType(SqlTypeFactoryImpl.java:70)
2020-03-07T00:35:23.8304600Z 	at org.apache.flink.table.planner.calcite.FlinkTypeFactory.createSqlType(FlinkTypeFactory.scala:245)
2020-03-07T00:35:23.8305836Z 	at org.apache.calcite.rex.RexBuilder.<init>(RexBuilder.java:120)
2020-03-07T00:35:23.8307197Z 	at org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest.<init>(SelectivityEstimatorTest.scala:69)
2020-03-07T00:35:23.8308550Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-03-07T00:35:23.8309967Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-03-07T00:35:23.8311476Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-03-07T00:35:23.8312736Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-03-07T00:35:23.8314266Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTestInstance(PowerMockJUnit44RunnerDelegateImpl.java:197)
2020-03-07T00:35:23.8316099Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTest(PowerMockJUnit44RunnerDelegateImpl.java:182)
2020-03-07T00:35:23.8318030Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:204)
2020-03-07T00:35:23.8320066Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:160)
2020-03-07T00:35:23.8321832Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:134)
2020-03-07T00:35:23.8323341Z 	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:34)
2020-03-07T00:35:23.8324508Z 	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:44)
2020-03-07T00:35:23.8326007Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:136)
2020-03-07T00:35:23.8327909Z 	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:117)
2020-03-07T00:35:23.8329637Z 	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:57)
2020-03-07T00:35:23.8331221Z 	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
2020-03-07T00:35:23.8332426Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-03-07T00:35:23.8333876Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-03-07T00:35:23.8335252Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-03-07T00:35:23.8336523Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-03-07T00:35:23.8337998Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-03-07T00:35:23.8339456Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-03-07T00:35:23.8340858Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-03-07T00:35:23.8342096Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-03-07T00:35:23.8342779Z 
2020-03-07T00:35:24.9610362Z ScriptEngineManager providers.next(): javax.script.ScriptEngineFactory: Provider jdk.nashorn.api.scripting.NashornScriptEngineFactory not a subtype
2020-03-07T00:35:25.0466199Z 2020-03-07 00:35:25,039 main ERROR Could not reconfigure JMX java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/javassist/JavassistMockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
2020-03-07T00:35:25.0469113Z 	at java.lang.ClassLoader.defineClass1(Native Method)
2020-03-07T00:35:25.0470660Z 	at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
2020-03-07T00:35:25.0472271Z 	at org.powermock.core.classloader.javassist.JavassistMockClassLoader.loadUnmockedClass(JavassistMockClassLoader.java:90)
2020-03-07T00:35:25.0474050Z 	at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:104)
2020-03-07T00:35:25.0475860Z 	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)
2020-03-07T00:35:25.0477817Z 	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)
2020-03-07T00:35:25.0479429Z 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
2020-03-07T00:35:25.0480937Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
2020-03-07T00:35:25.0482406Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
2020-03-07T00:35:25.0484057Z 	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:165)
2020-03-07T00:35:25.0485643Z 	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:141)
2020-03-07T00:35:25.0487266Z 	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:590)
2020-03-07T00:35:25.0488896Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
2020-03-07T00:35:25.0490626Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
2020-03-07T00:35:25.0492120Z 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
2020-03-07T00:35:25.0493648Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
2020-03-07T00:35:25.0495381Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
2020-03-07T00:35:25.0496936Z 	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
2020-03-07T00:35:25.0498835Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
2020-03-07T00:35:25.0500729Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
2020-03-07T00:35:25.0502293Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
2020-03-07T00:35:25.0504005Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
2020-03-07T00:35:25.0505416Z 	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
2020-03-07T00:35:25.0506942Z 	at org.apache.calcite.util.trace.CalciteTrace.getParserTracer(CalciteTrace.java:111)
2020-03-07T00:35:25.0508534Z 	at org.apache.calcite.util.trace.CalciteTrace.<clinit>(CalciteTrace.java:56)
2020-03-07T00:35:25.0510219Z 	at org.apache.calcite.sql.parser.SqlParserUtil.<clinit>(SqlParserUtil.java:73)
2020-03-07T00:35:25.0511651Z 	at org.apache.calcite.sql.SqlCollation.<init>(SqlCollation.java:86)
2020-03-07T00:35:25.0512954Z 	at org.apache.calcite.sql.SqlCollation.<init>(SqlCollation.java:106)
2020-03-07T00:35:25.0514306Z 	at org.apache.calcite.sql.SqlCollation.<clinit>(SqlCollation.java:36)
2020-03-07T00:35:25.0515783Z 	at org.apache.calcite.sql.type.SqlTypeUtil.addCharsetAndCollation(SqlTypeUtil.java:1109)
2020-03-07T00:35:25.0517340Z 	at org.apache.calcite.sql.type.SqlTypeFactoryImpl.createSqlType(SqlTypeFactoryImpl.java:70)
2020-03-07T00:35:25.0519244Z 	at org.apache.flink.table.planner.calcite.FlinkTypeFactory.createSqlType(FlinkTypeFactory.scala:245)
2020-03-07T00:35:25.0520861Z 	at org.apache.calcite.rex.RexBuilder.<init>(RexBuilder.java:120)
2020-03-07T00:35:25.0522555Z 	at org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.<init>(AggCallSelectivityEstimatorTest.scala:68)
2020-03-07T00:35:25.0524186Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-03-07T00:35:25.0525546Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-03-07T00:35:25.0527263Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-03-07T00:35:25.0528827Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-03-07T00:35:25.0530888Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTestInstance(PowerMockJUnit44RunnerDelegateImpl.java:197)
2020-03-07T00:35:25.0533089Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTest(PowerMockJUnit44RunnerDelegateImpl.java:182)
2020-03-07T00:35:25.0535236Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:204)
2020-03-07T00:35:25.0537550Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:160)
2020-03-07T00:35:25.0539721Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:134)
2020-03-07T00:35:25.0541678Z 	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:34)
2020-03-07T00:35:25.0543104Z 	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:44)
2020-03-07T00:35:25.0544778Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:136)
2020-03-07T00:35:25.0546859Z 	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:117)
2020-03-07T00:35:25.0548887Z 	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:57)
2020-03-07T00:35:25.0550897Z 	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
2020-03-07T00:35:25.0552395Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-03-07T00:35:25.0553911Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-03-07T00:35:25.0555763Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-03-07T00:35:25.0557261Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-03-07T00:35:25.0559014Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-03-07T00:35:25.0561000Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-03-07T00:35:25.0562494Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-03-07T00:35:25.0564133Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-03-07T00:35:25.0564938Z 
2020-03-07T00:35:26.3527172Z [INFO] Tests run: 28, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.223 s - in org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest
2020-03-07T00:35:26.3542708Z [INFO] Running org.apache.flink.table.planner.plan.metadata.FlinkRelMdDistinctRowCountTest
2020-03-07T00:35:26.4374479Z [INFO] Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.08 s - in org.apache.flink.table.planner.plan.metadata.FlinkRelMdDistinctRowCountTest
2020-03-07T00:35:26.4385241Z [INFO] Running org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest
{code}

Also later
{code}
2020-03-07T00:35:42.1007963Z [INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.047 s - in org.apache.flink.table.planner.operations.SqlToOperationConverterTest
2020-03-07T00:35:42.1071931Z 2020-03-07 00:35:42,105 pool-5-thread-1 ERROR Unable to unregister MBeans java.lang.LinkageError: javax/management/MBeanServer
2020-03-07T00:35:42.1073900Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
2020-03-07T00:35:42.1075391Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
2020-03-07T00:35:42.1076902Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:249)
2020-03-07T00:35:42.1078469Z 	at org.apache.logging.log4j.core.LoggerContext.stop(LoggerContext.java:362)
2020-03-07T00:35:42.1080437Z 	at org.apache.logging.log4j.core.LoggerContext$1.run(LoggerContext.java:303)
2020-03-07T00:35:42.1082200Z 	at org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry$RegisteredCancellable.run(DefaultShutdownCallbackRegistry.java:109)
2020-03-07T00:35:42.1084172Z 	at org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry.run(DefaultShutdownCallbackRegistry.java:74)
2020-03-07T00:35:42.1085579Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-07T00:35:42.1086173Z 
2020-03-07T00:35:53.4173788Z [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.509 s - in org.apache.flink.table.planner.codegen.SortCodeGeneratorTest
2020-03-07T00:35:53.4237819Z 2020-03-07 00:35:53,422 pool-2-thread-1 ERROR Unable to unregister MBeans java.lang.LinkageError: javax/management/MBeanServer
2020-03-07T00:35:53.4239501Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
2020-03-07T00:35:53.4240709Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
2020-03-07T00:35:53.4241982Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:249)
2020-03-07T00:35:53.4243138Z 	at org.apache.logging.log4j.core.LoggerContext.stop(LoggerContext.java:362)
2020-03-07T00:35:53.4244277Z 	at org.apache.logging.log4j.core.LoggerContext$1.run(LoggerContext.java:303)
2020-03-07T00:35:53.4245728Z 	at org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry$RegisteredCancellable.run(DefaultShutdownCallbackRegistry.java:109)
2020-03-07T00:35:53.4247407Z 	at org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry.run(DefaultShutdownCallbackRegistry.java:74)
2020-03-07T00:35:53.4248633Z 	at java.lang.Thread.run(Thread.java:748)
{code}

These issues were not reported as the root cause for the test failures.",,godfreyhe,jark,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,"godfreyhe commented on pull request #11357: [FLINK-16476] [table-planner-blink] Remove PowerMockito to avoid LinkageError in SelectivityEstimatorTest
URL: https://github.com/apache/flink/pull/11357
 
 
   
   
   ## What is the purpose of the change
   
   *Remove PowerMockito to avoid LinkageError in SelectivityEstimatorTest*
   
   
   ## Brief change log
   
     - *Remove PowerMockito to avoid LinkageError in SelectivityEstimatorTest*
   
   
   ## Verifying this change
   
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Mar/20 02:34;githubbot;600","wuchong commented on pull request #11357: [FLINK-16476] [table-planner-blink] Remove PowerMockito to avoid LinkageError in SelectivityEstimatorTest
URL: https://github.com/apache/flink/pull/11357
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Apr/20 03:54;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16519,,FLINK-16477,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 03:54:56 UTC 2020,,,,,,,,,,"0|z0ca5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/20 02:47;jark;cc [~godfreyhe], can {{@PowerMockIgnore(""javax.management.*"")}} annotation fix this? 

https://stackoverflow.com/questions/41609436/powermock-after-log4j2-3-upgrade-could-not-reconfigure-jmx-java-lang-linkageerro;;;","09/Mar/20 21:15;rmetzger;This issue actually applies to other tests as well:
https://travis-ci.org/apache/flink/jobs/660152153?utm_medium=notification&utm_source=slack
{code:java}
15:52:14.291 [INFO] Running org.apache.flink.runtime.checkpoint.CompletedCheckpointTest
2020-03-09 15:52:14,550 main ERROR Could not reconfigure JMX java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/javassist/JavassistMockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
	at org.powermock.core.classloader.javassist.JavassistMockClassLoader.loadUnmockedClass(JavassistMockClassLoader.java:90)
	at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:104)
	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)
	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:165)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:141)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:590)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:349)
	at org.apache.flink.util.TestLogger.<init>(TestLogger.java:36)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.<init>(CheckpointCoordinatorFailureTest.java:55)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTestInstance(PowerMockJUnit44RunnerDelegateImpl.java:197)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTest(PowerMockJUnit44RunnerDelegateImpl.java:182)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:204)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:160)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:134)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:34)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:44)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:136)
	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:117)
	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:57)
	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
15:52:16.969 [INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.676 s - in org.apache.flink.runtime.checkpoint.CompletedCheckpointTest
ERROR StatusLogger Unrecognized format specifier [d]
ERROR StatusLogger Unrecognized conversion specifier [d] starting at position 16 in conversion pattern.
ERROR StatusLogger Unrecognized format specifier [thread]
ERROR StatusLogger Unrecognized conversion specifier [thread] starting at position 25 in conversion pattern.
ERROR StatusLogger Unrecognized format specifier [level]
ERROR StatusLogger Unrecognized conversion specifier [level] starting at position 35 in conversion pattern.
ERROR StatusLogger Unrecognized format specifier [logger]
ERROR StatusLogger Unrecognized conversion specifier [logger] starting at position 47 in conversion pattern.
ERROR StatusLogger Unrecognized format specifier [msg]
ERROR StatusLogger Unrecognized conversion specifier [msg] starting at position 54 in conversion pattern.
ERROR StatusLogger Unrecognized format specifier [n]
ERROR StatusLogger Unrecognized conversion specifier [n] starting at position 56 in conversion pattern.
15:52:17.890 [INFO] Running org.apache.flink.runtime.checkpoint.CheckpointTypeTest
 {code}

[~chesnay] could this related to the log4j2 migration?

;;;","10/Mar/20 01:15;godfreyhe;[~rmetzger] thanks for reporting this.
 [~jark] yes, the annotation you mentioned can fix this. It's better add {{javax.script.*}} into the annotation to solve {{javax.script.ScriptEngineFactory: Provider jdk.nashorn.api.scripting.NashornScriptEngineFactory not a subtype}};;;","10/Mar/20 02:19;godfreyhe;I think it's better to remove {{PowerMock}}, I would like to take this ticket.;;;","10/Mar/20 02:52;jark;But it seems that {{CompletedCheckpointTest}} doesn't use {{PowerMock}}.;;;","10/Mar/20 02:59;godfreyhe;The error message is reported by {{CheckpointCoordinatorFailureTest}} not {{CompletedCheckpointTest}};;;","10/Mar/20 03:05;godfreyhe;I create another issue (https://issues.apache.org/jira/browse/FLINK-16519) to report LinkageErrors in {{CheckpointCoordinatorFailureTest}};;;","10/Mar/20 03:11;jark;Good catch [~godfreyhe]! 

{{org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest}} also uses it, if we want to remove {{PowerMock}}, we should also take this one into account. ;;;","10/Mar/20 06:58;godfreyhe;agree with you [~jark];;;","18/Mar/20 15:33;pnowojski;Is this the same failure/issue?

{noformat}
09:42:17.516 [INFO] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.333 s - in org.apache.flink.table.api.stream.ExplainTest
2020-03-17 09:42:17,551 pool-7-thread-1 ERROR Unable to unregister MBeans java.lang.LinkageError: javax/management/MBeanServer
	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:249)
	at org.apache.logging.log4j.core.LoggerContext.stop(LoggerContext.java:362)
	at org.apache.logging.log4j.core.LoggerContext$1.run(LoggerContext.java:303)
	at org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry$RegisteredCancellable.run(DefaultShutdownCallbackRegistry.java:109)
	at org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry.run(DefaultShutdownCallbackRegistry.java:74)
	at java.lang.Thread.run(Thread.java:748)
{noformat}

https://api.travis-ci.org/v3/job/663408376/log.txt

and another one?

https://api.travis-ci.org/v3/job/663801328/log.txt;;;","01/Apr/20 12:41;rmetzger;The error still persists: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6912&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=294c2388-20e6-57a2-5721-91db544b1e69

[~godfreyhe] what is the status of addressing this?;;;","01/Apr/20 12:46;godfreyhe;[~rmetzger] No one reviews the pr, I will ping sb to help to review;;;","02/Apr/20 02:28;jark;Maybe this is related to FLINK-16906? Maybe the suggested ""disable the JMX integration of log4j2"" can also fix this problem. ;;;","03/Apr/20 03:54;jark;Fixed in mater (1.11.0) with 00526eba0f37f8869e62f41f43a40906e4169790 and b93c75b31e8ce17f57c8311f591a70316180df04;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MemorySizeTest#testToHumanReadableString() is not portable,FLINK-16467,13290139,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,michael-o,michael-o,06/Mar/20 17:11,09/Mar/20 09:35,13/Jul/23 08:07,09/Mar/20 09:34,1.10.0,,,,,1.10.1,1.11.0,,,API / Core,,,,,0,pull-request-available,,,,"Runing this test from master gives me:

{noformat}
[ERROR] Failures:
[ERROR]   MemorySizeTest.testToHumanReadableString:242
Expected: is ""1.001kb (1025 bytes)""
     but: was ""1,001kb (1025 bytes)""
[INFO]
{noformat}

The test is not locale-portable.","$ mvn -v
Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T20:33:14+02:00)
Maven home: /usr/local/apache-maven-3.5.4
Java version: 1.8.0_242, vendor: Oracle Corporation, runtime: /usr/local/openjdk8/jre
Default locale: de_DE, platform encoding: UTF-8
OS name: ""freebsd"", version: ""12.1-stable"", arch: ""amd64"", family: ""unix""
",michael-o,xtsong,zhuzh,,,,,,,,,,,,,,,,,,,"xintongsong commented on pull request #11348: [FLINK-16467][core] Fix MemorySizeTest#testToHumanReadableString locale problem.
URL: https://github.com/apache/flink/pull/11348
 
 
   ## What is the purpose of the change
   
   This PR fix the problem that `MemorySize#toHumanReadableString` generate string in different formats depending on the Locale, which causes test failures.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/20 04:18;githubbot;600","zhuzhurk commented on pull request #11348: [FLINK-16467][core] Fix MemorySizeTest#testToHumanReadableString locale problem.
URL: https://github.com/apache/flink/pull/11348
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/20 09:32;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 09 09:35:01 UTC 2020,,,,,,,,,,"0|z0c9e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/20 03:47;xtsong;[~michael-o] Thanks for reporting this issue.

I think this is probably due to the locale issue. We use {{String.format}} for generating the human readable string for {{MemorySize}}, and I find this [post|https://stackoverflow.com/questions/5236056/force-point-as-decimal-separator-in-java] that describes the problem and how to fix it.;;;","09/Mar/20 03:56;zhuzh;Thanks [~michael-o] for reporting the issue.
And thanks [~xintongsong] for looking into it. I have assigned the ticket to you. Feel free to open a PR to fix it.;;;","09/Mar/20 04:21;xtsong;[~zhuzh], thanks for assigning me, I've opened a PR. Please take a look at your convenience.

[~michael-o], could you please also help confirm whether the PR fix the problem in your environment? Thanks.;;;","09/Mar/20 07:44;michael-o;I will try later this day and let you know.;;;","09/Mar/20 09:19;michael-o;The patch works for me.;;;","09/Mar/20 09:34;zhuzh;Fixed via:
master: cac40c6b2c4aa2b397b6df562aa23bd1f18dd3b0
release-1.10: 77c9fe18a6ca093c7698fe5c23effd3e4357adcd;;;","09/Mar/20 09:35;xtsong;Thanks for the input, [~michael-o].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
result-mode tableau may  shift when content contains Chinese String in SQL CLI ,FLINK-16464,13290062,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,leonard,leonard,leonard,06/Mar/20 13:00,13/Mar/20 07:13,13/Jul/23 08:07,13/Mar/20 07:13,1.11.0,,,,,1.11.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,," 

result-mode tableau may shift when column content contains Chinese String in SQL CLI  as following: 
{code:java}
Flink SQL> select * from user_ino;
+-----+----------------------+--------+----------------------+
| +/- |            user_name | is_new |          content_col |
+-----+----------------------+--------+----------------------+
|   + |                  sam |   true |              content |
|   + |                   中文名 |  false |              content |
|   + |              leonard |   true |              content |
{code}
We can calculate column widths with UDTF-8 format bytes not length of string to avoid this.",,godfreyhe,leonard,,,,,,,,,,,,,,,,,,,,"leonardBang commented on pull request #11334: [FLINK-16464][sql-client]result-mode tableau may shift when content contains Chinese String in SQL CLI
URL: https://github.com/apache/flink/pull/11334
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   * This pull request  fix result-mode tableau may shift when content contains Chinese String. *
   
   
   ## Brief change log
   
     - *update file org/apache/flink/table/client/cli/CliTableauResultView.java*
   
   
   ## Verifying this change
   
    Add test in CliTableauResultViewTest.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/20 15:53;githubbot;600","KurtYoung commented on pull request #11334: [FLINK-16464][sql-client]result-mode tableau may shift when content contains Chinese String in SQL CLI
URL: https://github.com/apache/flink/pull/11334
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Mar/20 07:12;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 07:13:07 UTC 2020,,,,,,,,,,"0|z0c8ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/20 15:55;leonard;Hi，[~ykt836] 
I open a PR to fix this, Could you assign this issue to me?
and could you help review if you have time？ :);;;","13/Mar/20 07:13;ykt836;fixed in master: 22915d051d614249ea2d2eae4a2366b5533136d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CodeGenUtils generates code that has two semicolons for GroupingWindowAggsHandler in blink ,FLINK-16463,13290059,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,hehuiyuan,hehuiyuan,hehuiyuan,06/Mar/20 12:44,08/Jul/20 18:31,13/Jul/23 08:07,09/Mar/20 07:33,,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,,,,,"!image-2020-03-06-20-43-20-300.png|width=452,height=297!

 

!image-2020-03-06-20-44-16-446.png|width=513,height=282!",,hehuiyuan,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/20 12:43;hehuiyuan;image-2020-03-06-20-43-20-300.png;https://issues.apache.org/jira/secure/attachment/12995850/image-2020-03-06-20-43-20-300.png","06/Mar/20 12:44;hehuiyuan;image-2020-03-06-20-44-16-446.png;https://issues.apache.org/jira/secure/attachment/12995849/image-2020-03-06-20-44-16-446.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 09 07:33:57 UTC 2020,,,,,,,,,,"0|z0c8w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/20 14:20;ykt836;[~hehuiyuan] would you like to fix this?;;;","07/Mar/20 03:55;hehuiyuan;Hi [~ykt836] , I'd love to fix it;;;","07/Mar/20 04:02;ykt836;[~hehuiyuan] assigned to you.;;;","09/Mar/20 07:33;jark;Fixed in master (1.11.0): e9fd832e1452631f25ff24e1f23ab53763c914dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Bindings with SLF4J,FLINK-16461,13290041,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,,nkruber,nkruber,06/Mar/20 11:18,04/Aug/21 10:49,13/Jul/23 08:07,04/Aug/21 10:49,1.11.0,,,,,,,,,Benchmarks,,,,,0,,,,,"Running benchmarks currently produces this warning:

{code}
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/nico/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.12.1/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/nico/.m2/repository/org/slf4j/slf4j-log4j12/1.7.7/slf4j-log4j12-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
{code}

The reason for this is this from the dependency tree:
{code}
org.apache.flink.benchmark:flink-hackathon-benchmarks:jar:0.1
+- org.apache.flink:flink-test-utils-junit:jar:1.11-SNAPSHOT:compile
|  +- org.apache.logging.log4j:log4j-slf4j-impl:jar:2.12.1:compile
+- org.slf4j:slf4j-log4j12:jar:1.7.7:compile
{code}",,knaufk,nkruber,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23393,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 29 07:28:15 UTC 2021,,,,,,,,,,"0|z0c8s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/20 13:45;roman;changing priority to minor;;;","14/Apr/21 22:44;flink-jira-bot;This issue and all of its Sub-Tasks have not been updated for 180 days. So, it has been labeled ""stale-minor"". If you are still affected by this bug or are still interested in this issue, please give an update and remove the label. In 7 days the issue will be closed automatically.;;;","22/Apr/21 22:55;flink-jira-bot;This issue has been labeled ""stale-minor"" for 7 days. It is closed now. If you are still affected by this or would like to raise the priority of this ticket please re-open, removing the label ""auto-closed"" and raise the ticket priority accordingly.;;;","29/Jul/21 07:28;knaufk;Re-opening in accordance with https://issues.apache.org/jira/browse/FLINK-23206.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Heavy deployment end-to-end test fails with OutOfMemoryError on JDK11,FLINK-16456,13289994,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,06/Mar/20 08:52,11/Mar/20 15:02,13/Jul/23 08:07,11/Mar/20 15:02,,,,,,,,,,Tests,,,,,0,pull-request-available,test-stability,,,"Example: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5990&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=7d4f7375-52df-5ce0-457f-b2ffbb2289a4

Full logs:
{code}
2020-03-06T02:06:40.0854061Z ==============================================================================
2020-03-06T02:06:40.0854897Z Running 'Heavy deployment end-to-end test'
2020-03-06T02:06:40.0855434Z ==============================================================================
2020-03-06T02:06:40.0879460Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-40087556829
2020-03-06T02:06:42.9071078Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-03-06T02:06:43.0165322Z Starting cluster.
2020-03-06T02:06:43.3033440Z Starting standalonesession daemon on host fv-az655.
2020-03-06T02:06:47.4295013Z Starting taskexecutor daemon on host fv-az655.
2020-03-06T02:06:47.5642249Z Waiting for Dispatcher REST endpoint to come up...
2020-03-06T02:06:48.9222210Z Waiting for Dispatcher REST endpoint to come up...
2020-03-06T02:06:49.9999171Z Waiting for Dispatcher REST endpoint to come up...
2020-03-06T02:06:51.0875110Z Waiting for Dispatcher REST endpoint to come up...
2020-03-06T02:06:52.1357983Z Dispatcher REST endpoint is up.
2020-03-06T02:06:52.1359104Z Start 4 more task managers
2020-03-06T02:06:54.0539574Z [INFO] 1 instance(s) of taskexecutor are already running on fv-az655.
2020-03-06T02:06:54.0555611Z Starting taskexecutor daemon on host fv-az655.
2020-03-06T02:06:58.3302348Z [INFO] 2 instance(s) of taskexecutor are already running on fv-az655.
2020-03-06T02:06:58.3332574Z Starting taskexecutor daemon on host fv-az655.
2020-03-06T02:07:03.1996934Z [INFO] 3 instance(s) of taskexecutor are already running on fv-az655.
2020-03-06T02:07:03.2008989Z Starting taskexecutor daemon on host fv-az655.
2020-03-06T02:07:08.1889175Z [INFO] 4 instance(s) of taskexecutor are already running on fv-az655.
2020-03-06T02:07:08.1924895Z Starting taskexecutor daemon on host fv-az655.
2020-03-06T02:07:11.4246015Z WARNING: An illegal reflective access operation has occurred
2020-03-06T02:07:11.4248047Z WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar) to method sun.security.krb5.Config.getInstance()
2020-03-06T02:07:11.4249610Z WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
2020-03-06T02:07:11.4250537Z WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
2020-03-06T02:07:11.4251120Z WARNING: All illegal access operations will be denied in a future release
2020-03-06T02:07:19.2044003Z Job has been submitted with JobID 0e8275e9837d3d8c6af63f96de5a412d
2020-03-06T02:09:13.1583035Z 
2020-03-06T02:09:13.1588601Z ------------------------------------------------------------
2020-03-06T02:09:13.1591628Z  The program finished with the following exception:
2020-03-06T02:09:13.1593934Z 
2020-03-06T02:09:13.1598865Z org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 0e8275e9837d3d8c6af63f96de5a412d)
2020-03-06T02:09:13.1602462Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
2020-03-06T02:09:13.1618004Z 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
2020-03-06T02:09:13.1621120Z 	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138)
2020-03-06T02:09:13.1624030Z 	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664)
2020-03-06T02:09:13.1627101Z 	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
2020-03-06T02:09:13.1633089Z 	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895)
2020-03-06T02:09:13.1636423Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)
2020-03-06T02:09:13.1639232Z 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2020-03-06T02:09:13.1642291Z 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2020-03-06T02:09:13.1645255Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
2020-03-06T02:09:13.1648026Z 	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
2020-03-06T02:09:13.1650561Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)
2020-03-06T02:09:13.1654121Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 0e8275e9837d3d8c6af63f96de5a412d)
2020-03-06T02:09:13.1657096Z 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
2020-03-06T02:09:13.1661419Z 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
2020-03-06T02:09:13.1663973Z 	at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:83)
2020-03-06T02:09:13.1666679Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1619)
2020-03-06T02:09:13.1670258Z 	at org.apache.flink.deployment.HeavyDeploymentStressTestProgram.main(HeavyDeploymentStressTestProgram.java:70)
2020-03-06T02:09:13.1673776Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-06T02:09:13.1676775Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-06T02:09:13.1680182Z 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-06T02:09:13.1683585Z 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2020-03-06T02:09:13.1687024Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
2020-03-06T02:09:13.1690459Z 	... 11 more
2020-03-06T02:09:13.1694521Z Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 0e8275e9837d3d8c6af63f96de5a412d)
2020-03-06T02:09:13.1699478Z 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$getJobExecutionResult$6(ClusterClientJobClientAdapter.java:112)
2020-03-06T02:09:13.1705394Z 	at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
2020-03-06T02:09:13.1710044Z 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
2020-03-06T02:09:13.1713603Z 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
2020-03-06T02:09:13.1741956Z 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$21(RestClusterClient.java:565)
2020-03-06T02:09:13.1778690Z 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
2020-03-06T02:09:13.1780421Z 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
2020-03-06T02:09:13.1781249Z 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
2020-03-06T02:09:13.1782242Z 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
2020-03-06T02:09:13.1782931Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:291)
2020-03-06T02:09:13.1783771Z 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
2020-03-06T02:09:13.1784480Z 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
2020-03-06T02:09:13.1788206Z 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
2020-03-06T02:09:13.1789279Z 	at java.base/java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:610)
2020-03-06T02:09:13.1794328Z 	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1085)
2020-03-06T02:09:13.1795887Z 	at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478)
2020-03-06T02:09:13.1799443Z 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2020-03-06T02:09:13.1800381Z 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2020-03-06T02:09:13.1801261Z 	at java.base/java.lang.Thread.run(Thread.java:834)
2020-03-06T02:09:13.1802541Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-03-06T02:09:13.1803488Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-03-06T02:09:13.1805884Z 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$getJobExecutionResult$6(ClusterClientJobClientAdapter.java:110)
2020-03-06T02:09:13.1816696Z 	... 18 more
2020-03-06T02:09:13.1817310Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=3, backoffTimeMS=0)
2020-03-06T02:09:13.1818478Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110)
2020-03-06T02:09:13.1819521Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76)
2020-03-06T02:09:13.1820834Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:190)
2020-03-06T02:09:13.1821764Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:184)
2020-03-06T02:09:13.1822544Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:178)
2020-03-06T02:09:13.1823296Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:505)
2020-03-06T02:09:13.1824014Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:383)
2020-03-06T02:09:13.1824823Z 	at jdk.internal.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
2020-03-06T02:09:13.1831152Z 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-06T02:09:13.1831904Z 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2020-03-06T02:09:13.1832686Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
2020-03-06T02:09:13.1835462Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
2020-03-06T02:09:13.1836161Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-03-06T02:09:13.1843413Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-03-06T02:09:13.1856462Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-03-06T02:09:13.1859425Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-03-06T02:09:13.1862822Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-03-06T02:09:13.1864340Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-03-06T02:09:13.1865122Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-03-06T02:09:13.1865885Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-03-06T02:09:13.1866597Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-03-06T02:09:13.1870122Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-03-06T02:09:13.1874876Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-03-06T02:09:13.1875638Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-03-06T02:09:13.1876331Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-03-06T02:09:13.1877301Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-03-06T02:09:13.1881571Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-03-06T02:09:13.1882249Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-03-06T02:09:13.1889904Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-03-06T02:09:13.1894764Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-03-06T02:09:13.1921746Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-03-06T02:09:13.1936050Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-03-06T02:09:13.1940032Z Caused by: org.apache.flink.runtime.io.network.netty.exception.LocalTransportException: Sending the partition request to '10.1.0.4/10.1.0.4:45911' failed.
2020-03-06T02:09:13.1940803Z 	at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient$1.operationComplete(NettyPartitionRequestClient.java:124)
2020-03-06T02:09:13.1941473Z 	at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient$1.operationComplete(NettyPartitionRequestClient.java:115)
2020-03-06T02:09:13.1964619Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:500)
2020-03-06T02:09:13.1965592Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:474)
2020-03-06T02:09:13.1966540Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:413)
2020-03-06T02:09:13.1967310Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:538)
2020-03-06T02:09:13.1967893Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:531)
2020-03-06T02:09:13.1968496Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:111)
2020-03-06T02:09:13.2061660Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.PromiseNotificationUtil.tryFailure(PromiseNotificationUtil.java:64)
2020-03-06T02:09:13.2062676Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.notifyOutboundHandlerException(AbstractChannelHandlerContext.java:818)
2020-03-06T02:09:13.2063884Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:718)
2020-03-06T02:09:13.2064648Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:708)
2020-03-06T02:09:13.2065474Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.access$1700(AbstractChannelHandlerContext.java:56)
2020-03-06T02:09:13.2066255Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1102)
2020-03-06T02:09:13.2067091Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1149)
2020-03-06T02:09:13.2068087Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1073)
2020-03-06T02:09:13.2068872Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
2020-03-06T02:09:13.2069624Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:416)
2020-03-06T02:09:13.2070306Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:331)
2020-03-06T02:09:13.2070995Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
2020-03-06T02:09:13.2071682Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2020-03-06T02:09:13.2074112Z 	at java.base/java.lang.Thread.run(Thread.java:834)
2020-03-06T02:09:13.2080865Z Caused by: java.io.IOException: Error while serializing message: PartitionRequest(e31c6619b12f95de209b79116518f862@d8284ea36fe919a6f045c7e3c9d6f8a8:0:2)
2020-03-06T02:09:13.2084207Z 	at org.apache.flink.runtime.io.network.netty.NettyMessage$NettyMessageEncoder.write(NettyMessage.java:177)
2020-03-06T02:09:13.2087709Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:716)
2020-03-06T02:09:13.2096874Z 	... 11 more
2020-03-06T02:09:13.2102146Z Caused by: java.io.IOException: java.lang.OutOfMemoryError: Direct buffer memory
2020-03-06T02:09:13.2108955Z 	at org.apache.flink.runtime.io.network.netty.NettyMessage$PartitionRequest.write(NettyMessage.java:497)
2020-03-06T02:09:13.2112895Z 	at org.apache.flink.runtime.io.network.netty.NettyMessage$NettyMessageEncoder.write(NettyMessage.java:174)
2020-03-06T02:09:13.2115899Z 	... 12 more
2020-03-06T02:09:13.2119089Z Caused by: java.lang.OutOfMemoryError: Direct buffer memory
2020-03-06T02:09:13.2123496Z 	at java.base/java.nio.Bits.reserveMemory(Bits.java:175)
2020-03-06T02:09:13.2138944Z 	at java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118)
2020-03-06T02:09:13.2145321Z 	at java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317)
2020-03-06T02:09:13.2145859Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:772)
2020-03-06T02:09:13.2153742Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:748)
2020-03-06T02:09:13.2154327Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:245)
2020-03-06T02:09:13.2154884Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocate(PoolArena.java:215)
2020-03-06T02:09:13.2155419Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocate(PoolArena.java:147)
2020-03-06T02:09:13.2197616Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:342)
2020-03-06T02:09:13.2202548Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
2020-03-06T02:09:13.2203453Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178)
2020-03-06T02:09:13.2204077Z 	at org.apache.flink.runtime.io.network.netty.NettyMessage.allocateBuffer(NettyMessage.java:148)
2020-03-06T02:09:13.2215990Z 	at org.apache.flink.runtime.io.network.netty.NettyMessage.allocateBuffer(NettyMessage.java:111)
2020-03-06T02:09:13.2216595Z 	at org.apache.flink.runtime.io.network.netty.NettyMessage$PartitionRequest.write(NettyMessage.java:482)
2020-03-06T02:09:13.2217039Z 	... 13 more
2020-03-06T02:09:13.2545683Z [FAIL] Test script contains errors.
2020-03-06T02:09:13.2641336Z Checking of logs skipped.
2020-03-06T02:09:13.2641873Z 
2020-03-06T02:09:13.2643137Z [FAIL] 'Heavy deployment end-to-end test' failed after 2 minutes and 31 seconds! Test exited with exit code 1
{code}",,rmetzger,,,,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11364: [FLINK-16456][FLINK-16417][e2e] Increase off heap memory for unstable jdk11 e2e tests
URL: https://github.com/apache/flink/pull/11364
 
 
   ## What is the purpose of the change
   
   Both of these tests run out of memory sometimes. I am increasing the off heap memory for the end to end tests.
   
   
   ## Verifying this change
   
   I have executed the test with this memory configuration 5 times. So far, 5 runs were enough to trigger the failure in one of the runs.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Mar/20 10:13;githubbot;600","rmetzger commented on pull request #11364: [FLINK-16456][FLINK-16417][e2e] Increase off heap memory for unstable jdk11 e2e tests
URL: https://github.com/apache/flink/pull/11364
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/20 15:01;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 11 15:02:13 UTC 2020,,,,,,,,,,"0|z0c8hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/20 08:53;rmetzger;The issue + fix is probably very similar to FLINK-16417.;;;","11/Mar/20 15:02;rmetzger;Resolved in 10e8918b9ee3fa939d102a29d98f852790be22e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix IndexOutOfBoundsException for DISTINCT AGG with constants,FLINK-16451,13289945,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,hackergin,hackergin,06/Mar/20 02:48,04/Jun/20 07:30,13/Jul/23 08:07,04/Jun/20 07:30,1.10.0,1.9.2,,,,1.10.2,1.11.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When I use lisgagg with distinct and over window.
{code:java}
//代码占位符
""select listagg(distinct product, '|') over(partition by user order by proctime rows between 200 preceding and current row) as product, user from "" + testTable
{code}
I got the follwing exception
{code:java}
//代码占位符

Exception in thread ""main"" java.lang.IndexOutOfBoundsException: Index: 3, Size: 3 at java.util.ArrayList.rangeCheck(ArrayList.java:657) at java.util.ArrayList.get(ArrayList.java:433) at java.util.Collections$UnmodifiableList.get(Collections.java:1311) at org.apache.flink.table.types.logical.RowType.getTypeAt(RowType.java:174) at org.apache.flink.table.planner.codegen.GenerateUtils$.generateFieldAccess(GenerateUtils.scala:635) at org.apache.flink.table.planner.codegen.GenerateUtils$.generateFieldAccess(GenerateUtils.scala:620) at org.apache.flink.table.planner.codegen.GenerateUtils$.generateInputAccess(GenerateUtils.scala:524) at org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen$$anonfun$10.apply(DistinctAggCodeGen.scala:374) at org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen$$anonfun$10.apply(DistinctAggCodeGen.scala:374) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.mutable.ArrayOps$ofInt.map(ArrayOps.scala:234) at org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.generateKeyExpression(DistinctAggCodeGen.scala:374) at org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.accumulate(DistinctAggCodeGen.scala:192) at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$12.apply(AggsHandlerCodeGenerator.scala:871) at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$12.apply(AggsHandlerCodeGenerator.scala:871) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186) at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.genAccumulate(AggsHandlerCodeGenerator.scala:871) at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.generateAggsHandler(AggsHandlerCodeGenerator.scala:329) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.createBoundedOverProcessFunction(StreamExecOverAggregate.scala:425) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.translateToPlanInternal(StreamExecOverAggregate.scala:255) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.translateToPlanInternal(StreamExecOverAggregate.scala:56) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.translateToPlan(StreamExecOverAggregate.scala:56) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:54) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:39)
{code}
But It worked with 
{code:java}
//代码占位符
select listagg(distinct product) over(partition by user order by proctime rows between 200 preceding and current row) as product, user from "" + testTable
{code}
 

The exception will be throw  at the below code. 
{code:java}
//代码占位符
private def generateKeyExpression(
    ctx: CodeGeneratorContext,
    generator: ExprCodeGenerator): GeneratedExpression = {
  val fieldExprs = distinctInfo.argIndexes.map(generateInputAccess(
    ctx,
    generator.input1Type,
    generator.input1Term,
    _,
    nullableInput = false,
    deepCopy = inputFieldCopy))
{code}
 

The distinctInfo.argIndexs is  [1, 3] .  But the index 3 is a logical index. It will be replaced by  '|' . And should not  generate Input Access for  index 3 ",,danny0405,hackergin,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 04:08:05 UTC 2020,,,,,,,,,,"0|z0c86w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 08:10;danny0405;cc [~Leonard Xu];;;","04/Jun/20 04:08;jark;- master (1.12.0): 44931eb58364d9347ac1d748ed0b93a2dd40b2fe
- 1.11.0: 492a2e22b02d9032ef6e9ff80f798458ef16211b
- 1.10.2: fefc92ace2e644f7bd0619bc5103095719f1a7bb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Raise japicmp.referenceVersion to 1.10.0,FLINK-16445,13289849,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gjy,gjy,gjy,05/Mar/20 15:44,06/Mar/20 15:19,13/Jul/23 08:07,06/Mar/20 15:07,1.11.0,,,,,1.11.0,,,,Build System,,,,,0,pull-request-available,,,,"In {{pom.xml}}, change property {{japicmp.referenceVersion}} to {{1.10.0}}

",,gjy,,,,,,,,,,,,,,,,,,,,,"GJL commented on pull request #11324: [FLINK-16445][build] Set property japicmp.referenceVersion to 1.10.0
URL: https://github.com/apache/flink/pull/11324
 
 
   ## What is the purpose of the change
   
   *Set property japicmp.referenceVersion to 1.10.0*
   
   
   ## Brief change log
     - *Set property japicmp.referenceVersion to 1.10.0*
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *building the project*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**yes** / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 15:49;githubbot;600","GJL commented on pull request #11324: [FLINK-16445][build] Set property japicmp.referenceVersion to 1.10.0
URL: https://github.com/apache/flink/pull/11324
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/20 15:07;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-16458,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 06 15:07:56 UTC 2020,,,,,,,,,,"0|z0c7lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/20 15:07;gjy;master: 809eb2ab292ad2916d74f4f761b45ec4aa2f5404;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix wrong fix for user-code CheckpointExceptions,FLINK-16443,13289813,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,sewen,sewen,05/Mar/20 13:50,22/Jun/21 14:05,13/Jul/23 08:07,03/Dec/20 08:02,,,,,,1.11.4,1.12.2,1.13.0,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"The problem of having exceptions that are only in the user code classloader was fixed by proactively serializing them inside the {{CheckpointException}}. That means all consumers of  {{CheckpointException}} now need to be aware of that and unwrap the serializable exception.

I believe the right way to fix this would have been to use a SerializedException in the {{DeclineCheckpoint}} message instead, which would have localized the change to the actual problem: RPC transport.

I would suggest to revert https://github.com/apache/flink/pull/9742 and instead apply the above described change.",,AHeise,aljoscha,klion26,sewen,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 08:09:01 UTC 2021,,,,,,,,,,"0|z0c7dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Dec/20 08:02;arvid;Merged into master as 208126aa242c4e217be493140aefcf16c3c3aba9.;;;","29/Jan/21 02:19;yunta;Merged into release-1.12 so that FLINK-20675 could leverage.

333d62619f498805a7df3fec0f348ee686d5520d;;;","29/Jan/21 08:09;yunta;Merged into release-1.11

20c6ed1e2301f578d2ab784fa4545729beefcbc7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace since decorator with versionadd to mark the version an API was introduced,FLINK-16435,13289755,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,05/Mar/20 09:34,06/Mar/20 12:35,13/Jul/23 08:07,06/Mar/20 12:30,1.10.0,,,,,1.10.1,1.11.0,,,API / Python,,,,,0,pull-request-available,,,,We need to replace since decorator with versionadded to mark the version of an API was introduced. The reason is that the type information is erased for methods with decorator and it results that the IDE could not infer the parameter type of an API which is marked with since decorator.,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,"HuangXingBo commented on pull request #11318: [FLINK-16435][python] Fix ide static check
URL: https://github.com/apache/flink/pull/11318
 
 
   ## What is the purpose of the change
   
   *This pull request will fix the ide static check in PyFlink*
   
   
   ## Brief change log
   
     - *Replace since decorator with versionadd*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 09:53;githubbot;600","dianfu commented on pull request #11318: [FLINK-16435][python] Replace since decorator with versionadd to mark the version an API was introduced
URL: https://github.com/apache/flink/pull/11318
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/20 12:22;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 06 12:30:46 UTC 2020,,,,,,,,,,"0|z0c70o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/20 12:30;dian.fu;Merged to
master via 37578f75e59d61b396194641e8a83d74bfd2c984
release-1.10 via a5841203f146572ba820ca6596ededdfc30aecc2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableEnvironmentImpl doesn't clear buffered operations when it fails to translate the operation,FLINK-16433,13289740,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,05/Mar/20 08:56,19/Mar/20 06:56,13/Jul/23 08:07,18/Mar/20 15:38,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / API,,,,,0,pull-request-available,,,,,,godfreyhe,jark,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11317: [FLINK-16433][table-planner-blink] TableEnvironmentImpl doesn't clear…
URL: https://github.com/apache/flink/pull/11317
 
 
   … buffered operations when it fails to translate the operation
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   TableEnvironmentImpl won't clear buffered operations if the translation throws an exception. This can cause problem for following DMLs.
   
   
   ## Brief change log
   
     - Always clear the buffered operations.
     - Add test case.
   
   
   ## Verifying this change
   
   Added test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 09:49;githubbot;600","JingsongLi commented on pull request #11317: [FLINK-16433][table-api] TableEnvironmentImpl doesn't clear…
URL: https://github.com/apache/flink/pull/11317
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/20 08:50;githubbot;600","lirui-apache commented on pull request #11442: [FLINK-16433][table-api] TableEnvironmentImpl doesn't clear buffered …
URL: https://github.com/apache/flink/pull/11442
 
 
   …operations when it fails to translate the operation
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   TableEnvironmentImpl won't clear buffered operations if the translation throws an exception. This can cause problem for following DMLs.
   
   
   ## Brief change log
   
     - Always clear the buffered operations.
     - Add test case.
   
   
   ## Verifying this change
   
   Added test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/20 12:56;githubbot;600","JingsongLi commented on pull request #11442: [FLINK-16433][table-api] TableEnvironmentImpl doesn't clear buffered …
URL: https://github.com/apache/flink/pull/11442
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/20 15:37;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,FLINK-16675,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 15:38:03 UTC 2020,,,,,,,,,,"0|z0c6xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/20 15:38;lzljs3620320;release-1.10: 15b08d0d9da49a92640a9bb59567292eb544947f

master: cd51ffc00de58eae6baa6f2e7c31d9fdf54016b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building Hive connector gives problems,FLINK-16432,13289738,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,nielsbasjes,nielsbasjes,05/Mar/20 08:51,22/Jun/21 14:05,13/Jul/23 08:07,15/Jun/20 11:18,1.10.2,1.11.0,,,,1.10.2,1.11.0,,,Connectors / Hive,,,,,0,pull-request-available,test-stability,,,"When building the current Flink source I keep running to problems with the hive connector.
The problems focus around dependencies that are not available by default:
- org.pentaho:pentaho-aggdesigner-algorithm
- javax.jms:jms",,AHeise,libenchao,lzljs3620320,nielsbasjes,pnowojski,rmetzger,,,,,,,,,,,,,,,,"nielsbasjes commented on pull request #11316: [FLINK-16432] Fix dependencies in Hive Connector build
URL: https://github.com/apache/flink/pull/11316
 
 
   ## What is the purpose of the change
   
   * I was unable to build the software due to dependency problems with the hive connector*
   
   ## Brief change log
   
     - *Added a valid repository so the pentaho dependency could be loaded.*
     - *Excluded javax.jms:jms because that is nowhere in a maven repo*
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *being able to actually build everything without errors*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): *yes*
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: *no*
     - The runtime per-record code paths (performance sensitive): *no*
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: *no*
     - The S3 file system connector: *no*
   
   ## Documentation
   
     - Does this pull request introduce a new feature? *no*
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 09:35;githubbot;600","JingsongLi commented on pull request #11316: [FLINK-16432] Fix dependencies in Hive Connector build
URL: https://github.com/apache/flink/pull/11316
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 12:26;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-16643,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 11:18:00 UTC 2020,,,,,,,,,,"0|z0c6ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/20 10:41;rmetzger;I believe the issue was introduced in FLINK-16455.

I made this ticket a blocker, you can currently not ""mvn clean install"" Flink with an empty .m2 directory.;;;","18/Mar/20 15:46;rmetzger;For better discoverability of this ticket, this is a typical example of the error message

{code}
2020-03-17T15:56:17.9023372Z [ERROR] Failed to execute goal on project flink-connector-hive_2.11: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.11:jar:1.11-SNAPSHOT: Could not transfer artifact javax.jms:jms:jar:1.1 from/to datanucleus (http://www.datanucleus.org/downloads/maven2): Connect to www.datanucleus.org:80 [www.datanucleus.org/80.86.85.8] failed: Connection refused (Connection refused) -> [Help 1]
{code};;;","18/Mar/20 15:48;pnowojski;another instances:
https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6328/logs/64
https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6336/logs/266
https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6344/logs/71
https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6350/logs/56
and many more...;;;","18/Mar/20 15:58;rmetzger;Don't try to list all instances: All azure builds are failing because of this (because there we are only caching artifacts in the compile phase);;;","19/Mar/20 12:29;lzljs3620320;Merged in Master: 8c3c54821dd5907fab2adab107e7391908980206

Hi [~rmetzger], Do you think we need to cherry-pick to release-1.10?;;;","19/Mar/20 12:32;rmetzger;Great! Thank you so much!
As far as I know, the issue does not occur on the release-1.10 branch?;;;","19/Mar/20 12:38;lzljs3620320;[~rmetzger] Got it.

Close this ticket, If there's a recurrence, feel free to open.;;;","15/Jun/20 06:23;arvid;Also 1.10 affected.;;;","15/Jun/20 11:18;rmetzger;Merged to release-1.10: https://github.com/apache/flink/commit/124c81926be52926d7675da3b010153fa3920e47

Thanks [~AHeise];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pass build profile into end to end test script on Azure,FLINK-16431,13289731,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,05/Mar/20 08:27,05/Mar/20 15:27,13/Jul/23 08:07,05/Mar/20 15:27,,,,,,,,,,Build System / Azure Pipelines,,,,,0,,,,,"The nightly tests scripts assumes that it has access to {{$PROFILE}}, which does not seem to be true.",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 15:27:49 UTC 2020,,,,,,,,,,"0|z0c6vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/20 08:33;rmetzger;Covered in this PR https://github.com/apache/flink/pull/11309;;;","05/Mar/20 15:27;rmetzger;Resolved in bd0d631f800cf5b8ee37e37b4798200e8d5d9a44;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
failed to restore flink job from checkpoints due to unhandled exceptions,FLINK-16429,13289677,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,yuyang08,yuyang08,05/Mar/20 00:53,06/Nov/20 10:48,13/Jul/23 08:07,06/Nov/20 10:48,1.9.1,,,,,1.12.0,,,,Runtime / Coordination,,,,,0,,,,,"We are trying to restore our flink job from check-points, and run into AskTimeoutException related failures at a high frequency. Our environment is Hadoop 2.7.1 + Yarn + Flink 1.9.1. 

We hit this issue in 9 out of 10 runs, and were able to restore the application from given check-points from time to time. As the application can be restored, the check-point files shall not be corrupted. It seems that the issue is that jobmaster got timeout when it handles job submission request.  

 

Below is the exception stack trace, it is thrown from

[https://github.com/apache/flink/blob/2ec645a5bfd3cfadaf0057412401e91da0b21873/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/AbstractHandler.java#L209]

2020-03-05 00:04:14,360 ERROR org.apache.flink.runtime.rest.handler.job.JobSubmitHandler - Unhandled exception: httpRequest uri:/v1/jobs, context: ChannelHandlerContext(org.apache.flink.runtime.rest.handler.router.RouterHandler_ROUTED_HANDLER, [id: 0xc39aca33, L:/10.1.85.22:41000 - R:/10.1.16.251:44]) akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#-34498396]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply. at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635
 at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635
 at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648
 at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205
 at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601
 at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109
 at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599
 at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328
 at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279
 at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283
 at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235
 at java.lang.Thread.run(Thread.java:748 undefined)",,sewen,trohrmann,yuyang08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16866,,,,,,,,,,,,,,,,,,FLINK-16018,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 15:19:33 UTC 2020,,,,,,,,,,"0|z0c6jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/20 09:26;sewen;Thank you for reporting this.

It might be similar to the issue where the creation of the Execution Graph takes too long and the REST handler's ask times out. That can happen for example due to some blocking calls when initializing the File System connectors for checkpoints or source/sinks.

Can you check if the job actually restores, and only the REST handlers report the timeout? Or is the restore actually failing? To find that out, you could check the logs of the master, or check the web UI periodically later, so see if a job ends up running after all.;;;","27/Mar/20 15:19;trohrmann;[~yuyang08] could you provide us with the logs of the {{Dispatcher}}? I agree with Stephan that it looks very similar to FLINK-16018.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hide hive version to avoid user confuse,FLINK-16418,13289467,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,04/Mar/20 09:04,05/Mar/20 09:09,13/Jul/23 08:07,05/Mar/20 09:09,1.10.0,,,,,1.11.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"Version in Yaml/HiveCatalog needs to be consistent with the dependencies version. There are three places: version in metastore, version in dependencies, version in Yaml/HiveCatalog, users are easy to make mistakes.",,lzljs3620320,zjffdu,,,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #11304: [FLINK-16418][hive] Hide hive version to avoid user confuse
URL: https://github.com/apache/flink/pull/11304
 
 
   
   ## What is the purpose of the change
   
   Version in Yaml/HiveCatalog needs to be consistent with the dependencies version. There are three places: version in metastore, version in dependencies, version in Yaml/HiveCatalog, users are easy to make mistakes.
   
   ## Brief change log
   
   - Add constructor without hive version to HiveCatalog and HiveModule
   - Remove hive version in document
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/20 09:18;githubbot;600","JingsongLi commented on pull request #11304: [FLINK-16418][hive] Hide hive version to avoid user confuse
URL: https://github.com/apache/flink/pull/11304
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 09:01;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 09:09:24 UTC 2020,,,,,,,,,,"0|z0c58o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/20 09:10;zjffdu;[~lzljs3620320] Do you mean to detect hive version for users ? ;;;","04/Mar/20 09:42;lzljs3620320;[~zjffdu] Yes, using {{HiveShimLoader.getHiveVersion}} is enough.;;;","05/Mar/20 09:09;lzljs3620320;Master: b4730308a75f5a99f930622ccdd5289380c5d73c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectedComponents iterations with high parallelism end-to-end test fails with OutOfMemoryError: Direct buffer memory,FLINK-16417,13289460,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,04/Mar/20 08:22,11/Mar/20 15:02,13/Jul/23 08:07,11/Mar/20 15:02,,,,,,,,,,API / DataSet,Tests,,,,0,pull-request-available,test-stability,,,"Logs: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=74&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=ce095137-3e3b-5f73-4b79-c42d3d5f8283

{code}
2020-03-04T08:03:46.0786078Z 2020-03-04 08:03:42,628 INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - starting iteration [1]:  Reduce (MIN(1), at main(HighParallelismIterationsTestProgram.java:61) (12/25)
2020-03-04T08:03:46.0787503Z 2020-03-04 08:03:42,875 ERROR org.apache.flink.runtime.io.network.netty.PartitionRequestQueue [] - Encountered error while consuming partitions
2020-03-04T08:03:46.0788060Z java.lang.OutOfMemoryError: Direct buffer memory
2020-03-04T08:03:46.0788460Z 	at java.nio.Bits.reserveMemory(Bits.java:175) ~[?:?]
2020-03-04T08:03:46.0788904Z 	at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118) ~[?:?]
2020-03-04T08:03:46.0789537Z 	at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317) ~[?:?]
2020-03-04T08:03:46.0790381Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:772) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0791491Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:748) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0792483Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:245) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0793416Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocate(PoolArena.java:215) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0794359Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocate(PoolArena.java:147) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0795385Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:342) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0796471Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0797575Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0798718Z 	at org.apache.flink.shaded.netty4.io.netty.channel.unix.PreferredDirectByteBufAllocator.ioBuffer(PreferredDirectByteBufAllocator.java:53) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0799951Z 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:114) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0801172Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollRecvByteAllocatorHandle.allocate(EpollRecvByteAllocatorHandle.java:75) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0802572Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:779) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0803719Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:424) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0804763Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:326) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0806007Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0807050Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0807612Z 	at java.lang.Thread.run(Thread.java:834) [?:?]
2020-03-04T08:03:46.0808499Z 2020-03-04 08:03:43,572 ERROR org.apache.flink.runtime.operators.BatchTask                 [] - Error in task code:  Reduce (MIN(1), at main(HighParallelismIterationsTestProgram.java:61) (5/25)
2020-03-04T08:03:46.0810179Z java.lang.Exception: The data preparation for task 'Reduce (MIN(1), at main(HighParallelismIterationsTestProgram.java:61)' , caused an error: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: readAddress(..) failed: Connection reset by peer (connection to '10.1.0.4/10.1.0.4:44453')
2020-03-04T08:03:46.0811472Z 	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:480) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0813477Z 	at org.apache.flink.runtime.iterative.task.AbstractIterativeTask.run(AbstractIterativeTask.java:157) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0814813Z 	at org.apache.flink.runtime.iterative.task.IterationIntermediateTask.run(IterationIntermediateTask.java:107) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0816257Z 	at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:369) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0817111Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:717) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0817911Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:541) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0818381Z 	at java.lang.Thread.run(Thread.java:834) [?:?]
2020-03-04T08:03:46.0819353Z Caused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: readAddress(..) failed: Connection reset by peer (connection to '10.1.0.4/10.1.0.4:44453')
2020-03-04T08:03:46.0820498Z 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:650) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0821448Z 	at org.apache.flink.runtime.operators.BatchTask.getInput(BatchTask.java:1110) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0822376Z 	at org.apache.flink.runtime.operators.GroupReduceDriver.prepare(GroupReduceDriver.java:99) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0823248Z 	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:474) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0823661Z 	... 6 more
2020-03-04T08:03:46.0824426Z Caused by: java.io.IOException: Thread 'SortMerger Reading Thread' terminated due to an exception: readAddress(..) failed: Connection reset by peer (connection to '10.1.0.4/10.1.0.4:44453')
2020-03-04T08:03:46.0825507Z 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:831) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0826579Z Caused by: org.apache.flink.runtime.io.network.netty.exception.LocalTransportException: readAddress(..) failed: Connection reset by peer (connection to '10.1.0.4/10.1.0.4:44453')
2020-03-04T08:03:46.0827970Z 	at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.exceptionCaught(CreditBasedPartitionRequestClientHandler.java:165) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0829232Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:297) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0830423Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0831611Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:268) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0832773Z 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.exceptionCaught(DefaultChannelPipeline.java:1388) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0834969Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:297) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0836413Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0838310Z 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireExceptionCaught(DefaultChannelPipeline.java:918) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0839629Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.handleReadException(AbstractEpollStreamChannel.java:730) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0841070Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:820) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0842211Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:424) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0843214Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:326) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0844284Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0845351Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0845828Z 	... 1 more
2020-03-04T08:03:46.0846253Z Caused by: org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
{code}",,azagrebin,rmetzger,sewen,simahao,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11315: [FLINK-16417][e2e] Slightly increase offheap memory for ConnectedComp…
URL: https://github.com/apache/flink/pull/11315
 
 
   ## What is the purpose of the change
   
   The connected components tests was failing due to an off heap out of memory on JDK11. This test increases the memory setting
   
   See JIRA ticket for details: https://issues.apache.org/jira/browse/FLINK-16417
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 08:46;githubbot;600","rmetzger commented on pull request #11315: [FLINK-16417][e2e] Slightly increase offheap memory for ConnectedComp…
URL: https://github.com/apache/flink/pull/11315
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 15:24;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 11 15:02:29 UTC 2020,,,,,,,,,,"0|z0c574:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/20 09:32;chesnay;ping [~azagrebin], any chance this is related to the memory changes? Or should we go with the assumption that this test is simply too heavy on azure?;;;","04/Mar/20 10:40;rmetzger;I now realize that this is the first time I'm running this test on AZP with JDK11. Maybe it is related to that?;;;","04/Mar/20 12:39;azagrebin;Indeed, this can signal that there is not enough direct memory for the netty arenas because of high parallelism. Maybe, netty behaves differently in Java 11. If there is no leak of network buffers with Java 8, we can try to increase `taskmanager.memory.framework.off-heap.size` a bit to see if it helps. Otherwise, we have to investigate why the direct memory consumption increased with Java 11 and that there is no leak. I would assume if there are no special job dependencies, network stack is the main direct memory consumer there.;;;","04/Mar/20 15:20;rmetzger;Thanks. I'll try and report back here ...;;;","05/Mar/20 08:40;rmetzger;The calculated off-heap size is 130 MB.
The test passes on JDK 11 when setting it to 270 MB.
The test fails on JDK 11 when setting it to 135 MB.
The test passes on JDK 11 when setting it to 160 MB.

I will open a PR for 160MB, ok?
;;;","05/Mar/20 09:16;sewen;AFAIK Netty uses Direct Memory in Java 11, but not in Java 8. In Java 8 it uses some unsafe memory allocation, similar as Flink does in 1.10, but drops that from Java 9 onwards due to the more restrictive access control (Jigsaw modules) to avoid the ""illegal reflective access"" warnings that we also currently see in Flink.

That probably explainsthe difference.

FLINK-10742 should reduce the Netty memory footprint significantly, btw, making fixes like this one here hopefully unnecessary.;;;","05/Mar/20 15:25;rmetzger;Resolved in 2b19918f1fd9e3d056c2b729555b9582f9f30656;;;","08/Mar/20 09:24;rmetzger;The test has failed again: https://dev.azure.com/georgeryan1322/1f944fb8-b0ed-412d-93d0-78667eff17ba/_apis/build/builds/110/logs/12
Increasing to 200mb and doing more runs (I'm mostly doing the runs to fix another test);;;","11/Mar/20 15:02;rmetzger;Resolved (again) in bf1195232a49cce1897c1fa86c5af9ee005212c6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
create udaf/udtf function using sql casuing ValidationException: SQL validation failed. null,FLINK-16414,13289440,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Terry1897,Terry1897,Terry1897,04/Mar/20 05:53,05/Mar/20 05:24,13/Jul/23 08:07,05/Mar/20 05:24,1.10.0,,,,,1.10.1,,,,Table SQL / API,,,,,0,pull-request-available,,,,"When using TableEnvironment#sqlupdate to create a udaf or udtf function, which doesn't override the getResultType() method, it's normal. But when using this function in later insert sql,  some exception like following will be throwed:

Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. null
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:130)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:105)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:127)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:342)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:142)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:484)

The reason is in FunctionDefinitionUtil#createFunctionDefinition, we shouldn't direct call t.getResultType or a.getAccumulatorType() or a.getResultType() but using UserDefinedFunctionHelper#getReturnTypeOfTableFunction
 UserDefinedFunctionHelper#getAccumulatorTypeOfAggregateFunction 
UserDefinedFunctionHelper#getReturnTypeOfAggregateFunction instead.
```

		if (udf instanceof ScalarFunction) {
			return new ScalarFunctionDefinition(
				name,
				(ScalarFunction) udf
			);
		} else if (udf instanceof TableFunction) {
			TableFunction t = (TableFunction) udf;
			return new TableFunctionDefinition(
				name,
				t,
				t.getResultType()
			);
		} else if (udf instanceof AggregateFunction) {
			AggregateFunction a = (AggregateFunction) udf;

			return new AggregateFunctionDefinition(
				name,
				a,
				a.getAccumulatorType(),
				a.getResultType()
			);
		} else if (udf instanceof TableAggregateFunction) {
			TableAggregateFunction a = (TableAggregateFunction) udf;

			return new TableAggregateFunctionDefinition(
				name,
				a,
				a.getAccumulatorType(),
				a.getResultType()
			);
```


",,jark,lzljs3620320,Terry1897,,,,,,,,,,,,,,,,,,,"zjuwangg commented on pull request #11302: [FLINK-16414]fix sql validation failed when using udaf/udtf which doesn't implement getResultType
URL: https://github.com/apache/flink/pull/11302
 
 
   ## What is the purpose of the change
   
   *fix sql validation failed when using udaf/udtf which doesn't implement getResultTyp)*
   
   
   ## Brief change log
   
     - * d549e32  fix sql validation bug and add test case *
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     - *add more test case in FunctionDefinitionUtilTest.java *
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no )
     - The runtime per-record code paths (performance sensitive): (no )
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? ( no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/20 07:24;githubbot;600","zjuwangg commented on pull request #11310: [FLINK-16414]fix sql validation failed when using udaf/udtf which doesn't  implement getResultType
URL: https://github.com/apache/flink/pull/11310
 
 
   ## What is the purpose of the change
   
   *fix sql validation failed when using udaf/udtf which doesn't implement getResultTyp)*
   
   
   ## Brief change log
   
     - * d4e0c55  fix sql validation bug and add test case *
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     - *add more test case in FunctionDefinitionUtilTest.java *
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no )
     - The runtime per-record code paths (performance sensitive): (no )
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? ( no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/20 12:57;githubbot;600","JingsongLi commented on pull request #11302: [FLINK-16414]fix sql validation failed when using udaf/udtf which doesn't implement getResultType
URL: https://github.com/apache/flink/pull/11302
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 01:56;githubbot;600","JingsongLi commented on pull request #11310: [FLINK-16414]fix sql validation failed when using udaf/udtf which doesn't  implement getResultType
URL: https://github.com/apache/flink/pull/11310
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 05:23;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 05:24:55 UTC 2020,,,,,,,,,,"0|z0c52o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/20 06:06;Terry1897;cc [~bli] to confirm~;;;","05/Mar/20 05:24;lzljs3620320;master: 0362d200e3cd9ed86fd363f0c48f1a7d2d7e852f

release-1.10: c73220cb196ccf648047d3dc8b838e1e1882b471;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce hive source parallelism when limit push down,FLINK-16413,13289434,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangjun,lzljs3620320,lzljs3620320,04/Mar/20 05:24,18/Mar/20 08:54,13/Jul/23 08:07,18/Mar/20 08:54,1.10.0,,,,,1.10.1,1.11.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,"User started hive source parallelism automatic inference. For example, Set the maximum parallelism of inference to 10.

User have a similar SQL SELECT * from mytable limit 1;

There are more than 10 files in the hive table mytable. Is it a bit wasteful to start 10 parallelism.",,godfreyhe,lzljs3620320,zhangjun,,,,,,,,,,,,,,,,,,,"zhangjun888 commented on pull request #11398: [FLINK-16413]Reduce hive source parallelism when limit push down
URL: https://github.com/apache/flink/pull/11398
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Mar/20 06:41;githubbot;600","zhangjun888 commented on pull request #11398: [FLINK-16413]Reduce hive source parallelism when limit push down
URL: https://github.com/apache/flink/pull/11398
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Reduce hive source parallelism when limit push down
   
   
   ## Brief change log
   
   Reduce hive source parallelism when limit push down
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager noCheckpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Mar/20 06:42;githubbot;600","zhangjun888 commented on pull request #11398: [FLINK-16413]Reduce hive source parallelism when limit push down
URL: https://github.com/apache/flink/pull/11398
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Mar/20 00:31;githubbot;600","zhangjun888 commented on pull request #11405: [FLINK-16413]Reduce hive source parallelism when limit push down
URL: https://github.com/apache/flink/pull/11405
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Reduce hive source parallelism when limit push down
   
   
   ## Brief change log
   
   when limit push down ,set the parallelism to min(parallelism,limit)
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector:no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Mar/20 06:51;githubbot;600","JingsongLi commented on pull request #11405: [FLINK-16413]Reduce hive source parallelism when limit push down
URL: https://github.com/apache/flink/pull/11405
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/20 02:47;githubbot;600","zhangjun888 commented on pull request #11426: [FLINK-16413]Reduce hive source parallelism when limit push down
URL: https://github.com/apache/flink/pull/11426
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Reduce hive source parallelism when limit push down
   
   
   ## Brief change log
   
   Reduce hive source parallelism when limit push down
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector:no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/20 08:07;githubbot;600","zhangjun888 commented on pull request #11426: [FLINK-16413]Reduce hive source parallelism when limit push down
URL: https://github.com/apache/flink/pull/11426
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/20 08:11;githubbot;600","zhangjun888 commented on pull request #11429: [FLINK-16413] Reduce hive source parallelism when limit push down
URL: https://github.com/apache/flink/pull/11429
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Reduce hive source parallelism when limit push down
   
   
   ## Brief change log
   
   Reduce hive source parallelism when limit push down
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/20 12:12;githubbot;600","JingsongLi commented on pull request #11429: [FLINK-16413] Reduce hive source parallelism when limit push down
URL: https://github.com/apache/flink/pull/11429
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/20 08:53;githubbot;600",,0,5400,,,0,5400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 08:53:58 UTC 2020,,,,,,,,,,"0|z0c51c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/20 06:45;zhangjun;when limit push down ,we set the min(limit,parallelism) to parallelism, I commit a PR.  [~lzljs3620320];;;","14/Mar/20 06:59;zhangjun;hi,[~lzljs3620320],  I add a test method on HiveTableSourceTest,and submit a new pr;;;","14/Mar/20 07:30;lzljs3620320;[~zhangjun] Next time you don't need submit a new PR, you can just push a new commit to your own related branch.;;;","18/Mar/20 08:53;lzljs3620320;release-1.10: c4245bdcd2e9ba48cdab9760d8120df60aa0e2d5

master: 1063d56154caddc6d116605fb6590ec06ab6e5d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow embedded metastore in HiveCatalog production code,FLINK-16412,13289379,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,04/Mar/20 03:06,22/Apr/20 07:11,13/Jul/23 08:07,22/Apr/20 07:11,1.10.0,,,,,1.11.0,,,,Connectors / Hive,,,,,1,pull-request-available,usability,,,"Embedded metastore can cause weird problems for HiveCatalog, e.g. missing DN dependencies. Since embedded mode is rarely used in production, we should ban it in HiveCatalog production code. This can give users a clearer message when something goes wrong, and makes it easier for dependency management.",,leonard,lirui,lzljs3620320,Terry1897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 22 07:11:39 UTC 2020,,,,,,,,,,"0|z0c4yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/20 03:11;lirui;[~lzljs3620320] could you please assign this to me?;;;","04/Mar/20 03:19;lzljs3620320;[~lirui] Assigned to you. I mark it as a usability issue, since a lot of users are bothered by this.;;;","22/Apr/20 07:11;lzljs3620320;master: 838badc5668ad07a92be20082df28d0ab232229a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven central connection timeouts on Azure Pipelines,FLINK-16411,13289338,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,03/Mar/20 21:31,18/Apr/20 05:43,13/Jul/23 08:07,18/Apr/20 05:43,,,,,,1.11.0,,,,Build System / Azure Pipelines,,,,,0,pull-request-available,,,,"Some test stages invoke maven again, where additional dependencies are downloaded, sometimes failing the build.

This ticket is about using the Google mirror wherever possible.

Examples of failing tests:
- https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5882&view=logs&j=636f54dd-dda5-5b4b-f495-2d92ec493b6c&t=6c30efdf-a92a-5da3-9a6a-004c8552b2df

A failure looks like this:
{code}
[ERROR] Failed to execute goal on project flink-hadoop-fs: Could not resolve dependencies for project org.apache.flink:flink-hadoop-fs:jar:1.11-SNAPSHOT: Could not transfer artifact org.apache.flink:flink-shaded-hadoop-2:jar:2.8.3-10.0 from/to central (https://repo.maven.apache.org/maven2): GET request of: org/apache/flink/flink-shaded-hadoop-2/2.8.3-10.0/flink-shaded-hadoop-2-2.8.3-10.0.jar from central failed: Connection reset -> [Help 1]
{code}",,jark,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11314: [FLINK-16411][AZP] Use google mvn mirror globally
URL: https://github.com/apache/flink/pull/11314
 
 
   ## What is the purpose of the change
   
   We observed build failures due to connection issues to the apache maven central mirror in later build stages (some e2e tests, and other profiles are running maven again).
   
   ## Brief change log
   - Introduce a new step that copies the maven settings file to the home directory
   - do not pass maven settings anymore in the compile stage (as they are picked up from the home dir)
   
   
   ## Verifying this change
   
   - I verified this change by removing the test stage. All artifacts were downloaded from the google mirrors.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 08:10;githubbot;600","rmetzger commented on pull request #11314: [FLINK-16411][AZP] Use google mvn mirror globally
URL: https://github.com/apache/flink/pull/11314
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 18:17;githubbot;600","rmetzger commented on pull request #11582: [FLINK-16411][AZP] Cache maven artifacts for tests as well
URL: https://github.com/apache/flink/pull/11582
 
 
   ## What is the purpose of the change
   
   Our CI tests are failing very frequently due to connectivity issues. Most likely, the servers have some network problems.
   As a temporary mitigation, I'm enabling the maven caches for all jobs. This will add ~10 minutes of build time for each task, reducing our overall throughput. But I want our builds to be greener :) 
   In the meantime, I will try to understand the root cause of this.
   
   ## Brief change log
   
   - relax the condition for enabling the cache task.
   
   
   ## Verifying this change
   
   In 6 runs, there was only one failure, and that was caused by an unstable e2e test:
   <img width=""965"" alt=""Screenshot 2020-03-31 16 39 07"" src=""https://user-images.githubusercontent.com/89049/78039241-2904d200-736e-11ea-94b3-4caef13590e5.png"">
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 14:39;githubbot;600","rmetzger commented on pull request #11582: [FLINK-16411][AZP] Cache maven artifacts for tests as well
URL: https://github.com/apache/flink/pull/11582
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Mar/20 15:24;githubbot;600","rmetzger commented on pull request #11800: [FLINK-16411][Azure] Use Maven cache in the same data center
URL: https://github.com/apache/flink/pull/11800
 
 
   ## What is the purpose of the change
   
   It is a known problem (https://github.com/microsoft/azure-pipelines-tasks/issues/11864) that the Caching task in AZP is slow.
   This PR is introducing a check if the mirror in the datacenter is reachable. If so, we'll use it, instead of Google's mirror.
   
   ## Verifying this change
   
   I've triggered this also to a private AZP to validate the conditions: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=298&view=results
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/20 17:42;githubbot;600","rmetzger commented on pull request #11800: [FLINK-16411][Azure] Use Maven cache in the same data center
URL: https://github.com/apache/flink/pull/11800
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Apr/20 05:42;githubbot;600",,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16720,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 05:43:35 UTC 2020,,,,,,,,,,"0|z0c4ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/20 09:26;rmetzger;A similar example: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6051&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=2b7514ee-e706-5046-657b-3430666e7bd9

{code}[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (end-to-end-tests) on project flink-metrics-availability-test: Unable to generate classpath: org.apache.maven.artifact.resolver.ArtifactResolutionException: Could not transfer artifact org.apache.maven.surefire:surefire-grouper:jar:2.22.1 from/to central (https://repo.maven.apache.org/maven2): Entry [id:41][route:{s}->https://repo.maven.apache.org:443][state:null] has not been leased from this pool
{code};;;","18/Mar/20 15:38;pnowojski;Is this Another example of this issue?

{code}
2020-03-17T10:54:37.1283612Z [ERROR] Failed to execute goal on project flink-s3-fs-presto: Could not resolve dependencies for project org.apache.flink:flink-s3-fs-presto:jar:1.11-SNAPSHOT: Could not transfer artifact com.facebook.presto.hadoop:hadoop-apache2:jar:2.7.3-1 from/to central (https://repo.maven.apache.org/maven2): GET request of: com/facebook/presto/hadoop/hadoop-apache2/2.7.3-1/hadoop-apache2-2.7.3-1.jar from central failed: Connection reset -> [Help 1]
{code}

 https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6319/logs/65

and another one?

{code:java}
2020-03-17T15:56:17.9023372Z [ERROR] Failed to execute goal on project flink-connector-hive_2.11: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.11:jar:1.11-SNAPSHOT: Could not transfer artifact javax.jms:jms:jar:1.1 from/to datanucleus (http://www.datanucleus.org/downloads/maven2): Connect to www.datanucleus.org:80 [www.datanucleus.org/80.86.85.8] failed: Connection refused (Connection refused) -> [Help 1]
{code}


https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6328/logs/64
;;;","18/Mar/20 15:45;rmetzger;[~pnowojski] The first message belongs to this ticket (we have almost no connection resets from the google mirrors (the apache maven mirror is employing some rate-limiting).

The second issue is an unresolvable dependency: FLINK-16432;;;","26/Mar/20 18:18;rmetzger;Resolved in 6a0805141088ba5dd2773aa138af7bc43bbfa432;;;","27/Mar/20 10:15;rmetzger;I reopened this ticket. After adding the change for centrally controlling all mvn invocations, we still have connection timeouts (so far it seems, that maven is not getting stuck anymore, so there is at least some improvement)

[~chesnay] What do you think about the connection reset issues? They mostly occur on the Alibaba-hosted machines. I wonder if there's some connectivity issues in their DC.
I see two approaches we could pursue 
1) add caching for maven artifacts for all testing jobs as well (currently we only cache for compile and python)
2) We use a newer maven version with proper retries for downloading missing dependencies, then we call our old maven version.;;;","27/Mar/20 10:33;chesnay;I'd opt for option 1), god knows what happens if we try to mix different maven versions.;;;","27/Mar/20 15:04;rmetzger;Thanks. I'll investigate that.
Sadly, the situation has actually worsened (at least subjectively).

I have analyzed some recent build failures, and could not find any correlation between them:
{code}

20200327.2 / misc / AlibabaCI003-agent2

[ERROR] Failed to execute goal on project flink-runtime-web_2.11: Could not resolve dependencies for project org.apache.flink:flink-runtime-web_2.11:jar:1.11-SNAPSHOT: Could not transfer artifact org.apache.flink:flink-shaded-jackson-module-jsonSchema:jar:2.10.1-10.0 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): GET request of: org/apache/flink/flink-shaded-jackson-module-jsonSchema/2.10.1-10.0/flink-shaded-jackson-module-jsonSchema-2.10.1-10.0.jar from google-maven-central failed: Connection reset -> [Help 1]


20200327.6 / connectors / AlibabaCI008-agent2

[ERROR] Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.1.10:revision (default) on project flink-runtime_2.11: Execution default of goal pl.project13.maven:git-commit-id-plugin:2.1.10:revision failed: Plugin pl.project13.maven:git-commit-id-plugin:2.1.10 or one of its dependencies could not be resolved: Could not transfer artifact com.fasterxml.jackson.core:jackson-databind:jar:2.2.3 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): GET request of: com/fasterxml/jackson/core/jackson-databind/2.2.3/jackson-databind-2.2.3.jar from google-maven-central failed: Connection reset -> [Help 1]

20200327.6 / tests / AlibabaCI002-agent2

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (default-test) on project flink-parent: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test failed: Plugin org.apache.maven.plugins:maven-surefire-plugin:2.22.1 or one of its dependencies could not be resolved: Could not transfer artifact org.apache.maven.surefire:maven-surefire-common:jar:2.22.1 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): GET request of: org/apache/maven/surefire/maven-surefire-common/2.22.1/maven-surefire-common-2.22.1.jar from google-maven-central failed: Connection reset -> [Help 1]

20200327.7 / connectors / AlibabaCI002-agent2

[ERROR] Failed to execute goal on project flink-sql-parser: Could not resolve dependencies for project org.apache.flink:flink-sql-parser:jar:1.11-SNAPSHOT: Could not transfer artifact org.apache.calcite:calcite-core:jar:1.21.0 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): GET request of: org/apache/calcite/calcite-core/1.21.0/calcite-core-1.21.0.jar from google-maven-central failed: Connection reset -> [Help 1]

20200327.7 / kafka_gelly / AlibabaCI005-agent1

[ERROR] Failed to execute goal org.scalastyle:scalastyle-maven-plugin:1.0.0:check (default) on project flink-runtime_2.11: Execution default of goal org.scalastyle:scalastyle-maven-plugin:1.0.0:check failed: Plugin org.scalastyle:scalastyle-maven-plugin:1.0.0 or one of its dependencies could not be resolved: Could not transfer artifact org.scalastyle:scalastyle_2.11:jar:1.0.0 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): GET request of: org/scalastyle/scalastyle_2.11/1.0.0/scalastyle_2.11-1.0.0.jar from google-maven-central failed: Read timed out -> [Help 1]


20200327.7 / tests / AlibabaCI004-agent1

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (validate) on project flink-parent: Execution validate of goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check failed: Plugin org.apache.maven.plugins:maven-checkstyle-plugin:2.17 or one of its dependencies could not be resolved: Could not transfer artifact com.puppycrawl.tools:checkstyle:jar:8.14 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): GET request of: com/puppycrawl/tools/checkstyle/8.14/checkstyle-8.14.jar from google-maven-central failed: Connection reset -> [Help 1]

20200327.7 / legacy_scheduler / AlibabaCI003-agent2

[ERROR] Failed to execute goal org.scalastyle:scalastyle-maven-plugin:1.0.0:check (default) on project flink-runtime_2.11: Execution default of goal org.scalastyle:scalastyle-maven-plugin:1.0.0:check failed: Plugin org.scalastyle:scalastyle-maven-plugin:1.0.0 or one of its dependencies could not be resolved: Could not transfer artifact org.scalastyle:scalastyle_2.11:jar:1.0.0 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): GET request of: org/scalastyle/scalastyle_2.11/1.0.0/scalastyle_2.11-1.0.0.jar from google-maven-central failed: Connection reset -> [Help 1]

20200327.7 / misc / AlibabaCI002-agent2

[ERROR] Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.1.10:revision (default) on project flink-runtime_2.11: Execution default of goal pl.project13.maven:git-commit-id-plugin:2.1.10:revision failed: Plugin pl.project13.maven:git-commit-id-plugin:2.1.10 or one of its dependencies could not be resolved: Could not transfer artifact com.fasterxml.jackson.core:jackson-databind:jar:2.2.3 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): GET request of: com/fasterxml/jackson/core/jackson-databind/2.2.3/jackson-databind-2.2.3.jar from google-maven-central failed: Connection reset -> [Help 1]


20200327.8 / core / AlibabaCI006-agent1

[ERROR] Failed to execute goal on project flink-table-planner_2.11: Could not resolve dependencies for project org.apache.flink:flink-table-planner_2.11:jar:1.11-SNAPSHOT: Could not transfer artifact org.codehaus.janino:janino:jar:3.0.9 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): GET request of: org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar from google-maven-central failed: Connection reset -> [Help 1]
{code};;;","27/Mar/20 16:35;chesnay;Well, since we spread out or calls to different maven repositories before then maybe this prevented some rate limiting or similar?;;;","27/Mar/20 18:58;rmetzger;Yes, I was wondering the same. I think I'll reach out to the Google folks running the mirror.

I have also experimented with using {{-Dhttp.keepAlive=false}} instead of {{-Dmaven.wagon.http.pool=false}} (as this was suggested in WAGON-486), but that made the situation even worse.

My experiments with using caching for all tasks look quite promising. I'll run 5 builds over night to see what the failure rate is.

;;;","28/Mar/20 10:28;pnowojski;Can we run our own mvn cache on the AliCloud or sth like that?;;;","30/Mar/20 14:32;rmetzger;Yes, its very easy to set up a maven cache. I've considered this option as well.

The google guys mentioned there were were some issues (but I don't believe that this was the cause. We had more cases of the issue today).

Using Azure caching is very reliable, but it adds another 10 minutes of build time per job (it is only downloading 2GBs of artifacts, but there are quite some retries). These retries are another indicator for the problem being our machines / connectivity.;;;","31/Mar/20 15:25;rmetzger;Merged a PR in f728eec8c71f57447702840f3e1f8f7aaf2bd16f that enables the cache for all tasks to get the builds green for now.

I will also run some experiments with a maven cache in the same network as the CI servers.;;;","02/Apr/20 08:30;rmetzger;I have set up a Maven Cache, my results:
- Out of 8 builds (each build has 11 jobs), 0 jobs failed
- I analyzed the time of one build, it is 22% faster (93 minutes)

I have two concerns against introducing a Maven Cache:
1. we introduce a single point of failure
2. Because the cache is only accessible for the CI machines, we will need to add a few conditions to our build logic (disable the cache on ali machines + configure a different maven repository). This will make the build config files more complicated.

[~chesnay] what do you think? I tend to give the maven cache a try, because of the much improved build times.

;;;","02/Apr/20 14:57;rmetzger;Another case of
{code:java}
Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (end-to-end-tests) on project flink-metrics-availability-test: Unable to generate classpath: org.apache.maven.artifact.resolver.ArtifactResolutionException: Could not transfer artifact org.apache.maven.surefire:surefire-grouper:jar:2.22.1 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): Entry [id:13][route:{s}->https://maven-central-eu.storage-download.googleapis.com:443][state:null] has not been leased from this pool
 {code}
[https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6982&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]

 

Edit: there's now a separate ticket for this: FLINK-16947;;;","02/Apr/20 15:03;chesnay;There aren't exactly many other options so +1 to setting up the cache.;;;","02/Apr/20 15:46;rmetzger;Thanks. I will wait till mid next week for this. I would really like to have a few days of green builds and no experiments.
It might be a bit slower, but it at least works.;;;","09/Apr/20 15:32;rmetzger;Another approach would be adding the maven dependencies into the docker image.

I got the idea from here, and I also posted our problem there: https://github.com/microsoft/azure-pipelines-tasks/issues/11864;;;","09/Apr/20 15:46;chesnay;Are the docker image downloads not affected by the (apparently) slow connection?;;;","09/Apr/20 18:24;rmetzger;The connection of the machines is limited, but not to justify more than 1hr for downloading 1gb. You can see that the ""pipeline artifacts"" from compile to the test stages usually download in ~2m.
On the Alibaba-hosted machines, the containers are usually already on the machine, so no need to download.

I will actually experiment with this variant, as it might really speed things up insanely. We just need to regularly update the images, which is a bit shitty.
But maybe Azure is going to fix the issue in the meantime.;;;","10/Apr/20 07:46;jark;Another instance: [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7281&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5&l=6705]

 
{code:java}
[ERROR] No plugin found for prefix 'archetype' in the current project and in the plugin groups [org.apache.maven.plugins, org.codehaus.mojo] available from the repositories [local (/home/vsts/work/1/.m2/repository), google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/)] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/NoPluginFoundForPrefixException
{code};;;","18/Apr/20 05:43;rmetzger;For the self-hosted machines in AliCloud, we now use a nexus mirror in ali cloud: https://github.com/apache/flink/commit/bdc6693de8928e097d3f785aa1dd5ec586fc1943

Closing this ticket for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PrometheusReporterEndToEndITCase fails with ClassNotFoundException,FLINK-16410,13289337,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,rmetzger,rmetzger,03/Mar/20 21:24,06/Mar/20 14:48,13/Jul/23 08:07,05/Mar/20 15:06,,,,,,1.11.0,,,,Runtime / Metrics,Tests,,,,1,pull-request-available,test-stability,,,"Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5883&view=logs&j=b1623ac9-0979-5b0d-2e5e-1377d695c991&t=e7804547-1789-5225-2bcf-269eeaa37447

{code}
[INFO] Running org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.005 s <<< FAILURE! - in org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase
[ERROR] testReporter(org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase)  Time elapsed: 0.005 s  <<< ERROR!
java.lang.NoClassDefFoundError: org/apache/flink/runtime/rest/messages/RequestBody
	at org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.<init>(PrometheusReporterEndToEndITCase.java:119)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.runtime.rest.messages.RequestBody
	at org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.<init>(PrometheusReporterEndToEndITCase.java:119)

[INFO] 

{code}",,rmetzger,tison,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11305: [FLINK-16410][e2e][build] Add explicit flink-runtime dependency
URL: https://github.com/apache/flink/pull/11305
 
 
   The `provided` `flink-dist` dependency allows `flink-end-to-end-tests-common` to access `flink-runtime`, but the shade-plugin throws this dependency out when it creates the dependency-reduced pom.
   As a result downstream users no longer see flink-runtime, resulting in missing classes.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/20 09:30;githubbot;600","zentol commented on pull request #11305: [FLINK-16410][e2e][build] Add explicit flink-runtime dependency
URL: https://github.com/apache/flink/pull/11305
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 15:06;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-16462,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 15:06:34 UTC 2020,,,,,,,,,,"0|z0c4pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/20 15:06;chesnay;master: b6f07ddfe779698f929959289c317dbafefc4eb5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HdfsKindTest.testS3Kind fails in Hadoop 2.4.1 nightly test,FLINK-16400,13289156,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,rmetzger,rmetzger,03/Mar/20 09:08,22/Jun/21 14:05,13/Jul/23 08:07,05/Mar/20 11:34,,,,,,1.11.0,,,,FileSystems,Tests,,,,0,pull-request-available,test-stability,,,"Log: [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5843&view=logs&j=f8cdcc9b-111a-5332-0026-209cb3eb5d15&t=57d35dc9-027e-5d4a-fbeb-1c24315e6ffb] and: [https://travis-ci.org/apache/flink/jobs/657296261]
{code:java}
15:57:21.539 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.291 s <<< FAILURE! - in org.apache.flink.runtime.fs.hdfs.HdfsKindTest
15:57:21.552 [ERROR] testS3Kind(org.apache.flink.runtime.fs.hdfs.HdfsKindTest)  Time elapsed: 0.032 s  <<< ERROR!
org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 's3'. The scheme is directly supported by Flink through the following plugins: flink-s3-fs-hadoop, flink-s3-fs-presto. Please ensure that each plugin resides within its own subfolder within the plugins directory. See https://ci.apache.org/projects/flink/flink-docs-stable/ops/plugins.html for more information. If you want to use a Hadoop file system for that scheme, please add the scheme to the configuration fs.allowed-fallback-filesystems. For a full list of supported file systems, please see https://ci.apache.org/projects/flink/flink-docs-stable/ops/filesystems/.
	at org.apache.flink.runtime.fs.hdfs.HdfsKindTest.testS3Kind(HdfsKindTest.java:57)

15:57:21.574 [INFO] Running org.apache.flink.runtime.fs.hdfs.HadoopRecoverableWriterOldHadoopWithNoTruncateSupportTest {code}",,aljoscha,arvid heise,pnowojski,rmetzger,sewen,,,,,,,,,,,,,,,,,"AHeise commented on pull request #11312: [FLINK-16400][fs] Fixing e2e tests that directly use Hadoop fs.
URL: https://github.com/apache/flink/pull/11312
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fixes two different kinds of errors caused by FLINK-16015, which disables direct access to Hadoop fs if we offer a plugin.
   
   ## Brief change log
   
   - Fixing YarnFileStageTestS3ITCase for direct Hadoop access by adding an exception in configuration.
   - Removing filesystem kind. The need to differentiate between object storage and file system disappeared over the years. getKind() is now dead code.
   
   
   
   
   
   ## Verifying this change
   
   Fixes existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (**yes** / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/20 15:42;githubbot;600","pnowojski commented on pull request #11312: [FLINK-16400][fs] Fixing e2e tests that directly use Hadoop fs.
URL: https://github.com/apache/flink/pull/11312
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 11:33;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 11:34:09 UTC 2020,,,,,,,,,,"0|z0c41c:",9223372036854775807,"`org.apache.flink.core.fs.FileSystem#getKind` method has been formally deprecated, as it was not used by Flink.",,,,,,,,,,,,,,,,,,,"03/Mar/20 15:12;rmetzger;The same error also occurs in the {{YarnFileStageTestS3ITCase}}:
{code:java}
17:16:23.508 [INFO] Running org.apache.flink.yarn.YarnFileStageTestS3ITCase
17:16:29.337 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.826 s <<< FAILURE! - in org.apache.flink.yarn.YarnFileStageTestS3ITCase
17:16:29.337 [ERROR] testRecursiveUploadForYarnS3a(org.apache.flink.yarn.YarnFileStageTestS3ITCase)  Time elapsed: 0.071 s  <<< ERROR!
org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 's3a'. The scheme is directly supported by Flink through the following plugin: flink-s3-fs-hadoop. Please ensure that each plugin resides within its own subfolder within the plugins directory. See https://ci.apache.org/projects/flink/flink-docs-stable/ops/plugins.html for more information. If you want to use a Hadoop file system for that scheme, please add the scheme to the configuration fs.allowed-fallback-filesystems. For a full list of supported file systems, please see https://ci.apache.org/projects/flink/flink-docs-stable/ops/filesystems/.
	at org.apache.flink.yarn.YarnFileStageTestS3ITCase.testRecursiveUploadForYarn(YarnFileStageTestS3ITCase.java:157)
	at org.apache.flink.yarn.YarnFileStageTestS3ITCase.testRecursiveUploadForYarnS3a(YarnFileStageTestS3ITCase.java:197)
17:16:29.368 [INFO] 
17:16:29.368 [INFO] Results:
{code}

In this run: https://travis-ci.org/apache/flink/jobs/657296271



Interestingly, it does not surface on the same run on AZP: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5843&view=logs&j=c2f345e3-6738-50c0-333e-11265e9cd7e4&t=bfc49226-e770-5168-1d5a-8fe08e0d5386
It logs 
{code}
2020-03-03T01:32:52.8933937Z [INFO]  T E S T S
2020-03-03T01:32:52.8934558Z [INFO] -------------------------------------------------------
2020-03-03T01:32:53.1954466Z [INFO] Running org.apache.flink.yarn.YarnFileStageTestS3ITCase
2020-03-03T01:32:53.6854001Z [WARNING] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.488 s - in org.apache.flink.yarn.YarnFileStageTestS3ITCase
2020-03-03T01:32:54.0205161Z [INFO] 
2020-03-03T01:32:54.0206010Z [INFO] Results:
{code}
... so it seems the test was skipped because the {{NativeS3FileSystem}} was not in the classpath.

[~chesnay] Do you have an idea why this is happening?
The only difference I can see between these two tests is that on Travis, we are using {{PROFILE=""-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12 -Phive-1.2.1""}}, while on azure, it is {{PROFILE=""-Dinclude-hadoop -Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12 -Phive-1.2.1""}} (-Dinclude-hadoop is set on AZP). ;;;","03/Mar/20 15:19;arvid;That is most likely caused by FLINK-16015 .

We could either remove these tests or use a workaround similar to [https://github.com/apache/flink/blob/91399fe2cd23850ba59b9d157863188cc194962c/flink-core/src/test/java/org/apache/flink/core/fs/FileSystemTest.java#L91-L106] .;;;","04/Mar/20 09:20;pnowojski;What is the purpose of those tests? Should they be using fallback filesystem in the first place? I think we shouldn't remove them unless we know what are they testing and that we have the test coverage for the same thing somewhere else.;;;","04/Mar/20 09:37;sewen;We added the ""kind"" at some point to differentiate between file systems with proper directory semantics and object stores. To avoid doing some ""create parent directory"" and ""delete directory"" calls, which are very expensive (metadata operations).

I think this is not important any more, with new changes in the checkpointing abstraction and the streaming file sink. We do this better now, having proper initialization phases. We can probably remove this, the ""getFsKind()"" method seems also unused outside tests.;;;","04/Mar/20 14:21;chesnay;Can we just revert the commit for now? Multiple tests were broken.;;;","04/Mar/20 14:30;chesnay;[~rmetzger] S3 tests are skipped on AZP because credentials aren't set.;;;","05/Mar/20 11:34;pnowojski;Merged as be0eb54777 3e66fcfd25;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis consumer unnecessarily creates record emitter thread w/o source sync,FLINK-16393,13289066,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,thw,thw,thw,02/Mar/20 23:20,12/Mar/20 20:00,13/Jul/23 08:07,04/Mar/20 18:06,1.10.0,1.8.3,1.9.2,,,1.11.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,The asynchronous record emitter depends on the periodic watermark calculation. If no periodic watermark is configured then records will be directly emitted by the shard consumer threads and the record emitter thread never used. We should skip the thread creation in that case.,,mxm,thw,,,,,,,,,,,,,,,,,,,,"tweise commented on pull request #11292: [FLINK-16393][kinesis] Skip record emitter thread creation w/o source sync
URL: https://github.com/apache/flink/pull/11292
 
 
   ## What is the purpose of the change
   
   * Don't start the thread when watermark synchronizatiom not configured
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Mar/20 00:16;githubbot;600","tweise commented on pull request #11292: [FLINK-16393][kinesis] Skip record emitter thread creation w/o source sync
URL: https://github.com/apache/flink/pull/11292
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/20 18:05;githubbot;600",,,,,,,,,0,1200,,,0,1200,,FLINK-16573,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-03-02 23:20:17.0,,,,,,,,,,"0|z0c3hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use LinkedHashMap for deterministic order in HeapMapState.java,FLINK-16387,13288909,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,cpugputpu,cpugputpu,02/Mar/20 15:11,05/Mar/20 09:30,13/Jul/23 08:07,05/Mar/20 09:30,,,,,,,,,,,,,,,1,pull-request-available,,,,"The tests in _org.apache.flink.table.runtime.operators.join.RowTimeBoundedStreamJoinTest#testRowTimeInnerJoinWithCommonBounds_ and _org.apache.flink.table.runtime.operators.join.ProcTimeBoundedStreamJoinTest#testProcTimeInnerJoinWithCommonBounds_ fail due to a different iteration order of a HashMap. The failure is presented as follows.

org.junit.internal.ArrayComparisonFailure: output wrong: arrays first differed at element [0]; 
 expected:<(+|1,1a1,1,1b3)> 
 but was:<(+|1,1a3,1,1b3)>
 at org.apache.flink.table.runtime.operators.join.ProcTimeBoundedStreamJoinTest.testProcTimeInnerJoinWithCommonBounds(ProcTimeBoundedStreamJoinTest.java:101_

 

The root cause of this failure lies in a HashMap's iterator, which makes no guarantee about the iteration order. A brief stack trace is for your reference:

ava.util.HashMap$EntrySet.iterator(HashMap.java:1014)
 org.apache.flink.runtime.state.heap.HeapMapState.iterator(HeapMapState.java:161)
 org.apache.flink.runtime.state.UserFacingMapState.iterator(UserFacingMapState.java:95)
 org.apache.flink.table.runtime.operators.join.TimeBoundedStreamJoin.processElement2(TimeBoundedStreamJoin.java:246)
 org.apache.flink.table.runtime.operators.join.ProcTimeBoundedStreamJoin.processElement2(ProcTimeBoundedStreamJoin.java:29)
 org.apache.flink.table.runtime.operators.join.TimeBoundedStreamJoin.processElement2(TimeBoundedStreamJoin.java:52)
 org.apache.flink.streaming.api.operators.co.LegacyKeyedCoProcessOperator.processElement2(LegacyKeyedCoProcessOperator.java:89)
 org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.processElement2(TwoInputStreamOperatorTestHarness.java:57)
 org.apache.flink.table.runtime.operators.join.ProcTimeBoundedStreamJoinTest.testProcTimeInnerJoinWithCommonBounds(ProcTimeBoundedStreamJoinTest.java:70)

The specification about HashMap says that ""this class makes no guarantees as to the order of the map; in particular, it does not guarantee that the order will remain constant over time"". The documentation is here for your reference: [https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html]

 

The fix is to use LinkedHashMap instead of HashMap. In this way, the test will not suffer from failure any more and the code will be more stable, free of this non-deterministic behaviour.

 ",,cpugputpu,jark,sewen,,,,,,,,,,,,,,,,,,,"cpugputpu commented on pull request #11287: [FLINK-16387][runtime]Use LinkedHashMap for deterministic order in HeapMapState.java
URL: https://github.com/apache/flink/pull/11287
 
 
   ## What is the purpose of the change
   This PR aims to solve the issue here: https://issues.apache.org/jira/browse/FLINK-16387
   
   The fix is to use LinkedHashMap instead of HashMap. In this way, the test in `org.apache.flink.table.runtime.operators.join.ProcTimeBoundedStreamJoinTest#testProcTimeInnerJoinWithCommonBounds` and `org.apache.flink.table.runtime.operators.join.RowTimeBoundedStreamJoinTest#testRowTimeInnerJoinWithCommonBounds` will not suffer from failure any more and the code will be more stable, free of non-deterministic behaviours.
   
   
   ## Verifying this change
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): don't know
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Mar/20 15:18;githubbot;600","zentol commented on pull request #11287: [FLINK-16387][runtime]Use LinkedHashMap for deterministic order in HeapMapState.java
URL: https://github.com/apache/flink/pull/11287
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Mar/20 15:27;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 09:30:09 UTC 2020,,,,,,,,,,"0|z0c2so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/20 09:30;sewen;This is the wrong approach. There should be no production code changes to fix tests determinism.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"KafkaProducerExactlyOnceITCase.testExactlyOnceRegularSink fails with ""The producer has already been closed""",FLINK-16383,13288867,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,rmetzger,rmetzger,02/Mar/20 12:51,22/Jun/21 14:04,13/Jul/23 08:07,19/May/20 13:12,,,,,,1.11.0,,,,Runtime / Task,Tests,,,,0,pull-request-available,test-stability,,,"Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5779&view=logs&j=a54de925-e958-5e24-790a-3a6150eb72d8&t=24e561e9-4c8d-598d-a290-e6acce191345

{code}
2020-03-01T01:06:57.4738418Z 01:06:57,473 [Source: Custom Source -> Map -> Sink: Unnamed (1/1)] INFO  org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer [] - Flushing new partitions
2020-03-01T01:06:57.4739960Z 01:06:57,473 [FailingIdentityMapper Status Printer] INFO  org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper [] - ============================> Failing mapper  0: count=680, totalCount=1000
2020-03-01T01:06:57.4909074Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-03-01T01:06:57.4910001Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-03-01T01:06:57.4911000Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:648)
2020-03-01T01:06:57.4912078Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:77)
2020-03-01T01:06:57.4913039Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1619)
2020-03-01T01:06:57.4914421Z 	at org.apache.flink.test.util.TestUtils.tryExecute(TestUtils.java:35)
2020-03-01T01:06:57.4915423Z 	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testExactlyOnce(KafkaProducerTestBase.java:370)
2020-03-01T01:06:57.4916483Z 	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testExactlyOnceRegularSink(KafkaProducerTestBase.java:309)
2020-03-01T01:06:57.4917305Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-01T01:06:57.4917982Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-01T01:06:57.4918769Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-01T01:06:57.4919477Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-01T01:06:57.4920156Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-03-01T01:06:57.4920995Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-03-01T01:06:57.4921927Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-03-01T01:06:57.4922728Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-03-01T01:06:57.4923428Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-03-01T01:06:57.4924048Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-03-01T01:06:57.4924779Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-03-01T01:06:57.4925528Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-03-01T01:06:57.4926318Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-03-01T01:06:57.4927214Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-03-01T01:06:57.4927872Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-03-01T01:06:57.4928587Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-03-01T01:06:57.4929289Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-03-01T01:06:57.4929943Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-03-01T01:06:57.4930672Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-03-01T01:06:57.4931512Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-03-01T01:06:57.4932255Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-03-01T01:06:57.4932962Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-03-01T01:06:57.4933741Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-03-01T01:06:57.4934344Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-03-01T01:06:57.4935193Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-03-01T01:06:57.4936245Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-03-01T01:06:57.4937113Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-03-01T01:06:57.4937925Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-03-01T01:06:57.4938763Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-03-01T01:06:57.4939656Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-03-01T01:06:57.4940451Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-03-01T01:06:57.4941302Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-03-01T01:06:57.4942240Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
2020-03-01T01:06:57.4943374Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110)
2020-03-01T01:06:57.4944802Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76)
2020-03-01T01:06:57.4945836Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:190)
2020-03-01T01:06:57.4946730Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:184)
2020-03-01T01:06:57.4947705Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:178)
2020-03-01T01:06:57.4948647Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:505)
2020-03-01T01:06:57.4949500Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:383)
2020-03-01T01:06:57.4950225Z 	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
2020-03-01T01:06:57.4950917Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-01T01:06:57.4951721Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-01T01:06:57.4952412Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
2020-03-01T01:06:57.4953238Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
2020-03-01T01:06:57.4954080Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-03-01T01:06:57.4955045Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-03-01T01:06:57.4955760Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-03-01T01:06:57.4956435Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-03-01T01:06:57.4957091Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-03-01T01:06:57.4957800Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-03-01T01:06:57.4958491Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-03-01T01:06:57.4959183Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-03-01T01:06:57.4959872Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-03-01T01:06:57.4960521Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-03-01T01:06:57.4961227Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-03-01T01:06:57.4961875Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-03-01T01:06:57.4962453Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-03-01T01:06:57.4963028Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-03-01T01:06:57.4963601Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-03-01T01:06:57.4964151Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-03-01T01:06:57.4965046Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-03-01T01:06:57.4965802Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-03-01T01:06:57.4966510Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-03-01T01:06:57.4967258Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-03-01T01:06:57.4967954Z Caused by: java.lang.RuntimeException: Error while confirming checkpoint
2020-03-01T01:06:57.4968749Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:899)
2020-03-01T01:06:57.4969694Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$7(StreamTask.java:873)
2020-03-01T01:06:57.4970599Z 	at org.apache.flink.util.function.FunctionUtils.lambda$asCallable$5(FunctionUtils.java:125)
2020-03-01T01:06:57.4971407Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-03-01T01:06:57.4972310Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:85)
2020-03-01T01:06:57.4973437Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
2020-03-01T01:06:57.4974275Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:79)
2020-03-01T01:06:57.4975421Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:138)
2020-03-01T01:06:57.4976419Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:113)
2020-03-01T01:06:57.4977340Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:117)
2020-03-01T01:06:57.4978239Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:117)
2020-03-01T01:06:57.4979154Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:78)
2020-03-01T01:06:57.4980051Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:305)
2020-03-01T01:06:57.4980894Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:503)
2020-03-01T01:06:57.4981770Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:482)
2020-03-01T01:06:57.4982490Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:717)
2020-03-01T01:06:57.4983157Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:541)
2020-03-01T01:06:57.4983716Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-01T01:06:57.4984462Z Caused by: org.apache.flink.util.FlinkRuntimeException: Committing one of transactions failed, logging first encountered failure
2020-03-01T01:06:57.4985606Z 	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:302)
2020-03-01T01:06:57.4986723Z 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:130)
2020-03-01T01:06:57.4987718Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointComplete$8(StreamTask.java:884)
2020-03-01T01:06:57.4988771Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:99)
2020-03-01T01:06:57.4989793Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:879)
2020-03-01T01:06:57.4990396Z 	... 17 more
2020-03-01T01:06:57.4990886Z Caused by: java.lang.IllegalStateException: The producer has already been closed
2020-03-01T01:06:57.4991892Z 	at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.ensureNotClosed(FlinkKafkaInternalProducer.java:251)
2020-03-01T01:06:57.4993008Z 	at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.commitTransaction(FlinkKafkaInternalProducer.java:102)
2020-03-01T01:06:57.4994228Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.commit(FlinkKafkaProducer.java:905)
2020-03-01T01:06:57.4995310Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.commit(FlinkKafkaProducer.java:97)
2020-03-01T01:06:57.4996317Z 	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:289)
2020-03-01T01:06:57.4997044Z 	... 21 more
2020-03-01T01:06:57.4998645Z 01:06:57,493 [                main] ERROR org.apache.flink.streaming.connectors.kafka.KafkaProducerExactlyOnceITCase [] - 
{code}",,aljoscha,becket_qin,dwysakowicz,gaoyunhaii,jark,kezhuw,klion26,pnowojski,rmetzger,wanglijie,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17378,,,,,,,,,,,,,,,,FLINK-22142,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 13:12:22 UTC 2020,,,,,,,,,,"0|z0c2jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/20 07:33;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1107&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20

{code}
============================> Failing mapper  0: count=670, totalCount=1000
2020-05-12T17:24:45.4532543Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-12T17:24:45.4533923Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-05-12T17:24:45.4534660Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:659)
2020-05-12T17:24:45.4535393Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:81)
2020-05-12T17:24:45.4536371Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1645)
2020-05-12T17:24:45.4537095Z 	at org.apache.flink.test.util.TestUtils.tryExecute(TestUtils.java:35)
2020-05-12T17:24:45.4537837Z 	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testExactlyOnce(KafkaProducerTestBase.java:370)
2020-05-12T17:24:45.4538810Z 	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testExactlyOnceRegularSink(KafkaProducerTestBase.java:309)
2020-05-12T17:24:45.4539495Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-12T17:24:45.4540093Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-12T17:24:45.4540767Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-12T17:24:45.4541377Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-12T17:24:45.4541975Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-12T17:24:45.4542656Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-12T17:24:45.4543349Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-12T17:24:45.4544046Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-12T17:24:45.4544680Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-12T17:24:45.4545226Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-12T17:24:45.4545755Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-12T17:24:45.4546924Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-12T17:24:45.4547584Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-12T17:24:45.4650351Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-12T17:24:45.4651057Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-12T17:24:45.4651841Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-12T17:24:45.4652538Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-12T17:24:45.4653134Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-12T17:24:45.4653777Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-12T17:24:45.4654422Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-12T17:24:45.4655070Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-12T17:24:45.4655761Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-12T17:24:45.4656441Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-12T17:24:45.4656984Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-12T17:24:45.4657578Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-12T17:24:45.4658283Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-12T17:24:45.4659069Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-12T17:24:45.4659771Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-12T17:24:45.4660506Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-12T17:24:45.4661243Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-12T17:24:45.4661944Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-12T17:24:45.4662673Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-12T17:24:45.4663504Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
2020-05-12T17:24:45.4664441Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:112)
2020-05-12T17:24:45.4665366Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-05-12T17:24:45.4666326Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:189)
2020-05-12T17:24:45.4774925Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:183)
2020-05-12T17:24:45.4775890Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:177)
2020-05-12T17:24:45.4776894Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:497)
2020-05-12T17:24:45.4777646Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:384)
2020-05-12T17:24:45.4778235Z 	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
2020-05-12T17:24:45.4778929Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-12T17:24:45.4779539Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-12T17:24:45.4780136Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-05-12T17:24:45.4780858Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-05-12T17:24:45.4781576Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-05-12T17:24:45.4782654Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-05-12T17:24:45.4783271Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-05-12T17:24:45.4783852Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-05-12T17:24:45.4784437Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-05-12T17:24:45.4785012Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-05-12T17:24:45.4785680Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-05-12T17:24:45.4786387Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-12T17:24:45.4786977Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-12T17:24:45.4787548Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-05-12T17:24:45.4788080Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-05-12T17:24:45.4788650Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-05-12T17:24:45.4789267Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-05-12T17:24:45.4789776Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-05-12T17:24:45.4790279Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-05-12T17:24:45.4790740Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-05-12T17:24:45.4791274Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-05-12T17:24:45.4791885Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-05-12T17:24:45.4792529Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-05-12T17:24:45.4793256Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-05-12T17:24:45.4793872Z Caused by: java.lang.RuntimeException: Error while confirming checkpoint
2020-05-12T17:24:45.4794537Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:922)
2020-05-12T17:24:45.4795330Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$7(StreamTask.java:909)
2020-05-12T17:24:45.4796204Z 	at org.apache.flink.util.function.FunctionUtils.lambda$asCallable$5(FunctionUtils.java:125)
2020-05-12T17:24:45.4796838Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-05-12T17:24:45.4797612Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
2020-05-12T17:24:45.4798444Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
2020-05-12T17:24:45.4799194Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:79)
2020-05-12T17:24:45.4800080Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:138)
2020-05-12T17:24:45.4800947Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:113)
2020-05-12T17:24:45.4801708Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:117)
2020-05-12T17:24:45.4805156Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:117)
2020-05-12T17:24:45.4805950Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:78)
2020-05-12T17:24:45.4808012Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:300)
2020-05-12T17:24:45.4808843Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:553)
2020-05-12T17:24:45.4811037Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:532)
2020-05-12T17:24:45.4811688Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713)
2020-05-12T17:24:45.4812246Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539)
2020-05-12T17:24:45.4814313Z 	at java.lang.Thread.run(Thread.java:748)
2020-05-12T17:24:45.4814934Z Caused by: org.apache.flink.util.FlinkRuntimeException: Committing one of transactions failed, logging first encountered failure
2020-05-12T17:24:45.4817040Z 	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:302)
2020-05-12T17:24:45.4818015Z 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:130)
2020-05-12T17:24:45.4821678Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:177)
2020-05-12T17:24:45.4824078Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:915)
2020-05-12T17:24:45.4824715Z 	... 17 more
2020-05-12T17:24:45.4825180Z Caused by: java.lang.IllegalStateException: The producer has already been closed
2020-05-12T17:24:45.4826172Z 	at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.ensureNotClosed(FlinkKafkaInternalProducer.java:251)
2020-05-12T17:24:45.4827195Z 	at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.commitTransaction(FlinkKafkaInternalProducer.java:102)
2020-05-12T17:24:45.4828137Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.commit(FlinkKafkaProducer.java:910)
2020-05-12T17:24:45.4829014Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.commit(FlinkKafkaProducer.java:98)
2020-05-12T17:24:45.4829960Z 	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:289)
2020-05-12T17:24:45.4830622Z 	... 20 more
{code};;;","13/May/20 12:03;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1128&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20;;;","13/May/20 12:05;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1129&view=logs&j=16ccbdb7-2a3e-53da-36eb-fb718edc424a&t=cf61ce33-6fba-5fbe-2c0c-e41c4013e891
;;;","13/May/20 12:12;rmetzger;Same error, but 
{code}
[ERROR] Failures: 
[ERROR]   Kafka011ProducerExactlyOnceITCase>KafkaProducerTestBase.testExactlyOnceRegularSink:309->KafkaProducerTestBase.testExactlyOnce:370 Test failed: Job execution failed.
[INFO] 

{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1133&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","13/May/20 12:15;rmetzger;Upgrading blocker: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1165&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","13/May/20 17:56;rmetzger;The increased error rate might indicate that this is caused by another change: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7996&view=logs&j=5a8a90c3-b1a0-5409-0a40-5284e4660a64&t=7e1e4eb1-b500-57c1-6458-c7270cc1134e&l=18968;;;","14/May/20 02:24;zjwang;Another instance [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1174&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20];;;","14/May/20 02:28;jark;Another similar instance, {{KafkaProducerExactlyOnceITCase.testMultipleSinkOperators}} is failed.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1192&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","14/May/20 11:10;pnowojski;I'm starting to suspect that maybe something here FLINK-17307 broke the test stability. This issue started to pop up constantly the same day that [PR|https://github.com/apache/flink/pull/12018] was merged and it's I think the only one that was touching Kafka.

CC [~dwysakowicz] [~aljoscha];;;","14/May/20 11:26;aljoscha;I started looking into this and trying to reproduce locally, no dice so far.;;;","14/May/20 11:39;aljoscha;I doubt it's FLINK-17307 because {{KafkaProducerExactlyOnceITCase. testExactlyOnceRegularSink}} does not involve a KafkaConsumer, and those changes only touch the consumer side.;;;","14/May/20 13:25;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1222&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","14/May/20 14:13;aljoscha;I added a potential fix on master in 2f0c4d4ec3241679465c792c4bb5c2ef0e4a150e.

Also added debugging logging in 9d44834199488612fa7c7c8f9b9c641d9785fb95, so if this occurs again, please re-open the issue and post the log.;;;","14/May/20 17:03;rmetzger;Reopening ... {{The producer 1502537416 has already been closed}}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1312&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20;;;","15/May/20 07:30;rmetzger;{code}
Caused by: org.apache.flink.util.FlinkRuntimeException: Committing one of transactions failed, logging first encountered failure
	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:302)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:130)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:177)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:923)
	... 17 more
Caused by: java.lang.IllegalStateException: The producer 1100101099 has already been closed
	at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaProducer.ensureNotClosed(FlinkKafkaProducer.java:302)
	at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaProducer.commitTransaction(FlinkKafkaProducer.java:149)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.commit(FlinkKafkaProducer011.java:740)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.commit(FlinkKafkaProducer011.java:96)
	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:289)
	... 20 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1352&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","15/May/20 07:54;rmetzger;More cases from my daily check of the CI builds from last night:
- https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1352&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20
- https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1345&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8
- https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1343&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=603cb7fd-6f38-5c99-efca-877e1439232f
- https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1343&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=d4549d78-6fab-5c0c-bdb9-abaafb66ea8b
- https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1312&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20
- https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1304&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","15/May/20 08:37;aljoscha;This is an extract of the relevant log entries

{code}
03:04:24,078 [Source: Custom Source -> Map -> Sink: Unnamed (1/1)] DEBUG org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaProducer [] - Closed internal KafkaProducer 1100101099. Stacktrace: java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaProducer.close(FlinkKafkaProducer.java:201)
org.apache.flink.util.IOUtils.closeQuietly(IOUtils.java:263)
org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.lambda$close$1(FlinkKafkaProducer011.java:692)
java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
java.util.Iterator.forEachRemaining(Iterator.java:116)
java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.close(FlinkKafkaProducer011.java:692)
org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43)
org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:109)
org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.closeOperator(StreamOperatorWrapper.java:186)
org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$deferCloseOperatorToMailbox$3(StreamOperatorWrapper.java:160)
org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:79)
org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:138)
org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:113)
org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:117)
org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:117)
org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:78)
org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:300)
org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:561)
org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:533)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:539)


Caused by: java.lang.RuntimeException: Error while confirming checkpoint
	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:930)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$7(StreamTask.java:917)
	at org.apache.flink.util.function.FunctionUtils.lambda$asCallable$5(FunctionUtils.java:125)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:79)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:138)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:113)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:117)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:117)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:78)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:300)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:561)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:533)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkRuntimeException: Committing one of transactions failed, logging first encountered failure
	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:302)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:130)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:177)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:923)
	... 17 more
Caused by: java.lang.IllegalStateException: The producer 1100101099 has already been closed
	at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaProducer.ensureNotClosed(FlinkKafkaProducer.java:302)
	at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaProducer.commitTransaction(FlinkKafkaProducer.java:149)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.commit(FlinkKafkaProducer011.java:740)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.commit(FlinkKafkaProducer011.java:96)
	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:289)
	... 20 more

{code};;;","15/May/20 09:13;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1372&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","15/May/20 09:44;becket_qin;I think the problem is following:
The shutdown sequence in {{StreamOperatorWrapper}} is that # The mailbox quiesceProcessingTImeService
 # after the future of step 1 is done, the mailbox sends a mail to close the operators.
 # after the future of step 2 is done, the mailbox sends a closeMail to complete the shutdown sequence.
 # after the closeMail is processed. The shutdown sequence is completed

The problems is that between step 2 and step 3, a checkpoint complete notification was enqueued to the mailbox. So the mailbox has not shutdown yet, but the operator has been closed.;;;","15/May/20 14:34;jark;https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/1406/logs/75;;;","15/May/20 15:11;klion26;seems another instance [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1230&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8&l=18475]
{code:java}
2020-05-14T04:20:36.9953507Z 04:20:36,993 [Source: Custom Source -> Sink: Unnamed (3/3)] WARN org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher [] - Committing offsets to Kafka takes longer than the checkpoint interval. Skipping commit of previous offsets because newer complete checkpoint offsets are available. This does not compromise Flink's checkpoint integrity. 2020-05-14T04:20:37.2142509Z 04:20:37,174 [program runner thread] ERROR org.apache.flink.streaming.connectors.kafka.KafkaTestBase [] - Job Runner failed with exception 2020-05-14T04:20:37.2143266Z org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: cdf00a9c3a8b9c3afb72f57e7ef936c8) 2020-05-14T04:20:37.2144122Z at org.apache.flink.client.ClientUtils.submitJobAndWaitForResult(ClientUtils.java:115) ~[flink-clients_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] 2020-05-14T04:20:37.2145249Z at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.lambda$runCancelingOnEmptyInputTest$2(KafkaConsumerTestBase.java:1075) ~[flink-connector-kafka-base_2.11-1.11-SNAPSHOT-tests.jar:?] 2020-05-14T04:20:37.2145858Z at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242] 2020-05-14T04:20:37.2146393Z Caused by: org.apache.flink.runtime.client.JobCancellationException: Job was cancelled. 2020-05-14T04:20:37.2151756Z at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:149) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] 2020-05-14T04:20:37.2153457Z at org.apache.flink.client.ClientUtils.submitJobAndWaitForResult(ClientUtils.java:113) ~[flink-clients_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] 2020-05-14T04:20:37.2154166Z ... 2 more
{code};;;","17/May/20 07:15;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1506&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","18/May/20 09:41;becket_qin;[~AHeise] It seems the bug fix in [https://github.com/apache/flink/pull/12186] was incomplete. We need to make sure all the methods called on the wrapped {{StreamOperator}} are going through the {{StreamOperatorWrapper}}.

In this case, the test failure was because `notifyCheckpointComplete()` was invoked directly on the wrapper operator instead of the wrapper. See:

https://github.com/apache/flink/blob/20c28ac77ecd6b8d11e38ed84c9a5c36317721f3/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java#L178;;;","19/May/20 09:01;wanglijie;I meet the same problem. [https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/970/logs/74];;;","19/May/20 13:12;pnowojski;Fixed on master as cbd9ca0ca3 on release-1.11 as 14f0bcef06;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AZP: Python test fails on jdk11 nightly test (misc profile),FLINK-16380,13288860,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,02/Mar/20 12:19,02/Mar/20 16:58,13/Jul/23 08:07,02/Mar/20 16:58,,,,,,,,,,Build System / Azure Pipelines,,,,,0,pull-request-available,,,,"Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5779&view=logs&j=d5dbfc72-24cf-5a8f-e213-1ae80d4b2df8&t=cb83ed8c-7d59-59ba-b58d-25e43fbaa4b2

{code}
----------------------------- Captured stderr call -----------------------------
Error: A JNI error has occurred, please check your installation and try again
Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/flink/client/python/PythonGatewayServer has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)
___________________ StreamTableWindowTests.test_over_window ____________________

{code}
",,rmetzger,,,,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11289: [FLINK-16380][AZP] Fix jdk11 switch
URL: https://github.com/apache/flink/pull/11289
 
 
   ## What is the purpose of the change
   
   - Fix the failing python tests in the jdk11 profile
   
   
   ## Brief change log
   
   - fix issue in setting the java version. (This issue was not discovered before because maven evaluates JAVA_HOME, which was set correctly)
   
   ## Verifying this change
   
   JDK11 + python is passing here: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=57&view=logs&j=584fa981-f71a-5840-1c49-f800c954fe4b&t=0d0f8639-a7cb-550f-f4bb-13e97d089a48
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Mar/20 16:04;githubbot;600","rmetzger commented on pull request #11289: [FLINK-16380][AZP] Fix jdk11 switch
URL: https://github.com/apache/flink/pull/11289
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Mar/20 16:56;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 16:58:13 UTC 2020,,,,,,,,,,"0|z0c2hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/20 12:21;chesnay;Looks like the environment isn't setup correctly since it is trying to run things with Java 8.;;;","02/Mar/20 12:22;rmetzger;I suspect an issue with our Azure, as this error is not occurring on travis (reference: https://travis-ci.org/apache/flink/jobs/656930995);;;","02/Mar/20 16:58;rmetzger;Resolved in 195b0d03ad994a82baee8864c1fedf1ae68b0144;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable JDK 11 Docker tests on AZP,FLINK-16378,13288852,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,02/Mar/20 11:40,05/Mar/20 15:26,13/Jul/23 08:07,05/Mar/20 15:26,,,,,,,,,,Build System / Azure Pipelines,,,,,0,pull-request-available,,,,"Build log: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5779&view=logs&j=eec879f1-c5a2-5810-2b49-ba5c6bfecb27&t=484f04d6-55db-5161-9f93-391b1677737d

{code}Error: A JNI error has occurred, please check your installation and try again
Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/flink/client/cli/CliFrontend has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)
Running the job failed.
{code}",,rmetzger,,,,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11309: [FLINK-16378][AZP] Disable Docker tests when running with JDK11
URL: https://github.com/apache/flink/pull/11309
 
 
   ## What is the purpose of the change
   
   The docker tests are not passing on JDK11. Until this is implemented, we need to skip them
   
   
   ## Brief change log
   
   - Skip tests using docker somehow in the `run-nightly-tests.sh` script.
   
   ## Verifying this change
   
   Set the `jdk: jdk11` in `azure-pipelines.yml` and see the e2e tests passing
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/20 12:40;githubbot;600","rmetzger commented on pull request #11309: [FLINK-16378][AZP] Disable Docker tests when running with JDK11
URL: https://github.com/apache/flink/pull/11309
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 15:26;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13719,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 15:26:55 UTC 2020,,,,,,,,,,"0|z0c2g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/20 11:50;rmetzger;Thanks for pointing to the other ticket. I would still like to use this ticket to open a PR for AZP to disable test execution for all container tests on jdk11.;;;","05/Mar/20 15:26;rmetzger;Resolved in 1a4fb25df802253be17f27d0f89e51c5762c3d7d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"StreamingKafkaITCase: IOException: error=13, Permission denied",FLINK-16374,13288834,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,02/Mar/20 10:40,11/Mar/20 14:51,13/Jul/23 08:07,11/Mar/20 14:51,1.11.0,,,,,1.11.0,,,,Connectors / Kafka,Tests,,,,0,pull-request-available,test-stability,,,"Build: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5792&view=logs&j=25197a20-5964-5b06-5716-045f87dc0ea9&t=0c53f4dc-c81e-5ebb-13b2-08f1994a2d32

{code}
2020-03-02T05:13:23.4758068Z [INFO] Running org.apache.flink.tests.util.kafka.StreamingKafkaITCase
2020-03-02T05:13:55.8260013Z [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 32.346 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.StreamingKafkaITCase
2020-03-02T05:13:55.8262664Z [ERROR] testKafka[0: kafka-version:0.10.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 9.217 s  <<< ERROR!
2020-03-02T05:13:55.8264067Z java.io.IOException: Cannot run program ""/tmp/junit5236495846374568650/junit4714535957173883866/bin/start-cluster.sh"": error=13, Permission denied
2020-03-02T05:13:55.8264733Z 	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)
2020-03-02T05:13:55.8265242Z Caused by: java.io.IOException: error=13, Permission denied
2020-03-02T05:13:55.8265717Z 	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)
2020-03-02T05:13:55.8266083Z 
2020-03-02T05:13:55.8271420Z [ERROR] testKafka[1: kafka-version:0.11.0.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 11.228 s  <<< ERROR!
2020-03-02T05:13:55.8272670Z java.io.IOException: Cannot run program ""/tmp/junit8038960384540194088/junit1280636219654303027/bin/start-cluster.sh"": error=13, Permission denied
2020-03-02T05:13:55.8273343Z 	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)
2020-03-02T05:13:55.8273847Z Caused by: java.io.IOException: error=13, Permission denied
2020-03-02T05:13:55.8274418Z 	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)
2020-03-02T05:13:55.8274768Z 
2020-03-02T05:13:55.8275429Z [ERROR] testKafka[2: kafka-version:2.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 11.89 s  <<< ERROR!
2020-03-02T05:13:55.8276386Z java.io.IOException: Cannot run program ""/tmp/junit5500905670445852005/junit4695208010500962520/bin/start-cluster.sh"": error=13, Permission denied
2020-03-02T05:13:55.8277257Z 	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)
2020-03-02T05:13:55.8277760Z Caused by: java.io.IOException: error=13, Permission denied
2020-03-02T05:13:55.8278228Z 	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)
{code}",,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11329: [FLINK-16374] Ignore unstable rocksdb state processing api test
URL: https://github.com/apache/flink/pull/11329
 
 
   ## What is the purpose of the change
   
   The RocksDBStateBackendReaderKeyedStateITCase causes RocksDB to crash during test execution, making builds unstable.
   While we are investigating the issue, I hereby propose to ignore the test.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/20 08:07;githubbot;600","rmetzger commented on pull request #11355: [FLINK-16374][AZP] Disable java e2e tests in misc profile
URL: https://github.com/apache/flink/pull/11355
 
 
   ## What is the purpose of the change
   
   Disable java e2e tests in misc profile because the java e2e tests are executed in the e2e profile
   
   
   ## Brief change log
   
   - There is already a condition in the travis watchdog to check if we are on AZP or Travis. This change expands the AZP case to exclude anything e2e test execution related.
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/20 20:43;githubbot;600","rmetzger commented on pull request #11355: [FLINK-16374][AZP] Disable java e2e tests in misc profile
URL: https://github.com/apache/flink/pull/11355
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/20 14:50;githubbot;600",,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 11 14:51:06 UTC 2020,,,,,,,,,,"0|z0c2c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/20 13:44;trohrmann;Another instance: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5785&view=logs&j=25197a20-5964-5b06-5716-045f87dc0ea9&t=0c53f4dc-c81e-5ebb-13b2-08f1994a2d32&l=31034;;;","08/Mar/20 09:35;rmetzger;Another instance [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6027&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=27d1d645-cbce-54e2-51c4-d8b45fe24607];;;","08/Mar/20 10:17;rmetzger;The test failures are most likely caused by the docker / custom build agent setup, because they are executed on the e2e test profile as well, and we have never observed a failure there.

 

I propose to not further debug this issue, and disable the execution of the java end to end tests in the misc profile.;;;","11/Mar/20 14:51;rmetzger;Resolved in 091d1d38f7f2c9ec0c97606682486d777758041b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EmbeddedLeaderService: IllegalStateException: The RPC connection is already closed,FLINK-16373,13288830,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,rmetzger,rmetzger,02/Mar/20 10:36,02/Apr/20 09:01,13/Jul/23 08:07,02/Apr/20 09:01,1.11.0,,,,,1.10.1,1.11.0,1.9.3,,Runtime / Coordination,,,,,0,pull-request-available,,,,"In our CI system, I see a lot of these error messages:
{code}
09:52:41,108 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Allocated slot for fd36e49b9105838fc7fb8fff442ade28.
09:52:41,108 [mini-cluster-io-thread-14] WARN  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService  - Error notifying leader listener about new leader
java.lang.IllegalStateException: The RPC connection is already closed
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:195)
	at org.apache.flink.runtime.registration.RegisteredRpcConnection.start(RegisteredRpcConnection.java:90)
	at org.apache.flink.runtime.taskexecutor.JobLeaderService$JobManagerLeaderListener.notifyLeaderAddress(JobLeaderService.java:334)
	at org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService$NotifyOfLeaderCall.run(EmbeddedLeaderService.java:515)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
09:52:41,109 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.JobLeaderService        - Resolved JobManager address, beginning registration
{code}

Example cases 
- https://transfer.sh/BAzbF/20200302.17.tar.gz
- https://transfer.sh/8344E/20200302.19.tar.gz
",,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #11313: [FLINK-16373] Make JobManagerLeaderListener thread safe
URL: https://github.com/apache/flink/pull/11313
 
 
   ## What is the purpose of the change
   
   The JobManagerLeaderListener used by the JobLeaderService was not thread safe. Stopping the listener
   while notifying a new leader could lead to an IllegalStateException where the rpcConnection which
   was supposed to be started was concurrently closed by the stop call.
   
   ## Verifying this change
   
   * Added `JobLeaderServiceTest.handlesConcurrentJobAdditionsAndLeaderChanges`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/20 17:05;githubbot;600","tillrohrmann commented on pull request #11313: [FLINK-16373] Make JobManagerLeaderListener thread safe
URL: https://github.com/apache/flink/pull/11313
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/20 15:50;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/20 15:20;rmetzger;20200302.17.tar.gz;https://issues.apache.org/jira/secure/attachment/12995475/20200302.17.tar.gz","03/Mar/20 15:20;rmetzger;20200302.19.tar.gz;https://issues.apache.org/jira/secure/attachment/12995474/20200302.19.tar.gz",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 09:01:24 UTC 2020,,,,,,,,,,"0|z0c2b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/20 15:57;chesnay;Please list which tests have failed.;;;","02/Mar/20 17:00;rmetzger;No tests have failed. But I wonder if this error message indicates a problem that we should address.

Some logs had over 100 instances of these messages;;;","03/Mar/20 13:41;chesnay;Let's rephrase my request: Please list during which test runs this was printed.;;;","03/Mar/20 14:46;rmetzger;Looks I've been making a false statement: This log: [https://transfer.sh/8344E/20200302.19.tar.gz] is from this build: [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5807&view=logs&j=41cba0bb-1271-5adb-01cc-4768f26a8311&t=97bfbae5-9730-50fb-c7f7-7ec78a126729]... which has failed;;;","03/Mar/20 14:47;rmetzger;[https://transfer.sh/BAzbF/20200302.17.tar.gz] + [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5805&view=logs&j=636f54dd-dda5-5b4b-f495-2d92ec493b6c&t=6c30efdf-a92a-5da3-9a6a-004c8552b2df]

... and this one is successful;;;","03/Mar/20 15:10;chesnay;Transfer.sh files are no longer available.

It is exactly because of this limited availability that we usually write detailed information into the JIRA instead of linking to the artifacts. As of right no one could take targeted approach to this issue, except you in case you still have the artifacts locally.;;;","03/Mar/20 15:21;rmetzger;I just uploaded the artifacts to the ticket.
;;;","03/Mar/20 17:17;chesnay;What happens is that multiple slots are requested in quick succession, causing multiple calls to {{JobLeaderService#addJob}} in {{TaskExecutor#requestSlot}} for the same job ID.
This results in prior {{RegisteredRpcConnections}} being closed, which lead to the exception once the JM responds to said request.

One solution would be to guard the job addition with a simple check:
{{code:java}}
// a previous slot request may have kicked off the connection establishment
if (!jobLeaderService.containsJob(jobId)) {
	jobLeaderService.addJob(jobId, targetAddress);
}
{{code}}

[~trohrmann] WDYT?;;;","04/Mar/20 15:29;trohrmann;I don't think that this is the solution since every new slot request could update the target address of a changing {{JobMaster}}. Instead, the problem seems to be that we are executing the {{EmbeddedLeaderService#NotifyOfLeaderCall}} even though the respective leader retrieval service has been closed.;;;","04/Mar/20 16:02;trohrmann;I think the underlying problem is that {{JobManagerLeaderListener}} is not thread safe.;;;","18/Mar/20 15:50;trohrmann;Fixed via cf7cc89ac99554dc51da5cbffc3b76fe32ef5fea;;;","18/Mar/20 15:52;rmetzger;Thanks a lot :) ;;;","02/Apr/20 09:01;trohrmann;Fixed via

1.10.1: 15dd5fec6a8cad358ba69ab964bf6469b6868ab2
1.9.3: f8852e8f764ee91b0b713bb9c90f2f170096e553;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopCompressionBulkWriter fails with 'java.io.NotSerializableException',FLINK-16371,13288737,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zenfenan,zenfenan,zenfenan,02/Mar/20 10:17,10/Mar/20 11:49,13/Jul/23 08:07,10/Mar/20 11:49,1.10.0,,,,,1.10.1,1.11.0,,,Connectors / FileSystem,,,,,1,pull-request-available,,,,"When using CompressWriterFactory with Hadoop compression codec, the execution fails with java.io.NotSerializableException. 

I guess this is probably to do with the the instance creation for Hadoop's CompressionCodec being done here at [CompressWriterFactory.java#L59|https://github.com/apache/flink/blob/master/flink-formats/flink-compress/src/main/java/org/apache/flink/formats/compress/CompressWriterFactory.java#L59] and thus it has to be sent over the wire causing the exception to be thrown.

So I did a quick test on my end by changing the way the CompressionCodec is initialised and ran it on a Hadoop cluster, and it has been working just fine. Will raise a PR in a day or so.",,aljoscha,eskabetxe,f.pompermaier,kkl0u,zenfenan,,,,,,,,,,,,,,,,,"zenfenan commented on pull request #11307: [FLINK-16371] [BulkWriter] Fix Hadoop Compression BulkWriter
URL: https://github.com/apache/flink/pull/11307
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fixes the NotSerializableException which is thrown when CompressWriterFactory is created with Hadoop compression provided.
   
   
   ## Brief change log
   
    - FLINK-13634 introduces Hadoop Compression based `BulkWriter` implementation. However, it takes an instance of Hadoop `CompressionCodec` which causes java.io.NotSerializableException to be thrown since this object has to be sent over the wire when the overriden `create()` method gets called.
    - This PR changes the way the CompressionCodec instance is created i.e, it gets initialised lazily and inside the `create()` method.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     - Updated the test cases to only use the String representation of a Compression Codec i.e. codec name or alias, instead of an object.
     - Added test that validates that an exception should be thrown when a non-existing codec name is provided
     - Manually verified the change on a cluster with a streaming pipeline.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/20 12:07;githubbot;600","kl0u commented on pull request #11307: [FLINK-16371] [BulkWriter] Fix Hadoop Compression BulkWriter
URL: https://github.com/apache/flink/pull/11307
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Mar/20 11:46;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 10 11:49:47 UTC 2020,,,,,,,,,,"0|z0c29s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/20 11:49;kkl0u;Merged on master with 79de2ea5ab64de03b46e8ad6a0df3bbde986d124
and on release-1.10 with ea5197eeefcf72eb9aa33ad83e591faf856bbda9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-dist bundles ZK 3.5 as JDK11-exclusive dependency,FLINK-16370,13288731,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,rmetzger,rmetzger,02/Mar/20 10:02,02/Apr/20 10:38,13/Jul/23 08:07,03/Mar/20 13:18,1.11.0,,,,,1.11.0,,,,Build System,,,,,0,pull-request-available,test-stability,,,"This is the output from the CI system (https://travis-ci.org/apache/flink/jobs/656931001)
{code}
16:35:30.798 [ERROR] testKillYarnSessionClusterEntrypoint(org.apache.flink.yarn.YARNHighAvailabilityITCase)  Time elapsed: 10.363 s  <<< ERROR!
org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn session cluster
	at org.apache.flink.yarn.YARNHighAvailabilityITCase.deploySessionCluster(YARNHighAvailabilityITCase.java:296)
	at org.apache.flink.yarn.YARNHighAvailabilityITCase.lambda$testKillYarnSessionClusterEntrypoint$0(YARNHighAvailabilityITCase.java:165)
	at org.apache.flink.yarn.YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint(YARNHighAvailabilityITCase.java:157)
Caused by: org.apache.flink.yarn.YarnClusterDescriptor$YarnDeploymentException: 
The YARN application unexpectedly switched to state FAILED during deployment. 
Diagnostics from YARN: Application application_1583080501498_0002 failed 2 times in previous 10000 milliseconds due to AM Container for appattempt_1583080501498_0002_000002 exited with  exitCode: 1
Failing this attempt.Diagnostics: Exception from container-launch.
Container id: container_1583080501498_0002_02_000001
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972)
	at org.apache.hadoop.util.Shell.run(Shell.java:869)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)

... snip ...

16:44:14.840 [INFO] Results:
16:44:14.840 [INFO] 
16:44:14.840 [ERROR] Errors: 
16:44:14.840 [ERROR]   YARNHighAvailabilityITCase.testJobRecoversAfterKillingTaskManager:187->YarnTestBase.runTest:242->lambda$testJobRecoversAfterKillingTaskManager$1:191->deploySessionCluster:296 Â» ClusterDeployment
16:44:14.840 [ERROR]   YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint:157->YarnTestBase.runTest:242->lambda$testKillYarnSessionClusterEntrypoint$0:165->deploySessionCluster:296 Â» ClusterDeployment
16:44:14.840 [INFO] 
16:44:14.840 [ERROR] Tests run: 25, Failures: 0, Errors: 2, Skipped: 4
{code}

Digging deeper into the problem, this seems to be the root cause:
{code}
2020-03-01 16:35:14,444 INFO  org.apache.flink.shaded.curator4.org.apache.curator.utils.Compatibility [] - Using emulated InjectSessionExpiration
2020-03-01 16:35:14,466 WARN  org.apache.flink.shaded.curator4.org.apache.curator.CuratorZookeeperClient [] - session timeout [1000] is less than connection timeout [15000]
2020-03-01 16:35:14,491 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting YarnSessionClusterEntrypoint down with application status FAILED. Diagnostics java.lang.NoSuchMethodError: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.quorum.flexible.QuorumMaj.<init>(Ljava/util/Map;)V
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.EnsembleTracker.<init>(EnsembleTracker.java:57)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.<init>(CuratorFrameworkImpl.java:159)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:165)
	at org.apache.flink.runtime.util.ZooKeeperUtils.startCuratorFramework(ZooKeeperUtils.java:138)
	at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:128)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:305)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:263)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:207)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518)
	at org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.main(YarnSessionClusterEntrypoint.java:80)
.
2020-03-01 16:35:14,502 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2020-03-01 16:35:14,512 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
2020-03-01 16:35:14,514 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
2020-03-01 16:35:14,548 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
2020-03-01 16:35:14,604 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2020-03-01 16:35:14,592 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Could not start cluster entrypoint YarnSessionClusterEntrypoint.
org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint YarnSessionClusterEntrypoint.
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:187) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.main(YarnSessionClusterEntrypoint.java:80) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
Caused by: java.lang.NoSuchMethodError: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.quorum.flexible.QuorumMaj.<init>(Ljava/util/Map;)V
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.EnsembleTracker.<init>(EnsembleTracker.java:57) ~[flink-shaded-zookeeper-3.4.10.jar:3.4.10-10.0]
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.<init>(CuratorFrameworkImpl.java:159) ~[flink-shaded-zookeeper-3.4.10.jar:3.4.10-10.0]
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:165) ~[flink-shaded-zookeeper-3.4.10.jar:3.4.10-10.0]
	at org.apache.flink.runtime.util.ZooKeeperUtils.startCuratorFramework(ZooKeeperUtils.java:138) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:128) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:305) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:263) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:207) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
	at javax.security.auth.Subject.doAs(Subject.java:423) ~[?:?]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	... 2 more
{code}",,rmetzger,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11283: [FLINK-16370][build] Only bundle javax as Java11-exclusive dependency
URL: https://github.com/apache/flink/pull/11283
 
 
   Fixes an issue where ZK 3.5 was bundled in flink-dist as a java11-exclusive dependency.
   
   The underlying issue was a lack of clarity int he build process of flink-dist; some dependencies are downloaded by the `dependency-plugin` to `flink-dist/target/temporary`. In a later step we bundled all dependencies in this directory as java11-exclusive dependencies.
   In the introduction of ZK 3.5 this directory was re-used for ZK (which is bundled in /opt), which led to it's inclusion.
   
   There is now a dedicated directory for dependencies that should be bundled as java11-exclusive dependencies, and various names were adjusted to increase clarity.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Mar/20 13:24;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,FLINK-13417,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 13:18:02 UTC 2020,,,,,,,,,,"0|z0c28g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/20 10:52;chesnay;Where did you find the NoSuchMethodError?;;;","02/Mar/20 10:57;rmetzger;In the files uploaded to transfer.sh, in 42719.25/yarn-tests/container_1583080501498_0001_01_000001/jobmanager.log;;;","03/Mar/20 13:18;chesnay;master: dfd79bce78a16af9b5a534f1e9539c530979ba58;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 connector on hive 2.0.1 don't  support type conversion from STRING to VARCHAR,FLINK-16360,13288616,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,wgcn,wgcn,02/Mar/20 06:06,02/Mar/20 10:00,13/Jul/23 08:07,02/Mar/20 10:00,1.10.0,,,,,1.10.1,1.11.0,,,Connectors / Hive,,,,,0,pull-request-available,,,," it threw  exception  when we query hive 2.0.1 by flink 1.10.0

 Exception stack：

org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=50, backoffTimeMS=10000)
 at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110)
 at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:186)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:180)
 at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:484)
 at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380)
 at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
 at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
 at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
 at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
 at akka.actor.ActorCell.invoke(ActorCell.scala:561)
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
 at akka.dispatch.Mailbox.run(Mailbox.scala:225)
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
 at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:76)
 at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:123)
 at org.apache.flink.orc.OrcSplitReader.<init>(OrcSplitReader.java:73)
 at org.apache.flink.orc.OrcColumnarRowSplitReader.<init>(OrcColumnarRowSplitReader.java:55)
 at org.apache.flink.orc.OrcSplitReaderUtil.genPartColumnarRowReader(OrcSplitReaderUtil.java:96)
 at org.apache.flink.connectors.hive.read.HiveVectorizedOrcSplitReader.<init>(HiveVectorizedOrcSplitReader.java:65)
 at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:117)
 at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:56)
 at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:85)
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
 at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:196)
Caused by: java.lang.reflect.InvocationTargetException
 at sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.commons.lang3.reflect.MethodUtils.invokeExactMethod(MethodUtils.java:204)
 at org.apache.commons.lang3.reflect.MethodUtils.invokeExactMethod(MethodUtils.java:165)
 at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:74)
 ... 11 more
Caused by: java.io.IOException: ORC does not support type conversion from STRING to VARCHAR
 at org.apache.hadoop.hive.ql.io.orc.SchemaEvolution.validateAndCreate(SchemaEvolution.java:96)
 at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:255)
 at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:79)
 at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$Builder.build(RecordReaderImpl.java:236)
 at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:680)
 ... 17 more","os:centos
java: 1.8.0_92

flink :1.10.0

hadoop: 2.7.2

hive:2.0.1

 ",lzljs3620320,wgcn,,,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #11277: [FLINK-16360][orc] Flink STRING data type should map to ORC STRING type
URL: https://github.com/apache/flink/pull/11277
 
 
   
   ## What is the purpose of the change
   
   Hive 2.0 ORC not support schema evolution from STRING to VARCHAR.
   We need produce STRING in ORC for VarcharType(MAX_LENGHT) in Flink.
   
   ## Brief change log
   
   Flink STRING data type should map to ORC STRING type in `OrcSplitReaderUtil`
   
   ## Verifying this change
   
   `OrcSplitReaderUtilTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Mar/20 06:57;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/20 05:52;wgcn;exceptionstack;https://issues.apache.org/jira/secure/attachment/12995092/exceptionstack",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 10:00:41 UTC 2020,,,,,,,,,,"0|z0c220:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/20 06:16;lzljs3620320;Thanks [~wgcn] for reporting, Hive 2.0 ORC not support schema evolution from STRING to VARCHAR.

We need produce STRING in ORC for VarcharType(MAX_LENGHT) in Flink.;;;","02/Mar/20 10:00;lzljs3620320;master: d9b9aad6c92bc4f65bae04821a041082b3ed6cf8

release-1.10: ec4155a298eaf3012bbbb139082713e86a4e60b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing HashMap to LinkedHashMap for deterministic iterations in ExpressionTest,FLINK-16352,13288476,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,testfixer,testfixer,testfixer,01/Mar/20 00:40,02/Mar/20 06:14,13/Jul/23 08:07,02/Mar/20 06:13,,,,,,1.11.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"The test `testValueLiteralString` in `ExpressionTest` may fail due if `HashMap` iterates in a different order. The final variable `map` is a `HashMap`. However, `HashMap` does not guarantee any specific order of entries. Thus, the test can fail due to a different iteration order.",,jark,testfixer,,,,,,,,,,,,,,,,,,,,"testfixer commented on pull request #11269: [FLINK-16352][flink-table/flink-table-common]Use LinkedHashMap for deterministic iterations
URL: https://github.com/apache/flink/pull/11269
 
 
   The test `testValueLiteralString` in `ExpressionTest` may fail due if `HashMap` iterates in a different order. The final variable `map` is a `HashMap`. However, `HashMap` does not guarantee any specific order of entries. Thus, the test can fail due to a different iteration order.
   
   ## What is the purpose of the change
   
   In this PR, we propose to fix [FLINK-16352] by making the test assertions more stable by considering any order of result.
   
   ## Brief change log
   
   In `flink-table/flink-table-common/src/test/java/org/apache/flink/table/expressions/ExpressionTest.java`, we use `LinkedHashMap` instead of `HashMap` in `testValueLiteralString`.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (don't know)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Mar/20 00:48;githubbot;600","wuchong commented on pull request #11269: [FLINK-16352][flink-table/flink-table-common]Use LinkedHashMap for deterministic iterations
URL: https://github.com/apache/flink/pull/11269
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Mar/20 06:12;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 06:13:33 UTC 2020,,,,,,,,,,"0|z0c16w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/20 06:13;jark;Fixed in master(1.11.0): da7a6888cbee26f3e7ebc4957ea8d9993c0b53f8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlobsCleanupITCase.testBlobServerCleanupCancelledJob fails on Travis,FLINK-16346,13288330,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,trohrmann,trohrmann,28/Feb/20 16:29,15/Dec/21 01:40,13/Jul/23 08:07,10/Sep/21 11:11,1.11.0,1.12.5,,,,1.10.2,1.11.0,1.12.8,1.9.4,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"{{BlobsCleanupITCase.testBlobServerCleanupCancelledJob}} and {{BlobsCleanupITCase.testBlobServerCleanupFinishedJob}} fails on Travis with 

{code}
12:22:22.314 [ERROR] testBlobServerCleanupCancelledJob(org.apache.flink.runtime.jobmanager.BlobsCleanupITCase)  Time elapsed: 30.037 s  <<< FAILURE!
java.lang.AssertionError: Timeout while waiting for /tmp/junit226171740428505862/junit6248655677631371057/blobStore-0eb47b50-41ae-4c3c-885e-4ee944b8ef38 to become empty. Current contents: [job_bf8d556f932a0d225c1dcc60f1665a26]
	at org.apache.flink.runtime.jobmanager.BlobsCleanupITCase.waitForEmptyBlobDir(BlobsCleanupITCase.java:276)
	at org.apache.flink.runtime.jobmanager.BlobsCleanupITCase.testBlobServerCleanup(BlobsCleanupITCase.java:233)
	at org.apache.flink.runtime.jobmanager.BlobsCleanupITCase.testBlobServerCleanupCancelledJob(BlobsCleanupITCase.java:141)

12:22:22.314 [ERROR] testBlobServerCleanupFinishedJob(org.apache.flink.runtime.jobmanager.BlobsCleanupITCase)  Time elapsed: 30.098 s  <<< FAILURE!
java.lang.AssertionError: Timeout while waiting for /tmp/junit226171740428505862/junit6248655677631371057/blobStore-0eb47b50-41ae-4c3c-885e-4ee944b8ef38 to become empty. Current contents: [job_bf8d556f932a0d225c1dcc60f1665a26]
	at org.apache.flink.runtime.jobmanager.BlobsCleanupITCase.waitForEmptyBlobDir(BlobsCleanupITCase.java:276)
	at org.apache.flink.runtime.jobmanager.BlobsCleanupITCase.testBlobServerCleanup(BlobsCleanupITCase.java:233)
	at org.apache.flink.runtime.jobmanager.BlobsCleanupITCase.testBlobServerCleanupFinishedJob(BlobsCleanupITCase.java:133)
{code}

https://api.travis-ci.com/v3/job/292161105/log.txt",,dwysakowicz,rmetzger,trohrmann,xtsong,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11753: [FLINK-16346][tests] Use fixed JobIDs
URL: https://github.com/apache/flink/pull/11753
 
 
   I can't reproduce the issue locally (and will close the JIRA accordingly).
   
   The goal of this PR is to slightly ease debugging by clarifying which test run the leaked files belong to; this is currently not clear.
   If it occurs again we at least know whether the successful/canceled job leaked them (or maybe even the successful test runs, who knows).
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/20 11:09;githubbot;600","zentol commented on pull request #11753: [FLINK-16346][tests] Use fixed JobIDs
URL: https://github.com/apache/flink/pull/11753
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/20 07:58;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 10 11:11:54 UTC 2021,,,,,,,,,,"0|z0c0ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/20 07:58;chesnay;Debugging was eased in ee9ea10eff2c8665bc27208d5141487c0f4c3389 in case it occurs again.;;;","30/Apr/20 18:03;rmetzger;Reopening, as it happened again: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=482&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d;;;","06/May/20 16:17;trohrmann;[~chesnay] could you take another look with the now hopefully more detailed logs?;;;","07/May/20 10:12;chesnay;This failure is a separate issue.

There are 2 problems here:
* The Job to be canceled also uses the FailingBlockingInvokable, but we never actually want the Invokable to throw an exception. It should just use a BlockingNoOpInvokable.
* The FailingBlockingInvokable uses static state to control the behavior; if you don't fork the JVM this can result in unexpected behavior in subsequent tests. I believe this is what is happening here; the failing test runs first, sets the blocking flag to false, then the cancel test runs and goes straight to throwing the exception instead of blocking.

;;;","11/May/20 07:21;chesnay;master: 477f39fd5127592b8925e5715d15cb823c1318a3
1.10: e346215edcf2252cc60c5cef507ea77ce2ac9aca
1.9: 36440fb938b50aecd2c8de607806f381cc2af587;;;","01/Sep/21 15:09;dwysakowicz;Reappeared in 1.12: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23291&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0&l=8205;;;","02/Sep/21 01:45;xtsong;1.12: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23344&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=fd9796c3-9ce8-5619-781c-42f873e126a6&l=6910;;;","08/Sep/21 02:46;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23719&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=19336553-69ec-5b03-471a-791a483cced6&l=7042;;;","09/Sep/21 06:14;chesnay;You've gotta be kidding me, why is this reappearing after a _year_?!?;;;","09/Sep/21 06:30;chesnay;We seem to be loosing 5 minutes for some reason. I'll just crank up the cleanup timeout to infinity.
{code:java}
22:50:50,653 [flink-akka.actor.default-dispatcher-3] INFO  ...
22:55:50,532 [flink-akka.actor.default-dispatcher-63] INFO  ...
{code}

;;;","10/Sep/21 09:53;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23897&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0&l=8205;;;","10/Sep/21 11:11;chesnay;1.12: https://github.com/apache/flink/commit/d2490948e4b69cb42d48c5518c85402cb2ccae88;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Computed column can not refer time attribute column ,FLINK-16345,13288322,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,leonard,leonard,28/Feb/20 16:00,16/Apr/20 04:28,13/Jul/23 08:07,22/Mar/20 09:43,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"If a computed column refer a time attribute column, computed column will lose  time attribute and cause validation fail.
{code:java}
CREATE TABLE orders (
  order_id STRING,
  order_time TIMESTAMP(3),
  amount DOUBLE,
  amount_kg as amount * 1000,
  // can not select computed column standard_ts which from column order_time that used as WATERMARK
  standard_ts as order_time + INTERVAL '8' HOUR,
  WATERMARK FOR order_time AS order_time
) WITH (
  'connector.type' = 'kafka',
  'connector.version' = '0.10',
  'connector.topic' = 'flink_orders',
  'connector.properties.zookeeper.connect' = 'localhost:2181',
  'connector.properties.bootstrap.servers' = 'localhost:9092',
  'connector.properties.group.id' = 'testGroup',
  'connector.startup-mode' = 'earliest-offset',
  'format.type' = 'json',
  'format.derive-schema' = 'true'
);

{code}
The query `select amount_kg from orders` runs normally,  

the` he query `select standard_ts from orders` throws a validation exception message as following:
{noformat}
[ERROR] Could not execute SQL statement. Reason:
 java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
 validated type:
 RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" order_id, TIME ATTRIBUTE(ROWTIME) order_time, DOUBLE amount, DOUBLE amount_kg, TIMESTAMP(3) ts) NOT NULL
 converted type:
 RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" order_id, TIME ATTRIBUTE(ROWTIME) order_time, DOUBLE amount, DOUBLE amount_kg, TIME ATTRIBUTE(ROWTIME) ts) NOT NULL
 rel:
 LogicalProject(order_id=[$0], order_time=[$1], amount=[$2], amount_kg=[$3], ts=[$4])
 LogicalWatermarkAssigner(rowtime=[order_time], watermark=[$1])
 LogicalProject(order_id=[$0], order_time=[$1], amount=[$2], amount_kg=[*($2, 1000)], ts=[+($1, 28800000:INTERVAL HOUR)])
 LogicalTableScan(table=[[default_catalog, default_database, orders, source: [Kafka010TableSource(order_id, order_time, amount)]]])
 {noformat}
 

 ",,danny0405,godfreyhe,hazy,jark,leonard,libenchao,twalthr,xingoo,,,,,,,,,,,,,,"wuchong commented on pull request #11424: [FLINK-16345][table-planner-blink] Fix computed column can not refer row time attribute column
URL: https://github.com/apache/flink/pull/11424
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   If a computed column refers a rowtime column (the column has been defined watermark), a query on the table will be failed with the following exception:
   
   ```
   [ERROR] Could not execute SQL statement. Reason:
    java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
    validated type:
    RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" order_id, TIME ATTRIBUTE(ROWTIME) order_time, DOUBLE amount, DOUBLE amount_kg, TIMESTAMP(3) ts) NOT NULL
    converted type:
    RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" order_id, TIME ATTRIBUTE(ROWTIME) order_time, DOUBLE amount, DOUBLE amount_kg, TIME ATTRIBUTE(ROWTIME) ts) NOT NULL
    rel:
    LogicalProject(order_id=[$0], order_time=[$1], amount=[$2], amount_kg=[$3], ts=[$4])
    LogicalWatermarkAssigner(rowtime=[order_time], watermark=[$1])
    LogicalProject(order_id=[$0], order_time=[$1], amount=[$2], amount_kg=[*($2, 1000)], ts=[+($1, 28800000:INTERVAL HOUR)])
    LogicalTableScan(table=[[default_catalog, default_database, orders, source: [Kafka010TableSource(order_id, order_time, amount)]]])
   ```
   
   
   ## Brief change log
   
   - erase time indicators before translate `CatalogSourceTable` into `RelNode`s in `CatalogSourceTable#toRel`.
   
   ## Verifying this change
   
   - added a unit plan test to reproduce this problem and verify the changes.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/20 03:46;githubbot;600","wuchong commented on pull request #11424: [FLINK-16345][table-planner-blink] Fix computed column can not refer row time attribute column
URL: https://github.com/apache/flink/pull/11424
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Mar/20 07:35;githubbot;600","wuchong commented on pull request #11476: [FLINK-16345][table-planner-blink] Fix computed column can not refer row time attribute column
URL: https://github.com/apache/flink/pull/11476
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   If a computed column refers a rowtime column (the column has been defined watermark), a query on the table will be failed with the following exception:
   
   ```
   [ERROR] Could not execute SQL statement. Reason:
    java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
    validated type:
    RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" order_id, TIME ATTRIBUTE(ROWTIME) order_time, DOUBLE amount, DOUBLE amount_kg, TIMESTAMP(3) ts) NOT NULL
    converted type:
    RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" order_id, TIME ATTRIBUTE(ROWTIME) order_time, DOUBLE amount, DOUBLE amount_kg, TIME ATTRIBUTE(ROWTIME) ts) NOT NULL
    rel:
    LogicalProject(order_id=[$0], order_time=[$1], amount=[$2], amount_kg=[$3], ts=[$4])
    LogicalWatermarkAssigner(rowtime=[order_time], watermark=[$1])
    LogicalProject(order_id=[$0], order_time=[$1], amount=[$2], amount_kg=[*($2, 1000)], ts=[+($1, 28800000:INTERVAL HOUR)])
    LogicalTableScan(table=[[default_catalog, default_database, orders, source: [Kafka010TableSource(order_id, order_time, amount)]]])
   ```
   
   
   ## Brief change log
   
   - erase time indicators before translate `CatalogSourceTable` into `RelNode`s in `CatalogSourceTable#toRel`.
   
   ## Verifying this change
   
   - added a unit plan test to reproduce this problem and verify the changes.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Mar/20 07:41;githubbot;600","wuchong commented on pull request #11476: [FLINK-16345][table-planner-blink] Fix computed column can not refer row time attribute column
URL: https://github.com/apache/flink/pull/11476
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Mar/20 09:42;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 04:28:50 UTC 2020,,,,,,,,,,"0|z0c0a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Feb/20 08:33;libenchao;[~Leonard Xu] Thanks for the reporting, it's indeed a bug.

In {{CatalogTableSource.toRel}} we applied computed column, and the result type differs with the rowType of {{CatalogTableSource}}.
IMO, before we apply `convertToRexNodes`, we can remove time properties from the rowType, because we'll put add watermark later anyway which will add time attribute.;;;","02/Mar/20 05:58;jark;{{standard_ts}} is not a rowtime attribute, I think the bug is somewhere regard {{standard_ts}} as a rowtime attribute. ;;;","16/Mar/20 11:53;twalthr;[~jark] to me this seems like a regular bug. Can we downgrade it to ""Major"" instead of ""Critical""?;;;","17/Mar/20 05:11;danny0405;I think the right place to fix is the SqlToOperationConverter#createTableSchema, before we make the computed column type inference, the time attribute should be patched up based on the watermark definitions.

The logic seems a little tricky, from the DDL definition, the watermark definition is ""after"" the computed column, so should we translate the computed column with time attributes or with normal types ?

From the logical plan generation, we patched the time attributes to the table source, thus the computed column translation works based on that, but from the DDL, it seems there is no need the column ""standard_ts"" must be a time attribute column.;;;","17/Mar/20 05:57;jark;Hi [~danny0405], I guess you may misunderstood this issue. Here I agree with [~libenchao]'s way which is also the PR I submitted. 

{{SqlToOperationConverter#createTableSchema}} does all the things right, it's just a encapsulation of the DDL information. It's not the cause of this issue. The root cause is we are using {{CatalogSourceTable#rowType}} to generate RexNodes for computed column. However, {{CatalogSourceTable#rowType}} is patched with time indicators, so {{order_time}} and {{order_time + INTERVAL '8' HOUR}} are both ROWTIME TIMESTAMP according to the Calcite's type inference. 

The {{CatalogSourceTable#rowType}} contains rowtime indicator is also correct, becuase it represents the whole row type of the DDL (including watermarks). The bug indeed is that RexNode generation uses  {{CatalogSourceTable#rowType}}. The responsibility of {{CatalogSourceTable#toRel}} is converting DDL into 3 RelNodes: TableScan, Project, and Watermark. So before watermark node, there shouldn't be any rowtime indicators. That's why I think the erasion of time indicator happens in {{CatalogSourceTable#toRel}}.

Another idea is building a new rowType using the {{catalogTable#getSchema}} which doesn't contain rowtime indicators. In this way, we will not have something generates first then remove.


;;;","19/Mar/20 03:10;danny0405;Let me re-represent the cause of the issue in detail:

Assumes we have a DDL

{code:sql}
create table t(
  a timestamp,
  b a - INTERVAL '5' HOUR,
  WATERMARK FOR a AS a
)
{code}

In SqlToOperationConverter, during TableSchema generation, when we do a type inference for column b, we actually recard column a as a normal
timestamp type column, so the result type of b is also a normal timestamp.[A] So the generated schema is:

--
  +- a: TIMESTAMP
  +- b as a - INTERVAL '5' HOUR: TIMESTAMP

Then we generates the CatalogSchemaTable and patched up time attributes for the row type ROW(a: TIMESTAMP, b:TIMESTAMP),
based on the watermark definition, we think that a is a rowtime, so the row type becomes: ROW(a: RowTimeType, b:TIMESTAMP).

Then in CatalogSourceTable, we generates the node for column b with expression ""a - INTERVAL '5' HOUR"" with patched row type ROW(a: RowTimeType, b:TIMESTAMP), this time we have a inference that the result type is a rowtype.[B]

Basically i think that:
- CatalogSchemaTable and CatalogSourceTable all have correct patched row type as an unexpanded logical table;
- There is no need to do any type erase because 1: There is alreay a tool to do that RelTimeIndicatorConverter(although it does nothing to TableScan) 2: if we want to erase, why we generates it firstly 3: row type is immutable, we generates it once and reuse it everywhere.

The root cause is that logic [A] and [B] are in-consistent while actually they have the same logic. Erase the time attributes may work for this case but seems hacky.;;;","20/Mar/20 06:48;jark;After an offline discussion with [~danny0405], we reached an consensus that erasing time indicators in {{CatalogSourceTable#toRel}} is the only way to fix this for now. So we will keep the current implementation in PR. ;;;","22/Mar/20 07:42;jark;Fixed in 
 - master (1.11.0): 54395d9fcb3ab7b7ad6548e9003e24c24f9081c4
 - 1.10.1: fa181e7759bca90bc83dc9f0eceac0f504386129;;;","10/Apr/20 00:59;xingoo;I fix the code like 1.10.1: fa181e7759bca90bc83dc9f0eceac0f504386129.

But when I use join in streaming, I found this exception:
{code:java}
//代码占位符
Flink SQL> CREATE TABLE kafka_test1 (
>   id varchar,
>   a varchar,
>   b int,
>   ts as PROCTIME()
> ) WITH (
>   'connector.type' = 'kafka',       
>   'connector.version' = '0.11',
>   'connector.topic' = 'test',
>   'connector.properties.zookeeper.connect' = 'localnode2:2181',
>   'connector.properties.bootstrap.servers' = 'localnode2:9092',
>   'connector.properties.group.id' = 'testGroup',
>   'connector.startup-mode' = 'latest-offset',
>   'format.type' = 'json'
> )
> ;
[INFO] Table has been created.


Flink SQL> select a.*,b.* from kafka_test1 a join hbase_test1 FOR SYSTEM_TIME AS OF a.ts as b on a.id = b.rowkey;
{code}
exception:
{code:java}
//代码占位符
[ERROR] Could not execute SQL statement. Reason:
java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" a, INTEGER b, TIMESTAMP(3) NOT NULL ts, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" rowkey, RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" a, INTEGER b) f) NOT NULL
converted type:
RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" a, INTEGER b, TIME ATTRIBUTE(PROCTIME) NOT NULL ts, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" rowkey, RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" a, INTEGER b) f) NOT NULL
rel:
LogicalProject(id=[$0], a=[$1], b=[$2], ts=[$3], rowkey=[$4], f=[$5])
  LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{0, 3}])
    LogicalProject(id=[$0], a=[$1], b=[$2], ts=[PROCTIME()])
      LogicalTableScan(table=[[tgou, collie, kafka_test1, source: [Kafka011TableSource(id, a, b)]]])
    LogicalFilter(condition=[=($cor1.id, $0)])
      LogicalSnapshot(period=[$cor1.ts])
        LogicalTableScan(table=[[tgou, collie, hbase_test1, source: [HBaseTableSource[schema=[rowkey, f], projectFields=null]]]])
{code}
Is this code can not work with proctime?

 ;;;","10/Apr/20 07:08;jark;[~xingoo], could you try to build release-1.10 branch for yourself and try the query again?;;;","16/Apr/20 00:24;xingoo;Hi, [~jark]. I build the pull request on 1.10.0, and the code is refer to the 1.10.1, but it doesn't work.;;;","16/Apr/20 04:28;jark;Hi [~xingoo], sorry, I couldn't reproduce your problem using your code even on 1.10.0. Is that exactly the code you are using? It's quite simple and shouldn't be a problem. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove source licenses for old WebUI,FLINK-16331,13288257,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,28/Feb/20 11:03,31/Mar/20 16:24,13/Jul/23 08:07,28/Feb/20 16:50,1.10.0,,,,,1.10.1,1.11.0,,,Release System,,,,,0,pull-request-available,,,,"When we removed the old WebUI we only removed the licenses from flink-runtime-web, but missed the ones for the source release.",,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11251: [FLINK-16331][legal] Remove source licenses for old WebUI
URL: https://github.com/apache/flink/pull/11251
 
 
   With the removal of the WebUI we can drop the licenses from the source release.
   
   The only remaining licenses are cloudpickle/py4j for the python api, and font-awesome for the documentation.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Feb/20 11:06;githubbot;600","zentol commented on pull request #11251: [FLINK-16331][legal] Remove source licenses for old WebUI
URL: https://github.com/apache/flink/pull/11251
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Feb/20 16:49;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,FLINK-14984,,FLINK-16888,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 16:50:32 UTC 2020,,,,,,,,,,"0|z0bzvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/20 16:50;chesnay;master: f3a26f179ea5077eab7a763a7e83daba1ca67390

1.10: 9ac337f002138848e287f7c510384cf14e796240 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PersistedTable and AppendingBufferAccessor use different type system then Persisted Value,FLINK-16318,13288084,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igal,sjwiesman,sjwiesman,27/Feb/20 15:24,28/Feb/20 01:24,13/Jul/23 08:07,28/Feb/20 01:24,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,In FlinkState PersisteValue is backed by a value state whose type information is determined by DynamicallyRegisteredTypes. AppendingBufferAccessor and PersistedTable use List and Map states whose state descriptors use classes which may return a different type information. ,,sjwiesman,tzulitai,,,,,,,,,,,,,,,,,,,,"igalshilman commented on pull request #40: [FLINK-16318] Use DynamicallyRegisteredTypes for PersistedTable and PersistedBuffer
URL: https://github.com/apache/flink-statefun/pull/40
 
 
   This PR fixes a bug where the `PersistedAppendingBuffer` and the `PersistedTable`, didn't use statefun provided `TypeInformation`.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 16:06;githubbot;600","tzulitai commented on pull request #40: [FLINK-16318] Use DynamicallyRegisteredTypes for PersistedTable and PersistedBuffer
URL: https://github.com/apache/flink-statefun/pull/40
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Feb/20 01:23;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 01:24:58 UTC 2020,,,,,,,,,,"0|z0byt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/20 01:24;tzulitai;Fixed in master via 1700316b70a6190e855ff4ba5a12cb092d264277;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-state-processor-api: surefire execution unstable on Azure,FLINK-16313,13288061,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sjwiesman,rmetzger,rmetzger,27/Feb/20 14:06,07/Mar/20 06:39,13/Jul/23 08:07,06/Mar/20 17:09,1.10.1,1.11.0,,,,1.10.1,1.11.0,,,API / State Processor,Tests,,,,0,pull-request-available,test-stability,,,"Log file: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5686&view=logs&j=41cba0bb-1271-5adb-01cc-4768f26a8311&t=44574c85-1cd0-5978-cccf-f0cf7e87a36a

{code}
2020-02-27T12:36:35.2860111Z [INFO] flink-table-planner ................................ SUCCESS [01:47 min]
2020-02-27T12:36:35.2860966Z [INFO] flink-cep-scala .................................... SUCCESS [  5.041 s]
2020-02-27T12:36:35.2861740Z [INFO] flink-sql-client ................................... SUCCESS [03:00 min]
2020-02-27T12:36:35.2862503Z [INFO] flink-state-processor-api .......................... FAILURE [ 15.394 s]
2020-02-27T12:36:35.2863237Z [INFO] ------------------------------------------------------------------------
2020-02-27T12:36:35.2863587Z [INFO] BUILD FAILURE
2020-02-27T12:36:35.2864071Z [INFO] ------------------------------------------------------------------------
2020-02-27T12:36:35.2864428Z [INFO] Total time: 05:38 min
2020-02-27T12:36:35.2866349Z [INFO] Finished at: 2020-02-27T12:36:35+00:00
2020-02-27T12:36:35.9345815Z [INFO] Final Memory: 147M/2914M
2020-02-27T12:36:35.9347238Z [INFO] ------------------------------------------------------------------------
2020-02-27T12:36:35.9355362Z [WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
2020-02-27T12:36:35.9367919Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (integration-tests) on project flink-state-processor-api_2.11: There are test failures.
2020-02-27T12:36:35.9368804Z [ERROR] 
2020-02-27T12:36:35.9369489Z [ERROR] Please refer to /__w/2/s/flink-libraries/flink-state-processing-api/target/surefire-reports for the individual test results.
2020-02-27T12:36:35.9370249Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
2020-02-27T12:36:35.9370713Z [ERROR] ExecutionException Error occurred in starting fork, check output in log
2020-02-27T12:36:35.9371279Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException Error occurred in starting fork, check output in log
2020-02-27T12:36:35.9372275Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
2020-02-27T12:36:35.9372917Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
2020-02-27T12:36:35.9373498Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)
2020-02-27T12:36:35.9374064Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2020-02-27T12:36:35.9374636Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-02-27T12:36:35.9375344Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-02-27T12:36:35.9376194Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-02-27T12:36:35.9376791Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-02-27T12:36:35.9377375Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-02-27T12:36:35.9377898Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-02-27T12:36:35.9378435Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-02-27T12:36:35.9379063Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-02-27T12:36:35.9379709Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-02-27T12:36:35.9380367Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-02-27T12:36:35.9381007Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-02-27T12:36:35.9381510Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-02-27T12:36:35.9381973Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-02-27T12:36:35.9382404Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-02-27T12:36:35.9382839Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-02-27T12:36:35.9383248Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-02-27T12:36:35.9383661Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-02-27T12:36:35.9384126Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-02-27T12:36:35.9384659Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-02-27T12:36:35.9385145Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2020-02-27T12:36:35.9385606Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-02-27T12:36:35.9386293Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-02-27T12:36:35.9386930Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-02-27T12:36:35.9387471Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-02-27T12:36:35.9388056Z [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: Error occurred in starting fork, check output in log
2020-02-27T12:36:35.9388731Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:622)
2020-02-27T12:36:35.9389289Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2020-02-27T12:36:35.9389864Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)
2020-02-27T12:36:35.9390411Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)
2020-02-27T12:36:35.9390986Z [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-02-27T12:36:35.9391458Z [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-02-27T12:36:35.9391991Z [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-27T12:36:35.9392419Z [ERROR] at java.lang.Thread.run(Thread.java:748)
2020-02-27T12:36:35.9392894Z [ERROR] -> [Help 1]
2020-02-27T12:36:35.9393077Z [ERROR] 
2020-02-27T12:36:35.9393553Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2020-02-27T12:36:35.9394108Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2020-02-27T12:36:35.9394392Z [ERROR] 
2020-02-27T12:36:35.9394713Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2020-02-27T12:36:35.9395211Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
2020-02-27T12:36:35.9395525Z [ERROR] 
2020-02-27T12:36:35.9395889Z [ERROR] After correcting the problems, you can resume the build with the command
2020-02-27T12:36:35.9396511Z [ERROR]   mvn <goals> -rf :flink-state-processor-api_2.11
2020-02-27T12:36:36.2427441Z MVN exited with EXIT CODE: 1.
2020-02-27T12:36:36.2427867Z Trying to KILL watchdog (1633).
{code}
",,azagrebin,klion26,liyu,pnowojski,rmetzger,roman,sjwiesman,trohrmann,tzulitai,,,,,,,,,,,,,"rmetzger commented on pull request #11329: [FLINK-16313] Ignore unstable rocksdb state processing api test
URL: https://github.com/apache/flink/pull/11329
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/20 08:22;githubbot;600","rmetzger commented on pull request #11330: [FLINK-16313] Ignore unstable rocksdb state processing api test
URL: https://github.com/apache/flink/pull/11330
 
 
   ## What is the purpose of the change
   
   The RocksDBStateBackendReaderKeyedStateITCase causes RocksDB to crash during test execution, making builds unstable.
   While we are investigating the issue, I hereby propose to ignore the test.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/20 08:22;githubbot;600","rmetzger commented on pull request #11330: [FLINK-16313] Ignore unstable rocksdb state processing api test
URL: https://github.com/apache/flink/pull/11330
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/20 12:37;githubbot;600","sjwiesman commented on pull request #11335: [FLINK-16313][state-processor-api] Properly dispose of native resources when closing input split
URL: https://github.com/apache/flink/pull/11335
 
 
   ## What is the purpose of the change
   
   FLINK-15014 refactored the `KeyedStateInputFormat` to be more composable. The change did not properly account for the new ownership of the `StateBackend` object and did not dispose of native resources. 
   
   This fix needs to be backported to 1.10
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/20 15:54;githubbot;600","asfgit commented on pull request #11335: [FLINK-16313][state-processor-api] Properly dispose of native resources when closing input split
URL: https://github.com/apache/flink/pull/11335
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/20 17:04;githubbot;600",,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 07 06:39:38 UTC 2020,,,,,,,,,,"0|z0byo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/20 10:12;rmetzger;Another case https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5723&view=logs&j=41cba0bb-1271-5adb-01cc-4768f26a8311&t=97bfbae5-9730-50fb-c7f7-7ec78a126729;;;","02/Mar/20 10:11;rmetzger;Another case, on master: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5807&view=logs&s=0491af50-44b2-5583-ba82-45044b1ef751&j=41cba0bb-1271-5adb-01cc-4768f26a8311 with transfer.sh logs: https://transfer.sh/8344E/20200302.19.tar.gz;;;","04/Mar/20 13:48;trohrmann;Another instance: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5896&view=logs&j=41cba0bb-1271-5adb-01cc-4768f26a8311&t=97bfbae5-9730-50fb-c7f7-7ec78a126729;;;","05/Mar/20 14:08;rmetzger;The jvm is writing a {{.dumpstream}} file with the following contents:

{code}
# Created at 2020-03-05T12:51:24.963
pure virtual method called

# Created at 2020-03-05T12:51:24.964
terminate called without an active exception

# Created at 2020-03-05T12:51:26.982
Aborted (core dumped)
{code}

... investigating further;;;","05/Mar/20 17:07;rmetzger;More information:

From the coredump and gdb, I get the following:
{code}
root@ed674fdc4d9b:/home/test/flink/flink-libraries/flink-state-processing-api/target# gdb $JAVA_HOME/bin/java core.11410
GNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1
Copyright (C) 2016 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type ""show copying""
and ""show warranty"" for details.
This GDB was configured as ""x86_64-linux-gnu"".
Type ""show configuration"" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
<http://www.gnu.org/software/gdb/documentation/>.
For help, type ""help"".
Type ""apropos word"" to search for commands related to ""word""...
Reading symbols from /usr/lib/jvm/java-8-openjdk-amd64/bin/java...(no debugging symbols found)...done.
[New LWP 11468]
[New LWP 11470]
[New LWP 11475]
[New LWP 11476]
[New LWP 11481]
[New LWP 11480]
[New LWP 11486]
[New LWP 11469]
[New LWP 11471]
[New LWP 11472]
[New LWP 11473]
[New LWP 11474]
[New LWP 11478]
[New LWP 11477]
[New LWP 11479]
[New LWP 11482]
[New LWP 11463]
[New LWP 11823]
[New LWP 11460]
[New LWP 11465]
[New LWP 11421]
[New LWP 11483]
[New LWP 11492]
[New LWP 11975]
[New LWP 11821]
[New LWP 11426]
[New LWP 11416]
[New LWP 11417]
[New LWP 11466]
[New LWP 11484]
[New LWP 11418]
[New LWP 11414]
[New LWP 11433]
[New LWP 11424]
[New LWP 11412]
[New LWP 11415]
[New LWP 11462]
[New LWP 11411]
[New LWP 11430]
[New LWP 11429]
[New LWP 11422]
[New LWP 11461]
[New LWP 11978]
[New LWP 11977]
[New LWP 11427]
[New LWP 11488]
[New LWP 11434]
[New LWP 11419]
[New LWP 11428]
[New LWP 11431]
[New LWP 11425]
[New LWP 12072]
[New LWP 11464]
[New LWP 11485]
[New LWP 12136]
[New LWP 11410]
[New LWP 11423]
[New LWP 11420]
[New LWP 11432]
[New LWP 11467]
[New LWP 11413]

warning: Could not load shared library symbols for /tmp/junit1547658367137473582/junit5791642854447555054/rocksdb-lib-b2b47d85aecef16c175b116af00b6d57/librocksdbjni-linux64.so.
Do you need ""set solib-search-path"" or ""set sysroot""?
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
Core was generated by `/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNum'.
Program terminated with signal SIGABRT, Aborted.
#0  0x00007f69aaec6428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
54	../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
[Current thread is 1 (Thread 0x7f68ade2c700 (LWP 11468))]
Installing openjdk unwinder
Traceback (most recent call last):
  File ""/usr/share/gdb/auto-load/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so-gdb.py"", line 52, in <module>
    class Types(object):
  File ""/usr/share/gdb/auto-load/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so-gdb.py"", line 66, in Types
    nmethodp_t = gdb.lookup_type('nmethod').pointer()
gdb.error: No type named nmethod.
(gdb) where
#0  0x00007f69aaec6428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
#1  0x00007f69aaec802a in __GI_abort () at abort.c:89
#2  0x00007f69a960e84d in __gnu_cxx::__verbose_terminate_handler() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#3  0x00007f69a960c6b6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#4  0x00007f69a960c701 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00007f69a960d23f in __cxa_pure_virtual () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00007f678e9110d5 in ?? ()
#7  0x00007f676c055b08 in ?? ()
#8  0x00007f676c053660 in ?? ()
#9  0x00007f68ade2b4af in ?? ()
#10 0x00007f68ade2b4b0 in ?? ()
#11 0x00007f68ade2b4c0 in ?? ()
#12 0x00007f68ade2b730 in ?? ()
#13 0x00007f676c055d38 in ?? ()
#14 0x00007f676c055b58 in ?? ()
#15 0x00007f69aaa6e270 in stack_used () from /lib/x86_64-linux-gnu/libpthread.so.0
#16 0x0000000000000001 in ?? ()
#17 0x00007f68ade2c700 in ?? ()
#18 0x00007f69aa85d5c9 in __free_stacks (limit=41943040) at allocatestack.c:288
#19 queue_stack (stack=<optimized out>) at allocatestack.c:312
#20 __deallocate_stack (pd=<optimized out>) at allocatestack.c:774
#21 __free_tcb (pd=<optimized out>) at pthread_create.c:243
#22 0x0000000000000000 in ?? ()
{code}

Previous runs have also resulted in such files:
{code}
java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:283)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.Reader.read(Reader.java:100)
	at java.util.Scanner.readInput(Scanner.java:804)
	at java.util.Scanner.findWithinHorizon(Scanner.java:1685)
	at java.util.Scanner.hasNextLine(Scanner.java:1500)
	at org.apache.maven.surefire.booter.PpidChecker$ProcessInfoConsumer.execute(PpidChecker.java:354)
	at org.apache.maven.surefire.booter.PpidChecker.unix(PpidChecker.java:190)
	at org.apache.maven.surefire.booter.PpidChecker.isProcessAlive(PpidChecker.java:123)
	at org.apache.maven.surefire.booter.ForkedBooter$2.run(ForkedBooter.java:214)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


# Created at 2020-03-05T15:49:56.955
System.exit() or native command error interrupted process checker.
java.lang.IllegalStateException: error [STOPPED] to read process 15274
	at org.apache.maven.surefire.booter.PpidChecker.checkProcessInfo(PpidChecker.java:145)
	at org.apache.maven.surefire.booter.PpidChecker.isProcessAlive(PpidChecker.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter$2.run(ForkedBooter.java:214)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

I believe I am only able to reproduce this issue within this docker container {{rmetzger/flink-ci:ubuntu-amd64-3528acd}}, and NOT on the azure hosted ubuntu 16.04 machines.
But on a {{CentOS Linux release 7.6.1810}} host machine.;;;","05/Mar/20 18:10;rmetzger;More information

{code}
root@ed674fdc4d9b:/home/test/flink/flink-libraries/flink-state-processing-api/target# jstack -J-d64 $JAVA_HOME/bin/java core.11410
Attaching to core core.11410 from executable /usr/lib/jvm/java-8-openjdk-amd64/bin/java, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 25.242-b08
Deadlock Detection:

No deadlocks found.

Thread 12136: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=215 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(long) @bci=78, line=2078 (Interpreted frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take() @bci=124, line=1093 (Interpreted frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take() @bci=1, line=809 (Interpreted frame)
 - java.util.concurrent.ThreadPoolExecutor.getTask() @bci=149, line=1074 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=26, line=1134 (Interpreted frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)


Thread 12072: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=215 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(long) @bci=78, line=2078 (Interpreted frame)
 - java.util.concurrent.LinkedBlockingQueue.poll(long, java.util.concurrent.TimeUnit) @bci=62, line=467 (Compiled frame)
 - org.apache.flink.shaded.netty4.io.netty.util.concurrent.GlobalEventExecutor.takeTask() @bci=49, line=94 (Interpreted frame)
 - org.apache.flink.shaded.netty4.io.netty.util.concurrent.GlobalEventExecutor$TaskRunner.run() @bci=4, line=247 (Interpreted frame)
 - org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run() @bci=11, line=74 (Interpreted frame)
 - org.apache.flink.shaded.netty4.io.netty.util.concurrent.FastThreadLocalRunnable.run() @bci=4, line=30 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)


Thread 11823: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Interpreted frame)
 - java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=215 (Interpreted frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(long) @bci=78, line=2078 (Interpreted frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take() @bci=124, line=1093 (Interpreted frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take() @bci=1, line=809 (Interpreted frame)
 - java.util.concurrent.ThreadPoolExecutor.getTask() @bci=149, line=1074 (Interpreted frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=26, line=1134 (Interpreted frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)


Thread 11821: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.ForkJoinPool.awaitWork(java.util.concurrent.ForkJoinPool$WorkQueue, int) @bci=350, line=1824 (Interpreted frame)
 - java.util.concurrent.ForkJoinPool.runWorker(java.util.concurrent.ForkJoinPool$WorkQueue) @bci=44, line=1693 (Interpreted frame)
 - java.util.concurrent.ForkJoinWorkerThread.run() @bci=24, line=157 (Interpreted frame)


Thread 11492: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Interpreted frame)
 - java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=215 (Interpreted frame)
 - java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(java.util.concurrent.SynchronousQueue$TransferStack$SNode, boolean, long) @bci=160, line=460 (Interpreted frame)
 - java.util.concurrent.SynchronousQueue$TransferStack.transfer(java.lang.Object, boolean, long) @bci=102, line=362 (Interpreted frame)
 - java.util.concurrent.SynchronousQueue.poll(long, java.util.concurrent.TimeUnit) @bci=11, line=941 (Interpreted frame)
 - java.util.concurrent.ThreadPoolExecutor.getTask() @bci=134, line=1073 (Interpreted frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=26, line=1134 (Interpreted frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)


Thread 11488: (state = BLOCKED)
 - java.io.FileInputStream.readBytes(byte[], int, int) @bci=0 (Compiled frame; information may be imprecise)
 - java.io.FileInputStream.read(byte[], int, int) @bci=4, line=255 (Compiled frame)
 - java.io.BufferedInputStream.fill() @bci=214, line=246 (Interpreted frame)
 - java.io.BufferedInputStream.read() @bci=12, line=265 (Compiled frame)
 - java.io.DataInputStream.readInt() @bci=4, line=387 (Compiled frame)
 - org.apache.maven.surefire.booter.MasterProcessCommand.decode(java.io.DataInputStream) @bci=1, line=115 (Interpreted frame)
 - org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run() @bci=40, line=391 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)


Thread 11470: (state = BLOCKED)
 - java.lang.Thread.exit() @bci=0, line=757 (Interpreted frame)


Thread 11469: (state = BLOCKED)


Thread 11468: (state = IN_NATIVE)
 - org.rocksdb.RocksDB.disposeInternal(long) @bci=0 (Interpreted frame)
 - org.rocksdb.RocksObject.disposeInternal() @bci=5, line=37 (Interpreted frame)
 - org.rocksdb.AbstractImmutableNativeReference.close() @bci=13, line=57 (Compiled frame)
 - org.rocksdb.AbstractNativeReference.dispose() @bci=1, line=53 (Compiled frame)
 - org.rocksdb.AbstractNativeReference.finalize() @bci=8, line=73 (Compiled frame)
 - java.lang.System$2.invokeFinalize(java.lang.Object) @bci=1, line=1275 (Compiled frame)
 - java.lang.ref.Finalizer.runFinalizer(sun.misc.JavaLangAccess) @bci=46, line=102 (Compiled frame)
 - java.lang.ref.Finalizer.access$100(java.lang.ref.Finalizer, sun.misc.JavaLangAccess) @bci=2, line=34 (Compiled frame)
 - java.lang.ref.Finalizer$FinalizerThread.run() @bci=45, line=217 (Interpreted frame)


Thread 11467: (state = BLOCKED)
 - java.lang.Object.wait(long) @bci=0 (Interpreted frame)
 - java.lang.Object.wait() @bci=2, line=502 (Compiled frame)
 - java.lang.ref.Reference.tryHandlePending(boolean) @bci=54, line=191 (Compiled frame)
 - java.lang.ref.Reference$ReferenceHandler.run() @bci=1, line=153 (Interpreted frame)


Thread 11411: (state = BLOCKED)
 - java.lang.Shutdown.halt0(int) @bci=0 (Interpreted frame)
 - java.lang.Shutdown.halt(int) @bci=7, line=139 (Interpreted frame)
 - java.lang.Shutdown.exit(int) @bci=99, line=213 (Interpreted frame)
 - java.lang.Runtime.exit(int) @bci=14, line=110 (Interpreted frame)
 - java.lang.System.exit(int) @bci=4, line=973 (Interpreted frame)
 - org.apache.maven.surefire.booter.ForkedBooter.acknowledgedExit() @bci=70, line=338 (Interpreted frame)
 - org.apache.maven.surefire.booter.ForkedBooter.execute() @bci=145, line=145 (Interpreted frame)
 - org.apache.maven.surefire.booter.ForkedBooter.main(java.lang.String[]) @bci=35, line=418 (Interpreted frame)
{code}

[~azagrebin] do you believe the {{org.rocksdb.RocksDB.disposeInternal(long)}} could cause this?;;;","05/Mar/20 19:45;rmetzger;Putting the rocksdb.so back into the tmp folder revealed more / confirms:

{code}
(gdb) where
#0  0x00007f69aaec6428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
#1  0x00007f69aaec802a in __GI_abort () at abort.c:89
#2  0x00007f69a960e84d in __gnu_cxx::__verbose_terminate_handler() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#3  0x00007f69a960c6b6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#4  0x00007f69a960c701 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00007f69a960d23f in __cxa_pure_virtual () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00007f678e9110d5 in rocksdb::DBImpl::CloseHelper() ()
   from /tmp/junit1547658367137473582/junit5791642854447555054/rocksdb-lib-b2b47d85aecef16c175b116af00b6d57/librocksdbjni-linux64.so
#7  0x00007f678e91c17b in rocksdb::DBImpl::~DBImpl() () from /tmp/junit1547658367137473582/junit5791642854447555054/rocksdb-lib-b2b47d85aecef16c175b116af00b6d57/librocksdbjni-linux64.so
#8  0x00007f678e91c451 in rocksdb::DBImpl::~DBImpl() () from /tmp/junit1547658367137473582/junit5791642854447555054/rocksdb-lib-b2b47d85aecef16c175b116af00b6d57/librocksdbjni-linux64.so
#9  0x00007f6994e0f607 in ?? ()
#10 0x3338303638366431 in ?? ()
#11 0x3435615f66373731 in ?? ()
#12 0x6138646564336562 in ?? ()
#13 0x6530373737633131 in ?? ()
#14 0x00007f68ade2b7c0 in ?? ()
#15 0x0000000000000000 in ?? ()
{code};;;","05/Mar/20 20:18;rmetzger;Running all tests in the {{flink-state-processor-api}} module in IntelliJ on my mac also leads to this on stdout

{code}
Assertion failed: (last_ref), function ~ColumnFamilySet, file db/column_family.cc, line 1238.
{code}

and this in osx crashreport:

{code}
Thread 25 Crashed:: Java: Finalizer
0   libsystem_kernel.dylib              0x00007fff71ff97fa __pthread_kill + 10
1   libsystem_pthread.dylib             0x00007fff720b6bc1 pthread_kill + 432
2   libsystem_c.dylib                   0x00007fff71f80a1c abort + 120
3   libsystem_c.dylib                   0x00007fff71f7fcd6 __assert_rtn + 314
4   librocksdbjni-osx.jnilib            0x000000012d560038 rocksdb::ColumnFamilySet::~ColumnFamilySet() + 344
5   librocksdbjni-osx.jnilib            0x000000012d72fb07 rocksdb::VersionSet::~VersionSet() + 71
6   librocksdbjni-osx.jnilib            0x000000012d5c7497 rocksdb::DBImpl::CloseHelper() + 2519
7   librocksdbjni-osx.jnilib            0x000000012d5c7779 rocksdb::DBImpl::~DBImpl() + 57
8   librocksdbjni-osx.jnilib            0x000000012d5c7f0e rocksdb::DBImpl::~DBImpl() + 14
9   ???                                 0x00000001171d65a7 0 + 4682769831
10  ???                                 0x00000001171c5ffd 0 + 4682702845
{code};;;","05/Mar/20 21:19;azagrebin;Looks like native column ref leak in  RocksDB. Possibly some column family object is not closed properly before closing RocksDB instance. If this started to happen recently but worked before, we should check any related latest changes.
cc [~liyu] [~yunta] [~sjwiesman];;;","05/Mar/20 22:31;sjwiesman;FLINK-14231 made changes to how StreamTask closes operators and the state proc api uses stream task in a non-standard way. I will investigate. ;;;","05/Mar/20 23:31;sjwiesman;[~rmetzger] I ran all the tests 100 times and so far I am unable to reproduce. Is this failure occurring for a certain test or across all? Particularly if its on the reader or writer side, as those comonents don't share much code. 

I checked the changes made to BoundedStreamTask by FLINK-14231 and they do not look like they should be causing this issue. The state proc api does not ever work directly with RocksDB but only through the state backend interface. ;;;","06/Mar/20 07:00;rmetzger;[~sjwiesman] Thanks for looking into it.
I am also not able to reproduce it by executing the tests through maven on my mac.
But running *ALL* tests of the state processor API module together triggers it every time.
And it looks like the test after the RocksDBB test is failing, not the rocks test itself. But if you ignore the rocks test, all tests are passing.

To execute all tests, right click on the module, and click ""Run all tests"";;;","06/Mar/20 08:09;rmetzger;I opened a PR to ignore this test in the meantime: https://github.com/apache/flink/pull/11330;;;","06/Mar/20 08:15;liyu;I could also reproduce the issue locally by running All tests for {{flink-state-processing-api}} module with below error, which truly indicates some resource leak problem. Will take a closer look into it.
{quote}
Assertion failed: (last_ref), function ~ColumnFamilySet, file db/column_family.cc, line 1238.
{quote};;;","06/Mar/20 12:37;rmetzger;Thank you for looking into it Yu. The test is ignored in master for now: https://github.com/apache/flink/commit/d1f1d40a297dace6942c6d1d49bc4aca264b9c49;;;","06/Mar/20 12:38;pnowojski;[~liyu], we have merged PR to ignore the test for now. Can you take over this issue for a proper fix?;;;","06/Mar/20 16:05;sjwiesman;[~liyu] No need, I found the issue and have opened a PR. ;;;","06/Mar/20 17:09;tzulitai;Fixed by:

{{master}} / {{1.11.0}} - 560e8052c8632ae6b1db73dd6152b837402c1290
{{1.10.1}} - 93d6324ee0e3fb7c7975fb0c56280357adc66f24 ;;;","06/Mar/20 18:57;rmetzger;Thanks to everybody who was involved in this :);;;","07/Mar/20 06:39;liyu;Thanks for the quick fix and review [~sjwiesman] [~tzulitai]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stateful Function's HttpFunction falls into endless loop resending 0-sized batch requests,FLINK-16312,13288059,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,27/Feb/20 13:48,27/Feb/20 15:19,13/Jul/23 08:07,27/Feb/20 15:19,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,,,,,"This is caused by FLINK-16311, because the {{HttpFunction}} was relying on contracts of {{PersistedAppendingBuffer}} that were not correct.

Specifically, the culprit is here:
https://github.com/apache/flink-statefun/blob/master/statefun-flink/statefun-flink-core/src/main/java/org/apache/flink/statefun/flink/core/httpfn/HttpFunction.java#L152

This branch checks if the obtained view is {{null}}, if not, resends a batch request.
However, that would never be {{null}} due to FLINK-16311.",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 27 15:19:24 UTC 2020,,,,,,,,,,"0|z0bynk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/20 15:19;tzulitai;Fixed in master via e0eff08f04c83b5bf61721a797ef9d00a77c3d84;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PersistedAppendingBuffer state primitive has incorrect contracts for view() method,FLINK-16311,13288055,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,27/Feb/20 13:40,27/Feb/20 15:19,13/Jul/23 08:07,27/Feb/20 15:19,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"The {{PersistedAppendingBuffer}} state primitive in Stateful Functions, states that {{view()}} returns {{null}} if the buffer wasn't accessed before, or after a {{clear()}}.

This actually is not possible. The {{PersistedAppendingBuffer}} primitive is based on Flink's {{ListState}} state handle, which always returns an empty list if the elements of the list is empty, and never a {{null}}.

This means that the {{PersistedAppendingBuffer}} won't be able to differentiate the cases (e.g. if the state wasn't accessed or cleared v.s. an empty buffer).
We suggest to just change the contract so that {{PersistedAppendingBuffer#view()}} follows the behavior of Flink's {{ListState}} to never return null.",,tzulitai,,,,,,,,,,,,,,,,,,,,,"tzulitai commented on pull request #39: [FLINK-16311] [FLINK-16312] Fix PersistedAppendingBuffer contracts and HttpFunction.onAsyncResult
URL: https://github.com/apache/flink-statefun/pull/39
 
 
   The previous contracts for `PersistedAppendingBuffer` were not possible to be provided, since Flink `ListState` never returns `null` when retrieving the list, regardless of if the access was after a clear, or the list was actually just empty.
   
   This PR changes the contract of the `view()` method to always return non-null, and also disallow appending / update the buffer with `null` elements or lists.
   
   ---
   
   Following that, since `HttpFunction` was relying on the previous incorrect contracts, it had a problem of endlessly resend 0-sized batch requests.
   This PR fixes that as well by relying on the new contracts.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 14:55;githubbot;600","tzulitai commented on pull request #39: [FLINK-16311] [FLINK-16312] Fix PersistedAppendingBuffer contracts and HttpFunction.onAsyncResult
URL: https://github.com/apache/flink-statefun/pull/39
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 15:18;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 27 15:19:06 UTC 2020,,,,,,,,,,"0|z0bymo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/20 15:19;tzulitai;Merged to master via 64bf21835be12c86e01c5fdc6379c080c141b79a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ElasticSearch 7 connector is missing in SQL connector list,FLINK-16309,13288048,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,fhueske,fhueske,27/Feb/20 13:17,02/Jul/20 04:06,13/Jul/23 08:07,02/Jul/20 03:45,1.10.0,,,,,1.10.2,,,,Documentation,Table SQL / Ecosystem,,,,0,pull-request-available,,,,The ES7 connector is not listed on https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/connect.html,,aljoscha,fhueske,leonard,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 02 03:59:56 UTC 2020,,,,,,,,,,"0|z0byl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/20 15:59;leonard;Hello, [~fhueske] 
I update the fix version to 1.10.2, I have checked 1.11.0 and 1.12.0 is ok.;;;","02/Jul/20 03:45;lzljs3620320;release-1.10: a519b04dc4c2f2a3473397a06d8cccb47b745b20;;;","02/Jul/20 03:59;leonard;Thanks Jingsong very much, but the fix is in release-1.10 branch.
 release-1.10: a519b04dc4c2f2a3473397a06d8cccb47b745b20;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL connector download links are broken,FLINK-16308,13288045,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fhueske,fhueske,fhueske,27/Feb/20 13:15,17/Apr/20 10:43,13/Jul/23 08:07,17/Apr/20 10:43,,,,,,1.10.1,1.11.0,1.9.3,,Documentation,Table SQL / Ecosystem,,,,0,pull-request-available,,,,"The download links for the SQL connectors on https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/connect.html are broken because central.maven.org is down.

The URLs should be updated to https://repo.maven.apache.org/maven2/org/apache/flink/",,aljoscha,fhueske,,,,,,,,,,,,,,,,,,,,"fhueske commented on pull request #11243: [FLINK-16308] [docs] Fix Maven download links for SQL connectors.
URL: https://github.com/apache/flink/pull/11243
 
 
   ## What is the purpose of the change
   
   * Fixes broken download links in the SQL Connector documentation. `http://central.maven.org` is down and was replaced by `https://repo.maven.apache.org`.
   
   ## Brief change log
   
   * Update links
   
   ## Verifying this change
   
   * this is a docs-only change
   
   ## Does this pull request potentially affect one of the following parts:
   
   no affects on code
   
   ## Documentation
   
   * Fix updates documentation
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 14:04;githubbot;600","aljoscha commented on pull request #11243: [FLINK-16308] [docs] Fix Maven download links for SQL connectors.
URL: https://github.com/apache/flink/pull/11243
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/20 10:43;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-17217,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 10:42:58 UTC 2020,,,,,,,,,,"0|z0bykg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/20 10:42;aljoscha;release-1.9: 5128b860fbfa55af8ffe8699bd5350da99c1cb64
release-1.10: 327d3a5e19e7c1b5ff8a37654997fdbe82706633
master: 4ed8bb8ccde595207a0403f88b1e2f7534940c6b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Annoying ""Cannot find FunctionDefinition"" messages with SQL for f_proctime or =",FLINK-16301,13287969,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,nkruber,nkruber,27/Feb/20 09:04,28/Feb/20 03:25,13/Jul/23 08:07,28/Feb/20 03:25,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When running the following SQL query
{code}
SELECT
    D1.col1 AS A,
    D1.col2 AS B,
    D1.col3 AS C,
    D1.col4 AS D,
    D1.col5 AS E,
    D2.col1 AS F,
    D2.col2 AS G,
    D2.col3 AS H,
    D2.col4 AS I,
    D2.col5 AS J,
    D3.col1 AS K,
    D3.col2 AS L,
    D3.col3 AS M,
    D3.col4 AS N,
    D3.col5 AS O,
    D4.col1 AS P,
    D4.col2 AS Q,
    D4.col3 AS R,
    D4.col4 AS S,
    D4.col5 AS T,
    D5.col1 AS U,
    D5.col2 AS V,
    D5.col3 AS W,
    D5.col4 AS X,
    D5.col5 AS Y
FROM
    fact_table,
    LATERAL TABLE (dimension_table1(f_proctime)) AS D1,
    LATERAL TABLE (dimension_table2(f_proctime)) AS D2,
    LATERAL TABLE (dimension_table3(f_proctime)) AS D3,
    LATERAL TABLE (dimension_table4(f_proctime)) AS D4,
    LATERAL TABLE (dimension_table5(f_proctime)) AS D5
WHERE
    fact_table.dim1     = D1.id
    AND fact_table.dim2 = D2.id
    AND fact_table.dim3 = D3.id
    AND fact_table.dim4 = D4.id
    AND fact_table.dim5 = D5.id
{code}

with the Blink planner, it prints a log of bogus warnings about unknown functions for things like {{f_proctime}} or {{=}} at INFO level which should be DEBUG level at least in order not to bother the users with it. The messages I got are:

{code}
13:33:59,590 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition f_proctime from any loaded modules
13:33:59,641 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition f_proctime from any loaded modules
13:33:59,644 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition f_proctime from any loaded modules
13:33:59,647 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition f_proctime from any loaded modules
13:33:59,650 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition f_proctime from any loaded modules
13:33:59,662 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition = from any loaded modules
13:33:59,665 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition = from any loaded modules
13:33:59,666 INFO  org.apache.flink.table.module.ModuleManager                   - Got FunctionDefinition and from module core
13:33:59,667 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition = from any loaded modules
13:33:59,668 INFO  org.apache.flink.table.module.ModuleManager                   - Got FunctionDefinition and from module core
13:33:59,669 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition = from any loaded modules
13:33:59,670 INFO  org.apache.flink.table.module.ModuleManager                   - Got FunctionDefinition and from module core
{code}",,jark,nkruber,,,,,,,,,,,,,,,,,,,,"wuchong commented on pull request #11244: [FLINK-16301][table-api] Improve logging message in ModuleManager
URL: https://github.com/apache/flink/pull/11244
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The unknown functions in `ModuleManager` is logged in INFO level which is very verbose and useless for users.
   
   ## Brief change log
   
   - Change the log level to DEBUG for unknown functions. (maybe we can even remove it)
   - Improve the messages to make the function name and module name more obvious.
   
   ## Verifying this change
   
   This change added a test which transforms a nested data type.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 14:10;githubbot;600","wuchong commented on pull request #11244: [FLINK-16301][table-api] Improve logging message in ModuleManager
URL: https://github.com/apache/flink/pull/11244
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Feb/20 03:20;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 03:25:40 UTC 2020,,,,,,,,,,"0|z0by3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/20 03:25;jark;Fixed in 
 - master (1.11.0): ff0e366c7b4b0b7a08ad1f68dd703b04bc8d61af
 - 1.10.1: f800753865eecaea313cfd154dc240090d45fc87;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Count(*) doesn't work with Hive module,FLINK-16291,13287761,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lirui,lirui,26/Feb/20 14:36,09/Jun/20 16:45,13/Jul/23 08:07,09/Jun/20 04:14,,,,,,1.11.0,,,,Connectors / Hive,Table SQL / Planner,,,,0,pull-request-available,,,,"The following test
{code}
	@Test
	public void test() throws Exception {
		hiveShell.execute(""create table foo (x int)"");
		TableEnvironment tableEnv = getTableEnvWithHiveCatalog();
		tableEnv.unloadModule(""core"");
		tableEnv.loadModule(""hive"", new HiveModule(hiveCatalog.getHiveVersion()));
		tableEnv.loadModule(""core"", CoreModule.INSTANCE);
		TableUtils.collectToList(tableEnv.sqlQuery(""select count(*) from foo""));
	}
{code}
fails with
{noformat}
Caused by: org.apache.calcite.runtime.CalciteContextException: At line 1, column 14: Unknown identifier '*'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:834)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:819)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4840)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5666)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5586)
	at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:317)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1690)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1675)
	at org.apache.calcite.sql.SqlOperator.constructArgTypeList(SqlOperator.java:593)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:237)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:219)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5599)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5586)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1690)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1675)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:478)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4104)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3388)
{noformat}",,godfreyhe,libenchao,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 04:14:09 UTC 2020,,,,,,,,,,"0|z0bwtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/20 10:37;lirui;The {{*}} identifier needs special handling in the parser. So let's ban {{count}} in hive module for now.;;;","09/Jun/20 04:14;lzljs3620320;master: 2be8239fc0f61524d9456c1e38d76f0c16b9ee7b

release-1.11: 513ac5ae8d4fc1e679a7faf07a7d020f073a09fc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HttpUrl might return NULL if a schema is missing,FLINK-16290,13287759,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igal,igal,igal,26/Feb/20 14:28,28/Feb/20 01:24,13/Jul/23 08:07,28/Feb/20 01:24,,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"okhttp's HttpUrl class might be null if the original endpoint definition
is missing the schema (or it is not http or https)
To prevent that, we need to validate the parsed URI in JsonModule.",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,"igalshilman commented on pull request #38: [FLINK-16290][http] Add validation to the HTTP endpoint schema
URL: https://github.com/apache/flink-statefun/pull/38
 
 
   This PR adds a check for the user supplied endpoint (in `module.yaml`) to be a uri with
   a schema, that is either `http` or `https`.
   Failing to do so, would cause a `NPE` at runtime due to the way okhttp`s `HttpUrl` handles URIs with a missing schema.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/20 14:33;githubbot;600","tzulitai commented on pull request #38: [FLINK-16290][http] Add validation to the HTTP endpoint schema
URL: https://github.com/apache/flink-statefun/pull/38
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Feb/20 01:23;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 01:24:31 UTC 2020,,,,,,,,,,"0|z0bwsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/20 01:24;tzulitai;Fixed in master via d7cce0c2639a6b7fe28a68126bb8483fcd7419f6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ES6 sql jar relocates log4j2,FLINK-16287,13287707,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,26/Feb/20 09:57,26/Feb/20 13:19,13/Jul/23 08:07,26/Feb/20 13:19,1.11.0,,,,,1.11.0,,,,Build System,Connectors / ElasticSearch,,,,0,pull-request-available,,,,"{{flink-sql-connector-elasticsearch6}} still defines a relocation rule for log4j2, but this dependency is no longer bundled and instead provided by flink-dist.",,libenchao,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11221: [FLINK-16287][es][build] Remove Log4j2 relocation
URL: https://github.com/apache/flink/pull/11221
 
 
   Log4j2 is no longer bundled in the jar since it is now bundled in flink-dist. The relocation is hence unnecessary and currently breaks the connector.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/20 09:59;githubbot;600","zentol commented on pull request #11221: [FLINK-16287][es][build] Remove Log4j2 relocation
URL: https://github.com/apache/flink/pull/11221
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/20 13:19;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,FLINK-15672,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 13:19:19 UTC 2020,,,,,,,,,,"0|z0bwhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/20 13:19;chesnay;master: 16231;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in GroupAggProcessFunction.close(),FLINK-16283,13287576,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,rmetzger,rmetzger,25/Feb/20 19:27,26/Feb/20 13:32,13/Jul/23 08:07,26/Feb/20 12:17,1.11.0,,,,,1.11.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,test-stability,,,"CI run: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5586&view=logs&j=b1623ac9-0979-5b0d-2e5e-1377d695c991&t=48867695-c47f-5af3-2f21-7845611247b9

{code}
java.lang.NullPointerException: null
	at org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.close(GroupAggProcessFunction.scala:182) ~[flink-table_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:627) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:565) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:483) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:717) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:541) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]

{code}",,jark,libenchao,rmetzger,,,,,,,,,,,,,,,,,,,"wuchong commented on pull request #11217: [FLINK-16283][table-planner] Fix potential NullPointerException when invoking close() on generated function
URL: https://github.com/apache/flink/pull/11217
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix potential NullPointerException when invoking close() on generated function in legacy planner.
   
   ## Brief change log
   
   - Check the generated function is not null before close it.
   
   ## Verifying this change
   
   N/A
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/20 03:11;githubbot;600","wuchong commented on pull request #11217: [FLINK-16283][table-planner] Fix potential NullPointerException when invoking close() on generated function
URL: https://github.com/apache/flink/pull/11217
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/20 12:16;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 13:32:17 UTC 2020,,,,,,,,,,"0|z0bvo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/20 12:17;jark;Fixed in master (1.11.0): 10b33f89e3a4e2e57831ff231f6881398c22aafa;;;","26/Feb/20 12:48;libenchao;[~jark] sorry for the late comment.

I'm curious about when would the `function` be `null`? Do we have somewhere invoking `GroupAggProcessFunction.close()` without `GroupAggProcessFunction.open()` ?

Or the generated function indeed can be null? If that, shouldn't we check for null in the `open` too?;;;","26/Feb/20 13:27;jark;Hi [~libenchao], the function is assigned in open() function, if the job failed before the assignment, then function is null. That's why we should check null in close() method. ;;;","26/Feb/20 13:32;libenchao;[~jark] Sounds reasonable, thanks for the explanation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong exception using DESCRIBE SQL command,FLINK-16282,13287543,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,nkruber,nkruber,25/Feb/20 15:57,29/May/20 10:29,13/Jul/23 08:07,29/May/20 10:29,1.10.0,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,,,,,"When trying to describe a table like this


{code:java}
Table facttable = tEnv.sqlQuery(""DESCRIBE fact_table"");
{code}

currently, you get a strange exception which should rather be a ""not supported"" exception

{code}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 10 to line 1, column 19: Column 'fact_table' not found in any table
	at org.apache.flink.table.calcite.FlinkPlannerImpl.validateInternal(FlinkPlannerImpl.scala:130)
	at org.apache.flink.table.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:105)
	at org.apache.flink.table.sqlexec.SqlToOperationConverter.convert(SqlToOperationConverter.java:124)
	at org.apache.flink.table.planner.ParserImpl.parse(ParserImpl.java:66)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:464)
	at com.ververica.LateralTableJoin.main(LateralTableJoin.java:92)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 10 to line 1, column 19: Column 'fact_table' not found in any table
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:834)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:819)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4841)
	at org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:259)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateIdentifier(SqlValidatorImpl.java:2943)
	at org.apache.calcite.sql.SqlIdentifier.validateExpr(SqlIdentifier.java:297)
	at org.apache.calcite.sql.SqlOperator.validateCall(SqlOperator.java:407)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateCall(SqlValidatorImpl.java:5304)
	at org.apache.calcite.sql.SqlCall.validate(SqlCall.java:116)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:943)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:650)
	at org.apache.flink.table.calcite.FlinkPlannerImpl.validateInternal(FlinkPlannerImpl.scala:126)
	... 5 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Column 'fact_table' not found in any table
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463)
	at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:572)
	... 17 more

{code}
 

 ",,dwysakowicz,godfreyhe,jark,leonard,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16366,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 10:28:52 UTC 2020,,,,,,,,,,"0|z0bvgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/20 02:58;jark;Hi [~NicoK], {{DESCRIBE fact_table}} is only supported on SQL CLI. You can call {{tableEnv.from(""fact_table"").getSchema().toString()}} to achieve the same purpose. 

{{tableEnv.sqlQuery()}} only accepts ""SELECT"" queries.;;;","26/Feb/20 07:50;nkruber;Yes, this ticket is about the error message from trying to use {{DESCRIBE}};;;","26/Feb/20 08:27;jark;Yes. We should improve the exception message.;;;","29/May/20 10:28;leonard;The reason is we do not support DESCRIBE statement in flink-1.10, but FLINK-16366 have supported this：
{code:java}
org.apache.flink.table.api.ValidationException: Unsupported SQL query! sqlQuery() only accepts a single SQL query of type SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.org.apache.flink.table.api.ValidationException: Unsupported SQL query! sqlQuery() only accepts a single SQL query of type SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.
 at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:639) 
{code}
So, I'd like to close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
parameter 'maxRetryTimes' can not work in JDBCUpsertTableSink,FLINK-16281,13287539,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,25/Feb/20 15:44,13/May/20 16:46,13/Jul/23 08:07,03/Mar/20 13:49,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / Ecosystem,,,,,0,pull-request-available,,,,"When I insert data to a mysql table that do no exists in my test database will get exception,
{code:java}
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test.gmv_table' doesn't existCaused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test.gmv_table' doesn't exist at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at com.mysql.jdbc.Util.handleNewInstance(Util.java:425) at com.mysql.jdbc.Util.getInstance(Util.java:408) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3933) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3869) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2524) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2675) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2465) at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1912) at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2133) at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1810) ... 44 more
{code}
but after I increased the 'connector.write.max-retries' value from 1 to 3, the exception disappeared. :(


So, I look up the flush  implement code :
{code:java}
public synchronized void flush() throws Exception {
   checkFlushException();

   for (int i = 1; i <= maxRetryTimes; i++) {
      try {
         jdbcWriter.executeBatch();
         batchCount = 0;
         break;
      } catch (SQLException e) {
         LOG.error(""JDBC executeBatch error, retry times = {}"", i, e);
         if (i >= maxRetryTimes) {
            throw e;
         }
         Thread.sleep(1000 * i);
      }
   }
}{code}
I found the `jdbcWriter` will clear its `batchedArgs` member after first call `jdbcWriter.executeBatch()` as follows:
{code:java}
//com.mysql.jdbc.PreparedStatement
finally {
    this.statementExecuting.set(false);
    clearBatch();
}

// clearBatch() function implement
public void clearBatch() throws SQLException {
 synchronized (checkClosed().getConnectionMutex()) {
 if (this.batchedArgs != null) {
 this.batchedArgs.clear();
 }
 }
}
{code}
and the next time（ where i> 1） to call `jdbcWriter.executeBatch()` ,  the function will return empty array rather than execute the flush data
{code:java}
//com.mysql.jdbc.PreparedStatement
if (this.batchedArgs == null || this.batchedArgs.size() == 0) {
    return new long[0];
}
... // flush data code{code}
 

 ",,jark,leonard,libenchao,liyu,lzljs3620320,,,,,,,,,,,,,,,,,"leonardBang commented on pull request #11223: [FLINK-16281][Table SQL / Ecosystem] parameter 'maxRetryTimes' can not work in JDBCUpsertTableSink.
URL: https://github.com/apache/flink/pull/11223
 
 
   parameter 'maxRetryTimes' can not work in JDBCUpsertTableSink.
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *This pull request fix 'maxRetryTimes' can not work properly in JDBCUpsertTableSink( only AppendOnlyWriter) .*
   
   
   ## Brief change log
   
     - *Cache data in AppendOnlyWriter for multiple call*
     - *Update executeBatch() in AppendOnlyWriter*
   
   
   ## Verifying this change
   
    Test exception will be thrown normally in JDBCUpsertTableSinkITCase.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/20 13:40;githubbot;600","JingsongLi commented on pull request #11223: [FLINK-16281][Table SQL / Ecosystem] parameter 'maxRetryTimes' can not work in JDBCUpsertTableSink.
URL: https://github.com/apache/flink/pull/11223
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Mar/20 01:25;githubbot;600","leonardBang commented on pull request #11293: [FLINK-16281][jdbc] Parameter 'maxRetryTimes' can not work in AppendOnlyWriter
URL: https://github.com/apache/flink/pull/11293
 
 
   Parameter 'maxRetryTimes' can not work in AppendOnlyWriter
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *This pull request fix 'maxRetryTimes' can not work properly in JDBCUpsertTableSink( only AppendOnlyWriter) .*
   
   
   ## Brief change log
   
     - *Cache data in AppendOnlyWriter for multiple call*
     - *Update executeBatch() in AppendOnlyWriter*
   
   
   ## Verifying this change
   
     Test exception will be thrown normally in JDBCAppenOnlyWriterTest.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Mar/20 05:35;githubbot;600",,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 16:46:32 UTC 2020,,,,,,,,,,"0|z0bvg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/20 01:17;leonard;CC: [~lzljs3620320]

 ;;;","26/Feb/20 01:26;lzljs3620320;Thanks [~Leonard Xu] for reporting. Can you give more information? Where do your codes come from? What is this bug? How to re-produce this bug? I don't get what are you mean.;;;","26/Feb/20 01:50;leonard;Hi， [~lzljs3620320] 
I update the description information, hope it's clear enough.:D;;;","26/Feb/20 02:48;lzljs3620320;I got it, More specifically, it is bug in {{AppendOnlyWriter}}, {{UpsertWriter}} is OK, right?;;;","26/Feb/20 02:48;lzljs3620320;Feel free to open a pull request. ;);;;","26/Feb/20 02:54;leonard;[~lzljs3620320] Yes， it only exists in AppendOnlyWriter, I'd like to open a PR.;;;","03/Mar/20 01:26;lzljs3620320;master: 3e10f0a5ca1179f8a95185c3d3c03ec9fe0dabb6;;;","03/Mar/20 01:26;lzljs3620320;[~Leonard Xu] can you open a PR for release-1.10?;;;","03/Mar/20 02:05;leonard;Hi, [~lzljs3620320] 
{quote}[~Leonard Xu] can you open a PR for release-1.10?
{quote}
ok, I'll open a PR soon.;;;","03/Mar/20 13:48;leonard;release-1.10: 833924dcab13879a5c896effb8fce8dc1963e826;;;","13/May/20 16:46;liyu;Remove the release note since the existing one only marks commits for different branches rather than any notice-able content like incompatible changes or usage for new features, etc.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTableEnvironment.toAppendStream fails with Decimal types,FLINK-16277,13287491,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,BenoitParis,BenoitParis,25/Feb/20 13:04,25/May/21 07:37,13/Jul/23 08:07,25/May/21 07:36,1.10.0,,,,,,,,,Table SQL / API,,,,,0,,,,,"The following fails when there is a Decimal type in the underlying TableSource:

 
{code:java}
DataStream<Row> appendStream = tEnv.toAppendStream(
  asTable,
  asTable.getSchema().toRowType()
);{code}
Yielding the following error:

 

ValidationException: Type ROW<`y` DECIMAL(38, 18)> of table field 'payload' does not match with the physical type ROW<`y` LEGACY('DECIMAL', 'DECIMAL')> of the 'payload' field of the TableSource return type
----
 

Remarks:
 * toAppendStream is not ready for the new type system, does not accept the new DataTypes
 * The LegacyTypeInformationType transition type hinders things. Replacing it with the new DataTypes.DECIMAL type makes things work.
 * flink-json is not ready for the new type system, does not give the new DataTypes

 

Workaround: reprocess TypeConversions.fromLegacyInfoToDataType's output to replace LegacyTypeInformationType types when they are of DECIMAL typeroot with the new types.

 

Included is reproduction and workaround (activated by line 127) code, with java + pom + stacktrace files.

 ",,aljoscha,BenoitParis,godfreyhe,jark,jingzhang,kezhuw,leonard,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/20 23:18;BenoitParis;DecimalType 38 18 Logical - stacktrace.txt;https://issues.apache.org/jira/secure/attachment/12997071/DecimalType+38+18+Logical+-+stacktrace.txt","25/Feb/20 13:04;BenoitParis;flink-test-schema-update.zip;https://issues.apache.org/jira/secure/attachment/12994546/flink-test-schema-update.zip",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 25 07:36:58 UTC 2021,,,,,,,,,,"0|z0bv5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/20 15:47;twalthr;A workaround is to simply call:
{code}
tEnv.toAppendStream(
  asTable,
  Row.class
);
{code};;;","18/Mar/20 15:50;twalthr;Where does {{DECIMAL(38, 18)}} come from?;;;","18/Mar/20 15:59;twalthr;I looked into your code. It mixes type information and data types together. I totally agree that the current situation is a messy, but if one just uses either `org.apache.flink.table.api.Types` or `DataTypes` + TypeConversions everything should work as expected.;;;","18/Mar/20 23:20;BenoitParis;I tried with the `Row.class` call; it does not seem to have an influence and yields the same stacktrace.
----
DECIMAL(38, 18): I followed the execution from where the ValidationException signaled this type.
 It comes from org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.LegacyTypeToPlannerTypeConverter, which mentions a Decimal.DECIMAL_SYSTEM_DEFAULT (which is at 38, 18), for BasicTypeInfo.BIG_DEC_TYPE_INFO's. I'm attaching the creation stacktrace of the type in the JIRA.

I trialed-and-errored on replacing LegacyTypeInformationType with this type. Other precision and scales values do not work.

I'll try to summarize my limited understanding: the bug seems to be that in toAppendStream call, the Logical types got a prior conversion from a Legacy Wrapper to a new Blink Type, when the physical types never got it.
----
> mixes type information and data types together

I must admit I'm a bit confused by the types. Are toAppendStream's TypeInformation's on the same boat as org.apache.flink.table.api.Types as well?

For now I have something that sort of works with band-aids; but I get to use the new planner with my old code.;;;","22/Apr/21 11:28;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","24/May/21 22:50;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","25/May/21 07:36;twalthr;Fixed as part of FLIP-136 in FLINK-21934.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AggsHandlerCodeGenerator can fail with custom module,FLINK-16275,13287476,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,25/Feb/20 11:42,27/Feb/20 09:05,13/Jul/23 08:07,27/Feb/20 09:04,,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Code generated by {{AggsHandlerCodeGenerator}} can fail to compile if certain functions get overridden by custom module:
{noformat}
Caused by: org.codehaus.commons.compiler.CompileException: Line 20, Column 215: A method named ""getRuntimeContext"" is not declared in any enclosing class nor any supertype, nor through a static import
{noformat}",,jark,libenchao,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11215: [FLINK-16275][table-planner-blink] AggsHandlerCodeGenerator can fail …
URL: https://github.com/apache/flink/pull/11215
 
 
   …with custom module
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   To fix the issue of `AggsHandlerCodeGenerator` working with a custom module.
   
   Code generated by `AggsHandlerCodeGenerator` can fail to compile if certain functions get overridden by custom module:
   ```
   Caused by: org.codehaus.commons.compiler.CompileException: Line 20, Column 215: A method named ""getRuntimeContext"" is not declared in any enclosing class nor any supertype, nor through a static import
   ```
   
   
   ## Brief change log
   
     - Make sure the generated `AggsHandleFunction` can get `RuntimeContext` properly.
     - Add test case
   
   
   ## Verifying this change
   
   New test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 13:36;githubbot;600","JingsongLi commented on pull request #11215: [FLINK-16275][table-planner-blink] AggsHandlerCodeGenerator can fail …
URL: https://github.com/apache/flink/pull/11215
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 09:02;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16268,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 27 09:04:08 UTC 2020,,,,,,,,,,"0|z0bv20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/20 09:04;lzljs3620320;Master: 68afd048fbf63898c584973023fbb9f3675c8f58;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generic type can not be matched when convert table to stream.,FLINK-16269,13287407,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,xiemeilong,xiemeilong,25/Feb/20 03:55,14/May/20 05:56,13/Jul/23 08:07,05/Mar/20 11:42,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / API,,,,,0,pull-request-available,,,,"The query result schema printed by table.printSchema():
{noformat}
 |-- deviceId: BIGINT
 |-- channel: STRING
 |-- schemaId: BIGINT
 |-- productId: BIGINT
 |-- schema: LEGACY('RAW', 'ANY<com.yunmo.iot.schema.Schema>')
{noformat}
then excuting table.toRetractStream[DeviceSchema].print(), exception throwed:
{noformat}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink do not match.
 Query schema: [deviceId: BIGINT, channel: STRING, schemaId: BIGINT, productId: BIGINT, schema: RAW('com.yunmo.iot.schema.Schema', ?)]
 Sink schema: [deviceId: BIGINT, channel: STRING, schemaId: BIGINT, productId: BIGINT, schema: LEGACY('RAW', 'ANY<com.yunmo.iot.schema.Schema>')]{noformat}
The com.yunmo.iot.schema.Schema is a generic type.

The schema field of Query schema change from LEGACY('RAW' to RAW, but the Sink schema still a LEGACY('RAW'",,jark,libenchao,xiemeilong,,,,,,,,,,,,,,,,,,,"wuchong commented on pull request #11236: [FLINK-16269][FLINK-16108][table-planner-blink] Fix schema of query and sink do not match when generic or POJO type is requested
URL: https://github.com/apache/flink/pull/11236
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix schema of query and sink do not match when generic or POJO type is requested, e.g. `table.toAppendStream[MyPojo]`.
   
   ## Brief change log
   
   1. Generic type
     - Currently, we create a `LEGACY('RAW', ...)` type for generic type, however, after optimization, the type will be `RAW('MyPojo', ..)`. Then the type is mistach.
     - We should convert generic type info into `RAW(...)` at the begining.
   2.  POJO type
     - Currently, we infer sink schema from the PojoTypeInfo. However, the fields order may be mismatch with the query schema fields. 
     - We should generate sink schema using the types from PojoTypeInfo, but field order from query schema. 
   
   ## Verifying this change
   
   This change added a test which transforms a nested data type.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 10:32;githubbot;600","wuchong commented on pull request #11236: [FLINK-16269][FLINK-16108][table-planner-blink] Fix schema of query and sink do not match when generic or POJO type is requested
URL: https://github.com/apache/flink/pull/11236
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/20 11:40;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 05:56:42 UTC 2020,,,,,,,,,,"0|z0bumo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/20 11:42;jark;Fixed in
 - mater(1.11.0): 377024ba85058e3ba5fa092aaf5c92be33d7ae09
 - 1.10.1: 9685642da56554eb5d0292f3ffe193c48329d423;;;","13/May/20 03:22;xiemeilong;After upgrade to 1.10.1,  a new type mismatch occurred.
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink  do not match.
Query schema: [......., evaluator: ROW<`className` STRING, `code` STRING>, ........]
Sink schema: [........, evaluator: LEGACY('STRUCTURED_TYPE', 'POJO<com.yunmo.iot.dsl.GeneratedRuleEvaluator>'),.......]{code}
The evaluator  in query is returned by a registed function, the evaluator in sink is a field of case class.  they are the same GeneratedRuleEvaluator class.

 ;;;","14/May/20 03:10;xiemeilong;[~jark] Is it the same issue or related?;;;","14/May/20 03:51;jark;[~xiemeilong] Could you create another issue for this and provide the code which can reproduce the exception?;;;","14/May/20 05:56;xiemeilong;[~jark]   [FLINK-17683 | https://issues.apache.org/jira/browse/FLINK-17683]
  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to run rank over window with Hive built-in functions,FLINK-16268,13287398,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,25/Feb/20 03:06,18/Nov/20 03:24,13/Jul/23 08:07,18/Nov/20 03:24,1.10.0,,,,,1.11.3,,,,Connectors / Hive,Table SQL / Planner,,,,1,pull-request-available,,,,"The following test:
{code}
	@Test
	public void test() throws Exception {
		hiveShell.execute(""create table emp (dep string,name string,salary int)"");
		hiveShell.insertInto(""default"", ""emp"").addRow(""1"", ""A"", 1).addRow(""1"", ""B"", 2).addRow(""2"", ""C"", 3).commit();
		TableEnvironment tableEnv = // create table env...
		tableEnv.unloadModule(""core"");
		tableEnv.loadModule(""hive"", new HiveModule(hiveCatalog.getHiveVersion()));
		tableEnv.loadModule(""core"", CoreModule.INSTANCE);
		List<Row> results = TableUtils.collectToList(tableEnv.sqlQuery(""select dep,name,rank() over (partition by dep order by salary) as rnk from emp""));
	}
{code}

fails with:
{noformat}
java.lang.NullPointerException
	at org.apache.flink.table.functions.hive.conversion.HiveInspectors.toInspectors(HiveInspectors.java:126)
	at org.apache.flink.table.functions.hive.HiveGenericUDF.getHiveResultType(HiveGenericUDF.java:97)
	at org.apache.flink.table.functions.hive.HiveScalarFunction.getResultType(HiveScalarFunction.java:75)
	at org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils$.getResultTypeOfScalarFunction(UserDefinedFunctionUtils.scala:620)
	at org.apache.flink.table.planner.expressions.PlannerScalarFunctionCall.resultType(call.scala:165)
	at org.apache.flink.table.planner.expressions.PlannerTypeInferenceUtilImpl.runTypeInference(PlannerTypeInferenceUtilImpl.java:75)
	at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.runLegacyTypeInference(ResolveCallByArgumentsRule.java:213)
	at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.lambda$visit$2(ResolveCallByArgumentsRule.java:134)
	at java.util.Optional.orElseGet(Optional.java:267)
	at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.visit(ResolveCallByArgumentsRule.java:134)
	at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.visit(ResolveCallByArgumentsRule.java:89)
	at org.apache.flink.table.expressions.ApiExpressionVisitor.visit(ApiExpressionVisitor.java:39)
	at org.apache.flink.table.expressions.UnresolvedCallExpression.accept(UnresolvedCallExpression.java:135)
	at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.lambda$apply$0(ResolveCallByArgumentsRule.java:83)
	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.apply(ResolveCallByArgumentsRule.java:84)
......
{noformat}",,FrankZou,lirui,lzljs3620320,xtsong,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11209: [FLINK-16268][hive][table-planner-blink] Failed to run rank over wind…
URL: https://github.com/apache/flink/pull/11209
 
 
   …ow with Hive built-in functions
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   To fix the failure of running rank over window query with Hive built-in functions:
   ```
   java.lang.NullPointerException
   	at org.apache.flink.table.functions.hive.conversion.HiveInspectors.toInspectors(HiveInspectors.java:126)
   	at org.apache.flink.table.functions.hive.HiveGenericUDF.getHiveResultType(HiveGenericUDF.java:97)
   	at org.apache.flink.table.functions.hive.HiveScalarFunction.getResultType(HiveScalarFunction.java:75)
   	at org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils$.getResultTypeOfScalarFunction(UserDefinedFunctionUtils.scala:620)
   	at org.apache.flink.table.planner.expressions.PlannerScalarFunctionCall.resultType(call.scala:165)
   	at org.apache.flink.table.planner.expressions.PlannerTypeInferenceUtilImpl.runTypeInference(PlannerTypeInferenceUtilImpl.java:75)
   	at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.runLegacyTypeInference(ResolveCallByArgumentsRule.java:213)
   	at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.lambda$visit$2(ResolveCallByArgumentsRule.java:134)
   	at java.util.Optional.orElseGet(Optional.java:267)
   	at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.visit(ResolveCallByArgumentsRule.java:134)
   	at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.visit(ResolveCallByArgumentsRule.java:89)
   	at org.apache.flink.table.expressions.ApiExpressionVisitor.visit(ApiExpressionVisitor.java:39)
   	at org.apache.flink.table.expressions.UnresolvedCallExpression.accept(UnresolvedCallExpression.java:135)
   	at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.lambda$apply$0(ResolveCallByArgumentsRule.java:83)
   	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267)
   	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
   	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
   	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
   	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
   	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
   	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
   	at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.apply(ResolveCallByArgumentsRule.java:84)
   ......
   ```
   
   
   ## Brief change log
   
     - TBD
     - TBD
     - Add test case
   
   
   ## Verifying this change
   
   Added test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 08:31;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13191,FLINK-16275,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 03:24:35 UTC 2020,,,,,,,,,,"0|z0buko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/20 03:28;lirui;I managed to fix the NPE in {{PlannerTypeInferenceUtilImpl}}, by invoking the {{setArgumentTypesAndConstants}} method of {{HiveGenericUDF}}. And then I hit this bellowing issue:
{noformat}
Caused by: org.codehaus.commons.compiler.CompileException: Line 24, Column 184: A method named ""getRuntimeContext"" is not declared in any enclosing class nor any supertype, nor through a static import
{noformat}
By checking the generated {{BoundedOverAggregateHelper}} class, I think it's because {{BoundedOverAggregateHelper}} now has to call the Hive functions. Therefore in {{BoundedOverAggregateHelper::open}} it opens the Hive functions, which in turn needs to call {{getRuntimeContext}} to create a {{FunctionContext}}, leading to the exception that {{getRuntimeContext}} is not available.;;;","25/Feb/20 03:29;lirui;cc [~lzljs3620320];;;","25/Feb/20 03:32;lirui;BTW, the Hive functions {{BoundedOverAggregateHelper}} needs to use are something like {{isnull}}, {{not}} etc.;;;","25/Feb/20 08:41;lzljs3620320;Thanks [~lirui] for reporting.

This problem may be user can not use hive functions in Table API.

Can you verify it? [~lirui];;;","25/Feb/20 08:43;lzljs3620320;Maybe we should wait for FLINK-13191;;;","25/Feb/20 11:41;lirui;Discussed this with [~lzljs3620320] offline. The fix for the NPE is hacky, therefore I'll hold on this ticket and wait for FLINK-13191.
I'll open another ticket to handle the issue of generated code.;;;","29/Oct/20 08:18;lirui;Although hive UDF has been migrated to new type inference, it's only pushed to 1.12, which means 1.11 users still suffer from the problem here.
[~lzljs3620320] Do you think it makes sense to have a fix (the hacky way) just for 1.11?;;;","03/Nov/20 07:07;lzljs3620320;release-1.11: c677334727bc388fd9fd051c7aac0cf6e595d82c;;;","18/Nov/20 03:08;xtsong;[~lirui] [~lzljs3620320]

Should this ticket be closed now?;;;","18/Nov/20 03:24;lirui;[~xintongsong] Yes it can be closed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TPC-H end-to-end test (Blink Planner): Encodings that differ from the schema are not supported yet for CsvTableSources.,FLINK-16265,13287370,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,rmetzger,rmetzger,24/Feb/20 23:25,28/Feb/20 09:59,13/Jul/23 08:07,28/Feb/20 09:59,1.11.0,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,"https://travis-ci.org/apache/flink/jobs/654409371

{code}
Dispatcher REST endpoint is up.
Running query #1...
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:190)
Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context.
	at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:779)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.openSession(LocalExecutor.java:228)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:98)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178)
Caused by: org.apache.flink.table.api.TableException: Encodings that differ from the schema are not supported yet for CsvTableSources.
	at org.apache.flink.table.sources.CsvTableSourceFactoryBase.createTableSource(CsvTableSourceFactoryBase.java:127)
	at org.apache.flink.table.sources.CsvAppendTableSourceFactory.createStreamTableSource(CsvAppendTableSourceFactory.java:46)
	at org.apache.flink.table.factories.StreamTableSourceFactory.createTableSource(StreamTableSourceFactory.java:55)
	at org.apache.flink.table.factories.TableSourceFactory.createTableSource(TableSourceFactory.java:63)
	at org.apache.flink.table.factories.TableSourceFactory.createTableSource(TableSourceFactory.java:74)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.createTableSource(ExecutionContext.java:384)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$6(ExecutionContext.java:585)
	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:583)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:520)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:165)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:122)
	at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:768)
	... 3 more
[FAIL] Test script contains errors.
{code}",,jark,lzljs3620320,rmetzger,twalthr,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #11205: [FLINK-16265][table] Use LogicalType to equals in TableSchema
URL: https://github.com/apache/flink/pull/11205
 
 
   
   ## What is the purpose of the change
   
   ```
     - name: TableNumber2
       type: source
       $VAR_UPDATE_MODE
       schema:
         - name: IntegerField2
           type: INT
         - name: StringField2
           type: VARCHAR
         - name: TimestampField3
           type: TIMESTAMP
       connector:
         type: filesystem
         path: ""$VAR_SOURCE_PATH2""
       format:
         type: csv
         fields:
           - name: IntegerField2
             type: INT
           - name: StringField2
             type: VARCHAR
           - name: TimestampField3
             type: TIMESTAMP
         line-delimiter: ""\n""
         comment-prefix: ""#""
   ```
   Table like this will fail in SQL-CLI.
   The root cause is we will convert the properties into CatalogTableImpl and then convert into properties again. The schema type properties will use new type systems then which is not equal to the legacy types.
   
   ## Brief change log
   
   I think the fix we can do could be comparing LogicalType in TableSchema.equals/hashCode instead of DataType with conversion classes.
   
   ## Verifying this change
   
   `LocalExecutorITCase`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 04:32;githubbot;600","JingsongLi commented on pull request #11214: [FLINK-16265][table][csv] CsvTableSourceFactoryBase should compare LogicalTypes instead of TableSchema
URL: https://github.com/apache/flink/pull/11214
 
 
   ## What is the purpose of the change
   
   ```
     - name: TableNumber2
       type: source
       $VAR_UPDATE_MODE
       schema:
         - name: IntegerField2
           type: INT
         - name: StringField2
           type: VARCHAR
         - name: TimestampField3
           type: TIMESTAMP
       connector:
         type: filesystem
         path: ""$VAR_SOURCE_PATH2""
       format:
         type: csv
         fields:
           - name: IntegerField2
             type: INT
           - name: StringField2
             type: VARCHAR
           - name: TimestampField3
             type: TIMESTAMP
         line-delimiter: ""\n""
         comment-prefix: ""#""
   ```
   Table like this will fail in SQL-CLI.
   The root cause is we will convert the properties into CatalogTableImpl and then convert into properties again. The schema type properties will use new type systems then which is not equal to the legacy types due to conversion classes.
   
   ## Brief change log
   
   We should avoid compare TableSchema in `CsvTableSourceFactoryBase.createTableSource`.
   
   ## Verifying this change
   
   `LocalExecutorITCase`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 13:02;githubbot;600","JingsongLi commented on pull request #11214: [FLINK-16265][table][csv] CsvTableSourceFactoryBase should compare LogicalTypes instead of TableSchema
URL: https://github.com/apache/flink/pull/11214
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/20 01:59;githubbot;600","JingsongLi commented on pull request #11205: [FLINK-16265][table] Use LogicalType to equals in TableSchema
URL: https://github.com/apache/flink/pull/11205
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/20 02:01;githubbot;600","JingsongLi commented on pull request #11229: [FLINK-16265][table][csv] CsvTableSinkFactoryBase should compare LogicalTypes instead of TableSchema
URL: https://github.com/apache/flink/pull/11229
 
 
   ## What is the purpose of the change
   
   ```
     - name: TableSourceSink
       type: source-sink-table
       schema:
         - name: IntegerField
           type: INT
         - name: StringField
           type: VARCHAR
         - name: TimestampField
           type: TIMESTAMP
       connector:
         type: filesystem
       format:
         type: csv
         fields:
           - name: IntegerField
             type: INT
           - name: StringField
             type: VARCHAR
           - name: TimestampField
             type: TIMESTAMP
         line-delimiter: ""\n""
         comment-prefix: ""#""
   ```
   Table like this will fail in SQL-CLI.
   The root cause is we will convert the properties into CatalogTableImpl and then convert into properties again. The schema type properties will use new type systems then which is not equal to the legacy types due to conversion classes.
   
   ## Brief change log
   
   We should avoid compare TableSchema in `CsvTableSinkFactoryBase.createTableSink`.
   
   ## Verifying this change
   
   `LocalExecutorITCase`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 02:05;githubbot;600","JingsongLi commented on pull request #11229: [FLINK-16265][table][csv] CsvTableSinkFactoryBase should compare LogicalTypes instead of TableSchema
URL: https://github.com/apache/flink/pull/11229
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 05:52;githubbot;600",,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16270,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 09:59:44 UTC 2020,,,,,,,,,,"0|z0bueg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/20 03:16;jark;I think this is also introduced by FLINK-15912. I reproduced the problem using the following yaml:


{code:java}
tables:
  - name: lineitem
    type: source-table
    update-mode: append
    connector:
      type: filesystem
      path: ""$TABLE_DIR/lineitem.csv""
    format:
      type: csv
      fields:
        - name: l_linestatus
          type: VARCHAR
        - name: l_shipdate
          type: DATE
        - name: l_commitdate
          type: TIMESTAMP
      field-delimiter: ""|""
      line-delimiter: ""\n""
      comment-prefix: ""--""
    schema:
      - name: l_linestatus
        type: VARCHAR
      - name: l_shipdate
        type: DATE
      - name: l_commitdate
        type: TIMESTAMP
{code}

The root cause is we will convert the properties into CatalogTableImpl and then convert into properties again. The schema type properties will use new type systems then which is not equal to the legacy types. 

;;;","25/Feb/20 04:04;lzljs3620320;This is something related to FLINK-15038.

{{TableSchema}} should not contains various conversion classes.;;;","25/Feb/20 04:05;lzljs3620320;I think the fix we can do could be comparing {{LogicalType}} in {{TableSchema.equals/hashCode}} instead of DataType with conversion classes.;;;","25/Feb/20 12:31;twalthr;I think {{TableSchema}} should be just a POJO and forward what it contains. Thus, a equals/hashCode should verify the DataType entirely. Of course it doesn't make much sense that {{TableSchema}} contains conversion classes, but in the future this class will not be exposed to users anymore. It will only be used in {{CatalogTable}}. Usually, I suspect that the problem lies somewhere else either ""during the comparison of schemas"" or ""extraction what is actually needed from the schema"".;;;","25/Feb/20 12:46;lzljs3620320;Thanks [~twalthr] and [~jark] for your involving. You have the same thoughts: FLINK-16270 is invalid. I like the sentence: ""this class will not be exposed to users anymore"".

What do you think about present situation, It appears outside the planner and work with table(source/sink), which means that it may have inconsistent conversion classes, so the advice we should give is to avoid calling equals/hashcode? If it is in this way, I should modify {{CsvTableSourceFactoryBase.createTableSource}}.;;;","26/Feb/20 02:00;lzljs3620320;Master: 7a283be4093f7258dd33ba13200e32b22fb582ca

Wait for next E2E test.;;;","26/Feb/20 08:55;rmetzger;Thanks for the quick fix. We have to wait for another night to see if the issue is fixed.;;;","26/Feb/20 23:00;rmetzger;I believe the error has not been fixed: https://travis-ci.org/apache/flink/jobs/655356423;;;","27/Feb/20 01:38;lzljs3620320;[~rmetzger] Thanks for reporting, sorry for missing sink, it is another fail, but similar reason.

I'll create a pull request to fix it asap.;;;","27/Feb/20 01:39;jark;I think we missed to fix it in {{CsvTableSinkFactoryBase}}.;;;","27/Feb/20 05:54;lzljs3620320;Fix csv sink in master: 293fd5a9b4fd27c99dd6e7c9856d7a7af2ddfade;;;","28/Feb/20 09:59;rmetzger;Thank you all!

I've closed this ticket as the nightly test is green again!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLClientKafkaITCase fails with: Could not map the schema field 'rowtime' to a field from source.,FLINK-16264,13287369,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,rmetzger,rmetzger,24/Feb/20 23:21,28/Feb/20 10:00,13/Jul/23 08:07,28/Feb/20 10:00,1.11.0,,,,,1.11.0,,,,Table SQL / Client,Tests,,,,0,pull-request-available,test-stability,,,"https://travis-ci.org/apache/flink/jobs/654409366

{code}
18:42:33.227 [INFO] Running org.apache.flink.tests.util.kafka.SQLClientKafkaITCase
18:44:08.035 [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 94.798 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientKafkaITCase
18:44:08.041 [ERROR] testKafka[0: kafka-version:0.10 kafka-sql-version:.*kafka-0.10.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase)  Time elapsed: 32.305 s  <<< ERROR!
java.io.IOException: 
Process execution failed due error. Error output:Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:190)
Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context.
	at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:779)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.openSession(LocalExecutor.java:228)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:98)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178)
Caused by: org.apache.flink.table.api.ValidationException: Could not map the schema field 'rowtime' to a field from source. Please specify the source field from which it can be derived.
	at org.apache.flink.table.descriptors.SchemaValidator.deriveFieldMapping(SchemaValidator.java:302)
	at org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase.createStreamTableSource(KafkaTableSourceSinkFactoryBase.java:170)
	at org.apache.flink.table.factories.StreamTableSourceFactory.createTableSource(StreamTableSourceFactory.java:55)
	at org.apache.flink.table.factories.TableSourceFactory.createTableSource(TableSourceFactory.java:63)
	at org.apache.flink.table.factories.TableSourceFactory.createTableSource(TableSourceFactory.java:74)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.createTableSource(ExecutionContext.java:384)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$6(ExecutionContext.java:585)
	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:583)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:520)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:165)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:122)
	at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:768)
	... 3 more

	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:175)
	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:148)

{code}
",,lzljs3620320,rmetzger,,,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #11204: [FLINK-16264][table] Fix ConnectTableDescriptor loose time attribute bug
URL: https://github.com/apache/flink/pull/11204
 
 
   
   ## What is the purpose of the change
   
   In `CatalogTableImpl.fromProperties`, we can not remove all `schema.*` for TableSchema, because Schema (it is a descriptor) is not same to TableSchema.  Schema contains time attribute, so we need keep them in properties.
   
   ## Brief change log
   
   Remove per key for TableSchema in `CatalogTableImpl.fromProperties`.
   Add test `ConnectTableDescriptorTest`.
   
   ## Verifying this change
   
   `ConnectTableDescriptorTest`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 03:13;githubbot;600","JingsongLi commented on pull request #11204: [FLINK-16264][table] Fix ConnectTableDescriptor loose time attribute bug
URL: https://github.com/apache/flink/pull/11204
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 12:29;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 10:00:47 UTC 2020,,,,,,,,,,"0|z0bue8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/20 02:35;lzljs3620320;Thanks [~rmetzger] for reporting, it is introduced in FLINK-15912 , I will fix it and add related test.;;;","25/Feb/20 12:31;lzljs3620320;Merged master: dd288c958861105b0ffc0966a8d32c1188342649

We can wait for next e2e test to verify the fixing.;;;","28/Feb/20 10:00;rmetzger;Thanks for fixing this.
The build is green again!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"BaseRowArrowReaderWriterTest/RowArrowReaderWriterTest sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available",FLINK-16263,13287366,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,rmetzger,rmetzger,24/Feb/20 23:16,25/Feb/20 09:28,13/Jul/23 08:07,25/Feb/20 09:26,1.11.0,,,,,1.11.0,,,,API / Python,Tests,,,,0,pull-request-available,test-stability,,,"https://travis-ci.org/apache/flink/jobs/654409364

{code}
18:17:45.003 [INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.924 s - in org.apache.flink.table.runtime.arrow.ArrowUtilsTest
18:17:45.019 [INFO] Running org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest
sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available
java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available
	at io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:399)
	at io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:257)
	at io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:247)
	at io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:248)
	at org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:228)
	at org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:242)
	at org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132)
	at org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120)
	at org.apache.flink.table.runtime.arrow.ArrowReaderWriterTestBase.testBasicFunctionality(ArrowReaderWriterTestBase.java:68)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
18:17:45.128 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.102 s <<< FAILURE! - in org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest
18:17:45.128 [ERROR] testBasicFunctionality(org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest)  Time elapsed: 0.097 s  <<< FAILURE!
java.lang.AssertionError: Exception in test: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available

18:17:45.143 [INFO] Running org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest
sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available
java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available
	at io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:399)
	at io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:257)
	at io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:247)
	at io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:248)
	at org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:228)
	at org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:242)
	at org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132)
	at org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120)
	at org.apache.flink.table.runtime.arrow.ArrowReaderWriterTestBase.testBasicFunctionality(ArrowReaderWriterTestBase.java:68)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
18:17:45.209 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.057 s <<< FAILURE! - in org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest
18:17:45.209 [ERROR] testBasicFunctionality(org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest)  Time elapsed: 0.056 s  <<< FAILURE!
java.lang.AssertionError: Exception in test: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available
{code}",,dian.fu,hequn8128,rmetzger,,,,,,,,,,,,,,,,,,,"dianfu commented on pull request #11206: [FLINK-16263][python][tests] Set io.netty.tryReflectionSetAccessible to true for JDK9+
URL: https://github.com/apache/flink/pull/11206
 
 
   
   ## What is the purpose of the change
   
   *This pull request sets io.netty.tryReflectionSetAccessible to true for JDK9+. See https://issues.apache.org/jira/browse/ARROW-5412 for more details.*
   
   
   ## Brief change log
   
     - *Adds `-Dio.netty.tryReflectionSetAccessible=true` option to the flink-python unit tests*
   
   ## Verifying this change
   
   Tests manually in my local environment.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 06:07;githubbot;600","hequn8128 commented on pull request #11206: [FLINK-16263][python][tests] Set io.netty.tryReflectionSetAccessible to true for JDK9+
URL: https://github.com/apache/flink/pull/11206
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 09:25;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16121,,FLINK-16273,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 25 09:25:59 UTC 2020,,,,,,,,,,"0|z0budk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/20 01:16;dian.fu;[~rmetzger] Thanks for reporting this issue. I will take a look at.;;;","25/Feb/20 09:25;hequn8128;Fixed in master via 93da5ecf096657e22e2692da73346b5ddaf5feba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Class loader problem with FlinkKafkaProducer.Semantic.EXACTLY_ONCE and usrlib directory,FLINK-16262,13287287,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,maguowei,jkreileder,jkreileder,24/Feb/20 18:38,02/Apr/20 10:37,13/Jul/23 08:07,28/Mar/20 13:43,1.10.0,,,,,1.10.1,1.11.0,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"We're using Docker images modeled after [https://github.com/apache/flink/blob/master/flink-container/docker/Dockerfile] (using Java 11)

When I try to switch a Kafka producer from AT_LEAST_ONCE to EXACTLY_ONCE, the taskmanager startup fails with:
{code:java}
2020-02-24 18:25:16.389 INFO  o.a.f.r.t.Task                           Create Case Fixer -> Sink: Findings local-krei04-kba-digitalweb-uc1 (1/1) (72f7764c6f6c614e5355562ed3d27209) switched from RUNNING to FAILED.
org.apache.kafka.common.config.ConfigException: Invalid value org.apache.kafka.common.serialization.ByteArraySerializer for configuration key.serializer: Class org.apache.kafka.common.serialization.ByteArraySerializer could not be found.
 at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:718)
 at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:471)
 at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:464)
 at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:62)
 at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:75)
 at org.apache.kafka.clients.producer.ProducerConfig.<init>(ProducerConfig.java:396)
 at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:326)
 at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:298)
 at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.<init>(FlinkKafkaInternalProducer.java:76)
 at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.lambda$abortTransactions$2(FlinkKafkaProducer.java:1107)
 at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(Unknown Source)
 at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source)
 at java.base/java.util.stream.AbstractPipeline.copyInto(Unknown Source)
 at java.base/java.util.stream.ForEachOps$ForEachTask.compute(Unknown Source)
 at java.base/java.util.concurrent.CountedCompleter.exec(Unknown Source)
 at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
 at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
 at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
 at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
 at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source){code}
This looks like a class loading issue: If I copy our JAR to FLINK_LIB_DIR instead of FLINK_USR_LIB_DIR, everything works fine.

(AT_LEAST_ONCE producers works fine with the JAR in FLINK_USR_LIB_DIR)

 ",openjdk:11-jre with a slightly modified Flink 1.10.0 build (nothing changed regarding Kafka and/or class loading).,azagrebin,begginghard,gaoyunhaii,gjy,jkreileder,kevin.cyj,kezhuw,maguowei,pnowojski,Shadowell,stevenz3wu,zjwang,,,,,,,,,,"guoweiM commented on pull request #11247: [FLINK-16262][Connectors] Set the context classloader for parallel stream in FlinkKafkaProducer
URL: https://github.com/apache/flink/pull/11247
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *Set the context classloader of the parrallel stream with the context classloader of task thread.*
   
   
   ## Brief change log
   
   - *Retrieve the classloader of the task and set the classloader for the parallel stream*
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 15:21;githubbot;600","guoweiM commented on pull request #11497: [FLINK-16262][Connectors] Set the context classloader for parallel stream in FlinkKafkaProducer
URL: https://github.com/apache/flink/pull/11497
 
 
   This PR picks [pull-11247](https://github.com/apache/flink/pull/11247) to release-1.10.
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Mar/20 08:18;githubbot;600","zhijiangW commented on pull request #11247: [FLINK-16262][Connectors] Set the context classloader for parallel stream in FlinkKafkaProducer
URL: https://github.com/apache/flink/pull/11247
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Mar/20 13:24;githubbot;600","zhijiangW commented on pull request #11497: [FLINK-16262][Connectors] Set the context classloader for parallel stream in FlinkKafkaProducer
URL: https://github.com/apache/flink/pull/11497
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Mar/20 13:26;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,FLINK-13498,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 28 13:43:40 UTC 2020,,,,,,,,,,"0|z0bu48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/20 11:00;gjy;[~maguowei] Do you see something suspicious? ;;;","26/Feb/20 10:44;maguowei;Hi, [~jkreileder] could you provide more information? such as
 # The version of Kafka connector
 #  List the files in the lib and usrlib directories in the container.
 #  Do you change the docker-entrypoint.sh ?;;;","26/Feb/20 12:48;jkreileder;Hi [~maguowei],

1. We're using the universal connector, i.e. flink-connector-kafka_2.12

2. 
{code:java}
flink@2194d2cf5949:~$ ls -al lib usrlib
lib:
total 105236
drwxr-xr-x 2 flink flink      4096 Feb 24 13:47 .
drwxr-xr-x 1 flink flink      4096 Feb 25 12:35 ..
-rw-r--r-- 1 flink flink 101157478 Feb 12 22:39 flink-dist_2.12-1.10.0-empolis.jar
-rw-r--r-- 1 flink flink   1051320 Feb 12 22:36 flink-metrics-influxdb-1.10.0-empolis.jar
-rw-r--r-- 1 flink flink    489884 Feb 12 21:52 log4j-1.2.17.jar
-rw-r--r-- 1 flink flink   4210517 Feb  9 10:45 ojdbc8-19.3.0.0.jar
-rw-r--r-- 1 flink flink    825943 Feb  9 10:45 postgresql-42.2.5.jar
-rw-r--r-- 1 flink flink      9931 Feb 12 21:52 slf4j-log4j12-1.7.15.jar


usrlib:
total 40176
drwxr-xr-x 2 flink flink     4096 Feb 25 12:35 .
drwxr-xr-x 1 flink flink     4096 Feb 25 12:35 ..
-rw-r--r-- 1 flink flink 41129331 Feb 25 12:35 iana-stack.jar{code}
flink-1.10.0-empolis is a Java 11/Scala 2.12 build.
 iana-stack.jar is an uber JAR which has flink-connector-kafka_2.12 and kafka-clients:2.2.0 included.
{code:java}
[info]   | +-org.apache.flink:flink-connector-kafka-base_2.12:1.10.0-empolis
[info]   | | +-com.google.code.findbugs:jsr305:1.3.9
[info]   | | +-org.apache.flink:force-shading:1.10.0-empolis
[info]   | | +-org.slf4j:slf4j-api:1.7.15 (evicted by: 1.7.25)
[info]   | | +-org.slf4j:slf4j-api:1.7.25
[info]   | | 
[info]   | +-org.apache.flink:force-shading:1.10.0-empolis
[info]   | +-org.apache.kafka:kafka-clients:2.2.0
[info]   | | +-com.github.luben:zstd-jni:1.3.8-1
[info]   | | +-org.lz4:lz4-java:1.5.0
[info]   | | +-org.slf4j:slf4j-api:1.7.25
[info]   | | +-org.xerial.snappy:snappy-java:1.1.7.2
{code}
3. Yes, we're use a customized docker entrypoint based on [https://github.com/docker-flink/docker-flink/blob/master/docker-entrypoint.sh]. The only changes are env-variable based modifications of conf/log4j-console.properties and generation of savepoint options.

 

 ;;;","26/Feb/20 16:45;maguowei;1. The direct cause of this problem should be that FlinkKafkaProducer has been using system classloader instead of user classloader in _abortTransactions_ in whatever situation.
2. FlinkKafkaProducer: line 1098 _transactionalIds.parallelStream().forEach(.....)_. This would use a static thread pool and the context Classloader of this thread pool should be the system.

I think this is a FlinkKafkaProducer bug that should be fixed.  I would contact the committer to double-check it tomorrow.

Thanks for your reporting.;;;","27/Feb/20 12:51;pnowojski;Yes, I think [~maguowei] is right. The problem is with {{parallelStream()}} not setting the thread context ClassLoader. I think the easiest solution would be to set the correct context class loader manually (we have a helper class for that {{org.apache.flink.util.TemporaryClassLoaderContext}}). 

Also I don't think this is a blocker strictly speaking, as this error is probably in the code since Flink 1.5. Or has something changed in Flink 1.10? ;;;","27/Feb/20 12:56;gjy;[~pnowojski] In 1.10 we enabled user code class loading for per-job clusters. Before the job jar was just added to the classpath.;;;","27/Feb/20 14:41;pnowojski;Ok, [~gjy]. I think this is a combination of what you said and this change: https://issues.apache.org/jira/browse/FLINK-13498 which actually introduced the bug. So it's a new thing in 1.10. I've restored the BLOCKER status. 

It would be nice to provide a test coverage for this issue, but I don't see if it could be done without adding another end to end test? I'm not sure if that's worth it?;;;","02/Mar/20 05:55;maguowei;Thanks for [~gjy]'s  explanation. This also reminds one thing. Currently, in the Yarn/Mesos per job the user class loader is not enabled by default. I think maybe we should keep the same behavior in per-job clusters. For example we could provide a arguments –with-usrlib to build.sh. Only if user give this parameter to build.sh we should copy the user jar to usrlib/ directory.

What do you think [~gjy]?;;;","02/Mar/20 10:28;gjy;[~maguowei] It sounds reasonable to make this behavior configurable for the docker images. That should be a new ticket however. IIRC [~azagrebin] is currently consolidating our Dockerfiles.
;;;","03/Mar/20 13:38;azagrebin;As a quick fix for build.sh, I would be ok with –with-usrlib.
For the note, there is a preliminary plan for refactoring user facing docker components (still to discuss as FLIP). There we want to rather improve user documentation about how to extend the mainstream docker hub image instead of maintaining the build.sh script. In this regard, we may end up completely removing the scripts.;;;","04/Mar/20 09:23;pnowojski;I think there is no need for quick fix, as we have the proper fix almost done (just missing test coverage);;;","18/Mar/20 15:07;maguowei;hi, [~jkreileder] could you reproduce this problem in the session mode?;;;","28/Mar/20 13:43;zjwang;Merged in release-1.10: e39cfe7660daaeed4213f04ccbce6de1e8d90fe5

Merged in master: ff0d0c979d7cf67648ecf91850e782e99d557240;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docs don't build,FLINK-16255,13287160,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,chesnay,chesnay,24/Feb/20 09:00,05/Mar/20 11:12,13/Jul/23 08:07,04/Mar/20 10:34,1.11.0,,,,,,,,,Documentation,,,,,0,,,,,"[https://ci.apache.org/builders/flink-docs-master/builds/1748/steps/Build%20docs/logs/stdio]
{code:java}
Liquid Exception: Could not find document 'dev/datastream_api.md' in tag 'link'. Make sure the document exists and the path is correct. in concepts/index.zh.md {code}
 ",,aljoscha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15999,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 11:12:31 UTC 2020,,,,,,,,,,"0|z0btc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/20 09:01;chesnay;ping [~aljoscha];;;","04/Mar/20 10:34;aljoscha;master: [https://github.com/apache/flink/commit/095da703bb91b82ca281d61896c396614ceebcfd];;;","05/Mar/20 11:12;aljoscha;It's building again: https://ci.apache.org/builders/flink-docs-master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update quickstart documentation to Log4j2,FLINK-16243,13287052,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/Feb/20 11:25,23/Feb/20 16:28,13/Jul/23 08:07,23/Feb/20 16:28,1.11.0,,,,,1.11.0,,,,Documentation,Quickstarts,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11193: [FLINK-16243][docs] Update quickstarts to Log4j2 
URL: https://github.com/apache/flink/pull/11193
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Feb/20 11:27;githubbot;600","zentol commented on pull request #11193: [FLINK-16243][docs] Update quickstarts to Log4j2 
URL: https://github.com/apache/flink/pull/11193
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Feb/20 16:28;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15672,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 23 16:28:46 UTC 2020,,,,,,,,,,"0|z0bso0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/20 16:28;chesnay;master: https://github.com/flink-ci/flink;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinaryGeneric serialization error cause checkpoint failure,FLINK-16242,13287046,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wind_ljy,wind_ljy,wind_ljy,23/Feb/20 09:53,17/Apr/20 01:55,13/Jul/23 08:07,26/Feb/20 02:12,1.9.2,,,,,1.10.1,1.11.0,1.9.3,,Table SQL / Planner,Table SQL / Runtime,,,,0,pull-request-available,,,,"The serialization error occurs from time to time when we're using {{RoaringBitmap}} as the accumulator of a UDAF.

I've attached the screenshot of the error.",,jark,libenchao,lzljs3620320,wind_ljy,,,,,,,,,,,,,,,,,,"buptljy commented on pull request #11194: [FLINK-16242] [Table/SQL] - duplicate field serializers in BaseRowSerializer
URL: https://github.com/apache/flink/pull/11194
 
 
   ## What is the purpose of the change
   
   Fix bug in duplicate() fucntion in #BaseRowSerializer
   
   ## Brief change log
   
   Use duplicated field serializers in #BaseRowSerializer#duplicate()
   
   
   
   ## Verifying this change
   
   ## Does this pull request potentially affect one of the following parts:
     - The serializers: yes
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Feb/20 12:08;githubbot;600","JingsongLi commented on pull request #11194: [FLINK-16242][table-runtime-blink] Duplicate field serializers in BaseRowSerializer
URL: https://github.com/apache/flink/pull/11194
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 06:57;githubbot;600","Jiayi-Liao commented on pull request #11211: [FLINK-16242][table-runtime-blink] Duplicate field serializers in Bas…
URL: https://github.com/apache/flink/pull/11211
 
 
   #11194  for release-1.9
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 08:58;githubbot;600","Jiayi-Liao commented on pull request #11212: [FLINK-16242][table-runtime-blink] Duplicate field serializers in BaseRowSerializer
URL: https://github.com/apache/flink/pull/11212
 
 
   #11194 for release-1.10
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 09:00;githubbot;600","JingsongLi commented on pull request #11211: [FLINK-16242][table-runtime-blink] Duplicate field serializers in Bas…
URL: https://github.com/apache/flink/pull/11211
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/20 02:08;githubbot;600","JingsongLi commented on pull request #11212: [FLINK-16242][table-runtime-blink] Duplicate field serializers in BaseRowSerializer
URL: https://github.com/apache/flink/pull/11212
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/20 02:11;githubbot;600",,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13702,,,,,,,,,"23/Feb/20 09:53;wind_ljy;error_serialization;https://issues.apache.org/jira/secure/attachment/12994238/error_serialization",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 02:12:34 UTC 2020,,,,,,,,,,"0|z0bsmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/20 10:07;wind_ljy;Based on the stack trace, It looks like we forgot to duplicate the field serializers when duplicating {{BaseRowSerializer}}. And it works fine after I duplicate the field serializers like this
{code:java}
TypeSerializer<?>[] duplicateFieldSerializers = new TypeSerializer[fieldSerializers.length];
for (int i = 0; i < fieldSerializers.length; i++) {
        duplicateFieldSerializers[i] = fieldSerializers[i].duplicate();
}
return new BaseRowSerializer(types, duplicateFieldSerializers);
{code};;;","23/Feb/20 11:23;libenchao;cc [~lzljs3620320];;;","23/Feb/20 11:56;lzljs3620320;Thanks [~wind_ljy] for reporting, and you are right, it is bug here.;;;","23/Feb/20 11:59;jark;Yes. I think we should duplicate field serializers. ;;;","26/Feb/20 02:12;lzljs3620320;master: 3a830be76a4a5657ee7b7b6669b167afe0fe332a

1.9: 754b3f16c009b715ab1b1d4578642d8145b84cd7

1.10: d09c8068ed747d564defabb735e7e10de6c77992;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the license and notice file in flink-ml-lib module on release-1.10 branch,FLINK-16241,13287032,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hequn8128,hequn8128,hequn8128,23/Feb/20 03:48,10/Mar/20 09:20,13/Jul/23 08:07,10/Mar/20 09:20,1.10.0,,,,,1.10.1,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,"The jar of flink-ml-lib should not contain the license and notice file as it actually does not bundle the related dependencies. We should remove these file on branch release-1.10.

BTW. The release-1.9 branch does not have this problem since the license and notice are added in 1.10. And on master(1.11), we will bundle the dependencies, so the license and notice file should be kept, see FLINK-15847.",,hequn8128,liyu,rongr,,,,,,,,,,,,,,,,,,,"hequn8128 commented on pull request #11350: [FLINK-16241][ml] Remove the license and notice file in flink-ml-lib module on release-1.10 branch
URL: https://github.com/apache/flink/pull/11350
 
 
   
   ## What is the purpose of the change
   
   The jar of flink-ml-lib should not contain the license and notice file as it actually does not bundle the related dependencies. We should remove these file on branch release-1.10.
   
   This pull request removes the license and notice file in flink-ml-lib module on branch release-1.10.
   
   ## Brief change log
   
     - Removes the license and notice file in flink-ml-lib module on branch release-1.10.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/20 05:52;githubbot;600","hequn8128 commented on pull request #11350: [FLINK-16241][ml] Remove the license and notice file in flink-ml-lib module on release-1.10 branch
URL: https://github.com/apache/flink/pull/11350
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Mar/20 09:18;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 10 09:20:11 UTC 2020,,,,,,,,,,"0|z0bsjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/20 06:40;hequn8128;There are two options to solve this problem:
1. Bundle the dependencies in the flink-ml-lib jar.
2. Remove the license and notice file.

Since it seems not good to add dependencies in a hotfix version, maybe it's better to remove the license and notice file directly(option 2). 
What do you think?  [~chesnay] [~rongr];;;","24/Feb/20 16:40;rongr;I think removing the license seems to be more reasonable.
Even if we decided to change how flink-ml is released in subsequence 1.10.x versions, we can always add them back.

;;;","08/Mar/20 17:26;liyu;Thanks for reporting this issue [~hequn8128]. Please let me know when the PR is prepared and you need someone to review. Thanks.;;;","10/Mar/20 09:20;hequn8128;Fixed in 1.10.1 via d969226cd7ac4bf572a05b9d6b04f650a62faf29;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several places missing Log4j2 configuration property,FLINK-16237,13287023,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,guoyangze,guoyangze,23/Feb/20 00:30,24/Feb/20 10:44,13/Jul/23 08:07,24/Feb/20 10:44,1.11.0,,,,,1.11.0,,,,Tests,,,,,0,pull-request-available,test-stability,,,"Wordcount on Docker test (custom fs plugin) fails on Travis. [https://travis-ci.org/KarmaGYZ/flink/builds/653829954]
{code:bash}
pass WordCount
Checking for errors...
Found error in log files:
Attaching to docker_job-cluster_1
job-cluster_1  | Starting the job-cluster
job-cluster_1  | Starting standalonejob as a console application on host 042c3c490edc.
job-cluster_1  | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
Attaching to docker_taskmanager_1
taskmanager_1  | Starting the task-manager
taskmanager_1  | Starting taskexecutor as a console application on host 059bd37e4232.
taskmanager_1  | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
Checking for exceptions...
{code}",,guoyangze,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11192: [FLINK-16237][build] Add Log4j2 configuration properties
URL: https://github.com/apache/flink/pull/11192
 
 
   The property for configuration the log4j filepath has changed after switching to Log4j2 (from `log4j.configuration` to `log4j.configurationFile`).
   
   All production scripts/utils must be adjusted to specify both properties to retain Log4j1 support.
   In testing code we only refer to `configurationFile` naturally, as we only use Log4j2 here.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Feb/20 11:00;githubbot;600","zentol commented on pull request #11192: [FLINK-16237][build] Add Log4j2 configuration properties
URL: https://github.com/apache/flink/pull/11192
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Feb/20 10:44;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-16253,,,,,,,,FLINK-15672,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 24 10:44:40 UTC 2020,,,,,,,,,,"0|z0bshk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/20 00:33;guoyangze;It seems related to FLINK-15672. cc: [~chesnay];;;","24/Feb/20 10:44;chesnay;master: 99690832d812ca883bab1df9f96ffc523320438d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unstable cases in StreamingJobGraphGeneratorTest,FLINK-16234,13286988,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,cpugputpu,cpugputpu,22/Feb/20 14:50,27/Feb/20 07:39,13/Jul/23 08:07,27/Feb/20 07:39,1.10.0,,,,,1.10.1,1.11.0,,,,,,,,0,pull-request-available,,,,"The test in _org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest#testSlotSharingOnAllVerticesInSameSlotSharingGroupByDefaultDisabled_ will cause the following failure:

java.lang.AssertionError: expected:<SlotSharingGroup feca28aff5a3958840bee985ee7de4d3> but was:<SlotSharingGroup 798f7268aeb5fde00858b7c9723d65f1>
at org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.assertSameSlotSharingGroup(StreamingJobGraphGeneratorTest.java:843)
at org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.testSlotSharingOnAllVerticesInSameSlotSharingGroupByDefaultDisabled(StreamingJobGraphGeneratorTest.java:814)

I analyze the assertion failure and find that the root cause of it lies in the clear() method in StreamGraph.java, where the variable _sources_ is initialized as a HashSet. Because the iteration order of HashSet is non-deterministic, so the test becomes flaky. 

The fix is to change _HashSet_ to _LinkedHashSet_ and then the failure above is removed. 

 

The stacktrace information is presented as follows for your reference:

java.util.HashSet.iterator(HashSet.java:173)
org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.setChaining(StreamingJobGraphGenerator.java:251)
org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:166)
org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:104)
org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:100)
org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.testSlotSharingOnAllVerticesInSameSlotSharingGroupByDefaultDisabled(StreamingJobGraphGeneratorTest.java:803)

 

 ",,cpugputpu,zhuzh,,,,,,,,,,,,,,,,,,,,"cpugputpu commented on pull request #11187: [FLINK-16234]Use LinkedHashSet for a deterministic iteration order
URL: https://github.com/apache/flink/pull/11187
 
 
   This PR aims to solve the issue presented here: https://issues.apache.org/jira/browse/FLINK-16234
   ## What is the purpose of the change
   
   The fix is to change the HashSet to LinkedHashSet to make the tests more stable.
   
   ## Verifying this change
   
   This change is already covered by existing tests, and it can pass them successfully.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers:  don't know
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Feb/20 15:02;githubbot;600","zhuzhurk commented on pull request #11187: [FLINK-16234][tests] Fix unstable cases in StreamingJobGraphGeneratorTest
URL: https://github.com/apache/flink/pull/11187
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 07:23;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,FLINK-16235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"The specification about HashSet says that ""it makes no guarantees as to the iteration order of the set; in particular, it does not guarantee that the order will remain constant over time"".
The documentation here is for your reference:
https://docs.oracle.com/javase/8/docs/api/java/util/HashSet.html",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 27 07:39:59 UTC 2020,,,,,,,,,,"0|z0bs9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/20 07:39;zhuzh;Fixed via:
master: 39e75eb2f1be9f6ce872e558bb8cf234897dfbfc
release-1.10: af0f78b4083226a05e7dfc036938793237df5fff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive connector missing log4j1 exclusions against certain hive versions,FLINK-16233,13286977,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/Feb/20 10:15,22/Feb/20 14:14,13/Jul/23 08:07,22/Feb/20 14:14,1.11.0,,,,,1.11.0,,,,Build System,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11185: [FLINK-16233][hive][build] Add additional log4j 1 exclusions
URL: https://github.com/apache/flink/pull/11185
 
 
   Adds several missing log4j1 exclusions, that are only necessary for certain hive versions.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Feb/20 10:19;githubbot;600","zentol commented on pull request #11185: [FLINK-16233][hive][build] Add additional log4j 1 exclusions
URL: https://github.com/apache/flink/pull/11185
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Feb/20 14:13;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 22 14:14:18 UTC 2020,,,,,,,,,,"0|z0bs7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/20 14:14;chesnay;master: 494f94beeb51ac8d93cac9abe232014f26112b8f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive connector is missing jdk.tools exclusion against Hive 2.x.x,FLINK-16231,13286975,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/Feb/20 10:14,26/Feb/20 12:22,13/Jul/23 08:07,26/Feb/20 12:22,1.10.0,,,,,1.10.1,1.11.0,,,Build System,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11183: [FLINK-16231][hive][build] Add missing jdk.tools exclusion
URL: https://github.com/apache/flink/pull/11183
 
 
   Failed the build on Java 11 with Hive 2.x.y profiles.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Feb/20 10:18;githubbot;600","zentol commented on pull request #11183: [FLINK-16231][hive][build] Add missing jdk.tools exclusion
URL: https://github.com/apache/flink/pull/11183
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/20 12:20;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,FLINK-10725,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 12:22:27 UTC 2020,,,,,,,,,,"0|z0bs6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/20 12:22;chesnay;master: 8760769816e7dfa096f31c5c1bb7c6dba2e07e77

1.10: 3ca9a6621783e3db37e3c8103cbdaae8fc6a2bb8 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redundant double-quote in Travis profiles,FLINK-16228,13286921,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,rmetzger,rmetzger,22/Feb/20 00:11,27/Feb/20 08:24,13/Jul/23 08:07,24/Feb/20 09:31,1.11.0,,,,,1.11.0,,,,Deployment / Mesos,Tests,Travis,,,0,test-stability,,,,"In a recent cron build, the mesos wordcount test failed: https://travis-ci.org/apache/flink/jobs/653454544

{code}
2020-02-21 20:37:44,334 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Shutting MesosSessionClusterEntrypoint down with application status FAILED. Diagnostics java.lang.NoClassDefFoundError: org/apache/hadoop/security/UserGroupInformation
	at org.apache.flink.runtime.clusterframework.overlays.HadoopUserOverlay$Builder.fromEnvironment(HadoopUserOverlay.java:74)
	at org.apache.flink.mesos.util.MesosUtils.applyOverlays(MesosUtils.java:152)
	at org.apache.flink.mesos.util.MesosUtils.createContainerSpec(MesosUtils.java:131)
	at org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerFactory.createActiveResourceManager(MesosResourceManagerFactory.java:81)
	at org.apache.flink.runtime.resourcemanager.ActiveResourceManagerFactory.createResourceManager(ActiveResourceManagerFactory.java:57)
	at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:170)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:215)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518)
	at org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.main(MesosSessionClusterEntrypoint.java:126)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.security.UserGroupInformation
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	... 12 more
.
{code}
",,guoyangze,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15785,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 27 08:24:14 UTC 2020,,,,,,,,,,"0|z0bruw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/20 14:37;guoyangze;Currently, the Flink's Mesos integration is not hadoop-free, see FLINK-8247. The error message shows there is something wrong with hadoop classpath.

 

My gut feeling is that this is related to FLINK-15785.
{code:java}
env: PROFILE=""-Dinclude-hadoop -Dhadoop.version=2.8.3 -Dscala-2.12 -Pe2e-travis1,e2e-hadoop""""{code}
The double-quote at the end of this line seems to be redundant. I'm not sure whether it is the root cause at the moment. However, I'll verify it and give a PR asap.

 

Update: Travis gives green light to this test after removing the redundant double-quote. [https://travis-ci.org/KarmaGYZ/flink/builds/653943626]

[~rmetzger] Could you assign this to me?;;;","24/Feb/20 09:31;chesnay;master: 127fd56edc3a2ccdf3ce9488be634395b700e7cc;;;","27/Feb/20 08:24;rmetzger;Thanks for fixing.

A comment for everybody who finds this ticket because the end to end tests are failing because of that: I have a fix for this ready already, and will open a PR soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonRowSerializationSchema throws cast exception : NullNode cannot be cast to ArrayNode,FLINK-16220,13286808,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,libenchao,libenchao,libenchao,21/Feb/20 14:42,20/Mar/20 06:45,13/Jul/23 08:07,20/Mar/20 06:45,,,,,,1.10.1,1.11.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,"It's because the object reuse. For the below schema:
{code:java}
create table sink {
  col1 int,
  col2 array<int>
}{code}
if col2 is null, then the reused object will be {{NullNode}}. for the next record, if it's not null, we will cast the reused object {{NullNode}} to {{ArrayNode}}, which will throw cast exception.

 

cc [~jark] [~twalthr] ",,jark,libenchao,,,,,,,,,,,,,,,,,,,,"libenchao commented on pull request #11180: [FLINK-16220][json] Fix JsonRowSerializationSchema cast exception due…
URL: https://github.com/apache/flink/pull/11180
 
 
   … to object reuse
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix JsonRowSerializationSchema cast exception.
   
   ## Brief change log
   
   Add instantof before cast to avoid nested array/map/row cast exception.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   - JsonRowSerializationSchemaTest.testNestedSchema()
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Feb/20 02:54;githubbot;600","wuchong commented on pull request #11180: [FLINK-16220][json] Fix JsonRowSerializationSchema cast exception due…
URL: https://github.com/apache/flink/pull/11180
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Mar/20 06:44;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,FLINK-16628,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 20 06:45:56 UTC 2020,,,,,,,,,,"0|z0br5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/20 06:45;jark;Fixed in 
 - master (1.11.0): 0619a5b3e000855f00d11e71ae2f1b9deef05c98
 - 1.10.1: 4cde4be46213b73dcb90864b103c61caa0e22666;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileUtilsTest fails on Mac OS,FLINK-16198,13286736,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fishzhe,azagrebin,azagrebin,21/Feb/20 11:20,10/Jun/20 08:54,13/Jul/23 08:07,10/Jun/20 08:54,,,,,,1.12.0,,,,FileSystems,Tests,,,,0,pull-request-available,starter,,,"The following tests fail if run on Mac OS (IDE/maven).

 

FileUtilsTest.testCompressionOnRelativePath: 
{code:java}
java.nio.file.NoSuchFileException: ../../../../../var/folders/67/v4yp_42d21j6_n8k1h556h0c0000gn/T/junit6496651678375117676/compressDir/rootDirjava.nio.file.NoSuchFileException: ../../../../../var/folders/67/v4yp_42d21j6_n8k1h556h0c0000gn/T/junit6496651678375117676/compressDir/rootDir
 at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) at java.nio.file.Files.createDirectory(Files.java:674) at org.apache.flink.util.FileUtilsTest.verifyDirectoryCompression(FileUtilsTest.java:440) at org.apache.flink.util.FileUtilsTest.testCompressionOnRelativePath(FileUtilsTest.java:261) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}
 

FileUtilsTest.testDeleteDirectoryConcurrently: 
{code:java}
java.nio.file.FileSystemException: /var/folders/67/v4yp_42d21j6_n8k1h556h0c0000gn/T/junit7558825557740784886/junit3566161583262218465/ab1fa0bde8b22cad58b717508c7a7300/121fdf5f7b057183843ed2e1298f9b66/6598025f390d3084d69c98b36e542fe2/8db7cd9c063396a19a86f5b63ce53f66: Invalid argument	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectoryInternal(FileUtils.java:324)
	at org.apache.flink.util.FileUtils.guardIfWindows(FileUtils.java:391)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectory(FileUtils.java:258)
	at org.apache.flink.util.FileUtils.cleanDirectoryInternal(FileUtils.java:376)
	at org.apache.flink.util.FileUtils.deleteDirectoryInternal(FileUtils.java:335)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectoryInternal(FileUtils.java:320)
	at org.apache.flink.util.FileUtils.guardIfWindows(FileUtils.java:391)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectory(FileUtils.java:258)
	at org.apache.flink.util.FileUtils.cleanDirectoryInternal(FileUtils.java:376)
	at org.apache.flink.util.FileUtils.deleteDirectoryInternal(FileUtils.java:335)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectoryInternal(FileUtils.java:320)
	at org.apache.flink.util.FileUtils.guardIfWindows(FileUtils.java:391)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectory(FileUtils.java:258)
	at org.apache.flink.util.FileUtils.cleanDirectoryInternal(FileUtils.java:376)
	at org.apache.flink.util.FileUtils.deleteDirectoryInternal(FileUtils.java:335)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectoryInternal(FileUtils.java:320)
	at org.apache.flink.util.FileUtils.guardIfWindows(FileUtils.java:391)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectory(FileUtils.java:258)
	at org.apache.flink.util.FileUtils.cleanDirectoryInternal(FileUtils.java:376)
	at org.apache.flink.util.FileUtils.deleteDirectoryInternal(FileUtils.java:335)
	at org.apache.flink.util.FileUtils.guardIfWindows(FileUtils.java:391)
	at org.apache.flink.util.FileUtils.deleteDirectory(FileUtils.java:276)
	at org.apache.flink.util.FileUtilsTest$Deleter.go(FileUtilsTest.java:515)
	at org.apache.flink.core.testutils.CheckedThread.run(CheckedThread.java:74)java.nio.file.FileSystemException: /var/folders/67/v4yp_42d21j6_n8k1h556h0c0000gn/T/junit7558825557740784886/junit3566161583262218465/ab1fa0bde8b22cad58b717508c7a7300/121fdf5f7b057183843ed2e1298f9b66/6598025f390d3084d69c98b36e542fe2/8db7cd9c063396a19a86f5b63ce53f66: Invalid argument

	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectoryInternal(FileUtils.java:324)
	at org.apache.flink.util.FileUtils.guardIfWindows(FileUtils.java:391)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectory(FileUtils.java:258)
	at org.apache.flink.util.FileUtils.cleanDirectoryInternal(FileUtils.java:376)
	at org.apache.flink.util.FileUtils.deleteDirectoryInternal(FileUtils.java:335)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectoryInternal(FileUtils.java:320)
	at org.apache.flink.util.FileUtils.guardIfWindows(FileUtils.java:391)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectory(FileUtils.java:258)
	at org.apache.flink.util.FileUtils.cleanDirectoryInternal(FileUtils.java:376)
	at org.apache.flink.util.FileUtils.deleteDirectoryInternal(FileUtils.java:335)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectoryInternal(FileUtils.java:320)
	at org.apache.flink.util.FileUtils.guardIfWindows(FileUtils.java:391)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectory(FileUtils.java:258)
	at org.apache.flink.util.FileUtils.cleanDirectoryInternal(FileUtils.java:376)
	at org.apache.flink.util.FileUtils.deleteDirectoryInternal(FileUtils.java:335)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectoryInternal(FileUtils.java:320)
	at org.apache.flink.util.FileUtils.guardIfWindows(FileUtils.java:391)
	at org.apache.flink.util.FileUtils.deleteFileOrDirectory(FileUtils.java:258)
	at org.apache.flink.util.FileUtils.cleanDirectoryInternal(FileUtils.java:376)
	at org.apache.flink.util.FileUtils.deleteDirectoryInternal(FileUtils.java:335)
	at org.apache.flink.util.FileUtils.guardIfWindows(FileUtils.java:391)
	at org.apache.flink.util.FileUtils.deleteDirectory(FileUtils.java:276)
	at org.apache.flink.util.FileUtilsTest$Deleter.go(FileUtilsTest.java:515)
	at org.apache.flink.core.testutils.CheckedThread.run(CheckedThread.java:74)%MCEPASTEBIN%{code}
 ",,aljoscha,azagrebin,fishzhe,frank wang,klion26,rmetzger,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16229,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 08:54:27 UTC 2020,,,,,,,,,,"0|z0bqps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/20 09:00;rmetzger;I ran into this issue as well today. It's a bit dissatisfying that the tests are not passing on macs :( ;;;","28/Apr/20 08:38;frank wang;I test the example,
1. i delete the some code, the test is ok 

{quote} @Test
	public void testCompressionOnRelativePath() throws IOException {
		final java.nio.file.Path compressDir = tmp.newFolder(""compressDir"").toPath();
                 delete this code
		/*final java.nio.file.Path relativeCompressDir =
			Paths.get(new File("""").getAbsolutePath()).relativize(compressDir);
*/
		verifyDirectoryCompression(compressDir);
	}{quote}

2. i think the problems is duing to concurrency, in FileUtils.java, the guardIfWindows method
{quote}private static void guardIfWindows(ThrowingConsumer<File, IOException> toRun, File file) throws IOException {
		if (!OperatingSystem.isWindows()) {
			toRun.accept(file);
		}
		else {
			// for windows, we synchronize on a global lock, to prevent concurrent delete issues
			// >
			// in the future, we may want to find either a good way of working around file visibility
			// in Windows under concurrent operations (the behavior seems completely unpredictable)
			// or  make this locking more fine grained, for example  on directory path prefixes
			synchronized (WINDOWS_DELETE_LOCK) {
				for (int attempt = 1; attempt <= 10; attempt++) {
					try {
						toRun.accept(file);
						break;
					}
					catch (AccessDeniedException e) {
						// ah, windows...
					}

					// briefly wait and fall through the loop
					try {
						Thread.sleep(1);
					} catch (InterruptedException e) {
						// restore the interruption flag and error out of the method
						Thread.currentThread().interrupt();
						throw new IOException(""operation interrupted"");
					}
				}
			}
		}
	}{quote}
because it hasnot concurrency control on mac or linux, may be we can modify some code to follow
{quote}private static void guardIfWindows(ThrowingConsumer<File, IOException> toRun, File file) throws IOException {
		// for windows, we synchronize on a global lock, to prevent concurrent delete issues
		// >
		// in the future, we may want to find either a good way of working around file visibility
		// in Windows under concurrent operations (the behavior seems completely unpredictable)
		// or  make this locking more fine grained, for example  on directory path prefixes
		synchronized (WINDOWS_DELETE_LOCK) {
			for (int attempt = 1; attempt <= 10; attempt++) {
				try {
					toRun.accept(file);
					break;
				}
				catch (AccessDeniedException e) {
					// ah, windows...
				}

				// briefly wait and fall through the loop
				try {
					Thread.sleep(1);
				} catch (InterruptedException e) {
					// restore the interruption flag and error out of the method
					Thread.currentThread().interrupt();
					throw new IOException(""operation interrupted"");
				}
			}
		}
	}{quote}
;;;","03/May/20 01:48;fishzhe;h3. Failed test case 1: FileUtilsTest.testCompressionOnRelativePath

*Root cause*: failed to create directory using relative path when preparing file for testing.

*Solution*: Since goal of this test case is to verify correctness of FileUtils.compressDirectory and FileUtils.expandDirectory
 Thus refactor verifyDirectoryCompression to prepare test data using absolute path while we will still use relative path to test FileUtils method will be sufficient. 
h3. Failed test case 2: FileUtilsTest.testDeleteDirectoryConcurrently

*Root cause:* FileUtils.deleteDirectory is not thread safe. This test case is not always failing. In my local env(MacOS), it failed 7,8 times out of 10. The test case is creating 3 level file hierarchy like below:

 a

   - b

      - e

   - c 

      - d

Then the test case started 3 threads. All of them are trying to iterate from root directory: a. Thus, race condition will happen. There will be a chance 2 threads trying to delete /a/b/e/ at the same time. When this happens the test will file. Maybe I'm missing something given [this comment|#L248]

*Solution:* make [deleteFileOrDirectoryInternal|#L317] synchronized will protect the test from failing and [guardIfWindows|[https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/util/FileUtils.java#L389]] can be removed. But it also means concurrent deletion is disabled if 2 threads trying to delete from the same root. 

 

If above proposal sounds reasonable to you, feel free to assign this issue to me. I'm also more than glad to talk about both of them. ;;;","27/May/20 17:56;aljoscha;Something seems to be wrong with this util. If I change {{guardIfWindows()}} to also synchronize this on mac {{testDeleteDirectoryConcurrently}} succeeds but now {{testDeleteDirectory}} fails. I think the solution is not to make this synchronized for all operating systems, since the original intent of the method is exactly to be safe with concurrency.

Regarding the relative paths: I would also not simply delete it because there was an original intent with this and the tests do fail so something seems wrong.;;;","28/May/20 10:02;sewen;[~rmetzger] I believe that. But try building it on windows, even WSL. It is even worse, no chance to pass a build.


;;;","28/May/20 12:39;aljoscha;I have downgraded to major since this behaviour has existed since Flink 1.2 (when this util was added). We should definitely fix this but not block the release.

To summarise what I said earlier:
 - the problem is that file deletion is not thread safe on MacOS (like on windows), we can add an additional guard like for windows
 - we should not make the test serialized because then it wouldn't test what it is supposed to test anymore
 - the relative paths test doesn't work on MacOS, we shouldn't remove the test but find out why it doesn't work;;;","28/May/20 12:49;trohrmann;Does it make sense to disable this test for MacOS and Windows until we have (if ever) a proper solution for it (if this hasn't been done already) [~aljoscha]?;;;","28/May/20 13:03;chesnay;FileUtilsTest does not fail on Windows at the moment (I think [~sewen] was referring to Flink as a whole; with many tests failing on Windows/WSL).

We should not just disable it. It is used by too many components; if this test fails on MacOS then chances are high that others tests fail to.;;;","28/May/20 15:40;aljoscha;The test was only recently un-ignored: FLINK-16065. I think the solution is to extend the same guard we have for windows to macOS, because the method just is not thread safe on macOS.

[~fishzhe] Would you be interested in doing this?;;;","28/May/20 15:52;sewen;I agree with Aljoscha and Chesnay: Keep all tests, they all serve an important purpose.
  - serialize cleanup on MaxOS just like on Windows
  - Diagnose and adjust the relative path directory zipping test

I assume support for parallel cleanup is most important on Unix/Linux where most of the production stuff runs.;;;","29/May/20 06:10;fishzhe;Hi, [~aljoscha] I'm more than happy to take this. 


Since this is about two test cases, I'm planning to create 2 sub issues for them. But I'm also OK to deal with them together in this ticket. Let me know which way you prefer. Feel free to assign this issue to me. 

 

Besides, for the zipping test. I think it's also easy to fix, because the test trying to prepare data using relative path, which break the test. We can just use absolute path to prepare data, when test compressDirectory method feed relative path. 

Please let me know if this makes sense to you. 

 

 ;;;","29/May/20 08:28;aljoscha;I think we can keep only this one issue. I'm assigning it to you.;;;","05/Jun/20 04:49;fishzhe;PR: [https://github.com/apache/flink/pull/12413];;;","10/Jun/20 08:54;aljoscha;master: 20e82afbb36d42be5b18f8e8a0fe45b122e645e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to query partitioned table when partition folder is removed,FLINK-16197,13286690,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,21/Feb/20 09:17,25/May/20 02:25,13/Jul/23 08:07,25/May/20 02:24,1.10.1,,,,,1.11.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"If a partition exists in HMS but not in HDFS, the query fails with
{noformat}
Caused by: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://...........
        at org.apache.hadoop.mapred.LocatedFileStatusFetcher.getFileStatuses(LocatedFileStatusFetcher.java:155)
        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:237)
        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
        at org.apache.flink.connectors.hive.read.HiveTableInputFormat.createInputSplits(HiveTableInputFormat.java:219)
        at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:152)
{noformat}
Expected behavior is to run the query ignoring this partition.",,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11175: [FLINK-16197][hive] Failed to query partitioned table when partition …
URL: https://github.com/apache/flink/pull/11175
 
 
   …folder is removed
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   To fix the issue of querying a table whose partition folder is removed.
   
   
   ## Brief change log
   
     - When generating input splits, skip a partition if the data folder doesn't exist.
     - Add test
   
   
   ## Verifying this change
   
   Added test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Feb/20 13:27;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 25 02:25:25 UTC 2020,,,,,,,,,,"0|z0bqfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/20 02:24;lzljs3620320;master: e8767af47f1d85318ee4853f678df24075e2e794

release-1.11: bdffe91c6ae81a2cdb6033d241946c8d09b96c7c;;;","25/May/20 02:25;lzljs3620320;[~lirui] If you think it is worth to finish in 1.10 too, please re-open this and submit a PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speed up ElasticsearchITCase#testInvalidElasticsearchCluster,FLINK-16186,13286441,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,20/Feb/20 10:37,21/Feb/20 17:45,13/Jul/23 08:07,21/Feb/20 17:45,1.9.0,,,,,1.11.0,,,,Connectors / ElasticSearch,Tests,,,,0,pull-request-available,,,,"{{ElasticsearchITCase#testInvalidElasticsearchCluster}} runs for a significant time (30-40 seconds), as it tries to connect to a non-existent clusters.

We should find a way to reduce the timeout, or implement the test in some other fashion.",,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11157: [FLINK-16186][es][tests] Reduce connect timeout to 5 seconds
URL: https://github.com/apache/flink/pull/11157
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Feb/20 17:45;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 21 17:45:48 UTC 2020,,,,,,,,,,"0|z0bowg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/20 17:45;chesnay;master: 9511243a902b44f48d33cc8cd752ba3ec999f2c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove invalid check in input type inference logic,FLINK-16182,13286405,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,20/Feb/20 08:22,20/Feb/20 17:50,13/Jul/23 08:07,20/Feb/20 17:50,,,,,,1.11.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"In {{org.apache.flink.table.types.inference.TypeInferenceUtil#inferInputTypes}} we force that the input type inference must never return {{NULL}} type.

This precondition will never be satisfied when trying to infer output type based on surrounding info. This check is invalid anyway.",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,"dawidwys commented on pull request #11152: [FLINK-16182][table-api] Remove check against null types as a result of an input type inference
URL: https://github.com/apache/flink/pull/11152
 
 
   ## What is the purpose of the change
   
   It removes invalid check for null types as a result of an input type inference. Moreover it always fail for inferring output type based on surrounding info.
   
   
   ## Verifying this change
   The behaviour in chained calls will be tested as part of FLINK-16033
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Feb/20 08:26;githubbot;600","dawidwys commented on pull request #11152: [FLINK-16182][table-api] Remove check against null types as a result of an input type inference
URL: https://github.com/apache/flink/pull/11152
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Feb/20 17:49;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 20 17:50:34 UTC 2020,,,,,,,,,,"0|z0boog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/20 17:50;dwysakowicz;Fixed in 333e374883666f0f55fe23faa0f7db07dfe14b4a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IfCallGen will throw NPE for primitive types in blink,FLINK-16181,13286368,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,libenchao,libenchao,libenchao,20/Feb/20 02:54,14/Jul/20 05:55,13/Jul/23 08:07,14/Jul/20 05:55,1.10.1,1.11.0,1.9.3,,,1.11.1,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"It can be reproduced by a simple test case:

Add below to {{ScalarOperatorsTest}}
{code:java}
testSqlApi(""IF(true, CAST('non-numeric' AS BIGINT), 0)"", ""null"")
{code}
 

IMO, it's {{IfCallGen}}'s bug, which should judge the nullTerm of operands first before assignment.

cc [~lzljs3620320] [~jark]",,jark,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,"libenchao commented on pull request #11161: [FLINK-16181][table-planner-blink] Fix IfCallGen throw NPE when opera…
URL: https://github.com/apache/flink/pull/11161
 
 
   …nd's resultTerm is null
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix IfCallGen throw NPE when operand's resultTerm is null
   
   ## Brief change log
   
   Fix IfCallGen throw NPE when operand's resultTerm is null
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   - ScalarOperatersTest.testOtherExpressions()
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Feb/20 14:21;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 14 05:54:15 UTC 2020,,,,,,,,,,"0|z0bog8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/20 02:59;lzljs3620320;Thanks [~libenchao] for reporting.

You are right, it is a bug. We should deal with nullTerms. Do you want to fix this?;;;","20/Feb/20 03:08;libenchao;[~lzljs3620320] Thanks for your confirmation, I'd like to fix this.;;;","14/Jul/20 05:54;libenchao;Fixed via:

1.12.0: d677f02853ed7395730c6282cd33bb0f5bbf4bd3

1.11.1: 39b4778a3eaae81902203d558335ed4fbbd8e379;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SearchTemplateRequest ClassNotFoundException when use flink-sql-connector-elasticsearch7,FLINK-16170,13286226,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,leonard,jark,jark,19/Feb/20 12:24,30/Apr/20 13:47,13/Jul/23 08:07,26/Mar/20 11:41,1.10.0,,,,,1.10.1,1.11.0,,,Connectors / ElasticSearch,,,,,0,pull-request-available,test-stability,,,"When run SQL CLI with elasticsearch7, when running a query insert into elasticsearch, a 
SearchTemplateRequest ClassNotFoundException will be thrown.

{code:java}
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
  at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110)
  at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76)
  at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)
  at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:186)
  at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:180)
  at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:484)
  at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
  at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
  at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
  at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
  at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
  at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
  at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
  at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
  at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
  at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
  at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
  at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
  at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
  at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
  at akka.actor.ActorCell.invoke(ActorCell.scala:561)
  at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
  at akka.dispatch.Mailbox.run(Mailbox.scala:225)
  at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
  at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
  at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
  at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
  at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/elasticsearch7/shaded/org/elasticsearch/script/mustache/SearchTemplateRequest
  at org.apache.flink.streaming.connectors.elasticsearch7.Elasticsearch7ApiCallBridge.createClient(Elasticsearch7ApiCallBridge.java:76)
  at org.apache.flink.streaming.connectors.elasticsearch7.Elasticsearch7ApiCallBridge.createClient(Elasticsearch7ApiCallBridge.java:48)
  at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.open(ElasticsearchSinkBase.java:299)
  at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
  at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
  at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:48)
  at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1007)
  at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)
  at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)
  at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)
  at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)
  at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
  at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
  at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.elasticsearch7.shaded.org.elasticsearch.script.mustache.SearchTemplateRequest
  at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:60)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  ... 14 more
{code}

It seems that the {{flink-sql-connector-elasticsearch7}} has 3 more exclusion than {{flink-sql-connector-elasticsearch6}}:

{code:java}
<exclude>org.elasticsearch:elasticsearch-geo</exclude>
<exclude>org.elasticsearch.plugin:lang-mustache-client</exclude>				<exclude>com.github.spullara.mustache.java:compiler</exclude>
{code}

I guess this is the root case. 
E2E test didn't alert this problem because we only test against elasticsearch6 in {{test_sql_client.sh}}. We should also add elasticsearch7 e2e there. 
",,godfreyhe,jark,leonard,libenchao,liyu,,,,,,,,,,,,,,,,,"leonardBang commented on pull request #11396: [FLINK-16170][connectors/elasticsearch]SearchTemplateRequest ClassNotFoundException when use flink-sql-connector-elasticsearch7
URL: https://github.com/apache/flink/pull/11396
 
 
   [FLINK-16170][Connectors/elasticsearch]SearchTemplateRequest ClassNotFoundException when use flink-sql-connector-elasticsearch7
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *This pull request fix the ClassNotFoundException  flink-sql-connector-elasticsearch7*
   
   
   ## Brief change log
   
     - *Correct the shade exclusions*
     - *Remove relocation rule for log4j2*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage,
   but I checked the flink-sql-connector-elasticsearch7_2.11-1.11-SNAPSHOT.jar in sql-client :
   `
   Flink SQL> CREATE TABLE es_table (
   >   aggId varchar ,
   >   pageId varchar ,
   >   ts varchar ,
   >   expoCnt int ,
   >   clkCnt int
   > ) WITH (
   > 'connector.type' = 'elasticsearch',
   > 'connector.version' = '7',
   > 'connector.hosts' = 'http://localhost:9200',
   > 'connector.index' = 'cli_test',
   > 'connector.document-type' = '_doc',
   > 'update-mode' = 'upsert',
   > 'connector.key-delimiter' = '$',
   > 'connector.key-null-literal' = 'n/a',
   > 'connector.bulk-flush.interval' = '1000',
   > 'format.type' = 'json'
   > );
   [INFO] Table has been created.
   
   Flink SQL> INSERT INTO es_table
   >   SELECT  pageId,eventId,cast(recvTime as varchar) as ts, 1, 1 from csv;
   [INFO] Submitting SQL update statement to the cluster...
   [INFO] Table update statement has been successfully submitted to the cluster:
   Job ID: c7aa42cf8d8982167361eae157922503
   `
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Mar/20 04:59;githubbot;600","wuchong commented on pull request #11396: [FLINK-16170][connectors/elasticsearch]SearchTemplateRequest ClassNotFoundException when use flink-sql-connector-elasticsearch7
URL: https://github.com/apache/flink/pull/11396
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 02:13;githubbot;600","leonardBang commented on pull request #11508: [FLINK-16170][connectors/elasticsearch]SearchTemplateRequest ClassNotFoundException when use flink-sql-connector-elasticsearch7
URL: https://github.com/apache/flink/pull/11508
 
 
   [FLINK-16170][Connectors/elasticsearch]SearchTemplateRequest ClassNotFoundException when use flink-sql-connector-elasticsearch7
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *This pull request fix the ClassNotFoundException  flink-sql-connector-elasticsearch7*
   
   
   ## Brief change log
   
     - *Correct the shade exclusions*
     - *Exclude Log4j-charsets.properties file when shade*
   
   ## Verifying this change
   
     - *Add end-to-end test in test_sql_client.sh for elasticsearch7*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 11:45;githubbot;600","wuchong commented on pull request #11508: [FLINK-16170][connectors/elasticsearch]SearchTemplateRequest ClassNotFoundException when use flink-sql-connector-elasticsearch7
URL: https://github.com/apache/flink/pull/11508
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 11:40;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,FLINK-17123,,,,,,,,,,,,,FLINK-17483,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 02:15:35 UTC 2020,,,,,,,,,,"0|z0bnko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/20 12:24;jark;cc [~twalthr], [~aljoscha];;;","19/Feb/20 13:43;leonard;Hi，[~jark] Thanks for your report.
I get same Exception when I use 'flink-sql-connector-elasticsearch7_2.11'  in pom.xml.  Looks like we miss something when shade.


 ;;;","19/Feb/20 14:58;aljoscha;Yep, it seems that the ES7 client needs the mustache dependency. :(;;;","20/Feb/20 13:32;libenchao;seems {{com.carrotsearch:hppc}} is also needed.;;;","10/Mar/20 12:16;liyu;Setting affect version to 1.10.0 since this issue seems to be caused by/related to the shading mechanism we introduced in 1.10.0.;;;","10/Mar/20 15:18;jark;You are right. [~liyu] thank you. ;;;","25/Mar/20 02:15;jark;Fixed in
 - master (1.11.0): 1827e4dddfbac75a533ff2aea2f3e690777a3e5e
 - 1.10.1: 2e96e49e8a1f4b467441c1cb7ace5fb1190a2d94;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadoc doclint option not working on Java 11,FLINK-16166,13286214,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,19/Feb/20 10:57,25/Mar/20 17:10,13/Jul/23 08:07,25/Mar/20 17:09,1.11.0,,,,,1.11.0,,,,Build System,Documentation,,,,0,pull-request-available,,,,The doclint option is overridden in the java 11 profile.,,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11402: [FLINK-16166][build] Append additional javadoc options
URL: https://github.com/apache/flink/pull/11402
 
 
   Ensures that additional options can be specified for the `javadoc-plugin` in profiles/modules.
   Without this change the javadoc-plugin configuration in the java 11 profile overrides the doclint option, since the `additionalJOptions` attribute is an array which are merged by index.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Mar/20 12:01;githubbot;600","zentol commented on pull request #11402: [FLINK-16166][build] Append additional javadoc options
URL: https://github.com/apache/flink/pull/11402
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 17:09;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,FLINK-15633,,,,,,FLINK-10725,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 17:09:10 UTC 2020,,,,,,,,,,"0|z0bni0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/20 17:09;chesnay;master: 66101d8781207326885f644fe2ad1ca4fd65f495;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump flink-shaded to 10.0,FLINK-16162,13286182,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,19/Feb/20 09:22,19/Feb/20 16:22,13/Jul/23 08:07,19/Feb/20 16:22,,,,,,1.11.0,,,,Build System,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11131: [FLINK-16162][build] Bump flink-shaded to 10.0
URL: https://github.com/apache/flink/pull/11131
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Feb/20 09:24;githubbot;600","zentol commented on pull request #11131: [FLINK-16162][build] Bump flink-shaded to 10.0
URL: https://github.com/apache/flink/pull/11131
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Feb/20 16:22;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 16:22:43 UTC 2020,,,,,,,,,,"0|z0bnaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/20 16:22;chesnay;master: 57215c4a5254198329ab15a366ab8a274502763e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Statistics zero should be unknown in HiveCatalog,FLINK-16161,13286180,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,19/Feb/20 09:04,25/Feb/20 01:34,13/Jul/23 08:07,24/Feb/20 11:31,1.10.0,,,,,1.10.1,1.11.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,"In hive, treat statistics zero as unknown, but in Flink HiveCatalog, treat zero as real value.

This lead wrong inputs to CBO.

Previous discussed in [https://github.com/apache/flink/pull/10380]",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #11199: [FLINK-16161][hive] Statistics zero should be unknown in HiveCatalog
URL: https://github.com/apache/flink/pull/11199
 
 
   
   ## What is the purpose of the change
   
   In hive, treat statistics zero as unknown, but in Flink HiveCatalog, treat zero as real value.
   This lead wrong inputs to CBO.
   Previous discussed in https://github.com/apache/flink/pull/10380
   
   ## Brief change log
   
   Fix in `HiveCatalog`. treat zero as unknown value.
   
   ## Verifying this change
   
   `HiveCatalogHiveMetadataTest.testHiveStatistics`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Feb/20 08:38;githubbot;600","JingsongLi commented on pull request #11199: [FLINK-16161][hive] Statistics zero should be unknown in HiveCatalog
URL: https://github.com/apache/flink/pull/11199
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Feb/20 11:29;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 24 11:31:09 UTC 2020,,,,,,,,,,"0|z0bnag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/20 11:31;lzljs3620320;master: fe9f448a446a9f5747ece9fa8b0f1b4928c17248

release-1.10: f7dcb02f933b94d773b00ea844ba66099c45b8f1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Co-location constraints are not reset on task recovery in DefaultScheduler,FLINK-16139,13285942,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,zhuzh,zhuzh,18/Feb/20 10:04,21/Feb/20 03:58,13/Jul/23 08:07,21/Feb/20 03:58,1.10.0,,,,,1.10.1,1.11.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The colocation constraints are not reset on task recovery, which may lead to task recovery failures when allocating slots.
We should reset the colocation constraints before resetting vertices, just like what we do in the legacy scheduler.",,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,"zhuzhurk commented on pull request #11121: [FLINK-16139][runtime] Reset colocation constraints when restarting tasks in DefaultScheduler
URL: https://github.com/apache/flink/pull/11121
 
 
   ## What is the purpose of the change
   
   The colocation constraints are not reset on task recovery, which may lead to task recovery failures when allocating slots.
   We should reset the colocation constraints before resetting vertices, just like what we do in the legacy scheduler.
   
   ## Brief change log
   
     - *Reset colocation constraints when restarting tasks in DefaultScheduler#resetForNewExecutions(...)*
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     - *Added a unit test*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**yes** / no / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Feb/20 10:50;githubbot;600","zhuzhurk commented on pull request #11121: [FLINK-16139][runtime] Reset colocation constraints when restarting tasks in DefaultScheduler
URL: https://github.com/apache/flink/pull/11121
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Feb/20 03:53;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 21 03:58:11 UTC 2020,,,,,,,,,,"0|z0bltk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/20 10:04;zhuzh;cc [~gjy];;;","21/Feb/20 03:58;zhuzh;Fixed via:
master: 8c01397018f20865094dec8c37cf44279651a279
release-1.10: fd3e6e7dcaee213dee2e1fa1da997f55738c8aeb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testDynamicTableFunction fails,FLINK-16118,13285755,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,twalthr,roman,roman,17/Feb/20 11:21,17/Feb/20 21:05,13/Jul/23 08:07,17/Feb/20 21:05,1.11.0,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/5186/logs/16

 
{code:java}
2020-02-14T14:46:56.8515984Z [ERROR] testDynamicTableFunction(org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase) Time elapsed: 3.452 s <<< FAILURE!
 2020-02-14T14:46:56.8517003Z java.lang.AssertionError:
 2020-02-14T14:46:56.8517232Z
 2020-02-14T14:46:56.8517485Z Expected: <[Test is a string, 42, null]>
 2020-02-14T14:46:56.8517739Z but: was <[42, Test is a string, null]>
 2020-02-14T14:46:56.8518067Z at org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.testDynamicTableFunction(FunctionITCase.java:611){code}
 

 

The change was to enable chaining of the ContinuousFileReaderOperator (https://github.com/apache/flink/pull/11097).",,pnowojski,roman,twalthr,,,,,,,,,,,,,,,,,,,"twalthr commented on pull request #11111: [FLINK-16118][table-planner-blink] Ignore order for FunctionITCase.testDynamicTableFunction
URL: https://github.com/apache/flink/pull/11111
 
 
   ## What is the purpose of the change
   
   Fixes the test.
   
   ## Brief change log
   
   See commit message.
   
   ## Verifying this change
   
   This change is a trivial rework.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Feb/20 12:42;githubbot;600","pnowojski commented on pull request #11111: [FLINK-16118][table-planner-blink] Ignore order for FunctionITCase.testDynamicTableFunction
URL: https://github.com/apache/flink/pull/11111
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Feb/20 21:04;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 17 21:05:26 UTC 2020,,,,,,,,,,"0|z0bko0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/20 11:23;roman;[~twalthr], could you please take a look?;;;","17/Feb/20 11:37;pnowojski;Another instance: https://api.travis-ci.com/v3/job/287786493/log.txt;;;","17/Feb/20 12:11;twalthr;Thanks, will look into it.;;;","17/Feb/20 21:05;pnowojski;merged commit e099310;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aliyun oss filesystem could not work with plugin mechanism,FLINK-16115,13285740,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,wangyang0918,wangyang0918,17/Feb/20 10:25,26/Feb/20 13:22,13/Jul/23 08:07,26/Feb/20 13:22,1.10.0,,,,,1.10.1,1.11.0,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,"From release-1.9, Flink suggest users to load all filesystem with plugin, including oss. However, it could not work for oss filesystem. The root cause is it does not shade the {{org.apache.flink.runtime.fs.hdfs}} and {{org.apache.flink.runtime.util}}. So they will always be loaded by system classloader and throw the following exceptions.

 
{code:java}
2020-02-17 17:28:47,247 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Could not start cluster entrypoint StandaloneSessionClusterEntrypoint.
org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint StandaloneSessionClusterEntrypoint.
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:187)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518)
        at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:64)
Caused by: java.lang.NoSuchMethodError: org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.<init>(Lorg/apache/flink/fs/shaded/hadoop3/org/apache/hadoop/fs/FileSystem;)V
        at org.apache.flink.fs.osshadoop.OSSFileSystemFactory.create(OSSFileSystemFactory.java:85)
        at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:61)
        at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:441)
        at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:362)
        at org.apache.flink.core.fs.Path.getFileSystem(Path.java:298)
        at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:100)
        at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:89)
        at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:125)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:305)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:263)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:207)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169)
        at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168)
        ... 2 more
{code}",,klion26,liyu,phoenixjiangnan,wangyang0918,,,,,,,,,,,,,,,,,,"wangyang0918 commented on pull request #11117: [FLINK-16115][filesystem] Make aliyun oss filesystem could work with plugin mechanism
URL: https://github.com/apache/flink/pull/11117
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   From release-1.9, Flink suggest users to load all filesystem with plugin, including oss. However, it could not work for oss filesystem. The root cause is it does not shade the org.apache.flink.runtime.fs.hdfs and org.apache.flink.runtime.util. So they will always be loaded by system classloader.
   What we need to do is shading the two packages `org.apache.flink.runtime.fs.hdfs` and `org.apache.flink.runtime.util`, just like s3 and azure.
   
   ## Brief change log
   
   * shading two packages `org.apache.flink.runtime.fs.hdfs` and `org.apache.flink.runtime.util`
   
   
   ## Verifying this change
   
   * Configure the HA storage to oss and manually run a standalone Flink cluster 
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Feb/20 07:37;githubbot;600","zentol commented on pull request #11117: [FLINK-16115][filesystem] Make aliyun oss filesystem could work with plugin mechanism
URL: https://github.com/apache/flink/pull/11117
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/20 13:21;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16116,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 13:22:05 UTC 2020,,,,,,,,,,"0|z0bkko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/20 17:46;phoenixjiangnan;[~fly_in_gis] can you help find OSS people to fix this bug? Also bumped it to critical;;;","17/Feb/20 22:11;chesnay;ping [~AHeise]; we had the same problem with S3 IIRC .;;;","18/Feb/20 01:08;wangyang0918;To fix this issue, we need to add the following relocations. And i have confirmed that it could work. For the long term, we should also remove the shading for oss filesystem. I have create another ticket [FLINK-16116|https://issues.apache.org/jira/browse/FLINK-16116] to track.

[~chesnay] could you please assign this ticket to me? I could work on this.

 
{code:java}
<!-- shade Flink's Hadoop FS adapter classes, forces plugin classloader for them -->
<relocation>
   <pattern>org.apache.flink.runtime.fs.hdfs</pattern>
   <shadedPattern>org.apache.flink.fs.osshadoop.common</shadedPattern>
</relocation>
<!-- shade Flink's Hadoop FS utility classes, forces plugin classloader for them -->
<relocation>
   <pattern>org.apache.flink.runtime.util</pattern>
   <shadedPattern>org.apache.flink.fs.osshadoop.common</shadedPattern>
{code}
 

 ;;;","18/Feb/20 06:57;liyu;[~fly_in_gis] Just assigned to you, thanks for tracking and volunteering to fix the issue.;;;","18/Feb/20 07:54;wangyang0918;[~liyu] Thanks for your help.

I have attached a PR to fix this issue. [~AHeise] could you please take a look?;;;","26/Feb/20 13:22;chesnay;master: b4a1f0b23548210727135bda9c801d6bf3af1990

1.10: e79bae3d609bbe91288e0b8b99f14fe1bc0e8e9c ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExpressionReducer shouldn't escape the reduced string value,FLINK-16113,13285715,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,jark,jark,17/Feb/20 08:50,19/Apr/20 11:38,13/Jul/23 08:07,17/Feb/20 14:49,1.10.0,,,,,1.10.1,1.11.0,1.9.4,,Table SQL / Planner,,,,,0,pull-request-available,,,,"ExpressionReducer shouldn't escape the reduced string value, the escaping should only happen in code generation, otherwise the output result is inccorect. 

The problem is this line I guess: https://github.com/apache/flink/blob/master/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ExpressionReducer.scala#L142

Here is a simple example to reproduce the problem:

{code:java}
  val smallTupleData3: Seq[(Int, Long, String)] = {
    val data = new mutable.MutableList[(Int, Long, String)]
    data.+=((1, 1L, ""你好""))
    data.+=((2, 2L, ""你好""))
    data.+=((3, 2L, ""你好世界""))
    data
  }

  @Test
  def test(): Unit = {
    val t = env.fromCollection(smallTupleData3)
      .toTable(tEnv, 'a, 'b, 'c)
    tEnv.createTemporaryView(""MyTable"", t)
    val sqlQuery = s""select * from MyTable where c = '你好'""

    val result = tEnv.sqlQuery(sqlQuery).toAppendStream[Row]
    val sink = new TestingAppendSink
    result.addSink(sink)
    env.execute()
    println(sink.getAppendResults.mkString(""\n""))
  }
{code}

The output:

{code:java}
1,1,\u4F60\u597D
2,2,\u4F60\u597D
{code}

This is also mentioned in user mailing list: http://apache-flink.147419.n8.nabble.com/ParquetTableSource-blink-table-planner-tp1696p1720.html
",,jark,kyledong,libenchao,,,,,,,,,,,,,,,,,,,"wuchong commented on pull request #11108: [FLINK-16113][table-planner-blink] ExpressionReducer shouldn't escape the reduced string value
URL: https://github.com/apache/flink/pull/11108
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   ExpressionReducer shouldn't escape the reduced string value, the escaping should only happen in code generation, otherwise the output result is inccorect.
   
   ## Brief change log
   
   Remove the escape string logic from `ExpressionReducer`.
   
   ## Verifying this change
   
   Added a plan test and an integration test.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Feb/20 09:50;githubbot;600","wuchong commented on pull request #11108: [FLINK-16113][table-planner-blink] ExpressionReducer shouldn't escape the reduced string value
URL: https://github.com/apache/flink/pull/11108
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Feb/20 14:45;githubbot;600","libenchao commented on pull request #11807: [FLINK-16113][table-planner-blink] ExpressionReducer shouldn't escape…
URL: https://github.com/apache/flink/pull/11807
 
 
   … the reduced string value
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   ExpressionReducer shouldn't escape the reduced string value, the escaping should only happen in code generation, otherwise the output result is inccorect.
   
   ## Brief change log
   
   Remove the escape string logic from ExpressionReducer.
   
   ## Verifying this change
   
   Added a plan test and an integration test.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Apr/20 06:53;githubbot;600","wuchong commented on pull request #11807: [FLINK-16113][table-planner-blink] ExpressionReducer shouldn't escape…
URL: https://github.com/apache/flink/pull/11807
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Apr/20 11:36;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,FLINK-17153,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 19 11:38:05 UTC 2020,,,,,,,,,,"0|z0bkf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/20 14:49;jark;Fixed in 
 - master(1.11.0): 22899a153c774bd7d491b569e26c1a2e9fd8cd85
 - 1.10.1: ab5df9b8e7390ed1254876bc0ce71169c01f4ea4
;;;","14/Apr/20 10:12;libenchao;[~jark] this bug also affects 1.9 branch, do we need to also fix that?;;;","15/Apr/20 03:12;jark;Sure. Would you like to open a pull request for 1.9 branch? There may conflicts when cherry-pick this to 1.9. [~libenchao];;;","15/Apr/20 03:17;libenchao;[~jark] sure, I'd like to do it.;;;","19/Apr/20 11:38;jark;Fixed in 1.9.4: 0fdb8079394055cffabf5060130751250e35b620;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kubernetes deployment does not respect ""taskmanager.cpu.cores"".",FLINK-16111,13285690,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,xtsong,xtsong,17/Feb/20 07:32,25/Feb/20 09:07,13/Jul/23 08:07,25/Feb/20 09:07,1.10.0,,,,,1.10.1,1.11.0,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"The Kubernetes deployment uses `kubernetes.taskmanager.cpu` for configuring TM cpu cores, and will fallback to number-of-slots if not specified.

FLINK-14188 introduced a common option `taskmanager.cpu.cores` (ATM not exposed to users and for internal usage only). A common logic is to decide the TM cpu cores following the fallback order of ""common option -> K8s/Yarn/Mesos specific option -> numberOfSlot"".

The above fallback rules are not respected by the Kubernetes deployment.",,azagrebin,guoyangze,xtsong,,,,,,,,,,,,,,,,,,,"xintongsong commented on pull request #11110: [FLINK-16111][k8s] Fix Kubernetes deployment not respecting 'taskmanager.cpu.cores'.
URL: https://github.com/apache/flink/pull/11110
 
 
   ## What is the purpose of the change
   
   This PR fix that Kubernetes deployment not respecting 'taskmanager.cpu.core'.
   The expected behavior for cpu configuration is to fallback the cpu configuration with the following order:
   1. common cpu config option ('taskmanager.cpu.core')
   2. K8s/Yarn/Mesos config option ('kubernetes.taskmanager.cpu' for Kubernetes)
   3. number of slots ('taskmanager.numberOfTaskSlots')
   
   ## Brief change log
   
   - a43e139a4ffeeb06b00db4163e329dfcbfbb9f3b: Make Kubernetes to respect 'taskmanager.cpu.cores'.
   - 9db123635d9b3142f3bf9b1b61347f7d46ca4890, c330e78bc64b398d2ff483ab3f996db3958ec2fd: Hotfixes for adding test cases validating the same behavior on Yarn/Mesos deployments.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   - Add test cases in `KubernetesResourceManagerTest`
   - Add test cases in `YarnResourceManagerTest`
   - Update existing test cases in `MesosTaskManagerParametersTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Feb/20 12:06;githubbot;600","azagrebin commented on pull request #11110: [FLINK-16111][k8s] Fix Kubernetes deployment not respecting 'taskmanager.cpu.cores'.
URL: https://github.com/apache/flink/pull/11110
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 09:03;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-9953,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 25 09:07:39 UTC 2020,,,,,,,,,,"0|z0bk9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/20 07:33;xtsong;cc [~fly_in_gis] [~azagrebin];;;","25/Feb/20 09:07;azagrebin;merged into master by 28365501edb2671efe6160c921325829d9e088d3
merged into 1.10 by ac2aaf9277333a6d8ac5aa1c0c81189f56e6ffd4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamSQLExample is failed if running in blink planner,FLINK-16108,13285666,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,jark,jark,17/Feb/20 03:30,25/Dec/20 03:03,13/Jul/23 08:07,05/Mar/20 11:42,,,,,,1.10.1,1.11.0,,,Table SQL / Planner,,,,,0,,,,,"{{StreamSQLExample}} in flink-example will fail if the specified planner is blink planner. Exception is as following:

{code}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink  do not match.
Query schema: [user: BIGINT, product: STRING, amount: INT]
Sink schema: [amount: INT, product: STRING, user: BIGINT]
	at org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSchemaAndApplyImplicitCast(TableSinkUtils.scala:96)
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:229)
	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:150)
	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:150)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:150)
	at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.java:361)
	at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.java:269)
	at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.java:260)
	at org.apache.flink.table.examples.java.StreamSQLExample.main(StreamSQLExample.java:90)

Process finished with exit code 1
{code}

That's because blink planner will also validate the sink schema even if it is come from {{toAppendStream()}}. However, the {{TableSinkUtils#inferSinkPhysicalDataType}} should derive sink schema from query schema when the requested type is POJO [1], because fields order of POJO is not deterministic.


[1]: https://github.com/apache/flink/blob/master/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/TableSinkUtils.scala#L237


",,gyfora,jark,libenchao,wtog,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 25 03:02:14 UTC 2020,,,,,,,,,,"0|z0bk48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/20 11:42;jark;Fixed in
 - mater(1.11.0): 377024ba85058e3ba5fa092aaf5c92be33d7ae09
 - 1.10.1: 9685642da56554eb5d0292f3ffe193c48329d423;;;","24/Dec/20 15:35;wtog;hi [~jark] 

[DynamicSinkUtils.validateSchemaAndApplyImplicitCast|[https://github.com/apache/flink/blob/e710bec72669b5786671b25501d2350fcc997364/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/DynamicSinkUtils.java]] throws createSchemaMismatchException() by different fields order of query schema from sink schema 

 

should the method validateSchemaAndApplyImplicitCast be rewrite to validate the datatype which the sink schema field and query schema field have the same field name
{code:java}
// code placeholder
sinkFields.forEach(sinkField -> {
   RowField queryField = queryFields.stream()
         .filter(q -> q.getName().equals(sinkField.getName()))
         .findFirst()
         .orElseThrow(() -> createSchemaMismatchException(
         String.format(
               ""Not found query column for sink column '%s' at position"",
               sinkField.getName()
         ),
         sinkIdentifier,
         queryFields,
         sinkFields));

   final LogicalType queryColumnType = queryField.getType();
   final LogicalType sinkColumnType = sinkField.getType();
   if (!supportsImplicitCast(queryColumnType, sinkColumnType)) {
      throw createSchemaMismatchException(
            String.format(
                  ""Incompatible types for sink column '%s' at position"",
                  sinkField.getName()
            ),
            sinkIdentifier,
            queryFields,
            sinkFields);
   }
   if (!supportsAvoidingCast(queryColumnType, sinkColumnType)) {
      requiresCasting.set(true);
   }
});
{code}
 

 ;;;","25/Dec/20 03:02;jark;Hi [~wtog], {{INSERT INTO}} maps fields of query to fields of sink table by the fields order, not the fields name. Because not all the query fields have field names, and it's hard to maintain field name mapping from query to sink.

This is the standard SQL behavior and also all the databases follow this rule.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blink planner can not extract correct unique key for UpsertStreamTableSink ,FLINK-16070,13285509,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,godfreyhe,leonard,leonard,15/Feb/20 12:51,30/Mar/20 08:00,13/Jul/23 08:07,30/Mar/20 08:00,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"I reproduce an Elasticsearch6UpsertTableSink issue which user reported in mail list[1] that Blink planner can not extract correct unique key for following query, but legacy planner works well. 
{code:java}
// user code
INSERT INTO ES6_ZHANGLE_OUTPUT  
 SELECT aggId, pageId, ts_min as ts,  
       count(case when eventId = 'exposure' then 1 else null end) as expoCnt,  
       count(case when eventId = 'click' then 1 else null end) as clkCnt  
 FROM  (    
     SELECT        
       'ZL_001' as aggId,
        pageId,        
        eventId,        
        recvTime,        
        ts2Date(recvTime) as ts_min    
     from kafka_zl_etrack_event_stream    
     where eventId in ('exposure', 'click')  
 ) as t1  
 group by aggId, pageId, ts_min
{code}
I  found that blink planner can not extract correct unique key in `*FlinkRelMetadataQuery.getUniqueKeys(relNode)*`, legacy planner works well in  `*org.apache.flink.table.plan.util.UpdatingPlanChecker.getUniqueKeyFields(...)* `. A simple ETL job to reproduce this issue can refers[2]

 

[1][http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Flink-1-10-es-sink-exception-td32773.html]

[2][https://github.com/leonardBang/flink-sql-etl/blob/master/etl-job/src/main/java/kafka2es/Kafka2UpsertEs.java]

 

 ",,danny0405,godfreyhe,jark,leonard,libenchao,liyu,lzljs3620320,Terry1897,wuyanzu,,,,,,,,,,,,,"godfreyhe commented on pull request #11158: [FLINK-16070] [table-planner-blink] blink stream planner supports remove constant keys from an aggregate
URL: https://github.com/apache/flink/pull/11158
 
 
   
   
   ## What is the purpose of the change
   
   *currently AggregateProjectPullUpConstantsRule is in cost-based phase (logical phase), while RelMdPredicates does not work when meeting RelSubset. So we can add AggregateProjectPullUpConstantsRule into hep planner to fix the bug.*
   
   ## Brief change log
   
     - *add AggregateProjectPullUpConstantsRule into default_write phase*
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Added testGroupByConstantKey method in AggregateTest to validate constant group key in queres*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Feb/20 12:21;githubbot;600","JingsongLi commented on pull request #11158: [FLINK-16070] [table-planner-blink] blink stream planner supports remove constant keys from an aggregate
URL: https://github.com/apache/flink/pull/11158
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Mar/20 05:20;githubbot;600","godfreyhe commented on pull request #11558: [FLINK-16070] [table-planner-blink] blink stream planner supports remove constant keys from an aggregate
URL: https://github.com/apache/flink/pull/11558
 
 
   
   ## What is the purpose of the change
   
   *currently AggregateProjectPullUpConstantsRule is in cost-based phase (logical phase), while RelMdPredicates does not work when meeting RelSubset. So we can add AggregateProjectPullUpConstantsRule into hep planner to fix the bug. cherry pick from https://github.com/apache/flink/pull/11158*
   
   
   ## Brief change log
   
     - *add AggregateProjectPullUpConstantsRule into default_write phase*
   
   
   ## Verifying this change
   
   
   
   This change added tests and can be verified as follows:
   
     - *Added testGroupByConstantKey method in AggregateTest to validate constant group key in queries*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Mar/20 05:29;githubbot;600","JingsongLi commented on pull request #11558: [FLINK-16070] [table-planner-blink] blink stream planner supports remove constant keys from an aggregate
URL: https://github.com/apache/flink/pull/11558
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Mar/20 07:58;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/20 06:07;wuyanzu;image-2020-03-12-14-07-37-429.png;https://issues.apache.org/jira/secure/attachment/12996497/image-2020-03-12-14-07-37-429.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 08:00:00 UTC 2020,,,,,,,,,,"0|z0bj5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/20 13:01;leonard;I'm not sure the root cause comes from blink planner or underlying Calcite up to now.

cc: [~jark] [~danny0405]

 ;;;","15/Feb/20 13:15;lzljs3620320;Hi [~Leonard Xu] , thanks for reporting.

{{FlinkRelMdUniqueKeys}} is in the flink code, it deal with unqiue keys, which means maybe it has some bugs lead to this bug. CC author: [~godfreyhe] ;;;","16/Feb/20 04:14;danny0405;Thanks for reporting this [~Leonard Xu] ~

The AggregateProjectPullUpConstantsRule would reduce the constant grouping keys of aggregate, add for blink-planner, we deduce the unique keys with aggregate grouping keys:

{code:java}
  def getUniqueKeysOnAggregate(
      grouping: Array[Int],
      mq: RelMetadataQuery,
      ignoreNulls: Boolean): util.Set[ImmutableBitSet] = {
    // group by keys form a unique key
    ImmutableSet.of(ImmutableBitSet.of(grouping.indices: _*))
  }
{code}

To fix this problem, you can append the constant grouping key to he set.

Actually i'm curious why you need the constant key in the metadata ?;;;","16/Feb/20 05:36;leonard;Hi, [~danny0405] thank you for your explanation.
the query comes from user mail list and I think it's a corner case too.;;;","16/Feb/20 07:22;jark;I think this is a bug in the {{FlinkRelMdUniqueKeys}}. I didn't look into the code, I don't know why it lost the unique key when there is a constant in the group keys. ;;;","16/Feb/20 13:36;godfreyhe;thanks for reporting this bug [~Leonard Xu]

hi [~danny0405], the constant value is already in the grouping. the plan is
{code:java}
Calc(select=[CAST(_UTF-16LE'ZL_001':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AS aggId, pageId, ts, CAST(expoCnt) AS expoCnt, CAST(clkCnt) AS clkCnt])
+- GroupAggregate(groupBy=[aggId, pageId, ts], select=[aggId, pageId, ts, COUNT($f3) AS expoCnt, COUNT($f4) AS clkCnt])
   +- Exchange(distribution=[hash[aggId, pageId, ts]])
      +- Calc(select=[_UTF-16LE'ZL_001' AS aggId, pageId, ts2Date(recvTime) AS ts, CASE(=(eventId, _UTF-16LE'exposure':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), 1, null:INTEGER) AS $f3, CASE(=(eventId, _UTF-16LE'click':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), 1, null:INTEGER) AS $f4], where=[OR(=(eventId, _UTF-16LE'exposure'), =(eventId, _UTF-16LE'click'))])
         +- TableSourceScan(table=[[default_catalog, default_database, csv, source: [CsvTableSource(read fields: pageId, eventId, recvTime)]]], fields=[pageId, eventId, recvTime])
{code}
 

I think there are two bugs:
one is in {{FlinkRelMdUniqueKeys}}, project/calc should handle constant field when deriving unique keys.
and another is the constant value should be removed from grouping key after {{AggregateProjectPullUpConstantsRule}} is applied.

I would like to fix these bugs.;;;","11/Mar/20 15:03;wuyanzu;Hi, [~godfreyhe] . 

I think meet similar problem in 1.10 . I copy the grammar 'EMIT' from blink( add SqlEmit in RichSqlInsert), 

and the first SQL is OK when translating to stream,

but the second one that does DATE_FORMAT throw the exception：

UpsertStreamTableSink requires that Table has a full primary keys if it is updated.

is it will be normal after this bug fixed?

 
{code:java}
INSERT INTO sink1 
SELECT  
COUNT(DISTINCT tradeNo), 
TUMBLE_START(tradeTimeTs, INTERVAL '10' SECOND) as ts 
FROM payInfo GROUP BY TUMBLE(tradeTimeTs, INTERVAL '10' SECOND) 
EMIT WITH DELAY '2' SECOND BEFORE WATERMARK

{code}
 

 
{code:java}
INSERT INTO sink1 
SELECT 
COUNT(DISTINCT tradeNo), 
DATE_FORMT(TUMBLE_START(tradeTimeTs, INTERVAL '10' SECOND),'yyyyMMddHHmmss') as ts 
FROM payInfo GROUP BY TUMBLE(tradeTimeTs, INTERVAL '10' SECOND) 
EMIT WITH DELAY '2' SECOND BEFORE WATERMARK

{code}
 ;;;","12/Mar/20 01:47;godfreyhe;hi [~wuyanzu] you can apply the [patch|https://github.com/apache/flink/pull/11158] to your local env. I will push this pr forward asap. ;;;","30/Mar/20 08:00;lzljs3620320;master: 179d7a64983e3a1a7e3bc956891e2a24d84248bc

release-1.10: 841c5207e2bc0db9db5f34b4387311e40289a246;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table with keyword-escaped columns and computed_column_expression columns,FLINK-16068,13285472,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,libenchao,pangliang,pangliang,15/Feb/20 03:54,02/Mar/20 06:21,13/Jul/23 08:07,18/Feb/20 07:11,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / Client,,,,,0,pull-request-available,,,,"I use sql-client to create a table with keyword-escaped column and computed_column_expression column, like this:
{code:java}
CREATE TABLE source_kafka (
    log STRING,
    `time` BIGINT,
    pt as proctime()
) WITH (
  'connector.type' = 'kafka',       
  'connector.version' = 'universal',
  'connector.topic' = 'k8s-logs',
  'connector.startup-mode' = 'latest-offset',
  'connector.properties.zookeeper.connect' = 'zk-1.zk:2181,zk-2.zk:2181,zk-3.zk:2181/kafka',
  'connector.properties.bootstrap.servers' = 'kafka.default:9092',
  'connector.properties.group.id' = 'testGroup',
  'format.type'='json',
  'format.fail-on-missing-field' = 'true',
  'update-mode' = 'append'
);
{code}
Then I simply used it :
{code:java}
SELECT * from source_kafka limit 10;{code}
got an exception:
{code:java}
java.io.IOException: Fail to run stream sql job
	at org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:164)
	at org.apache.zeppelin.flink.FlinkStreamSqlInterpreter.callSelect(FlinkStreamSqlInterpreter.java:108)
	at org.apache.zeppelin.flink.FlinkSqlInterrpeter.callCommand(FlinkSqlInterrpeter.java:203)
	at org.apache.zeppelin.flink.FlinkSqlInterrpeter.runSqlList(FlinkSqlInterrpeter.java:151)
	at org.apache.zeppelin.flink.FlinkSqlInterrpeter.interpret(FlinkSqlInterrpeter.java:104)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:676)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:569)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:121)
	at org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:39)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered ""time"" at line 1, column 12.
Was expecting one of:
    ""ABS"" ...
    ""ARRAY"" ...
    ""AVG"" ...
    ""CARDINALITY"" ...
    ""CASE"" ...
    ""CAST"" ...
    ""CEIL"" ...
    ""CEILING"" ...
    ......
    
	at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:50)
	at org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNodes(SqlExprToRexConverterImpl.java:79)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:111)
	at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3328)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2357)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2005)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:646)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:627)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3181)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:563)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:148)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:135)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:522)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:436)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:154)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:464)
	at org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:104)
	... 13 more
{code}
I also did some tests, the following can run:
{code:java}
CREATE TABLE source_kafka (
    log STRING,
    `aaaaa` BIGINT,
    pt as proctime()
)

CREATE TABLE source_kafka (
    log STRING,
    `time` BIGINT
)

CREATE TABLE source_kafka (
    log STRING,
    pt as proctime()
){code}
can not run:

`time` , `select`, `string`

 ",,jark,leonard,libenchao,pangliang,,,,,,,,,,,,,,,,,,"libenchao commented on pull request #11101: [FLINK-16068][table-planner-blink] Fix computed column with escaped k…
URL: https://github.com/apache/flink/pull/11101
 
 
   …eyword cannot work
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix a bug that computed column and escaped keyword cannot work together.
   
   ## Brief change log
   
   - Add escape to field name in computed column.
   - Add Java doc to `SqlExprToRexConverter` to remind others.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   - CatalogTableITCase. testComputedColumnWithEscapedKeywordField()
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Feb/20 15:01;githubbot;600","wuchong commented on pull request #11101: [FLINK-16068][table-planner-blink] Fix computed column with escaped k…
URL: https://github.com/apache/flink/pull/11101
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Feb/20 06:44;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,FLINK-16358,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 18 07:11:34 UTC 2020,,,,,,,,,,"0|z0bix4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/20 07:45;jark;cc [~danny0405], could you help to take a look at this? It seems there is a bug in the SQL parser when the column name is a keyword and a computed column in the definition.  ;;;","15/Feb/20 16:35;libenchao;[~pangliang] Thanks for reporting this, and your nice example sql which can be easily reproduced.

[~jark] After looking into the stack trace and source code, I found it's because in {{CatalogSourceTable}}:L103, we use field names and computed column expressions to construct a new expr list, and we simply use field name as the expression for non computed column.

IMO, a simple fix here is we can add escape character around the field name. (actually, in our internal computed column implementation, we encountered this problem as well, and resolved it this way).;;;","16/Feb/20 07:19;jark;Thanks for looking into this [~libenchao]. I think the bug is in {{org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl#convertToRexNodes}} where we should escape all field names and functions for the constructed query. 

If you are interested in it, I can assign it to you [~libenchao]. ;;;","16/Feb/20 07:55;libenchao;[~jark] I'm interested in this issue, thanks for assigning this to me.;;;","16/Feb/20 10:09;libenchao;[~jark] I looked into this again, and found that exprs in {{TableColumn}} has been escaped correctly, and it should be guaranteed by {{TableSchema.field(String name, DataType dataType, String expression)}}. 

Even not for that, escaping an expr from {{String}} type is a hard work, for example ""udf_1(col1, col2)"" should be escaped like ""`udf1`(`col1`, `col2`)"" which is a very hard work. So it's easy and natural to generate escaped expr from Calcite's {{SqlNode}} in early phase.

Then, we only need to escape column name, instead of column name and exprs.

If you are fine with this way of fixing, I'll open the pr later.;;;","16/Feb/20 11:36;jark;You are right [~libenchao]. We have added a long description for the expression parameter of {{TableSchema#field(name, type, expression)}} that the expression should be escaped. ;;;","18/Feb/20 07:11;jark;Fixed in
 - master(1.11.0): 00649491e2b05bb1ade46355b3002a5dad75c7eb
 - 1.10.1: b3c035da1f0f7ef88233f4957bcf9c2c1f06310f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink's CalciteParser swallows error position information,FLINK-16067,13285364,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,fhueske,fhueske,14/Feb/20 16:03,24/Feb/20 15:19,13/Jul/23 08:07,24/Feb/20 15:19,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"The parser used to parse SQL queries submitted through {{TableEnvironmentImpl.sqlUpdate}} does not add the original exception from Calcite as a cause to Flink's {{org.apache.flink.table.api.SqlParserException}}. 

However, Calcite's exception contains the position in the SQL query where the parser failed.
This info would help users to fix their queries.

This used to work with Flink 1.9.x.

CC [~dwysakowicz]",,dwysakowicz,fhueske,jark,libenchao,,,,,,,,,,,,,,,,,,"dawidwys commented on pull request #11172: [FLINK-16067][table] Forward Calcite exception when parsing a sql query
URL: https://github.com/apache/flink/pull/11172
 
 
   Minor change. Forwarding Calcite exception when parsing a sql query
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Feb/20 10:44;githubbot;600","dawidwys commented on pull request #11172: [FLINK-16067][table] Forward Calcite exception when parsing a sql query
URL: https://github.com/apache/flink/pull/11172
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Feb/20 15:14;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 24 15:19:29 UTC 2020,,,,,,,,,,"0|z0bi94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/20 15:19;dwysakowicz;Fixed in:
master: f3415d74212e765cbd189c73147289c1f6ba451f
1.10.1: 6f82b251669dff6c84f1045c803050ca47f8db17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unignore FileUtilsTest.testDeleteDirectoryConcurrently(),FLINK-16065,13285354,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,14/Feb/20 15:12,20/Feb/20 14:23,13/Jul/23 08:07,20/Feb/20 14:23,1.10.0,,,,,1.11.0,,,,API / Core,,,,,0,,,,,"For some reason, this test is ignored. The introducing commit was an unrelated hotfix, so I can only assume that this was by accident.",,rmetzger,sewen,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 20 14:23:51 UTC 2020,,,,,,,,,,"0|z0bi6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/20 15:35;rmetzger;Is this ticket resolved through https://github.com/apache/flink/commit/fc59aa4ecc2a7170bfda14ffadf0a30aa2b793bf ?;;;","20/Feb/20 14:23;sewen;Fixed in 1.11.0 via fc59aa4ecc2a7170bfda14ffadf0a30aa2b793bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in ContinuousFileReaderOperator,FLINK-16057,13285302,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,roman,roman,14/Feb/20 10:13,18/Nov/20 10:03,13/Jul/23 08:07,02/Jun/20 17:42,1.11.0,,,,,1.11.0,1.12.0,,,API / DataStream,Runtime / Task,,,,0,pull-request-available,,,,"After switching CFRO to a single-threaded execution model performance regression was expected to be about 15-20% (benchmarked in November).

But after merging to master it turned out to be about 50%.

  

One reason is that the chaining strategy isn't set by default in CFRO factory.

Without that even reading and outputting all records of a split in a single mail action doesn't reverse the regression (only about half).

However,  with strategy set AND batching enabled fixes the regression (starting from batch size 6).

Though batching can't be used in practice because it can significantly delay checkpointing.

 

Another approach would be to process one record and the repeat until defaultMailboxActionAvailable OR haveNewMail.

This reverses regression and even improves the performance by about 50% compared to the old version.

 

The final solution could also be FLIP-27.

 

Other things tried (didn't help):
 * CFRO rework without subsequent commits (removing checkpoint lock)
 * different batch sizes, including the whole split, without chaining strategy fixed - partial improvement only
 * disabling close
 * disabling checkpointing
 * disabling output (serialization)
 * using LinkedList instead of PriorityQueue

 ",,aljoscha,pnowojski,roman,wind_ljy,,,,,,,,,,,,,,,,,,"rkhachatryan commented on pull request #11097: [FLINK-16057][runtime] chain ContinuousFileReaderOperator by default
URL: https://github.com/apache/flink/pull/11097
 
 
   ## What is the purpose of the change
   
   *Use ChainingStrategy ALWAYS in ContinuousFileReaderOperatorFactory*
   
   ## Verifying this change
   
   This change is a trivial rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Feb/20 13:59;githubbot;600","pnowojski commented on pull request #11097: [FLINK-16057][runtime] chain ContinuousFileReaderOperator by default
URL: https://github.com/apache/flink/pull/11097
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Feb/20 08:39;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 17:42:47 UTC 2020,,,,,,,,,,"0|z0bhvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/20 08:40;pnowojski;Hopefully is partially fixed by commit 82b709f;;;","16/Apr/20 09:14;aljoscha;[~pnowojski] [~roman_khachatryan] Is there any more work happening on this one?;;;","16/Apr/20 10:27;roman;The impact was reduced to the expected levels (~20%) by the linked PR.

It probably will be eliminated by FLIP-27 entirely.

If not, the next step I think should be to assess the real impact by adding a more realistic test with IO (I suspect it's low).;;;","22/May/20 09:30;roman;Got unexpected results on actual files: newer version is faster (double-checking).

 

Old version ([http://codespeed.dak8s.net:8080/job/flink-benchmark-request/162/)|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/162/]

 
{code:java}
""Benchmark"",""Mode"",""Threads"",""Samples"",""Score"",""Score Error (99.9%)"",""Unit"",""Param: folder"" ""org.apache.flink.benchmark.ContinuousFileReaderOperatorIoBenchmark.readFiles"",""thrpt"",1,30,7352.674778,240.954385,""ops/ms"",txt-100-1000-10 ""org.apache.flink.benchmark.ContinuousFileReaderOperatorIoBenchmark.readFiles"",""thrpt"",1,30,5783.989828,102.949992,""ops/ms"",txt-1000-100-10
{code}
 

New version ([http://codespeed.dak8s.net:8080/job/flink-benchmark-request/163/)|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/163/]

 
{code:java}
""Benchmark"",""Mode"",""Threads"",""Samples"",""Score"",""Score Error (99.9%)"",""Unit"",""Param: folder"" ""org.apache.flink.benchmark.ContinuousFileReaderOperatorIoBenchmark.readFiles"",""thrpt"",1,30,16931.351736,551.851266,""ops/ms"",txt-100-1000-10 ""org.apache.flink.benchmark.ContinuousFileReaderOperatorIoBenchmark.readFiles"",""thrpt"",1,30,6156.304362,92.567005,""ops/ms"",txt-1000-100-10
{code}
 

 ;;;","25/May/20 11:49;roman;I was able to reproduce the above results both locally and on codespeed.dak8s.net (*newer* version being faster).

The benchmark: [https://github.com/dataArtisans/flink-benchmarks/pull/62];;;","25/May/20 16:36;pnowojski;Why is that the case? Why the new version should be twice as fast? It sounds strange, like we are missing something important.;;;","25/May/20 22:09;roman;I don't the reason. 

My first guess was that the benchmark is wrong, but I couldn't spot any issues.

Can you please take a look: [https://github.com/dataArtisans/flink-benchmarks/pull/62] ?

 

I also repeated the non-io-benchmark (that generates numbers without reading files):

 
{code:java}
old:
""Benchmark"",""Mode"",""Threads"",""Samples"",""Score"",""Score Error (99.9%)"",""Unit"" ""org.apache.flink.benchmark.ContinuousFileReaderOperatorBenchmark.readFileSplit"",""thrpt"",1,30,18193.296713,1288.369575,""ops/ms""
new:
""Benchmark"",""Mode"",""Threads"",""Samples"",""Score"",""Score Error (99.9%)"",""Unit"" ""org.apache.flink.benchmark.ContinuousFileReaderOperatorBenchmark.readFileSplit"",""thrpt"",1,30,12491.844935,165.684694,""ops/ms""
{code}
 ;;;","27/May/20 21:54;roman;Optimization with ""mailbox.isEmpty"" [https://github.com/rkhachatryan/flink/tree/f16057] gave some improvement but not much:

 
{code:java}
Current master (167):
""Benchmark"",""Mode"",""Threads"",""Samples"",""Score"",""Score Error (99.9%)"",""Unit"" ""org.apache.flink.benchmark.ContinuousFileReaderOperatorBenchmark.readFileSplit"",""thrpt"",1,30,12491.844935,165.684694,""ops/ms""
 
Optimized loop (168):
""Benchmark"",""Mode"",""Threads"",""Samples"",""Score"",""Score Error (99.9%)"",""Unit"" ""org.apache.flink.benchmark.ContinuousFileReaderOperatorBenchmark.readFileSplit"",""thrpt"",1,30,13757.349358,123.252329,""ops/ms""
 
Old (separate thread) (166):
""Benchmark"",""Mode"",""Threads"",""Samples"",""Score"",""Score Error (99.9%)"",""Unit"" ""org.apache.flink.benchmark.ContinuousFileReaderOperatorBenchmark.readFileSplit"",""thrpt"",1,30,18193.296713,1288.369575,""ops/ms""
{code}
 ;;;","29/May/20 09:52;roman;The problem with the improvement above was:
 # source in the benchmark stopped after the target count of elements reached
 # stream task received END_OF_INPUT and closed all the operators - without suspending default action
 # ContinousFileReadOperator saw that default action is not suspended and re-enqueued mails (no optimization)

After adding a check ""mailboxProcessor.allActionCompleted"" I got the improvement.

However, this is a bit cheating because all elements were processed in CLOSING phase in a loop.

So I updated source in the benchmark to wait until sink receives all the elements.;;;","02/Jun/20 17:42;pnowojski;Merged to release-1.11 as  6551b6b592..e514c71b09 and to master as 78b4e3da6f..31d661dfaf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException during ContinuousFileProcessingITCase,FLINK-16056,13285300,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,14/Feb/20 10:12,21/Feb/20 07:36,13/Jul/23 08:07,18/Feb/20 16:43,,,,,,1.11.0,,,,API / DataStream,,,,,0,pull-request-available,,,,"[https://api.travis-ci.org/v3/job/650176066/log.txt] 

ContinuousFileReaderOperator was changed recently to be created using a factory. But some tests create it using constructor directly. This causes problems with deserialization.

 

Another issue is that tests don't fail but rather log the exception:
{code:java}
00:16:03.935 [INFO] Running org.apache.flink.hdfstests.ContinuousFileProcessingITCase Formatting using clusterid: testClusterID org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147) at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:648) at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:77) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1620) at org.apache.flink.hdfstests.ContinuousFileProcessingITCase$1.run(ContinuousFileProcessingITCase.java:148) Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:186) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:180) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:493) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:383) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: java.lang.NullPointerException at org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.open(ContinuousFileReaderOperator.java:263) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:977) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:456) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:451) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:463) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:717) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:541) at java.lang.Thread.run(Thread.java:748) Exception in thread ""Thread-83"" java.lang.AssertionError: Job execution failed. at org.junit.Assert.fail(Assert.java:88) at org.apache.flink.hdfstests.ContinuousFileProcessingITCase$1.run(ContinuousFileProcessingITCase.java:161)

  {code}",,aljoscha,pnowojski,roman,wind_ljy,,,,,,,,,,,,,,,,,,"rkhachatryan commented on pull request #11096: [FLINK-16056][runtime][tests] fix CFRO creation in tests
URL: https://github.com/apache/flink/pull/11096
 
 
   ## What is the purpose of the change
   
   *ContinuousFileReaderOperator was changed recently to be created using factory. But some tests create it using constructor directly. This causes problems with deserialization.*
   
   
   ## Brief change log
   
     - *fix test to actually fail on exception - previously error was only logged*
     - *use factory in tests*
     - *remove testing constructor*
     - *make class package-private*
   
   ## Verifying this change
   
   This change is already covered by existing tests: `ContinuousFileProcessingITCase`, `ContinuousFileProcessingMigrationTest`, `ContinuousFileProcessingTest`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Feb/20 13:11;githubbot;600","pnowojski commented on pull request #11096: [FLINK-16056][runtime][tests] fix CFRO creation in tests
URL: https://github.com/apache/flink/pull/11096
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Feb/20 14:13;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 18 16:43:06 UTC 2020,,,,,,,,,,"0|z0bhuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/20 10:34;pnowojski;I've seen the same exception in {{ContinuousFileProcessingTest}};;;","18/Feb/20 16:43;roman;Fixed by

[https://github.com/apache/flink/pull/11096/commits/03ed8e68f6175a3fa9cb86e4ab1e22fe3844b56b];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid catalog functions when listing Hive built-in functions,FLINK-16055,13285293,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,14/Feb/20 09:50,18/Feb/20 18:34,13/Jul/23 08:07,18/Feb/20 18:34,,,,,,1.11.0,,,,Connectors / Hive,Tests,,,,0,pull-request-available,,,,"When we create catalog functions with {{hiveShell}} during tests, the functions get registered with {{FunctionRegistry}}. If later on we try to list Hive built-in functions, {{FunctionRegistry::getFunctionInfo}} will try to register the catalog functions with a session registry. And therefore fails if we don't have a {{SessionState}} in the test. The following example exposes this issue.

{code}
	@Test
	public void test() throws Exception {
		hiveShell.execute(""create function hiveudtf as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode'"");
		HiveModule hiveModule = new HiveModule(hiveCatalog.getHiveVersion());
		hiveModule.listFunctions();
	}
{code}

Fails with:
{noformat}
Caused by: java.lang.RuntimeException: Function registery for session is not initialized
	at org.apache.hadoop.hive.ql.session.SessionState.getRegistryForWrite(SessionState.java:979)
	at org.apache.hadoop.hive.ql.exec.Registry.registerToSessionRegistry(Registry.java:657)
	at org.apache.hadoop.hive.ql.exec.Registry.getQualifiedFunctionInfoUnderLock(Registry.java:616)
	at org.apache.hadoop.hive.ql.exec.Registry.getFunctionInfo(Registry.java:321)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfo(FunctionRegistry.java:549)
	at org.apache.flink.table.catalog.hive.client.HiveShimV120.getFunctionInfo(HiveShimV120.java:162)
	at org.apache.flink.table.catalog.hive.client.HiveShimV120.lambda$listBuiltInFunctions$0(HiveShimV120.java:142)
{noformat}",,dwysakowicz,lirui,,,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11093: [FLINK-16055][hive] Avoid catalog functions when listing Hive built-i…
URL: https://github.com/apache/flink/pull/11093
 
 
   …n functions
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Avoid catalog functions when listing Hive built-in functions
   
   
   ## Brief change log
   
     - Skip catalog function names when listing built-in functions
   
   
   ## Verifying this change
   
   Existing tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Feb/20 11:34;githubbot;600","bowenli86 commented on pull request #11093: [FLINK-16055][hive] Avoid catalog functions when listing Hive built-i…
URL: https://github.com/apache/flink/pull/11093
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Feb/20 18:32;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 14 10:41:55 UTC 2020,,,,,,,,,,"0|z0bhtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/20 10:01;dwysakowicz;Does this apply to tests only? Or is it also a problem for the production code?;;;","14/Feb/20 10:41;lirui;[~dwysakowicz] For now it only applies to tests, because we don't register functions with {{FunctionRegistry}} in production code.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blink planner produces wrong aggregate results with state clean up,FLINK-16047,13285095,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,twalthr,twalthr,13/Feb/20 14:16,17/Mar/20 10:27,13/Jul/23 08:07,17/Mar/20 09:16,1.9.0,,,,,1.10.1,1.11.0,1.9.3,,Table SQL / Planner,,,,,0,pull-request-available,,,,"It seems that FLINK-10674 has not been ported to the Blink planner.

Because state clean up happens in processing time, it might be the case that retractions are arriving after the state has been cleaned up. Before these changes, a new accumulator was created and invalid retraction messages were emitted. This change drops retraction messages for which no accumulator exists.

These lines are missing in {{org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction}}:
{code}
if (null == accumulators) {
      // Don't create a new accumulator for a retraction message. This
      // might happen if the retraction message is the first message for the
      // key or after a state clean up.
      if (!inputC.change) {
        return
      }
      // first accumulate message
      firstRow = true
      accumulators = function.createAccumulators()
    } else {
      firstRow = false
    }
{code}

The bug has not been verified. I spotted it only by looking at the code.",,jark,libenchao,liyu,twalthr,,,,,,,,,,,,,,,,,,"wuchong commented on pull request #11419: [FLINK-16047][table-planner-blink] Fix Blink planner produces wrong aggregate results with state clean up
URL: https://github.com/apache/flink/pull/11419
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Before these changes, a new accumulator was created and invalid retraction messages were emitted. This change drops retraction messages for which no accumulator exists.
   
   This picks the implementation and tests from FLINK-10674.
   
   ## Brief change log
   
   - drops retraction messages for which no accumulator exists.
   
   ## Verifying this change
   
   - added an integration test for distinct with retraction (passed before this change)
   - added a harness test to cover this change (failed before this change).
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/20 13:05;githubbot;600","wuchong commented on pull request #11419: [FLINK-16047][table-planner-blink] Fix Blink planner produces wrong aggregate results with state clean up
URL: https://github.com/apache/flink/pull/11419
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/20 08:49;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 17 10:27:30 UTC 2020,,,,,,,,,,"0|z0bglc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/20 14:18;twalthr;CC [~ykt836] [~jark] Is my observation correct or is the Blink planner handling it differently?;;;","14/Feb/20 06:51;jark;[~twalthr] you are right. We didn't spot this problem before. Let's fix it! ;;;","13/Mar/20 14:03;liyu;[~jark] Any update on this one? Thanks.;;;","13/Mar/20 15:39;jark;I will pick up this issue and submit a PR soon.;;;","17/Mar/20 09:16;jark;Fixed in 
 - master (1.11.0): fac0cf15765764fcf2c6c64fca159e161ac042dd
 - 1.10.1: 42efb26d0b7365d5909f256ea2e491bbd4bb0bf4
 - 1.9.3: cfa467bcc277f55c7e08724bacc5332df846b314;;;","17/Mar/20 10:27;liyu;Thanks for the efforts [~jark]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Travis failed due to python setup,FLINK-16026,13284969,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,lzljs3620320,lzljs3620320,13/Feb/20 04:22,14/Feb/20 12:10,13/Jul/23 08:07,13/Feb/20 06:32,,,,,,1.10.1,1.11.0,,,API / Python,,,,,0,pull-request-available,,,,"[https://api.travis-ci.com/v3/job/286671652/log.txt]

[https://api.travis-ci.org/v3/job/649754603/log.txt]

[https://api.travis-ci.com/v3/job/286409130/log.txt]

Collecting avro-python3<2.0.0,>=1.8.1; python_version >= ""3.0"" (from apache-beam==2.19.0->apache-flink==1.11.dev0) Using cached https://files.pythonhosted.org/packages/31/21/d98e2515e5ca0337d7e747e8065227ee77faf5c817bbb74391899613178a/avro-python3-1.9.2.tar.gz Complete output from command python setup.py egg_info: Traceback (most recent call last): File ""<string>"", line 1, in <module> File ""/tmp/pip-install-d6uvsl_b/avro-python3/setup.py"", line 41, in <module> import pycodestyle ModuleNotFoundError: No module named 'pycodestyle' ---------------------------------------- Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-install-d6uvsl_b/avro-python3/ You are using pip version 10.0.1, however version 20.0.2 is available. You should consider upgrading via the 'pip install --upgrade pip' command.",,dian.fu,hxbks2ks,lzljs3620320,sunjincheng121,,,,,,,,,,,,,,,,,,"HuangXingBo commented on pull request #11078: [FLINK-16026][python] Limits the avro-python3 version in Flink
URL: https://github.com/apache/flink/pull/11078
 
 
   ## What is the purpose of the change
   
   *This pull request will limit the avro-python3 version in Flink*
   
   ## Brief change log
   
     - *limit the avro-python3 version*
   
   
   ## Verifying this change
   
   
   This change added tests and can be verified as follows:
   
   *tox test*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Feb/20 05:00;githubbot;600","dianfu commented on pull request #11078: [FLINK-16026][python] Limits the avro-python3 version in Flink
URL: https://github.com/apache/flink/pull/11078
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Feb/20 06:25;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 14 12:10:20 UTC 2020,,,,,,,,,,"0|z0bftc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/20 04:54;hxbks2ks;This issue is caused by the latest released avro package. This package is broken and it has already been discussed in avro community to release a patched version(AVRO-2737). However, I'm not sure when the new package will be available and I suggest to limit the avro-python3 version in Flink to work around this issue. I will submit a PR ASAP.;;;","13/Feb/20 05:05;dian.fu;[~lzljs3620320] Thanks for reporting this issue.
[~hxbks2ks] Thanks for the analysis and fix. +1 to limit the version of avro-python3 in PyFlink.;;;","13/Feb/20 06:32;dian.fu;Merged to 
master via c40b0b232f4b04102b8220ced99cc4ae638a65df
release-1.10 via b59e348c4d5ae2c5e2817eca768635f9d3794278;;;","13/Feb/20 09:18;chesnay;Alright guys, this was now the 3rd or 4th instance of the build process being broken because some new dependency was released.

Can we do a full sweep over the entire dependency tree and prevent this from happening again?

[~dian.fu] [~jincheng];;;","13/Feb/20 09:52;sunjincheng121;Thanks for the reminder [~chesnay].

We have offline discussed about initializing all current dependencies to a fixed version before this patch. The current patch is a quick fix for Travis. Will feedback here when have the final conclusion.;;;","14/Feb/20 02:50;dian.fu;[~chesnay] Thanks a lot for your suggestions. Appreciated!

I have investigated this issue with [~sunjincheng121] offline and want to share our thoughts as following:
 - It's a good idea to limit the versions of the direct dependencies and I think we have already done that in PyFlink.
 - It may be not a good idea to limit the versions of the transitive dependencies to a different version declared by the direct dependency:
 -- We should trust the direct dependency, e.g. if apache-beam declares that it support avro-python3<2, we should trust that it does support all versions of avro-python3 which are less than 2.
 -- We should not change the range of versions of the transitive dependencies declared by the direct dependency, e.g. if apache-beam declares that it supports avro-python3<2, it may not be a good idea to change the version limit to avro-python3<1.9.1 in PyFlink. This will introduce a lot of issues:
 --- We have to upgrade the versions of the transitive dependencies periodically if new packages have been released
 --- For released package of PyFlink, there is no way to upgrade the versions of the transitive dependencies any more
 -- If there are exceptions for the above cases, we could address them case by case(just like this JIRA does). For example, regarding to the issue of this JIRA, it's because that avro community has released an error package. We could just disable it for the time being. This is a trade off, however, I guess most Python projects handle this kind of issues in this way.

What's your thoughts?;;;","14/Feb/20 08:00;chesnay;{{It may be not a good idea to limit the versions of the transitive dependencies to a different version declared by the direct dependency:}}

I disagree.

{{We should trust the direct dependency, e.g. if apache-beam declares that it support avro-python3<2, we should trust that it does support all versions of avro-python3 which are less than 2.}}

Not a single project can guarantee that it supports any future version of another library, as they have no control over it.
For all we now, the next version of avro-python3 breaks the entire API.
This is just a broken concept, and we see the fallout of this repeatedly in this project and in the JavaScript ecosystem.

{{We have to upgrade the versions of the transitive dependencies periodically if new packages have been released}}

Correct, we have to ensure that when we bump a dependency everything still works. I don't consider this a bad thing.

{{For released package of PyFlink, there is no way to upgrade the versions of the transitive dependencies any more}}

This is not a problem imo, but a benefit. It means the release is stable and will not magically stop working 2 years down the road.;;;","14/Feb/20 12:10;dian.fu;The management of Python dependencies is quite different from the Java dependencies. Regarding to the Java dependencies, they are bundled in the distribution and so it's fine to use any version which could work. However, for the Python dependencies, they are not bundled in the distribution and we rely on the Python dependencies provided in the execution environment for Python UDF execution. The execution environment may be shared between many applications. This means that it may not be a good idea to limit the version to a very short range if possible. For example, if we limit avro-python3 to version <= 1.9.1, what will happen if users want to share the Python environment between multiple applications and another application requires a high version, e.g. 1.9.2? This introduces unnecessary dependency problems for users as we limit the version just for test stability instead because that version is not supported actually. Regarding to the avro-python3 problem, it's not caused by API breaking in the new version. It was unexpected and an error package was released just by mistaken and avro community has already provided a quick fix to address this problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Service could expose blob server port mismatched with JM Container,FLINK-16025,13284955,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,felixzheng,felixzheng,felixzheng,13/Feb/20 01:56,10/Mar/20 10:33,13/Jul/23 08:07,10/Mar/20 10:33,1.10.0,,,,,1.10.1,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"The Service would always expose 6124 port if it should expose that port, and while building ServicePort we do not explicitly specify a target port, so the target port would always be 6124 too.
{code:java}
// From ServiceDecorator.java

servicePorts.add(getServicePort(
 getPortName(BlobServerOptions.PORT.key()),
 Constants.BLOB_SERVER_PORT));

private ServicePort getServicePort(String name, int port) {
   return new ServicePortBuilder()
      .withName(name)
      .withPort(port)
      .build();
}


{code}
 

meanwhile, the Container of the JM would expose the blob server port which is configured in the Flink Configuration,
{code:java}
// From FlinkMasterDeploymentDecorator.java

final int blobServerPort = KubernetesUtils.parsePort(flinkConfig, BlobServerOptions.PORT);

...

final Container container = createJobManagerContainer(flinkConfig, mainClass, hasLogback, hasLog4j, blobServerPort);
{code}
 

so there is a risk that in non-HA mode the TM could not execute Task due to dependencies fetching failure if the Service exposes a blob server port which is different from the JM Container when one configures the blob server port with a value different from 6124.

 ",,felixzheng,tison,wangyang0918,,,,,,,,,,,,,,,,,,,"zhengcanbin commented on pull request #11349: [FLINK-16025][k8s] Parse BlobServer port from Configuration instead f using constant
URL: https://github.com/apache/flink/pull/11349
 
 
   ## What is the purpose of the change
   
   Fix bug reported by [FLINK-16025](https://issues.apache.org/jira/browse/FLINK-16025?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel)
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/20 05:18;githubbot;600","TisonKun commented on pull request #11349: [FLINK-16025][k8s] Parse BlobServer port from Configuration instead f using constant
URL: https://github.com/apache/flink/pull/11349
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Mar/20 10:32;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-16194,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 10 10:33:36 UTC 2020,,,,,,,,,,"0|z0bfq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/20 07:43;wangyang0918;Nice catch. I think it is a bug. We should use {{KubernetesUtils.parsePort(flinkConfig, BlobServerOptions.PORT)}} instead of {{Constants.BLOB_SERVER_PORT}} in ServiceDecorator.;;;","20/Feb/20 12:45;felixzheng;You are right [~fly_in_gis];;;","09/Mar/20 03:15;wangyang0918;I am afraid we should remove the {{1.10.1}} from the fixed version. Since FLINK-16194 is not merged to branch release-1.10.;;;","09/Mar/20 03:21;felixzheng;Thanks for the reminder [~fly_in_gis]. 1.10.1 is not included in the fixed version list, but I think it's better to reopen this ticket to fix this bug for 1.10.1. WDYT?;;;","09/Mar/20 03:26;wangyang0918;I agree to backport the fix to 1.10.;;;","09/Mar/20 04:49;felixzheng;Backport the fix to 1.10.1;;;","10/Mar/20 10:33;tison;1.10 via 66ea7496e871f820ce3336a444e0954ba3b96f12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DescriptorProperties.putTableSchema does not include constraints,FLINK-16021,13284824,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,danny0405,twalthr,twalthr,12/Feb/20 14:22,28/May/20 02:34,13/Jul/23 08:07,28/May/20 02:34,1.10.0,,,,,1.11.0,,,,Table SQL / API,Table SQL / Ecosystem,,,,0,pull-request-available,,,,FLINK-14978 added primary keys as the first constraints but forgot about adding them to the property map as well.,,aljoscha,danny0405,dwysakowicz,jark,pnowojski,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 02:34:08 UTC 2020,,,,,,,,,,"0|z0bex4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/20 11:47;twalthr;[~jark] do you think this should have ""Critical"" priority? The release manager is pinging me. I think ""Major"" should be enough, no?;;;","16/Mar/20 13:45;jark;I think ""major"" is also fine, because we don't provide primary key syntax yet, the only usage of primary key information is from Hive which doesn't rely on the property. ;;;","11/May/20 07:27;pnowojski;[~jark] have you intentionally bumped the priority to {{Critical}} again? As it's now, this ticket is again blocking 1.11 release.;;;","11/May/20 07:54;jark;Hi [~pnowojski], yes, I think we should bump the priority, because we officially support the PRIMARY KEY syntax in DDL since 1.11.;;;","11/May/20 08:22;danny0405;I would like to take this issue ~;;;","11/May/20 08:27;jark;Thanks [~danny0405], assigned to you.;;;","28/May/20 02:34;jark;master (1.12.0): 3a322e0c4babf2f31223fb75ef61ec23f229011b
1.11.0: a325a23786b8d4b921023071c912c12e9dff9cf2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ContinuousFileReaderOperatorBenchmark restarts indefinetly on failure,FLINK-16019,13284800,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,12/Feb/20 12:37,21/Feb/20 07:26,13/Jul/23 08:07,21/Feb/20 07:25,,,,,,,,,,Benchmarks,,,,,0,pull-request-available,,,,"CFRO was changed to be created using factory, but the benchmark wasn’t updated.

This resulted in error in CFRO super.dispose as it wasn’t properly initialized.

The error wasn’t reported properly in CFRO (flipped arguments to firstOrSuppressed)

The benchmark was running a job with a restarting strategy, so it repeated indefinitely.",,pnowojski,roman,,,,,,,,,,,,,,,,,,,,"rkhachatryan commented on pull request #11071: [FLINK-16019][runtime] fix ContinuousFileReaderOperator error handling
URL: https://github.com/apache/flink/pull/11071
 
 
   ## What is the purpose of the change
   
   *Fix error reporting in ContinuousFileReaderOperator*
   
   ## Verifying this change
   
   This change is a trivial rework without any test coverage; adding a test is also problematic, because the only difference between old and new behavior is the error details.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Feb/20 14:46;githubbot;600","pnowojski commented on pull request #11071: [FLINK-16019][runtime] fix ContinuousFileReaderOperator error reporting
URL: https://github.com/apache/flink/pull/11071
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Feb/20 07:26;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 21 07:25:29 UTC 2020,,,,,,,,,,"0|z0bers:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/20 07:25;pnowojski;Merged to master as 0f71ab3edb and dddbc2b63c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The out-of-date warning doesn't show up in release-1.6 document,FLINK-16016,13284775,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,liyu,liyu,12/Feb/20 10:16,21/Mar/20 12:35,13/Jul/23 08:07,20/Mar/20 21:09,,,,,,,,,,Documentation,,,,,0,,,,,"We already have {{show_outdated_warning: true}} set in {{docs/_config.yml}} in release-1.6, and manually triggered a [new build|https://ci.apache.org/builders/flink-docs-release-1.6/builds/392] of the doc through build bot. However, the out-of-date warning still doesn't show, while the same process takes effect for {{release-1.8}}, so there must be something wrong for release-1.6.",,liyu,rmetzger,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 21 12:35:12 UTC 2020,,,,,,,,,,"0|z0bem8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/20 21:09;rmetzger;I recently fixed the problem: https://ci.apache.org/projects/flink/flink-docs-release-1.6/;;;","21/Mar/20 12:35;liyu;Great to know! Thanks for the efforts [~rmetzger]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S3 plugin ClassNotFoundException SAXParser,FLINK-16014,13284766,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,arvid,arvid heise,12/Feb/20 09:50,08/Sep/21 08:18,13/Jul/23 08:07,11/Mar/20 08:31,1.10.0,1.11.0,,,,1.10.1,1.11.0,,,FileSystems,,,,,0,pull-request-available,,,,"While stress-testing s3 plugin on EMR.

 
{noformat}
org.apache.flink.util.FlinkRuntimeException: Could not perform checkpoint 2 for operator Map (114/160).
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:839)
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:104)
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.notifyBarrierReceived(CheckpointBarrierUnaligner.java:149)
	at org.apache.flink.streaming.runtime.io.InputProcessorUtil$1.lambda$notifyBarrierReceived$0(InputProcessorUtil.java:80)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:255)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:508)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:492)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3://emr-unaligned-checkpoints-testing-eu-central-1/inflight/9ae223e41008b17568d7f63c12360268_output/part-file.-1: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader: Couldn't initialize a SAX driver to create an XMLReader
	at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.checkErroneousUnsafe(BufferPersisterImpl.java:262)
	at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.add(BufferPersisterImpl.java:137)
	at org.apache.flink.runtime.io.network.BufferPersisterImpl.addBuffers(BufferPersisterImpl.java:66)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.prepareInflightDataSnapshot(StreamTask.java:935)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:898)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:870)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:826)
	... 12 more
Caused by: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3://emr-unaligned-checkpoints-testing-eu-central-1/inflight/9ae223e41008b17568d7f63c12360268_output/part-file.-1: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader: Couldn't initialize a SAX driver to create an XMLReader
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:177)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:145)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2251)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2149)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2088)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1734)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:2970)
	at org.apache.flink.fs.s3hadoop.common.HadoopFileSystem.exists(HadoopFileSystem.java:152)
	at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.exists(PluginFileSystemFactory.java:143)
	at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.exists(SafetyNetWrapperFileSystem.java:102)
	at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.get(BufferPersisterImpl.java:213)
	at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.run(BufferPersisterImpl.java:167)
Caused by: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader
	at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.<init>(XmlResponsesSaxParser.java:118)
	at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:87)
	at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:77)
	at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62)
	at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31)
	at com.amazonaws.http.response.AwsResponseHandlerAdapter.handle(AwsResponseHandlerAdapter.java:70)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleResponse(AmazonHttpClient.java:1554)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1272)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4325)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4272)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4266)
	at com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:876)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$5(S3AFileSystem.java:1262)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:280)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:1255)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2223)
	... 9 more
Caused by: org.xml.sax.SAXException: SAX2 driver class org.apache.xerces.parsers.SAXParser not found
java.lang.ClassNotFoundException: org.apache.xerces.parsers.SAXParser
	at org.xml.sax.helpers.XMLReaderFactory.loadClass(XMLReaderFactory.java:230)
	at org.xml.sax.helpers.XMLReaderFactory.createXMLReader(XMLReaderFactory.java:191)
	at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.<init>(XmlResponsesSaxParser.java:115)
	... 32 more
Caused by: java.lang.ClassNotFoundException: org.apache.xerces.parsers.SAXParser
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at org.apache.flink.core.plugin.PluginLoader$PluginClassLoader.loadClass(PluginLoader.java:149)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.xml.sax.helpers.NewInstance.newInstance(NewInstance.java:82)
	at org.xml.sax.helpers.XMLReaderFactory.loadClass(XMLReaderFactory.java:228)
	... 34 more{noformat}",,arvid heise,pnowojski,,,,,,,,,,,,,,,,,,,,"AHeise commented on pull request #11123: [FLINK-16014] Adding xerces to s3 plugins to avoid ClassNotFound issues.
URL: https://github.com/apache/flink/pull/11123
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Adding xerces to s3 plugins to avoid ClassNotFound issues.
   Root cause is a [bug](https://bugs.openjdk.java.net/browse/JDK-8015099) in JDK8. `XMLReaderFactory` caches the class name independent of the classloader. On EMR, xercesImpl is on classpath (because of HDFS) and will be loaded at some point in time.
   Also overwriting the faulty `XMLReaderFactory` in the plugins with a non-caching implementation that only loads the bundled xerces.
   
   ## Brief change log
   
   - Adding xerces to s3 plugins
   - Overwriting the faulty `XMLReaderFactory` in the plugins with a non-caching implementation that only loads the bundled xerces.
   
   
   ## Verifying this change
   
   Manually tested on EMR.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**yes** / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (**yes** / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Feb/20 13:18;githubbot;600","zentol commented on pull request #11123: [FLINK-16014][s3] Force usage of SAXParserFactory over XMLReaderFactory in aws-java-sdk-s3.
URL: https://github.com/apache/flink/pull/11123
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/20 08:31;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,FLINK-16550,FLINK-24183,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 11 08:31:38 UTC 2020,,,,,,,,,,"0|z0bek8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/20 16:31;arvid;Root cause is a [bug in JDK8|https://bugs.openjdk.java.net/browse/JDK-8015099]. `XMLReaderFactory` [caches|https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/jdk8u222-b10/jaxp/src/org/xml/sax/helpers/XMLReaderFactory.java#L146-L174] the class name independent of the classloader. On EMR, xercesImpl is on classpath (because of HDFS) and will be loaded at some point in time.

We have some workarounds, none of which would solve it in all cases. Since the user may bundle any `SAXParser` in his code or put it in lib, the plugin might always save a `XMLReader` that is not visible from the plugin.

Workarounds:

1) Add xercesImpl to s3 plugin. Would blow up file size but cover the most common case of a `SAXParser`.

2) Add another smaller `SAXParser` to avoid having none.

For all options, we should eagerly initialize the `XMLReaderFactory` in S3AFileSystem to win against user code and hopefully even `lib`.;;;","11/Mar/20 08:31;chesnay;master: ffc8e42d6c891c0959ee3b4c0347903eed369e8e
1.10: f1bd35c3fb3fecb9026d679d360c632d639f8499;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
List and map config options could not be parsed correctly,FLINK-16013,13284734,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,wangyang0918,wangyang0918,12/Feb/20 06:30,25/Feb/20 07:59,13/Jul/23 08:07,25/Feb/20 07:59,1.10.0,,,,,1.10.1,1.11.0,,,Runtime / Configuration,,,,,0,pull-request-available,,,,"Currently, if a config option is {{List}} type and written to a flink-conf.yaml, it could not be parsed correctly when reloaded from yaml resource. The root cause is we use {{List#toString}} to save into the yaml resource. However, when we want to parse a List from a string, we use semicolon to split the value.

 

Also the Map, Duration type have the same problem.

 

The following is a unit test to reproduce this problem.
{code:java}
public void testWriteConfigurationAndReload() throws IOException {
 final File flinkConfDir = temporaryFolder.newFolder().getAbsoluteFile();
 final Configuration flinkConfig = new Configuration();
 final ConfigOption<List<String>> listConfigOption = ConfigOptions
  .key(""test-list-string-key"")
  .stringType()
  .asList()
  .noDefaultValue();
 final List<String> values = Arrays.asList(""value1"", ""value2"", ""value3"");
 flinkConfig.set(listConfigOption, values);
 assertThat(values, Matchers.containsInAnyOrder(flinkConfig.get(listConfigOption).toArray()));

 BootstrapTools.writeConfiguration(flinkConfig, new File(flinkConfDir, ""flink-conf.yaml""));
 final Configuration loadedFlinkConfig = GlobalConfiguration.loadConfiguration(flinkConfDir.getAbsolutePath());
 assertThat(values, Matchers.containsInAnyOrder(loadedFlinkConfig.get(listConfigOption).toArray()));
}
{code}",,dwysakowicz,kkl0u,wangyang0918,wind_ljy,,,,,,,,,,,,,,,,,,"wangyang0918 commented on pull request #11070: [FLINK-16013][core] Write and parse list config option correctly
URL: https://github.com/apache/flink/pull/11070
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Currently, if a config option is List type and written to a flink-conf.yaml, it could not be parsed correctly when reloaded from yaml resource. The root cause is we use List#toString to save into the yaml resource. However, when we want to parse a List from a string, we use semicolon to split the value.
   
   When the configuration value is List type, we need to convert it to a semicolon-separated string in `Configuration#toMap`. And If we want to write the configuration to a yaml file, `Configuration#toMap` should be used to get all the keys and values.
   
   
   ## Brief change log
   
   * Convert list to semicolon-separated  string in `Configuration#toMap`
   
   
   ## Verifying this change
   
   * The changes is covered by two new added tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Feb/20 12:48;githubbot;600","dawidwys commented on pull request #11070: [FLINK-16013][core] Make complex type config options could be parsed correctly
URL: https://github.com/apache/flink/pull/11070
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/20 07:56;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 25 07:59:49 UTC 2020,,,,,,,,,,"0|z0bed4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/20 08:50;wangyang0918;cc [~kkloudas] 

I think the {{PipelineOptions#JARS, PipelineOptions#CLASSPATHS, etc.}} could not be parsed correctly.;;;","12/Feb/20 09:37;kkl0u;[~fly_in_gis] thanks for reporting this. I think that your concern is correct. Should I assign the issue to you?;;;","12/Feb/20 12:54;wangyang0918;[~kkl0u] Thanks for your confirmation. I have some bandwidth to work on this and please assign this ticket to me.;;;","12/Feb/20 13:45;dwysakowicz;Hi [~fly_in_gis],
good catch. I think we could fix it by changing {{org.apache.flink.configuration.Configuration#convertToString}} method so that it formats {{Lists}} according to the expected format for parsing.;;;","13/Feb/20 04:17;wangyang0918;[~dwysakowicz] Thanks for your suggestion. It really makes sense to put the converting logic in {{convertToString}}.;;;","25/Feb/20 07:59;dwysakowicz;Fixed in:
master: 4c43be9a4c61cb89c660660546f3be23132aa13c
1.10.1: 543e24c3861336fffebe897bf2d3f75c69cab9f3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ShardConsumer errors cannot be logged,FLINK-16003,13284609,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,iemre,oripwk,oripwk,11/Feb/20 15:36,20/Mar/23 09:08,13/Jul/23 08:07,20/Mar/23 09:08,1.6.3,aws-connector-4.1.0,,,,aws-connector-4.2.0,,,,Connectors / Kinesis,,,,,0,auto-deprioritized-major,auto-deprioritized-minor,pull-request-available,,"Some of the errors in the Kinesis connector show on Flink UI, but are not logged. This causes a serious problem since we cannot see them in our logging aggregation platform, and we cannot create alerts on them. One of the errors is the following:

 
{code:java}
java.lang.RuntimeException: Rate Exceeded for getRecords operation - all 3 retry attempts returned ProvisionedThroughputExceededException.
 at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.getRecords(KinesisProxy.java:234)
 at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.getRecords(ShardConsumer.java:311)
 at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:219)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748){code}
 

 

The code shows that this exception is not caught and hence is not logged and can be detected only from the ""exceptions"" tab in Flink UI. People who use the connector cannot leverage logging and metrics when such an exception occurs.

It can be useful to catch all the throwables from the ShardConsumer's run() method and log them.

 

 ",,dannycranmer,oripwk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 20 09:08:38 UTC 2023,,,,,,,,,,"0|z0bdlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 11:29;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:17;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","09/Nov/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Nov/21 22:38;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","20/Mar/23 09:08;dannycranmer;Merged commit [{{72a78e6}}|https://github.com/apache/flink-connector-aws/commit/72a78e64c30692290179904b314a6322f749008f] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SELECT 1.0e0 / 0.0e0 throws NumberFormatException,FLINK-15987,13284510,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xuyangzhong,TsReaper,TsReaper,11/Feb/20 07:36,15/Dec/21 01:44,13/Jul/23 08:07,29/Oct/21 02:36,1.10.0,,,,,1.13.6,1.14.3,1.15.0,,Table SQL / Planner,,,,,0,auto-deprioritized-major,pull-request-available,,,"{code:sql}
SELECT 1.0e / 0.0e
{code}

throws the following exception

{code:java}
Caused by: java.lang.NumberFormatException: Infinite or NaN
	at java.math.BigDecimal.<init>(BigDecimal.java:895)
	at java.math.BigDecimal.<init>(BigDecimal.java:872)
	at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:189)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:695)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:616)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:301)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:560)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:419)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:256)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:215)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:202)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:83)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:56)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:44)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:44)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:44)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:248)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:151)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:682)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.insertIntoInternal(TableEnvironmentImpl.java:355)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.insertInto(TableEnvironmentImpl.java:343)
	at org.apache.flink.table.api.internal.TableImpl.insertInto(TableImpl.java:428)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeQueryInternal$11(LocalExecutor.java:610)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:240)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQueryInternal(LocalExecutor.java:608)
	... 47 more
{code}
",,godfreyhe,libenchao,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 29 02:36:25 UTC 2021,,,,,,,,,,"0|z0bczc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 11:29;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:17;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","29/Oct/21 02:36;godfreyhe;Fixed in 1.15.0: 8c603d8d07984ec136fe5b1778ccb39b039c5d75
Fixed in 1.14.1: 48fe11dcb81b2aac5c10f69ac4c33e8e67603d9b
Fixed in 1.13.4: c72db290b7c8e0266f5df43998a39fb4fe1ac62a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Quickstarts Java nightly end-to-end test' is failed on travis,FLINK-15982,13284480,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,jark,jark,11/Feb/20 03:21,17/Feb/20 15:29,13/Jul/23 08:07,17/Feb/20 15:29,1.11.0,,,,,1.11.0,,,,Tests,,,,,0,pull-request-available,,,,"{code:java}
==============================================================================
Running 'Quickstarts Java nightly end-to-end test'
==============================================================================
TEST_DATA_DIR: /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491
Flink dist directory: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
22:16:44.021 [INFO] Scanning for projects...
22:16:44.095 [INFO] ------------------------------------------------------------------------
22:16:44.095 [INFO] BUILD FAILURE
22:16:44.095 [INFO] ------------------------------------------------------------------------
22:16:44.098 [INFO] Total time: 0.095 s
22:16:44.099 [INFO] Finished at: 2020-02-10T22:16:44+00:00
22:16:44.143 [INFO] Final Memory: 5M/153M
22:16:44.143 [INFO] ------------------------------------------------------------------------
22:16:44.144 [ERROR] The goal you specified requires a project to execute but there is no POM in this directory (/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491). Please verify you invoked Maven from the correct directory. -> [Help 1]
22:16:44.144 [ERROR] 
22:16:44.145 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
22:16:44.145 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
22:16:44.145 [ERROR] 
22:16:44.145 [ERROR] For more information about the errors and possible solutions, please read the following articles:
22:16:44.145 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException
/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test_quickstarts.sh: line 57: cd: flink-quickstart-java: No such file or directory
cp: cannot create regular file '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491/flink-quickstart-java/src/main/java/org/apache/flink/quickstart/Elasticsearch5SinkExample.java': No such file or directory
sed: can't read /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491/flink-quickstart-java/src/main/java/org/apache/flink/quickstart/Elasticsearch5SinkExample.java: No such file or directory
awk: fatal: cannot open file `pom.xml' for reading (No such file or directory)
sed: can't read pom.xml: No such file or directory
sed: can't read pom.xml: No such file or directory
22:16:45.312 [INFO] Scanning for projects...
22:16:45.386 [INFO] ------------------------------------------------------------------------
22:16:45.386 [INFO] BUILD FAILURE
22:16:45.386 [INFO] ------------------------------------------------------------------------
22:16:45.391 [INFO] Total time: 0.097 s
22:16:45.391 [INFO] Finished at: 2020-02-10T22:16:45+00:00
22:16:45.438 [INFO] Final Memory: 5M/153M
22:16:45.438 [INFO] ------------------------------------------------------------------------
22:16:45.440 [ERROR] The goal you specified requires a project to execute but there is no POM in this directory (/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491). Please verify you invoked Maven from the correct directory. -> [Help 1]
22:16:45.440 [ERROR] 
22:16:45.440 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
22:16:45.440 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
22:16:45.440 [ERROR] 
22:16:45.440 [ERROR] For more information about the errors and possible solutions, please read the following articles:
22:16:45.440 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException
/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test_quickstarts.sh: line 73: cd: target: No such file or directory
java.io.FileNotFoundException: flink-quickstart-java-0.1.jar (No such file or directory)
	at java.util.zip.ZipFile.open(Native Method)
	at java.util.zip.ZipFile.<init>(ZipFile.java:225)
	at java.util.zip.ZipFile.<init>(ZipFile.java:155)
	at java.util.zip.ZipFile.<init>(ZipFile.java:126)
	at sun.tools.jar.Main.list(Main.java:1115)
	at sun.tools.jar.Main.run(Main.java:293)
	at sun.tools.jar.Main.main(Main.java:1288)
Success: There are no flink core classes are contained in the jar.
Failure: Since Elasticsearch5SinkExample.class and other user classes are not included in the jar. 
[FAIL] Test script contains errors.
{code}

Here are some instances:
- https://api.travis-ci.org/v3/job/648404584/log.txt
- https://api.travis-ci.org/v3/job/648404591/log.txt
- https://api.travis-ci.org/v3/job/648404598/log.txt
",,jark,leonard,rmetzger,,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #11067: [FLINK-15982][e2e] Pick the right Flink version to fix the java quickstart test
URL: https://github.com/apache/flink/pull/11067
 
 
   I know that calling `head -n 1` is not the most robust way.
   [Multiline matching with sed](https://unix.stackexchange.com/questions/26284/how-can-i-use-sed-to-replace-a-multi-line-string) is fairly complicated. Using maven:
   ```
   mvn --file ../pom.xml org.apache.maven.plugins:maven-help-plugin:3.1.0:evaluate -Dexpression=project.version -q -DforceStdout
   ```
   takes roughly 4 seconds. The common.sh script is sourced in many scripts, this would slow down the test execution.
   
   
   ## Verifying this change
   
   Run the ""Quickstarts Java nightly end-to-end test""
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Feb/20 08:30;githubbot;600","rmetzger commented on pull request #11067: [FLINK-15982][e2e] Pick the right Flink version to fix the java quickstart test
URL: https://github.com/apache/flink/pull/11067
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Feb/20 09:10;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 17 15:29:07 UTC 2020,,,,,,,,,,"0|z0bcso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/20 07:35;rmetzger;I ran into the issue as well. I believe the problem is this change: https://github.com/apache/flink/commit/2b28169646c75327b05108198ea27af009d40a29#diff-aa05a351945a1297c1a64f9809632a3a combined with this line in the {{common.sh}} script:

{code:java}
FLINK_VERSION=$(cat ${END_TO_END_DIR}/pom.xml | sed -n 's/.*<version>\(.*\)<\/version>/\1/p')
{code}

In the quickstart test, {{FLINK_VERSION}} is set to:
{code:java}
2020-02-11T23:04:18.3614598Z ++ FLINK_VERSION='1.11-SNAPSHOT
2020-02-11T23:04:18.3615067Z 1.8.1
2020-02-11T23:04:18.3615489Z 2.5.9'
{code}

So above {{sed}} is extracting all versions in the end to end pom. I'll assign myself to this ticket & will provide a fix.
;;;","13/Feb/20 09:11;rmetzger;Fixed in master (1.11) in commit f4f6f8d4b773ce2580c0cace9b3742582aa0b1f3;;;","15/Feb/20 22:00;rmetzger;I need to reopen this ticket. The test is still failing in the latest master: https://travis-ci.org/apache/flink/jobs/650817656

I forgot to export the FLINK_VERSION.
This will be fixed in this PR:  https://github.com/apache/flink/pull/10976/;;;","17/Feb/20 15:29;rmetzger;Should be fixed now in https://github.com/apache/flink/commit/84fd23d82c2908192d58186d6e061c89b018cda5#diff-6f9df16a3c581688cde52134ef1e83edR22;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveGenericUDFTest#testMap relies on element order,FLINK-15975,13284378,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,testfixer,testfixer,testfixer,10/Feb/20 17:21,28/Feb/20 15:57,13/Jul/23 08:07,28/Feb/20 15:54,,,,,,1.11.0,,,,Connectors / Hive,Tests,,,,0,pull-request-available,,,,"The test `testMap` in `HiveGenericUDFTest` may fail due if HashMap iterates in a different order. `testMap` depends on `getConversion` and `toFlinkObject` in class `MapObjectInspector`. When the `inspector` is the instance of `MapObjectInspector`, it can return a `HashMap`. However, `HashMap` does not guarantee any specific order of entries. Thus, the test can fail due to a different iteration order.",,testfixer,,,,,,,,,,,,,,,,,,,,,"testfixer commented on pull request #11052: FLINK-15975 Use LinkedHashMap for deterministic iterations
URL: https://github.com/apache/flink/pull/11052
 
 
   ## What is the purpose of the change
   
   The test `testMap` in `HiveGenericUDFTest` invokes two methods `HiveGenericUDFTest.init` and `HiveScalarFunction.eval`.  These two methods depend on `getConversion` and `toFlinkObject` in class `HiveInspectors`. When the `inspector` is the instance of `MapObjectInspector`, it can return a `HashMap`. However, `HashMap` does not guarantee any specific order of entries. Thus, the test can fail due to a different iteration order.
   
   In this PR, we propose to use a `LinkedHashMap` instead to guarantee the iteration order.
   
   
   ## Brief change log
   
   In `flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/functions/hive/conversion/HiveInspectors.java`, I use LinkedHashMap instead of HashMap in line 108, line 217, and line 316.
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (don't know)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Feb/20 17:34;githubbot;600","zentol commented on pull request #11052: [FLINK-15975][flink-connectors] Making test assertion more stable
URL: https://github.com/apache/flink/pull/11052
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Feb/20 15:54;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/20 17:25;testfixer;FLINK-15975-000.patch;https://issues.apache.org/jira/secure/attachment/12993071/FLINK-15975-000.patch","12/Feb/20 21:11;testfixer;FLINK-15975-001.patch;https://issues.apache.org/jira/secure/attachment/12993299/FLINK-15975-001.patch",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 15:54:46 UTC 2020,,,,,,,,,,"0|z0bc60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/20 15:54;chesnay;master: 550ca7214c3e352dbda1bd38c5e1836ddb7607e2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job Status is hard to read for some Statuses,FLINK-15953,13283929,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,vthinkxie,gjy,gjy,07/Feb/20 12:57,19/Mar/20 09:19,13/Jul/23 08:07,19/Mar/20 09:19,1.10.0,1.9.2,,,,1.10.1,1.11.0,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,The job status {{RESTARTING}} is rendered in a white font on white background which makes it hard to read (see attachments).,,gjy,trohrmann,,,,,,,,,,,,,,,,,,,,"vthinkxie commented on pull request #11433: [FLINK-15953][web] fix restarting status in web
URL: https://github.com/apache/flink/pull/11433
 
 
   ## What is the purpose of the change
   
   fix https://issues.apache.org/jira/browse/FLINK-15787
   
   ## Brief change log
   
   support more metric display at once
   
   ## Verifying this change
   
     - *Submit a restarting job*
     - *Go to the dashboard*
     - *check if the restarting status correct*
   
   
   before:
   ![FireShot Capture 579 - Apache Flink Web Dashboard - localhost](https://user-images.githubusercontent.com/1506722/76935931-fe0d8d80-692c-11ea-9176-8c3d5b6d67cc.png)
   
   
   
   after:
   ![FireShot Capture 578 - Apache Flink Web Dashboard - localhost](https://user-images.githubusercontent.com/1506722/76936066-36ad6700-692d-11ea-8981-f3295c7cc765.png)
   
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/20 07:29;githubbot;600","GJL commented on pull request #11433: [FLINK-15953][web] fix restarting status in web
URL: https://github.com/apache/flink/pull/11433
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 09:13;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/20 12:57;gjy;769B08ED-D644-4DEB-BA4C-14B18E562A52.png;https://issues.apache.org/jira/secure/attachment/12992891/769B08ED-D644-4DEB-BA4C-14B18E562A52.png","07/Feb/20 12:58;gjy;D68BA5A5-21FB-46A4-A5E9-382AFE16B57A.png;https://issues.apache.org/jira/secure/attachment/12992892/D68BA5A5-21FB-46A4-A5E9-382AFE16B57A.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 09:19:28 UTC 2020,,,,,,,,,,"0|z0b9e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/20 09:19;gjy;1.10: 742d420dbe41c387515f8f87cee79e0073f78da3
master: 64abbdd6f274333fbdc3d6a51b2876c3110986d6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't use raw byte[] in MultiplexedState ,FLINK-15952,13283928,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igal,igal,igal,07/Feb/20 12:53,08/Feb/20 03:08,13/Jul/23 08:07,08/Feb/20 03:08,,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"Currently multiplexed state is backed by Flink's MapState, and uses raw byte[] for keys,
this breaks with the FsStateBackend.
",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,"igalshilman commented on pull request #15: [FLINK-15952] Replace the usage of a raw byte[] for keys in MultiplexedState
URL: https://github.com/apache/flink-statefun/pull/15
 
 
   This PR address the problem, that when using Flink's heap state backend keys to a `MapState` can not be of a primitive array type.
   This PR introduces a new Protobuf message to be used as a key in a multiplexed state context.
   ```
   message MultiplexedStateKey {
       string function_namespace = 1;
       string function_type = 2;
       string state_name = 3;
       repeated bytes user_keys = 4;
   }
   ```
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Feb/20 13:42;githubbot;600","tzulitai commented on pull request #15: [FLINK-15952] Replace the usage of a raw byte[] for keys in MultiplexedState
URL: https://github.com/apache/flink-statefun/pull/15
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Feb/20 03:06;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 08 03:08:18 UTC 2020,,,,,,,,,,"0|z0b9ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/20 03:08;tzulitai;Merged to master via {{0b60796f2e5549ce8ac332f5732c510496bc349b}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rowtime  field name cannot be the same as the json field,FLINK-15943,13283738,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gkgkgk,gkgkgk,06/Feb/20 16:20,29/Apr/21 16:42,13/Jul/23 08:07,29/Apr/21 16:42,1.9.0,,,,,,,,,Table SQL / API,Table SQL / Planner,,,,0,stale-major,,,,"Run the following sql:

-- sql start 
--source
CREATE TABLE dwd_user_log (
  id VARCHAR,
  ctime TIMESTAMP,
  sessionId VARCHAR,
  pageId VARCHAR,
  eventId VARCHAR,
  deviceId Decimal
) WITH (
  'connector.type' = 'kafka',
  'connector.version' = 'universal',
  'connector.topic' = 'dev_dwd_user_log_02',
  'connector.startup-mode' = 'latest-offset',
  'connector.properties.0.key' = 'zookeeper.connect',
  'connector.properties.0.value' = 'node01:2181',
  'connector.properties.1.key' = 'bootstrap.servers',
  'connector.properties.1.value' = 'node01:9092',
  'connector.properties.2.key' = 'group.id',
  'connector.properties.2.value' = 'dev-group',
  'update-mode' = 'append',
  'format.type' = 'json',
  -- 'format.derive-schema' = 'true',
  'format.json-schema' = '{
    ""type"": ""object"",
    ""properties"": {
      ""id"": {
      ""type"": ""string""
      },
      ""ctime"": {
      ""type"": ""string"",
      ""format"": ""date-time""
      },
      ""pageId"": {
      ""type"": ""string""
      },
      ""eventId"": {
      ""type"": ""string""
      },
      ""sessionId"": {
      ""type"": ""string""
      },
      ""deviceId"": {
      ""type"": ""number""
      }
    }
  }',
  'schema.1.rowtime.timestamps.type' = 'from-field',
  'schema.1.rowtime.timestamps.from' = 'ctime',
  'schema.1.rowtime.watermarks.type' = 'periodic-bounded',
  'schema.1.rowtime.watermarks.delay' = '10000'
);  


-- sink
-- sink for pv
CREATE TABLE dws_pv (
    windowStart TIMESTAMP,
  windowEnd TIMESTAMP,
  pageId VARCHAR,
  viewCount BIGINT
) WITH (
  'connector.type' = 'kafka',
  'connector.version' = 'universal',
  'connector.topic' = 'dev_dws_pvuv_02',
  'connector.startup-mode' = 'latest-offset',
  'connector.properties.0.key' = 'zookeeper.connect',
  'connector.properties.0.value' = 'node01:2181',
  'connector.properties.1.key' = 'bootstrap.servers',
  'connector.properties.1.value' = 'node01:9092',
  'connector.properties.2.key' = 'group.id',
  'connector.properties.2.value' = 'dev-group',
  'update-mode' = 'append',
  'format.type' = 'json',
  'format.derive-schema' = 'true'
);

-- pv
INSERT INTO dws_pv
SELECT
  TUMBLE_START(ctime, INTERVAL '20' SECOND)  AS windowStart,
  TUMBLE_END(ctime, INTERVAL '20' SECOND)  AS windowEnd,
  pageId,
  COUNT(deviceId) AS viewCount
FROM dwd_user_log
GROUP BY TUMBLE(ctime, INTERVAL '20' SECOND),pageId;
-- sql end
And hit the following error:
{code:java}
//Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Field 'ctime' could not be resolved by the field mapping.Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Field 'ctime' could not be resolved by the field mapping. at org.apache.flink.table.planner.sources.TableSourceUtil$.org$apache$flink$table$planner$sources$TableSourceUtil$$resolveInputField(TableSourceUtil.scala:357) at org.apache.flink.table.planner.sources.TableSourceUtil$$anonfun$org$apache$flink$table$planner$sources$TableSourceUtil$$resolveInputFields$1.apply(TableSourceUtil.scala:388) at org.apache.flink.table.planner.sources.TableSourceUtil$$anonfun$org$apache$flink$table$planner$sources$TableSourceUtil$$resolveInputFields$1.apply(TableSourceUtil.scala:388) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186) at org.apache.flink.table.planner.sources.TableSourceUtil$.org$apache$flink$table$planner$sources$TableSourceUtil$$resolveInputFields(TableSourceUtil.scala:388) at org.apache.flink.table.planner.sources.TableSourceUtil$$anonfun$getRowtimeExtractionExpression$1.apply(TableSourceUtil.scala:275) at org.apache.flink.table.planner.sources.TableSourceUtil$$anonfun$getRowtimeExtractionExpression$1.apply(TableSourceUtil.scala:270) at scala.Option.map(Option.scala:146) at org.apache.flink.table.planner.sources.TableSourceUtil$.getRowtimeExtractionExpression(TableSourceUtil.scala:270) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.translateToPlanInternal(StreamExecTableSourceScan.scala:117) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.translateToPlanInternal(StreamExecTableSourceScan.scala:55) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:54
{code}",,gkgkgk,jark,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 16:42:23 UTC 2021,,,,,,,,,,"0|z0b87k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/20 16:48;gkgkgk; 

 

 

tracking the source code:
 # at org.apache.flink.table.descriptors.SchemaValidator. deriveFieldMapping(SchemaValidator.java:275),  proctime/rowtime field  be removed from field mapping
 # at org.apache.flink.table.planner.sources.TableSourceUtil.resolveInputField(TableSourceUtil.scala:357),  resolve  field by field mapping

Is there a conflict between the above two？

 

        

 

 ;;;","23/Mar/20 15:33;gkgkgk;This might be related to [|https://issues.apache.org/jira/browse/FLINK-15801];;;","22/Apr/21 11:29;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 16:42;jark;The legacy connector and format may has some problems. Please try the new connector and format which can be used via {{'connector' = 'kafka', 'format' = 'json'}}, see docs: https://ci.apache.org/projects/flink/flink-docs-master/docs/connectors/table/kafka/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the Development Status for PyFlink,FLINK-15937,13283642,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sunjincheng121,sunjincheng121,sunjincheng121,06/Feb/20 07:48,07/Feb/20 20:13,13/Jul/23 08:07,06/Feb/20 08:21,1.10.0,,,,,1.10.0,,,,API / Python,,,,,0,pull-request-available,,,,"Correct the  `Development Status` value. 

From `'Development Status :: 1 - Planning'` to `'Development Status :: 5 - Production/Stable'`.

 The `Planning status` in PyPI means tell user that the package cannot be using in production [1]. So, correct the Development Status is very important for user. 

I would like to contains this fix in 1.10.0 release. 

 

[[1] https://pypi.org/search/?c=Development+Status+%3A%3A+1+-+Planning|https://pypi.org/search/?c=Development+Status+%3A%3A+1+-+Planning]

!image-2020-02-06-19-17-28-672.png|width=268,height=244!",,gjy,sunjincheng121,,,,,,,,,,,,,,,,,,,,"sunjincheng121 commented on pull request #11028: [FLINK-15937][python] Update the Development Status to 5 - Production/…
URL: https://github.com/apache/flink/pull/11028
 
 
   
   ## What is the purpose of the change
   
   Update the Development Status to `5 - Production/Stable`.
   
   
   ## Brief change log
     - *Update the Development Status to `5 - Production/Stable` in setup.py*
   
   ## Verifying this change
   This change do not need any test.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Feb/20 08:07;githubbot;600","sunjincheng121 commented on pull request #11028: [FLINK-15937][python] Update the Development Status to 5 - Production/…
URL: https://github.com/apache/flink/pull/11028
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Feb/20 08:12;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/20 11:17;sunjincheng121;image-2020-02-06-19-17-28-672.png;https://issues.apache.org/jira/secure/attachment/12992766/image-2020-02-06-19-17-28-672.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 07 03:48:50 UTC 2020,,,,,,,,,,"0|z0b7m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/20 08:21;sunjincheng121;Fixed in master: 0acd3493868d077884eaaa5142a5c22d56f6856b
Fixed in release-1.10: 62cfc8083b578d552d7063cbdbc60c68d6972669

Would cp to release-1.9 when necessary. ;;;","06/Feb/20 11:12;sunjincheng121;Hi [~gary] , Thanks for pay attention on this issue. I found that you had reset the fix  version from 1.10 to 1.10.1. Sorry, I am not add more info in the Jira when I open it. From the points of my view `Planning status` in PyPI means that the package cannot be using in production. and correct the Development Status is very important for user.  I would like to contains this patch in 1.10.0 release. 

 

What do you think?;;;","06/Feb/20 11:43;gjy;I will fix the version later when building RC3. For sake of correctness it should be 1.10.1 for now.;;;","07/Feb/20 03:48;sunjincheng121;Sounds good! Thanks [~gjy]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskExecutorTest#testSlotAcceptance deadlocks,FLINK-15936,13283641,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,azagrebin,gjy,gjy,06/Feb/20 07:46,14/Apr/20 09:25,13/Jul/23 08:07,14/Apr/20 09:25,1.10.0,1.11.0,,,,1.10.1,1.11.0,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,"https://api.travis-ci.org/v3/job/646510877/log.txt

{noformat}
""main"" #1 prio=5 os_prio=0 tid=0x00007f2f5800b800 nid=0x499 waiting on condition [0x00007f2f61733000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000008669b3a8> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1693)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1729)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.flink.runtime.taskexecutor.TaskExecutorTest.testSlotAcceptance(TaskExecutorTest.java:875)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}",,aljoscha,azagrebin,gjy,guoyangze,rmetzger,trohrmann,xtsong,,,,,,,,,,,,,,,"azagrebin commented on pull request #11667: [FLINK-15936] Harden TaskExecutorTest#testSlotAcceptance
URL: https://github.com/apache/flink/pull/11667
 
 
   ## What is the purpose of the change
   
   ### Concurrent  `TaskSlotTable` in test
   
   The test called `taskSlotTable.allocateSlot` from the test main thread,
   concurrently with `taskSlotTable.createSlotReport `while trying to register
   RM in the main TM thread. This silently failed the RM registration in `TM.runAsync`.
   As a result, RM.notifySlotAvailable was not called in TM.
   The taskSlotTable is not thread-safe and must be accessed only from the main RPC thread of TM.
   
   ### Failure in `establishResourceManagerConnection`
   
   Failure of `TaskExecutor#establishResourceManagerConnection` is not expected.
   It completely breaks the connection mechanism to RM in TM.
   As a hotfix, the PR suggests to log it on error level at least.
   Alternatively, we can consider calling `TM.onFatalError` as a follow-up.
   
   ## Brief change log
   
   The PR refactors the test to wait properly for RM registration
   and allocate slots through gateway in TM thread.
   The PR also uses proper testing RM/JM instead of mocks.
   
   ## Verifying this change
   
   To verify, the test has been looped locally 30k times.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Apr/20 07:41;githubbot;600","azagrebin commented on pull request #11667: [FLINK-15936] Harden TaskExecutorTest#testSlotAcceptance
URL: https://github.com/apache/flink/pull/11667
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Apr/20 09:14;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 09:25:42 UTC 2020,,,,,,,,,,"0|z0b7m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/20 12:11;chesnay;The issue can be produced locally by running the test in a loop a few thousand times.

Here are some logs:
{code}
484496 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Connecting to ResourceManager rm(95b0cc9c8e5447b50e53b2084c7e4a15).
484496 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Resolved ResourceManager address, beginning registration
484496 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Registration at ResourceManager attempt 1 (timeout=100ms)
484496 [main] DEBUG org.apache.flink.runtime.memory.MemoryManager [] - Initialized MemoryManager with total memory size 40960 ({OFF_HEAP=40960}), page size 4096.
484496 [main] DEBUG org.apache.flink.runtime.memory.MemoryManager [] - Initialized MemoryManager with total memory size 40960 ({OFF_HEAP=40960}), page size 4096.
484496 [main] INFO  org.apache.flink.runtime.taskexecutor.JobLeaderService [] - Add job 4022e64d9f9ffc91fa2ab18d2893fcd5 for job leader monitoring.
484496 [main] DEBUG org.apache.flink.runtime.taskexecutor.JobLeaderService [] - New leader information for job 4022e64d9f9ffc91fa2ab18d2893fcd5. Address: jm, leader id: a7b139b06d7c488ea9662b61850a4449.
484496 [main] INFO  org.apache.flink.runtime.taskexecutor.JobLeaderService [] - Try to register at job manager jm with leader id a9662b61-850a-4449-a7b1-39b06d7c488e.
484496 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskexecutor.JobLeaderService [] - Resolved JobManager address, beginning registration
484496 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskexecutor.JobLeaderService [] - Registration at JobManager attempt 1 (timeout=100ms)
484496 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.taskexecutor.JobLeaderService [] - Successful registration at job manager jm for job 4022e64d9f9ffc91fa2ab18d2893fcd5.
484496 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Establish JobManager connection for job 4022e64d9f9ffc91fa2ab18d2893fcd5.
484496 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Offer reserved slots to the leader of job 4022e64d9f9ffc91fa2ab18d2893fcd5.
484496 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Successful registration at resource manager rm under registration id 2f01db6b79bb432974004e65a8159f8e.
484496 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ea290b9891ef70052ab894ce5c3b94ac.
484496 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Free slot with allocation id 7704106cbb7c4c66aa93aec9cde0fb3b because: The slot was rejected by the JobManager.
484496 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:1, state:ALLOCATED, resource profile: ResourceProfile{cpuCores=1.0, taskHeapMemory=100,000kb (102400 bytes), taskOffHeapMemory=0 bytes, managedMemory=40,000kb (40960 bytes), networkMemory=100,000kb (102400 bytes)}, allocationId: 7704106cbb7c4c66aa93aec9cde0fb3b, jobId: 4022e64d9f9ffc91fa2ab18d2893fcd5).
java.lang.Exception: The slot was rejected by the JobManager.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$handleAcceptedSlotOffers$10(TaskExecutor.java:1208) ~[classes/:?]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) ~[?:?]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) ~[?:?]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[classes/:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.11-2.5.21.jar:2.5.21]
484496 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Releasing local state under allocation id 7704106cbb7c4c66aa93aec9cde0fb3b.
485501 [flink-akka.actor.default-dispatcher-10] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - The heartbeat of JobManager with id jm timed out.
485501 [flink-akka.actor.default-dispatcher-10] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Close JobManager connection for job 4022e64d9f9ffc91fa2ab18d2893fcd5.
{code};;;","24/Mar/20 15:43;rmetzger;Another case: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6535&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=fb588352-ef18-568d-b447-699986250ccb;;;","14/Apr/20 09:25;azagrebin;merged into master by 280f344db45d84ad72b59f7f91a75a8875a368e7
merged into 1.10 by 5c524faac5ca882546eeed1227a60e445237ad5e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to use watermark when depends both on flink planner and blink planner,FLINK-15935,13283636,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,zjffdu,zjffdu,06/Feb/20 06:56,07/Feb/20 02:29,13/Jul/23 08:07,07/Feb/20 02:29,1.10.0,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Run the following code in module `flink-examples-table` (must be under this module)
{code:java}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.flink.table.examples.java;


import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.java.StreamTableEnvironment;

/**
 * javadoc.
 */
public class TableApiExample {

   /**
    *
    * @param args
    * @throws Exception
    */
   public static void main(String[] args) throws Exception {

      StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();
      bsEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
      EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
      StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);
      bsTableEnv.sqlUpdate( ""CREATE TABLE sink_kafka (\n"" +
         ""    status  STRING,\n"" +
         ""    direction STRING,\n"" +
         ""    event_ts TIMESTAMP(3),\n"" +
         ""    WATERMARK FOR event_ts AS event_ts - INTERVAL '5' SECOND\n"" +
         "") WITH (\n"" +
         ""  'connector.type' = 'kafka',       \n"" +
         ""  'connector.version' = 'universal',    \n"" +
         ""  'connector.topic' = 'generated.events2',\n"" +
         ""  'connector.properties.zookeeper.connect' = 'localhost:2181',\n"" +
         ""  'connector.properties.bootstrap.servers' = 'localhost:9092',\n"" +
         ""  'connector.properties.group.id' = 'testGroup',\n"" +
         ""  'format.type'='json',\n"" +
         ""  'update-mode' = 'append'\n"" +
         "")\n"");

   }
}
 {code}
 

And hit the following error:
{code:java}
Exception in thread ""main"" org.apache.calcite.runtime.CalciteContextException: From line 5, column 31 to line 5, column 38: Unknown identifier 'event_ts'Exception in thread ""main"" org.apache.calcite.runtime.CalciteContextException: From line 5, column 31 to line 5, column 38: Unknown identifier 'event_ts' at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:834) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:819) at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4841) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5667) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587) at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:317) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676) at org.apache.calcite.sql.SqlOperator.deriveType(SqlOperator.java:501) at org.apache.calcite.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:144) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5600) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:947) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateParameterizedExpression(SqlValidatorImpl.java:930) at org.apache.flink.table.planner.operations.SqlToOperationConverter.lambda$createTableSchema$8(SqlToOperationConverter.java:509) at java.util.Optional.ifPresent(Optional.java:159) at org.apache.flink.table.planner.operations.SqlToOperationConverter.createTableSchema(SqlToOperationConverter.java:505) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertCreateTable(SqlToOperationConverter.java:177) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:130) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:484) at org.apache.flink.table.examples.java.TableApiExample.main(TableApiExample.java:43)Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Unknown identifier 'event_ts' at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463) at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:572) ... 25 more {code}",,jark,libenchao,liyu,zjffdu,,,,,,,,,,,,,,,,,,"wuchong commented on pull request #11030: [FLINK-15935][table] Fix watermark can't work when depending both on flink planner and blink planner in project
URL: https://github.com/apache/flink/pull/11030
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The reason is that we didn't sync the fixing for org.apache.calcite.sql.validate.ParameterScope to flink-planner. When an application project dependent flink-planner and blink-planner at the same time, the classloader may use the ParameterScope from Calcite intead of from planner, which leads to this exception.
   
   The fixing is simple, just copy the org.apache.calcite.sql.validate.ParameterScope class from blink planner to flink planner.
   
   ## Brief change log
   
   - copy `org.apache.calcite.sql.validate.ParameterScope` class from blink planner to flink planner.
   - add an `StreamWindowSQLExample` which uses DDL and watermark to reproduce the problem, and also this example is useful for users to learn about the usage of DDL and watermark statement.
   
   ## Verifying this change
   
   By running the `StreamWindowSQLExample`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Feb/20 08:50;githubbot;600","wuchong commented on pull request #11030: [FLINK-15935][table] Fix watermark can't work when depending both on flink planner and blink planner in project
URL: https://github.com/apache/flink/pull/11030
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Feb/20 02:24;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,FLINK-15075,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 07 02:29:25 UTC 2020,,,,,,,,,,"0|z0b7kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/20 06:59;zjffdu;\cc [~jark] [~lzljs3620320] [~ykt836] [~liyu] [~gjy] ;;;","06/Feb/20 07:12;jark;The reason is that we didn't sync the fixing for {{org.apache.calcite.sql.validate.ParameterScope}} to flink-planner.  When an application project dependent flink-planner and blink-planner at the same time, the classloader may use the {{ParameterScope}} from Calcite intead of from planner, which leads to this exception.

The fixing is simple, just copy the {{org.apache.calcite.sql.validate.ParameterScope}} class from blink planner to flink planner. ;;;","07/Feb/20 02:19;jark;I created FLINK-15944 to track the long-term solution.;;;","07/Feb/20 02:29;jark;Fixed in
 - master (1.11.0): aa38468ff520f850eafadb463c696d105292f420
 - 1.10.0: cdf125f2059122fee44d6819596f454a82259204;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MemoryManager shouldn't allow releasing more memory than reserved,FLINK-15919,13283459,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,liyu,liyu,liyu,05/Feb/20 12:15,07/Feb/20 20:14,13/Jul/23 08:07,06/Feb/20 12:19,1.10.0,,,,,1.10.0,,,,Runtime / Task,,,,,0,pull-request-available,,,,"Currently {{MemoryManager}} allows releasing more memory than reserved for the same owner object, which could be reproduced by adding below test case into {{MemoryManagerTest}}:

{code}
	@Test
	public void testMemoryReleaseGuard() throws MemoryReservationException {
		Object owner = new Object();
		Object owner2 = new Object();

		long totalHeapMemorySize = memoryManager.availableMemory(MemoryType.HEAP);
		memoryManager.reserveMemory(owner, MemoryType.HEAP, PAGE_SIZE);
		memoryManager.reserveMemory(owner2, MemoryType.HEAP, PAGE_SIZE);
		memoryManager.releaseMemory(owner, MemoryType.HEAP, PAGE_SIZE);
		memoryManager.releaseMemory(owner, MemoryType.HEAP, PAGE_SIZE);
		long heapMemoryLeft = memoryManager.availableMemory(MemoryType.HEAP);
		assertEquals(""Memory leak happens"", totalHeapMemorySize - PAGE_SIZE, heapMemoryLeft);
		memoryManager.releaseAllMemory(owner2, MemoryType.HEAP);
	}
{code}",,liyu,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,"carp84 commented on pull request #11025: [FLINK-15919][core][mem] MemoryManager shouldn't allow releasing more memory than reserved
URL: https://github.com/apache/flink/pull/11025
 
 
   
   ## What is the purpose of the change
   
   Currently `MemoryManager` allows to release more memory than the left reserved size and this PR aims at fixing the problem.
   
   
   ## Brief change log
   
   Change the `MemoryManager.releaseMemory` method, recording the actual memory to release during the `reservedMemory` computation and use it for releasing `budgetByType`.
   
   
   ## Verifying this change
   
   This change added a new `testMemoryReleaseMoreThanReserved` test in `MemoryManagerTest` to cover the issue case.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 13:11;githubbot;600","carp84 commented on pull request #11025: [FLINK-15919][core][mem] MemoryManager shouldn't allow releasing more memory than reserved
URL: https://github.com/apache/flink/pull/11025
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Feb/20 11:36;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 06 12:38:06 UTC 2020,,,,,,,,,,"0|z0b6hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/20 12:16;liyu;[~trohrmann] [~sewen] FYI. Please help decide whether this is a blocker for 1.10.0 or it's ok to fix in later versions. Thanks.;;;","05/Feb/20 12:23;liyu;cc [~GJL] [~azagrebin];;;","06/Feb/20 04:00;xtsong;Thanks for reporting this, [~liyu].

I think this is indeed a problem that would be good to fix, but I would not consider this as a blocker for 1.10.0.

Memory manager is an internal class, and as long as there is no memory consumer tries to release memory more than reserved / allocated, it should be fine. Are we already aware of any problem that a memory consumer may release more than reserved / allocated? If not, I would suggest to fix it in later versions.;;;","06/Feb/20 05:05;liyu;bq. Are we already aware of any problem that a memory consumer may release more than reserved / allocated?
None from me.

Some background: the test mentioned in description was motivated by the [discussion|https://github.com/apache/flink/pull/11018#pullrequestreview-353658300] during PR review of FLINK-15905, where we depend on the guard in {{MemoryManager}} to make sure *_later_* changes won't cause ""double release"" of the memory accounting.;;;","06/Feb/20 12:19;liyu;Fix in:
* release-1.10 via f4aa9f81ccc052a7802aee6ad3dfee2b2f8be514
* master via d3f53dd0865b1dc390c09566d1baa49581591e2b;;;","06/Feb/20 12:38;liyu;Change the fix version from {{1.10.0}} to {{1.10.1}}. Will change it back if there comes a RC3 for 1.10.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uptime Metric not reset on Job Restart,FLINK-15918,13283445,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gjy,gjy,gjy,05/Feb/20 11:14,10/Feb/20 15:17,13/Jul/23 08:07,10/Feb/20 15:17,1.10.0,1.9.2,,,,1.10.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"*Description*
The {{uptime}} metric is not reset when the job restarts, which is a change in behavior compared to Flink 1.8.
This change of behavior exists since 1.9.0 if {{jobmanager.execution.failover-strategy: region}} is configured,
which we do in the default flink-conf.yaml.


*Workarounds*
Users that find this behavior problematic can set {{jobmanager.scheduler: legacy}} and unset {{jobmanager.execution.failover-strategy: region}} in their {{flink-conf.yaml}}


*How to reproduce*
trivial

*Expected behavior*
{{uptime}} should be reset on any vertex restart.",,gjy,glaksh100,libenchao,liyu,shriya_a,stevenz3wu,thw,trohrmann,wind_ljy,zhuzh,,,,,,,,,,,,"GJL commented on pull request #11032: [WIP][FLINK-15918]
URL: https://github.com/apache/flink/pull/11032
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Feb/20 14:15;githubbot;600","GJL commented on pull request #11032: [FLINK-15918][FLINK-15917] Uptime Metric not reset on Job Restart / Root Exception not shown in Web UI
URL: https://github.com/apache/flink/pull/11032
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Feb/20 15:14;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 10 08:16:55 UTC 2020,,,,,,,,,,"0|z0b6eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/20 16:20;trohrmann;[~stevenz3wu] [~thw] do you use the {{uptime}} metric to monitor Flink jobs? If yes, then it would be helpful to understand how exactly you use it.;;;","05/Feb/20 17:37;thw;[~trohrmann] our users rely on the metric to monitor their jobs. Continuously increasing uptime is interpreted as necessary for a healthy job. This assumption is broken with the recent change.

> This change of behavior exists since 1.9.0 if {{jobmanager.execution.failover-strategy: region}} is configured, which we do in the default flink-conf.yaml.  

And the change in behavior is not present in 1.9 when the above setting is not present in flink-conf.yaml. It is, therefore, a regression for 1.10.;;;","05/Feb/20 18:25;stevenz3wu;quote an answer from our user, which is similar to what Thomas said.

{quote}

to detect a job that is not running. the reason to use uptime is to catch the case where the job is continually restarting, so it is mostly “up”, but never for a long time

{quote};;;","05/Feb/20 19:36;shriya_a;[~trohrmann] We have, in the past, experienced failure scenarios where a job crashed silently and abruptly, what I mean by that is that it did not exhibit other unhealthy symptoms like failing checkpoints, frequent restarts etc, and in that case uptime is an important metric to rely to know if the job is actually running. ;;;","05/Feb/20 20:26;glaksh100;Just adding to what [~thw]  and [~shriya_a] mentioned, our users today get alerted on the uptime metric being zero. If the metric is not reset (which would mean we don't have time periods of it being zero when the job is not running), then it may mean that our users don't get alerts for when their job is down. ;;;","07/Feb/20 21:32;gjy;1.10: 1268a7b9a3a3d07f76ea1fe78a0b1a6a7d0ef7eb
master: b902ed507e2586c322f33e5612ace4222b3d9d26;;;","10/Feb/20 08:13;liyu;Are we leaving this JIRA open to cherry-pick the changes to master branch? [~gjy] Thanks.;;;","10/Feb/20 08:16;gjy;Yes, ticket is left open because I need to cherry-pick the changes to master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Root Exception not shown in Web UI,FLINK-15917,13283444,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gjy,gjy,gjy,05/Feb/20 11:13,10/Feb/20 15:15,13/Jul/23 08:07,10/Feb/20 15:15,1.10.0,1.9.2,,,,1.10.0,,,,Runtime / Coordination,,,,,0,,,,,"*Description*
 On the job details page in the Exceptions → Root Exception tab, exceptions that cause the job to restart are not displayed.
 This is already a problem since 1.9.0 if {{jobmanager.execution.failover-strategy: region}} is configured,
 which we do in the default flink-conf.yaml.

*Workarounds*
 Users that run into this problem can set {{jobmanager.scheduler: legacy}} and unset {{jobmanager.execution.failover-strategy}} in their {{flink-conf.yaml}}

*How to reproduce*
 In {{flink-conf.yaml}} set {{restart-strategy: fixed-delay}} so enable job restarts.
{noformat}
$ bin/start-cluster.sh
$ bin/flink run -d examples/streaming/TopSpeedWindowing.jar
$ bin/taskmanager.sh stop
{noformat}
Assert that no exception is displayed in the Web UI.

*Expected behavior*
 The stacktrace of the exception should be displayed. Whether the exception should be also shown if only a partial region of the job failed is up for discussion.",,aljoscha,gjy,libenchao,maguowei,trohrmann,wind_ljy,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 07 21:33:01 UTC 2020,,,,,,,,,,"0|z0b6e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/20 08:25;zhuzh;If we define the root exception in the Web UI to be the latest happened exception that caused a failover, how about to always set the failure cause into ExecutionGraph, no matter it's a task failure or a global failure (so partial failover would also have a root cause displayed in Web UI). 
This is not a temporary hack but a valid long term fix.;;;","07/Feb/20 21:33;gjy;1.10: 97960e28efb500e90d4618ba6492bca5f3da08cc
master: ba0e2e0b367a7b81f0c3a018a73fd8cc97592c06;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Miss the barrier alignment metric for the case of two inputs,FLINK-15914,13283409,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zjwang,zjwang,zjwang,05/Feb/20 08:34,11/Feb/20 14:06,13/Jul/23 08:07,05/Feb/20 14:34,1.10.0,,,,,1.10.0,,,,Runtime / Checkpointing,Runtime / Metrics,,,,0,pull-request-available,,,,"When the StreamTwoInputSelectableProcessor was introduced before, it was missing to add the barrier alignment metric in the constructor. But it does not cause problems then, because only StreamTwoInputProcessor works at that time.

After StreamTwoInputProcessor is replaced by StreamTwoInputSelectableProcessor as now, this bug is exposed and we will not see the barrier alignment metric for the case of two inputs.

The solution is to add this metric while constructing the current StreamTwoInputProcessor.",,trohrmann,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,"zhijiangW commented on pull request #11019: [FLINK-15914][checkpointing][metrics] Miss the barrier alignment metric for the case of two inputs
URL: https://github.com/apache/flink/pull/11019
 
 
   ## What is the purpose of the change
   
   When the `StreamTwoInputSelectableProcessor` was introduced before, it was missing to add the barrier alignment metric in the constructor. But it does not cause problems then, because only `StreamTwoInputProcessor` works at that time.  After `StreamTwoInputProcessor` is replaced by `StreamTwoInputSelectableProcessor` as now, this bug is exposed and we will not see the barrier alignment metric for the case of two inputs.
       
   The solution is to add this metric while constructing the `CheckpointBarrierHandler`.
   
   ## Brief change log
   
     - *Add the metric of barrier alignment while constructing the `CheckpointBarrierHandler`*
   
   ## Verifying this change
   
   Via the job testing.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 09:12;githubbot;600","zhijiangW commented on pull request #11019: [FLINK-15914][checkpointing][metrics] Miss the barrier alignment metric for the case of two inputs
URL: https://github.com/apache/flink/pull/11019
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 14:32;githubbot;600","zhijiangW commented on pull request #11031: [FLINK-15914][checkpointing][metrics] Miss the checkpoint related metrics for the case of two inputs
URL: https://github.com/apache/flink/pull/11031
 
 
   ## What is the purpose of the change
   
   When the `StreamTwoInputSelectableProcessor` was introduced before, it forgot adding the checkpoint related metrics in the constructor. But it did not cause any problems, because only the `StreamTwoInputProcessor` actually worked before.
   
   After `StreamTwoInputProcessor` is replaced by `StreamTwoInputSelectableProcessor` as now, this bug is exposed and we will not see the checkpoint related metrics for the case of two inputs.
   
   The solution is to add these metrics while constructing the `CheckpointBarrierHandler`.
   
   ## Brief change log
   
   Add the related metrics while constructing the `CheckpointBarrierHandler` for two input cases.
   
   Refactor the adding of related metrics for one input in order to reuse/unify the codes.
   
   ## Verifying this change
   
   Via the job testing
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Feb/20 11:00;githubbot;600","zhijiangW commented on pull request #11031: [FLINK-15914][checkpointing][metrics] Miss the checkpoint related metrics for the case of two inputs
URL: https://github.com/apache/flink/pull/11031
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Feb/20 10:04;githubbot;600","zhijiangW commented on pull request #11057: [FLINK-15914][tests] Fix the fragile tests caused by race condition of multiple threads
URL: https://github.com/apache/flink/pull/11057
 
 
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   ## Brief change log
   
     - *Fix the `OneInputStreamTaskTest#testCheckpointBarrierMetrics`*
     - *Fix the `TwoInputStreamTaskTest#testCheckpointBarrierMetrics`*
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Feb/20 11:00;githubbot;600","zhijiangW commented on pull request #11057: [FLINK-15914][tests] Fix the fragile tests caused by race condition of multiple threads
URL: https://github.com/apache/flink/pull/11057
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Feb/20 14:06;githubbot;600",,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 11 10:09:29 UTC 2020,,,,,,,,,,"0|z0b66g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/20 14:34;zjwang;Merged in release-1.10: 72b076cbcbfd2636d4980a9fca979e7482b2729c;;;","11/Feb/20 10:09;zjwang;Merged in master: 2a0555bc85e5256718e6b546ffa768a80386e892;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink does not work over NAT,FLINK-15911,13283393,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xtsong,trohrmann,trohrmann,05/Feb/20 07:34,22/Mar/21 12:51,13/Jul/23 08:07,25/Mar/20 01:28,1.10.0,,,,,1.11.0,,,,Runtime / Coordination,,,,,0,pull-request-available,usability,,,"Currently, it is not possible to run Flink over network address translation. The problem is that the Flink processes do not allow to specify separate bind and external ports. Moreover, the {{TaskManager}} tries to resolve the given {{taskmanager.host}} which might not be resolvable. This breaks NAT or docker setups where the external address is not resolvable from within the container/internal network.",,Bo Cui,cyrusand,felixzheng,guoyangze,trohrmann,xtsong,zjwang,,,,,,,,,,,,,,,"xintongsong commented on pull request #11284: [FLINK-15911][runtime] Make Flink work with NAT.
URL: https://github.com/apache/flink/pull/11284
 
 
   ## What is the purpose of the change
   
   This PR makes Flink work with NAT by introducing separated configuration options for address/port and bind-address/bind-port of JM/TM RPC services and Netty Shuffle Service.
   
   ## Brief change log
   
   - 592a79c26d047920e5d7ba20ba99c01c926797ec: Minor code clean-ups.
   - c7f8860864951b33aef27404b1666f0648a26f8b: Refactor to create `AkkaRpsService` with builder class. Currently there are too many nested methods for creating a `AkkaRpsService`, making it hard to understand which changes affect which code paths.
   - 24fb21896bc3bc95532c925057cca5031a1721a3: Separate config options for JM/TM hostname/bind-hostname and RPC port/bind-port.
   - 00808b0c3814cf7bd028b069e5b7022e92569c18: Separate config options for Netty Shuffle Service data port/bind-port.
   - 26db673d7bf7505bd46186fe2c69ffb11c4f76f9: Introduce e2e test for Flink over NAT.
   - 4683aaeae4a1ee29a403bf6495f2065c539b224f: **DO_NOT_MERGE_THIS** Add the NAT e2e test to pre-commit tests, for the convenience of review.
   
   ## Verifying this change
   
   Added a docker-based e2e test for verifying Flink works over NAT.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Mar/20 13:27;githubbot;600","tillrohrmann commented on pull request #11284: [FLINK-15911][runtime] Make Flink work with NAT.
URL: https://github.com/apache/flink/pull/11284
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Mar/20 14:26;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13717,FLINK-15154,FLINK-16751,,,,,,,,,"22/Mar/21 12:44;Bo Cui;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13022768/screenshot-1.png","22/Mar/21 12:44;Bo Cui;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13022771/screenshot-2.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 22 12:48:32 UTC 2021,,,,,,,,,,"0|z0b62w:",9223372036854775807,"Flink now supports working over NAT networks, by configuring separated external and local bind addresses/ports.

The following configuration options will be used for external addresses/ports.
* jobmanager.rpc.address
* jobmanager.rpc.port
* taskmanager.host
* taskmanager.rpc.port
* taskmanager.data.port

The following configuration options will be used for local bind addresses/ports.
* jobmanager.bind-host
* jobmanager.rpc.bind-port
* taskmanager.bind-host
* taskmanager.rpc.bind-port
* taskmanager.data.bind-port

Note: Currently, the new feature does not work with queryable state.
Note: If hostname is used for the external address of a Flink process (JM/TM), it is required to be resolvable from all the other processes (excluding itself).",,,,,,,,,,,,,,,,,,,"21/Feb/20 10:39;xtsong;I've spent some time looking into this issue, together with FLINK-13171 & FLINK-15154. I think these three ticket can be resolved together. Therefore, I'm closing the other two tickets and working on this ticket.

I'm posting my proposed solution to this issue below, in case someone has a different thought.

In addition to the current configuration options ('jobmanager.rpc.[address|port]', 'taskmanager.[host|rpc.port]'), I propose to introduce the 4 new config options: '[jobmanager|taskmanager].rpc.bind-[address|port]'.
- The original config options will be used to decided the exposed address and port, same as they are now.
- The new config options will be used to decide the bind address and port, without default values.
- If the bind address/port is not specified, by default JM/TM binds to the IP address 0.0.0.0 and the port that is exposed, same as the current behavior.
- Note that, while '[jobmanager|taskmanager].rpc.port' allows specifying a range of ports (e.g., 50100-50200) or any free port (if the value is 0), '[jobmanager|taskmanager].rpc.bind-port' accepts only one specific non-zero port. 

Minor: I'm actually thinking about changing 'taskmanager.host' to 'taskmanager.rpc.address', with backward compatibility on the old key, for unifying the config keys' format. Not absolutely necessary though.;;;","27/Feb/20 15:12;trohrmann;The plan sounds good to me [~xintongsong]. Please go ahead with the implementation.;;;","02/Mar/20 05:59;xtsong;Some updates on this ticket.

I've decoupled bind-address/bind-port from the address/port of JM/TM RPC services, verified successfully with a docker-based e2e test with the default parallelism 1. But I run into problems when increasing the parallelism to have multiple TMs, because TMs failed to find each other's Netty shuffle address/port.

I talked to [~zjwang] offline. He confirmed that Netty shuffle service uses TM address in two ways:
 * The address passed into NettyShuffleEnvironment is used for binding to the local address. It should use the bind-address.
 * The address wrapped in TaskManagerLocation will be sent to JobMaster, which will be used by tasks for accessing the TM's shuffle service.

I will continue trying to resolve address/port problem of Netty shuffle service.

In addition, the address/port and bind-address/bind-port of the following services may also need to separated. I would like to exclude them from the scope of this ticket, to keep a minimum set of changes in this ticket for getting Flink work over NAT.
 * Blob Server on JM. This is only needed if we we want to submit jobs from outside of NAT to a Flink session cluster whose JM runs behind NAT. I will try to address this in FLINK-15154.
 * KvStateService on TM. This is only used for queryable state, which I'm not sure how many use cases do we have. Also, I'm not familiar with how the KvStateService works. If we want to get it work over NAT, I would need help from someone familiar with it.;;;","24/Mar/20 14:27;trohrmann;Fixed via

03b6e763d5578c3b503246a2a40e5fd1fb990570
8a1dbcc07d6b4ed7702741cd8b1c73d95f0d2583
3c997796e3aa6c4498849bcb029ffe6f5db515fc
9dc7a4a47f85153b98e1f50eb8917dd6c8e15c85
5c35d9d95144593193244a07b965e310707488a9;;;","25/Mar/20 01:27;xtsong;Reopen to add release notes.;;;","22/Mar/21 02:17;Bo Cui;[~xintongsong] in yarn cluster, what are  bind-address of JM&TM？ It is not a good practice to listen on all interfaces(0.0.0.0). and many companies are not allowed to listen on 0.0.0.0 or ::.
thx;;;","22/Mar/21 02:48;xtsong;Hi [~Bo Cui],

Could you explain why listening on 0.0.0.0 is not allowed? And if not 0.0.0.0, which address should flink bind to in your case?;;;","22/Mar/21 03:30;Bo Cui;1、all 0 has some security risks.
2、i think add new config, like network.allow.listen.0, default true.  if false, we shold use other address;;;","22/Mar/21 03:46;xtsong;bq. 1、all 0 has some security risks.
Could you explain in specific what security risks are caused by listening to 0.0.0.0? Are all of your Yarn NM's exposed via external IP addressed? Shouldn't it be the responsibility of something like a firewall to prevent unwanted accesses?

bq. 2、i think add new config, like network.allow.listen.0, default true. if false, we shold use other address
The question is which ""other address"" should be used?;;;","22/Mar/21 06:29;Bo Cui;1、 all client IP can access the port through any NIC. yes ,the problem can be solved by using the firewall, but no network adapter is missing from the firewall, and it is not a good practice to listen on all interfaces
2、like https://github.com/apache/flink/blob/a33e6bd390a9935c3e25b6913bed0ff6b4a78818/flink-yarn/src/main/java/org/apache/flink/yarn/YarnTaskExecutorRunner.java#L140, we can use taskmanager.host, or update value of taskmanager.bind-host in YarnTaskExecutorRunner#setupConfigurationFromVariables. Or other better way...
thx;;;","22/Mar/21 08:07;xtsong;I think it's a good practice to listen on as less interfaces as possible *only if* we know which interface exactly should be used.

In your case, IIUC, the security concern comes from the fact that there are multiple network interfaces on your Yarn NM machines, and not all of them are safe. IMO, this is a flaw of Yarn cluster management. Applications running on Yarn (i.e. Flink) should not understand which interface is safe and which is not. These physical specifications are supposed to be transparent to the applications.

I'm afraid binding to `taskmanager.host` is not an option.
 * This configuration option represents the external address that other peers should use to connect to the current peer. Depending on the network setups, this address may not be resolvable locally.
 * As the codes suggest, this configuration option may not alway be set. The value can be `null`.;;;","22/Mar/21 08:24;Bo Cui;got...thank you very much [~xintongsong](y);;;","22/Mar/21 09:49;Bo Cui;but *on yarn*, taskmanager.host can not be null, and All addresses can be resolved locally. right？ so we can update value of taskmanager.bind-host in YarnTaskExecutorRunner#setupConfigurationFromVariables？ [~xintongsong];;;","22/Mar/21 10:03;xtsong;Isn't it depending on the machine setups that whether all addresses can be resolved locally?

And most importantly, as explained above, I'm not sure if the application should decide which network interface on the machine should be used.;;;","22/Mar/21 12:48;Bo Cui;!screenshot-1.png! 
 !screenshot-2.png! 
 i think it might be better on yarn...taskExecutorHostname must be resolved
{quote}I'm not sure if the application should decide which network interface on the machine should be used.
{quote}
TM use the IP address assigned by the JM.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Race Condition when releasing shared memory resource,FLINK-15905,13283314,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sewen,sewen,04/Feb/20 19:56,05/Feb/20 12:13,13/Jul/23 08:07,05/Feb/20 12:13,,,,,,1.10.0,,,,Runtime / Task,,,,,0,pull-request-available,,,,"There is a race condition around releasing an {{OpaqueMemoryResource}} obtained via {{MemoryManager.getSharedMemoryResourceForManagedMemory}}.

{code}
final boolean allDisposed = sharedResources.release(type, leaseHolder);
if (allDisposed) {
	releaseMemory(type, MemoryType.OFF_HEAP, size);
}
{code}

If another allocation occurs while the releasing thread is between the ""sharedResources.release""and the ""if block"", then there is no resource to pick up any more and memory has not yet been returned to the memory bookkeeping.",,jark,libenchao,sewen,trohrmann,wind_ljy,xtsong,,,,,,,,,,,,,,,,"StephanEwen commented on pull request #11018: [FLINK-15905][runtime] Fix race condition between allocation and release of OpaqueMemoryResource
URL: https://github.com/apache/flink/pull/11018
 
 
   ## What is the purpose of the change
   
   Fixes the race condition when releasing `OpaqueMemoryResource`between the following lines: 
   ```java
   final boolean allDisposed = sharedResources.release(type, leaseHolder);
   if (allDisposed) {
   	releaseMemory(type, MemoryType.OFF_HEAP, size);
   }
   ```
     - The shared resource release happens under lock
     - Releasing the memory to the memory manager is outside the lock
     - Another thread can find the resource released, but the memory not yet returned
   
   The solution is to push the memory release (via a lambda) into the `sharedResources` such that the release also happens under the lock.
   
   ## Verifying this change
   
   This extends the existing unit tests.
   Thread safety is not verified in unit tests, as this is not easily possible.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 08:04;githubbot;600","StephanEwen commented on pull request #11018: [FLINK-15905][runtime] Fix race condition between allocation and release of OpaqueMemoryResource
URL: https://github.com/apache/flink/pull/11018
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 11:54;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,FLINK-15900,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 05 12:13:19 UTC 2020,,,,,,,,,,"0|z0b5lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/20 20:04;sewen;I have a patch, waiting for CI run to open a pull request.;;;","05/Feb/20 12:13;sewen;Fixed in
  - 1.10.0 via c305784f868d2795b433534764847edfec05a969
  - master (1.11.0) via 939889aa07f9c69e54eb5dabf74da06c37428ed0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Make Kafka Consumer work with activated ""disableGenericTypes()""",FLINK-15904,13283246,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Oleksandr Nitavskyi,aljoscha,aljoscha,04/Feb/20 13:28,03/Jun/20 13:09,13/Jul/23 08:07,20/Feb/20 17:00,,,,,,1.10.1,1.11.0,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"A user reported a problem that the Kafka Consumer doesn't work in that case: https://lists.apache.org/thread.html/r462a854e8a0ab3512e2906b40411624f3164ea3af7cba61ee94cd760%40%3Cuser.flink.apache.org%3E. We should use a different constructor for {{ListStateDescriptor}} that takes {{TypeSerializer}} here: https://github.com/apache/flink/blob/68cc21e4af71505efa142110e35a1f8b1c25fe6e/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java#L860. This will circumvent the check.

My full analysis from the email thread:

{quote}
Unfortunately, the fact that the Kafka Sources use Kryo for state serialization is a very early design misstep that we cannot get rid of for now. We will get rid of that when the new source interface lands ([1]) and when we have a new Kafka Source based on that.

As a workaround, we should change the Kafka Consumer to go through a different constructor of ListStateDescriptor which directly takes a TypeSerializer instead of a TypeInformation here: [2]. This should sidestep the ""no generic types"" check.

[1] https://cwiki.apache.org/confluence/display/FLINK/FLIP-27%3A+Refactor+Source+Interface
[2] https://github.com/apache/flink/blob/68cc21e4af71505efa142110e35a1f8b1c25fe6e/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java#L860
{quote}",,aljoscha,alpinegizmo,dwysakowicz,maguowei,Oleksandr Nitavskyi,,,,,,,,,,,,,,,,,"JTaky commented on pull request #11145: [FLINK-15904][connectors/kafka] Make Kafka Consumer work with activated ""disableGenericTypes()""
URL: https://github.com/apache/flink/pull/11145
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Currently, class KafkaTopic KafkaTopicPartition uses GenericTypeInformation during the serialization in the unionOffsetStates inside the KafkaConsumer. Which prevents any flink application to `disableGenericTypes`.
   We can still continue to use compatible KryoSerialization, but by specifying serialization explicitly we can avoid check on generic types.
   
   ## Brief change log
   
   - Replace TypeHint in the state initialization by explicit TupleSerializer which takes KryoSerializer as the parameter.
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: don't know
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? No
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Feb/20 14:55;githubbot;600","aljoscha commented on pull request #11145: [FLINK-15904][connectors/kafka] Make Kafka Consumer work with activated ""disableGenericTypes()""
URL: https://github.com/apache/flink/pull/11145
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Feb/20 17:00;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,FLINK-12410,,,,,,,,,,,,FLINK-11911,,FLINK-11911,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 20 17:17:16 UTC 2020,,,,,,,,,,"0|z0b568:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/20 15:05;Oleksandr Nitavskyi;made a PR, but cannot assign the Jira ticket on me :(;;;","19/Feb/20 15:53;aljoscha;assigned!;;;","20/Feb/20 17:00;aljoscha;master: 7a9580e021ce157c8a1adfbcd79077b4647789d6;;;","20/Feb/20 17:17;aljoscha;release-1.10: cf448ae117f9615c38be075265f6a5572490fcd5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis consumer fails due to jackson-dataformat-cbor conflict in 1.10 RC1,FLINK-15868,13283034,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,thw,thw,03/Feb/20 18:54,05/Feb/20 14:53,13/Jul/23 08:07,05/Feb/20 14:53,1.10.0,,,,,1.10.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,There appears to be an issue with incompatible dependencies being shaded in the connector. This only happens when running against the actual Kinesis service (i.e. when CBOR isn't disabled).,,aljoscha,gjy,thw,trohrmann,,,,,,,,,,,,,,,,,,"tweise commented on pull request #11006: [FLINK-15868][kinesis] Resolve version conflict between jackson-core and jackson-dataformat-cbor
URL: https://github.com/apache/flink/pull/11006
 
 
   
   
   ## What is the purpose of the change
   
   For the Kinesis consumer to work, `jackson-core` and `jackson-dataformat-cbor` need to be at the same version. This change will ensure that users get the same version w/o having to override the `jackson-dataformat-cbor` dependency.
   
   ## Verifying this change
   
   Run `mvn dependency:tree` on a downstream project and check that versions are same. Was also tested with the 1.10 RC1 with one of our internal deployments.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Feb/20 23:20;githubbot;600","tillrohrmann commented on pull request #11006: [FLINK-15868][kinesis] Resolve version conflict between jackson-core and jackson-dataformat-cbor
URL: https://github.com/apache/flink/pull/11006
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 14:01;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 05 14:53:23 UTC 2020,,,,,,,,,,"0|z0b45s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/20 18:54;thw;{code:java}
org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)\n\t... 10 more\nCaused by: java.lang.RuntimeException: Jackson jackson-core/jackson-dataformat-cbor incompatible library version detected.\nYou have two possible resolutions:\n\t\t1) Ensure the com.fasterxml.jackson.core:jackson-core & com.fasterxml.jackson.dataformat:jackson-dataformat-cbor libraries on your classpath have the same version number\n\t\t2) Disable CBOR wire-protocol by passing the -Dcom.amazonaws.sdk.disableCbor property or setting the AWS_CBOR_DISABLE environment variable (warning this may affect performance)\n\tat org.apache.flink.kinesis.shaded.com.amazonaws.protocol.json.SdkCborGenerator.getBytes(SdkCborGenerator.java:69)\n\tat org.apache.flink.kinesis.shaded.com.amazonaws.protocol.json.internal.JsonProtocolMarshaller.finishMarshalling(JsonProtocolMarshaller.java:190)\n\tat org.apache.flink.kinesis.shaded.com.amazonaws.protocol.json.internal.NullAsEmptyBodyProtocolRequestMarshaller.finishMarshalling(NullAsEmptyBodyProtocolRequestMarshaller.java:53)\n\tat org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.model.transform.ListShardsRequestProtocolMarshaller.marshall(ListShardsRequestProtocolMarshaller.java:57)\n\t... 26 more\nCaused by: java.lang.NoSuchMethodError: shaded.spkinesis.com.fasterxml.jackson.dataformat.cbor.CBORGenerator.getOutputContext()Lshaded/spkinesis/com/fasterxml/jackson/core/json/JsonWriteContext;\n\tat shaded.spkinesis.com.fasterxml.jackson.dataformat.cbor.CBORGenerator.close(CBORGenerator.java:903)\n\tat org.apache.flink.kinesis.shaded.com.amazonaws.protocol.json.SdkJsonGenerator.close(SdkJsonGenerator.java:267)\n\tat org.apache.flink.kinesis.shaded.com.amazonaws.protocol.json.SdkJsonGenerator.getBytes(SdkJsonGenerator.java:282)\n\tat org.apache.flink.kinesis.shaded.com.amazonaws.protocol.json.SdkCborGenerator.getBytes(SdkCborGenerator.java:67)\n\t... 29 more\n\nEnd of exception on server side>""]}'
 {code};;;","03/Feb/20 19:05;gjy;Is there any way to enable CBOR in the E2E tests?;;;","03/Feb/20 19:45;thw;[~gjy] I'm not sure if kinesalite works with CBOR, you would have to try it.;;;","03/Feb/20 20:33;chesnay;When we pinned jackson to 2.10.1 we did not bump all jackson dependencies; I assumed that the formats are less dependent on the main releases; guess I was wrong on that one.;;;","03/Feb/20 20:39;chesnay;Note that the connector itself isn't shading jackson; this must be done by the user-jar.

Actually, can you show me the pom of the application? Dependency management entries in our project shouldn't affect user applications; unless they use flink-parent as a parent module.;;;","03/Feb/20 20:45;thw;I'm working on this and should have an update soon.;;;","04/Feb/20 09:22;chesnay;Shading strikes again; for the kinesis connector the shade-plugin is configured to promote transitive dependencies, which then actively puts jackson as an actual dependency into the pom, with the version being governed by the dependency management.
The promotion is somewhat-ish required since we shade the kinesis sdk but not all of its transitive dependencies, and otherwise these dependencies would just be lost.;;;","05/Feb/20 14:53;trohrmann;Fixed via

master:
e51ad132d2
264ccf6e2e
3a7edb36ee
02c9c0920a
652e979767
cf126ffbe1

1.10.0:
3ed189c815
ec5c9c6ab1
f42c0d3d8c
ccd390562e
f6be5679c8
81cf2f9e59
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClosureCleaner#getSuperClassOrInterfaceName throws NPE for Object,FLINK-15866,13282985,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wuxuyang78,wuxuyang78,wuxuyang78,03/Feb/20 14:11,27/May/20 06:05,13/Jul/23 08:07,19/Feb/20 14:50,1.9.2,,,,,1.11.0,,,,API / Core,,,,,0,pull-request-available,,,,"When param ‘cls’ is Object class, it will throw NPE.
{code:java}
private static String getSuperClassOrInterfaceName(Class<?> cls) {
   Class<?> superclass = cls.getSuperclass();
   if (superclass.getName().startsWith(""org.apache.flink"")) {
      return superclass.getSimpleName();
   } else {
      for (Class<?> inFace : cls.getInterfaces()) {
         if (inFace.getName().startsWith(""org.apache.flink"")) {
            return inFace.getSimpleName();
         }
      }
      return null;
   }
}{code}
Class<?> superclass = cls.getSuperclass();

superclass is null

 

 ",,aljoscha,wuxuyang78,,,,,,,,,,,,,,,,,,,,"LululuAlu commented on pull request #11003: [FLINK-15866][core]ClosureCleaner#getSuperClassOrInterfaceName throw NPE
URL: https://github.com/apache/flink/pull/11003
 
 
   Object type field was not implement Serializable, so checked it as unserialize object, and add check null before call superclass's method.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Feb/20 14:52;githubbot;600","aljoscha commented on pull request #11003: [FLINK-15866][core]ClosureCleaner#getSuperClassOrInterfaceName throw NPE
URL: https://github.com/apache/flink/pull/11003
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Feb/20 14:50;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 06:05:14 UTC 2020,,,,,,,,,,"0|z0b3uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/20 15:24;wuxuyang78;Object type field was not implement Serializable, so checked it as unserialize object, and add check null before call superclass's method.;;;","19/Feb/20 14:50;aljoscha;master: de2e82ae365c203d7b7104d876078e65b6b2d040;;;","27/May/20 06:05;wuxuyang78;thx;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix docs stating that savepoints are relocatable,FLINK-15863,13282955,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,bdine,nkruber,nkruber,03/Feb/20 10:51,07/Feb/20 11:20,13/Jul/23 08:07,05/Feb/20 11:08,1.10.0,1.9.2,,,,1.10.0,1.8.4,1.9.3,,Documentation,,,,,0,pull-request-available,usability,,,"This section from https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#preconditions states that savepoints are relocatable which they are not yet (see FLINK-5763). It should be fixed and/or removed; I'm unsure what change from 1.3 it should actually reflect.

{quote}Another important precondition is that for savepoints taken before Flink 1.3.x, all the savepoint data must be accessible from the new installation and reside under the same absolute path. Before Flink 1.3.x, the savepoint data is typically not self-contained in just the created savepoint file. Additional files can be referenced from inside the savepoint file (e.g. the output from state backend snapshots). Since Flink 1.3.x, this is no longer a limitation; savepoints can be relocated using typical filesystem operations..{quote}
",,bdine,mzuehlke,nkruber,,,,,,,,,,,,,,,,,,,"bdine commented on pull request #11015: [FLINK-15863][docs] Fix docs stating that savepoints are relocatable
URL: https://github.com/apache/flink/pull/11015
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix documentation on savepoints pre-conditions, and clarify that savepoints are not relocatable for now
   
   ## Brief change log
   
   - Changes occur on savepointing in 1.3
   - Documentation needs to be updated
   - New changes are coming w.r.t savepoint relocation
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Feb/20 12:23;githubbot;600","NicoK commented on pull request #11015: [FLINK-15863][docs] Fix docs stating that savepoints are relocatable
URL: https://github.com/apache/flink/pull/11015
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 11:00;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 05 11:08:54 UTC 2020,,,,,,,,,,"0|z0b3o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/20 07:56;bdine;Hello [~NicoK],
 Maybe we can just simplify it, by just removing the reference of 1.3
 Savepoints, with modification by State Processor API, can also reference other savepoints (as do incremental checkpoiting with checkpoint)
 And this is absolute path, so this is non relocatable too
 Maybe something like :
{code:java}
Another important precondition is that all the savepoint data must be accessible from the new installation. Additional files can be referenced from inside the savepoint file (e.g. the output from state backend snapshots) and savepoint can referenced others (e.g. modification with State Processor API *link* ?) The savepoint data are referenced through absolute path by meta data file and thus, a savepoint is typically not relocatable using typical filesystem operations.
{code};;;","04/Feb/20 09:23;nkruber;What do you think about this? (I could also create a small PR for this)
{code}
Another important precondition is that all the savepoint data must be accessible from the new installation under the same (absolute) path. This also includes access to any additional files that are referenced from inside the savepoint file (the output from state backend snapshots), including, but not limited to additional referenced savepoints from modifications with the State Processor API *link*. Any savepoint data is currently referenced by absolute paths inside the meta data file and thus a savepoint is typically not relocatable via typical filesystem operations.
{code};;;","04/Feb/20 09:35;bdine;Seems good to me ! Okay for the PR
I can not assign a Jira to myself as I am not contributor yet;;;","04/Feb/20 09:57;nkruber;sure, I can do it, then please create a PR [~bdine] and I'll review it;;;","04/Feb/20 12:25;bdine;Ok thanks, PR is on its way;;;","05/Feb/20 11:08;nkruber;Fixed via:
 * master: 7ef6eb45ce65baff20061a50c312118031a19f1b
 * release-1.10: bf722e7b3c0a1f9d43e685a8a7657405f3959226
 * release-1.9: e4eb30362d0a8902b8bb958137064e8456342ced
 * release-1.8: 3f307fb02b5844936d973ce519f86d1fd9d8fe49;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to use HiveCatalog and kafka together,FLINK-15858,13282916,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,zjffdu,zjffdu,03/Feb/20 07:15,06/Feb/20 05:02,13/Jul/23 08:07,05/Feb/20 10:04,1.10.0,,,,,1.10.0,,,,Table SQL / Ecosystem,,,,,0,pull-request-available,,,," 

HiveCatalog only support timestamp(9), but kafka only support timestamp(3). This make user unable to use HiveCatalog and kafka together
{code:java}

Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: HiveCatalog currently only supports timestamp of precision 9
	at org.apache.flink.table.catalog.hive.util.HiveTypeUtil$TypeInfoLogicalTypeVisitor.visit(HiveTypeUtil.java:272)
	at org.apache.flink.table.catalog.hive.util.HiveTypeUtil$TypeInfoLogicalTypeVisitor.visit(HiveTypeUtil.java:173)
	at org.apache.flink.table.types.logical.TimestampType.accept(TimestampType.java:151)
	at org.apache.flink.table.catalog.hive.util.HiveTypeUtil.toHiveTypeInfo(HiveTypeUtil.java:84)
	at org.apache.flink.table.catalog.hive.util.HiveTableUtil.createHiveColumns(HiveTableUtil.java:106) {code}",,lirui,liyu,lzljs3620320,phoenixjiangnan,zjffdu,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #11012: [FLINK-15858][hive] Store generic table schema as properties
URL: https://github.com/apache/flink/pull/11012
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Store schema of generic table as properties, so that `HiveCatalog` is able to support types that are not supported by Hive, e.g. `TIMESTAMP(!=9)`.
   
   
   ## Brief change log
   
     - Store table schema as properties when creating generic tables
     - Retrieve table schema from properties when getting generic tables
     - Add test
   
   
   ## Verifying this change
   
   Existing and new test cases.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? yes
     - If yes, how is the feature documented? docs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Feb/20 06:57;githubbot;600","KurtYoung commented on pull request #11012: [FLINK-15858][hive] Store generic table schema as properties
URL: https://github.com/apache/flink/pull/11012
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 06:47;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15901,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 06 05:02:52 UTC 2020,,,,,,,,,,"0|z0b3fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/20 07:16;zjffdu;\cc [~lirui] [~lzljs3620320];;;","03/Feb/20 07:46;lirui;This seems to be more of a limitation than a bug to me. HMS doesn't support storing precision info for {{TIMESTAMP}} columns. That's why we disallow creating {{TIMESTAMP(!=9)}} with HiveCatalog -- the precision will be discarded anyway.

Maybe for generic tables, we can store the schema as table properties, so as to support Flink-specific types.;;;","04/Feb/20 01:57;lzljs3620320;+1 to fix this.

I think this should be supported. When we design HiveCatalog, an important feature is to support generic tables, and Kafka is the most important generic table.;;;","04/Feb/20 01:58;lzljs3620320;If I remember correctly, 1.9 is supported. 1.10 broken.;;;","04/Feb/20 02:40;zjffdu;In that case, it is a regression issue, definitely need to fix in 1.10;;;","04/Feb/20 11:14;lzljs3620320;Hi [~zjffdu], it seems that we need take more time to finish it. I set it critical to not block RC release.;;;","05/Feb/20 01:58;zjffdu;[~lzljs3620320] But IIRC, this issue means users unable to use kafka with hivecatalog together. Doesn't that mean a blocker issue ?;;;","05/Feb/20 02:10;zjffdu;[~liyu] [~gjy] This ticket means users unable to use kafka with hivecatalog together when involving timestamp field. I believe this should be a blocker issue for 1.10 ;;;","05/Feb/20 03:12;lzljs3620320;[~zjffdu] Sorry, I underestimated its importance. Code looked good to me, we can add more it cases. I pulled [~bli] and [~ykt836] to review&merge.;;;","05/Feb/20 04:11;phoenixjiangnan;I'm ok with the proposed solution.

However, that means users cannot use basic SQL like ""describe <table>"" in Hive to probe the table, as stated in example of [https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/hive_catalog.html#step-4-start-sql-client-and-create-a-kafka-table-with-flink-sql-ddl]

 

Another solution is to store precisions for such data types in properties or the column names, and keep the schema still in Hive table schema.;;;","05/Feb/20 04:28;lzljs3620320;Hi [~phoenixjiangnan], Good point.

But I think maybe it's not just precision. Some other types of Flink can't be expressed in hive. And it doesn't look good to split type and precision.

I think we can leave it to the 1.11 later to consider whether we want to support the display and use of Flink's table in hive.;;;","05/Feb/20 05:35;ykt836;Consider we don't have much time, fix this issue and improve other usability sounds a reasonable solution. 

BTW, [~phoenixjiangnan] could you create a Jira to track what you want to achieve?;;;","05/Feb/20 05:52;lirui;bq. that means users cannot use basic SQL like ""describe <table>"" in Hive to probe the table

It doesn't seem to be a big issue to me. Since generic tables cannot be used in Hive anyway, what's the point to support displaying the schema via Hive CLI? In addition, it's not just about precision info, there can be Flink types that cannot be mapped to corresponding Hive types.

If users want to check whether a table is generic or not, they can call {{DESCRIBE FORMATTED}} to look for the {{is_generic}} property.;;;","05/Feb/20 10:04;ykt836;1.10.0: ba5b5b77cdfeafa4b84028d48af66f130a2cc071

master: 4fcd877e5c3290412f4a71d1cd01ff3a0b3a4562;;;","06/Feb/20 05:02;phoenixjiangnan;[~lirui] [~lzljs3620320] please go over Flink docs to update related content https://issues.apache.org/jira/browse/FLINK-15933;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job is submitted to the wrong session cluster,FLINK-15852,13282896,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kkl0u,felixzheng,felixzheng,03/Feb/20 04:07,20/Mar/20 01:18,13/Jul/23 08:07,19/Mar/20 16:05,1.10.0,,,,,1.10.1,1.11.0,,,Command Line Client,,,,,0,pull-request-available,,,,"Steps to reproduce the problem:
 # Deploy a YARN session cluster by command {{./bin/yarn-session.sh -d}}
 # Deploy a Kubernetes session cluster by command {{./bin/kubernetes-session.sh -Dkubernetes.cluster-id=test ...}}
 # Try to submit a Job to the Kubernetes session cluster by command {{./bin/flink run -d -e kubernetes-session -Dkubernetes.cluster-id=test examples/streaming/WordCount.jar}}

It's expected that the Job will be submitted to the Kubernetes session cluster whose cluster-id is *test*, however, the job was submitted to the YARN session cluster.

 ",,felixzheng,kkl0u,tison,,,,,,,,,,,,,,,,,,,"kl0u commented on pull request #11435: [FLINK-15852][cli] Prioritize ExecutorCLI over YarnSessionCLI for activeCLI
URL: https://github.com/apache/flink/pull/11435
 
 
   ## What is the purpose of the change
   
   This PR changes the order in which the `CliFrontend` selects the active CLI command. 
   
   Currently in Flink there are 3 custom command lines, the `DefaultCLI`, the `FlinkYarnSessionCli` and the `ExecutorCLI` and when the user submits his/her CLI command, the `CliFrontend` selects which one to use (i.e. which one is active) for further processing based on criteria specific to each custom command line.
   
   The `FlinkYarnSessionCli` is chosen if there is a file called `.yarn-properties-USERNAME`  (among other criteria) in a specific directory. This file can be written transparently to the user, if he/she used the CLI previously to launch a yarn session cluster. 
   
   The `ExecutorCLI` is activated if and only if there is an explicit `-e` flag specified by the user. 
   
   Given that currently Flink first checks if the yarn cli is active and, if not, it only then checks the `ExecutorCLI`, if this ""magic"" file exists from a previous run, there is no way for the user to use the `ExecutorCLI` without deleting the ""properties"" file, which may be useful in the future.
   
   Given that in the case of the `ExecutorCLI`, an explicit action is required by the user to activate it, while this is not the case for the yarn cli, this PR aims at changing the order in which the cli's are checked for being activated or not.
   
   ## Brief change log
   
   Changes the order of loading the clis in the `CliFrontend.loadCustomCommandLines()`.
   
   ## Verifying this change
   
   Added the `FlinkYarnSessionCliTest.testExecutorCLIisPrioritised()`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/20 08:22;githubbot;600","kl0u commented on pull request #11435: [FLINK-15852][cli] Prioritize ExecutorCLI over YarnSessionCLI for activeCLI
URL: https://github.com/apache/flink/pull/11435
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 16:03;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 20 01:18:21 UTC 2020,,,,,,,,,,"0|z0b3b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/20 06:34;tison;I think the problem is about how we load so-called active custom command-line

[~kkl0u] [~yangwang166];;;","03/Feb/20 06:35;tison;the stack layout is 

0. YARNCLI
1. ExecutorCLI
2. DefaultCLI

so the command is handled by YARNCLI if it is found due to hadoop(yarn) in classpath.;;;","03/Feb/20 08:48;kkl0u;Yes this seems to be problematic and it may have also to do with the ""magic"" yarn-properties file. ;;;","04/Feb/20 01:38;felixzheng;Yep it may have to do with the yarn-properperties file, anyone interested in this ticket could take over it.;;;","17/Mar/20 19:05;kkl0u;I will fix this by simply changing the custom command line prioritisation and checking first the {{ExecutorCLI}} and then the {{FlinkYarnSessionCLI}}. This actually makes sense, as the {{ExecutorCLI}} requires explicit action by the user ({{-e}}) so it cannot be activated ""by accident"". I will submit a PR soon.;;;","19/Mar/20 16:05;kkl0u;Merged on master with 6ee5dbabfa4e0efa7ac8f1f8e03a75c303ffc622
and on release-1.10 with aa4d6270e4e764daa392d70369b46df5b7ba4054;;;","20/Mar/20 01:18;felixzheng;Thanks for the fixup [~kkl0u]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update SQL-CLIENT document from type to data-type,FLINK-15849,13282889,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,danny0405,lzljs3620320,lzljs3620320,03/Feb/20 03:13,10/Aug/20 14:30,13/Jul/23 08:07,11/Jun/20 12:05,,,,,,1.10.2,1.11.0,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,There are documentation of {{type}} instead of {{data-type}} in sql-client.,,godfreyhe,jark,leonard,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 12:05:18 UTC 2020,,,,,,,,,,"0|z0b39k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/20 03:13;lzljs3620320;CC: [~jark];;;","11/Jun/20 12:05;lzljs3620320;master: 0028e2e8631d953887f5feef4bfd455b634e0f97

release-1.11: bc64a88de2d56b6ef4342334febe202b616e4402;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removal of JobWithJars.buildUserCodeClassLoader method without Configuration breaks backwards compatibility,FLINK-15844,13282840,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,iemejia,iemejia,iemejia,02/Feb/20 14:04,03/Feb/20 09:37,13/Jul/23 08:07,03/Feb/20 09:37,1.9.2,,,,,1.9.3,,,,Client / Job Submission,,,,,0,pull-request-available,,,,"The removal of the method `JobWithJars.buildUserCodeClassLoader` without `Configuration` is not backwards compatible with precedent versions of Flink 1.9.x

I was  trying to upgrade to the just released version on Apache Beam and it broke, so I found this:
 [https://output.jsbin.com/zudemis/3#Source_Removed]",,aljoscha,iemejia,wangyang0918,,,,,,,,,,,,,,,,,,,"iemejia commented on pull request #10991: [FLINK-15844][client] Expose JobWithJars.buildUserCodeClassLoader without Configuration to fix backwards compatibility
URL: https://github.com/apache/flink/pull/10991
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *Expose JobWithJars.buildUserCodeClassLoader without Configuration to fix backwards compatibility*
   
   
   ## Brief change log
   
   ## Verifying this change
   
   Check API of previous version
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   Yes
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   It worked this way in previous versions so 'kind of'.
   
   This change added tests and can be verified as follows:
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes (it fixes it)
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Feb/20 14:11;githubbot;600","aljoscha commented on pull request #10991: [FLINK-15844][client] Expose JobWithJars.buildUserCodeClassLoader without Configuration to fix backwards compatibility
URL: https://github.com/apache/flink/pull/10991
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Feb/20 09:37;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 03 09:37:44 UTC 2020,,,,,,,,,,"0|z0b2yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/20 14:11;iemejia;I am not sure if there will be a 1.9.3 but good to fix if so.;;;","03/Feb/20 09:14;aljoscha;Is this a problem because Beam uses that class or because a 1.9.1 Flink program doesn't work with Flink 1.9.2? I'm asking because the class is not declared public so we don't try and keep it compatible. I'm not against re-adding the method, though, if it's necessary and possible.;;;","03/Feb/20 09:32;iemejia;It is a problem because Beam uses this. I was not aware that Flink compatibility was only for tagged as public classes but I suppose for such a low level thing as the runner we would be hardly respecting the use of only those.

The public API of this class was also broken in the past in version 1.8.3 and then fixed back again to its precedent form in 1.9.0, so Beam for the 1.8 line stayed in Flink 1.8.2, and probably do the same for the 1.9.x (stay in 1.9.1).

However we (Beam) will have probably to look for some changes in the future because that class is removed from Flink 1.10.x;;;","03/Feb/20 09:34;aljoscha;I was going to mention that it was removed, yes. 😅;;;","03/Feb/20 09:37;aljoscha;release-1.9: e47b15a6ead9da2fd12e2d80fd4befd0915b1ddf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building Stateful Functions with JDK 11 fails due to outdated Spotbugs version,FLINK-15842,13282826,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tzulitai,tzulitai,tzulitai,02/Feb/20 09:42,04/Feb/20 04:04,13/Jul/23 08:07,04/Feb/20 04:04,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"When building with JDK 11, the build fails with this:
{code}
 [java] WARNING: An illegal reflective access operation has occurred
 [java] WARNING: Illegal reflective access by org.dom4j.io.SAXContentHandler (file:/home/shannon/ws/javaxmail-spotbugs/.m2/repository/dom4j/dom4j/1.6.1/dom4j-1.6.1.jar) to method com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser$LocatorProxy.getEncoding()
 [java] WARNING: Please consider reporting this to the maintainers of org.dom4j.io.SAXContentHandler
 [java] WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
 [java] WARNING: All illegal access operations will be denied in future versions
{code}

According to https://github.com/spotbugs/spotbugs/issues/499, this is because the current Spotbugs version we use (3.1.1) uses an outdated Dom4j version.

Upgrading to the latest Spotbugs stable version (3.1.12) fixes this issue.",,tzulitai,,,,,,,,,,,,,,,,,,,,,"tzulitai commented on pull request #8: [FLINK-15842] Upgrade Spotbugs to version 3.1.12
URL: https://github.com/apache/flink-statefun/pull/8
 
 
   Spotbugs needs to be upgraded because the old version we use uses an outdated Dom4J version that performs illegal reflective access operations. This prevents the project to be built using JDK 11.
   
   Can be verified by building the project with JDK 11.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Feb/20 04:23;githubbot;600","tzulitai commented on pull request #8: [FLINK-15842] Upgrade Spotbugs to version 3.1.12
URL: https://github.com/apache/flink-statefun/pull/8
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Feb/20 02:40;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,FLINK-15851,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 04 04:04:42 UTC 2020,,,,,,,,,,"0|z0b2vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/20 04:04;tzulitai;FIxed in {{master}} via 801d1b1a1306cce7e36e0d930f42f686bae2aad7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimeWindow.intersects return true for consecutive windows,FLINK-15841,13282778,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,joern,joern,joern,01/Feb/20 17:01,13/Apr/21 20:41,13/Jul/23 08:07,12/May/20 14:32,,,,,,1.11.0,,,,API / DataStream,,,,,0,pull-request-available,,,,"The TimeWindow JavaDoc explains that the start index is inclusive and the end index is exclusive, therefore two windows T0 to T1 and T1 to T2 are next to each other without overlapping.

To fix this the intersects comparison should be changed to: {{this.start < other.end && this.end > other.start}}

Also a test should be added to verify the methods works correctly.",,aljoscha,joern,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 14:32:20 UTC 2020,,,,,,,,,,"0|z0b2kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/20 17:02;joern;I would like to contribute a test case and the fix to TimeWindow.intersects.;;;","03/Feb/20 10:51;aljoscha;Did you try and do the change and check if all the tests still pass. If the behaviour of {{intersect()}} has been wrong for so long now, maybe some code relies on this quirk.;;;","03/Feb/20 19:31;joern;Just tried it now, there are many test failures after the change.

I could try to work through them. Is there an interest to get this fixed?;;;","04/Feb/20 08:05;aljoscha;Which tests are failing? I will try and take a look.

I think failing tests would indicate that we shouldn't change the behaviour now, even though it's wrong according to the Javadoc. Mostly because of what I mentioned above about things having been like this for so long.;;;","04/Feb/20 08:25;joern;A new method with the correct behavior could be added and then intersects could be deprecated. Flink probably has also code which uses the method as specified in the JavaDoc.

In flink-streaming-java these tests failed:
 {{[INFO] Results:
 [INFO] 
 [ERROR] Failures: 
 [ERROR] StreamSourceOperatorWatermarksTest.testNoMaxWatermarkOnAsyncCancel:127
 [ERROR] DynamicEventTimeSessionWindowsTest.testMergeConsecutiveWindows:121 
 Wanted but not invoked:
 mergeCallback.merge(
 iterable over [<TimeWindow

{start=0, end=1}>, <TimeWindow\{start=1, end=2}>, <TimeWindow\{start=2, end=3}>] in any order,
 TimeWindow\{start=0, end=3}
 );
 -> at org.apache.flink.streaming.runtime.operators.windowing.DynamicEventTimeSessionWindowsTest.testMergeConsecutiveWindows(DynamicEventTimeSessionWindowsTest.java:121)
 Actually, there were zero interactions with this mock.
 
 [ERROR] DynamicProcessingTimeSessionWindowsTest.testMergeConsecutiveWindows:126 
 Wanted but not invoked:
 mergeCallback.merge(
 iterable over [<TimeWindow\{start=0, end=1}

>, <TimeWindow

{start=1, end=2}>, <TimeWindow\{start=2, end=3}>] in any order,
 TimeWindow\{start=0, end=3}
 );
 -> at org.apache.flink.streaming.runtime.operators.windowing.DynamicProcessingTimeSessionWindowsTest.testMergeConsecutiveWindows(DynamicProcessingTimeSessionWindowsTest.java:126)
 Actually, there were zero interactions with this mock.
 
 [ERROR] EventTimeSessionWindowsTest.testMergeConsecutiveWindows:114 
 Wanted but not invoked:
 mergeCallback.merge(
 iterable over [<TimeWindow\{start=0, end=1}>, <TimeWindow\{start=1, end=2}

>, <TimeWindow

{start=2, end=3}>] in any order,
 TimeWindow\{start=0, end=3}
 );
 -> at org.apache.flink.streaming.runtime.operators.windowing.EventTimeSessionWindowsTest.testMergeConsecutiveWindows(EventTimeSessionWindowsTest.java:114)
 Actually, there were zero interactions with this mock.
 
 [ERROR] MergingWindowSetTest.testLateMerging:230 expected:<TimeWindow\{start=5, end=13}> but was:<TimeWindow\{start=8, end=10}>
 [ERROR] ProcessingTimeSessionWindowsTest.testMergeConsecutiveWindows:118 
 Wanted but not invoked:
 mergeCallback.merge(
 iterable over [<TimeWindow\{start=0, end=1}>, <TimeWindow\{start=1, end=2}>, <TimeWindow\{start=2, end=3}

>] in any order,
 TimeWindow

{start=0, end=3}

);
 -> at org.apache.flink.streaming.runtime.operators.windowing.ProcessingTimeSessionWindowsTest.testMergeConsecutiveWindows(ProcessingTimeSessionWindowsTest.java:118)
 Actually, there were zero interactions with this mock.
 }};;;","04/Feb/20 15:04;aljoscha;I think all code/tests expects the current behaviour, the expectation of the session/window merging code is that two windows that have zero gap, i.e. {{(t1, t2)}} and {{(t2, t3)}} will be merged because there is no gap in between. Therefore I think we shouldn't not (cannot) change the behaviour. We can extend the Javadoc of {{intersect()}} to better describe the behaviour.

The class-level Javadoc is still correct, in my opinion. Consider two windows {{w1 = (0, 2)}} and {{w2 = (2, 4)}}. They might have these elements in them:
{code}
// value, timestamp
0, 0 // in w1
1, 1 // in w1
2, 2 // in w2
3, 3 // in w2
{code}

I.e. {{w1}} doesn't contain elements of timestamp {{2}}, because the end is exclusive. Still, these are two windows that should be merged into one session, i.e. they should ""intersect"".
;;;","04/Feb/20 19:24;joern;Happy to then contribute the addition to the Javadoc and it would probably still be good to add a test case for the current behavior.;;;","05/Feb/20 08:01;aljoscha;Sure yes, that would be good. 👌

 ;;;","12/May/20 14:32;aljoscha;master: b9eee7816dda4b29a7f4261a7f46106113388799;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException is thrown when use tEnv.from for temp/catalog table under Blink planner,FLINK-15840,13282731,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,sunjincheng121,sunjincheng121,01/Feb/20 04:16,03/Feb/20 13:59,13/Jul/23 08:07,03/Feb/20 13:59,,,,,,1.10.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"ClassCastException is thrown when use ConnectorDescriptor under Blink planner.
The exception can be reproduced by the following test:
{code:java}
@Test
def testDescriptor(): Unit = {
 this.env = StreamExecutionEnvironment.getExecutionEnvironment
 val setting = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()
 this.tEnv = StreamTableEnvironment.create(env, setting)

 tEnv.connect(new FileSystem().path(""/tmp/input""))
 .withFormat(new OldCsv().field(""word"", DataTypes.STRING()))
 .withSchema(new Schema().field(""word"", DataTypes.STRING()))
 .createTemporaryTable(""sourceTable"")

 val sink = new TestingAppendSink
 tEnv.from(""sourceTable"").toAppendStream[Row].addSink(sink)
 env.execute()
}
{code}

Exceptions:
{code:java}
java.lang.ClassCastException: org.apache.calcite.plan.ViewExpanders$2 cannot be cast to org.apache.flink.table.planner.calcite.FlinkToRelContext

 at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:89)
 at org.apache.calcite.rel.rules.TableScanRule.onMatch(TableScanRule.java:55)
 at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319)
 at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:560)
 at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:419)
 at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:256)
 at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
 at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:215)
 at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:202)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60)
 at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
 at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
 at scala.collection.Iterator$class.foreach(Iterator.scala:891)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
 at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
 at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
 at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
 at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55)
 at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
 at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
 at scala.collection.immutable.Range.foreach(Range.scala:160)
 at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
 at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
 at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
 at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
 at scala.collection.Iterator$class.foreach(Iterator.scala:891)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
 at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
 at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
 at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
 at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
 at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:167)
 at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:89)
 at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
 at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:248)
 at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:151)
 at org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.scala:210)
 at org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.scala:107)
 at org.apache.flink.table.api.scala.TableConversions.toAppendStream(TableConversions.scala:101)
 at org.apache.flink.table.planner.runtime.stream.table.CalcITCase.testDescriptor(CalcITCase.scala:541)
{code}

It seems we should not cast `context` to `FlinkToRelContext` directly as it could also be an anonymous classes in `org.apache.calcite.plan.ViewExpanders`.

What do you think? ",,jark,liyu,lzljs3620320,sunjincheng121,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #10989: [FLINK-15840][table-planner-blink] ClassCastException is thrown when use tEnv.from for temp/catalog table
URL: https://github.com/apache/flink/pull/10989
 
 
   
   ## What is the purpose of the change
   
   `TableEnvironment.from/scan(string path)` cannot be used for all temporaryTable and CatalogTable (not DataStreamTable and ConnectorCatalogTable). Of course, it can be bypassed by `TableEnvironment.sqlQuery(""select * from t"")`, but `from/scan` are very important api of TableEnvironment and pure TableApi can't be used seriously.
   
   ## Brief change log
   
   The problem is that CatalogSourceTable.toRel wants to get the translator of the compute column. At present, CatalogSourceTable.toRe has two places to call:
   1. The parser stage, which passes the correct compute column translator.
   2. In the rule optimization stage, the correct compute column translator is not passed in the TableScanRule, so an error is reported.
   
   There are two solutions:
   1. Don't use ToRelContext to transfer the translators of compute column. Use FlinkContext to transfer so that we can get the correct translators of compute columns at any stage.
   2. In CatalogSourceTable.toRel, when there is a compute column and the compute column translator cannot be obtained, an error is reported, and other cases pass normally. The disadvantage of this is that it is currently unable to support compute columns on TableApi.
   
   Considering:
   - No plan to support compute column on TableApi.
   - Solution #1 changed too much
   - Solution #1 is also an intermediate version. In the next version, it is considered to separate the compute column translation and complete translation in the parser stage.
   We consider using solution #2.
   
   Another bug is CatalogSourceTable need override explainSourceAsString. Otherwise hep rule optimizer can not distinguish CatalogSourceTable and TableSourceTable.
   
   ## Verifying this change
   
   `TableScanTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Feb/20 08:21;githubbot;600","wuchong commented on pull request #10989: [FLINK-15840][table-planner-blink] ClassCastException is thrown when use tEnv.from for temp/catalog table
URL: https://github.com/apache/flink/pull/10989
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Feb/20 11:11;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 03 13:59:52 UTC 2020,,,,,,,,,,"0|z0b2ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/20 05:32;lzljs3620320;Thanks [~sunjincheng121] for reporting.

This ticket should be a blocker, lead to ""TableEnvironment.from/scan(string path)"" cannot be used for all temporaryTable and CatalogTable (not DataStreamTable and ConnectorCatalogTable). Of course, it can be bypassed by ""TableEnvironment.sqlQuery(""select * from t"")"", but ""from/scan"" are very important api of TableEnvironment and pure TableApi can't be used seriously.;;;","01/Feb/20 05:38;sunjincheng121;Thanks for the double check. do you want help to fix it? I would like to assign this issue to you? [~lzljs3620320];;;","01/Feb/20 05:39;lzljs3620320;Yes, please assign to me.;;;","01/Feb/20 06:36;lzljs3620320;CC: [~danny0405] [~jark] [~ykt836];;;","01/Feb/20 06:48;lzljs3620320;The problem is that CatalogSourceTable.toRel wants to get the translator of the compute column. At present, CatalogSourceTable.toRe has two places to call:
 # The parser stage, which passes the correct compute column translator.
 # In the rule optimization stage, the correct compute column translator is not passed in the TableScanRule, so an error is reported.

There are two solutions:
 # Don't use ToRelContext to transfer the translators of compute column. Use FlinkContext to transfer so that we can get the correct translators of compute columns at any stage.
 # In CatalogSourceTable.toRel, when there is a compute column and the compute column translator cannot be obtained, an error is reported, and other cases pass normally. The disadvantage of this is that it is currently unable to support compute columns on TableApi.

Considering:
 * No plan to support compute column on TableApi.
 * Solution #1 changed too much
 * Solution #1 is also an intermediate version. In the next version, it is considered to separate the compute column translation and complete translation in the parser stage.

We consider using solution #2.;;;","01/Feb/20 08:11;lzljs3620320;Another bug is {{CatalogSourceTable}} need override {{explainSourceAsString}}. Otherwise hep rule optimizer can not distinguish {{CatalogSourceTable}} and {{TableSourceTable}}.;;;","03/Feb/20 13:59;jark;Fixed in 1.11.0: 183c8d143bc3bb5a31747417355690adaba3d226
1.10.0: 9a8f15cba18d431550a2ec587d69b948b4bedabd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revisit Stateful Functions KafkaIngressBuilder properties resolution logic,FLINK-15839,13282659,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tzulitai,tzulitai,tzulitai,31/Jan/20 16:59,04/Feb/20 04:06,13/Jul/23 08:07,04/Feb/20 04:06,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"Currently, the properties resolution logic in {{KafkaIngressBuilder}} is a bit inconsistent for different configurations, and some actually incorrect.
The problem is around the fact that we allow users to directly pass in {{Properties}} to configure the Kafka client, but also support named methods to set some important configs like Kafka address / auto offset reset position.

For example, we always overwrite {{auto.offset.reset}} set in the properties with the {{autoOffsetResetPosition}} value in the builder. This is correct when the user had actually passed in a value via the named method {{withAutoOffsetResetPosition}}, but incorrect otherwise.
The same goes for the Kafka address configuration.

This should be revisited, so that we have a common strategy with dealing with named configurations v.s. properties, with an end goal that:

* Configs passed via named methods should always overwrite the value set via properties
* Any default values for named configuration methods should be defined in the builder
* If no config was passed via its named method, then we use the default value (if any) to overwrite the properties IFF the user also did not provide a value for it there.",,tzulitai,,,,,,,,,,,,,,,,,,,,,"tzulitai commented on pull request #7: [FLINK-15839] [kafka-io] Fix Stateful Functions KafkaIngressBuilder properties resolution logic
URL: https://github.com/apache/flink-statefun/pull/7
 
 
   This PR revisits how we resolve configs for the `KafkaIngressBuilder`, given that we have:
   - Named configuration methods for important settings
   - Some of which have default values, some don't
   - Some eventually resolve as a Kafka client property, some are not
   - At the same time, also accept a `Properties` which may also explicitly define values for the above.
   
   The end result of the changes in this PR is as follows:
   - Configs passed via named methods should always overwrite the value set via passed in `Properties`
   - Any default values for named configuration methods should be defined in the builder
   - If no config was passed via its named method, then we use the default value (if any) to overwrite the properties IFF the user also did not provide a value for it there.
   
   ---
   
   The individual commits and their messages / descriptions should be sufficient in explaining what was changed to achieve this.
   
   ---
   
   New tests in `KafkaIngressBuilderTest` was added to verify the changes.
   Please also see there to get an overview of the expected behaviour.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Feb/20 07:29;githubbot;600","tzulitai commented on pull request #7: [FLINK-15839] [kafka-io] Fix Stateful Functions KafkaIngressBuilder properties resolution logic
URL: https://github.com/apache/flink-statefun/pull/7
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Feb/20 03:59;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 04 04:06:07 UTC 2020,,,,,,,,,,"0|z0b1ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/20 04:06;tzulitai;Fixed in {{master}} via 7fc005555bff7642d50f4672fe0bb18ffb9cb6b1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dangling CountDownLatch.await(timeout),FLINK-15838,13282635,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ayushtkn,leventov,leventov,31/Jan/20 14:55,04/Mar/20 17:47,13/Jul/23 08:07,04/Mar/20 17:47,,,,,,1.10.1,1.11.0,,,Tests,,,,,0,pull-request-available,,,,"There are 16 occurrences in the codebase (all in test code) when the result of CountDownLatch.await(timeout, TimeUnit) is not checked. It's like not checking the result of File.delete(). The common fix is to wrap CDL.await() call into assertTrue().

All 16 places could be found using the following structural search in IntelliJ:

$x$.await($y$, $z$);

With ""CountDownLatch"" type constraint on the $x$ variable.",,ayushtkn,azagrebin,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,"ayushtkn commented on pull request #11005: [FLINK-15838] Dangling CountDownLatch.await(timeout)
URL: https://github.com/apache/flink/pull/11005
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Feb/20 19:28;githubbot;600","tillrohrmann commented on pull request #11005: [FLINK-15838] Dangling CountDownLatch.await(timeout)
URL: https://github.com/apache/flink/pull/11005
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/20 17:46;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/20 17:12;ayushtkn;image-2020-02-06-22-42-51-866.png;https://issues.apache.org/jira/secure/attachment/12992790/image-2020-02-06-22-42-51-866.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 17:47:00 UTC 2020,,,,,,,,,,"0|z0b1p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/20 14:57;leventov;See also [https://github.com/code-review-checklists/java-concurrency#check-await], [https://youtrack.jetbrains.com/issue/IDEA-231640];;;","02/Feb/20 14:22;ayushtkn;Is there anyway it can lead to a test failure? Or wrong test results?;;;","02/Feb/20 14:58;leventov;A typical outcome is a test not really testing production code, though falsely reporting ""coverage"". Such tests will be green regardless of whether the production logic is correct or not.;;;","02/Feb/20 17:22;ayushtkn;Well, technically speaking, it could be like that, but as far as present usages are concerned, seems won't be that critical, but for sure can be done for sanity.

If people agree, we can do that for sure, asserting the output shouldn't hurt;;;","03/Feb/20 17:51;azagrebin;Thanks for reporting this [~leventov]

Indeed, we should try to harden the tests. Do you want to work on this, [~leventov]?;;;","03/Feb/20 18:44;leventov;No;;;","03/Feb/20 18:48;ayushtkn;if no issues I can work on this.;;;","03/Feb/20 18:55;azagrebin;I have assigned you [~ayushsaxena], please, move the issue in progress if you start working on this;;;","03/Feb/20 19:30;ayushtkn;Thanx [~azagrebin], I have raised PR.

Seems the Jira assigned is to a different id, so can't change progress, my Jira id is ayushtkn, can you please change the assignee;;;","06/Feb/20 16:55;ayushtkn;{quote}please, move the issue in progress if you start working on this
{quote}
Seems I don't have the permissions to change, [~azagrebin] can you help?;;;","06/Feb/20 17:07;azagrebin;There should be 'start progress' button visible for you at the top of the page,
is it not available for you?;;;","06/Feb/20 17:12;ayushtkn;Nopes.

!image-2020-02-06-22-42-51-866.png!;;;","07/Feb/20 12:51;rmetzger;I don't know what's going on with the permissions.
I've set the status to in progress now, and assigned you again.;;;","04/Mar/20 17:47;trohrmann;Fixed via

master: b098ce505176720ba37da8f6d6c23096b1d3a260
1.10.1: d710a8cc995737f1fcab2912312834a3b799c288;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-kubernetes module does not have correct NOTICE file,FLINK-15837,13282625,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,trohrmann,trohrmann,31/Jan/20 14:28,03/Feb/20 14:36,13/Jul/23 08:07,03/Feb/20 10:56,1.10.0,,,,,1.10.0,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"The new module {{flink-kubernetes}} does not have the correct NOTICE file according to [Flink's license guide|https://cwiki.apache.org/confluence/display/FLINK/Licensing]. The problem is that we bundle {{dk.brics.automaton:automaton}} which has a BSD clause 2 license.",,liyu,trohrmann,,,,,,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #10986: [FLINK-15837] Add proper NOTICE file to flink-kubernetes
URL: https://github.com/apache/flink/pull/10986
 
 
   ## What is the purpose of the change
   
   Add proper NOTICE file to `flink-kubernetes` based on the shaded dependencies. 
   
   cc @GJL 
   
   ## Verifying this change
   
   - Verify via `mvn project-info-reports:dependencies` and `pom.xml` to see what is being shaded.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jan/20 15:08;githubbot;600","tillrohrmann commented on pull request #10986: [FLINK-15837] Add proper NOTICE file to flink-kubernetes
URL: https://github.com/apache/flink/pull/10986
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Feb/20 10:55;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 03 10:56:24 UTC 2020,,,,,,,,,,"0|z0b1mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/20 10:56;trohrmann;Fixed via

master: 36a0e4ca86fa08b6bc495edcc2addf836d7a9090
1.10.0: c2157a69f66c267b577b5b51b9277b0172a312d7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OSS filesystems bundles entire hadoop-common,FLINK-15835,13282610,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,31/Jan/20 13:22,06/Apr/20 09:24,13/Jul/23 08:07,19/Feb/20 09:58,1.9.0,,,,,1.11.0,,,,FileSystems,,,,,0,pull-request-available,,,,"The {{flink-oss-fs-hadoop}} filesystem has a dependency on {{flink-fs-hadoop-shaded}} (a trimmed down version of hadoop-common) and {{hadoop-aliyun}}, the latter also having a dependency on {{hadoop-common}}.

Since {{flink-fs-hadoop-shaded}} bundles everything and creates a dependency-reduced pom (that as a result contains no dependencies) the transitive hadoop-common dependency pulled in from {{hadoop-aliyun}} is not being altered in any way, and is hence included in it's entirety in the jar.

We should add an explicit exclusion for the transitive hadoop-common dependency.",,azagrebin,wangyang0918,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #10984: [FLINK-15835][build][oss] Exclude hadoop-common
URL: https://github.com/apache/flink/pull/10984
 
 
   Adds an explicit exclusion for `hadoop-common` to `hadoop-aliyun`, since a trimmed-down version of this dependency is already supplied by `flink-fs-hadoop-shaded`.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jan/20 13:25;githubbot;600","zentol commented on pull request #10984: [FLINK-15835][build][oss] Exclude hadoop-common
URL: https://github.com/apache/flink/pull/10984
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Feb/20 09:58;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,FLINK-16590,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 09:58:22 UTC 2020,,,,,,,,,,"0|z0b1jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/20 20:34;azagrebin;Is it also to be merged into 1.9 and 1.10?;;;","17/Feb/20 22:09;chesnay;This is a fairly invasive dependency change that I would only merge for 1.11 .;;;","19/Feb/20 09:58;chesnay;master: abf8010721a0614354e19ba84ce64f4a1a84de30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HistoryServer archiving is done in Dispatcher main thread,FLINK-15812,13282354,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,chesnay,chesnay,30/Jan/20 10:59,09/Apr/20 15:30,13/Jul/23 08:07,09/Apr/20 15:30,1.8.0,,,,,1.10.1,1.11.0,1.9.3,,Runtime / Coordination,,,,,0,pull-request-available,,,,{{Dispatcher#archiveExecutionGraph}} should call {{HistoryServerArchivist#archiveExecutionGraph}} asynchronously since the archiving may involve IO operations.,,kezhuw,trohrmann,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #11649: [FLINK-15812][runtime] Archive jobs asynchronously
URL: https://github.com/apache/flink/pull/11649
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Apr/20 20:26;githubbot;600","zentol commented on pull request #11677: [1.9][FLINK-15812][runtime] Archive jobs asynchronously 
URL: https://github.com/apache/flink/pull/11677
 
 
   Backport of #11649.
   
   Additionally required FLINK-14278 to have access to an Executor.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Apr/20 20:43;githubbot;600","zentol commented on pull request #11677: [1.9][FLINK-15812][runtime] Archive jobs asynchronously 
URL: https://github.com/apache/flink/pull/11677
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/20 06:34;githubbot;600",,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 15:30:40 UTC 2020,,,,,,,,,,"0|z0azyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/20 20:43;chesnay;master: 446dda14993161d96f6768438a5d226064dee871
1.10: 138da8220f47d966d9cc8c09a2a8301355bed8c9
1.9: bc86d198948f1503ca0eac6f457eff17b9671f51
;;;","09/Apr/20 15:27;trohrmann;Can this ticket be closed [~chesnay]?;;;","09/Apr/20 15:30;chesnay;yes, I forgot to close it after the 1.9 backport.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamSourceOperatorWatermarksTest.testNoMaxWatermarkOnAsyncCancel fails on Travis,FLINK-15811,13282353,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,chesnay,chesnay,30/Jan/20 10:55,10/Apr/20 08:39,13/Jul/23 08:07,05/Feb/20 16:34,1.10.0,1.11.0,,,,1.10.1,1.11.0,,,Runtime / Task,Tests,,,,0,pull-request-available,test-stability,,,"https://api.travis-ci.org/v3/job/643480766/log.txt

{code}
08:06:17.382 [ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.812 s <<< FAILURE! - in org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest
08:06:17.382 [ERROR] testNoMaxWatermarkOnAsyncCancel(org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest)  Time elapsed: 0.235 s  <<< FAILURE!
java.lang.AssertionError
	at org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest.testNoMaxWatermarkOnAsyncCancel(StreamSourceOperatorWatermarksTest.java:127)
{code}",,aljoscha,ayushtkn,liyu,pnowojski,rmetzger,roman,trohrmann,,,,,,,,,,,,,,,"rkhachatryan commented on pull request #11026: [FLINK-15811][task] report CancelTaskException from SourceStreamTask
URL: https://github.com/apache/flink/pull/11026
 
 
   ## What is the purpose of the change
   
   `SourceStreamTask` has it's own thread and on `cancel()` interrupts it.
   When the thread completes the failure if any is reported via mailbox to the upstream Task.
   
   So on cancel it reports InterruptedException.
   But the contract (or common approach) of `AbstractInvokable` is to throw CancelTaskException.
   
   In PR `SourceStreamTask` reports CancelTaskException if it was cancelled.
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as `StreamSourceOperatorWatermarksTest#testNoMaxWatermarkOnAsyncCancel`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 13:19;githubbot;600","pnowojski commented on pull request #11026: [FLINK-15811][task] report CancelTaskException from SourceStreamTask
URL: https://github.com/apache/flink/pull/11026
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 16:33;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 10 08:39:16 UTC 2020,,,,,,,,,,"0|z0azyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/20 15:35;ayushtkn;Seems like if the the cancel call reaches post the thread has completed the execution, and reached {{cleanUpInvoke}} the test tends to fail, since the exception won't be a {{CancellationException}} but a {{InterruptedException}};;;","05/Feb/20 08:40;aljoscha;This changed since the test was added. When you run the test in a loop on the commit that last touched the test no failure happens. When you run the test in a loop now, if fails almost immediately. [~pnowojski] I think something changed in the threading/cancelling behaviour between then and now, do you maybe have any idea?;;;","05/Feb/20 08:44;aljoscha;If the {{InfiniteSource}} that is used in the test is changed to swallow the {{InterruptedException}} the test passes, but before the task behaviour was to swallow any exceptions that happen during cancellation.;;;","05/Feb/20 09:10;aljoscha;This commit introduced the problem: https://github.com/apache/flink/commit/7c6ecc498b85d8eb1c550b468fab5b8a07b9ddb2;;;","05/Feb/20 09:50;trohrmann;Another instance: https://api.travis-ci.org/v3/job/645960125/log.txt;;;","05/Feb/20 09:52;trohrmann;Another instance: https://api.travis-ci.org/v3/job/646291876/log.txt;;;","05/Feb/20 16:34;pnowojski;Merged to master as 7d06b11;;;","09/Apr/20 13:10;liyu;The issue also happens in the latest release-1.10 nightly build: https://api.travis-ci.org/v3/job/672627753/log.txt

Will backport the fix to release-1.10 once [travis|https://travis-ci.org/github/carp84/flink/builds/672962999] is green.;;;","09/Apr/20 15:47;liyu;Merged into release-1.10 via: 374e5940d0bf1fe1d6e948eaa435c809d9bdf144;;;","10/Apr/20 08:39;pnowojski;Thanks [~liyu];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client end-to-end test for Kafka 0.10 nightly run hung on travis,FLINK-15778,13281874,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,liyu,liyu,28/Jan/20 08:26,15/Sep/20 15:31,13/Jul/23 08:07,15/Sep/20 15:31,1.10.0,1.11.0,,,,1.12.0,,,,Connectors / Kafka,,,,,0,test-stability,,,,"The ""SQL Client end-to-end test for Kafka 0.10"" end-to-end test hung on travis:
{noformat}
Waiting for broker...
Waiting for broker...


The job exceeded the maximum time limit for jobs, and has been terminated.
{noformat}

https://api.travis-ci.org/v3/job/642477196/log.txt",,aljoscha,becket_qin,dian.fu,godfreyhe,liyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 15:31:56 UTC 2020,,,,,,,,,,"0|z0ax00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/20 06:41;liyu;Another instance observed in release-1.10 cron build: https://api.travis-ci.org/v3/job/659858627/log.txt;;;","29/Apr/20 00:19;becket_qin;From the log it is hard to tell what happened. Some guess would be the broker start hit an exception and exited. This could be caused by issues such as the binding port is already taken. 

To know the exact cause of the failure, we need to look at the broker log. The broker should print the log to stdout by default, but for some reason the broker log was not available in the testing log. I saw some Confluent schema registry log printed to the testing log in other tests, but the broker logs are somehow missing. [~rmetzger] do you happen to know what might cause the log missing?;;;","22/May/20 01:17;becket_qin;This issue might be related to FLINK-14592 but I am not sure. Let's see if this still happen after FLINK-14592 is fixed.;;;","15/Sep/20 15:31;aljoscha;Fixed via FLINK-19152, which removes the Kafka 0.10 connector.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shaded Hadoop S3A with credentials provider end-to-end test fails on travis,FLINK-15772,13281576,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,liyu,liyu,26/Jan/20 07:14,09/Apr/20 12:56,13/Jul/23 08:07,06/Apr/20 10:48,1.10.0,1.11.0,,,,1.10.1,1.11.0,,,Connectors / FileSystem,Tests,,,,0,test-stability,,,,"Shaded Hadoop S3A with credentials provider end-to-end test fails on travis with below error:
{code}
Job with JobID 048b4651c0ba858b926aeb36f5315058 has finished.
Job Runtime: 6016 ms

sort: cannot read: '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-17605057822/temp/test_batch_wordcount-2abf3dbf-b4ba-4d3a-a43b-c43e710eb439*': No such file or directory
FAIL WordCount (hadoop_with_provider): Output hash mismatch.  Got d41d8cd98f00b204e9800998ecf8427e, expected 72a690412be8928ba239c2da967328a5.
head hexdump of actual:
head: cannot open '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-17605057822/temp/test_batch_wordcount-2abf3dbf-b4ba-4d3a-a43b-c43e710eb439*' for reading: No such file or directory
ed2bf7571ec8ab184b7316809da0b2facb9b367a7c7f0f1bdaac6dd5e6f107ae
ed2bf7571ec8ab184b7316809da0b2facb9b367a7c7f0f1bdaac6dd5e6f107ae
[FAIL] Test script contains errors.
Checking for errors...
No errors in log files.
Checking for exceptions...
No exceptions in log files.
Checking for non-empty .out files...
No non-empty .out files.

[FAIL] 'Shaded Hadoop S3A with credentials provider end-to-end test' failed after 0 minutes and 20 seconds! Test exited with exit code 1
{code}

https://api.travis-ci.org/v3/job/641444512/log.txt",,aljoscha,klion26,liyu,rmetzger,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17010,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 12:56:08 UTC 2020,,,,,,,,,,"0|z0av5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/20 07:44;pnowojski;Another instance: https://api.travis-ci.org/v3/job/663331003/log.txt , after next occurrence please bump the priority.;;;","20/Mar/20 09:16;pnowojski;Another instance https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6426&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=679407b1-ea2c-5965-2c8d-1467777fff88;;;","25/Mar/20 10:28;zjwang;Another instance [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6606&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5];;;","26/Mar/20 16:22;liyu;Another instance in release-1.10 crone build: https://api.travis-ci.org/v3/job/666898964/log.txt;;;","01/Apr/20 13:26;aljoscha;I added debug output in 4412c6ef4fbb208cba6e44e7a90fdfe1e07bf727. Let's see if this shows whether downloading from S3 works.;;;","04/Apr/20 08:46;rmetzger;I guess here you have your debug output:
{code}
Job with JobID 022d2eb602cba43a89cae0e99b55b8ab has finished.
Job Runtime: 5050 ms
Error executing aws command: s3 cp --quiet s3://[secure]/temp/test_batch_wordcount-057d4df3-e1f0-461d-b57b-80d9fe3c14be /hostdir//temp-test-directory-25704116629/temp/test_batch_wordcount-057d4df3-e1f0-461d-b57b-80d9fe3c14be --exclude '*' --include '*/false[!/]*'
sort: cannot read: '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-25704116629/temp/test_batch_wordcount-057d4df3-e1f0-461d-b57b-80d9fe3c14be*': No such file or directory
FAIL WordCount (hadoop): Output hash mismatch.  Got d41d8cd98f00b204e9800998ecf8427e, expected 72a690412be8928ba239c2da967328a5.
head hexdump of actual:
head: cannot open '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-25704116629/temp/test_batch_wordcount-057d4df3-e1f0-461d-b57b-80d9fe3c14be*' for reading: No such file or directory
5dbc1b27a868582f4415cb51c107b8be4ad2c6d79da43075a700b3e8fb423829
5dbc1b27a868582f4415cb51c107b8be4ad2c6d79da43075a700b3e8fb423829
[FAIL] Test script contains errors.
Checking for errors...
No errors in log files.
Checking for exceptions...
No exceptions in log files.
Checking for non-empty .out files...
No non-empty .out files.
[FAIL] 'Shaded Hadoop S3A end-to-end test' failed after 0 minutes and 27 seconds! Test exited with exit code 1
{code}
https://travis-ci.org/github/apache/flink/jobs/670250246?utm_medium=notification&utm_source=slack;;;","06/Apr/20 10:48;aljoscha;I added a (tentative) fix in 1a06dd82a06d2fb80b4a878e1bc90eda72d1c307. My suspicion is that S3 is flaky so I added retries to the file fetching. Please re-open if you see this again.;;;","07/Apr/20 04:41;rmetzger;Did the changes in 1a06dd82a06d2fb80b4a878e1bc90eda72d1c307 go through a PR?

I wonder if this is related: https://issues.apache.org/jira/browse/FLINK-17010
Since your commit, ""Streaming File Sink s3 end-to-end"" has failed twice, and has never passed.;;;","07/Apr/20 08:53;rmetzger;Sorry for accusing you of not having PR tested 1a06dd82a06d2fb80b4a878e1bc90eda72d1c307. Even a pull request would not have revealed the issue, since the test requires S3 credentials to be set. I've triggered a CI run to see if reverting the commit fixes FLINK-17010.;;;","07/Apr/20 16:53;rmetzger;Another case: https://travis-ci.org/github/apache/flink/jobs/672041886?utm_medium=notification&utm_source=slack;;;","08/Apr/20 09:31;aljoscha;{{release-1.10}} doesn't have the fix. I will also push it there now.

 ;;;","08/Apr/20 11:00;aljoscha;Fix on release-1.10: 0ad2f0634e81921472ebb506f9d826c7ad6f69a3;;;","09/Apr/20 12:56;liyu;Thanks for also taking care of release-1.10 branch [~aljoscha]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLClientKafkaITCase.testKafka failed on Travis,FLINK-15771,13281554,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,twalthr,trohrmann,trohrmann,25/Jan/20 17:24,29/May/20 06:21,13/Jul/23 08:07,29/May/20 06:20,1.10.0,,,,,1.11.0,,,,Connectors / Kafka,Table SQL / Client,Tests,,,0,pull-request-available,test-stability,,,"The end-to-end test {{SQLClientKafkaITCase.testKafka}} failed with

{code}
22:09:36.957 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 129.244 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientKafkaITCase
22:09:36.958 [ERROR] testKafka[2: kafka-version:universal kafka-sql-version:.*kafka.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase)  Time elapsed: 45.954 s  <<< FAILURE!
org.junit.ComparisonFailure: 
expected:<...-03-12 09:00:00.000,[Bob,This was another warning.,1,Success constant folding.
2018-03-12 09:00:00.000,Steve,This was another info.,2],Success constant fo...> but was:<...-03-12 09:00:00.000,[Steve,This was another info.,2,Success constant folding.
2018-03-12 09:00:00.000,Bob,This was another warning.,1],Success constant fo...>
	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.checkCsvResultFile(SQLClientKafkaITCase.java:226)
	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:154)

22:09:37.287 [INFO] 
22:09:37.288 [INFO] Results:
22:09:37.288 [INFO] 
22:09:37.288 [ERROR] Failures: 
22:09:37.288 [ERROR]   SQLClientKafkaITCase.testKafka:154->checkCsvResultFile:226 expected:<...-03-12 09:00:00.000,[Bob,This was another warning.,1,Success constant folding.
{code}

https://api.travis-ci.org/v3/job/641346345/log.txt",,aljoscha,dwysakowicz,jark,rmetzger,trohrmann,twalthr,,,,,,,,,,,,,,,,"twalthr commented on pull request #11438: [FLINK-15771][FLINK-16583][tests] Fix issues around SQLClientKafkaITCase
URL: https://github.com/apache/flink/pull/11438
 
 
   ## What is the purpose of the change
   
   This fixes a couple of issues related to the SQL Client and the SQLClientKafkaITCase.
   
   In particular, it fixes a classloading bug that was introduced in the last release while updating the code for the new `Pipeline` abstractions,
   
   ## Brief change log
   
   See commit messages.
   
   ## Verifying this change
   
   
   This change is already covered by existing tests, such as `SQLClientKafkaITCase`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/20 11:44;githubbot;600","twalthr commented on pull request #11438: [FLINK-15771][FLINK-16583][tests] Fix issues around SQLClientKafkaITCase
URL: https://github.com/apache/flink/pull/11438
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Mar/20 10:27;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,FLINK-16514,,,,,,,,,,,,,,FLINK-16156,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 06:21:13 UTC 2020,,,,,,,,,,"0|z0av0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/20 18:33;chesnay;A bit more readable error-message:
{code}
expected:<
...-03-12 09:00:00.000,[Bob,This was another warning.,1,Success constant folding.
2018-03-12 09:00:00.000,Steve,This was another info.,2],Success constant fo...>
but was:<
...-03-12 09:00:00.000,[Steve,This was another info.,2,Success constant folding.
2018-03-12 09:00:00.000,Bob,This was another warning.,1],Success constant fo...>
{code}

So that order is incorrect. I don't know whether the order is _supposed_ to be fixed (nothing in the query seems to guarantee that); maybe adding a sort statement to {{insertIntoCsvSinkTable}} is enough.;;;","27/Jan/20 15:14;trohrmann;[~openinx] could you help with fixing this test?;;;","28/Jan/20 12:26;dwysakowicz;I am not sure what can we do about it :(

I can't reproduce it locally. I run the test ~100 times and it succeeds all the time. It's true we are not strictly enforcing the order, but we are effectively running the test with parallelism of 1, which should produce deterministic results. We do group by window, but it should nevertheless output in order of incoming keys. Unless I am missing something here [~jark]

I agree it might make sense to add some sorting at the end. The problem is we cannot add the sorting in the query because it is a streaming query without time attributes. This would mean we would have to apply a global sorting on a stream which is not possible. We would have to rework the whole structure of the test.

I am also not 100% sure what do we actually want to test with this e2e test. If we just want to test that dependencies, classloading issues with different connectors. We could probably simplify the test. ;;;","03/Feb/20 03:52;jark;I guess the problem might be in the group window timer. The timers are ordered by the timestamp but not the key and namespace. However, ""Steve"" and ""Bob"" have the same timestamp but different keys. I guess {{InternalPriorityQueue.peak()}}  doesn't guarantee the insertion order, but only the timestamp order (is that right [~trohrmann]?). Hence, the result of the group window is not deterministic. 

I think we can sort the result before checking which is also the paradigm in our Table IT cases. What do you think [~dwysakowicz]?;;;","03/Feb/20 07:38;dwysakowicz;We came to the same conclusion with [~aljoscha].

Personally I don't like the automatic, generic sorting in our IT tests. I think it might be quite surprising at times, as it was surprising for me in this case. There are cases when I think it makes sense to assert also on the order. I'd be inclined to rather rework this IT test so that it produces deterministic results. I would start though with collecting a list of features that it tries to test. Right now it's unclear for me what is actually being testes and what is asserted there by coincidence.;;;","16/Mar/20 11:42;twalthr;How about we avoid using the same timestamp in this test? And check if this solve the problem long-term? We can still decide on sorting the results later.;;;","16/Mar/20 14:29;jark;Sounds good to me. I think the purpose of e2e tests are mainly for packaging and classloading, not the result order.
;;;","19/Mar/20 10:40;twalthr;Fixed in 1.11.0: 6269a2a1f6b604d2e850b7cf5b0f0050085798b7;;;","28/May/20 11:45;rmetzger;I'm reopening this, as this error happened again: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2323&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","28/May/20 11:52;chesnay;I'd argue that this is a different error, so there should be a separate JIRA. It's not about order; the result is just wrong.;;;","29/May/20 06:21;dwysakowicz;I agree with [~chesnay] here. I opened a separate issue for it FLINK-18020. I also observed that in my local fork.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stateful Functions State Processor module artifact id is not named correctly,FLINK-15770,13281523,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tzulitai,tzulitai,tzulitai,25/Jan/20 04:36,25/Jan/20 07:13,13/Jul/23 08:07,25/Jan/20 04:59,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,,,,,"It is currently named {{statefun-state-processor}}, whereas the root module {{statefun-flink}} POM reference it with {{statefun-flink-state-processor}}.

The correct one should probably be the name with the {{flink}} prefix, to be consistent with all other modules under {{statefun-flink}}.",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 25 04:59:10 UTC 2020,,,,,,,,,,"0|z0auu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/20 04:59;tzulitai;Fixed in master: 11af579c95b6b9b3e2afea0d2d3c0169999ce523;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ridesharing-example-simulator depends on different parent pom,FLINK-15760,13281455,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igalshilman,sewen,sewen,24/Jan/20 18:25,31/Jan/20 08:32,13/Jul/23 08:07,31/Jan/20 08:32,,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,,,,,"The parent of the {{ridesharing-example-simulator}} module is not inheriting from the {{statefun-parent}} pom.

Thus, it is not inheriting the relevant plugins for code style and license header checks.",,igal,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 31 08:32:11 UTC 2020,,,,,,,,,,"0|z0auew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/20 18:35;chesnay;I too stumbled on this once I saw the duplicate {{spotless-plugin}} definition; it looks suspiciously intentional but I'm not sure.;;;","27/Jan/20 18:40;igal;This is indeed on propose, the intention was to show that this is a completely independent service of stateful functions, that communicates with a running statefun application via Kafka.
But, the downside is a lack of uniformity in the build process. 
So I will change the parent to be a flink-statefun for the sake of uniformity.;;;","31/Jan/20 08:31;igal;This is fixed in master at 69d72782444c5aff2c24ed5751ca5ec6fc274aff.;;;","31/Jan/20 08:32;igal;Fixed at 69d72782444c5aff2c24ed5751ca5ec6fc274aff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ridesharing-example-simulator does not compile on Java 11,FLINK-15759,13281454,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igal,chesnay,chesnay,24/Jan/20 18:21,30/Jan/20 12:06,13/Jul/23 08:07,30/Jan/20 12:06,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"Related to lombok.

{code}
ExceptionInInitializerError: com.sun.tools.javac.code.TypeTags
{code}",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,"igalshilman commented on pull request #3: [FLINK-15759][FLINK-15757][FLINK-15764] Adjust POM definitions
URL: https://github.com/apache/flink-statefun/pull/3
 
 
   This PR addresses multiple build related issues.
   
   1) Removes Lombok so that stateful functions can be built with Java11
   2) Centralize exclusions in dependency management section
   3) Add a comment for each exclustion
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Jan/20 15:45;githubbot;600","tzulitai commented on pull request #3: [FLINK-15759][FLINK-15757][FLINK-15764] Adjust POM definitions
URL: https://github.com/apache/flink-statefun/pull/3
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jan/20 10:48;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13456,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 30 12:06:26 UTC 2020,,,,,,,,,,"0|z0aueo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/20 15:24;igal;I will just remove lombok.;;;","30/Jan/20 12:06;tzulitai;Fixed via 73850cd9d85bf8795d2f78546085faabe643a073;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Investigate potential out-of-memory problems due to managed unsafe memory allocation,FLINK-15758,13281453,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,azagrebin,azagrebin,azagrebin,24/Jan/20 18:20,27/Jan/21 06:46,13/Jul/23 08:07,19/May/20 07:47,,,,,,1.10.2,1.11.0,,,Runtime / Task,,,,,0,pull-request-available,,,,"After FLINK-13985, managed memory is allocated from UNSAFE, not as direct nio buffers as before 1.10.

in FLINK-14894, there was an attempt to release this memory only when all Java handles of the unsafe memory are about to be GC'ed. It is similar to how it was with direct nio buffers before 1.10 but the unsafe memory is not tracked by direct memory limit (-XX:MaxDirectMemorySize). The problem is that over-allocating of unsafe memory will not hit the direct limit and will not cause GC immediately which will be the only way to release it. In this case, it causes out-of-memory failures w/o triggering GC to release a lot of potentially already unused memory.

We have to investigate further optimisations, like:
 * directly monitoring phantom reference queue of the cleaner (if JVM detects quickly that there are no more reference to the memory) and explicitly release memory ready for GC asap, e.g. after Task exit
 * monitor allocated memory amount and block allocation until GC releases occupied memory instead of failing with out-of-memory immediately

cc [~sewen] [~trohrmann]",,azagrebin,guoyangze,kezhuw,libenchao,mproch,trohrmann,,,,,,,,,,,,,,,,"azagrebin commented on pull request #11109: [FLINK-15758][MemManager] Release segment and its unsafe memory in GC Cleaner
URL: https://github.com/apache/flink/pull/11109
 
 
   ## What is the purpose of the change
   
   After #9747, managed memory is allocated from UNSAFE, not as direct nio buffers as before 1.10.
   The releasing of segments released also underlying unsafe memory which is dangerous in general
   as there can be still references to java objects giving access to the released memory. If this reference
   ever leaks, the illegal memory access can result in memory corruption of other code parts w/o even segmentation fault.
   
   The solution can be similar to how Java handles direct memory limit:
   - underlying byte buffers of segments are registered to phantom reference queue with a Java GC cleaner which releases the unsafe memory
   - all allocations and reservations are managed by a memory limit and an atomic available memory
   - if available memory is not enough while reserving, the phantom reference queue processing is triggered to run hopefully discovered by GC cleaners
   - memory can be released directly or in a GC cleaner
   
   The GC is also sped up by nulling out byte buffer reference in `HybridMemorySegment#free` which is inaccessible anyways after freeing. Otherwise also a lot of tests, which hold accidental references to memory segments, have to be fixed to not hold them. The `MemoryManager#verifyEmpty` checks that everything can be GC'ed at the end of the tests and after slot closing in production to detect memory leaks if any other references are held, e.g. from `HybridMemorySegment#wrap`.
   
   ## Brief change log
   
     - Simply MemoryManager by removing KeyedBudgetManager as the managed memory is only on-heap now
     - Remove MemoryManager#AllocationRequest
     - Implement custom unsafe memory control, similar to what JVM does to control direct memory limit
     - Use AtomicLong to control allocated memory and optimistic allocation retry (like in nio.Bits#tryReserveMemory) and trigger GC cleaners if memory is unavailable
     - adjust and add unit tests
   
   ## Verifying this change
   
   unit tests
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Feb/20 10:07;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,FLINK-14894,,,,,,,,,,,,,,,,,FLINK-20663,FLINK-17822,FLINK-18646,FLINK-19852,FLINK-13985,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 07:47:41 UTC 2020,,,,,,,,,,"0|z0aueg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/20 15:00;trohrmann;As https://issues.apache.org/jira/browse/FLINK-14894?focusedCommentId=17024394&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17024394 says there is indeed a problem with higher memory pressure if the fix of FLINK-14894 is applied. We reverted this fix momentarily and need to properly fix this problem by monitoring the unsafe memory usage and trigger a clean if required.;;;","30/Jan/20 12:03;azagrebin;After looking more into this topic, the problem could be resolved with the following step:
 * Simply MemoryManager by removing KeyedBudgetManager as the managed memory is only on-heap now
 * Implement custom unsafe memory control, similar to what JVM does to control direct memory limit:
 ** Use AtomicLong to control allocated memory and optimistic allocation retry (like in nio.Bits#tryReserveMemory)
 ** Try to speed up running GC phantom ref cleaners with SharedSecrets.getJavaLangRefAccess and fallback to full GC if allocation fails before trowing OutOfMemoryError (like in nio.Bits#reserveMemory)

The last step will require wrapping of JavaLangRefAccess logic with the reflection calls as it was relocated in Java 9 and the API has changed.;;;","19/May/20 07:47;azagrebin;merged into master by 10c29b73fdbc2bcfe7cb6f9394ad8db7836041c3
merged into 1.10 by 2a4520935c8b546ac05f664f947d143b06322000;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
statefun-kafka-io pom comment is incomplete,FLINK-15757,13281451,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,chesnay,chesnay,24/Jan/20 18:13,30/Jan/20 12:07,13/Jul/23 08:07,30/Jan/20 12:07,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,,,,,"The following comment in the statfun-kafka-io pom is clearly incomplete and could use some massaging in general.

{code:java}
The following dependency has to be in scope compile, although it would be present in runtime,
since users of this artifact needs to obtain a reference of ConsumerRecord/ProducerRecord at al'
please note, that statefun-kafka-io does not uses a shade plugin therefore the kafka dependency
{code}",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 30 12:07:22 UTC 2020,,,,,,,,,,"0|z0aue0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/20 15:22;igal;I will remove that comment altogether because it is not relevant anymore.;;;","30/Jan/20 12:07;tzulitai;Fixed in master via 2ed5e8bd1d48bb49a21dffdbff1bb49c9d0b3042.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateFun poms are inconsistent in their scala version usages,FLINK-15756,13281450,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igal,chesnay,chesnay,24/Jan/20 18:09,30/Jan/20 12:08,13/Jul/23 08:07,30/Jan/20 12:08,statefun-2.0.0,,,,,statefun-2.0.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"Some statefun modules use hard-coded scala versions (e.g., statefun-flink-core), while others are parameterized (e.g., statefun-flink-harness).

Either we should change this or document why it works the way it does.

Naturally it also begs the questions whether the statefun modules also require scala suffixes.

 ",,igal,sewen,tzulitai,,,,,,,,,,,,,,,,,,,"igalshilman commented on pull request #2: [FLINK-15756] Use scala.binary.version property
URL: https://github.com/apache/flink-statefun/pull/2
 
 
   This PR removes all the hardcoded references to a specific Scala version in all the `pom.xml`s under statefun-flink, and replace them with a property defined at statefun-flink parent.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jan/20 15:16;githubbot;600","tzulitai commented on pull request #2: [FLINK-15756] Use scala.binary.version property
URL: https://github.com/apache/flink-statefun/pull/2
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jan/20 10:48;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 30 12:08:47 UTC 2020,,,,,,,,,,"0|z0auds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/20 18:13;sewen;Would be nice to keep Scala out of this as a whole.

StateFun does not use Scala and thinks of Flink as ""provided scope"". We could say that we depend on a specific Flink Scala version. Users can always use at runtime whichever Fink Scala version they want, it should work.
;;;","24/Jan/20 18:58;chesnay;I can see this working for production deployments but I'm not sure how feasible this is for testing.
Surely for tests you need more than just {{flink-statefun-sdk}}, at which point you _will_ have some provided 2.11 dependencies on the test classpath. 2.12 users would then have to override every single dependency to ensure they don't run into weird incompatibilities.;;;","27/Jan/20 15:09;igal;I'll replace the hardcoded 2.11 references with a property defined  at flink-statefun.xml ;;;","30/Jan/20 12:08;tzulitai;Fixed in master via f3b04fa1190ce97199f0243ce19b575d0e7169c2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set explict managed memory for RocksDBStateBenchmark,FLINK-15748,13281356,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,24/Jan/20 10:15,24/Jan/20 16:29,13/Jul/23 08:07,24/Jan/20 16:29,,,,,,1.10.0,,,,Benchmarks,,,,,0,,,,,"Current RocksDBStateBenchmark did not set explicit managed memory, which would use 16MB as default from MiniClusterConfiguration. However, this is not enough when we enable memory limit on RocksDB by default.",,liyu,sewen,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15749,,,,,,,,,,,,,,,,FLINK-15692,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 24 16:29:30 UTC 2020,,,,,,,,,,"0|z0atsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/20 16:29;sewen;Fixed in the benchmarks repo in 9621822de2acbe19f5eed005b10b73d9f0bf088d

https://github.com/dataArtisans/flink-benchmarks/commit/9621822de2acbe19f5eed005b10b73d9f0bf088d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some TaskManager Task exceptions are logged as info,FLINK-15744,13281329,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,longtimer,longtimer,longtimer,24/Jan/20 03:56,06/Mar/20 17:48,13/Jul/23 08:07,19/Feb/20 08:19,1.9.1,,,,,1.11.0,,,,Runtime / Configuration,,,,,0,pull-request-available,,,,"After investigating to find the cause for a submitted job alternating between the deploying and running states, I tried setting the logging to info level and realized that some deployment exceptions were being logged as info when they really should be a warning or error level.

In particular, the following line (at line 960) of the 1.9.1 branch of the Task class logs as info:

LOG.info(""{} ({}) switched from {} to {}."", taskNameWithSubtask, executionId, currentState, newState, cause);

This log line always has a non null exception cause so it would make sense to log at warning or error level for this state transition so it is more easily found.",1.9.1 on a recent ubuntu release,aljoscha,longtimer,,,,,,,,,,,,,,,,,,,,"lonerzzz commented on pull request #11042: FLINK-15744 Some TaskManager Task exceptions are logged as info
URL: https://github.com/apache/flink/pull/11042
 
 
   ## What is the purpose of the change
   
   The fundamental issue is that exceptions raised in the Task class are logged as log level info. This then requires retaining the log level at info level to be able to see these potentially important errors. However because the info level is intended for observation in some detail, the exceptions can be buried amongst the rest of the information.
   
   The proposed change is to set the output from exceptions at warning level so that info level logging need not be set in order to observe any exceptions occurring in the task.
   
   ## Brief change log
   
   Changed the log level to warn when exceptions occur in the Task
   
   ## Verifying this change
   
   This change is a trivial rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Feb/20 19:39;githubbot;600","aljoscha commented on pull request #11042: FLINK-15744 Some TaskManager Task exceptions are logged as info
URL: https://github.com/apache/flink/pull/11042
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Feb/20 08:19;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 08:19:08 UTC 2020,,,,,,,,,,"0|z0atmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/20 08:19;aljoscha;master: b38a70201ac34bdced145e00fd9d517d9da46d17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PojoSerializerUpgradeTest.testSpecifications fails with NPE on Java 12,FLINK-15739,13281172,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,chesnay,chesnay,23/Jan/20 11:55,27/Jan/20 10:37,13/Jul/23 08:07,27/Jan/20 10:37,1.11.0,,,,,1.11.0,,,,Tests,,,,,0,pull-request-available,,,,"Possibly related to the {{ClassRelocator}} annotation.

https://travis-ci.org/zentol/flink/jobs/640477280
{code}
11:37:47.019 [ERROR] initializationError(org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTest)  Time elapsed: 0.032 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTest.testSpecifications(PojoSerializerUpgradeTest.java:50)
{code}

/cc [~aljoscha] [~tzulitai]",,aljoscha,,,,,,,,,,,,,,,,,,,,,"aljoscha commented on pull request #10944: [FLINK-15739] Fix TypeSerializerUpgradeTestBase on Java 12
URL: https://github.com/apache/flink/pull/10944
 
 
   The problem was that some of the relocated classes in
   PojoSerializerUpgradeTest have enums in them. Java 12 introduced the
   EnumDesc class and enums will have an inner subclass of this defined
   that does not have a ClassLoader. We now filter out classes that don't
   have a classloader in the ClassRelocator because these classes don't
   have bytecode that we could relocate.
   
   cc @igalshilman Do you have an opinion on this?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jan/20 15:56;githubbot;600","aljoscha commented on pull request #10944: [FLINK-15739] Fix TypeSerializerUpgradeTestBase on Java 12
URL: https://github.com/apache/flink/pull/10944
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jan/20 10:37;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,FLINK-15737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 27 10:37:40 UTC 2020,,,,,,,,,,"0|z0aso0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/20 15:51;aljoscha;Yes, this does originate from the {{ClassRelocator}}, it happens when there is an enum in a class that should be relocated. For example in {{ModifiedPojoSchemaVerifier}}. Printing the list of classes to be remapped in {{ClassRelocator.remap()}} yields this:

Java 1.8:
{code}
org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTestSpecifications$ModifiedPojoSchemaVerifier$PojoAfterSchemaUpgrade
org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTestSpecifications$ModifiedPojoSchemaVerifier$PojoAfterSchemaUpgrade$Color
org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTestSpecifications$ModifiedPojoSchemaVerifier
{code}

Java 12:
{code}
java.lang.Enum$EnumDesc
org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTestSpecifications$ModifiedPojoSchemaVerifier$PojoAfterSchemaUpgrade
org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTestSpecifications$ModifiedPojoSchemaVerifier$PojoAfterSchemaUpgrade$Color
org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTestSpecifications$ModifiedPojoSchemaVerifier
{code}

The NPE is thrown because the class {{java.lang.Enum$EnumDesc}} does not have a {{ClassLoader}}. This is a new class in Java 12: https://docs.oracle.com/en/java/javase/12/docs/api/java.base/java/lang/Enum.EnumDesc.html. ;;;","27/Jan/20 10:37;aljoscha;master: 391dd4c02642a2a2a0fc4ae5be1720de09c056b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump powermock to 2.0.4,FLINK-15738,13281168,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/Jan/20 11:12,29/Jan/20 17:16,13/Jul/23 08:07,29/Jan/20 17:16,1.11.0,,,,,1.11.0,,,,Connectors / Kinesis,Tests,,,,0,pull-request-available,,,,"Kinesis tests are failing with a NullPointerException due to their {{Whitebox}} usages.

We have to bump powermock from 2.0.2 to at least 2.0.4.",,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #10958: [FLINK-15738][build] Bump powermock to 2.0.4
URL: https://github.com/apache/flink/pull/10958
 
 
   For Java 12 support.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jan/20 10:40;githubbot;600","zentol commented on pull request #10958: [FLINK-15738][build] Bump powermock to 2.0.4
URL: https://github.com/apache/flink/pull/10958
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jan/20 17:16;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,FLINK-15737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 29 17:16:19 UTC 2020,,,,,,,,,,"0|z0asn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/20 17:16;chesnay;master: 8699e03a3f7668b87893bc9f50236b197a0385b3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBCUpsertOutputFormat does not set bind parameter keyFields in updateStatement,FLINK-15728,13280817,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,dasbh,dasbh,21/Jan/20 18:16,30/Jul/20 07:11,13/Jul/23 08:07,30/Jul/20 07:11,1.9.1,,,,,1.12.0,,,,Connectors / JDBC,Table SQL / Ecosystem,,,,0,pull-request-available,,,,"When using JDBCUpsertOutputFormat custom dialect e.g. H2/Oracle which uses UpsertWriterUsingInsertUpdateStatement, code fails with below error.
{code:java}
Caused by: org.h2.jdbc.JdbcSQLDataException: Parameter ""#6"" is not set [90012-200] 
at org.h2.message.DbException.getJdbcSQLException(DbException.java:590) 
at org.h2.message.DbException.getJdbcSQLException(DbException.java:429) 
at org.h2.message.DbException.get(DbException.java:205) 
at org.h2.message.DbException.get(DbException.java:181) 
at org.h2.expression.Parameter.checkSet(Parameter.java:83) 
at org.h2.jdbc.JdbcPreparedStatement.addBatch(JdbcPreparedStatement.java:1275) 
at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter$UpsertWriterUsingInsertUpdateStatement.processOneRowInBatch(UpsertWriter.java:233) 
at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter.executeBatch(UpsertWriter.java:111) {code}
This is due to UpsertWriterUsingInsertUpdateStatement#processOneRowInBatch does not set all bind paramters in case of Update.

This bug does get surfaced while using Derby DB. 
 In JDBCUpsertOutputFormatTest if we replace Derby with H2 we can reproduce the bug.

The fix is trivial. Happy to raise PR.
{code:java}
//for update case replace below
setRecordToStatement(updateStatement, fieldTypes, row); 
//with
setRecordToStatement(updateStatement, fieldTypes + pkTypes, row  + pkRow);
//NOTE:  as prepared updateStatement contains additional where clause we need pass additional bind values and its sql Types



{code}",,aljoscha,dasbh,jark,leonard,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 30 07:11:43 UTC 2020,,,,,,,,,,"0|z0aqhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/20 19:02;dasbh;+ added [~jark] as original work was reviewed ;;;","22/Jan/20 03:02;jark;[~dasbh], I guess you are right, we didn't cover it in tests. Assigned issue to you.;;;","22/Jan/20 03:28;lzljs3620320;Hi [~dasbh], thanks for reporting.

But I don't think this is a bug or I didn't get what are you mean. If you want to support H2/Oracle, Shouldn't H2Dialect be provided? The default implementation of {{JDBCDialect}} can not work to every DBs. Then, if H2/Oracle need new mechanism to implement upsert, we should improve {{JDBCDialect}} interface. 

So I think this ticket is support H2Dialect and OracleDialect?;;;","22/Jan/20 04:16;lzljs3620320;And I tried Oracle 11g:
{code:java}
create table T (i int, j int);
insert into T values(1, 2);
update T set i = 2 where j = 2;
{code}
It works.;;;","22/Jan/20 12:01;dasbh;Sorry, should have added more context.

Added, H2Dialect as below (i.e. no support for UPSERT) which similar to Derby other than driver name URL
Note:  not overriding method * _getUpsertStatement_* from JDBCDialect similar to derby. So this would use *UpsertWriterUsingInsertUpdateStatement*

{code:java}
private static class H2Dialect implements JDBCDialect {

   private static final long serialVersionUID = 1L;

   @Override
   public boolean canHandle(String url) {
      return url.startsWith(""jdbc:h2:"");
   }

   @Override
   public Optional<String> defaultDriverName() {
      return Optional.of(""org.h2.Driver"");
   }

   @Override
   public String quoteIdentifier(String identifier) {
      return identifier;
   }
}{code}


With this, if you run the test JDBCUpsertOutputFormatTest. You will get 
{code:java}
Caused by: org.h2.jdbc.JdbcSQLDataException: Parameter ""#<index>"" is not set [90012-200] 
{code}

Also, with current *UpsertWriter* implementation Dialect upsert statement can only have  bind parameters ('?') same length as Row and in the same order
{code:sql}

//MySql uses VALUES so each num of ? is same as Row length
INSERT INTO `TAB`(`id`, `msg`) VALUES (?, ?) 
ON DUPLICATE KEY 
UPDATE `id`=VALUES(`id`), `msg`=VALUES(`msg`)

//postgres uses EXCLUDED so each num of ? is same as Row length
INSERT INTO TAB(id, msg) VALUES (?, ?) 
ON CONFLICT (id) 
DO UPDATE SET id=EXCLUDED.id, msg=EXCLUDED.msg

{code}

But if I write oracle upsert as below, setRecordToStatement in UpsertWriter.. will not work
{code:sql}
MERGE INTO TAB USING dual ON ( ""id""=? )
WHEN MATCHED THEN UPDATE SET ""msg""=?
WHEN NOT MATCHED THEN INSERT (""id"",""msg"") 
    VALUES ( ?, ? )
{code}

Hope, I have clarified the issue.;;;","22/Jan/20 12:17;lzljs3620320;Thanks [~dasbh] for explanation. It is clear.

I think it may not be a small change. We may need to modify the interface of JDBCDialect.

Before dialect, there was an interface call QueryExecutor, which can customize the process of upsert. When JDBCDialect was introduced to replace QueryExecutor, it was assumed that each dialect had the same requirements for set fields.

But seems that the ability to customize the upsert is needed.;;;","22/Jan/20 13:15;jark;Hi [~dasbh], thanks for reporting this. 

First of all, I think this is a bug. The default implemenation of {{JDBCDialect#getUpdateStatement}} is problematic. If we add a {{'connector.write.flush.max-rows'='1'}} property to {{JDBCUpsertTableSinkITCase#testUpsert}}, the result is incorrect.

Regarding to the fixing, I agree with [~lzljs3620320], it's not a small change. The root problem is that the current statement is based on index, not on name. We have a similar discussion to improve this issue in this [PR|https://github.com/apache/flink/pull/9335#discussion_r311845039]. An solution would be [named-statement|https://www.javaworld.com/article/2077706/named-parameters-for-preparedstatement.html] and I think it's the time to do it now. ;;;","22/Jan/20 15:03;lzljs3620320;[~dasbh] Your trivial fix is good to me too. I think we can do a little more to exclude pks from set fields.;;;","22/Jan/20 16:08;dasbh;Raised PR https://github.com/apache/flink/pull/10926;;;","22/Jan/20 20:36;dasbh;Also, I have found how to achieve upsert in Oracle and H2. I can add the dialect to this PR.

SQL
{code:sql}
MERGE INTO TAB t 
   USING (SELECT ? ""id"", ? ""msg"" FROM DUAL) v 
   ON ( t.""id""=v.""id"")
WHEN MATCHED THEN 
   UPDATE SET t.""msg""=v.""msg""
WHEN NOT MATCHED THEN 
    INSERT (""id"",""msg"") 
    VALUES ( v.""id"", v.""msg"" )
{code};;;","06/Feb/20 09:54;lzljs3620320;Hi [~dasbh] You can take https://issues.apache.org/jira/browse/FLINK-14100;;;","03/Jul/20 06:43;jark;The current implementation of ""JdbcDialect#getUpdateStatement"" is broken. As [~dasbh] said, the key fields are not binded into the statement, thus all the update statements do not change any rows. This can be reproduced when we set {{'sink.buffer-flush.max-rows' = '1'}}  for {{JdbcDynamicTableSinkITCase#testUpsert}}.;;;","03/Jul/20 06:45;jark;Hi [~dasbh], are you still insterest in this issue? If you don't have time, I can help to fix this.;;;","13/Jul/20 09:22;jark;Hi [~dasbh], I will continue this work based on the {{NamedPrepareStatement}} solution. ;;;","30/Jul/20 07:11;jark;Fixed in master: bdaf1dbd80c57ac23e2d5530d845e483261f7518;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LastValueAggFunctionWithOrderTest compilation error due to incompatible types,FLINK-15706,13280673,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,liyu,liyu,21/Jan/20 08:30,01/Jul/20 16:18,13/Jul/23 08:07,05/Feb/20 08:04,1.10.0,,,,,1.10.0,,,,Table SQL / Planner,Tests,,,,0,pull-request-available,,,,"{{LastValueAggFunctionWithOrderTest}} failed to compile in latest release-1.10 nightly build with below error:
{code}
03:02:41.792 [INFO] -------------------------------------------------------------
03:02:41.792 [ERROR] COMPILATION ERROR : 
03:02:41.792 [INFO] -------------------------------------------------------------
03:02:41.792 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/functions/aggfunctions/LastValueAggFunctionWithOrderTest.java:[78,37] incompatible types: inference variable T has incompatible bounds
    equality constraints: N,java.lang.Byte,N,T,T,T,T,org.apache.flink.table.dataformat.BinaryString,T,T
    lower bounds: java.lang.Byte,org.apache.flink.table.dataformat.BinaryString
{code}

https://api.travis-ci.org/v3/job/639573134/log.txt",,dwysakowicz,jark,liyu,lzljs3620320,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #10915: [FLINK-15706][table-planner-blink] Not use Arrays.asList to avoid incompatible error
URL: https://github.com/apache/flink/pull/10915
 
 
   
   ## What is the purpose of the change
   
   LastValueAggFunctionWithOrderTest failed to compile in nightly build with below error:
   ```
   03:02:41.792 [INFO] -------------------------------------------------------------
   03:02:41.792 [ERROR] COMPILATION ERROR : 
   03:02:41.792 [INFO] -------------------------------------------------------------
   03:02:41.792 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/functions/aggfunctions/LastValueAggFunctionWithOrderTest.java:[78,37] incompatible types: inference variable T has incompatible bounds
       equality constraints: N,java.lang.Byte,N,T,T,T,T,org.apache.flink.table.dataformat.BinaryString,T,T
       lower bounds: java.lang.Byte,org.apache.flink.table.dataformat.BinaryString
   ```
   
   ## Brief change log
   
   Not use `Arrays.asList`.
   
   ## Verifying this change
   
   None
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency):no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector:no
   
   ## Documentation
   
     - Does this pull request introduce a new feature?no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jan/20 11:12;githubbot;600","dawidwys commented on pull request #10915: [FLINK-15706][table-planner-blink] LastValueAggFunctionWithOrderTest compilation error due to incompatible types
URL: https://github.com/apache/flink/pull/10915
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 08:02;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-15699,,,,,,,,,,,,,,,,FLINK-15699,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 05 08:06:58 UTC 2020,,,,,,,,,,"0|z0aplc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/20 08:31;liyu;[~lzljs3620320] [~jark] FYI.;;;","23/Jan/20 10:35;dwysakowicz;I would decrease the priority of this issue. It's a problem of the test itself. Imo the problem is that the type hierarchy of the test infrastructure is overcomplicated and there is little control over type bounds in that test. Moreover it appears only occassionally.

Nevertheless I would aim to simplify these tests.;;;","23/Jan/20 11:06;lzljs3620320;Thanks for involving. I have no idea to test infrastructure. IMO {{LastValueAggFunctionWithOrderTest}} should be a valid java class.

However, there have been similar compilation errors before (I can't find the logs). Maybe we can avoid such a complex type structure in the code.;;;","23/Jan/20 11:10;dwysakowicz;Yes, we should definitely simplify the structure of the classes. I think the problem is somewhere around type inference of the top level test class, as it is actually always instantiated by reflection and as far as I can tell it will most probably always be inferred to Object, which might mismatch somewhere in subsequent calls.

I would try to target this for 1.11 and unblock the release for now (I had a mistake in my previous comment, actually I'm in favor of decreasing the priority).;;;","23/Jan/20 11:26;lzljs3620320;+1 to target this for 1.11.

My first thought was {{Arrays.asList according to the logs, and there are many type inference errors around it. The testData}} return {{AggFunctionTestSpec}} , this should be Object, and {{LastValueAggFunctionWithOrderTest}} instantiated by reflection should be Object too.
 But I am OK to make it more simple to avoid unknown errors too.;;;","03/Feb/20 09:08;dwysakowicz;Another instance: https://api.travis-ci.org/v3/job/645038395/log.txt;;;","05/Feb/20 08:04;dwysakowicz;Fixed in
master: e0afec8a7654334e3e9992433f5a0424d2ed51b8 b3c86a81161e54ee403a6471adbfdf2c4dfc2893
1.10: 1fa29014ad8dba0a5595c51cf01e5ace1d7930af f4c25df707571f60724c8516dfb7a1055b0fa63c;;;","05/Feb/20 08:06;dwysakowicz;The fix restructured the tests in a way the generic types should be handled properly.;;;","05/Feb/20 08:06;dwysakowicz;Please reopen if you see this reoccur.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FirstValueAggFunctionWithoutOrderTest failed on Traivs,FLINK-15699,13280637,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,klion26,klion26,21/Jan/20 01:49,01/Jul/20 16:18,13/Jul/23 08:07,01/Jul/20 16:18,1.11.0,,,,,1.11.0,,,,Table SQL / Planner,Tests,,,,0,,,,,"09:48:53.473 [ERROR] COMPILATION ERROR : 
09:48:53.473 [INFO] -------------------------------------------------------------
09:48:53.473 [ERROR] /home/travis/build/flink-ci/flink/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/functions/aggfunctions/FirstValueAggFunctionWithoutOrderTest.java:[78,37] incompatible types: inference variable T has incompatible bounds
 equality constraints: java.lang.Byte,T,T,T,T,T,java.lang.Boolean,T
 lower bounds: java.lang.Boolean
09:48:53.473 [INFO] 1 error

[https://travis-ci.com/flink-ci/flink/jobs/277466696]",,godfreyhe,jark,klion26,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15706,,,,,,,,,,,,FLINK-15706,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 21 01:49:38 UTC 2020,,,,,,,,,,"0|z0apdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/20 01:49;klion26;cc [~godfreyhe];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskExecutorPartitionLifecycleTest#runInTaskExecutorThreadAndWait does not run in main thread,FLINK-15691,13280578,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,trohrmann,trohrmann,20/Jan/20 15:20,24/Jan/20 08:58,13/Jul/23 08:07,23/Jan/20 14:58,1.9.0,,,,,1.10.0,,,,Runtime / Coordination,Tests,,,,0,pull-request-available,,,,"The method {{TaskExecutorPartitionLifecycleTest#runInTaskExecutorThreadAndWait}} does not run the given runnable in the {{TaskExecutors}} main thread. Instead it will run it in an arbitrary thread of the {{RpcService}}.

The reason why this does not cause test instabilities is that we execute operations which don't cause race conditions. However, in order to avoid spreading false friends I would suggest to get rid of this pattern. Moreover, I would question whether {{TaskExecutorPartitionLifecycleTest}} really needs access to the {{TaskExecutors}} internal state. I believe there is an easy way using the {{TaskExecutors}} public API to implement the tests.

cc [~chesnay], [~azagrebin]",,trohrmann,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #10923: [FLINK-15691][tests] Remove runInTaskExecutorThreadAndWait()
URL: https://github.com/apache/flink/pull/10923
 
 
   Removes `TaskExecutorPartitionLifecycleTest#runInTaskExecutorThreadAndWait()` and refactors existing usages.
   
   The method did not achieve the desired effect of running the action in the TE main thread. Given that the test has not shown instabilities despite this we can conclude that these actions can just be executed in the tests main thread instead.
   
   Additionally, this test removes/replaces accesses to the JobManagerTable of the TaskExecutor.
   The first access was removed since it is not really necessary since it effectively verified the test setup.
   The second access was replaced with a check for whether the disconnect-from-JM future has been completed, which _should_ behave the same way.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/20 09:40;githubbot;600","zentol commented on pull request #10923: [FLINK-15691][tests] Remove runInTaskExecutorThreadAndWait()
URL: https://github.com/apache/flink/pull/10923
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jan/20 14:54;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 23 14:58:08 UTC 2020,,,,,,,,,,"0|z0ap08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/20 14:58;chesnay;master: f7a7d89ca7156a57365252f947dab82455f2ff78

1.10: d7c84e6be16cfe0bf0c07baff1899f6828c02e5b ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SELECT 'ABC'; does not work in sql-client",FLINK-15686,13280523,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,20/Jan/20 10:40,21/Jan/20 13:22,13/Jul/23 08:07,21/Jan/20 13:22,1.10.0,,,,,1.10.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"A query like {{SELECT 'abc';}} fails in sql-client with blink planner enabled with an error:
{code}
org.apache.flink.table.api.ValidationException: Type CHAR(3) of table field 'EXPR$0' does not match with the physical type STRING of the 'EXPR$0' field of the TableSink consumed type.
{code}

The reason is that those sinks do not properly support new type system. There is no good way to define schema and consumed data type so that they match. We should update the in-memory sinks in sql-client to work with the legacy type system for now until the retract and upsert sinks work properly with the new type system.",,dwysakowicz,jark,,,,,,,,,,,,,,,,,,,,"dawidwys commented on pull request #10909: [FLINK-15686][sql-client] Use old type system in CollectStreamTableSink
URL: https://github.com/apache/flink/pull/10909
 
 
   ## What is the purpose of the change
   
   Fix mismatch between a declared logical schema  with a declared physical schema of a `CollectStreamTableSink`.
   
   ## Brief change log
   
     - Do not pass around a `RowType` in sql-client as it contains duplicated information with `TableSchema`
     - Use legacy types in `CollectStreamTableSink#getTableSchema` via `CollectStreamTableSink#getFieldTypes` & `CollectStreamTableSink#getFieldNames`
   
   
   ## Verifying this change
   
   Extended tests in `LocalExecutorITCase` to include a problematic `CHAR(3)` coming from a string literal in a result of a query.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jan/20 14:59;githubbot;600","dawidwys commented on pull request #10909: [FLINK-15686][sql-client] Use old type system in CollectStreamTableSink
URL: https://github.com/apache/flink/pull/10909
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jan/20 13:20;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 21 13:22:04 UTC 2020,,,,,,,,,,"0|z0aoo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/20 13:22;dwysakowicz;Fixed in:
master: 8d85f85e265158e4ffbef4718f31306562652f98
1.10: bcb2b707be5b814add6c3f44f9a7079016f54c57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink.tests.util.kafka.SQLClientKafkaITCase failed on traivs,FLINK-15685,13280520,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,klion26,klion26,20/Jan/20 10:15,13/Feb/20 09:44,13/Jul/23 08:07,13/Feb/20 09:44,1.11.0,,,,,1.11.0,,,,Table SQL / Client,Tests,,,,0,,,,,"{code}
08:50:02.717 [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 76.395 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientKafkaITCase
 
1897208:50:02.722 [ERROR] testKafka[0: kafka-version:0.10 kafka-sql-version:.*kafka-0.10.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 23.806 s <<< ERROR!
 
18973java.io.IOException: 
 
18974Process execution failed due error. Error output:Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
 
18975 at org.apache.flink.table.client.SqlClient.main(SqlClient.java:190)
 
18976Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context.
 
18977 at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:759)
 
18978 at org.apache.flink.table.client.gateway.local.LocalExecutor.openSession(LocalExecutor.java:228)
 
18979 at org.apache.flink.table.client.SqlClient.start(SqlClient.java:98)
 
18980 at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178)
 
18981Caused by: java.lang.NoClassDefFoundError: org/apache/avro/io/DatumReader
 
18982 at org.apache.flink.formats.avro.AvroRowFormatFactory.createDeserializationSchema(AvroRowFormatFactory.java:64)
 
18983 at org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase.getDeserializationSchema(KafkaTableSourceSinkFactoryBase.java:285)
 
18984 at org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase.createStreamTableSource(KafkaTableSourceSinkFactoryBase.java:163)
 
18985 at org.apache.flink.table.factories.StreamTableSourceFactory.createTableSource(StreamTableSourceFactory.java:49)
 
18986 at org.apache.flink.table.client.gateway.local.ExecutionContext.createTableSource(ExecutionContext.java:371)
 
18987 at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$6(ExecutionContext.java:552)
 
18988 at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
 
18989 at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:550)
 
18990 at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:487)
 
18991 at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:159)
 
18992 at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:118)
 
18993 at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:748)
 
18994 ... 3 more
 
18995Caused by: java.lang.ClassNotFoundException: org.apache.avro.io.DatumReader
 
18996 at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
 
18997 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 
18998 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 
18999 ... 15 more
 
19000
 
19001 at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178)
 
19002 at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)
 
19003
 
1900408:50:02.734 [ERROR] testKafka[1: kafka-version:0.11 kafka-sql-version:.*kafka-0.11.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 25.227 s <<< ERROR!
 
19005java.io.IOException: 
 
19006Process execution failed due error. Error output:Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
 
19007 at org.apache.flink.table.client.SqlClient.main(SqlClient.java:190)
 
19008Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context.
 
19009 at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:759)
 
19010 at org.apache.flink.table.client.gateway.local.LocalExecutor.openSession(LocalExecutor.java:228)
 
19011 at org.apache.flink.table.client.SqlClient.start(SqlClient.java:98)
 
19012 at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178)
 
19013Caused by: java.lang.NoClassDefFoundError: org/apache/avro/io/DatumReader
 
19014 at org.apache.flink.formats.avro.AvroRowFormatFactory.createDeserializationSchema(AvroRowFormatFactory.java:64)
 
19015 at org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase.getDeserializationSchema(KafkaTableSourceSinkFactoryBase.java:285)
 
19016 at org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase.createStreamTableSource(KafkaTableSourceSinkFactoryBase.java:163)
 
19017 at org.apache.flink.table.factories.StreamTableSourceFactory.createTableSource(StreamTableSourceFactory.java:49)
 
19018 at org.apache.flink.table.client.gateway.local.ExecutionContext.createTableSource(ExecutionContext.java:371)
 
19019 at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$6(ExecutionContext.java:552)
 
19020 at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
 
19021 at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:550)
 
19022 at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:487)
 
19023 at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:159)
 
19024 at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:118)
 
19025 at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:748)
 
19026 ... 3 more
 
19027Caused by: java.lang.ClassNotFoundException: org.apache.avro.io.DatumReader
 
19028 at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
 
19029 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 
19030 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 
19031 ... 15 more
 
19032
 
19033 at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178)
 
19034 at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)
 
19035
 
1903608:50:02.745 [ERROR] testKafka[2: kafka-version:universal kafka-sql-version:.*kafka.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 27.321 s <<< ERROR!
 
19037java.io.IOException: 
 
19038Process execution failed due error. Error output:Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
 
19039 at org.apache.flink.table.client.SqlClient.main(SqlClient.java:190)
 
19040Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context.
 
19041 at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:759)
 
19042 at org.apache.flink.table.client.gateway.local.LocalExecutor.openSession(LocalExecutor.java:228)
 
19043 at org.apache.flink.table.client.SqlClient.start(SqlClient.java:98)
 
19044 at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178)
 
19045Caused by: java.lang.NoClassDefFoundError: org/apache/avro/io/DatumReader
 
19046 at org.apache.flink.formats.avro.AvroRowFormatFactory.createDeserializationSchema(AvroRowFormatFactory.java:64)
 
19047 at org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase.getDeserializationSchema(KafkaTableSourceSinkFactoryBase.java:285)
 
19048 at org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase.createStreamTableSource(KafkaTableSourceSinkFactoryBase.java:163)
 
19049 at org.apache.flink.table.factories.StreamTableSourceFactory.createTableSource(StreamTableSourceFactory.java:49)
 
19050 at org.apache.flink.table.client.gateway.local.ExecutionContext.createTableSource(ExecutionContext.java:371)
 
19051 at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$6(ExecutionContext.java:552)
 
19052 at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
 
19053 at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:550)
 
19054 at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:487)
 
19055 at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:159)
 
19056 at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:118)
 
19057 at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:748)
 
19058 ... 3 more
 
19059Caused by: java.lang.ClassNotFoundException: org.apache.avro.io.DatumReader
 
19060 at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
 
19061 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 
19062 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 
19063 ... 15 more
 
19064
 
19065 at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178)
 
19066 at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)
 
19067
 
1906808:50:03.069 [INFO] 
 
1906908:50:03.069 [INFO] Results:
 
1907008:50:03.069 [INFO] 
 
1907108:50:03.069 [ERROR] Errors: 
 
1907208:50:03.069 [ERROR] SQLClientKafkaITCase.testKafka:151->insertIntoAvroTable:178 » IO Process execu...
 
1907308:50:03.069 [ERROR] SQLClientKafkaITCase.testKafka:151->insertIntoAvroTable:178 » IO Process execu...
 
1907408:50:03.069 [ERROR] SQLClientKafkaITCase.testKafka:151->insertIntoAvroTable:178 » IO Process execu...
 
1907508:50:03.069 [INFO] 
 
1907608:50:03.069 [ERROR] Tests run: 6, Failures: 0, Errors: 3, Skipped: 0
 {code}

[https://travis-ci.org/klion26/flink/jobs/639304872?utm_medium=notification&utm_source=github_status]",,jark,klion26,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 13 09:44:21 UTC 2020,,,,,,,,,,"0|z0aonc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/20 10:16;klion26;From the exception log, it was thrown in {{SqlClinet.java}}, so I attached this issue to {{Table SQL/Cline}} component. ;;;","25/Jan/20 17:25;trohrmann;Another instance: https://api.travis-ci.org/v3/job/641346359/log.txt;;;","26/Jan/20 18:26;chesnay;Bit of a weird one; my first instinct is that there's some issue in the build process for the avro jars, but I can't see how that could be non-deterministic.;;;","27/Jan/20 14:39;jark;It seems that the reason is similar to FLINK-15500 that the e2e test is not run with hadoop pre-bundled. ;;;","27/Jan/20 15:11;trohrmann;Do you have time to fix this issue [~jark]?;;;","27/Jan/20 16:24;jark;Hi [~trohrmann], if this is not urgent I think I can take this, because I'm still on vacation. 

If this is the root cause, I guess the fixing is not difficult, we can just copy the hadoop pre-bundled jars into {{flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/target/dependencies}} directory. But I'm wondering is the case always failed since {{SQLClientKafkaITCase}} was checked in or failed recently? ;;;","27/Jan/20 22:44;chesnay;I will fix this tomorrow. We just have to disable the test for builds that don't include hadoop in the distribution using the {{Hadoop}} category, although it seems I never set up this logic in the travis scripts.;;;","13/Feb/20 09:44;chesnay;master: d2ea1fcdaca5f81639cbfbc0fc5ef683feb5ec3c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL client can't cancel flink job,FLINK-15669,13280393,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,godfreyhe,godfreyhe,godfreyhe,19/Jan/20 11:36,27/Apr/20 09:22,13/Jul/23 08:07,27/Apr/20 09:22,1.10.0,,,,,1.10.1,1.11.0,,,Table SQL / Client,,,,,0,pull-request-available,,,,"in sql client, CLI client do cancel query operation through {{void cancelQuery(String sessionId, String resultId)}} method in {{Executor}}. However, the {{resultId}} is a random UUID, is not the job id. So CLI client can't cancel a running job.


related code in {{LocalExecutor}}:
{code:java}
private <C> ResultDescriptor executeQueryInternal(String sessionId, ExecutionContext<C> context, String query) {
	 ......

	// store the result with a unique id
	final String resultId = UUID.randomUUID().toString();
	resultStore.storeResult(resultId, result);

	......

	// create execution
	final ProgramDeployer deployer = new ProgramDeployer(
		configuration, jobName, pipeline);

	// start result retrieval
	result.startRetrieval(deployer);

	return new ResultDescriptor(
			resultId,
			removeTimeAttributes(table.getSchema()),
			result.isMaterialized());
}



private <T> void cancelQueryInternal(ExecutionContext<T> context, String resultId) {
	......

	// stop Flink job
	try (final ClusterDescriptor<T> clusterDescriptor = context.createClusterDescriptor()) {
		ClusterClient<T> clusterClient = null;
		try {
			// retrieve existing cluster
			clusterClient = clusterDescriptor.retrieve(context.getClusterId()).getClusterClient();
			try {
				// ======== cancel job through resultId =======
				clusterClient.cancel(new JobID(StringUtils.hexStringToByte(resultId))).get();
			} catch (Throwable t) {
				// the job might has finished earlier
			}
		} catch (Exception e) {
			throw new SqlExecutionException(""Could not retrieve or create a cluster."", e);
		} finally {
			try {
				if (clusterClient != null) {
					clusterClient.close();
				}
			} catch (Exception e) {
				// ignore
			}
		}
	} catch (SqlExecutionException e) {
		throw e;
	} catch (Exception e) {
		throw new SqlExecutionException(""Could not locate a cluster."", e);
	}
}
{code}



",,aljoscha,godfreyhe,gyfora,jark,liyu,zhangwei24,,,,,,,,,,,,,,,,"godfreyhe commented on pull request #10904: [FLINK-15669] [sql client] fix SQL client can't cancel flink job
URL: https://github.com/apache/flink/pull/10904
 
 
   
   ## What is the purpose of the change
   
   *reason: in sql client, CLI client do cancel query operaiton through void cancelQuery(String sessionId, String resultId) method in Executor. However, the resultId is a random UUID, is not the job id. 
   solution: changes the non-blocking deployment to blocking deployment in LocalExecutor#executeQueryInternal method, and then we can get jobId from JobClient, and then use it as resultId. The parameter of DynamicResult#startRetrieval method should also be changed from ProgramDeployer to JobClient, like void startRetrieval(JobClient jobClient);*
   
   
   ## Brief change log
   
     - *changes the non-blocking deployment to blocking deployment in LocalExecutor#executeQueryInternal method*
     - *changes the DynamicResult#startRetrieval method's parameter from ProgramDeployer to JobClient *
   
   
   ## Verifying this change
   
   This change is verified by manual testing.
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jan/20 13:04;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17405,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 09:22:10 UTC 2020,,,,,,,,,,"0|z0anv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/20 12:32;godfreyhe;I would like to fix it.

my solution is: changes the non-blocking deployment to blocking deployment in {{LocalExecutor#executeQueryInternal}} method, and then we can get jobId from {{JobClient}}, and then use it as resultId. The parameter of {{DynamicResult#startRetrieval}} method should also be changed from {{ProgramDeployer}} to {{JobClient}}, like {{void startRetrieval(JobClient jobClient);}};;;","20/Jan/20 13:43;aljoscha;I think that should work, yes. This at least waits until the job is successfully submitted, which seems good anyways.

maybe [~twalthr] has an opinion here as well.;;;","01/Apr/20 09:18;aljoscha;[~godfreyhe] Will you still work on this?;;;","01/Apr/20 09:25;godfreyhe;yes, The PR is ready for the fix. And I also verify the scenario [~twalthr] mentioned today, and give the reply in the PR.;;;","22/Apr/20 08:09;aljoscha;master: 0650e4b6b6a9725c3a9589a110b202478a68f90c
release-1.10: 6de489d136e142fad53a18abc6ab6d5ecee1f88f;;;","26/Apr/20 08:27;aljoscha;I partially reverted these changes:
Only the tests are reverted because the fix is still good.  The added
tests don't work because of a race condition: if the query finishes
before the test cancels them the job status will be FINISHED, and not
CANCELLED, as the text expects.

revert on release-1.10: 4518d18a726b35de9ff802d155fd8100dc711a63
revert on master: cbaea2924a90a1141d9794f6cd8f561e77ddc11a;;;","27/Apr/20 05:45;liyu;Since the fix is still good and only test reverted, I think it makes sense to still leave the fix version of this JIRA to 1.10.1. Maybe we could also close this one and open another JIRA to harden the test, what do you think? [~aljoscha] [~godfreyhe] Thanks.;;;","27/Apr/20 08:44;godfreyhe;I can create a random source which could be a bounded or unbounded table source and make sure the table source run longer than timeout. I will create a JIRA to fix this.;;;","27/Apr/20 08:54;aljoscha;Thanks for the fast response, [~godfreyhe]!;;;","27/Apr/20 09:22;liyu;Thanks for the quick action [~godfreyhe]!

Closing this JIRA since the test case improvement is tracked by FLINK-17405 instead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManagerHAProcessFailureRecoveryITCase.testDispatcherProcessFailure failed because of Could not find Flink job ,FLINK-15661,13280368,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,klion26,klion26,19/Jan/20 08:16,21/Feb/23 12:47,13/Jul/23 08:07,21/Feb/23 12:47,1.11.0,,,,,,,,,Runtime / Coordination,Tests,,,,0,test-stability,,,,"2020-01-19T06:25:02.3856954Z [ERROR] JobManagerHAProcessFailureRecoveryITCase.testDispatcherProcessFailure:347 The program encountered a ExecutionException : org.apache.flink.runtime.rest.util.RestClientException: [org.apache.flink.runtime.rest.handler.RestHandlerException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (47fe3e8df0e59994938485f683d1410e)
 2020-01-19T06:25:02.3857171Z at org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler.propagateException(JobExecutionResultHandler.java:91)
 2020-01-19T06:25:02.3857571Z at org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler.lambda$handleRequest$1(JobExecutionResultHandler.java:82)
 2020-01-19T06:25:02.3857866Z at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870)
 2020-01-19T06:25:02.3857982Z at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852)
 2020-01-19T06:25:02.3859852Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
 2020-01-19T06:25:02.3860440Z at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
 2020-01-19T06:25:02.3860732Z at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:872)
 2020-01-19T06:25:02.3860960Z at akka.dispatch.OnComplete.internal(Future.scala:263)
 2020-01-19T06:25:02.3861099Z at akka.dispatch.OnComplete.internal(Future.scala:261)
 2020-01-19T06:25:02.3861232Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
 2020-01-19T06:25:02.3861391Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
 2020-01-19T06:25:02.3861546Z at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
 2020-01-19T06:25:02.3861712Z at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
 2020-01-19T06:25:02.3861809Z at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
 2020-01-19T06:25:02.3861916Z at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
 2020-01-19T06:25:02.3862221Z at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
 2020-01-19T06:25:02.3862475Z at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
 2020-01-19T06:25:02.3862626Z at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
 2020-01-19T06:25:02.3862736Z at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
 2020-01-19T06:25:02.3862820Z at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
 2020-01-19T06:25:02.3867146Z at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
 2020-01-19T06:25:02.3867318Z at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
 2020-01-19T06:25:02.3867441Z at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
 2020-01-19T06:25:02.3867552Z at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
 2020-01-19T06:25:02.3867664Z at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
 2020-01-19T06:25:02.3867763Z at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
 2020-01-19T06:25:02.3867843Z at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
 2020-01-19T06:25:02.3867936Z at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
 2020-01-19T06:25:02.3868036Z at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
 2020-01-19T06:25:02.3868145Z at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
 2020-01-19T06:25:02.3868223Z at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 2020-01-19T06:25:02.3868313Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
 2020-01-19T06:25:02.3868390Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
 2020-01-19T06:25:02.3868520Z Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (47fe3e8df0e59994938485f683d1410e)
 2020-01-19T06:25:02.3868625Z at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$requestJobStatus$17(Dispatcher.java:516)
 2020-01-19T06:25:02.3868734Z at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870)
 2020-01-19T06:25:02.3868831Z at java.util.concurrent.CompletableFuture.uniExceptionallyStage(CompletableFuture.java:884)
 2020-01-19T06:25:02.3869143Z at java.util.concurrent.CompletableFuture.exceptionally(CompletableFuture.java:2196)
 2020-01-19T06:25:02.3869241Z at org.apache.flink.runtime.dispatcher.Dispatcher.requestJobStatus(Dispatcher.java:510)
 2020-01-19T06:25:02.3869319Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 2020-01-19T06:25:02.3869418Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 2020-01-19T06:25:02.3869506Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 2020-01-19T06:25:02.3869602Z at java.lang.reflect.Method.invoke(Method.java:498)
 2020-01-19T06:25:02.3869681Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
 2020-01-19T06:25:02.3869780Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
 2020-01-19T06:25:02.3869865Z at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
 2020-01-19T06:25:02.3869982Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
 2020-01-19T06:25:02.3870062Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
 2020-01-19T06:25:02.3870153Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
 2020-01-19T06:25:02.3870228Z at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
 2020-01-19T06:25:02.3870399Z at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
 2020-01-19T06:25:02.3870481Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
 2020-01-19T06:25:02.3870571Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
 2020-01-19T06:25:02.3870646Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
 2020-01-19T06:25:02.3870733Z at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
 2020-01-19T06:25:02.3870911Z at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
 2020-01-19T06:25:02.3871013Z at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
 2020-01-19T06:25:02.3871086Z at akka.actor.ActorCell.invoke(ActorCell.scala:561)
 2020-01-19T06:25:02.3871170Z at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
 2020-01-19T06:25:02.3871350Z at akka.dispatch.Mailbox.run(Mailbox.scala:225)
 2020-01-19T06:25:02.3871439Z at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
 2020-01-19T06:25:02.3871509Z ... 4 more
 2020-01-19T06:25:02.3871618Z Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (47fe3e8df0e59994938485f683d1410e)
 2020-01-19T06:25:02.3871721Z at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGatewayFuture(Dispatcher.java:776)
 2020-01-19T06:25:02.3871827Z at org.apache.flink.runtime.dispatcher.Dispatcher.requestJobStatus(Dispatcher.java:505)
 2020-01-19T06:25:02.3871903Z ... 26 more
 2020-01-19T06:25:02.3871975Z ]

 

[https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/4461/logs/15]",,klion26,mapohl,rmetzger,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31168,,,,,,,,,"18/Oct/22 06:52;mapohl;FLINK-15661.extracted-logs.tar.gz;https://issues.apache.org/jira/secure/attachment/13051050/FLINK-15661.extracted-logs.tar.gz",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 21 12:47:18 UTC 2023,,,,,,,,,,"0|z0anpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/20 15:43;trohrmann;Closing this issue for the moment because the build did not upload the Flink logs w/o debugging this problem is impossible. Please re-open this issue in case another test failure happens.;;;","25/May/20 05:33;rmetzger;I observed another failure of this test: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2085&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323

{code}
2020-05-24T20:47:19.2825741Z [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 31.901 s <<< FAILURE! - in org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase
2020-05-24T20:47:19.2826917Z [ERROR] testDispatcherProcessFailure[ExecutionMode PIPELINED](org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase)  Time elapsed: 15.971 s  <<< ERROR!
2020-05-24T20:47:19.2827780Z java.util.concurrent.ExecutionException: org.apache.flink.runtime.rpc.exceptions.RpcConnectionException: Could not connect to rpc endpoint under address akka.tcp://flink@127.0.0.1:45907/user/rpc/dispatcher_1.
2020-05-24T20:47:19.2828444Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-05-24T20:47:19.2829276Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-05-24T20:47:19.2829840Z 	at org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.testDispatcherProcessFailure(JobManagerHAProcessFailureRecoveryITCase.java:296)
2020-05-24T20:47:19.2830366Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-24T20:47:19.2830750Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-24T20:47:19.2831190Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-24T20:47:19.2831592Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-24T20:47:19.2832038Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-24T20:47:19.2832502Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-24T20:47:19.2832958Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-24T20:47:19.2833405Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-24T20:47:19.2833899Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-24T20:47:19.2834319Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-24T20:47:19.2834693Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-24T20:47:19.2835056Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-24T20:47:19.2835402Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-24T20:47:19.2835814Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-24T20:47:19.2836404Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-24T20:47:19.2836824Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-24T20:47:19.2837200Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-24T20:47:19.2847121Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-24T20:47:19.2847541Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-24T20:47:19.2847920Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-24T20:47:19.2848299Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-24T20:47:19.2848709Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-24T20:47:19.2849046Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-24T20:47:19.2849399Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-24T20:47:19.2849766Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-24T20:47:19.2850156Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-24T20:47:19.2850531Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-24T20:47:19.2850920Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-24T20:47:19.2851334Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-24T20:47:19.2851773Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-24T20:47:19.2852179Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-24T20:47:19.2852576Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-24T20:47:19.2853042Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-24T20:47:19.2853508Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-24T20:47:19.2853966Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-24T20:47:19.2854504Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-24T20:47:19.2854991Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-24T20:47:19.2862514Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-24T20:47:19.2862941Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-24T20:47:19.2863566Z Caused by: org.apache.flink.runtime.rpc.exceptions.RpcConnectionException: Could not connect to rpc endpoint under address akka.tcp://flink@127.0.0.1:45907/user/rpc/dispatcher_1.
2020-05-24T20:47:19.2864277Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcService.lambda$resolveActorAddress$10(AkkaRpcService.java:520)
2020-05-24T20:47:19.2864785Z 	at scala.concurrent.java8.FuturesConvertersImpl$CF$$anon$1.accept(FutureConvertersImpl.scala:59)
2020-05-24T20:47:19.2865291Z 	at scala.concurrent.java8.FuturesConvertersImpl$CF$$anon$1.accept(FutureConvertersImpl.scala:53)
2020-05-24T20:47:19.2865752Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-05-24T20:47:19.2866379Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-05-24T20:47:19.2866879Z 	at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:457)
2020-05-24T20:47:19.2867290Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2020-05-24T20:47:19.2867700Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2020-05-24T20:47:19.2868106Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2020-05-24T20:47:19.2868522Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
2020-05-24T20:47:19.2869149Z Caused by: akka.actor.ActorNotFound: Actor not found for: ActorSelection[Anchor(akka.tcp://flink@127.0.0.1:45907/), Path(/user/rpc/dispatcher_1)]
2020-05-24T20:47:19.2869825Z 	at akka.actor.ActorSelection$$anonfun$resolveOne$1.apply(ActorSelection.scala:71)
2020-05-24T20:47:19.2870267Z 	at akka.actor.ActorSelection$$anonfun$resolveOne$1.apply(ActorSelection.scala:69)
2020-05-24T20:47:19.2870670Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-05-24T20:47:19.2871205Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-05-24T20:47:19.2871626Z 	at akka.dispatch.BatchingExecutor$Batch.run(BatchingExecutor.scala:73)
2020-05-24T20:47:19.2872081Z 	at akka.dispatch.ExecutionContexts$sameThreadExecutionContext$.unbatchedExecute(Future.scala:81)
2020-05-24T20:47:19.2872542Z 	at akka.dispatch.BatchingExecutor$class.execute(BatchingExecutor.scala:120)
2020-05-24T20:47:19.2872975Z 	at akka.dispatch.ExecutionContexts$sameThreadExecutionContext$.execute(Future.scala:80)
2020-05-24T20:47:19.2873418Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2020-05-24T20:47:19.2873832Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2020-05-24T20:47:19.2874277Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2020-05-24T20:47:19.2874659Z 	at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:101)
2020-05-24T20:47:19.2875070Z 	at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:999)
2020-05-24T20:47:19.2875467Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-05-24T20:47:19.2875813Z 	at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:458)
2020-05-24T20:47:19.2876382Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-05-24T20:47:19.2876726Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-05-24T20:47:19.2877077Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-05-24T20:47:19.2877418Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-05-24T20:47:19.2877724Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-05-24T20:47:19.2878080Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-05-24T20:47:19.2878482Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-05-24T20:47:19.2878975Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-05-24T20:47:19.2881370Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-05-24T20:47:19.2881875Z 
{code};;;","11/Jun/20 16:29;chesnay;Is this test covering anything that is not covered by the HA e2e test?
The test is a bit odd since it spawns additional dispatcher/taskexecutor processes, without properly forwarding the log4j configuration. So FWIW if it fails there's basically nothing we can do to debug things.
;;;","15/Jun/20 09:28;trohrmann;It looks as if the root problem is an unstable ZooKeeper connection in the test. Due to this, it happens the following:

1) Dispatcher gets leadership granted & announces its address
2) Test case obtains leader address & tries to connect to it
3) Due to a ZooKeeper timeout, the Dispatcher loses the leadership
4) It regains it right away after the ZooKeeper connection is reconnected & announces a new leader address
5) The test fails because it still tries to connect to the old leader address

I see two options: Either hardening the whole test to tolerate ZooKeeper outages because it can happen at any point in time. Alternatively, we could remove this test if we have an e2e test which covers the same logic.

[~rmetzger] I am bit surprised to see these ZooKeeper timeouts to occur much more often on our new AZP CI infrastructure than we used to see it on Travis (never). I fear that many ZooKeeper tests actually assume that the ZooKeeper connection is stable and might now be susceptible to timeout issues. Can we figure out whether this has something to do with the underlying infrastructure? The logs look as if there are some time jumps as if the process only gets very little processing time. If there is nothing we can do about it, then we need to configure our ZooKeeper test servers to have a larger connection timeout.;;;","15/Jun/20 14:42;rmetzger;Thanks a lot for investigating the issue.
What is indeed concerning are log lines like this (on *AlibabaCI003-agent03*)
{code}
20:46:56,889 [                main] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@192.168.128.2:46158
20:47:00,486 [main-SendThread(127.0.0.1:43668)] WARN  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Client session timed out, have not heard from server in 4040ms for sessionid 0x1724870252b0004
{code}
Where nothing seems to be happening for ~4 seconds.

I have observed other ""hard to explain"" networking issues on the AliCloud-hosted CI machines. So far, I have suspected an issue in the network stack, but maybe I need to consider other OS components as well (process scheduler, docker, the underlying ""hardware"").

Why Scheduler? 
Machines are running up to 8 test jobs concurrently, on 32 cores. Not sure if the Kernel sometimes has some hiccups scheduling this many workloads.

Why Docker?
The machines run an older (still supported) CentOs with a Kernel from 2013, with Docker from 2019. It seems that Linux 3.10 (which we are using) is the first to be supported by Docker.

Why underlying hardware?
It's a cloud-hosted machine, which sometimes get's live-migrated (example: https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options#live_migrate)

What were the other 7 builders doing on the machine at that time?

 *AlibabaCI003-agent04* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2085&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=93018cfc-6498-565b-e034-4b68eb90fc80
{code}
2020-05-24T20:46:56.5617885Z [INFO] Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.164 s - in org.apache.flink.api.common.typeutils.base.LocalDateTimeSerializerTest
2020-05-24T20:46:56.5623520Z [INFO] Running org.apache.flink.api.common.typeutils.base.LocalDateSerializerTest
2020-05-24T20:47:00.5093823Z [INFO] Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.142 s - in org.apache.flink.api.common.typeutils.base.LocalDateSerializerTest
2020-05-24T20:47:00.5094515Z [INFO] Running org.apache.flink.api.common.typeutils.base.ShortComparatorTest
{code}
==> 4 seconds no output

*AlibabaCI003-agent06* was compiling Flink at that time: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2085&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=eb5f4d19-2d2d-5856-a4ce-acf5f904a994
{code}
2020-05-24T20:43:04.9430119Z [INFO] Building flink-hadoop-fs 1.11-SNAPSHOT
2020-05-24T20:43:04.9430595Z [INFO] ------------------------------------------------------------------------
2020-05-24T20:47:26.1287464Z [INFO] 
2020-05-24T20:47:26.1308319Z [INFO] --- maven-checkstyle-plugin:2.17:check (validate) @ flink-hadoop-fs ---
{code}
==> 4.5 minutes no output

*AlibabaCI003-agent07* was also compiling https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2085&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=b33fdd4f-3de5-542e-2624-5d53167bb672
{code}
2020-05-24T20:46:04.4727910Z [INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ flink-runtime_2.12 ---
2020-05-24T20:48:06.5131113Z [INFO] Nothing to compile - all classes are up to date
{code}
==> 2 minutes no output

*AlibabaCI003-agent01* (test_cron_scala212 tests)
{code}
2020-05-24T20:44:28.3630097Z [INFO] Building flink-hadoop-fs 1.11-SNAPSHOT
2020-05-24T20:44:28.3630832Z [INFO] ------------------------------------------------------------------------
2020-05-24T20:47:12.7348902Z [INFO] 
2020-05-24T20:47:12.7350119Z [INFO] --- maven-checkstyle-plugin:2.17:check (validate) @ flink-hadoop-fs ---
{code}
==> 3.6 minutes no output

*AlibabaCI003-agent08* (test_cron_jdk11 core)
{code}
2020-05-24T20:46:15.9185266Z [INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ flink-runtime_2.11 ---
2020-05-24T20:48:09.1374085Z [INFO] Nothing to compile - all classes are up to date
{code}
=> 2 min

*AlibabaCI003-agent05*: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2084&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=71a2d105-6e31-43b0-a3c1-093e49c4fc84
{code}
2020-05-24T20:45:03.0317850Z [INFO] Building flink-runtime 1.12-SNAPSHOT
2020-05-24T20:45:03.0318385Z [INFO] ------------------------------------------------------------------------
2020-05-24T20:48:17.1772639Z [INFO] 
2020-05-24T20:48:17.1774062Z [INFO] --- git-commit-id-plugin:4.0.0:revision (get-the-git-infos) @ flink-runtime_2.11 ---
{code}
=> 3 min

*AlibabaCI003-agent02*: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2084&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=55a6756c-4b75-47ef-a21a-004b6fe3c810
{code}
2020-05-24T20:46:55.8276647Z Starting null#executeTest[statebackend type =ROCKSDB_FULLY_ASYNC].
2020-05-24T20:47:09.7124370Z Finished null#executeTest[statebackend type =ROCKSDB_FULLY_ASYNC].
{code}
=> 15 seconds

Out of the 8 builders:
1 Running our timed-out test
1 running some rocksdb-statebackend test
6 compiling (3 downloading dependencies)

We observe ""no output"" for 4 seconds to 270 seconds (4.5 minutes). I don't think that the underlying infrastructure was undergoing some kind of migration.

I suspect the issue more on the OS level. I will continue investigating ...;;;","16/Jun/20 06:56;trohrmann;Given these observations, does it make sense to even use the Alibaba hardware to run our tests?;;;","16/Jun/20 07:39;rmetzger;I don't have hard proof that the hardware is the issue. There were tests that we responsive during that time. If ALL tests had a 4 minute blackout during that time, I would blame the underlying hardware.
But right now my suspicion is the software we are running on the machine. As a first step, I will do some machine maintenance (update all packages to the latest version, reboot).
I will also see if I can get some support within Alibaba. I have also considered asking on StackOverflow. ;;;","16/Jun/20 09:23;rmetzger;One machine (after rebooting it) logged in {{dmesg}} 
{code}
[ 1801.218157] list passed to list_sort() too long for efficiency
[ 1802.777858] sched: RT throttling activated
{code}
... there might be something going on with the linux scheduler;;;","16/Jun/20 11:38;trohrmann;This looks promising :-);;;","17/Jun/20 19:38;rmetzger;I have now update all 8 CI servers to use the latest dependencies. It's very unlikely that this fixes the issues, but we can probably not go and ask on StackOverflow or a CentOS forum if we are not running up to date software :);;;","18/Jun/20 11:09;rmetzger;I'm closing the ticket for now. Let's reopen it if we see further cases.;;;","18/Jun/20 17:20;rmetzger;I had this test fail on an Azure-hosted virtual machine (which is good, because it means our Alibaba-CI machines are not completely broken): https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8189&view=logs&j=70ad9b63-500e-5dc9-5a3c-b60356162d7e&t=944c7023-8984-5aa2-b5f8-54922bd90d3a;;;","18/Jun/20 18:26;rmetzger;But this time I guess it's a different error:
{code}
6:07:07,950 [flink-akka.actor.default-dispatcher-7] WARN  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Slot offering to JobManager failed. Freeing the slots and returning them to the ResourceManager.
java.lang.NullPointerException: null
	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.maybeRemapOrphanedAllocation(SlotPoolImpl.java:599) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.tryFulfillSlotRequestOrMakeAvailable(SlotPoolImpl.java:564) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.offerSlot(SlotPoolImpl.java:701) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.offerSlots(SlotPoolImpl.java:625) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.offerSlots(JobMaster.java:541) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_242]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_242]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.11-2.5.21.jar:2.5.21]
16:07:07,977 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=0.5000000000000000, taskHeapMemory=64.000mb (67108864 bytes), taskOffHeapMemory=0 bytes, managedMemory=2.000mb (2097152 bytes), networkMemory=1.563mb (1638400 bytes)}, allocationId: 4dcfe78bb09fcf1117bd0be11c039df9, jobId: 00a771c28c805577994e752b25bef01c).
{code}
The method with the null pointer has been added yesterday, as part of FLINK-17019 (CC [~zhuzh]);;;","19/Jun/20 05:13;zhuzh;Thanks for reporting this [~rmetzger].
Given that it is a different cause than the original one, FLINK-18372 is opened to track and fix it.;;;","19/Jun/20 07:53;trohrmann;Closing this issue again because the newly reported problem is tracked under FLINK-18372.

[~rmetzger] I think we should not reuse tickets if the cause of a failure is different.;;;","18/Oct/22 06:51;mapohl;I'm reopening the issue. This test caused [this build failure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42038&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=10996] (not exclusively).

I'm attaching the test-related logs to the ticket. We might want to reconsider removing this test if we have another e2e test covering this usecase (see [Till's comment above|https://issues.apache.org/jira/browse/FLINK-15661?focusedCommentId=17135670&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17135670]).;;;","21/Feb/23 12:47;mapohl;Closing this one again in favor of FLINK-31168. It looks like this is the same error. But it's hard to verify because the builds are already cleaned up on the Azure;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redundant AllocationID verification for allocateSlot in TaskSlotTable,FLINK-15660,13280363,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,tedxia,tedxia,19/Jan/20 07:42,08/Jan/21 10:33,13/Jul/23 08:07,08/Jan/21 10:33,1.10.0,,,,,1.13.0,,,,Runtime / Coordination,,,,,0,,,,,"!image-2020-01-19-15-46-42-664.png!

 

In function TaskSlotTable::allocateSlot, first we will check whether allocationId is exist, when exist we will refused this allocation, this was introduced by FLINK-14589 . But in -FLINK-14189-, when allocationId exist, we think this is valid, which is contradictory with the first check.

 The code is following:

[https://github.com/apache/flink/blob/310452e800355f0dcc4bc9dd26e9cecba263f3d6/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/slot/TaskSlotTable.java#L261]",,azagrebin,guoyangze,tedxia,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/20 07:46;tedxia;image-2020-01-19-15-46-42-664.png;https://issues.apache.org/jira/secure/attachment/12991311/image-2020-01-19-15-46-42-664.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 08 10:33:20 UTC 2021,,,,,,,,,,"0|z0anog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/20 08:33;xtsong;Thanks for reporting this issue, [~tedxia].

This is indeed inconsistency caused by concurrent efforts on FLINK-14589 and FLINK-14189. To my understanding, the root contradiction is what should {{TaskSlotTable::allocateSlot}} return for duplicated allocation id when other information (job id, slot index, etc.) is consistent. I think both returning true and false should work, as long as the contract of {{TaskSlotTable::allocateSlot}} is consistent.

The good news is that, this inconsistent logic does not cause any problem at the moment. All we need now is a code clean-up without any behavior changes.

Please help double check on this, [~azagrebin]. Thx~;;;","31/Jan/20 10:37;chesnay;Ping [~azagrebin];;;","28/Aug/20 09:53;azagrebin;I agree one of the highlighted 'if's is probably redundant. The second 'else if' will never be executed atm. I believe the first 'if' is already covered by the next 'if/else if'. In a long term, the next 'if/else if'  makes more sense for both static and dynamic covering -FLINK-14589- as well. Therefore, I would remove the first 'if'.;;;","28/Aug/20 10:12;chesnay;The code in question will likely be refactored once we have finalized FLIP-138 and it's follow-ups.;;;","08/Jan/21 10:33;xtsong;Subsumed by FLINK-20837.

Fixed via
* master (1.13): c6443a501ef26ebc0c30ddb630e75926757a12d3

Not fixing for previous major releases, since the dynamic slot feature were not activated.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The same sql run in a streaming environment producing a Exception, but a batch env can run normally.",FLINK-15658,13280358,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,xiaojin.wy,xiaojin.wy,19/Jan/20 06:52,04/Feb/20 08:43,13/Jul/23 08:07,04/Feb/20 08:43,1.10.0,,,,,1.10.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"*summary:*
The same sql can run in a batch environment normally,  but in a streaming environment there will be a exception like this:
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [f1]





*The sql is:*

CREATE TABLE `tenk1` (
	unique1 int,
	unique2 int,
	two int,
	four int,
	ten int,
	twenty int,
	hundred int,
	thousand int,
	twothousand int,
	fivethous int,
	tenthous int,
	odd int,
	even int,
	stringu1 varchar,
	stringu2 varchar,
	string4 varchar
) WITH (
	'connector.path'='/daily_regression_test_stream_postgres_1.10/test_join/sources/tenk1.csv',
	'format.empty-column-as-null'='true',
	'format.field-delimiter'='|',
	'connector.type'='filesystem',
	'format.derive-schema'='true',
	'format.type'='csv'
);

CREATE TABLE `int4_tbl` (
	f1 INT
) WITH (
	'connector.path'='/daily_regression_test_stream_postgres_1.10/test_join/sources/int4_tbl.csv',
	'format.empty-column-as-null'='true',
	'format.field-delimiter'='|',
	'connector.type'='filesystem',
	'format.derive-schema'='true',
	'format.type'='csv'
);

select a.f1, b.f1, t.thousand, t.tenthous from
  tenk1 t,
  (select sum(f1)+1 as f1 from int4_tbl i4a) a,
  (select sum(f1) as f1 from int4_tbl i4b) b
where b.f1 = t.thousand and a.f1 = b.f1 and (a.f1+b.f1+999) = t.tenthous;





","*Input data:*
tenk1 is:
4773|9990|1|1|3|13|73|773|773|4773|4773|146|147|PBAAAA|GUOAAA|OOOOxx
4093|9991|1|1|3|13|93|93|93|4093|4093|186|187|LBAAAA|HUOAAA|VVVVxx
6587|9992|1|3|7|7|87|587|587|1587|6587|174|175|JTAAAA|IUOAAA|AAAAxx
6093|9993|1|1|3|13|93|93|93|1093|6093|186|187|JAAAAA|JUOAAA|HHHHxx
429|9994|1|1|9|9|29|429|429|429|429|58|59|NQAAAA|KUOAAA|OOOOxx
5780|9995|0|0|0|0|80|780|1780|780|5780|160|161|IOAAAA|LUOAAA|VVVVxx
1783|9996|1|3|3|3|83|783|1783|1783|1783|166|167|PQAAAA|MUOAAA|AAAAxx
2992|9997|0|0|2|12|92|992|992|2992|2992|184|185|CLAAAA|NUOAAA|HHHHxx
0|9998|0|0|0|0|0|0|0|0|0|0|1|AAAAAA|OUOAAA|OOOOxx
2968|9999|0|0|8|8|68|968|968|2968|2968|136|137|EKAAAA|PUOAAA|VVVVxx

int4_tbl is:
0
123456
-123456
2147483647
-2147483647

*The sql-client configuration is :*

execution:
  planner: blink
  type: batch",jark,libenchao,xiaojin.wy,,,,,,,,,,,,,,,,,,,"wuchong commented on pull request #11011: [FLINK-15658][table-planner-blink] Fix duplicate field names exception when join on the same key multiple times
URL: https://github.com/apache/flink/pull/11011
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The exmaple raised in the JIRA that batch can run successfully, but streaming will fail. The root cause is that the join key may be duplicate and the constructed key RowType will have duplicate field names.
   
   ## Brief change log
   
   We do not provide field names for the result key type, because we may have duplicate key fields and the field names may conflict.
   
   ## Verifying this change
   
   Added an IT case to verify this, the IT case will fail without this fix. 
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Feb/20 06:36;githubbot;600","wuchong commented on pull request #11011: [FLINK-15658][table-planner-blink] Fix duplicate field names exception when join on the same key multiple times
URL: https://github.com/apache/flink/pull/11011
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Feb/20 08:39;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 04 08:43:20 UTC 2020,,,,,,,,,,"0|z0annc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/20 08:43;jark;Fixed in 1.11.0: de7440e5b625ef609ce7ad6631abc507b8ca109d
1.10.0: c84b754b60e62f106adda47e91bfeec5ae5edeb5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JarHandlerTest.testRunJar fails on travis,FLINK-15651,13280343,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,zhuzh,zhuzh,19/Jan/20 03:30,03/Feb/20 14:41,13/Jul/23 08:07,29/Jan/20 08:37,1.10.0,,,,,1.10.0,,,,Runtime / REST,,,,,0,pull-request-available,,,,"The test JarHandlerTest.testRunJar fails on travis with error below:


{code:java}
09:48:58.828 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.061 s <<< FAILURE! - in org.apache.flink.runtime.webmonitor.handlers.JarHandlerTest
09:48:58.829 [ERROR] testRunJar(org.apache.flink.runtime.webmonitor.handlers.JarHandlerTest)  Time elapsed: 0.2 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: a string containing ""ProgramInvocationException""
     but: was ""[Service temporarily unavailable due to an ongoing leader election. Please refresh.]""
	at org.apache.flink.runtime.webmonitor.handlers.JarHandlerTest.runTest(JarHandlerTest.java:125)
	at org.apache.flink.runtime.webmonitor.handlers.JarHandlerTest.testRunJar(JarHandlerTest.java:74)
{code}

Seems the cause is the client sends request before the leader election is done.

https://api.travis-ci.com/v3/job/276751147/log.txt",,lining,zhuzh,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #10933: [FLINK-15651][tests] Refactor JarHandlerTest
URL: https://github.com/apache/flink/pull/10933
 
 
   Refactors the JarHandlerTest to test directly against the handlers instead of firing up an entire cluster.
   
   - reuse handler setup from `JarSubmissionITCase`
   - removed the jar creation within the test since it fails on windows
   - added additional assertion that an exception is thrown
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jan/20 14:44;githubbot;600","zentol commented on pull request #10933: [FLINK-15651][tests] Refactor JarHandlerTest
URL: https://github.com/apache/flink/pull/10933
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jan/20 08:35;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15504,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 29 08:37:02 UTC 2020,,,,,,,,,,"0|z0ank0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/20 08:37;chesnay;master: 23ff230ee18087760e763337f50bdb63e57fbd7b
1.10: 0709bf1039078867c9165806a70be7e8429cd2c1 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
releasing/create_release_branch.sh does not set version in flink-python/pyflink/version.py,FLINK-15638,13280147,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hequn8128,gjy,gjy,17/Jan/20 14:38,04/Feb/20 08:44,13/Jul/23 08:07,18/Jan/20 02:46,1.10.0,,,,,1.10.0,1.9.3,,,Release System,,,,,0,pull-request-available,,,,"{{releasing/create_release_branch.sh}} does not set the version in {{flink-python/pyflink/version.py}}. Currently the version.py contains:

{noformat}
__version__ = ""1.10.dev0"" 
{noformat}

{{setup.py}} will replace .dev0 with -SNAPSHOT and tries to find the respective flink distribution in the flink-dist/target, which will not exist.
",,gjy,hequn8128,,,,,,,,,,,,,,,,,,,,"hequn8128 commented on pull request #10892: [FLINK-15638][release][python] Change version of pyflink to the release version when creating release branch
URL: https://github.com/apache/flink/pull/10892
 
 
   ## What is the purpose of the change
   
   This pull request sets the version in flink-python/pyflink/version.py when creating release branch.
   
   
   ## Brief change log
   
     - Sets the version in flink-python/pyflink/version.py when creating release branch
   
   ## Verifying this change
   
    - This change is a trivial rework / code cleanup without any test coverage.
    - Test manually by creating the release branch and install pyflink on local. 
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jan/20 02:20;githubbot;600","hequn8128 commented on pull request #10892: [FLINK-15638][release][python] Change version of pyflink to the release version when creating release branch
URL: https://github.com/apache/flink/pull/10892
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jan/20 02:22;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 18 02:46:34 UTC 2020,,,,,,,,,,"0|i3jpe4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/20 14:42;gjy;cc: [~sunjincheng121] [~hequn8128];;;","18/Jan/20 02:46;hequn8128;Fixed 
in 1.10.0 via 1416b0f9dac37c11c4a17c60a15747b25c6b8e3e
in 1.11.0 via 1ffa2990d5fdb32e9100bd7a18b6dae1630563a5
in 1.9.3 via fe15e284247b2ddabfb89a613f2e907443ccc548;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zookeeper HA service could not work for active kubernetes integration,FLINK-15632,13280108,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,wangyang0918,wangyang0918,17/Jan/20 10:19,22/Jan/20 03:56,13/Jul/23 08:07,22/Jan/20 03:56,,,,,,1.10.0,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"It will be some different, if we want to support HA for active Kubernetes integration.
 # The K8s service is designed for accessing the jobmanager out of K8s cluster. So Flink client will not use HA service to retrieve address of jobmanager. Instead, it always use Kubernetes service to contact with jobmanager via rest client. 
 # The Kubernetes DNS creates A and SRV records only for Services. It doesn't generate pods' A records. So the ip address, not hostname, will be used as jobmanager address.

 

All other behaviors will be same as Zookeeper HA for standalone and Yarn.

To fix this problem, we just need some minor changes to {{KubernetesClusterDescriptor}} and {{KubernetesSessionEntrypoint}}.",,eleanore0102,kezhuw,wangyang0918,zhuzh,,,,,,,,,,,,,,,,,,"wangyang0918 commented on pull request #10898: [FLINK-15632][kubernetes] Make zookeeper HA service could work for active kubernetes integration
URL: https://github.com/apache/flink/pull/10898
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   There will be the following differences if we want to support HA for active Kubernetes integration.
   1. The K8s service is designed for accessing the jobmanager out of K8s cluster. So Flink client will not use HA service to retrieve address of jobmanager. Instead, it always use Kubernetes service to contact with jobmanager via rest client.
   2. The Kubernetes DNS creates A and SRV records only for Services. It doesn't generate pods' A records. So the ip address, not hostname, will be used as jobmanager address.
   
   All other behaviors will be same as Zookeeper HA for standalone and Yarn.
   
   
   ## Brief change log
   
   * Always use Kubernetes service for client to contact with jobmanager in `KubernetesClusterDescriptor`
   * Set ip address to `jobmanager.rpc.address` in `KubernetesSessionClusterEntrypoint`
   
   
   ## Verifying this change
   
   * Add a new unit test `testDeployHighAvailabilitySessionCluster`
   * Test on a real Kubernetes cluster with zookeeper HA
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jan/20 09:26;githubbot;600","zhuzhurk commented on pull request #10898: [FLINK-15632][kubernetes] Make zookeeper HA service could work for active kubernetes integration
URL: https://github.com/apache/flink/pull/10898
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/20 03:45;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 22 03:56:13 UTC 2020,,,,,,,,,,"0|z0am3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/20 06:31;wangyang0918;[~zhuzh] Could you assign this ticket to me? I will try to give a fix and hope to get in the release-1.10.;;;","19/Jan/20 07:33;zhuzh;Thanks for reporting this issue [~fly_in_gis].
I have assigned it to you.;;;","22/Jan/20 03:56;zhuzh;Fixed via:

master:
003ceb9a78f90803b2a765e007e4487eead00e06
be4b713c4de8ecd6ba52fe930dfbea6b380416dc
cd9836c43bc615bd8a4475920df607e37cfd89ef

release-1.10:
dbfdfc469c8983682c8bb398e9511b94a45c3cf4
5bc7bdd4efb887ecea03f46b9df316fc6f474216
e1436f8172d61d610d892a3f4e24ab137d3e03b6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot use generic types as the result of an AggregateFunction in Blink planner,FLINK-15631,13280094,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,dwysakowicz,dwysakowicz,17/Jan/20 09:30,21/Jan/20 03:22,13/Jul/23 08:07,21/Jan/20 03:22,1.10.0,1.9.0,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"It is not possible to use a GenericTypeInfo for a result type of an {{AggregateFunction}} in a retract mode with state cleaning disabled.

{code}

  @Test
  def testGenericTypes(): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val setting = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()
    val tEnv = StreamTableEnvironment.create(env, setting)
    val t = env.fromElements(1, 2, 3).toTable(tEnv, 'a)

    val results = t
      .select(new GenericAggregateFunction()('a))
      .toRetractStream[Row]

    val sink = new TestingRetractSink
    results.addSink(sink).setParallelism(1)
    env.execute()
  }

class RandomClass(var i: Int)

class GenericAggregateFunction extends AggregateFunction[java.lang.Integer, RandomClass] {
  override def getValue(accumulator: RandomClass): java.lang.Integer = accumulator.i

  override def createAccumulator(): RandomClass = new RandomClass(0)

  override def getResultType: TypeInformation[java.lang.Integer] = new GenericTypeInfo[Integer](classOf[Integer])

  override def getAccumulatorType: TypeInformation[RandomClass] = new GenericTypeInfo[RandomClass](
    classOf[RandomClass])

  def accumulate(acc: RandomClass, value: Int): Unit = {
    acc.i = value
  }

  def retract(acc: RandomClass, value: Int): Unit = {
    acc.i = value
  }

  def resetAccumulator(acc: RandomClass): Unit = {
    acc.i = 0
  }
}
{code}

The code above fails with:

{code}
Caused by: java.lang.UnsupportedOperationException: BinaryGeneric cannot be compared
	at org.apache.flink.table.dataformat.BinaryGeneric.equals(BinaryGeneric.java:77)
	at GroupAggValueEqualiser$17.equalsWithoutHeader(Unknown Source)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:177)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:85)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:170)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.lang.Thread.run(Thread.java:748)
{code}

This is related to FLINK-13702",,dwysakowicz,jark,lzljs3620320,,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #10896: [FLINK-15631][table-planner-blink] Fix equals code generation for raw and timestamp type
URL: https://github.com/apache/flink/pull/10896
 
 
   
   ## What is the purpose of the change
   
   - equals for generic type not work
   - when close idle state, aggregation not work for generic type and timestamp type
   
   ## Brief change log
   
   - Fix equals code generation for raw type in ScalarOperatorGens
   - Fix raw and timestamp type in EqualiserCodeGenerator
   
   ## Verifying this change
   
   - ScalarFunctionsTest.testEquality
   - EqualiserCodeGeneratorTest
   - AggregateITCase.testGenericTypesWithoutStateClean
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jan/20 05:41;githubbot;600","wuchong commented on pull request #10896: [FLINK-15631][table-planner-blink] Fix equals code generation for raw and timestamp type
URL: https://github.com/apache/flink/pull/10896
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jan/20 03:21;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 21 03:22:19 UTC 2020,,,,,,,,,,"0|z0am0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/20 09:32;dwysakowicz;[~lzljs3620320] Could you have a look at this? Apparently it was not safe to remove the implementation of {{BinaryGeneric#equals}} as we discussed in FLINK-13702.;;;","17/Jan/20 12:30;jark;Is it the root cause of FLINK-15619? ;;;","17/Jan/20 12:59;dwysakowicz;[~jark] Not sure, but I doubt it. I am debugging a different bug now, so I am not sure if I have time to check it. Can try to check once I find some time.;;;","19/Jan/20 04:01;lzljs3620320;[~dwysakowicz] Good catch! I think this is bug. We need let it safe to remove the implementation of {{BinaryGeneric#equals}}.

I reproduced bug by closing {{IdleStateRetentionTime}}, CC: [~jark] , looks like we need test it because our it case must open {{IdleStateRetentionTime}}. I think there are three problem:
 # Equals code generation not implement generic type.
 # Aggregation {{EqualiserCodeGenerator}} need use code generation util {{ScalarOperatorGens}} to generate codes instead of another implementation version.
 # Aggregation {{EqualiserCodeGenerator}} handles timestamp type in a wrong way, timestamp internal format is not primitive anymore.

Can you assign this to me?;;;","19/Jan/20 05:53;lzljs3620320;Created FLINK-15655 to refactor current confused codes.;;;","21/Jan/20 03:22;jark;1.11.0: a22b9d8ff92764a4437347bb6d2deb10e9a4e054
1.10.0: ce72e50451cfd5ab7e134b5e3d1ac29228d8a763;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Buildling flink-python with maven profile docs-and-source fails,FLINK-15623,13279934,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,gjy,gjy,16/Jan/20 16:48,22/Jun/21 13:55,13/Jul/23 08:07,30/Jan/20 19:03,1.10.0,,,,,1.10.0,,,,Build System,,,,,0,pull-request-available,,,,"*Description*
Building flink-python with maven profile {{docs-and-source}} fails due to checkstyle violations. 

*How to reproduce*

Running

{noformat}
mvn clean install -pl flink-python -Pdocs-and-source -DskipTests -DretryFailedDeploymentCount=10
{noformat}

should fail with the following error

{noformat}
[...]
[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8343] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.
[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8344] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.
[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8345] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.
[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8346] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.
[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8347] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.
[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8348] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.
[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8349] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.
[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8350] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 18.046 s
[INFO] Finished at: 2020-01-16T16:44:01+00:00
[INFO] Final Memory: 158M/2826M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (validate) on project flink-python_2.11: You have 7603 Checkstyle violations. -> [Help 1]
[ERROR]
{noformat}",rev: 91d96abe5f42bd088a326870b4885d79611fccb5,arvid heise,gjy,hequn8128,sunjincheng121,,,,,,,,,,,,,,,,,,"sunjincheng121 commented on pull request #10876: [FLINK-15623][build] Exclude code style check of generation code in P…
URL: https://github.com/apache/flink/pull/10876
 
 
   
   ## What is the purpose of the change
   
   *In this PR will exclude code style check of generation code in PyFlink.*
   
   
   ## Brief change log
     - Exclude code style check of generation code in `suppressions.xml`.
   
   ## Verifying this change
   This change is already covered by existing tests, such as *(please describe tests)*.
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/20 03:19;githubbot;600","hequn8128 commented on pull request #10876: [FLINK-15623][build] Exclude code style check of generation code in P…
URL: https://github.com/apache/flink/pull/10876
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/20 06:04;githubbot;600","zentol commented on pull request #10886: [FLINK-15623][checkstyle] Exclude target directory
URL: https://github.com/apache/flink/pull/10886
 
 
   Excludes the entire `target` directory from checkstyle. 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/20 10:28;githubbot;600","zentol commented on pull request #10886: [FLINK-15623][checkstyle] Exclude target directory
URL: https://github.com/apache/flink/pull/10886
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jan/20 19:03;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 30 19:03:55 UTC 2020,,,,,,,,,,"0|z0al14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/20 18:26;chesnay;The maven-source-plugin, which runs as part of the {{release}} profile, re-executes the early maven life-cycle, including the checkstyle run. Unfortunately at this point the python module already generated the classes which violate our checkstyle rules.

I quickly looked at the protobuf-generator and source-plugin documentation and could not find a solution; so we'll likely just have to exclude them from checkstyle.;;;","16/Jan/20 19:32;gjy;I think it's reasonable to exclude generated sources from checkstyle.;;;","16/Jan/20 20:16;arvid;In general, I'd only whitelist stuff in `src/`.;;;","17/Jan/20 02:54;sunjincheng121;Agree with [~chesnay] 's analysis. The goal ""source:jar"" of maven-source-plugin will invoke the phase [generate-sources|https://maven.apache.org/plugins/maven-source-plugin/jar-mojo.html]. It will firstly execute the early phase ""validate"" during which maven-checkstyle-plugin will be executed. At this point, the generated sources are already generated and so checkstyle check failed. I think it makes sense to exclude the generated sources from the check style check.;;;","17/Jan/20 06:06;hequn8128;Fixed 
in 1.11.0 via 9575b91880e976923f4a0fc30bd9fe76e0f7e401
in 1.10.0 via 66b0d815a449e340ee7eeb645d446dda42471ed5 ;;;","17/Jan/20 09:40;gjy;Why don't we exclude everything under {{/generated-sources}}? ;;;","17/Jan/20 10:03;chesnay;We could also just exclude everything under /target; I don't see a reason to check files in there anyway..;;;","17/Jan/20 12:37;gjy;Re-opening because [~chesnay] opened a new PR.;;;","17/Jan/20 12:44;chesnay;[~gjy] Should we downgrade the priority since my PR is mostly a cleanup, but the current fix is working as intended?;;;","17/Jan/20 17:34;gjy;We can downgrade the priority.;;;","30/Jan/20 19:03;chesnay;Small rework to exclude everything under target/ from checkstyle:
master: e8f4f67c4ef45564d089dfe152b216165618d8bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docs: wrong guarantees stated for the file sink,FLINK-15615,13279798,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,16/Jan/20 10:29,17/Jan/20 10:14,13/Jul/23 08:07,17/Jan/20 10:14,1.10.0,1.9.1,,,,1.10.0,1.9.2,,,Documentation,,,,,0,pull-request-available,,,,"[https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/guarantees.html]

Instead of ""at least once"" it should be ""exactly once"".",,kkl0u,roman,,,,,,,,,,,,,,,,,,,,"rkhachatryan commented on pull request #10869: [FLINK-15615][docs] fix File Sink consistency guarantees
URL: https://github.com/apache/flink/pull/10869
 
 
   At https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/guarantees.html
   
   Instead of ""at least once"" should be ""exactly once"".
   
   This change is a trivial rework without any test coverage.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/20 10:59;githubbot;600","kl0u commented on pull request #10869: [FLINK-15615][docs] fix File Sink consistency guarantees
URL: https://github.com/apache/flink/pull/10869
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/20 10:09;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 17 10:14:27 UTC 2020,,,,,,,,,,"0|z0akko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/20 10:14;kkl0u;Fixed on master with 8d74bffdf86f077896fec76db7de27b0ebdd5f4b

on 1.10 with 2a4f649d8805153c092f07777e41fda0005e9a70

and on 1.9 with ddf1c218b752392f41bc51982b7db602ffc7e7d0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blink planner does not respect the precision when casting timestamp to varchar,FLINK-15602,13279584,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,docete,dwysakowicz,dwysakowicz,15/Jan/20 13:01,23/Jan/20 15:32,13/Jul/23 08:07,23/Jan/20 15:32,1.10.0,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"According to SQL 2011 Part 2 Section 6.13 General Rules 11) d)

{quote}
If SD is a datetime data type or an interval data type then let Y be the shortest character string that
conforms to the definition of <literal> in Subclause 5.3, “<literal>”, and such that the interpreted value
of Y is SV and the interpreted precision of Y is the precision of SD.
{quote}

That means:
{code}
select cast(cast(TO_TIMESTAMP('2014-07-02 06:14:00', 'YYYY-MM-DD HH24:mm:SS') as TIMESTAMP(0)) as VARCHAR(256)) from ...;
// should produce
// 2014-07-02 06:14:00

select cast(cast(TO_TIMESTAMP('2014-07-02 06:14:00', 'YYYY-MM-DD HH24:mm:SS') as TIMESTAMP(3)) as VARCHAR(256)) from ...;
// should produce
// 2014-07-02 06:14:00.000

select cast(cast(TO_TIMESTAMP('2014-07-02 06:14:00', 'YYYY-MM-DD HH24:mm:SS') as TIMESTAMP(9)) as VARCHAR(256)) from ...;
// should produce
// 2014-07-02 06:14:00.000000000
{code}

One possible solution would be to propagate the precision in {{org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens#localTimeToStringCode}}. If I am not mistaken this problem was introduced in [FLINK-14599]",,dwysakowicz,godfreyhe,jark,leonard,lzljs3620320,twalthr,,,,,,,,,,,,,,,,"docete commented on pull request #10877: [FLINK-15602][table-planner-blink] Padding TIMESTAMP type to respect …
URL: https://github.com/apache/flink/pull/10877
 
 
   …the precision when casting timestamp to varchar
   
   ## What is the purpose of the change
   
   According to SQL 2011 Part 2 Section 6.13 General Rules 11) d)
   
   > If SD is a datetime data type or an interval data type then let Y be the shortest character string that
   > conforms to the definition of <literal> in Subclause 5.3, “<literal>”, and such that the interpreted value of Y is SV and the interpreted precision of Y is the precision of SD.
   
   This PR padding the TIMESTAMP type to respect the precision when casting timestamp to varchar. 
   
   ## Brief change log
   
   - c53d150 Padding TIMESTAMP type to respect precision
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *TempralTypeTests*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (**yes** / no)
     - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/20 06:16;githubbot;600","wuchong commented on pull request #10877: [FLINK-15602][table-planner-blink] Padding TIMESTAMP type to respect …
URL: https://github.com/apache/flink/pull/10877
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jan/20 13:42;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,FLINK-14505,,,,,,FLINK-14599,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 23 15:32:05 UTC 2020,,,,,,,,,,"0|z07v7w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/20 13:07;dwysakowicz;cc [~jark] [~docete];;;","16/Jan/20 03:32;docete;Hi [~dwysakowicz] [~twalthr] altherI investigated the behavior of popular DBMS and found: PostgreSQL DO NOT pad '0' and Oracle/MS SQL pad '0' (MYSQL would pad '0' for TIMESTAMP type and not pad '0' for DATETIME type). And, hive/spark would not pad '0' too.

What's your opinion about the padding behavior? 

 ;;;","16/Jan/20 09:05;dwysakowicz;[~docete] Thank you for checking it.

I checked additionally INTERVAL and DECIMAL types and 
* SQL Server pads both types, 
* Oracle pads INTERVAL, does not pad DECIMAL, 
* PostgreSQL pads DECIMAL and does not pad INTERVAL.
Personally I am in favor of padding. 

[~twalthr] gave also a nice argument that padded values look nicer when there are multiple values being printed:

{code}
// Padded version
2014-07-02 06:14:00.010000000
2014-07-02 06:14:00.001000000
2014-07-02 06:14:00.001000001

// Not padded

2014-07-02 06:14:00.01
2014-07-02 06:14:00.001
2014-07-02 06:14:00.001000001
{code}

MySQL and Hive/Spark are not known to follow the SQL too closely so I would not take them as good examples.

If we decide not to pad the results we should fix the {{sql client kafka tests}};;;","16/Jan/20 09:23;docete;[~dwysakowicz] [~twalthr] [~jark]

Let's padding the TIMESTAMP type in both planner. I will file a PR soon. 
 ;;;","16/Jan/20 09:27;lzljs3620320;+1 to pad. JDK LocalDateTime also pad to second, millsecond, microsecond and nanos.;;;","16/Jan/20 09:43;twalthr;[~docete] I would leave the old planner untouched to not break existing setups. For the Blink planner, we should definitely fix this inconsistency and pad timestamps, decimal, and intervals.;;;","16/Jan/20 11:50;jark;+1 for padding.;;;","17/Jan/20 02:42;docete;[~twalthr] AFAIK we have padded DECIMAL type and intervals in Blink planner.

 ;;;","23/Jan/20 15:32;jark;Fixed in 1.11.0: 84e76b0149187be3e3237a004c3f08a515168315
1.10.0: 1290808c9b1ad32ccbc173280542575831f6d2b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL client requires both legacy and blink planner to be on the classpath,FLINK-15599,13279554,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,dwysakowicz,dwysakowicz,15/Jan/20 10:23,20/Jan/20 15:52,13/Jul/23 08:07,20/Jan/20 15:52,1.10.0,,,,,1.10.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"Sql client uses directly some of the internal classes of the legacy planner, thus it does not work with only the blink planner on the classpath.

The internal class that's being used is {{org.apache.flink.table.functions.FunctionService}}

This dependency was introduced in FLINK-13195",,dwysakowicz,godfreyhe,jark,leonard,lzljs3620320,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #10878: [FLINK-15599][table] SQL client requires both legacy and blink planner to be on the classpath
URL: https://github.com/apache/flink/pull/10878
 
 
   
   ## What is the purpose of the change
   
   Sql client uses directly some of the internal classes of the legacy planner, thus it does not work with only the blink planner on the classpath.
   The internal class that's being used is org.apache.flink.table.functions.FunctionService
   This dependency was introduced in FLINK-13195
   
   ## Brief change log
   
   - Port FunctionService and FunctionServiceTest to table-common
   - check explicitly for an instance of Blink's executor by PlannerFactory
   
   ## Verifying this change
   
   Manual testing
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/20 06:19;githubbot;600","dawidwys commented on pull request #10878: [FLINK-15599][table] SQL client requires both legacy and blink planner to be on the classpath
URL: https://github.com/apache/flink/pull/10878
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jan/20 15:51;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,FLINK-13195,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 20 15:52:47 UTC 2020,,,,,,,,,,"0|z0aj2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/20 10:25;dwysakowicz;cc [~ykt836] [~danny0405];;;","15/Jan/20 10:37;dwysakowicz;I am not 100% if this is a must, but I think it would be good to be able to run the sql-client with only a single planner on the classpath. Otherwise it becomes harder to remove the legacy planner in the future.;;;","16/Jan/20 02:30;lzljs3620320;Thanks [~dwysakowicz] , Good catch! +1, looks like {{FunctionService}} is the only dependent of sql-cli to legacy planner.;;;","16/Jan/20 03:37;lzljs3620320;We can port {{FunctionService}} to table-common. If no one follows up, you can assign this to me.;;;","16/Jan/20 10:55;dwysakowicz;One more dependency I found is that we check explicitly for an instance of Blink's executor in {{org.apache.flink.table.client.gateway.local.ExecutionContext#createPipeline(java.lang.String, org.apache.flink.configuration.Configuration)}}. This also might be problematic.

I will assign this issue to you.

;;;","20/Jan/20 15:52;dwysakowicz;Fixed in:
master: fd2b8e5894d43100802dfa17b7a1ea7d1e505d63
1.10: ca1cacd3494232bb7f34327c829033160a86abcd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory accuracy loss in YarnClusterDescriptor may lead to deployment failure.,FLINK-15598,13279553,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xtsong,xtsong,xtsong,15/Jan/20 10:23,16/Jan/20 15:50,13/Jul/23 08:07,16/Jan/20 15:50,,,,,,1.10.0,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"Currently, YarnClusterDescriptor parses/derives TM process memory size from configuration, store it in ClusterSpecification and validate ClusterSpecification, then overwrite the memory size back to configuration.

This logic is unnecessary. The memory validation is already covered by creating TaskExecutorResourceSpec from configuration in TaskExecutorResourceUtils.

Moreover, the memory size is stored in MB in ClusterSpecification. The accuracy loss may lead to memory validation failure, which prevent the cluster from being deployed.",,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,"xintongsong commented on pull request #10863: [FLINK-15598][yarn] Memory accuracy loss in YarnClusterDescriptor may lead to deployment failure.
URL: https://github.com/apache/flink/pull/10863
 
 
   ## What is the purpose of the change
   
   This PR removes  memory validations and overwrites in YarnClusterDescriptor, which is unnecessary and may cause failures.
   
   ## Brief change log
   
   - df5f8e44c335cdd2b6f8eb6974af28d7bb856ae7: Hotfix to remove unsed env keys
   - da1ec060cbef03b127f6f763e14e6ac5c7f82ff7: Hotfix to remove unsed numberTaskManagers from ClusterSpecification
   - d924dfce471036d6bcd7cca5d0548d9b78a08ed1: Remove memory validation from YarnClusterDescriptor
   - cd199a2a3b3dba0c38c38df5db11278feabb544d: Remove memory overwriting from YarnClusterDescriptor
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jan/20 11:19;githubbot;600","tillrohrmann commented on pull request #10863: [FLINK-15598][yarn] Memory accuracy loss in YarnClusterDescriptor may lead to deployment failure.
URL: https://github.com/apache/flink/pull/10863
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/20 15:50;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,FLINK-15564,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 16 15:50:52 UTC 2020,,,,,,,,,,"0|z0aj2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/20 15:50;trohrmann;Fixed via

master:
7acc0e6e08e20dc4e7b0cb1b3218c4049c57bf67
12f7873db54cfbc5bf853d66ccd4093f9b749c9a

1.10.0:
7b6aa1a63f45742557efc5d0bef2db40ebcc683d
91d96abe5f42bd088a326870b4885d79611fccb5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming sql throw hive exception when it doesn't use any hive table,FLINK-15592,13279489,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,zjffdu,zjffdu,15/Jan/20 03:30,23/Jan/20 17:43,13/Jul/23 08:07,23/Jan/20 17:43,1.10.0,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"I use the following streaming sql to query a kafka table whose metadata is store in hive metastore via HiveCatalog. But it will throw hive related exception which is very confusing.

SQL
{code}
SELECT *
FROM (
   SELECT *,
     ROW_NUMBER() OVER(
       ORDER BY event_ts) AS rownum
   FROM source_kafka)
WHERE rownum <= 10
{code}

Exception
{code}
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. java.lang.reflect.InvocationTargetException
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:130)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:105)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:127)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:464)
	at org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:103)
	... 13 more
Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.flink.table.planner.functions.utils.HiveFunctionUtils.invokeGetResultType(HiveFunctionUtils.java:77)
	at org.apache.flink.table.planner.functions.utils.HiveAggSqlFunction.lambda$createReturnTypeInference$0(HiveAggSqlFunction.java:82)
	at org.apache.calcite.sql.SqlOperator.inferReturnType(SqlOperator.java:470)
	at org.apache.calcite.sql.SqlOperator.validateOperands(SqlOperator.java:437)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:303)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:219)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5600)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676)
	at org.apache.calcite.sql.SqlCallBinding.getOperandType(SqlCallBinding.java:237)
	at org.apache.calcite.sql.type.OrdinalReturnTypeInference.inferReturnType(OrdinalReturnTypeInference.java:40)
	at org.apache.calcite.sql.type.SqlTypeTransformCascade.inferReturnType(SqlTypeTransformCascade.java:54)
	at org.apache.calcite.sql.SqlOperator.inferReturnType(SqlOperator.java:470)
	at org.apache.calcite.sql.SqlOperator.validateOperands(SqlOperator.java:437)
	at org.apache.calcite.sql.SqlOverOperator.deriveType(SqlOverOperator.java:86)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5600)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676)
	at org.apache.calcite.sql.SqlAsOperator.deriveType(SqlAsOperator.java:133)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5600)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:479)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4105)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3389)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1008)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:968)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3122)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3104)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3376)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1008)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:968)
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:216)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:943)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:650)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:126)
	... 18 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.table.planner.functions.utils.HiveFunctionUtils.invokeGetResultType(HiveFunctionUtils.java:73)
	... 63 more
Caused by: org.apache.flink.table.functions.hive.FlinkHiveUDFException: Failed to get Hive result type from org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber
	at org.apache.flink.table.functions.hive.HiveGenericUDAF.getHiveResultType(HiveGenericUDAF.java:202)
	... 68 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Only COMPLETE mode supported for row_number function
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber$GenericUDAFAbstractRowNumberEvaluator.init(GenericUDAFRowNumber.java:100)
	at org.apache.flink.table.functions.hive.HiveGenericUDAF.init(HiveGenericUDAF.java:93)
	at org.apache.flink.table.functions.hive.HiveGenericUDAF.getHiveResultType(HiveGenericUDAF.java:196)
	... 68 more
ERROR   
Took 2 sec. Last updated by anonymous at January 15 2020, 11:12:16 AM.
{code}",,godfreyhe,jark,libenchao,lirui,lzljs3620320,phoenixjiangnan,twalthr,zjffdu,,,,,,,,,,,,,,"lirui-apache commented on pull request #10894: [FLINK-15592][hive] Add black list for Hive built-in functions
URL: https://github.com/apache/flink/pull/10894
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Add black list for Hive built-in functions
   
   
   ## Brief change log
   
     - Add black list for Hive built-in functions
     - Add test case
   
   
   ## Verifying this change
   
   New test
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jan/20 04:16;githubbot;600","bowenli86 commented on pull request #10894: [FLINK-15592][hive] Add black list for Hive built-in functions
URL: https://github.com/apache/flink/pull/10894
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jan/20 17:42;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15595,FLINK-15593,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 23 17:43:44 UTC 2020,,,,,,,,,,"0|z0aio8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/20 03:39;jark;cc [~lirui] [~bli], it seems that the ModuleManager resolve the {{ROW_NUMBER()}} as a Hive function. 
However, according to FLIP-68 the resolve order should be the moudle order which should be {{core}} and then {{hive}}. I guess it might be a bug in the resolve order? ;;;","15/Jan/20 03:59;lirui;[~zjffdu] Are you using hive module? Or is it possible a function named ""row_number"" exists in your HMS (you can verify this by calling HiveCatalog::listFunctions)?
cc [~phoenixjiangnan];;;","15/Jan/20 04:17;lzljs3620320;Hi [~jark], ROW_NUMBER should not be resolved in {{FunctionCatalogOperatorTable.lookupOperatorOverloads}}. Here should return empty, and calcite will treat it as an over function later. Then it works.

So maybe we should add black list or white list to Hive ModuleManager.;;;","15/Jan/20 04:23;lzljs3620320;CC: [~danny0405] What is the mechanism that calcite parses ROW_NUMBER?;;;","15/Jan/20 05:24;phoenixjiangnan;I checked BuiltInFunctionDefinitions and there's no udf called ""row_number"". that's why it's resolved to HiveModule, and this error may be due to that Hive aggregate udfs are using in streaming mode. I'll add doc to advice users to not use Hive aggregate udf in streaming mode in FLINK-15593.

Thus I think [~lzljs3620320] is right, and planner should probably resolve ""row_number"" to calcite, which may interfere with function resolution order.;;;","15/Jan/20 05:37;lzljs3620320;Create FLINK-15595 to solve module problem.

Keep this ticket, because we need black list in hive module too, user can change resolution order to be hive function first.;;;","15/Jan/20 05:58;lirui;[~phoenixjiangnan] row_number is defined in {{FlinkSqlOperatorTable}}. Therefore I think CoreModule shouldn't only check {{BuiltInFunctionDefinitions}} for ""core"" built-in functions. And FLINK-15593 is probably orthogonal to this issue.;;;","15/Jan/20 06:15;jark;Shouldn't we add ROW_NUMBER into {{BuiltInFunctionDefinitions}} and then this problem should be fixed? ;;;","15/Jan/20 06:28;lzljs3620320;> Shouldn't we add ROW_NUMBER into {{BuiltInFunctionDefinitions}} and then this problem should be fixed?

Hi [~jark]

Keep this ticket, because we need black list in hive module too, user can change resolution order to be hive function first. If we just add ROW_NUMBER into {{BuiltInFunctionDefinitions.}} Not work too.;;;","15/Jan/20 06:34;jark;I got what you mean [~lzljs3620320]. {{ROW_NUMBER}} and {{RANK}} and {{DENSE_RANK}} is declarative aggregates, not imperative aggregates, so they should always be interpreted by Flink, not matter what the module order is.;;;","17/Jan/20 04:33;lzljs3620320;[~lirui] Do you want to add black list to hive function module and take this ticket?;;;","17/Jan/20 04:34;lzljs3620320;Increased priority, since we don't have a good solution in FLINK-15595;;;","17/Jan/20 05:08;lirui;OK, [~jark] please assign this to me. Thanks.;;;","17/Jan/20 08:09;lirui;Following is the list of functions that are in {{FlinkSqlOperatorTable}} and *not* in {{BuiltInFunctionDefinitions}}. [~lzljs3620320] any suggestions which of them should be included in the black list?

{code}
[$literalchain, $scalar_query, $sum0, *, +, -, /, /int, <, <=, <>, =, >, >=, ascii, auxiliary_group, between asymmetric, between symmetric, case, char_length, character_length, chr, classifier, coalesce, convert_tz, current_date, current_time, current_timestamp, date_format, dayofmonth, dayofweek, dayofyear, decode, dense_rank, dot, encode, except, except all, exists, final, first, first_value, from_base64, from_unixtime, group_id, grouping, grouping_id, hash_code, hop, hop_end, hop_proctime, hop_rowtime, hop_start, hour, if, initcap, instr, intersect, intersect all, is distinct from, is false, is not distinct from, is not false, is not null, is not true, is not unknown, is null, is true, is unknown, is_alpha, is_decimal, is_digit, item, lag, last, last_value, lead, left, listagg, localtime, localtimestamp, locate, lower, match_proctime, match_rowtime, minute, month, multiset, next, not between asymmetric, not between symmetric, not in, not like, not similar to, now, nullif, nulls first, nulls last, overlaps, parse_url, prev, print, proctime_materialize, quarter, rand_integer, rank, regexp, regexp_extract, regexp_replace, reinterpret, reverse, right, row_number, running, second, session, session_end, session_proctime, session_rowtime, session_start, similar to, single_value, split_index, stddev, stddev_pop, stddev_samp, str_to_map, streamrecord_timestamp, substr, timestampadd, timestampdiff, to_base64, to_date, to_timestamp, tumble, tumble_end, tumble_proctime, tumble_rowtime, tumble_start, union, union all, unix_timestamp, upper, var_pop, var_samp, variance, week, year, ||]
{code}

Or maybe we can take a conservative approach and ban all of them in {{HiveModule}}?;;;","17/Jan/20 08:16;lirui;If we only ban functions like {{row_number}}, and user specifies {{CoreModule}} before {{HiveModule}}, it seems {{HiveModule}} will override other functions like {{ascii}}, which violates the resolution order proposed in FLIP-68.

On the other hand, if we ban all of them, and user specifies {{HiveModule}} first, we won't be using functions like {{ascii}} from {{HiveModule}}, which also violates FLIP-68...;;;","17/Jan/20 08:22;lzljs3620320;Hi [~lirui],
 * if it is runnable, I think it is OK, in FLIP-15595, maybe we will remove CoreMudule and let hive module functions first.
 * We need ban the functions which disturb regular SQL.;;;","23/Jan/20 17:43;phoenixjiangnan;1.10: 6afd2874aa0ab2578692fb0e71ab0a5c2137ff82;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala walkthrough archetype does not compile on Java 11,FLINK-15583,13279325,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,arvid,arvid heise,14/Jan/20 11:19,22/Jun/21 14:06,13/Jul/23 08:07,16/Jan/20 14:44,1.10.0,,,,,1.10.0,,,,Quickstarts,Test Infrastructure,,,,0,,,,,"While compiling a projected created with walkthrough archetype, the following error occurs
{noformat}
02:55:58.048 [ERROR] error: java.lang.NoClassDefFoundError: javax/tools/ToolProvider
02:55:58.048 [INFO] 	at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301){noformat}",,arvid heise,guoyangze,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 16 14:44:54 UTC 2020,,,,,,,,,,"0|z0aho0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jan/20 11:20;arvid;Related to FLINK-14276 and FLINK-9781.;;;","16/Jan/20 14:44;chesnay;master:
bf32fca22d9c7158ad72727865dda3acca1817ba
9ac44ba105901e308b80c04e3b0de7d7e8a06d14
1.10:
51e8390bec8541a75e0fc36f4b52c336658358b3
e3dc6cef47b5d889aa108a5b059d4e8da4b8a93a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpsertStreamTableSink should work on batch mode,FLINK-15579,13279252,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,eric.zheng,eric.zheng,eric.zheng,14/Jan/20 04:08,27/Mar/20 01:25,13/Jul/23 08:07,25/Mar/20 05:12,1.10.0,1.9.2,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Because JDBCTableSourceSinkFactory.createStreamTableSink() create JDBCUpsertTableSink. But BatchExecSink can not work with UpsertStreamTableSink.

{code:scala}
  override protected def translateToPlanInternal(
      planner: BatchPlanner): Transformation[Any] = {
    val resultTransformation = sink match {
      case _: RetractStreamTableSink[T] | _: UpsertStreamTableSink[T] =>
        throw new TableException(""RetractStreamTableSink and UpsertStreamTableSink is not"" +
          "" supported in Batch environment."")
{code}

DDL like:
CREATE TABLE USER_RESULT(
NAME VARCHAR,
CITY VARCHAR,
SCORE BIGINT
) WITH (
'connector.type' = 'jdbc',
'connector.url' = '',
'connector.table' = '',
'connector.driver' = 'com.mysql.cj.jdbc.Driver',
'connector.username' = 'root',
'connector.password' = '',
'connector.write.flush.interval' = '1s')
",,eric.zheng,godfreyhe,jark,libenchao,lzljs3620320,xingoo,,,,,,,,,,,,,,,,"nezhazheng commented on pull request #11045: [FLINK-15579][table-planner-blink] Support UpsertStreamTableSink on Blink batch mode.
URL: https://github.com/apache/flink/pull/11045
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Blink batch mode do not support jdbc sink right now . Becuse JDBCTableSourceSinkFactory.createStreamTableSink() create JDBCUpsertTableSink. But BatchExecSink can not work with UpsertStreamTableSink.
   
   This PR made BatchExecSink just like StreamExecSink, let BatchExecSink support RetractStreamTableSink and UpsertStreamTableSink.
   
   
   ## Brief change log
   
     - RetractStreamTableSink and UpsertStreamTableSink translate to Transformation on BatchExecSink.translateToPlanInternal()
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as JDBCUpsertTableSinkITCase.testBatchUpsert().
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Feb/20 06:51;githubbot;600","JingsongLi commented on pull request #11490: [FLINK-15579][table-planner-blink] UpsertStreamTableSink should work on batch mode
URL: https://github.com/apache/flink/pull/11490
 
 
   
   ## What is the purpose of the change
   
   JDBCTableSourceSinkFactory.createStreamTableSink() create JDBCUpsertTableSink. But BatchExecSink can not work with UpsertStreamTableSink.
   
   ## Brief change log
   
   Fix `BatchExecSink`.
   
   ## Verifying this change
   
   - Tests in `JDBCUpsertTableSinkITCase`
   - Tests in `TableSinkITCase`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Mar/20 11:06;githubbot;600","JingsongLi commented on pull request #11490: [FLINK-15579][table-planner-blink] UpsertStreamTableSink should work on batch mode
URL: https://github.com/apache/flink/pull/11490
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 05:09;githubbot;600","JingsongLi commented on pull request #11045: [FLINK-15579][table-planner-blink] Support UpsertStreamTableSink on Blink batch mode.
URL: https://github.com/apache/flink/pull/11045
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 05:11;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 01:25:41 UTC 2020,,,,,,,,,,"0|z0ah7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jan/20 04:27;eric.zheng;[~ykt836] Could you assign this bug to me? I would like to fix it.;;;","14/Jan/20 06:05;ykt836;So you want to use JDBC as a batch sink?;;;","14/Jan/20 08:06;eric.zheng;Yes, We have this requirement.;;;","14/Jan/20 08:42;ykt836;Sounds good to me, assigning to you [~eric.zheng].;;;","14/Jan/20 08:45;lzljs3620320;Hi [~eric.zheng], I changed the title to ""Support UpsertStreamTableSink on Blink batch mode"". FYI.;;;","15/Jan/20 06:26;eric.zheng;[~jark] Hi jark, I'm wondering why BatchExecSink do not support UpsertStreamTableSink now, Could you give me any suggestion? I want to modify BatchExecSink.translateToPlanInternal() and JDBCTableSourceSinkFactory.createStreamTableSink() related code to support UpsertStreamTableSink.;;;","15/Jan/20 06:40;jark;In old planner, {{UpsertStreamTableSink}} is only used for streaming mode. For batch mode, a sink should implement {{BatchTableSink}}.
After introducing the new blink planner, streaming and batch are both using {{StreamTableSink}} now. 
And I think it's fine to support UpsertStreamTableSink for batch mode with all change flag set to {{true}}. ;;;","15/Jan/20 08:09;eric.zheng;[~jark] Thank you for the suggestion, In other words,  BatchExecSink. translateToPlanInternal() and StreamExecSink.translateToPlanInternal() should have exactly same logic now. right?;;;","16/Mar/20 07:08;xingoo;I'm try the code in this pullrequest: [https://github.com/apache/flink/pull/11045/files/ab87a7aa2192c0129929f5600f3fcdfec9512e46]

and I found if use upsert, must close the primary key validation in org.apache.flink.table.planner.operations.SqlToOperationConverter(line 169)
{code:java}
//代码占位符
    private Operation convertCreateTable(SqlCreateTable sqlCreateTable) {
      // primary key and unique keys are not supported
//    if ((sqlCreateTable.getPrimaryKeyList().size() > 0)
//       || (sqlCreateTable.getUniqueKeysList().size() > 0)) {
//       throw new SqlConversionException(""Primary key and unique key are not supported yet."");
//    }
{code}
if not, the tableKeys can not found
{code:java}
//代码占位符
case upsertSink: UpsertStreamTableSink[T] =>
  // check for append only table
  val isAppendOnlyTable = UpdatingPlanChecker.isAppendOnly(this)
  upsertSink.setIsAppendOnly(isAppendOnlyTable)
  val tableKeys = {
    val sinkFieldNames = upsertSink.getTableSchema.getFieldNames
    UpdatingPlanChecker.getUniqueKeyFields(getInput, planner, sinkFieldNames) match {
      case Some(keys) => keys.sortBy(_.length).headOption
      case None => None
    }
  }

{code}
 ;;;","16/Mar/20 07:18;lzljs3620320;Hi [~xingoo], you can not use primary key in DDL now.
 * Now the keys are inferred by the plan, as your list codes. Your query/plan must produce changelog like aggregation query. 
 * We are doing to support primary keys in DDL. Maybe will be supported in the 1.11.;;;","23/Mar/20 11:03;lzljs3620320;Change it to bug, because so many users treat it as a bug...;;;","25/Mar/20 05:12;lzljs3620320;master:

56424794c291b9115080005220a17963048c3621

d38a010c55ad78f4e421d581ec72a96a79324dfe;;;","27/Mar/20 01:25;xingoo;Hi [~lzljs3620320] ,

how can i use upsert in batch jdbc sink. 
{code:java}
//代码占位符
case upsertSink: UpsertStreamTableSink[T] =>
  
  // 这里什么时候是true?
  val isAppendOnlyTable = UpdatingPlanChecker.isAppendOnly(this)

  upsertSink.setIsAppendOnly(isAppendOnlyTable)
  // 这里什么时候才能获取到keys?
  val tableKeys = {
    val sinkFieldNames = upsertSink.getTableSchema.getFieldNames
    UpdatingPlanChecker.getUniqueKeyFields(getInput, planner, sinkFieldNames) match {
      case Some(keys) => keys.sortBy(_.length).headOption
      case None => None
    }
  }
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WindowAggregate RelNodes missing Window specs in digest,FLINK-15577,13279186,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,b.hanotte,b.hanotte,b.hanotte,13/Jan/20 22:24,20/Jan/20 07:21,13/Jul/23 08:07,20/Jan/20 07:21,1.9.1,,,,,1.10.0,1.9.2,,,Table SQL / Legacy Planner,,,,,0,pull-request-available,,,,"The RelNode's digest (AbstractRelNode.getDigest()), along with its RowType, is used by the Calcite HepPlanner to avoid adding duplicate Vertices to the graph. If an equivalent vertex is already present in the graph, then that vertex is used in place of the newly generated one: 
https://github.com/apache/calcite/blob/branch-1.21/core/src/main/java/org/apache/calcite/plan/hep/HepPlanner.java#L828

This means that *the digest needs to contain all the information necessary to identify a vertex and distinguish it from similar - but not equivalent - vertices*.

In the case of `LogicalWindowAggregation` and `FlinkLogicalWindowAggregation`, the window specs are currently not in the digest, meaning that two aggregations with the same signatures and expressions but different windows are considered equivalent by the planner, which is not correct and will lead to an invalid Physical Plan.

For instance, the following query would give an invalid plan:

{code}
WITH window_1h AS (
    SELECT HOP_ROWTIME(`timestamp`, INTERVAL '1' HOUR, INTERVAL '1' HOUR) as `timestamp`
    FROM my_table
    GROUP BY HOP(`timestamp`, INTERVAL '1' HOUR, INTERVAL '1' HOUR)
),
window_2h AS (
    SELECT HOP_ROWTIME(`timestamp`, INTERVAL '1' HOUR, INTERVAL '2' HOUR) as `timestamp`
    FROM my_table
    GROUP BY HOP(`timestamp`, INTERVAL '1' HOUR, INTERVAL '2' HOUR)
)
(SELECT * FROM window_1h)
UNION ALL
(SELECT * FROM window_2h)
{code}

The invalid plan generated by the planner is the following (*Please note the windows in the two DataStreamGroupWindowAggregates nodes being the same when they should be different*):

{code}
DataStreamUnion(all=[true], union all=[timestamp]): rowcount = 200.0, cumulative cost = {800.0 rows, 802.0 cpu, 0.0 io}, id = 176
  DataStreamCalc(select=[w$rowtime AS timestamp]): rowcount = 100.0, cumulative cost = {300.0 rows, 301.0 cpu, 0.0 io}, id = 173
    DataStreamGroupWindowAggregate(window=[SlidingGroupWindow('w$, 'timestamp, 7200000.millis, 3600000.millis)], select=[start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime]): rowcount = 100.0, cumulative cost = {200.0 rows, 201.0 cpu, 0.0 io}, id = 172
      DataStreamScan(id=[1], fields=[timestamp]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 171
  DataStreamCalc(select=[w$rowtime AS timestamp]): rowcount = 100.0, cumulative cost = {300.0 rows, 301.0 cpu, 0.0 io}, id = 175
    DataStreamGroupWindowAggregate(window=[SlidingGroupWindow('w$, 'timestamp, 7200000.millis, 3600000.millis)], select=[start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime]): rowcount = 100.0, cumulative cost = {200.0 rows, 201.0 cpu, 0.0 io}, id = 174
      DataStreamScan(id=[1], fields=[timestamp]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 171
{code}
",,b.hanotte,godfreyhe,,,,,,,,,,,,,,,,,,,,"BenoitHanotte commented on pull request #10854: [FLINK-15577][table-planner] Fix similar aggregations with different windows being considered the same
URL: https://github.com/apache/flink/pull/10854
 
 
   ## What is the purpose of the change
   
   The RelNode's digest is used by the Calcite HepPlanner to avoid adding duplicate vertices to the graph. If an equivalent vertex was already present in the graph, then that vertex is used in place of the newly generated one. This means that the digest needs to contain all the information
   necessary to identifying a vertex and distinguishing it from similar (but not equivalent) vertices.
   
   In the case of the `WindowAggregation` nodes, the window specs are currently not in the digest, meaning that **two aggregations with the same signatures and expressions but different windows are considered equivalent by the planner, which is not correct and will lead to an invalid Physical Plan**.
   
   This commit fixes this issue and adds a test ensuring that the window specs are in the digest, as well as similar aggregations on two different windows will not be considered equivalent.
   
   Only the old planner is subject to the issue, the Blink planner correctly uses the window specs in the nodes' digests, allowing Blink to correctly differentiate between the nodes.
   
   More info and an example of an invalid plan are avalaible at https://issues.apache.org/jira/browse/FLINK-15577 
   
   ## Brief change log
   
   Added window specs to the following RelNodes:
   - LogicalWindowAggregate
   - FlinkLogicalWindowAggregate
   - LogicalWindowTableAggregate
   - FlinkLogicalWindowTableAggregate
   
   Added unit tests to the legacy planner and to the blink planner to prevent future regressions.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   - Added unit tests for the old planner to ensure window specs are in digest and that similar aggregations with different windows are not considered equivalent in the physical plan.
   - Added unit tests for the blink planner to ensure no such regression can be introduced in the future.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jan/20 13:51;githubbot;600","KurtYoung commented on pull request #10854: [FLINK-15577][table-planner] Fix similar aggregations with different windows being considered the same
URL: https://github.com/apache/flink/pull/10854
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jan/20 07:15;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 20 07:21:29 UTC 2020,,,,,,,,,,"0|z0agt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jan/20 01:33;ykt836;[~b.hanotte]  Thanks for the nice investigation, would you also like to fix this?;;;","14/Jan/20 04:33;b.hanotte;Hi [~ykt836], yes, I'll push a PR today;;;","14/Jan/20 06:07;ykt836;[~b.hanotte] Thanks for the fix, looks like blink planner also has this issue, could you fix both?;;;","14/Jan/20 07:25;b.hanotte;[~ykt836] yes, I'll have a look at both;;;","14/Jan/20 13:53;b.hanotte;[~ykt836] I pushed the PR atr https://github.com/apache/flink/pull/10854

Only the legacy planner is concerned by this issue as Blink was already correctly setting the window specs in the nodes digests. I added the tests to both planners to make sure that case is covered and no such regression can be introduced in the future.;;;","20/Jan/20 07:21;ykt836;1.9.2: 578a70901230e83351287ffa6b73b27f5a16d8ad

1.10.0: 98d209eab2f6e6cad0bc678876a36090c774082b

master: 244718553742c086eefc95f927d7b26af597d40a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove isTemporary property from CatalogFunction API,FLINK-15576,13279172,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,phoenixjiangnan,phoenixjiangnan,phoenixjiangnan,13/Jan/20 20:56,15/Jan/20 02:06,13/Jul/23 08:07,14/Jan/20 23:42,1.10.0,,,,,1.10.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"according to FLIP-79, CatalogFunction shouldn't have ""isTemporary"" property. Moving that from CatalogFunction to Create/AlterCatalogFunctionOperation",,lzljs3620320,phoenixjiangnan,,,,,,,,,,,,,,,,,,,,"bowenli86 commented on pull request #10846: [FLINK-15576] remove isTemporary property from CatalogFunction API
URL: https://github.com/apache/flink/pull/10846
 
 
   ## What is the purpose of the change
   
   according to FLIP-79, CatalogFunction shouldn't have ""isTemporary"" property. Moving that from CatalogFunction to Create/AlterCatalogFunctionOperation
   
   ## Brief change log
   
   - removed ""isTemporary"" property from CatalogFunction
   - moved ""isTemporary"" property from CatalogFunction to Create/AlterCatalogFunctionOperation
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *FunctionITCase*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jan/20 21:18;githubbot;600","bowenli86 commented on pull request #10846: [FLINK-15576][table] remove isTemporary property from CatalogFunction API
URL: https://github.com/apache/flink/pull/10846
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jan/20 23:39;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 15 02:06:09 UTC 2020,,,,,,,,,,"0|z0agq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jan/20 06:03;lzljs3620320;Hi [~phoenixjiangnan], looks like it is just an internal code refactor, no public api change, so it could not be a blocker?;;;","14/Jan/20 23:42;phoenixjiangnan;1.10: 7667d43cf216184818dac56d2c243c0c56f31ef1;;;","15/Jan/20 00:07;phoenixjiangnan;[~lzljs3620320] I think catalog API as of 1.10 are public evolving APIs to some users who want to do customization. We might need to officially mark them as @publicevolving in 1.11;;;","15/Jan/20 02:06;lzljs3620320;+1 to publicevolving, and I think maybe we should also add Builders.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Azure Filesystem Shades Wrong Package ""httpcomponents""",FLINK-15575,13279117,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,knaufk,knaufk,knaufk,13/Jan/20 16:08,24/Jan/20 09:58,13/Jul/23 08:07,20/Jan/20 08:07,1.10.0,1.9.1,,,,1.10.0,1.9.3,,,FileSystems,,,,,0,pull-request-available,,,,"Instead of shading ""org.apache.httpcomponents"" (this package does not exist) the azure filesystem should shade ""org.apache.http"". 

This e.g. causes problems when the azure filesystem and elasticsearch6 connector are both on the classpath.",,knaufk,pnowojski,uce,,,,,,,,,,,,,,,,,,,"knaufk commented on pull request #10901: [FLINK-15575] [filesystems] fix shading of azure filesystem
URL: https://github.com/apache/flink/pull/10901
 
 
   ## What is the purpose of the change
   
   fix shading of azure filesystem
   
   ## Brief change log
   
   * instead of shading the non-existing package org.apache.httpcomponents, we shaded org.apache.http now
   
   
   ## Verifying this change
   
   *(example:)*
     - build azure-hadoop-fs and check that classes are correctly shaded
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jan/20 10:40;githubbot;600","pnowojski commented on pull request #10901: [FLINK-15575] [filesystems] fix shading of azure filesystem
URL: https://github.com/apache/flink/pull/10901
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jan/20 08:05;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 24 09:32:00 UTC 2020,,,,,,,,,,"0|z0agds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jan/20 14:54;pnowojski;[~knaufk] are you sure that it should be {{org.apache.http}}? I can not find such package in the dependencies of the {{flink-filesystems/flink-azure-fs-hadoop}} module, but I can easily find {{org.apache.httpcomponents}}:

{noformat}
[INFO] +- org.apache.hadoop:hadoop-azure:jar:3.1.0:compile
[INFO] |  +- com.fasterxml.jackson.core:jackson-core:jar:2.10.1:compile
[INFO] |  +- org.apache.httpcomponents:httpclient:jar:4.5.3:compile
[INFO] |  |  +- org.apache.httpcomponents:httpcore:jar:4.4.6:compile
{noformat}
 ;;;","14/Jan/20 18:41;knaufk;[~pnowojski] Thanks for having a look. I believe so. The groupId is ""org.apache.httpcomponents"", but the actual package is ""org.apache.http"", e.g. check https://hc.apache.org/httpcomponents-client-ga/httpclient/apidocs/index.html. ;;;","15/Jan/20 13:26;pnowojski;Thanks for the explanation. Yes, you are right, I've checked those artefacts and all of the classes in there are from {{org.apache.http}} package. ;;;","20/Jan/20 08:07;pnowojski;Merged to release-1.10 as 4318a4a2cb
Merged to master as ad96288;;;","23/Jan/20 08:23;knaufk;[~pnowojski] Can you also merge this to release-1.8 and release-1.9?;;;","24/Jan/20 09:27;pnowojski;merged to release-1.9 as 01b9e04bf66c19e04be25f7d09d38cdc9127c5fb.

I don't think there was an azure connector in 1.8 - I can not find/cherry-pick the change because of missing files.;;;","24/Jan/20 09:32;knaufk;Thanks and my mistake.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incompatible types of expression and result type thrown in codegen,FLINK-15565,13278992,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,xiaojin.wy,xiaojin.wy,13/Jan/20 04:41,14/Jan/20 03:01,13/Jul/23 08:07,14/Jan/20 03:01,1.10.0,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"*The sql is:*
 CREATE TABLE `over10k` (
 t tinyint,
 si smallint,
 i int,
 b bigint,
 f float,
 d double,
 bo boolean,
 s varchar,
 ts timestamp,
 deci decimal(4,2),
 bin varchar
 ) WITH (
 'connector.path'='/daily_regression_batch_hive_1.10/test_window_with_specific_behavior/sources/over10k.csv',
 'format.empty-column-as-null'='true',
 'format.field-delimiter'='|',
 'connector.type'='filesystem',
 'format.derive-schema'='true',
 'format.type'='csv'
 );
 select s, rank() over (partition by s order by si), sum(b) over (partition by s order by si) from over10k limit 100;

*The data is :*
 109|277|65620|4294967305|97.25|7.80|true|nick quirinius|2013-03-01 09:11:58.703226|27.72|undecided
 93|263|65725|4294967341|6.06|4.12|false|calvin king|2013-03-01 09:11:58.703299|32.44|values clariffication
 108|383|65629|4294967510|39.55|47.67|false|jessica zipper|2013-03-01 09:11:58.703133|74.23|nap time
 89|463|65537|4294967493|64.82|13.79|true|ethan white|2013-03-01 09:11:58.703243|89.52|nap time
 88|372|65645|4294967358|34.48|11.18|true|quinn thompson|2013-03-01 09:11:58.703168|84.86|forestry
 123|432|65626|4294967435|2.39|16.49|true|david white|2013-03-01 09:11:58.703136|61.24|joggying
 57|486|65551|4294967397|36.11|9.88|true|katie xylophone|2013-03-01 09:11:58.703142|57.10|zync studies
 59|343|65787|4294967312|66.89|6.54|true|mike laertes|2013-03-01 09:11:58.703209|27.56|xylophone band
 74|267|65671|4294967409|21.14|14.64|true|priscilla miller|2013-03-01 09:11:58.703197|89.06|undecided
 25|336|65587|4294967336|71.01|14.90|true|tom ichabod|2013-03-01 09:11:58.703127|74.32|zync studies
 48|346|65712|4294967315|45.01|16.08|true|zach brown|2013-03-01 09:11:58.703108|21.68|zync studies
 84|385|65776|4294967452|35.80|32.13|false|xavier zipper|2013-03-01 09:11:58.703311|99.46|education
 58|389|65766|4294967416|95.55|20.62|false|sarah miller|2013-03-01 09:11:58.703215|70.92|history
 22|403|65565|4294967381|99.65|35.42|false|yuri johnson|2013-03-01 09:11:58.703154|94.47|geology
 55|428|65733|4294967535|99.54|5.35|false|jessica king|2013-03-01 09:11:58.703233|30.30|forestry
 117|410|65706|4294967391|50.15|0.21|false|quinn johnson|2013-03-01 09:11:58.703248|65.99|yard duty
 95|423|65573|4294967378|47.59|17.37|true|alice robinson|2013-03-01 09:11:58.703133|54.57|linguistics
 87|332|65748|4294967320|19.83|41.67|false|fred ellison|2013-03-01 09:11:58.703289|79.02|mathematics
 114|263|65674|4294967405|84.44|33.18|true|victor van buren|2013-03-01 09:11:58.703092|63.74|linguistics
 5|369|65780|4294967488|92.02|38.59|true|zach polk|2013-03-01 09:11:58.703271|67.29|yard duty
 -3|430|65667|4294967469|65.50|40.46|true|yuri xylophone|2013-03-01 09:11:58.703258|30.94|american history
 120|264|65769|4294967486|89.97|41.18|false|xavier hernandez|2013-03-01 09:11:58.703140|66.89|philosophy
 107|317|65634|4294967488|5.68|18.89|false|priscilla ichabod|2013-03-01 09:11:58.703196|39.42|joggying
 29|386|65723|4294967328|71.48|6.13|false|ulysses ichabod|2013-03-01 09:11:58.703215|86.65|xylophone band
 22|434|65768|4294967543|44.25|27.56|false|tom polk|2013-03-01 09:11:58.703306|12.30|kindergarten
 -1|274|65755|4294967300|22.01|35.52|false|oscar king|2013-03-01 09:11:58.703141|33.35|chemistry
 6|365|65603|4294967522|18.51|5.60|false|gabriella king|2013-03-01 09:11:58.703104|34.20|geology
 97|414|65757|4294967325|31.82|22.37|false|rachel nixon|2013-03-01 09:11:58.703127|61.00|nap time
 72|448|65538|4294967524|80.09|7.73|true|luke brown|2013-03-01 09:11:58.703090|95.81|american history
 51|280|65589|4294967486|57.46|23.35|false|zach xylophone|2013-03-01 09:11:58.703299|11.54|education
 12|447|65583|4294967389|0.98|29.79|true|yuri polk|2013-03-01 09:11:58.703305|1.89|wind surfing
 -1|360|65539|4294967464|4.08|39.51|false|oscar davidson|2013-03-01 09:11:58.703144|59.47|nap time
 0|380|65569|4294967425|0.94|28.93|false|sarah robinson|2013-03-01 09:11:58.703176|88.81|xylophone band
 66|478|65669|4294967339|23.66|38.34|true|yuri carson|2013-03-01 09:11:58.703228|64.68|opthamology
 12|322|65771|4294967545|84.87|10.76|false|sarah allen|2013-03-01 09:11:58.703271|0.79|joggying
 79|308|65563|4294967347|4.06|44.84|false|nick underhill|2013-03-01 09:11:58.703097|76.53|industrial engineering
 4|382|65719|4294967329|7.26|39.92|true|fred polk|2013-03-01 09:11:58.703073|73.64|mathematics
 10|448|65675|4294967392|26.20|16.30|true|rachel laertes|2013-03-01 09:11:58.703200|18.01|xylophone band
 45|281|65685|4294967513|81.33|32.22|true|oscar allen|2013-03-01 09:11:58.703285|71.38|religion
 57|288|65599|4294967422|90.33|44.25|false|bob young|2013-03-01 09:11:58.703185|11.16|biology
 77|452|65706|4294967512|22.90|5.35|true|bob van buren|2013-03-01 09:11:58.703290|14.58|debate
 103|492|65773|4294967404|58.29|48.28|false|yuri thompson|2013-03-01 09:11:58.703249|84.38|undecided
 84|411|65737|4294967486|63.13|1.10|true|katie ichabod|2013-03-01 09:11:58.703086|29.57|american history
 28|378|65589|4294967511|26.41|39.79|true|yuri polk|2013-03-01 09:11:58.703267|28.62|values clariffication
 88|478|65752|4294967364|80.59|45.13|true|victor garcia|2013-03-01 09:11:58.703081|34.90|chemistry
 37|388|65608|4294967350|32.94|39.06|false|mike polk|2013-03-01 09:11:58.703273|42.48|quiet hour
 25|264|65648|4294967402|90.83|30.96|false|tom ichabod|2013-03-01 09:11:58.703268|65.58|history
 17|455|65738|4294967508|15.73|27.01|false|david young|2013-03-01 09:11:58.703254|26.24|american history
 62|438|65655|4294967511|91.77|1.90|false|sarah steinbeck|2013-03-01 09:11:58.703150|16.41|chemistry
 65|298|65669|4294967328|68.89|2.75|true|david miller|2013-03-01 09:11:58.703077|51.86|values clariffication
 25|491|65641|4294967387|94.82|10.04|false|ulysses thompson|2013-03-01 09:11:58.703124|63.75|linguistics
 25|497|65708|4294967497|2.45|49.99|false|ethan laertes|2013-03-01 09:11:58.703320|49.72|yard duty
 117|288|65591|4294967530|75.18|2.71|false|fred quirinius|2013-03-01 09:11:58.703221|99.58|geology
 62|404|65706|4294967549|86.06|40.01|true|irene zipper|2013-03-01 09:11:58.703139|13.38|kindergarten
 99|362|65709|4294967399|50.48|26.34|false|jessica white|2013-03-01 09:11:58.703294|83.53|kindergarten
 62|395|65685|4294967446|56.73|14.87|false|victor johnson|2013-03-01 09:11:58.703194|31.42|history
 62|386|65615|4294967359|44.03|43.78|true|luke underhill|2013-03-01 09:11:58.703099|86.73|nap time
 15|302|65698|4294967526|91.38|3.59|true|wendy carson|2013-03-01 09:11:58.703111|9.46|religion
 92|507|65699|4294967512|8.44|34.72|false|calvin xylophone|2013-03-01 09:11:58.703198|66.89|study skills
 3|279|65756|4294967439|87.65|24.72|false|david white|2013-03-01 09:11:58.703233|47.19|study skills
 114|330|65754|4294967500|76.20|39.35|true|rachel quirinius|2013-03-01 09:11:58.703145|76.16|undecided
 24|500|65717|4294967535|60.96|21.51|false|victor falkner|2013-03-01 09:11:58.703318|82.83|nap time
 -2|331|65707|4294967335|67.12|13.51|false|bob ovid|2013-03-01 09:11:58.703285|62.32|joggying
 101|463|65740|4294967425|52.27|11.58|true|priscilla robinson|2013-03-01 09:11:58.703078|13.09|yard duty
 106|269|65577|4294967524|17.11|38.45|true|rachel falkner|2013-03-01 09:11:58.703197|79.89|xylophone band
 121|500|65690|4294967517|49.31|9.85|false|luke robinson|2013-03-01 09:11:58.703074|37.91|topology
 37|351|65587|4294967410|99.66|20.51|false|quinn falkner|2013-03-01 09:11:58.703221|80.69|history
 6|340|65612|4294967345|54.08|3.53|true|oscar white|2013-03-01 09:11:58.703279|68.67|debate
 115|366|65785|4294967330|90.00|25.79|true|jessica carson|2013-03-01 09:11:58.703143|2.72|xylophone band
 124|307|65649|4294967368|81.66|19.35|true|wendy ichabod|2013-03-01 09:11:58.703254|73.76|opthamology
 11|286|65752|4294967355|72.33|20.94|false|xavier carson|2013-03-01 09:11:58.703109|23.28|history
 15|320|65716|4294967505|49.25|27.53|false|fred carson|2013-03-01 09:11:58.703263|18.08|industrial engineering
 76|316|65706|4294967460|12.99|35.53|true|rachel davidson|2013-03-01 09:11:58.703300|85.43|quiet hour
 -2|485|65788|4294967510|9.99|22.75|false|luke carson|2013-03-01 09:11:58.703217|82.56|mathematics
 87|482|65612|4294967327|16.51|22.21|true|katie nixon|2013-03-01 09:11:58.703083|47.09|xylophone band
 21|400|65777|4294967354|4.05|11.10|false|david quirinius|2013-03-01 09:11:58.703205|25.69|geology
 97|343|65764|4294967427|47.79|18.94|true|ethan miller|2013-03-01 09:11:58.703308|39.81|topology
 2|292|65783|4294967420|38.86|12.14|true|wendy robinson|2013-03-01 09:11:58.703239|72.70|wind surfing
 48|440|65570|4294967438|41.44|13.11|true|bob thompson|2013-03-01 09:11:58.703122|57.67|american history
 87|333|65592|4294967296|71.77|8.28|false|yuri nixon|2013-03-01 09:11:58.703302|87.58|quiet hour
 -1|344|65616|4294967444|29.44|19.94|false|oscar falkner|2013-03-01 09:11:58.703203|28.22|geology
 1|425|65625|4294967531|51.83|38.18|false|holly xylophone|2013-03-01 09:11:58.703198|0.31|geology
 108|363|65715|4294967467|99.69|17.10|true|yuri xylophone|2013-03-01 09:11:58.703177|44.91|geology
 93|500|65778|4294967442|82.52|38.24|true|xavier falkner|2013-03-01 09:11:58.703277|25.41|history
 112|260|65612|4294967500|51.90|24.53|false|rachel falkner|2013-03-01 09:11:58.703211|65.45|american history
 89|294|65754|4294967450|94.21|35.55|true|gabriella falkner|2013-03-01 09:11:58.703156|18.36|topology
 32|389|65700|4294967525|42.65|32.59|true|yuri king|2013-03-01 09:11:58.703253|1.70|undecided
 13|395|65715|4294967317|64.24|36.77|false|fred ovid|2013-03-01 09:11:58.703168|74.25|yard duty
 5|262|65726|4294967543|8.85|12.89|true|rachel garcia|2013-03-01 09:11:58.703222|45.65|yard duty
 65|324|65569|4294967315|93.15|41.46|false|alice brown|2013-03-01 09:11:58.703110|77.23|topology
 73|477|65764|4294967542|27.96|44.68|false|bob steinbeck|2013-03-01 09:11:58.703173|90.95|undecided
 6|337|65616|4294967456|38.34|34.04|true|rachel hernandez|2013-03-01 09:11:58.703223|60.63|debate
 51|384|65649|4294967423|14.62|5.33|true|oscar king|2013-03-01 09:11:58.703232|21.96|history
 87|369|65626|4294967403|20.94|26.46|true|ulysses hernandez|2013-03-01 09:11:58.703076|35.79|values clariffication
 48|365|65558|4294967361|66.17|6.28|true|alice xylophone|2013-03-01 09:11:58.703081|51.13|study skills
 12|388|65642|4294967298|58.26|34.09|false|jessica brown|2013-03-01 09:11:58.703081|92.61|linguistics
 12|353|65703|4294967414|54.55|5.92|true|jessica johnson|2013-03-01 09:11:58.703289|91.71|chemistry
 117|499|65566|4294967328|32.18|19.59|true|priscilla king|2013-03-01 09:11:58.703214|66.88|philosophy
 116|363|65719|4294967513|18.59|48.19|false|priscilla johnson|2013-03-01 09:11:58.703237|55.47|history
 21|433|65551|4294967366|84.35|34.09|false|oscar thompson|2013-03-01 09:11:58.703291|7.99|values clariffication
 -2|409|65717|4294967343|39.62|9.79|true|irene ichabod|2013-03-01 09:11:58.703315|64.80|joggying
 23|495|65785|4294967473|30.91|21.95|true|fred robinson|2013-03-01 09:11:58.703240|66.34|nap time
 30|507|65673|4294967453|83.51|40.92|true|oscar thompson|2013-03-01 09:11:58.703281|65.25|values clariffication
 13|365|65594|4294967446|13.41|34.03|true|irene white|2013-03-01 09:11:58.703084|52.53|topology
 92|419|65771|4294967310|64.82|3.01|false|yuri brown|2013-03-01 09:11:58.703271|18.05|undecided
 81|351|65781|4294967473|48.46|15.80|false|bob nixon|2013-03-01 09:11:58.703254|99.35|debate
 105|490|65543|4294967334|32.91|42.91|false|yuri steinbeck|2013-03-01 09:11:58.703233|42.19|xylophone band
 25|402|65619|4294967340|6.28|49.92|true|victor xylophone|2013-03-01 09:11:58.703210|84.32|philosophy
 88|485|65557|4294967391|95.95|46.22|true|irene xylophone|2013-03-01 09:11:58.703141|63.31|mathematics
 81|285|65758|4294967338|37.83|38.23|true|irene ichabod|2013-03-01 09:11:58.703322|43.31|quiet hour
 96|316|65764|4294967442|86.76|32.89|false|wendy miller|2013-03-01 09:11:58.703190|10.35|geology
 43|321|65538|4294967422|81.78|6.07|false|zach van buren|2013-03-01 09:11:58.703273|26.02|topology
 60|496|65614|4294967376|34.40|45.59|true|jessica steinbeck|2013-03-01 09:11:58.703076|81.95|xylophone band
 44|395|65611|4294967443|15.58|1.53|false|gabriella thompson|2013-03-01 09:11:58.703295|11.00|values clariffication
 73|409|65767|4294967371|36.93|36.16|true|quinn ellison|2013-03-01 09:11:58.703105|82.70|religion
 121|330|65772|4294967508|70.46|44.50|true|quinn zipper|2013-03-01 09:11:58.703272|11.31|philosophy
 61|421|65541|4294967410|34.59|27.52|false|calvin johnson|2013-03-01 09:11:58.703299|3.52|history
 65|370|65674|4294967474|6.94|4.38|false|tom falkner|2013-03-01 09:11:58.703142|63.24|wind surfing
 41|462|65699|4294967391|58.03|17.26|false|calvin xylophone|2013-03-01 09:11:58.703322|92.60|study skills
 97|460|65591|4294967515|46.39|2.16|false|mike carson|2013-03-01 09:11:58.703265|97.16|values clariffication
 -1|435|65624|4294967377|73.60|45.63|true|irene hernandez|2013-03-01 09:11:58.703208|31.35|study skills
 22|282|65782|4294967318|75.19|40.78|false|quinn ichabod|2013-03-01 09:11:58.703122|44.85|topology
 46|487|65748|4294967318|67.01|24.13|false|victor zipper|2013-03-01 09:11:58.703273|95.40|linguistics
 18|275|65757|4294967307|80.45|18.92|false|bob hernandez|2013-03-01 09:11:58.703307|38.25|education
 103|264|65587|4294967306|97.65|11.36|false|david ovid|2013-03-01 09:11:58.703265|42.76|wind surfing
 86|466|65642|4294967333|40.96|26.06|true|david young|2013-03-01 09:11:58.703155|2.99|kindergarten
 119|437|65637|4294967494|18.93|31.04|true|calvin brown|2013-03-01 09:11:58.703241|30.45|debate
 62|285|65593|4294967518|83.43|2.05|false|rachel xylophone|2013-03-01 09:11:58.703084|45.21|quiet hour
 1|283|65752|4294967528|95.01|1.76|false|ethan ichabod|2013-03-01 09:11:58.703072|16.68|history
 8|333|65732|4294967503|22.43|21.80|false|mike polk|2013-03-01 09:11:58.703160|71.80|industrial engineering
 90|425|65648|4294967323|50.68|40.41|false|victor allen|2013-03-01 09:11:58.703146|58.75|kindergarten
 110|319|65620|4294967332|32.36|35.17|true|ethan davidson|2013-03-01 09:11:58.703269|73.03|history
 111|313|65711|4294967418|70.04|10.88|true|priscilla nixon|2013-03-01 09:11:58.703206|66.32|mathematics
 96|399|65719|4294967401|52.35|4.01|true|rachel hernandez|2013-03-01 09:11:58.703076|32.45|values clariffication
 83|353|65714|4294967384|10.12|15.81|false|rachel miller|2013-03-01 09:11:58.703110|16.39|philosophy
 11|475|65747|4294967303|98.29|32.30|false|yuri king|2013-03-01 09:11:58.703285|11.06|forestry
 84|295|65682|4294967463|17.75|23.28|true|alice zipper|2013-03-01 09:11:58.703306|79.77|industrial engineering
 8|348|65626|4294967373|52.54|31.29|false|bob underhill|2013-03-01 09:11:58.703189|82.40|undecided
 0|339|65603|4294967356|32.42|31.31|false|katie young|2013-03-01 09:11:58.703238|49.14|forestry
 82|280|65688|4294967427|19.11|0.10|false|holly young|2013-03-01 09:11:58.703256|71.39|chemistry
 119|465|65781|4294967467|23.83|0.95|false|yuri zipper|2013-03-01 09:11:58.703094|96.06|history
 10|356|65586|4294967339|71.96|32.54|true|oscar zipper|2013-03-01 09:11:58.703091|73.01|quiet hour
 25|364|65682|4294967449|50.96|34.46|true|sarah steinbeck|2013-03-01 09:11:58.703139|18.28|philosophy
 47|270|65652|4294967393|85.46|33.87|true|luke zipper|2013-03-01 09:11:58.703173|96.68|philosophy
 89|470|65676|4294967314|39.34|37.35|false|ulysses miller|2013-03-01 09:11:58.703303|69.67|values clariffication
 105|393|65703|4294967359|19.00|45.80|false|oscar johnson|2013-03-01 09:11:58.703086|99.42|linguistics
 120|415|65785|4294967498|54.68|32.92|true|calvin hernandez|2013-03-01 09:11:58.703086|93.09|linguistics
 94|486|65649|4294967549|33.47|35.42|false|jessica carson|2013-03-01 09:11:58.703089|34.30|mathematics
 38|288|65634|4294967304|5.10|44.83|false|ethan white|2013-03-01 09:11:58.703083|0.94|xylophone band
 91|268|65578|4294967501|43.98|2.77|false|jessica white|2013-03-01 09:11:58.703195|51.68|joggying
 123|409|65629|4294967431|29.23|27.30|false|ulysses garcia|2013-03-01 09:11:58.703141|70.01|philosophy
 7|454|65697|4294967394|62.25|3.38|false|tom underhill|2013-03-01 09:11:58.703121|47.97|values clariffication
 13|488|65662|4294967457|25.08|4.01|false|quinn van buren|2013-03-01 09:11:58.703272|35.40|history
 118|388|65642|4294967438|52.78|15.67|true|rachel falkner|2013-03-01 09:11:58.703158|61.13|opthamology
 1|315|65713|4294967509|43.80|24.95|false|nick brown|2013-03-01 09:11:58.703287|83.95|mathematics
 11|416|65658|4294967433|19.94|8.97|false|jessica nixon|2013-03-01 09:11:58.703117|63.58|joggying
 42|457|65669|4294967534|13.45|16.47|true|calvin polk|2013-03-01 09:11:58.703257|59.51|yard duty
 119|467|65639|4294967304|57.17|35.89|false|nick nixon|2013-03-01 09:11:58.703088|0.98|history
 5|383|65629|4294967302|70.92|32.41|false|rachel young|2013-03-01 09:11:58.703314|1.72|opthamology
 108|304|65557|4294967498|26.30|33.01|true|tom nixon|2013-03-01 09:11:58.703189|70.64|opthamology
 60|447|65778|4294967546|65.11|14.36|true|yuri robinson|2013-03-01 09:11:58.703284|45.69|joggying
 65|406|65613|4294967522|93.10|16.27|false|xavier laertes|2013-03-01 09:11:58.703178|25.19|philosophy
 113|482|65739|4294967311|51.17|36.29|true|priscilla steinbeck|2013-03-01 09:11:58.703084|13.07|kindergarten
 58|453|65780|4294967484|25.45|1.99|false|alice ichabod|2013-03-01 09:11:58.703307|25.71|nap time
 24|320|65759|4294967315|23.99|43.22|false|irene robinson|2013-03-01 09:11:58.703095|24.36|chemistry
 112|438|65622|4294967483|62.47|21.21|false|tom laertes|2013-03-01 09:11:58.703257|54.45|nap time
 89|382|65708|4294967459|40.10|45.17|false|luke ovid|2013-03-01 09:11:58.703325|59.38|yard duty
 63|410|65561|4294967330|86.99|24.01|false|fred underhill|2013-03-01 09:11:58.703288|29.48|religion
 103|462|65658|4294967533|48.98|46.63|true|wendy laertes|2013-03-01 09:11:58.703272|85.64|philosophy
 97|279|65563|4294967322|79.42|41.65|false|yuri thompson|2013-03-01 09:11:58.703308|43.37|mathematics
 122|375|65717|4294967513|99.32|27.37|true|rachel falkner|2013-03-01 09:11:58.703095|65.37|philosophy
 25|481|65672|4294967454|98.90|37.58|false|oscar ovid|2013-03-01 09:11:58.703293|73.85|biology
 71|409|65667|4294967420|1.98|44.05|true|alice brown|2013-03-01 09:11:58.703117|38.55|religion
 86|399|65568|4294967404|26.97|34.10|true|priscilla ichabod|2013-03-01 09:11:58.703283|87.92|yard duty
 114|348|65752|4294967368|18.90|42.15|false|irene zipper|2013-03-01 09:11:58.703154|63.92|debate
 31|464|65683|4294967364|20.61|48.84|false|irene garcia|2013-03-01 09:11:58.703219|80.62|american history
 30|302|65688|4294967477|7.75|5.34|false|quinn polk|2013-03-01 09:11:58.703085|80.36|geology
 72|423|65665|4294967353|54.78|15.57|false|fred quirinius|2013-03-01 09:11:58.703219|56.86|philosophy
 78|408|65609|4294967534|83.25|24.25|false|quinn falkner|2013-03-01 09:11:58.703074|29.42|quiet hour
 35|308|65659|4294967371|89.52|45.35|true|luke carson|2013-03-01 09:11:58.703276|78.07|wind surfing
 13|310|65558|4294967399|60.05|38.39|false|priscilla polk|2013-03-01 09:11:58.703194|53.92|mathematics
 80|450|65537|4294967548|74.10|8.87|true|ulysses falkner|2013-03-01 09:11:58.703139|56.48|nap time
 30|295|65743|4294967359|17.51|44.20|true|bob hernandez|2013-03-01 09:11:58.703242|59.71|quiet hour
 25|372|65606|4294967412|99.40|36.98|false|yuri quirinius|2013-03-01 09:11:58.703242|87.18|zync studies
 -3|454|65733|4294967544|73.83|18.42|false|bob ichabod|2013-03-01 09:11:58.703240|95.56|debate
 9|440|65773|4294967362|30.46|44.91|true|xavier falkner|2013-03-01 09:11:58.703098|62.35|religion
 105|289|65576|4294967342|76.65|29.47|false|ulysses garcia|2013-03-01 09:11:58.703282|71.95|chemistry
 116|263|65757|4294967525|94.04|37.06|false|priscilla hernandez|2013-03-01 09:11:58.703072|13.75|linguistics
 124|458|65726|4294967483|7.96|0.29|false|zach laertes|2013-03-01 09:11:58.703281|1.46|study skills
 -3|507|65671|4294967305|60.28|41.50|false|quinn polk|2013-03-01 09:11:58.703244|77.17|industrial engineering
 -3|458|65679|4294967331|64.29|43.80|true|irene young|2013-03-01 09:11:58.703084|2.61|american history
 17|435|65739|4294967438|44.39|9.29|false|alice thompson|2013-03-01 09:11:58.703241|68.01|undecided
 33|390|65564|4294967305|8.20|17.36|false|calvin laertes|2013-03-01 09:11:58.703176|65.07|zync studies
 73|474|65789|4294967421|62.00|40.44|true|alice quirinius|2013-03-01 09:11:58.703101|98.80|geology
 46|313|65692|4294967310|93.40|34.70|true|fred hernandez|2013-03-01 09:11:58.703196|26.80|geology
 50|302|65581|4294967387|2.73|18.54|false|jessica carson|2013-03-01 09:11:58.703282|58.24|study skills
 115|311|65651|4294967423|44.94|33.29|true|ethan laertes|2013-03-01 09:11:58.703116|63.49|biology
 88|368|65556|4294967428|37.79|47.21|true|tom laertes|2013-03-01 09:11:58.703149|7.26|topology
 59|476|65560|4294967341|26.00|21.70|true|irene ovid|2013-03-01 09:11:58.703224|37.32|wind surfing
 33|489|65723|4294967491|52.08|36.13|false|quinn robinson|2013-03-01 09:11:58.703174|29.70|chemistry
 69|329|65580|4294967527|45.37|25.36|true|irene ichabod|2013-03-01 09:11:58.703267|95.34|joggying
 8|342|65542|4294967486|86.51|30.05|true|ulysses johnson|2013-03-01 09:11:58.703164|4.89|kindergarten
 47|327|65660|4294967329|53.96|10.07|false|fred white|2013-03-01 09:11:58.703313|48.34|zync studies
 77|296|65771|4294967420|94.25|12.67|true|ulysses underhill|2013-03-01 09:11:58.703080|45.67|biology
 63|451|65581|4294967493|44.66|40.63|true|alice miller|2013-03-01 09:11:58.703071|97.98|geology
 103|303|65605|4294967540|54.00|47.97|true|fred davidson|2013-03-01 09:11:58.703087|68.42|zync studies
 68|300|65577|4294967395|8.00|27.76|false|quinn quirinius|2013-03-01 09:11:58.703124|14.35|values clariffication
 41|424|65684|4294967396|44.97|44.01|false|calvin polk|2013-03-01 09:11:58.703161|31.72|linguistics
 84|448|65649|4294967425|5.81|28.49|true|ulysses ichabod|2013-03-01 09:11:58.703317|96.87|history
 30|398|65577|4294967306|71.32|39.24|false|katie zipper|2013-03-01 09:11:58.703310|97.22|wind surfing
 70|361|65695|4294967371|6.97|45.29|false|oscar falkner|2013-03-01 09:11:58.703268|79.32|opthamology
 92|371|65702|4294967518|29.30|18.48|false|david ellison|2013-03-01 09:11:58.703192|30.01|topology
 10|298|65666|4294967460|82.71|16.06|true|irene white|2013-03-01 09:11:58.703198|64.62|quiet hour
 109|496|65699|4294967536|36.99|14.91|true|holly hernandez|2013-03-01 09:11:58.703123|66.43|geology
 68|383|65597|4294967334|84.64|1.14|true|holly falkner|2013-03-01 09:11:58.703210|96.35|kindergarten
 95|433|65738|4294967363|95.88|45.88|false|rachel steinbeck|2013-03-01 09:11:58.703308|34.85|history
 37|262|65773|4294967482|26.04|4.86|true|oscar hernandez|2013-03-01 09:11:58.703285|92.63|linguistics
 24|421|65676|4294967355|23.99|14.11|true|ulysses ovid|2013-03-01 09:11:58.703281|19.16|forestry
 91|485|65607|4294967315|55.90|17.62|false|zach nixon|2013-03-01 09:11:58.703305|83.23|joggying
 67|387|65790|4294967318|93.14|31.43|false|irene king|2013-03-01 09:11:58.703188|6.25|industrial engineering
 82|262|65571|4294967465|56.70|30.18|true|irene van buren|2013-03-01 09:11:58.703167|3.00|study skills
 98|505|65582|4294967365|17.40|40.51|false|sarah polk|2013-03-01 09:11:58.703121|56.65|history
 22|268|65612|4294967462|9.69|4.64|false|xavier ichabod|2013-03-01 09:11:58.703304|3.86|linguistics
 10|332|65685|4294967332|76.12|20.13|true|priscilla laertes|2013-03-01 09:11:58.703170|82.71|opthamology
 36|317|65641|4294967471|56.22|36.78|true|tom johnson|2013-03-01 09:11:58.703296|53.38|biology
 60|501|65555|4294967313|13.57|11.68|true|yuri davidson|2013-03-01 09:11:58.703183|10.42|religion
 123|267|65560|4294967438|40.69|11.41|true|ethan allen|2013-03-01 09:11:58.703086|91.03|undecided
 -2|482|65558|4294967487|36.92|49.78|true|nick johnson|2013-03-01 09:11:58.703204|39.91|industrial engineering
 59|270|65726|4294967372|48.94|37.15|false|oscar polk|2013-03-01 09:11:58.703221|12.67|quiet hour
 119|385|65595|4294967373|36.66|15.82|true|jessica nixon|2013-03-01 09:11:58.703127|5.26|zync studies
 122|306|65751|4294967471|56.79|48.37|true|bob hernandez|2013-03-01 09:11:58.703186|50.61|kindergarten
 64|402|65777|4294967481|77.49|13.11|false|nick carson|2013-03-01 09:11:58.703264|66.64|study skills
 48|465|65758|4294967485|75.39|30.96|false|ethan allen|2013-03-01 09:11:58.703076|10.00|joggying
 117|458|65603|4294967342|53.32|32.59|true|ethan garcia|2013-03-01 09:11:58.703204|47.35|yard duty
 23|283|65557|4294967415|24.61|14.57|false|fred white|2013-03-01 09:11:58.703082|12.44|chemistry
 56|507|65538|4294967507|67.82|42.13|false|alice king|2013-03-01 09:11:58.703297|54.64|american history
 96|436|65737|4294967528|81.66|27.09|false|tom zipper|2013-03-01 09:11:58.703199|85.16|debate
 88|292|65578|4294967546|91.57|37.42|false|nick zipper|2013-03-01 09:11:58.703294|96.08|religion
 73|481|65717|4294967391|40.07|27.66|true|yuri xylophone|2013-03-01 09:11:58.703120|18.21|history
 80|280|65620|4294967482|58.09|40.39|false|fred polk|2013-03-01 09:11:58.703136|23.61|xylophone band
 96|464|65659|4294967493|74.22|21.71|true|jessica ichabod|2013-03-01 09:11:58.703226|92.72|undecided
 103|485|65707|4294967436|94.57|21.16|true|zach van buren|2013-03-01 09:11:58.703313|3.93|study skills
 31|410|65566|4294967518|36.11|16.72|true|nick ellison|2013-03-01 09:11:58.703305|61.53|biology
 -3|270|65702|4294967512|38.05|1.07|true|david carson|2013-03-01 09:11:58.703136|28.07|philosophy
 3|404|65709|4294967473|14.86|48.87|true|mike quirinius|2013-03-01 09:11:58.703099|37.99|xylophone band
 124|473|65644|4294967314|65.16|19.33|false|oscar white|2013-03-01 09:11:58.703194|33.17|debate
 103|321|65572|4294967353|64.79|0.22|false|david robinson|2013-03-01 09:11:58.703187|20.31|linguistics
 41|395|65686|4294967428|61.99|11.61|false|sarah steinbeck|2013-03-01 09:11:58.703278|17.45|biology
 -3|469|65752|4294967350|55.41|32.11|true|oscar johnson|2013-03-01 09:11:58.703110|47.32|philosophy
 98|336|65641|4294967519|82.11|7.91|true|tom davidson|2013-03-01 09:11:58.703320|83.43|debate
 54|422|65655|4294967551|15.74|34.11|true|bob garcia|2013-03-01 09:11:58.703086|46.93|yard duty
 70|462|65671|4294967385|82.68|7.94|false|fred white|2013-03-01 09:11:58.703167|45.89|joggying
 62|325|65751|4294967342|36.71|28.42|true|priscilla garcia|2013-03-01 09:11:58.703239|0.56|mathematics
 56|504|65635|4294967318|93.88|34.87|true|holly polk|2013-03-01 09:11:58.703227|89.14|american history
 50|275|65697|4294967322|58.10|27.56|false|priscilla johnson|2013-03-01 09:11:58.703096|6.19|biology
 114|428|65680|4294967498|62.68|3.90|true|yuri nixon|2013-03-01 09:11:58.703086|53.28|xylophone band
 100|277|65739|4294967382|1.61|18.22|true|wendy garcia|2013-03-01 09:11:58.703137|78.35|industrial engineering
 7|494|65601|4294967403|20.76|19.41|false|david underhill|2013-03-01 09:11:58.703164|70.81|topology
 79|448|65744|4294967479|18.18|36.26|true|david xylophone|2013-03-01 09:11:58.703310|76.40|joggying
 19|289|65562|4294967344|56.25|33.81|true|sarah van buren|2013-03-01 09:11:58.703301|64.05|forestry
 10|508|65589|4294967473|96.49|7.56|false|priscilla brown|2013-03-01 09:11:58.703134|2.08|education
 89|451|65686|4294967396|21.20|13.22|true|oscar king|2013-03-01 09:11:58.703127|49.12|undecided
 45|323|65540|4294967436|29.79|5.69|false|tom falkner|2013-03-01 09:11:58.703102|53.85|nap time
 34|319|65780|4294967523|80.40|9.05|true|sarah falkner|2013-03-01 09:11:58.703179|75.06|yard duty
 30|510|65632|4294967373|60.94|21.31|true|gabriella steinbeck|2013-03-01 09:11:58.703146|69.16|undecided
 72|350|65742|4294967491|3.33|30.48|false|katie johnson|2013-03-01 09:11:58.703315|55.83|topology
 96|402|65620|4294967320|19.38|49.45|false|oscar steinbeck|2013-03-01 09:11:58.703303|25.84|yard duty
 95|405|65536|4294967338|18.26|1.46|false|sarah thompson|2013-03-01 09:11:58.703073|29.27|education
 80|396|65675|4294967379|30.21|28.41|false|rachel white|2013-03-01 09:11:58.703316|11.37|topology
 5|507|65715|4294967297|87.39|16.09|true|sarah xylophone|2013-03-01 09:11:58.703321|0.46|nap time
 52|322|65635|4294967296|13.25|10.02|false|wendy falkner|2013-03-01 09:11:58.703094|2.51|industrial engineering
 64|345|65744|4294967316|23.26|29.25|true|sarah brown|2013-03-01 09:11:58.703245|96.45|kindergarten
 97|502|65654|4294967405|0.09|3.10|false|victor robinson|2013-03-01 09:11:58.703141|29.03|religion
 25|424|65599|4294967303|49.92|33.86|true|calvin miller|2013-03-01 09:11:58.703095|76.80|study skills
 115|298|65599|4294967457|78.69|11.89|false|luke steinbeck|2013-03-01 09:11:58.703245|22.81|geology
 49|496|65722|4294967407|17.46|33.62|false|ethan underhill|2013-03-01 09:11:58.703158|7.67|forestry
 77|315|65592|4294967532|28.72|38.15|false|nick robinson|2013-03-01 09:11:58.703296|78.69|debate
 33|258|65780|4294967448|5.78|19.07|true|calvin davidson|2013-03-01 09:11:58.703133|18.12|study skills
 98|390|65592|4294967397|36.40|29.61|false|sarah young|2013-03-01 09:11:58.703314|74.60|wind surfing
 41|415|65618|4294967426|2.23|46.43|true|nick van buren|2013-03-01 09:11:58.703225|14.78|yard duty
 62|427|65671|4294967359|75.01|38.93|false|bob ovid|2013-03-01 09:11:58.703195|17.17|values clariffication
 -2|294|65588|4294967301|8.51|2.16|false|zach zipper|2013-03-01 09:11:58.703208|35.15|debate
 94|309|65653|4294967447|6.14|5.65|false|yuri van buren|2013-03-01 09:11:58.703279|94.47|study skills
 120|377|65615|4294967364|24.99|12.26|true|oscar nixon|2013-03-01 09:11:58.703250|71.62|industrial engineering
 3|500|65756|4294967445|98.38|39.43|true|luke nixon|2013-03-01 09:11:58.703243|29.49|yard duty
 -1|505|65611|4294967338|75.26|22.98|false|mike allen|2013-03-01 09:11:58.703123|95.80|linguistics
 124|466|65612|4294967456|72.76|15.57|false|calvin polk|2013-03-01 09:11:58.703235|37.15|biology
 1|490|65591|4294967329|69.89|40.29|false|luke laertes|2013-03-01 09:11:58.703104|58.27|quiet hour
 70|385|65553|4294967506|69.14|44.05|false|ethan xylophone|2013-03-01 09:11:58.703150|93.69|chemistry
 68|330|65573|4294967506|66.87|17.31|true|jessica hernandez|2013-03-01 09:11:58.703124|30.57|zync studies
 82|421|65699|4294967550|84.77|40.40|false|gabriella white|2013-03-01 09:11:58.703292|29.99|history
 9|346|65646|4294967449|66.32|24.07|false|jessica xylophone|2013-03-01 09:11:58.703084|94.86|undecided
 116|336|65638|4294967327|64.45|11.24|true|jessica falkner|2013-03-01 09:11:58.703087|60.05|study skills
 19|376|65770|4294967536|79.12|20.11|false|victor carson|2013-03-01 09:11:58.703243|72.69|industrial engineering
 27|433|65767|4294967395|22.53|18.81|false|bob polk|2013-03-01 09:11:58.703097|52.68|linguistics
 31|468|65654|4294967361|33.08|29.95|false|bob young|2013-03-01 09:11:58.703210|16.48|philosophy
 84|411|65564|4294967493|49.25|7.84|true|oscar nixon|2013-03-01 09:11:58.703274|47.54|american history
 37|409|65769|4294967384|25.89|42.27|false|katie underhill|2013-03-01 09:11:58.703172|66.93|zync studies
 10|356|65628|4294967475|98.07|13.86|false|david carson|2013-03-01 09:11:58.703222|7.37|nap time
 105|437|65664|4294967535|2.05|17.01|true|holly laertes|2013-03-01 09:11:58.703144|5.69|industrial engineering
 117|508|65788|4294967319|66.86|25.25|false|ulysses davidson|2013-03-01 09:11:58.703283|85.22|industrial engineering
 108|322|65697|4294967529|20.24|40.23|true|mike carson|2013-03-01 09:11:58.703083|6.04|philosophy
 80|426|65735|4294967533|73.85|41.99|false|quinn hernandez|2013-03-01 09:11:58.703098|69.55|mathematics
 49|434|65692|4294967336|89.33|14.24|true|yuri underhill|2013-03-01 09:11:58.703127|3.91|quiet hour
 74|501|65657|4294967451|88.85|11.09|true|bob king|2013-03-01 09:11:58.703175|51.36|quiet hour
 8|380|65734|4294967369|84.11|10.24|false|victor underhill|2013-03-01 09:11:58.703291|78.90|opthamology
 89|364|65735|4294967334|12.41|24.02|false|nick nixon|2013-03-01 09:11:58.703272|34.80|debate
 53|479|65579|4294967303|7.50|43.05|false|rachel ellison|2013-03-01 09:11:58.703148|48.50|yard duty
 67|493|65626|4294967489|98.74|32.74|false|katie thompson|2013-03-01 09:11:58.703263|87.95|geology
 56|390|65676|4294967456|42.59|1.64|true|wendy king|2013-03-01 09:11:58.703307|39.31|joggying
 13|431|65624|4294967330|94.05|30.76|false|quinn ichabod|2013-03-01 09:11:58.703180|1.72|biology
 85|366|65627|4294967356|37.14|35.57|true|alice king|2013-03-01 09:11:58.703170|6.78|yard duty
 -2|286|65549|4294967493|9.20|1.23|true|ulysses king|2013-03-01 09:11:58.703218|93.35|study skills
 51|344|65698|4294967309|83.66|6.12|false|zach ellison|2013-03-01 09:11:58.703158|29.28|yard duty
 89|489|65610|4294967353|64.70|8.13|true|katie polk|2013-03-01 09:11:58.703120|56.34|education
 95|327|65747|4294967522|1.16|12.00|true|bob van buren|2013-03-01 09:11:58.703284|3.45|opthamology
 50|508|65541|4294967451|37.38|46.94|true|quinn steinbeck|2013-03-01 09:11:58.703081|20.90|forestry
 6|301|65693|4294967454|89.07|41.96|true|alice ichabod|2013-03-01 09:11:58.703297|16.13|religion
 7|322|65719|4294967434|1.02|29.24|false|quinn carson|2013-03-01 09:11:58.703293|47.99|forestry
 99|469|65751|4294967356|10.10|42.47|false|wendy young|2013-03-01 09:11:58.703180|63.14|opthamology
 18|269|65751|4294967544|87.84|0.60|true|mike steinbeck|2013-03-01 09:11:58.703167|36.04|religion
 22|361|65729|4294967328|67.51|15.52|false|zach ovid|2013-03-01 09:11:58.703317|26.96|quiet hour
 114|455|65723|4294967481|4.94|33.44|false|alice van buren|2013-03-01 09:11:58.703074|72.22|philosophy
 -3|384|65676|4294967453|71.97|31.52|false|alice davidson|2013-03-01 09:11:58.703226|14.28|xylophone band
 37|334|65775|4294967518|17.88|45.96|false|zach ellison|2013-03-01 09:11:58.703260|9.92|nap time
 28|427|65648|4294967309|45.65|3.90|true|bob robinson|2013-03-01 09:11:58.703308|89.89|chemistry
 86|469|65780|4294967466|64.61|24.76|true|david steinbeck|2013-03-01 09:11:58.703241|0.68|linguistics
 61|455|65567|4294967315|84.80|25.83|false|alice robinson|2013-03-01 09:11:58.703127|26.03|zync studies
 -3|387|65550|4294967355|84.75|22.75|true|holly thompson|2013-03-01 09:11:58.703073|52.01|biology
 14|492|65690|4294967388|98.07|15.98|true|david miller|2013-03-01 09:11:58.703096|15.69|forestry
 8|318|65687|4294967551|44.02|14.70|false|quinn thompson|2013-03-01 09:11:58.703205|23.43|joggying
 117|502|65789|4294967441|55.39|8.22|false|tom allen|2013-03-01 09:11:58.703129|74.48|xylophone band
 20|285|65783|4294967424|99.34|21.19|false|alice thompson|2013-03-01 09:11:58.703223|9.55|opthamology
 4|478|65538|4294967312|21.90|0.85|false|sarah thompson|2013-03-01 09:11:58.703089|79.07|xylophone band

*The conf is:*

execution:
 planner: blink
 type: batch

*After excuse the sql above, there will be the exception :*
 [ERROR] Could not execute SQL statement. Reason:
 org.apache.flink.table.planner.codegen.CodeGenException: Incompatible types of expression and result type. Expression[GeneratedExpression(((int) 0),false,,INT NOT NULL,Some(0))] type is [INT NOT NULL], result type is [SMALLINT]",,godfreyhe,jark,leonard,lzljs3620320,xiaojin.wy,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #10841: [FLINK-15565][table-planner-blink] Fix error conversion of TinyInt,SmallInt literals between Flink and Calcite
URL: https://github.com/apache/flink/pull/10841
 
 
   
   ## What is the purpose of the change
   
   In ExpressionConverter, we convert Flink expression to Calcite RexNode. For literal, Flink TinyInt/SmallInt literals are wrongly converted to Calcite Int/Int literals. This lead to incompatible types of expression and result type.
   
   ## Brief change log
   
   Fix tinyint/smallint/int literals in `ExpressionConverter.visit(ValueLiteralExpression)`.
   
   ## Verifying this change
   
   - `ExpressionConverter`
   - `OverWindowITCase.testOverWindowWithUDAG11G`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector:no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jan/20 08:52;githubbot;600","BenoitHanotte commented on pull request #10843: [FLINK-15565][table-planner-blink] Remove deprecated logic in LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
URL: https://github.com/apache/flink/pull/10843
 
 
   ## What is the purpose of the change
   
   It seems there exists 2 paths to do the conversion from DataType to LogicalType:
   
   1. TypeConversions.fromLegacyInfoToDataType():
   used for instance when calling TableSchema.fromTypeInformation().
   
   2. LogicalTypeDataTypeConverter.fromDataTypeToLogicalType():
   Deprecated but still used in TableSourceUtil and many other places.
   
   These 2 code paths can return a different LogicalType for the same input, leading to the planner throwing a ValidationException when the LogicalTypes are compared to ensure they are compatible.
   
   More info in https://issues.apache.org/jira/browse/FLINK-15574
   
   ## Brief change log
   
   - Removed the deprecated logic in LogicalTypeDataTypeConverter.fromDataTypeToLogicalType(), make it call directly TypeConversions.fromDataToLogicalType() instead.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as DataType.getLogicalType() implementations (TypeConversions.fromDataToLogicalType() simply calls DataType.getLogicalType()).
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jan/20 13:49;githubbot;600","wuchong commented on pull request #10841: [FLINK-15565][table-planner-blink] Fix error conversion of TinyInt,SmallInt literals between Flink and Calcite
URL: https://github.com/apache/flink/pull/10841
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jan/20 03:00;githubbot;600",,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 14 03:01:56 UTC 2020,,,,,,,,,,"0|z0afm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/20 06:38;ykt836;cc [~lzljs3620320];;;","13/Jan/20 08:45;lzljs3620320;[~ykt836] [~jark] can you assign this to me?;;;","13/Jan/20 08:48;lzljs3620320;The bug is:

In {{ExpressionConverter}}, we convert Flink expression to Calcite RexNode. For literal, Flink TinyInt/SmallInt literals are wrongly converted to Calcite Int/Int literals. This lead to incompatible types of expression and result type.;;;","13/Jan/20 10:09;ykt836;[~lzljs3620320] I've assigned to you.;;;","14/Jan/20 03:01;jark;1.11.0: a38d3f7420817785d6b62da3eb488f5dd0c8086c
1.10.0: 999756544d66756695eb7fafd99d88dd911a4474;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnClusterDescriptorTest failed to validate the original intended behavior,FLINK-15564,13278979,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,xtsong,xtsong,13/Jan/20 03:07,16/Jan/20 15:50,13/Jul/23 08:07,16/Jan/20 15:50,,,,,,1.10.0,,,,Deployment / YARN,,,,,0,pull-request-available,testability,,,"The following test cases of {{YarnClusterDescriptorTest}} have failed to validate the original intended behavior and are temporally skipped by PR#10834.
 - {{testFailIfTaskSlotsHigherThanMaxVcores}}
 - {{testConfigOverwrite}}

The original purpose of these two test cases was to verify the validation logic against yarn max allocation vcores (in {{5836f7eddb4849b95d4860cf20045bc61d061918}}).

These two cases should have failed when we change the validation logic to get yarn max allocation vcores from yarnClient instead of configuration (in {{e959e6d0cd42f0c5b21c0f03ce547f2025ac58d5}}), because there are no yarn cluster (neither {{MiniYARNCluster}}) started in these cases, thus {{yarnClient#getNodeReports}} will never return.

The cases have not failed because another {{IllegalConfigurationException}} was thrown in {{validateClusterSpecification}}, because of memory validation failure. The memory validation failure was by design, and in order to verify the original purpose these two test cases should have been updated with reasonable memory sizes, which is unfortunately overlooked. 

The problem could be fixed with the following changes:
- Update the memory setups for the test cases, to pass the memory validation and thus validate the original intended behavior.
- Extract the logic of getting yarn max allocation vcores into a separate method, and override it in the test cases to provide a constant max vcores.",,liyu,trohrmann,wangyang0918,xtsong,,,,,,,,,,,,,,,,,,"xintongsong commented on pull request #10852: [FLINK-15564][yarn][test] Fix YarnClusterDescriptorTest that failed to validate the original intended behavior
URL: https://github.com/apache/flink/pull/10852
 
 
   ## What is the purpose of the change
   
   This PR fix the `YarnClusterDescriptorTest#testFailIfTaskSlotsHigherThanMaxVcores` and `#testFailIfTaskSlotsHigherThanMaxVcores`, which should have failed long ago but was covered by other problem.
   
   The original purpose of these two test cases was to verify the validation logic against yarn max allocation vcores. These two cases should have failed when we change the validation logic to get yarn max allocation vcores from yarnClient instead of configuration, because there are no yarn cluster (neither `MiniYARNCluster`) started in these cases, thus `yarnClient#getNodeReports` will never return.
   
   The cases have not failed because another `IllegalConfigurationException` was thrown in `validateClusterSpecification`, because of memory validation failure. The memory validation failure was by design, and in order to verify the original purpose these two test cases should have been updated with reasonable memory sizes, which is unfortunately overlooked. 
   
   ## Brief change log
   
   - 04ffffe5cf919ab07ddba656a68d4cd2d17c64c5: Update memory setups to uncover the problem.
     - I leave this as a separate commit for the convenience of code review. This should be squashed at the merging time. 
   - 54fdafd9c9d9c4bd6df9bd9b25de461f8f5bbcec: Fix test cases by mocking the yarn max allocation vcores.
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jan/20 10:06;githubbot;600","tillrohrmann commented on pull request #10852: [FLINK-15564][yarn][test] Fix YarnClusterDescriptorTest that failed to validate the original intended behavior
URL: https://github.com/apache/flink/pull/10852
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/20 15:50;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,FLINK-15598,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 16 15:50:35 UTC 2020,,,,,,,,,,"0|z0afjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/20 15:50;trohrmann;Fixed via 

master:
e3acccd5f495ed8313c17ef597377dfb4d14ca75
1db3b553b9ebd00c2d7c3dfe8c31254a787386c6

1.10.0:
f5d742fdb6ebefbdd5799f6911855f493bc8fca7
f46048ce3b41a18d750f1be2998f1671f52778fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
parameters --library and --jar doesn't work for DDL in sqlClient,FLINK-15552,13278706,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,Terry1897,Terry1897,10/Jan/20 12:45,11/Feb/20 10:30,13/Jul/23 08:07,23/Jan/20 12:59,,,,,,1.10.0,,,,Table SQL / Client,Table SQL / Runtime,,,,0,pull-request-available,,,,"How to Reproduce:
first, I start a sql client and using `-l` to point to a kafka connector directory.

`
 bin/sql-client.sh embedded -l /xx/connectors/kafka/

`

Then, I create a Kafka Table like following 
`
Flink SQL> CREATE TABLE MyUserTable (
>   content String
> ) WITH (
>   'connector.type' = 'kafka',
>   'connector.version' = 'universal',
>   'connector.topic' = 'test',
>   'connector.properties.zookeeper.connect' = 'localhost:2181',
>   'connector.properties.bootstrap.servers' = 'localhost:9092',
>   'connector.properties.group.id' = 'testGroup',
>   'connector.startup-mode' = 'earliest-offset',
>   'format.type' = 'csv'
>  );
[INFO] Table has been created.
`

Then I select from just created table and an exception been thrown: 

`
Flink SQL> select * from MyUserTable;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.NoMatchingTableFactoryException: Could not find a suitable table factory for 'org.apache.flink.table.factories.TableSourceFactory' in
the classpath.

Reason: Required context properties mismatch.

The matching candidates:
org.apache.flink.table.sources.CsvBatchTableSourceFactory
Mismatched properties:
'connector.type' expects 'filesystem', but is 'kafka'

The following properties are requested:
connector.properties.bootstrap.servers=localhost:9092
connector.properties.group.id=testGroup
connector.properties.zookeeper.connect=localhost:2181
connector.startup-mode=earliest-offset
connector.topic=test
connector.type=kafka
connector.version=universal
format.type=csv
schema.0.data-type=VARCHAR(2147483647)
schema.0.name=content

The following factories have been considered:
org.apache.flink.table.sources.CsvBatchTableSourceFactory
org.apache.flink.table.sources.CsvAppendTableSourceFactory
`
Potential Reasons:
Now we use  `TableFactoryUtil#findAndCreateTableSource`  to convert a CatalogTable to TableSource,  but when call `TableFactoryService.find` we don't pass current classLoader to this method, the default loader will be BootStrapClassLoader, which can not find our factory.

I verified in my box, it's truly caused by this behavior.",,aljoscha,godfreyhe,jark,leonard,lzljs3620320,Terry1897,twalthr,,,,,,,,,,,,,,,"leonardBang commented on pull request #10874: [FLINK-15552][table api] parameters --library and --jar doesn't work for DDL in sqlClient
URL: https://github.com/apache/flink/pull/10874
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *This pull request use current classloader to load classes which fix parameters --library adn --jar in sqlClient can not play correctly .*
   
   
   ## Brief change log
   
     - *update file org/apache/flink/table/factories/TableFactoryService.java*
   
   
   ## Verifying this change
   
   This change is a trivial rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/20 14:46;githubbot;600","wuchong commented on pull request #10874: [FLINK-15552][table api] parameters --library and --jar doesn't work for DDL in sqlClient
URL: https://github.com/apache/flink/pull/10874
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jan/20 12:44;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15635,FLINK-15992,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 23 12:59:38 UTC 2020,,,,,,,,,,"0|z0aduo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/20 07:06;jark;Hi [~Terry1897], are you sure there is kafka sql jar under {{/xx/connectors/kafka/}} ? Because AFAIK, SQL CLI e2e tests also uses --library and --jars to test. ;;;","14/Jan/20 07:28;Terry1897;Hi, [~jark].  I'm sure about it. As for SQL CLI e2e tests, maybe there is kafka jar under /lib directory.;;;","14/Jan/20 08:59;leonard;I reproduce this too. [~Terry1897] 
 [~jark] Do we have some SQL CLI e2e test before？I had some e2e tests about Table sources/sinks/computed-column in local by maven project which is different with this.;;;","14/Jan/20 13:07;jark;[~Leonard Xu] you can see https://github.com/apache/flink/blob/master/flink-end-to-end-tests/test-scripts/test_sql_client_kafka011.sh;;;","16/Jan/20 10:02;leonard;Hi,[~jark] [~Terry1897]

e2e test [test_sql_client_kafka011.sh|https://github.com/apache/flink/blob/master/flink-end-to-end-tests/test-scripts/test_sql_client_kafka011.sh] can run normally because it defines Kafka source Table in YAML way as environment file  which will create when SqlClient start , and it will use FlinkUserCodeClassLoaders#ParentFirstClassLoader, so TableFactoryService#find() can find correct KafkaTableSource class from user defined jars(like flink-sql-connector-kafka-0.11_2.11-1.10-SNAPSHOT.jar) :
{code:java}
//ExecutionContext.java
// create class loader and load user defined jars
classLoader = FlinkUserCodeClassLoaders.parentFirst(
      dependencies.toArray(new URL[dependencies.size()]),
      this.getClass().getClassLoader());

//create table soure 

environment.getTables().forEach((name, entry) -> {
   if (entry instanceof SourceTableEntry || entry instanceof SourceSinkTableEntry) {
      tableSources.put(name, createTableSource(environment.getExecution(), entry.asMap(), classLoader));
   }
   if (entry instanceof SinkTableEntry || entry instanceof SourceSinkTableEntry) {
      tableSinks.put(name, createTableSink(environment.getExecution(), entry.asMap(), classLoader));
   }
});
});
// register table sources
tableSources.forEach(tableEnv::registerTableSource);
....
// use  FlinkUserCodeClassLoaders#ParentFirstClassLoader here
private static TableSource<?> createTableSource(ExecutionEntry execution, Map<String, String> sourceProperties, ClassLoader classLoader) {
   if (execution.isStreamingPlanner()) {
      final TableSourceFactory<?> factory = (TableSourceFactory<?>)
         TableFactoryService.find(TableSourceFactory.class, sourceProperties, classLoader);
      return factory.createTableSource(sourceProperties);
   } else if (execution.isBatchPlanner()) {
      final BatchTableSourceFactory<?> factory = (BatchTableSourceFactory<?>)
         TableFactoryService.find(BatchTableSourceFactory.class, sourceProperties, classLoader);
      return factory.createBatchTableSource(sourceProperties);
   }
   throw new SqlExecutionException(""Unsupported execution type for sources."");
}

{code}
when we define a Kafka source/sink table in cli way,  DDL sql will translate to tEnv.sqlUpdate(ddl), this will not throw exception right now because  tEnv.sqlUpdate() is executing lazily. when we input  `select * from kafkaTable`,  tEnv will create kafkaTable and execute the query by:

 
{code:java}
//LocalExecutor.java 

// create table
final Table table = createTable(context, context.getTableEnvironment(), query);
...
/**
 * Creates a table using the given query in the given table environment.
 */
private <C> Table createTable(ExecutionContext<C> context, TableEnvironment tableEnv, String selectQuery) {
   // parse and validate query
   try {
      return context.wrapClassLoader(() -> tableEnv.sqlQuery(selectQuery));
   } catch (Throwable t) {
      // catch everything such that the query does not crash the executor
      throw new SqlExecutionException(""Invalid SQL statement."", t);
   }
}

...
//DatabaseCalciteSchema.java
private Table convertCatalogTable(ObjectPath tablePath, CatalogTable table, @Nullable TableFactory tableFactory) {
   final TableSource<?> tableSource;
   if (tableFactory != null) {
      if (tableFactory instanceof TableSourceFactory) {
         tableSource = ((TableSourceFactory) tableFactory).createTableSource(tablePath, table);
      } else {
         throw new TableException(
            ""Cannot query a sink-only table. TableFactory provided by catalog must implement TableSourceFactory"");
      }
   } else {
      tableSource = TableFactoryUtil.findAndCreateTableSource(table);
   }{code}
 

 there are no `classLoader` parameter in TableFactoryUtil#findAndCreateTableSource(table) , and TableFactoryUtil will use its default classLoader  which can not load KafkaTableSource class from use defined jars(like flink-sql-connector-kafka-0.11_2.11-1.10-SNAPSHOT.jar). 

Do you have any suggestion to fix this ?

 

 ;;;","16/Jan/20 12:20;jark;Thanks for the investigation [~Leonard Xu]. I got it. The reason why e2e works is because it uses YAML which uses the correct classloader in SQL CLI.
 However, the classloader is not correctly if it is registered via DDL. So problem is {{--library}} doesn't work for DDL.

The fixing might be simple, we should use current classloader when {{TableFactoryUtil#findAndCreateTableSource(table)}} which contains the user's jars.

cc [~twalthr];;;","16/Jan/20 12:29;twalthr;Whenever we run the optimizer we need to make sure that we set the context class loader. There is a {{wrapClassloader}} utility for doing this. {{findAndCreateTableSource}} might also has a method for setting the classloader explicitly.;;;","16/Jan/20 14:35;leonard; 

I fix and test it in my local environment and test e2e SQL test too, I will file a PR soon, cc [~twalthr] [~jark]
{code:java}
Flink SQL> CREATE TABLE orders (
> order_id STRING,
> item    STRING,
> currency STRING,
> amount INT,
> order_time TIMESTAMP(3),
> proc_time as PROCTIME(),
> amount_kg as amount * 1000,
> ts as order_time + INTERVAL '1' SECOND,
> WATERMARK FOR order_time AS order_time
> ) WITH (
> 'connector.type' = 'kafka',
> 'connector.version' = '0.10',
> 'connector.topic' = 'flink_orders3',
> 'connector.properties.zookeeper.connect' = 'localhost:2181',
> 'connector.properties.bootstrap.servers' = 'localhost:9092',
> 'connector.properties.group.id' = 'testGroup3',
> 'connector.startup-mode' = 'earliest-offset',
> 'format.type' = 'json',
> 'format.derive-schema' = 'true'
> );
[INFO] Table has been created.Flink 
SQL> select order_id,item from orders;
[INFO] Result retrieval cancelled.
{code}
{code:java}
[2020-01-16T22:29:57,039][INFO ][o.e.c.m.MetaDataDeleteIndexService] [zjSoRnz] [my_users/RiFewmsDQ62pwkp8blJ7ug] deleting index
{""acknowledged"":true}Checking for errors...
No errors in log files.
Checking for exceptions...
No exceptions in log files.
Checking for non-empty .out files...
No non-empty .out files.
[PASS] 'flink-end-to-end-tests/test-scripts/test_sql_client.sh' passed after 2 minutes and 47 seconds! Test exited with exit code 0.
{code}
 ;;;","17/Jan/20 01:50;lzljs3620320;It looks like there are so many calling for {{tableEnv}}. They are dangerous. We don't know if we will depend on user jar in the future. So I suggest adding {{wrapClassLoader}} all.;;;","17/Jan/20 01:51;lzljs3620320;BTW, it is not the first time of classloader problem, see FLINK-15167 .;;;","17/Jan/20 02:00;jark;[~lzljs3620320] we are calling {{wrapClassLoader}} already right. The problem is in {{org.apache.flink.table.factories.TableFactoryService#discoverFactories}} which doesn't use thread's context class loader. ;;;","17/Jan/20 02:22;lzljs3620320;Hi [~jark], I see, we are calling {{wrapClassLoader}} in {{sqlQuery}} already right.

But I am talking about the other invokings, we have discussed in FLINK-15509 , we are considering to validate/create source/sink in {{sqlUpdate(DDL)}}. We can do more things to ensure safety.;;;","17/Jan/20 12:08;twalthr;I created FLINK-15635 for the long-term fix.;;;","23/Jan/20 12:59;jark;Fixed in 1.11.0: 338ae70d2468e5951d566b9e55d8dd18a649c9c9
1.10.0: 275cc946736857b4d8f1177d47e1c03cab723ec5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testCancelTaskExceptionAfterTaskMarkedFailed failed on azure,FLINK-15550,13278686,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,yunta,yunta,10/Jan/20 10:33,17/Feb/23 15:15,13/Jul/23 08:07,17/Feb/23 15:15,1.11.0,1.12.5,1.13.6,1.14.3,1.16.0,1.16.2,1.17.0,,,Runtime / Task,,,,,0,pull-request-available,stale-assigned,,,"Instance: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=4241&view=ms.vss-test-web.build-test-results-tab&runId=12434&resultId=108939&paneView=debug


{code:java}
java.lang.AssertionError: expected:<FAILED> but was:<CANCELED>
	at org.apache.flink.runtime.taskmanager.TaskTest.testCancelTaskExceptionAfterTaskMarkedFailed(TaskTest.java:525)
{code}


{code:java}
expected:<FAILED> but was:<CANCELED>
{code}

",,akalashnikov,gaoyunhaii,hxb,mapohl,martijnvisser,pnowojski,SleePy,wanglijie,xtsong,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26271,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 17 15:14:44 UTC 2023,,,,,,,,,,"0|z0adq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 11:33;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:18;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","14/Oct/21 02:45;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25018&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=7568;;;","02/Nov/21 11:33;pnowojski;merged to release-1.12 as 3d1f06eb8cd
merged commit e050f37 into apache:release-1.13
merged commit 1271842 into apache:release-1.14
merged commit 7635571 into apache:master;;;","01/Mar/22 03:05;gaoyunhaii;It seems the issue reproduced on 1.14: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32261&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=6775;;;","01/Mar/22 16:18;akalashnikov;from FLINK-26271

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31903&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=5762];;;","10/Mar/22 14:47;akalashnikov;Extra logs were added to all branches since the reason for failure is not clear. So let's wait for the next failure.;;;","10/Apr/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","09/Aug/22 10:35;wanglijie;It seems the issue reproduced on master(1.16) https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39695&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","14/Sep/22 02:43;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40982&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","16/Dec/22 08:29;martijnvisser;release-1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44002&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8349;;;","02/Jan/23 11:04;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44374&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=6690;;;","23/Jan/23 08:04;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45137&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=6829;;;","27/Jan/23 11:02;pnowojski;Another fix merged to master as 1706412^^..1706412;;;","13/Feb/23 09:59;mapohl;Could you also provide backports for the most-recent fix? It looks like the test stability still exists in {{release-1.16}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46035&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=6690;;;","17/Feb/23 15:14;akalashnikov;Merged to release-1.16: 658ba8eb, a3c9eb40, cb9f183f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integer overflow in SpillingResettableMutableObjectIterator,FLINK-15549,13278681,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,caojian0613,caojian0613,caojian0613,10/Jan/20 10:16,24/Jan/20 01:13,13/Jul/23 08:07,21/Jan/20 09:23,1.10.0,1.6.4,1.7.2,1.8.3,1.9.1,1.10.0,1.8.4,1.9.2,,API / DataSet,,,,,0,overflow,pull-request-available,,,"The SpillingResettableMutableObjectIterator has a data overflow problem if the number of elements in a single input exceeds Integer.MAX_VALUE.

The reason is inside the SpillingResettableMutableObjectIterator, it track the total number of elements and the number of elements currently read with two int type fileds (elementCount and currentElementNum), and if the number of elements exceeds Integer.MAX_VALUE, it will overflow.

If there is an overflow, then in the next iteration, after reset the input , the data will not be read or only part of the data will be read.

Therefore, we should changing the type of these two fields of SpillingResettableIterator* from int to long, and we also need a pre-check mechanism before such numerical.",,aljoscha,caojian0613,guoyangze,libenchao,pnowojski,trohrmann,,,,,,,,,,,,,,,,"killxdcj commented on pull request #10900: [FLINK-15549][API/DataSet] Fix Integer overflow in ResettableIterator.
URL: https://github.com/apache/flink/pull/10900
 
 
   ## What is the purpose of the change
   
   The ResettableIterator has a data overflow problem if the number of elements in a single input exceeds Integer.MAX_VALUE. This pull request intend to fix this problem.
   
   ## Brief change log
   
     - *Fix Integer overflow in ResettableIterator*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jan/20 10:38;githubbot;600","killxdcj commented on pull request #10900: [FLINK-15549][API/DataSet] Fix Integer overflow in ResettableIterator.
URL: https://github.com/apache/flink/pull/10900
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jan/20 11:10;githubbot;600","killxdcj commented on pull request #10903: [FLINK-15549][API/DataSet] Fix Integer overflow in ResettableIterator*.
URL: https://github.com/apache/flink/pull/10903
 
 
   ## What is the purpose of the change
   
   The ResettableIterator has a data overflow problem if the number of elements in a single input exceeds Integer.MAX_VALUE. This pull request intend to fix this problem.
   
   ## Brief change log
   
     - *Fix Integer overflow in ResettableIterator*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jan/20 12:52;githubbot;600","pnowojski commented on pull request #10903: [FLINK-15549][API/DataSet] Fix Integer overflow in ResettableIterator*.
URL: https://github.com/apache/flink/pull/10903
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jan/20 08:56;githubbot;600",,,,,,,0,2400,,,0,2400,,,,,,,,,,,,FLINK-15581,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 21 09:23:54 UTC 2020,,,,,,,,,,"0|z0adp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/20 11:36;guoyangze;Same issue is also exist in _SpillingResettableIterator_ . I'd like to work on it.;;;","11/Jan/20 03:13;caojian0613;We have fixed this problem and could someone assign this ticket to me?;;;","11/Jan/20 09:05;guoyangze;Hi, [~caojian0613]. Thanks for your effort:). I'll try to find a committer take over this ticket.

However, as the [Contributing guide|https://flink.apache.org/contributing/contribute-code.html#create-jira-ticket-and-reach-consensus] says, I think you'd better wait for a committer to assign this ticket to you before starting your work next time. Because we need to get a consensus before taking any effort. Thanks~


cc [~trohrmann][~gjy];;;","14/Jan/20 18:05;trohrmann;Pulling in [~aljoscha] who is keeping an eye on {{DataSet}} issues these days.;;;","15/Jan/20 10:55;aljoscha;Yes, this is a problem. [~pnowojski] Are you already tracking this? I noticed you assigned [~caojian0613];;;","15/Jan/20 13:20;pnowojski;Yes, I can take care of reviewing/merging this one.;;;","21/Jan/20 09:23;pnowojski;Merged to master as 4a31751e868d9f6fb7529dd569d703f79abc78ae
release-1.10 as 6b85cc1038eae87d8e99cf5ca136d70a66ec6a70
release-1.9 as 90ad2e6d98a2646f662a7e04e1792595b6a5d6c1
release-1.8 as 22b5831940f80f75b0b4381621abb917bb959347;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache Camel not bundled but listed in flink-dist NOTICE,FLINK-15543,13278482,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,09/Jan/20 15:16,17/Jan/20 11:07,13/Jul/23 08:07,10/Jan/20 11:55,1.9.0,,,,,1.10.0,1.8.4,1.9.2,,Release System,,,,,0,pull-request-available,,,,"Apache Camel dependencies are listed in the flink-dist NOTICE, but we removed the dependency in 1.9.0 (see FLINK-12040).",,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #10818: [FLINK-15543][legal] Remove APache Camel NOTICE entries
URL: https://github.com/apache/flink/pull/10818
 
 
   The dependency has been removed in 1.9.0 and is hence no longer bundled.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 15:20;githubbot;600","zentol commented on pull request #10818: [FLINK-15543][legal] Remove Apache Camel NOTICE entries
URL: https://github.com/apache/flink/pull/10818
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/20 11:54;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 10 11:55:33 UTC 2020,,,,,,,,,,"0|z0acgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/20 11:55;chesnay;master: 7ec228a5225eda68701f6a6b5713727ed5232eb9
1.10: 2a7f96b18732430d9ebeaf3c83bf687f1091b499 
1.9: 6dfbf3394fcc70f93b19ef7b74a4488430d9652d
1.8: 33641f65a5faa9a5b791a5cd4996462e1b1cf80c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
lz4-java licensing is incorrect,FLINK-15542,13278473,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,09/Jan/20 14:58,10/Jan/20 11:54,13/Jul/23 08:07,10/Jan/20 11:54,1.10.0,,,,,1.10.0,,,,Release System,,,,,0,pull-request-available,,,,"In FLINK-14845 an lz4-java dependency was moved from flink-table to flink-runtime. With this commit lz4 was included in the flink-runtime jar, and the NOTICE file for flink-runtime was adjusted appropriately.
The follow-up that removed the shading (FLINK-15311) did not update the NOTICE files however; it is still listed in flink-runtime when it should be in flink-dist.",,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #10817: [FLINK-15542][legal] Move NOTICE entry for lz4-java
URL: https://github.com/apache/flink/pull/10817
 
 
   Moves the NOTICE entry for lz4-java from flink-runtime to flink-dist, which is the actual jar bundling this dependency.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 15:01;githubbot;600","zentol commented on pull request #10817: [FLINK-15542][legal] Move NOTICE entry for lz4-java
URL: https://github.com/apache/flink/pull/10817
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/20 11:53;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 10 11:54:40 UTC 2020,,,,,,,,,,"0|z0aceg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/20 11:54;chesnay;master: 75cded93430f9c1fbb325e95c2c4795451199ae4
1.10: a274c0296b93fb4d0568714fa39dc7a2f417ba97;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKinesisConsumerTest.testSourceSynchronization is unstable on travis.,FLINK-15541,13278451,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,xtsong,xtsong,09/Jan/20 13:31,13/Jan/20 15:11,13/Jul/23 08:07,13/Jan/20 15:11,1.10.0,,,,,1.10.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,test-stability,,,"[https://api.travis-ci.org/v3/job/634712405/log.txt]
{code:java}
13:16:19.144 [ERROR] Tests run: 11, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 4.338 s <<< FAILURE! - in org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest
13:16:19.144 [ERROR] testSourceSynchronization(org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest)  Time elapsed: 1.001 s  <<< FAILURE!
java.lang.AssertionError: expected null, but was:<java.lang.RuntimeException: expected>
	at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.testSourceSynchronization(FlinkKinesisConsumerTest.java:1018)
{code}",,aljoscha,SleePy,xtsong,,,,,,,,,,,,,,,,,,,"aljoscha commented on pull request #10837: [FLINK-15541] Fix unstable case FlinkKinesisConsumerTest
URL: https://github.com/apache/flink/pull/10837
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jan/20 15:11;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 13 15:11:53 UTC 2020,,,,,,,,,,"0|z0ac9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/20 07:36;SleePy;This is an obvious bug of unstable test case. Please check the PR, it's easy to understand.;;;","13/Jan/20 15:11;aljoscha;Fixed on release-1.10 in 5bea705408ba455bd462d72d6943a59ee46e2a90
Fixed on master in 6f5f23bc05754e3ff7948df6da0111d4f9fc3acf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-shaded-hadoop-2-uber bundles wrong dependency versions,FLINK-15540,13278444,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Jan/20 12:53,09/Apr/20 01:08,13/Jul/23 08:07,16/Jan/20 09:13,shaded-9.0,,,,,shaded-10.0,,,,BuildSystem / Shaded,,,,,0,pull-request-available,,,,"For legacy reasons flink-shaded contains 2 modules for hadoop:
flink-shaded-hadoop-2, defining the core dependencies and versions via dependency management, and flink-shaded-hadoop-2-uber for creating a fat jar.

In this kind of setup the dependency management in {{flink-shaded-hadoop-2}} is ignored by the {{-uber}} module; dependency management entries only apply if they are located in a parent module or within the module itself.

As a result flink-shaded-hadoop-2-uber is bundling the wrong versions of several dependencies; among others we bundle commons-collections 3.2.1, instead of 3.2.2, which has a security issue.",,,,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #80: [FLINK-15540][hadoop] Move depMgmt into parent module
URL: https://github.com/apache/flink-shaded/pull/80
 
 
   Introduces a common parent for the 2 hadoop modules, which contains the dependency management entries previously located in `flink-shaded-hadoop-2`.
   This ensures that both hadoop modules are applying the entries.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 13:06;githubbot;600","zentol commented on pull request #80: [FLINK-15540][hadoop] Move depMgmt into parent module
URL: https://github.com/apache/flink-shaded/pull/80
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/20 09:13;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 16 09:13:52 UTC 2020,,,,,,,,,,"0|z0ac80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/20 09:13;chesnay;master: 01e95ca2975b409d4b367c4c0c81614846797963;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Type of keys should be `BinaryRow` when manipulating map state with `BaseRow` as key type.,FLINK-15537,13278425,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,icshuo,icshuo,icshuo,09/Jan/20 10:25,19/Jan/20 06:51,13/Jul/23 08:07,19/Jan/20 06:51,1.9.1,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"`BaseRow` is serialized and deserialized as `BinaryRow` by default, so when the key type of the map state is `BaseRow`, we should construct map keys with `BinaryRow` as type to get value from map state, otherwise, you would  always get Null...

Try it with following SQL:
{code:java}
// (b: Int, c: String)
SELECT 
      b, listagg(DISTINCT c, '#')
FROM MyTable
GROUP BY b
{code}
 ",,icshuo,jark,,,,,,,,,,,,,,,,,,,,"cshuo commented on pull request #10815: [FLINK-15537][table-planner-blink] Type of keys should be `BinaryRow`…
URL: https://github.com/apache/flink/pull/10815
 
 
   ## What is the purpose of the change
   
   Fix bug of distinct aggregate codegen when key type of distinct `MapView` is `BaseRow`.
   
   ## Brief change log
     - This PR mainly fix some problems of `DistinctAggCodeGen`.
     - BTW, refine name problems of `ListAgg` aggregate function in tests..
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Added integration tests in `AggregateITCase`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 12:55;githubbot;600","wuchong commented on pull request #10815: [FLINK-15537][table-planner-blink] Type of keys should be `BinaryRow`…
URL: https://github.com/apache/flink/pull/10815
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jan/20 06:50;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 19 06:51:53 UTC 2020,,,,,,,,,,"0|z0ac3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/20 11:16;jark;Nice catch! ;;;","19/Jan/20 06:51;jark;1.11.0: ac5e288524991d340309d118fa9fa29109e7427f
1.10.0: 791b93d451e371e88ed1d05a91b418fee364f0c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert removal of ConfigConstants.YARN_MAX_FAILED_CONTAINERS,FLINK-15536,13278414,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tison,tison,tison,09/Jan/20 09:27,09/Jan/20 09:55,13/Jul/23 08:07,09/Jan/20 09:55,,,,,,1.11.0,,,,Runtime / Configuration,,,,,0,pull-request-available,,,,"FLINK-15359 cleans up some constants with no power. However, {{ConfigConstants.YARN_MAX_FAILED_CONTAINERS}} inside {{ConfigConstants}} is {{Public}} so that we should revert the removal of it for b/w compatibility.",,tison,,,,,,,,,,,,,,,,,,,,,"TisonKun commented on pull request #10808: [FLINK-15536][configuration] Revert removal of ConfigConstants.YARN_MAX_FAILED_CONTAINERS
URL: https://github.com/apache/flink/pull/10808
 
 
   Revert breaking changes introduced by FLINK-15359 #10658 
   
   cc @tillrohrmann 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 09:30;githubbot;600","zentol commented on pull request #10808: [FLINK-15536][configuration] Revert removal of ConfigConstants.YARN_MAX_FAILED_CONTAINERS
URL: https://github.com/apache/flink/pull/10808
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 09:55;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 09 09:55:17 UTC 2020,,,,,,,,,,"0|z0ac1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/20 09:55;chesnay;master: 57912987e3c58b68b584bc15cf547329802d626e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add usage of ProcessFunctionTestHarnesses for testing documentation,FLINK-15535,13278401,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yanghua,yanghua,yanghua,09/Jan/20 08:27,15/Jan/20 10:34,13/Jul/23 08:07,15/Jan/20 10:32,1.10.0,,,,,1.10.0,,,,Documentation,,,,,0,pull-request-available,,,,"Recently, we added {{ProcessFunctionTestHarness}} for testing {{ProcessFunction}}. However, except {{ProcessFunctionTestHarnessesTest}} I can not find anything about this test harness in the master codebase.

Considering {{ProcessFunction}} is the very important and frenquency-used UDF.

I suggest that we could add a test example in the [testing documentation|https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/testing.html#integration-testing].",,aljoscha,kkl0u,yanghua,,,,,,,,,,,,,,,,,,,"yanghua commented on pull request #10833: [FLINK-15535][documentation] Add usage of ProcessFunctionTestHarnesses for testing documentation
URL: https://github.com/apache/flink/pull/10833
 
 
   
   
   ## What is the purpose of the change
   
   *This pull request adds usage of ProcessFunctionTestHarnesses for testing documentation*
   
   ## Brief change log
   
     - *Add usage of ProcessFunctionTestHarnesses for testing documentation*
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jan/20 03:39;githubbot;600","kl0u commented on pull request #10833: [FLINK-15535][documentation] Add usage of ProcessFunctionTestHarnesses for testing documentation
URL: https://github.com/apache/flink/pull/10833
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jan/20 10:31;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 15 10:32:51 UTC 2020,,,,,,,,,,"0|z0abyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/20 08:28;yanghua;[~aljoscha] and [~trohrmann] WDYT?;;;","09/Jan/20 13:28;aljoscha;sounds good.;;;","09/Jan/20 15:34;yanghua;bq. sounds good.

OK, will open a PR for that.;;;","15/Jan/20 10:32;kkl0u;Merged on master with d7edaaa8f28b2a1ae1090077477d994afb7b702f
and on release-1.10 with 8927e9723cb68dbda849c5a38dc674187f980ba3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Writing DataStream as text file fails due to output path already exists,FLINK-15533,13278391,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kkl0u,lirui,lirui,09/Jan/20 07:27,14/Jan/20 10:12,13/Jul/23 08:07,14/Jan/20 10:12,1.10.0,,,,,1.10.0,,,,Client / Job Submission,,,,,0,pull-request-available,,,,"The following program reproduces the issue.
{code}
    Configuration configuration = GlobalConfiguration.loadConfiguration();
    configuration.set(DeploymentOptions.TARGET, RemoteExecutor.NAME);
    StreamExecutionEnvironment streamEnv = new StreamExecutionEnvironment(configuration);

    DataStream dataStream = streamEnv.fromCollection(Arrays.asList(1,2,3));
    dataStream.writeAsText(""hdfs://localhost:8020/tmp/output"");

    streamEnv.execute();
{code}
The job will fail with the follow error, even though the output path doesn't exist before job submission:
{noformat}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.FileAlreadyExistsException): /tmp/output already exists as a directory
{noformat}",,aljoscha,kkl0u,lirui,lzljs3620320,tison,,,,,,,,,,,,,,,,,"kl0u commented on pull request #10835: [FLINK-15533] Fix parallelism setting propagation during graph instantiation.
URL: https://github.com/apache/flink/pull/10835
 
 
   ## What is the purpose of the change
   
   Before the introduction of the executor (FLIP-73) and the consolidation of the different environments, some environments were overwriting the parallelism setting in their constructor. Now, after the refactoring of the environments we were not always doing it. In addition, the problem now is even more evident as that the user can directly pass a `Configuration` in the `(Stream)ExecutionEnvironment` constructor.
   
   This PR makes sure that we always expose a consistent view of the parallelism when the user calls `env.getParallelism()`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jan/20 12:57;githubbot;600","kl0u commented on pull request #10835: [FLINK-15533] Fix parallelism setting propagation during graph instantiation.
URL: https://github.com/apache/flink/pull/10835
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jan/20 10:11;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15511,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 14 10:12:14 UTC 2020,,,,,,,,,,"0|z0abw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/20 07:30;lirui;Hit this issue when working on FLINK-15511. These comments might be helpful: [comment1|https://issues.apache.org/jira/browse/FLINK-15511?focusedCommentId=17011394&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17011394] and [comment2|https://issues.apache.org/jira/browse/FLINK-15511?focusedCommentId=17011472&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17011472];;;","09/Jan/20 07:34;lzljs3620320;Thanks [~lirui]  for tracking this. Can you reproduce it by pure DataStream job?;;;","09/Jan/20 08:07;lirui;[~lzljs3620320] Thanks for the suggestions. Yes it can be reproduced with simpler pure DataStream job. I'll update the description.;;;","09/Jan/20 08:19;lzljs3620320;The second execution will be successful, Looks like something related to ""StreamExecutionEnvironment.consolidateParallelismDefinitionsInConfiguration"".

CC: [~kkl0u] [~tison] [~aljoscha];;;","09/Jan/20 08:32;tison;[~lzljs3620320] but how? I don't see a clear reason since that method translates the default value placeholder to an effective default value.;;;","09/Jan/20 08:49;lirui;[~tison] If I set {{parallelism.default}} in flink-conf.yaml, then only the 1st execution would fail. Otherwise, all executions fail.;;;","09/Jan/20 08:53;tison;what is the value you set to {{parallelism.default}}?;;;","09/Jan/20 08:55;lirui;I set {{parallelism.default}} to 1;;;","09/Jan/20 08:58;tison;From my understanding if parallelism > 1, actually a number of files under the configured path({{/tmp/output}} here) will be created. So the effective parallelism in the case is significant.;;;","09/Jan/20 09:00;tison;Could you attach the running job DAG on JIRA? It sounds weird. If you delete the file/directory at first and then submit the job with {{parallelism == 1}}, it should work and there should be only one execution instead of 1st execution and others; in the other case, it should be the case the directory exists if you set {{parallelism > 1}}.;;;","09/Jan/20 09:03;tison;The default value of {{parallelism.default}} is {{1}}. So it should be no difference whether or not you reset it to {{1}}.;;;","09/Jan/20 09:16;lirui;When we create a {{DataStreamSink}}, we get parallelism from execution config, which by default is -1:
https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamSink.java#L40

I think failing to overwrite this default value leads to the issue here.;;;","09/Jan/20 09:32;tison;Sounds reasonable.

cc [~kkl0u];;;","09/Jan/20 09:48;kkl0u;Thanks for reporting this [~lirui] and also thanks for digging into it. I agree that this seems to be the problem and it should be fixed.

My first thought is that the {{consolidateParallelismDefinitionsInConfiguration}} should change to be {{if (config.getParallelism() == ExecutionConfig.PARALLELISM_DEFAULT)}} and it should be called from within the {{getParallelism()}}  so that we have a consistent view across the whole codebase. 

Another thing that I would like to point is also that this [check|https://github.com/kl0u/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/common/io/FileOutputFormat.java#L223] seems a bit weird to me. I would suggest that no matter what the parallelism is, the behaviour should be the same, i.e. always create a directory.

What do you think?;;;","09/Jan/20 09:53;lzljs3620320;[~kkl0u] Do you think this is blocker or critical?;;;","09/Jan/20 10:07;kkl0u;[~lzljs3620320] I made it a blocker but I hope that we can have a fix merged by today. ;;;","09/Jan/20 10:12;kkl0u;I have [branch|https://github.com/kl0u/flink/tree/FLINK-15533] with a simple fix  but I will wait for travis before opening a PR. It does not change the {{FileOutputFormat}}, as this change may be breaking for some users, but we can do it in a follow-up issue. 

I would appreciate also if [~lirui] could verify that the fix is actually fixing the problem.;;;","09/Jan/20 10:27;lirui;Hi [~kkl0u], thanks for the quick fix. IIUC, the change makes sure {{parallelism.default}} will be used if it's set. But what if user doesn't set this property? Guess we still use the default -1 for parallelism?;;;","09/Jan/20 10:36;kkl0u;Hi [~lirui], I am also try to figure out all the possible scenarios. In addition, I am trying to see if this bug exists also in previous versions of Flink. The reason is that nothing seems to have changed in 1.10 that would change the behaviour of a {{DataStreamSink}}. WDYT?;;;","09/Jan/20 11:16;kkl0u;[~lirui] To summarize the problem, your job:
 1) ALWAYS (not only the first execution) fails when you set no value for parallelism anywhere, and 
 2) it fails only the first time if you set parallelism.default to 1 in the conf.yaml.

and you do not change the default parallelism of the job during job submission (e.g. using the {{-p}} flag), for example set it to something > 1.

Is this correct?
;;;","09/Jan/20 11:31;lirui;[~kkl0u] That's correct.
I also quickly tried with 1.9.1 and it works.
Let me also try your patch and get back to you.;;;","09/Jan/20 11:47;lirui;Hi [~kkl0u], I tried your patch and as I expected, it solves the problem if {{parallelism.default}} is set. But jobs still always fail if it's not set.;;;","10/Jan/20 09:44;kkl0u;Hi [~lirui], I tried it on the {{master}} (without my patch) with Yarn and HDFS and job submission from the command line and I cannot reproduce it. My job is:
{code:java}
StreamExecutionEnvironment streamEnv = StreamExecutionEnvironment.getExecutionEnvironment();
DataStream dataStream = streamEnv.fromCollection(Arrays.asList(1, 2, 3));
dataStream.writeAsText(""hdfs://"" + args[0] + "":9000/tmp/output"");
streamEnv.execute();
{code}
and the command in the CLI : 
 {{./bin/flink run ./MY_JOB.jar HOSTNAME}}

Also I tried it with providing parallelism using the {{-p}} option and it still works.

Could you provide some more details so that I can reproduce it?;;;","10/Jan/20 12:13;lirui;Hi [~kkl0u], just confirmed the job succeeds if I write the program as you did. But the problem persists if I create the stream env as in the description:
{code}
    Configuration configuration = GlobalConfiguration.loadConfiguration();
    configuration.set(DeploymentOptions.TARGET, RemoteExecutor.NAME);
    StreamExecutionEnvironment streamEnv = new StreamExecutionEnvironment(configuration);
{code}
Debugged and found that in your program, the execution env is created with default parallelism == 1: https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/program/ContextEnvironment.java#L51
But my program doesn't go through that code path. Do you think this is an issue or I shouldn't create the env like that in the first place?;;;","10/Jan/20 12:56;kkl0u;I think I get the problem. Before 1.10 you could only use either the {{CLI}} (i.e. the {{ContextEnv}}) or the {{RemoteStreamEnv}} and in these cases, at some point of the job submission we were reseting the parallelism to a non-negative value, while now we do not. I pushed a change in the fix in the branch above.

Could you try it out and let me know if it works in all cases, i.e. even when you do not set the parallelism? ;;;","10/Jan/20 12:57;kkl0u;Sorry I just saw your message, I think that the current branch must fix the problem. Could you verify?;;;","11/Jan/20 06:44;lirui;[~kkl0u] I just tried the latest branch and it fixes the issue, no matter whether {{parallelism.default}} is set or not. Thanks.;;;","13/Jan/20 08:47;kkl0u;Thanks for the feedback [~lirui]! I opened a PR for the issue.;;;","14/Jan/20 10:12;kkl0u;Merged on master with 704b7d90e294a57436cfe08723d720f3b7946f12
and on release-1.10 with 2147060811fadf560f6d6ea836bbc84fa6f08153;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigConstants generally excluded from japicmp,FLINK-15523,13278198,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,08/Jan/20 16:15,12/Jan/20 20:50,13/Jul/23 08:07,12/Jan/20 20:50,1.8.0,,,,,1.10.0,1.8.4,1.9.2,,Build System,,,,,0,pull-request-available,,,,"It appears that {{ConfigConstants}}, despite being {{@Public}}, has been excluded from the japicmp check since {{1.5.0}}: [https://github.com/apache/flink/commit/1f9c2d9740ffea2b59b8f5f3da287a0dc890ddbf]

This is a bit of a surprise to me and looks like a mistake (what's the point of excluding specific fields if the entire class is excluded), and if so this has repercussions on some ongoing work:
{code:java}
Breaking the build because there is at least one incompatibility: org.apache.flink.configuration.ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY:FIELD_REMOVED,
org.apache.flink.configuration.ConfigConstants.TASK_MANAGER_MEMORY_OFF_HEAP_KEY:FIELD_REMOVED,
org.apache.flink.configuration.ConfigConstants.DEFAULT_TASK_MANAGER_MEMORY_PRE_ALLOCATE:FIELD_REMOVED,
org.apache.flink.configuration.ConfigConstants.TASK_MANAGER_MEMORY_PRE_ALLOCATE_KEY:FIELD_REMOVED,
org.apache.flink.configuration.ConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY:FIELD_REMOVED,
org.apache.flink.configuration.ConfigConstants.YARN_MAX_FAILED_CONTAINERS:FIELD_REMOVED,
org.apache.flink.configuration.ConfigConstants.DEFAULT_MEMORY_MANAGER_MEMORY_FRACTION:FIELD_REMOVED
 {code}",,azagrebin,liyu,sewen,trohrmann,xtsong,,,,,,,,,,,,,,,,,"zentol commented on pull request #10812: [FLINK-15523][conf] Japicmp checks ConfigConstants
URL: https://github.com/apache/flink/pull/10812
 
 
   Reverts the removal of several `@Public` fields and re-enables the japicmp checks for `ConfigConstants`.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 12:11;githubbot;600","zentol commented on pull request #10812: [FLINK-15523][conf] Japicmp checks ConfigConstants
URL: https://github.com/apache/flink/pull/10812
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jan/20 20:44;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 12 20:50:40 UTC 2020,,,,,,,,,,"0|z0ab1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/20 16:16;chesnay;I've put it down as a blocker since several memory options were removed for 1.10 .;;;","09/Jan/20 08:58;trohrmann;Due to FLIP-49, there are some breaking changes since some of the old options no longer have a respective counter part. This was intended and there will be a migration guide how to migrate from the old settings to the new settings. Consequently, I would not block the release on the removed {{TaskManager}} memory options. 

[~azagrebin], [~xintongsong] could you confirm that the memory related options were removed deliberately?

[~chesnay] is there a way to exclude certain options from the japicmp plugin check?;;;","09/Jan/20 09:03;trohrmann;{{YARN_MAX_FAILED_CONTAINERS}} seems to be removed as part of FLINK-15359, but this should only have happened in the master and not {{1.10.0}}.;;;","09/Jan/20 09:14;trohrmann;I would suggest the following: Re-enable japicmp check for {{ConfigConstants}}, revert the removal of {{YARN_MAX_FAILED_CONTAINERS}} and exclude the memory config options explicitly.;;;","09/Jan/20 10:05;azagrebin;ConfigConstants.YARN_MAX_FAILED_CONTAINERS is tracked by FLINK-15536
ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY is now TaskManagerOptions.MANAGED_MEMORY_SIZE as a deprecated key so it should be also deprecated.

All other options have to be deprecated and they have no effect anymore at all. If we add them back it makes sense to note this in the comments.

ConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY is substituted by TaskManagerOptions.MANAGED_MEMORY_FRACTION but w/o keeping it as a deprecated key because the new option has similar but different semantics.;;;","09/Jan/20 10:28;xtsong;Agree with [~azagrebin]. 

TASK_MANAGER_MEMORY_SIZE_KEY is still valid as deprecated key of TaskManagerOptions.MANAGED_MEMORY_SIZE.

The following are removed on purpose.
- TASK_MANAGER_MEMORY_OFF_HEAP_KEY
- DEFAULT_TASK_MANAGER_MEMORY_PRE_ALLOCATE
- TASK_MANAGER_MEMORY_PRE_ALLOCATE_KEY
- TASK_MANAGER_MEMORY_FRACTION_KEY
- DEFAULT_MEMORY_MANAGER_MEMORY_FRACTION

Also, it was not clear to me what is the purpose of keeping these constants in the same class, until I see this ticket. That's why I removed TASK_MANAGER_MEMORY_SIZE_KEY at the first place. Shall we make it more explicit in the javadoc of ConfigConstants that this can be used for japicmp checks?;;;","09/Jan/20 11:30;azagrebin;[~xintongsong] As I understand, it is a general contract for the API classes annotated with @Public. ConfigConstants was just accidentally not part of the check.;;;","09/Jan/20 11:31;sewen;True, the original exclusion looks like a mistake. Should not have been a hotfix :(;;;","12/Jan/20 20:50;chesnay;master:
5c037f2690fd0459c2395f3f82428077a3207fd2
7a6ca9c03f67f488e40a114e94c389a5cfb67836 
1.10:
584879551e5db3445b1e2f9a6789847e3e44a887
a08bc83c552f562c86c310124f29c7b12ee05613 
1.9:
c33f0d25cb5d33667fd00dfdb94bb9704bda0fa9
1.8:
098adf45801efcd851923000303c33321e32bc0c ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misleading root cause exception when cancelling the job,FLINK-15522,13278193,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,trohrmann,trohrmann,08/Jan/20 15:41,13/Jan/20 10:30,13/Jul/23 08:07,13/Jan/20 08:36,1.10.0,1.11.0,1.9.1,,,1.10.0,1.9.2,,,Client / Job Submission,Runtime / Coordination,,,,0,pull-request-available,,,,"When cancelling a Flink job, the following stack trace gets displayed

{code}
 The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: Job failed. (JobID: d0e8c2026709385166bcc0253c30742e)
        at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:262)
        at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:338)
        at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:60)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1507)
        at org.apache.flink.streaming.examples.statemachine.StateMachineExample.main(StateMachineExample.java:142)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:576)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:438)
        at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:274)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:746)
        at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:273)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:205)
        at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1010)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1083)
        at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1083)
Caused by: org.apache.flink.runtime.client.JobCancellationException: Job was cancelled.
        at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:148)
        at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:259)
        ... 18 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: No pooled slot available and request to ResourceManager for new slot failed
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.slotRequestToResourceManagerFailed(SlotPoolImpl.java:357)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.lambda$requestSlotFromResourceManager$1(SlotPoolImpl.java:345)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
        at akka.actor.ActorCell.invoke(ActorCell.scala:561)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
        at akka.dispatch.Mailbox.run(Mailbox.scala:225)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Could not fulfill slot request 7ab196daeb73e353c460455899a7622f.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:871)
        at akka.dispatch.OnComplete.internal(Future.scala:263)
        at akka.dispatch.OnComplete.internal(Future.scala:261)
        at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
        at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
        at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
        at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
        at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
        at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
        at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
        at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
        at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
        at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
        at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
        at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
        at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
        ... 4 more
Caused by: org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Could not fulfill slot request 7ab196daeb73e353c460455899a7622f.
        at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.registerSlotRequest(SlotManagerImpl.java:315)
        at org.apache.flink.runtime.resourcemanager.ResourceManager.requestSlot(ResourceManager.java:443)
        at sun.reflect.GeneratedMethodAccessor87.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
        at akka.actor.ActorCell.invoke(ActorCell.scala:561)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
        at akka.dispatch.Mailbox.run(Mailbox.scala:225)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
        ... 4 more
Caused by: org.apache.flink.runtime.resourcemanager.exceptions.UnfulfillableSlotRequestException: Could not fulfill slot request 7ab196daeb73e353c460455899a7622f. Requested resource profile (ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1}) is unfulfillable.
        at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.internalRequestSlot(SlotManagerImpl.java:768)
        at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.registerSlotRequest(SlotManagerImpl.java:310)
        ... 26 more
{code}

The reported root cause is {{UnfulfillableSlotRequestException}} which is a bit misleading. If the user cancels a running job then the root cause should be the cancellation.",,gjy,sewen,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,"zhuzhurk commented on pull request #10825: [FLINK-15522][runtime] JobResult takes ExecutionGraph failure cause as its failure cause only if job is FAILED
URL: https://github.com/apache/flink/pull/10825
 
 
   ## What is the purpose of the change
   
   In JobResult#createFrom it sets the latest global failure cause of the ExecutionGraph to be the cause of the JobResult if the job is terminated but not FINISHED.
   So if the job is CANCELED and any global failure had ever happened, the latest global failure cause would be shown as the root cause of the canceling. This results in a misleading root cause exception when a user cancels the job.
   
   ## Brief change log
   
     - *Changed JobResult to take ExecutionGraph failure cause as its failure cause only if the job is FAILED*
    - *also checked the failure cause to be not null if a job is FAILED*
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     - *Fixed testCancelledJobThrowsJobCancellationException*
     - *Added testFailureResultRequiresFailureCause*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/20 08:34;githubbot;600","zhuzhurk commented on pull request #10825: [FLINK-15522][runtime] JobResult takes ExecutionGraph failure cause as its failure cause only if job is FAILED
URL: https://github.com/apache/flink/pull/10825
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jan/20 08:30;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 13 08:36:21 UTC 2020,,,,,,,,,,"0|z0ab08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/20 22:55;sewen;The above stack trace was produced by launching the job from the command line and cancelling from the web UI.

The bug does not seem to be deterministic - it does not happen every time.;;;","09/Jan/20 07:49;zhuzh;I think the reason why this happens is that in {{JobResult#createFrom}} would set the latest global failure cause of the ExecutionGraph to be the cause of the {{JobResult}} if the job is terminated but not FINISHED. 
So if the job is CANCELED and any global failure had ever happened, the latest global failure cause would be shown as the root cause of the canceling.
I think in the case that a job is CANCELED, we can just keep {{JobResult#serializedThrowable}} to be null so that the root cause would simple be {{JobCancellationException: Job was cancelled.}}.
;;;","09/Jan/20 11:33;sewen;That sounds reasonable!;;;","10/Jan/20 02:11;zhuzh;I will take this issue then and open a PR soon.;;;","13/Jan/20 08:36;zhuzh;Fixed via:

master:
a1a27d01daa859e31d7b8bed1a1f176aa1e5c5ed

release-1.10:
ed57867e342c574907329de45b7227cb60a23007

release-1.9:
c0974b8c7346ef907fd6debafb05ec1ce3326757;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't hide web frontend side pane automatically,FLINK-15518,13278161,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,vthinkxie,aljoscha,aljoscha,08/Jan/20 13:44,09/Jan/20 13:57,13/Jul/23 08:07,09/Jan/20 13:57,1.10.0,1.9.0,,,,1.10.0,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,As mentioned in FLINK-13386 the side pane hides automatically in some cases but not all cases. When I was debugging or trying the web frontend I found this behaviour a bit disconcerting. Could we disable the hiding by default? The user can still manually hide if they want to.,,aljoscha,dwysakowicz,jark,liyu,vthinkxie,,,,,,,,,,,,,,,,,"vthinkxie commented on pull request #10803: [FLINK-15518][web]: don not hide web frontend side pane automatically
URL: https://github.com/apache/flink/pull/10803
 
 
   ## What is the purpose of the change
   
   https://issues.apache.org/jira/browse/FLINK-15518
   
   ## Brief change log
   
   support more metric display at once
   
   ## Verifying this change
   
     - *Go to the job page*
     - *Side tab do not hide automaticlly*
   
   
   before:
   
   ![image](https://user-images.githubusercontent.com/1506722/72031252-d4017480-32c6-11ea-9e31-99176c095097.png)
   
   
   after:
   
   ![image](https://user-images.githubusercontent.com/1506722/72031264-e085cd00-32c6-11ea-8429-7f3439558c77.png)
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 02:01;githubbot;600","wuchong commented on pull request #10803: [FLINK-15518][web]: don not hide web frontend side pane automatically
URL: https://github.com/apache/flink/pull/10803
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 13:53;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 09 13:57:22 UTC 2020,,,,,,,,,,"0|z0aat4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/20 13:44;aljoscha;cc [~vthinkxie];;;","08/Jan/20 15:03;dwysakowicz;+1 ;);;;","09/Jan/20 02:01;vthinkxie;Hi [~aljoscha]

the side pane will hide when the user first enters the job page under the current design

I have removed it in [https://github.com/apache/flink/pull/10803]

thanks for your suggestion.;;;","09/Jan/20 03:50;jark;Big +1 ;;;","09/Jan/20 13:57;jark;1.11.0: dba5b9e0138b667c3ecd32f7b16645d531477720
1.10.0: 4f6ce74618371b1dea50a57901be536aa564b138;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State TTL Compaction Filter option off by default,FLINK-15506,13277979,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,azagrebin,sewen,sewen,07/Jan/20 17:41,23/Jan/20 14:35,13/Jul/23 08:07,16/Jan/20 15:17,,,,,,1.10.0,,,,Runtime / State Backends,,,,,0,,,,,"Compaction filters for state TTL cleanup is on by default after FLINK-14898 in some places, but the config option still is set to {{false}} by default.

[https://github.com/apache/flink/blob/6ff392842f9dc4d3c9c808e7912558d477826379/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBOptions.java#L63]",,azagrebin,liyu,sewen,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15621,FLINK-15741,FLINK-10471,FLINK-14898,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 16 15:17:51 UTC 2020,,,,,,,,,,"0|z0a9oo:",9223372036854775807,'state.backend.rocksdb.ttl.compaction.filter.enabled' is now true by default. Disable it only if you experience serious performance degradation during RocksDB compaction.,,,,,,,,,,,,,,,,,,,"15/Jan/20 15:41;azagrebin;Having the TTL compaction filter always enabled, currently results in that it runs for all Flink states independently from whether they are configured to have TTL or not. This theoretically increases compaction time and wastes some CPU cycles on calling the filter with the disabled TTL check. To test this, we have to try it by running a job with high compaction pressure which is difficult to generate. The common performance benchmark should be at least checked. In general, we do not expect any significant performance degradation as the calling of idle filter should be a negligible internal native C++ call w/o using JNI.

This option can be also quite confusing for users. We can deprecate it but keep to fallback atm. Then see if there are any actual performance degradation reported by users. If none, we can remove it completely later.;;;","16/Jan/20 15:17;azagrebin;The common performance benchmark looked good.
 merged into master by 4a66f107b0e95d4729b777e9f289c2d42f95de28
 merged into 1.10 by 72a0158daeebbd821aa5d68a212f83648f4b1572;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink suppresses stdout/stderr during JobGraph generation from JAR,FLINK-15504,13277956,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,07/Jan/20 15:43,29/Jan/20 12:19,13/Jul/23 08:07,10/Jan/20 17:10,,,,,,1.10.0,,,,Client / Job Submission,,,,,1,pull-request-available,,,,"Flink uses the {{OptimizerPlanEnvironment}} which replaces stdout/stderr during job graph creation. This was intended only for previewing the plan, but other parts of Flink, e.g. the Rest API have started to use this code as well. 

It may be acceptable to suppress output during plan preview, but we should not suppress the output during execution, e.g. via the REST API.",,aljoscha,dwysakowicz,gjy,guoyangze,klion26,liyu,mxm,thw,wangyang0918,,,,,,,,,,,,,"mxm commented on pull request #10811: [FLINK-15504] Allow output to stdout/stderr during execution of PackagedProgram
URL: https://github.com/apache/flink/pull/10811
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   We suppress the output to stdout/stderr during plan extraction via
   PackagedProgram. This has unintended consequences for users who are looking into
   debugging their Flink programs during JobGraph creation.
   
   
   
   ## Brief change log
   
   This change removes the suppression of output when we run the JARs. The plan
   preview still suppresses the output to avoid spaming the logs during plan
   preview.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows: 
   
   Please see `JarHandlerTest` and `OptimizerPlanEnvironmentTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no )
     - The S3 file system connector: (no )
   
   ## Documentation
   
     - Does this pull request introduce a new feature? ( no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 12:11;githubbot;600","mxm commented on pull request #10811: [FLINK-15504] Allow output to stdout/stderr during execution of PackagedProgram
URL: https://github.com/apache/flink/pull/10811
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/20 17:08;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,BEAM-9060,,,,,,FLINK-15651,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 29 12:19:15 UTC 2020,,,,,,,,,,"0|z0a9jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/20 03:56;wangyang0918;[https://lists.apache.org/thread.html/0b47666d649501bab117b1b0f3262e7483ad8aa40872e76a414268d3%40%3Cuser.flink.apache.org%3E]

 

Some users come across with the same problem.;;;","10/Jan/20 17:16;mxm;Backporting this also to {{release-1.10}}.;;;","11/Jan/20 13:37;mxm;master: 4bb4a50c4ed6c909dc98788bc239ba77340d6fb7
release-1.10: 0ea4dd7e9d56a017743ca6794d28537800faab6f
;;;","20/Jan/20 15:21;gjy;After an offline discussion with [~aljoscha], we decided to remove the release note _""Allow outputting to stdout/stderr during JobGraph creation from JAR""_. If you think the release note is justified, feel free to add it back.;;;","21/Jan/20 14:35;mxm;I think it is not a bad idea to inform users of such changes, in case they have run into this themselves. I don't know what the barrier is for getting into the release notes. I don't mind if we do not include the note.;;;","28/Jan/20 17:51;gjy;There are some examples here [https://ci.apache.org/projects/flink/flink-docs-release-1.9/#release-notes]
If you expect existing setups to break due to this change, we should include your original release note. I have just created a PR for all the release notes: https://github.com/apache/flink/pull/10937;;;","29/Jan/20 08:52;mxm;I think it is worth including a note in the release notes. It's not a breaking change but it has been an annoyance to production users, so we should do our best to inform them.;;;","29/Jan/20 12:19;gjy;Thanks for the quick reply. Considering that we have more than twice as many release notes in 1.10 compared to 1.9, I would like not to expand the release notes further. Moreover, we have closed hundreds of bug issues for 1.10.0 which are not mentioned in the release notes in the user docs and some have arguably as much impact is this issue. A list of all issues grouped by issue type [1] will be published with the release blog post.

[1] https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&version=12345845;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming TopN operator doesn't reduce outputs when rank number is not required ,FLINK-15497,13277866,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingzhang,ykt836,ykt836,07/Jan/20 06:42,27/Jan/22 17:43,13/Jul/23 08:07,21/Jan/20 03:05,1.9.1,,,,,1.10.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"As we described in the doc: [https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/sql.html#top-n]

when rank number is not required, we can reduce some output, like unnecessary retract messages. 

Here is an example which can re-produce:
{code:java}
val data = List(
  (""aaa"", 97.0, 200.0),
  (""bbb"", 67.0, 200.0),
  (""bbb"", 162.0, 200.0)
)

val ds = failingDataSource(data).toTable(tEnv, 'guid, 'a, 'b)
tEnv.registerTable(""T"", ds)

val aggreagtedTable = tEnv.sqlQuery(
  """"""
    |select guid,
    |    sum(a) as reached_score,
    |    sum(b) as max_score,
    |    sum(a) / sum(b) as score
    |from T group by guid
    |"""""".stripMargin
)

tEnv.registerTable(""T2"", aggreagtedTable)

val sql =
  """"""
    |SELECT guid, reached_score, max_score, score
    |FROM (
    |  SELECT *,
    |      ROW_NUMBER() OVER (ORDER BY score DESC) as rank_num
    |  FROM T2)
    |WHERE rank_num <= 5
  """""".stripMargin

val sink = new TestingRetractSink
tEnv.sqlQuery(sql).toRetractStream[Row].addSink(sink).setParallelism(1)
env.execute()
{code}
In this case, the output is:
{code:java}
(true,aaa,97.0,200.0,0.485)
(true,bbb,67.0,200.0,0.335) 
(false,bbb,67.0,200.0,0.335) 
(true,bbb,229.0,400.0,0.5725) 
(false,aaa,97.0,200.0,0.485) 
(true,aaa,97.0,200.0,0.485)
{code}
But the last 2 messages are unnecessary. ",,jark,leonard,liyu,lzljs3620320,,,,,,,,,,,,,,,,,,"beyond1920 commented on pull request #10823: [FLINK-15497][table-planner-blink] Reduce outputs when rank number is not required in Retractable topN
URL: https://github.com/apache/flink/pull/10823
 
 
   ## What is the purpose of the change
   
   As we described in the doc: https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/sql.html#top-n
   
   When rank number is not required, we can reduce some output, like unnecessary retract messages. 
   
   However the optimization does not work for Retractable topN before, the pr aims to resolve the issue.
   
   ## Brief change log
   
     - Update Retractable TopN function
     - Add UT
   
   ## Verifying this change
   
   UT
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/20 08:13;githubbot;600","wuchong commented on pull request #10823: [FLINK-15497][table-planner-blink] Reduce outputs when rank number is not required in Retractable topN
URL: https://github.com/apache/flink/pull/10823
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jan/20 02:51;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 21 03:05:15 UTC 2020,,,,,,,,,,"0|z0a8zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/20 06:47;ykt836;cc [~jark]  [~qingru.zhang];;;","14/Jan/20 03:07;lzljs3620320;[~qingru.zhang] Please click ""start in-progress"", thanks.;;;","21/Jan/20 03:05;jark;1.11.0: bf0945769278079f0e004080b78e5dfe87f7c320
1.10.0: 9c7fc0d4eb91fc486f8497ab07e55aa7e15222de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
time field index wrong in LogicalWindowAggregateRuleBase,FLINK-15494,13277859,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,libenchao,libenchao,libenchao,07/Jan/20 05:06,04/Feb/20 08:35,13/Jul/23 08:07,04/Feb/20 08:35,1.10.0,1.9.1,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When we use two time window in blink planner, will result in calculating wrong time field index.

 

This has two phenomenon as far as I know:
 # wrong index may has correct field name, and will pass the build, but has wrong rowtime in runtime.
 # wrong index has incorrect field name, and will not pass the build.

 

How to reproduce this problem:

I added a unit test in `WindowAggregateITCase`:
{code:java}
@Test
def testDoubleTumbleWindow(): Unit = {
  val stream = failingDataSource(data)
    .assignTimestampsAndWatermarks(
      new TimestampAndWatermarkWithOffset
        [(Long, Int, Double, Float, BigDecimal, String, String)](10L))
  val table = stream.toTable(tEnv,
    'rowtime.rowtime, 'int, 'double, 'float, 'bigdec, 'string, 'name)
  tEnv.registerTable(""T1"", table)

  val sql =
    """"""
      |SELECT SUM(cnt)
      |FROM (
      |  SELECT COUNT(1) AS cnt, TUMBLE_ROWTIME(rowtime, INTERVAL '10' SECOND) AS ts
      |  FROM T1
      |  GROUP BY `int`, `string`, TUMBLE(rowtime, INTERVAL '10' SECOND)
      |)
      |GROUP BY TUMBLE(ts, INTERVAL '10' SECOND)
      |"""""".stripMargin

  val sink = new TestingAppendSink
  tEnv.sqlQuery(sql).toAppendStream[Row].addSink(sink)
  env.execute()

  val expected = Seq(""9"")
  assertEquals(expected.sorted, sink.getAppendResults.sorted)
}
{code}
which will result in exception:
{code:java}
java.lang.RuntimeException: Error while applying rule StreamExecGroupWindowAggregateRule(in:LOGICAL,out:STREAM_PHYSICAL), args [rel#308:FlinkLogicalWindowAggregate.LOGICAL.any.None: 0.false.UNKNOWN(input=RelSubset#307,group={},EXPR$0=SUM($1),window=TumblingGroupWindow('w$, int, 10000),properties=)]java.lang.RuntimeException: Error while applying rule StreamExecGroupWindowAggregateRule(in:LOGICAL,out:STREAM_PHYSICAL), args [rel#308:FlinkLogicalWindowAggregate.LOGICAL.any.None: 0.false.UNKNOWN(input=RelSubset#307,group={},EXPR$0=SUM($1),window=TumblingGroupWindow('w$, int, 10000),properties=)]
 at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:235) at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:631) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:327) at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:167) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:89) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:248) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:151) at org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.scala:210) at org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.scala:107) at org.apache.flink.table.api.scala.TableConversions.toAppendStream(TableConversions.scala:101) at org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.testDoubleTumbleWindow(WindowAggregateITCase.scala:125) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)Caused by: java.lang.IllegalArgumentException: field [int] not found; input fields are: [$f0, cnt] at org.apache.calcite.tools.RelBuilder.field(RelBuilder.java:433) at org.apache.calcite.tools.RelBuilder.field(RelBuilder.java:416) at org.apache.flink.table.planner.plan.utils.AggregateUtil$.timeFieldIndex(AggregateUtil.scala:696) at org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecGroupWindowAggregateRule.convert(StreamExecGroupWindowAggregateRule.scala:80) at org.apache.calcite.rel.convert.ConverterRule.onMatch(ConverterRule.java:144) at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:208) ... 62 morejava.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.flink.table.planner.runtime.utils.StreamingWithStateTestBase.after(StreamingWithStateTestBase.scala:81) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)java.lang.RuntimeException: Error while applying rule StreamExecGroupWindowAggregateRule(in:LOGICAL,out:STREAM_PHYSICAL), args [rel#630:FlinkLogicalWindowAggregate.LOGICAL.any.None: 0.false.UNKNOWN(input=RelSubset#629,group={},EXPR$0=SUM($1),window=TumblingGroupWindow('w$, int, 10000),properties=)]
 at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:235) at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:631) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:327) at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:167) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:89) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:248) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:151) at org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.scala:210) at org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.scala:107) at org.apache.flink.table.api.scala.TableConversions.toAppendStream(TableConversions.scala:101) at org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.testDoubleTumbleWindow(WindowAggregateITCase.scala:125) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)Caused by: java.lang.IllegalArgumentException: field [int] not found; input fields are: [$f0, cnt] at org.apache.calcite.tools.RelBuilder.field(RelBuilder.java:433) at org.apache.calcite.tools.RelBuilder.field(RelBuilder.java:416) at org.apache.flink.table.planner.plan.utils.AggregateUtil$.timeFieldIndex(AggregateUtil.scala:696) at org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecGroupWindowAggregateRule.convert(StreamExecGroupWindowAggregateRule.scala:80) at org.apache.calcite.rel.convert.ConverterRule.onMatch(ConverterRule.java:144) at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:208) ... 62 more
{code}",,jark,libenchao,liyu,lzljs3620320,,,,,,,,,,,,,,,,,,"libenchao commented on pull request #10784: [FLINK-15494] Fix time field index wrong in LogicalWindowAggregateRul…
URL: https://github.com/apache/flink/pull/10784
 
 
   …eBase
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix time field index error in some cases.
   
   ## Brief change log
   
   Calculate time field index based on new `LogicalProject` directly in `LogicalWindowAggregateRuleBase`.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   WindowAggregateITCase. testDoubleTumbleWindow
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 07:26;githubbot;600","wuchong commented on pull request #10784: [FLINK-15494] Fix time field index wrong in LogicalWindowAggregateRul…
URL: https://github.com/apache/flink/pull/10784
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Feb/20 05:29;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 04 08:35:10 UTC 2020,,,,,,,,,,"0|z0a8y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/20 07:02;jark;Thanks for reporting this [~libenchao]. Would like to contribute the fix?;;;","07/Jan/20 07:08;libenchao;[~jark] Yes. I have fixed this internally, and would like to contribute it to community. ;;;","01/Feb/20 05:09;liyu;Thanks for pointing this issue out in RC1 voting [~libenchao].

According to the current priority, this seems more like a nice-to-fix one for 1.10.0 instead of a blocker. Could you double check and let us know the reason if you think we should escalate the priority to Blocker [~libenchao] [~jark]? Thanks.

If it's not a blocker but the PR could be merged with proper process (no rush please), it will be automatically included in the next RC. However, we won't wait for it to be merged to produce the next RC or release, FYI.;;;","01/Feb/20 05:39;lzljs3620320;I think it should not be a blocker, but nice to merge it in 1.10. [~jark] will take a look and try merging but no guarantee.;;;","01/Feb/20 05:52;liyu;Thanks for the confirmation [~lzljs3620320], good to know :-);;;","04/Feb/20 08:35;jark;Fixed in 1.11.0: 5281212cac3d4a190f3ca2f100d5a9caa2db92e1
1.10.0: 2f6d13818baf6e783eb2028fda131b89ea823cc2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaInternalProducerITCase.testProducerWhenCommitEmptyPartitionsToOutdatedTxnCoordinator failed on travis,FLINK-15493,13277813,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fpaul,dian.fu,dian.fu,07/Jan/20 01:43,27/Dec/21 08:58,13/Jul/23 08:07,13/Dec/21 16:29,1.10.0,1.13.0,1.14.0,1.15.0,,1.13.6,1.14.3,1.15.0,,Connectors / Kafka,Tests,,,,0,auto-deprioritized-critical,auto-unassigned,pull-request-available,test-stability,"FlinkKafkaInternalProducerITCase.testProducerWhenCommitEmptyPartitionsToOutdatedTxnCoordinator failed on travis with the following exception:


{code}
Test testProducerWhenCommitEmptyPartitionsToOutdatedTxnCoordinator(org.apache.flink.streaming.connectors.kafka.FlinkKafkaInternalProducerITCase) failed with: org.junit.runners.model.TestTimedOutException: test timed out after 30000 milliseconds at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:502) at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:92) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260) at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.createTestTopic(KafkaTestEnvironmentImpl.java:177) at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.createTestTopic(KafkaTestEnvironment.java:115) at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.createTestTopic(KafkaTestBase.java:197) at org.apache.flink.streaming.connectors.kafka.FlinkKafkaInternalProducerITCase.testProducerWhenCommitEmptyPartitionsToOutdatedTxnCoordinator(FlinkKafkaInternalProducerITCase.java:176) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)
{code}
instance: [https://api.travis-ci.org/v3/job/633307060/log.txt]",,azagrebin,becket_qin,dian.fu,dwysakowicz,fpaul,gaoyunhaii,kezhuw,liyu,maguowei,mapohl,rmetzger,RocMarshal,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16610,,,,,,,,,,,,,,,,FLINK-25455,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 13 16:29:47 UTC 2021,,,,,,,,,,"0|z0a8ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/20 01:54;dian.fu;cc [~becket_qin];;;","07/Jan/20 07:08;becket_qin;It is unclear why the topic creation took longer than timeout. We could reduce the request timeout of the AdminClient to something like 15 seconds, so at least one retry would be made before the test timeout. But I would actually prefer do nothing here as it could be just caused by slow overwhelmed Travis machine.;;;","07/Jan/20 07:17;dian.fu;I checked the code and there are no changes for the Kafka connector recently. I'm +1 to do nothing right now and let's see if it happens frequently.;;;","07/Jan/20 11:20;becket_qin;[~trohrmann@apache.org] we tend to think this is an intermittent problem caused by slow Travis machine. I'll close this ticket as not reproducible for now. We can revisit if the topic creation failure happens again in test cases.;;;","13/Jan/20 13:25;azagrebin;Looks the same
[https://api.travis-ci.org/v3/job/636280114/log.txt];;;","09/Apr/20 17:41;rmetzger;https://api.travis-ci.org/v3/job/662458976/log.txt
https://travis-ci.org/github/apache/flink/jobs/672935021;;;","10/May/20 04:32;liyu;Another instance in latest release-1.10 crone build:
https://api.travis-ci.org/v3/job/685113682/log.txt
https://travis-ci.org/github/apache/flink/jobs/685113682;;;","22/May/20 14:57;trohrmann;Seems that the issue has not been completely resolved [~becket_qin].;;;","25/May/20 00:14;becket_qin;I have been running this test in a loop over the weekend, and was not able to reproduce the issue after over 1100 runs. I'll try running the full test instead of a single test method and see how it goes.;;;","25/May/20 02:51;becket_qin;I tried to run the full {{FlinkKafkaInternalProducerITCase}} for the morning with the debug logging enabled, and I saw a few timeout cases that were not always from the topic creation. It looks that this {{testProducerWhenCommitEmptyPartitionsToOutdatedTxnCoordinator}} is usually taking long to finish due to the process of broker restart. After I bumped up the test timeout from 30 seconds to 60 seconds, it seems much more stable.

However, I have not convinced myself that is the right way to go, given that most of the failures reported in this ticket is still from {{createTopic(). }}I'll run the full test a bit longer, say for a day, and see if the topic creation timeout can be reproduced. If that cannot be reproduced. I think we need to enable some debug level logging in the tests so we have more information to debug when the test fails again in CI.;;;","25/May/20 09:39;becket_qin;I was able to reproduce the issue after 118 full test runs. From the logs, this seems a purgatory problem on the Kafka broker side. At this point we should be able to workaround this by lower the request timeout in the `KafkaAdminClient`. I'll dig into this a bit more to see if the issue is still there in the Kafka trunk and submit tickets to Kafka.;;;","05/Jul/20 01:58;dian.fu;Another instance on the master branch: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4239&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7];;;","17/Sep/20 12:27;dian.fu;Another instance on 1.11: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6588&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20;;;","30/Oct/20 01:20;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8626&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=e8fcc430-213e-5cce-59d4-6942acf09121;;;","06/Nov/20 19:05;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9162&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","17/Nov/20 16:05;rmetzger;on 1.11: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9692&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20;;;","23/Feb/21 07:14;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13611&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","24/Feb/21 00:21;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13638&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","03/Mar/21 05:54;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14015&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=f09203c9-1af8-53a6-da0c-2e60f5418512;;;","16/Apr/21 11:01;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","22/Apr/21 14:22;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17035&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6959;;;","29/Apr/21 22:52;flink-jira-bot;This issue was marked ""stale-assigned"" 7 ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;","20/May/21 10:53;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","01/Jun/21 23:29;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","23/Jun/21 02:25;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19323&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6381;;;","16/Jul/21 05:52;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20519&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6587;;;","02/Aug/21 04:16;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21230&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=53f6305f-55e6-561c-8f1e-3a1dde2c77df&l=6573;;;","01/Sep/21 02:07;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23195&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7393;;;","08/Sep/21 03:59;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23718&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582&l=7214;;;","17/Sep/21 03:31;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24230&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6247;;;","18/Sep/21 02:02;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24282&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7428;;;","20/Sep/21 10:48;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=477&view=logs&j=d543d572-9428-5803-a30c-e8e09bf70915&t=4e4199a3-fbbb-5d5b-a2be-802955ffb013&l=7201;;;","18/Oct/21 09:09;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25121&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6334;;;","18/Oct/21 09:16;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25130&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6974;;;","09/Nov/21 10:18;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26149&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7457;;;","09/Nov/21 10:19;trohrmann;cc [~arvid], [~becket_qin];;;","23/Nov/21 16:58;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26859&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6952;;;","23/Nov/21 18:16;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26884&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=6742;;;","23/Nov/21 18:17;trohrmann;[~fpaul] did you have a chance looking at this problem?;;;","29/Nov/21 11:26;trohrmann;[~fpaul] can you give an update for this ticket?;;;","29/Nov/21 12:08;fpaul;Sorry for the late response I opened a pull request to add a retry rule to this test class [https://github.com/apache/flink/pull/17944] . We already do this on the older release branches and I think we cannot stabilize these tests without moving the complete test infrastructure to a dockerized environment. 

So far we haven't seen any of these timeout exceptions in the KafkaSink tests which are all running against a Kafka docker container. Hopefully, we can remove the old FlinkKafkaProducer soonish and with it all its test infrastructure.;;;","29/Nov/21 12:52;trohrmann;Thanks for the update [~fpaul]. Could you link this ticket to FLINK-21214 so that we don't forget to close it once the former ticket is closed?;;;","02/Dec/21 07:57;fpaul;I'll merged a fix to master 2e018d0bc04935e50f0f602405c2f610af744e1f and release-1.14 a67afb20402040f4c489dad7285308439c4776e8.
I am currently working on the backport to release-1.13.;;;","02/Dec/21 12:03;fpaul;Merged 

master: 2e018d0bc04935e50f0f602405c2f610af744e1f

release-1.14: a67afb20402040f4c489dad7285308439c4776e8

release-1.13: 8ce3446cb522e47ce6893be8c6bb1205ab55655f;;;","13/Dec/21 16:29;gaoyunhaii;Hi~ since this issue has been stabilized for some days after adding the retry rules, I'll first mark it as resolved and we might reopen it if it reproduced~ ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaITCase.testOneSourceMultiplePartitions fails on Travis,FLINK-15490,13277714,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,becket_qin,trohrmann,trohrmann,06/Jan/20 14:17,10/Jan/20 15:55,13/Jul/23 08:07,10/Jan/20 15:55,1.10.0,,,,,1.10.0,,,,Connectors / Kafka,Tests,,,,0,pull-request-available,test-stability,,,"The test {{KafkaITCase.testOneSourceMultiplePartitions}} failed on Travis because it received a duplicate value:

{code}
13:10:49,276 INFO  org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper  - ============================> Failing mapper  1: count=2802, totalCount=3167
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:648)
	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:77)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1628)
	at org.apache.flink.test.util.TestUtils.tryExecute(TestUtils.java:35)
	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runOneSourceMultiplePartitionsExactlyOnceTest(KafkaConsumerTestBase.java:912)
	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testOneSourceMultiplePartitions(KafkaITCase.java:102)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:186)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:181)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:175)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:476)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380)
	at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.Exception: Received a duplicate: 4095
	at org.apache.flink.streaming.connectors.kafka.testutils.ValidatingExactlyOnceSink.invoke(ValidatingExactlyOnceSink.java:57)
	at org.apache.flink.streaming.connectors.kafka.testutils.ValidatingExactlyOnceSink.invoke(ValidatingExactlyOnceSink.java:36)
	at org.apache.flink.streaming.api.functions.sink.SinkFunction.invoke(SinkFunction.java:52)
	at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:488)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.lang.Thread.run(Thread.java:748)
{code}

https://api.travis-ci.org/v3/job/633245929/log.txt",,becket_qin,gjy,liyu,trohrmann,,,,,,,,,,,,,,,,,,"becketqin commented on pull request #10794: [FLINK-15490][kafka][test-stability] Enable idempotence producing in …
URL: https://github.com/apache/flink/pull/10794
 
 
   …KafkaITCase to avoid intermittent test failure.
   
   ## What is the purpose of the change
   Kafka IT case produces messages without enabling idempotence for `KafkaProducer`. This may cause occasional failure in some tests with duplication check. This patch enables idempotent Kafka producer.
   
   ## Brief change log
   Enable idempotent Kafka producer in KafkaITCase.
   
   
   ## Verifying this change
   This change fixes an existing test case, i.e. KafkaITCase.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/20 06:07;githubbot;600","becketqin commented on pull request #10794: [FLINK-15490][kafka][test-stability] Enable idempotence producing in …
URL: https://github.com/apache/flink/pull/10794
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/20 15:54;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 10 15:55:09 UTC 2020,,,,,,,,,,"0|z0a81s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/20 14:17;trohrmann;[~jqin] have you seen such a failure before?;;;","06/Jan/20 15:29;becket_qin;[~trohrmann] I did not see this failure before. But from the logs and code, it is possible that the duplication is caused on the producer side because the producer was producing with {{AT_LEAST_ONCE}} semantic, so a {{ProducerRequest}} retry could cause duplication. We probably should fix that first.;;;","10/Jan/20 15:55;becket_qin;Merged.
master: f0f9343a35ff21017e2406614b34a9b1f2712330
release-1.10: a1ba3f98c400fbad0d9719b251470f0c00a51db3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebUI log refresh not working,FLINK-15489,13277700,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,vthinkxie,dwysakowicz,dwysakowicz,06/Jan/20 13:02,29/Jan/20 08:55,13/Jul/23 08:07,16/Jan/20 11:45,1.10.0,1.9.1,,,,1.10.0,1.9.2,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"There is no way to query the latest state of logs of jobmanager/taskmanager.

The Web UI show only the first version that was ever displayed.

How to reproduce:
* (not sure if necessary) configure logback as described here: https://ci.apache.org/projects/flink/flink-docs-stable/dev/best_practices.html#use-logback-when-running-flink-on-a-cluster
* start a cluster
* show jobmanager logs in the Web UI
* run example job
* check again the jobmanager logs, there is no trace of the job. Clicking the refresh button does not help",,dwysakowicz,gjy,guoyangze,lining,trohrmann,vthinkxie,,,,,,,,,,,,,,,,"vthinkxie commented on pull request #10842: [FLINK-15489][web]: add cache control no-cache to log api
URL: https://github.com/apache/flink/pull/10842
 
 
   ## What is the purpose of the change
   
   fix https://issues.apache.org/jira/browse/FLINK-14270
   
   ## Brief change log
   
   support more metric display at once
   
   ## Verifying this change
   
     - *Go to the log page*
     - *Click the refresh button*
     - *Check the network panel if the log with no-cache header*
   
   
   before:
   <img width=""1022"" alt=""before"" src=""https://user-images.githubusercontent.com/1506722/72252288-f7674f00-3639-11ea-9861-93157e789082.png"">
   
   
   after:
   <img width=""1190"" alt=""after"" src=""https://user-images.githubusercontent.com/1506722/72252293-fa623f80-3639-11ea-8a8a-049bf09b2677.png"">
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jan/20 11:22;githubbot;600","GJL commented on pull request #10842: [FLINK-15489][web]: add cache control no-cache to log api
URL: https://github.com/apache/flink/pull/10842
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/20 11:38;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/20 11:00;vthinkxie;6E9A4D72-6664-4B00-B02F-57EB2695D967.png;https://issues.apache.org/jira/secure/attachment/12990412/6E9A4D72-6664-4B00-B02F-57EB2695D967.png","09/Jan/20 10:58;vthinkxie;7A1CF604-032D-46D8-AC70-FBB884E868B9.png;https://issues.apache.org/jira/secure/attachment/12990411/7A1CF604-032D-46D8-AC70-FBB884E868B9.png","08/Jan/20 17:57;gjy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12990237/screenshot-1.png","09/Jan/20 11:08;gjy;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/12990413/screenshot-2.png",,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 16 11:45:03 UTC 2020,,,,,,,,,,"0|z0a7yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/20 15:44;gjy;I promoted this to Blocker.;;;","06/Jan/20 15:58;dwysakowicz;I did not mark it as a blocker, because I suspect it was present also in the previous versions. I have not checked it though.;;;","06/Jan/20 16:03;gjy;[~vthinkxie], [~lining] Do you want to take a look?;;;","07/Jan/20 02:26;vthinkxie;Hi [~dwysakowicz]

could you find the log file in the log folder? if the log file is in the folder, the problem may caused by the design of the previous version.

we are working to change it in FLIP-75 [https://docs.google.com/document/d/1tIa8yN2prWWKJI_fa1u0t6h1r6RJpp56m48pXEyh6iI/edit#heading=h.fqhp0akm21zi]

and the related Jira is here https://issues.apache.org/jira/browse/FLINK-13987;;;","08/Jan/20 18:01;gjy;I just tested this in 1.9.1. Indeed, the JM log refresh does not work as expected. In the Firefox inspector I can see that most of the XHR requests are hitting the browser cache when I press the refresh button (see attached screenshot).
 !screenshot-1.png! 

I think this is because we set the {{Cache-Control}} HTTP header in Flink's {{StaticFileServerHandler}} to 300 seconds [1]. Using {{curl}}, I can confirm that the header is set 

{noformat}
curl -vvv -s http://localhost:8081/jobmanager/log 1> /dev/null
*   Trying ::1...
* TCP_NODELAY set
* Connected to localhost (::1) port 8081 (#0)
> GET /jobmanager/log HTTP/1.1
> Host: localhost:8081
> User-Agent: curl/7.54.0
> Accept: */*
>
< HTTP/1.1 200 OK
< Content-Type: text/plain
< Date: Wed, 08 Jan 2020 17:54:17 GMT
< Expires: Wed, 08 Jan 2020 17:59:17 GMT
< Cache-Control: private, max-age=300
< Last-Modified: Wed, 08 Jan 2020 17:54:17 GMT
< Connection: keep-alive
< content-length: 641263
<
{ [16384 bytes data]
* Connection #0 to host localhost left intact
{noformat}

[1] https://github.com/apache/flink/blob/bd56224c3063fd23d508a4250e5698d4840fa488/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/files/StaticFileServerHandler.java#L361;;;","09/Jan/20 09:40;vthinkxie;cc [~lining];;;","09/Jan/20 09:49;gjy;An easy but maybe hacky fix would be to add a random query parameter to the GET request.;;;","09/Jan/20 09:52;vthinkxie;[~gjy] [~dwysakowicz]

I think the HTTP_CACHE_SECONDS is a trade-off, decrease the cache time could help users to get more real-time logs, but it would also increase the pressure of the rest API.

The HTTP_CACHE_SECONDS was set to 300 seconds since 5 years ago [https://github.com/apache/flink/blame/6f2e9abffb0b1ef68e4f2cf058a24524b61e88a1/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/files/StaticFileServerHandler.java#L104]

 

 ;;;","09/Jan/20 10:22;trohrmann;The strange thing is that the logs are being refreshed if I access http://localhost:8081/jobmanager/log instead of http://localhost:8081/#/job-manager/logs [~vthinkxie]. Hence, I believe there is a problem with how we issue the REST calls from Javascript. Moreover, I believe that this is actually quite an annoying bug for our users. Consequently, we should fix it for the {{1.10.0}} release.;;;","09/Jan/20 11:01;vthinkxie;Hi [~dwysakowicz] [~gjy] [~trohrmann]

I just check the frontend again and can not reproduce the problem

when clicking the refresh button, you may need to wait some seconds(maybe quite a long time if the log file is very big) before the API response all data and refresh the frontend data, the log won't refresh immediately.

could you open the dev tools to double-check the response text and response text?

!7A1CF604-032D-46D8-AC70-FBB884E868B9.png|width=536,height=349!
 ;;;","09/Jan/20 11:09;gjy;Mac OS X 10.14.6, Firefox 72.0.1 (64-bit)
Flink 1.9.1

 !screenshot-2.png! ;;;","09/Jan/20 11:33;vthinkxie;I can add a ""Cache-control: no-store"" HTTP header to the log rest API in frontend, but not sure if it could resolve this issue since I can not reproduce it.

The new frontend keeps the same HTTP header with the old one

new: [https://github.com/apache/flink/blob/57912987e3c58b68b584bc15cf547329802d626e/flink-runtime-web/web-dashboard/src/app/services/job-manager.service.ts#L38]

old: [https://github.com/apache/flink/blob/release-1.8.0/flink-runtime-web/web-dashboard/app/scripts/modules/jobmanager/jobmanager.svc.coffee#L42]
  
 ;;;","09/Jan/20 13:40;gjy;What browser/OS are you using?;;;","13/Jan/20 11:55;vthinkxie;Hi everyone

I have created a PR [https://github.com/apache/flink/pull/10842]

anyone who met this issue could have a try since I still can not reproduce it in my computer

 ;;;","16/Jan/20 11:45;gjy;1.9: 6a124f8be8b73f686033a1b89d934ec7b334bb9f
1.10: ff79be8adaf0570f3c309a6cfdeee0ad30cdebca
master: 871266781814066d4a81fd0a4c30b877cbcc11a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot start a taskmanger if using logback,FLINK-15488,13277699,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,guoyangze,dwysakowicz,dwysakowicz,06/Jan/20 12:52,13/Jan/20 09:44,13/Jul/23 08:07,13/Jan/20 09:44,1.10.0,,,,,1.10.0,,,,API / Core,Deployment / Scripts,,,,0,pull-request-available,,,,"When using logback it is not possible to start the taskmanager using {{taskamanger.sh}} scripts. The same problem (probably) occurs when using slf4j that logs into the console.

The problem is that when calculating memory configuration with {{BashJavaUtils}} class the result is returned through standard output. If something is logged into the console it may result in undefined behavior such as e.g. 
{code}
Error: Could not find or load main class 13:51:23.961
{code}",,aljoscha,azagrebin,dwysakowicz,gjy,guoyangze,liyu,trohrmann,wangyang0918,xtsong,zhuzh,,,,,,,,,,,,"KarmaGYZ commented on pull request #10804: [FLINK-15488] Obtain the JVM and TM param correctly
URL: https://github.com/apache/flink/pull/10804
 
 
   
   ## What is the purpose of the change
   
   When using logback it is not possible to start the taskmanager using taskamanger.sh scripts. To fix this issue, we need to make a contract that the calculation result is always outputted in the last line and in specific format. Then, we can fetch and verify it even when the log is redirected to console.
   
   ## Brief change log
   
   - Introduce a prefix to the calculation result of `BashJavaUtils` class
   - Fetch the calculation result and ignore side output.
   - Fail the script when error occurs during calculation.
   
   ## Verifying this change
   
   1. Using logback, you could follow [this guide](https://issues.apache.org/jira/browse/FLINK-15488?focusedCommentId=17009653&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17009653)
   
   2. Start a TaskExecutor by `${FLINK_DIR}/bin/taskmanager.sh`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: yes
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 05:38;githubbot;600","GJL commented on pull request #10804: [FLINK-15488] Obtain the JVM and TM param correctly
URL: https://github.com/apache/flink/pull/10804
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jan/20 09:43;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15519,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 13 09:44:25 UTC 2020,,,,,,,,,,"0|z0a7yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/20 14:16;gjy;cc: [~azagrebin] [~xintongsong];;;","07/Jan/20 03:12;xtsong;Thanks for reporting this issue, [~dwysakowicz]. This is indeed a design defect.

I think the simplest solution might be {{BashJavaUtils}} output the result with a specific key or pattern, so that {{taskmanager.sh} can find result among other potential outputs.

Or we can write the results into a temporal file. This is safer but a bit more complicate. We need to have unique per-TM file paths to avoid potential conflicts between multiple TMs, and also handle the file clean-ups

I think the first approach might be good enough. WDYT? [~azagrebin] [~dwysakowicz] [~gjy];;;","07/Jan/20 09:51;wangyang0918;I think using a fixed pattern to extract the wanted information makes sense to me. It is more straightforward.

Maybe we could add an argument {{--disable-logging}} to the {{BashJavaUtils}}. The new added arg is used to disable all the logs so that the stdout is cleaner.
{code:java}
Logger.getRootLogger().setLevel(Level.OFF);
{code};;;","07/Jan/20 09:55;guoyangze;Disable logging makes sense to me. Actually, it's a bit odd to allow logging in this utility class.;;;","07/Jan/20 11:03;gjy;Does anyone understand why the string _""Could not find or load main class""_ is logged in the first place? Why is this logged to stdout and not stderr? Where does the log message originate from?;;;","07/Jan/20 11:48;wangyang0918;[~gjy] We use {{org.apache.flink.runtime.util.BashJavaUtils}} to calculate the ResourceDynamicConfigs and ResourceJvmParams. When using logback, the configuration logs will show up in the stdout. So the timestamp in the log is wrongly regarded as the class. 

You could verify this by the following step.

1. Use [logback|https://ci.apache.org/projects/flink/flink-docs-stable/dev/best_practices.html#use-logback-when-running-flink-on-a-cluster]

2. Running the following command
{code:java}
java -classpath ""lib/*"" org.apache.flink.runtime.util.BashJavaUtils GET_TM_RESOURCE_JVM_PARAMS --configDir /tmp/flink-release/conf
{code}
3. Then you could get the following output.
{code:java}
19:35:57.383 [main] INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.rpc.address, localhost
19:35:57.387 [main] INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.rpc.port, 6123
19:35:57.387 [main] INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.heap.size, 1024m
19:35:57.387 [main] INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: taskmanager.memory.process.size, 1024m
19:35:57.388 [main] INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: taskmanager.numberOfTaskSlots, 1
19:35:57.388 [main] INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: parallelism.default, 1
19:35:57.388 [main] INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.execution.failover-strategy, region
-Xmx268435450 -Xms268435450 -XX:MaxDirectMemorySize=214748366 -XX:MaxMetaspaceSize=134217728
{code};;;","07/Jan/20 14:57;dwysakowicz;[~gjy]  [~fly_in_gis]'s explanation is on point.

[~xintongsong] I think using a regex is very fragile, I would not go that route. I would rather change the utility class to produce a deterministic result and disabling the logging sounds like a sensible solution. Maybe it could also make sense to overwrite the std out to make sure nothing else writes there. Similar to what {{org.apache.flink.client.program.OptimizerPlanEnvironment#getPipeline}} does. I don't have a strong opinion though. ;;;","07/Jan/20 14:59;azagrebin;After offline discussion with [~trohrmann], we think that making the logs of BashJavaUtils available for the user is important because it operates with initial user configuration and started TM process already gets all calculated values. Ideally, the logs should go into the same place as they configured by user for the JVM process of TM. Whatever approach we take, we should try to ensure that and check if possible. If the logs are outputted to stdout and the program result as well then we could grep the result out by a special marker as [~xintongsong] suggested but we need also to separate logs (e.g. inverse grep) and forward them to stdout of taskmanager.sh. If it is too complicated we can try the temporary file approach.

Also, possible error outputs looks muted at the moment. I do not see a reason to do that. ;;;","08/Jan/20 02:16;xtsong;I agree with [~azagrebin] and [~trohrmann] that the logs generated during {{BashJavaUtils}} should be preserved. Those logs contains information about how memory sizes are calculated from the configurations, and hints of how to fix an improper configuration, which are important to the users. I also agree that if the logs are configured to be outputted to stdout, we should also forward them to stdout of {{taskmanager.sh}}.

The example mentioned by [~dwysakowicz], {{OptimizerPlanEnvironment#getPipeline}}, inspires me with another approach. If separating logs and {{BashJavaUtils}} calculation results with regex is too fragile, maybe we can separate them inside {{BashJavaUtils}}, by overwriting the stdout.

To be specific, we can overwrite stdout to a {{ByteArrayOutputStream}} in {{BashJavaUtils#main}}, and make {{getTmResourceDynamicConfigs}} and {{getTmResourceJvmParams}} return string type results. Then we can make sure the calculation result is always outputted in the first or last line (it is already always the last line now but we can make it an explicit contract). If the result is not generated (say an exception is thrown during the calculation), we can use an empty line as a placeholder, and non-zero exit code should be returned so the shell script can learn that the result is invalid by checking the exit code. A {{try-catch(Throwable)-finally}} could be used to make sure the cached stdout will be outputted.;;;","08/Jan/20 14:51;azagrebin;BashJavaUtils is single-threaded atm. I do not see any reason for the result not to be the last line of stdout. In that case, we can just tail it from the output. We can have a sanity check by prefixing result with some string and assert it in bash to fail fast if this contract breaks.

For the logs, this looks like another issue which we have to tackle independently. Ideally, it would be nice to preserve BashJavaUtils logs and make them part of the task manager logs. I have created an issue for that [FLINK-15519|https://jira.apache.org/jira/browse/FLINK-15519].;;;","08/Jan/20 15:02;xtsong;I agree with [~azagrebin] that at the moment we can simply rely on the result to be the last line of stdout. We should make the contract explicit in comments. And a sanity check should be helpful for guarding this contract.;;;","13/Jan/20 09:44;gjy;1.10: 6c886ade7c02840423552025f5d5da535ae40d02
master: d76f2166396e3758c08851d024e2c23c8a297062;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalInputChannelTest.testConcurrentConsumeMultiplePartitions failing on ppc64le platform.,FLINK-15486,13277686,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,kevin.cyj,siddheshghadi,siddheshghadi,06/Jan/20 11:32,16/Apr/20 07:36,13/Jul/23 08:07,16/Apr/20 07:36,,,,,,1.11.0,,,,Runtime / Network,Tests,,,,0,pull-request-available,,,,"LocalInputChannelTest.testConcurrentConsumeMultiplePartitions fails due to timeout however when the timeout is increased, it is passing. Any pointer to possible solution/explanation for such behavior would be helpful.","arch: ppc64le
os: rhel7.6
jdk: 8
mvn: 3.6.2",kevin.cyj,siddheshghadi,zjwang,,,,,,,,,,,,,,,,,,,"wsry commented on pull request #11589: [FLINK-15486][tests] Remove timeout of LocalInputChannelTest#testConcurrentConsumeMultiplePartitions to avoid potential timeout problem on slow machine.
URL: https://github.com/apache/flink/pull/11589
 
 
   ## What is the purpose of the change
   Remove timeout of LocalInputChannelTest#testConcurrentConsumeMultiplePartitions to avoid potential timeout problem on slow machine.
   
   
   ## Brief change log
   
     - Remove timeout of LocalInputChannelTest#testConcurrentConsumeMultiplePartitions to avoid potential timeout problem on slow machine.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 06:10;githubbot;600","zhijiangW commented on pull request #11589: [FLINK-15486][tests] Remove timeout of LocalInputChannelTest#testConcurrentConsumeMultiplePartitions to avoid potential timeout problem on slow machine.
URL: https://github.com/apache/flink/pull/11589
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/20 07:31;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/20 11:31;siddheshghadi;surefire-report.txt;https://issues.apache.org/jira/secure/attachment/12990000/surefire-report.txt",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 07:36:00 UTC 2020,,,,,,,,,,"0|z0a7vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/20 10:09;zjwang;Sorry for touching it so delay [~siddheshghadi] . I would assign this issue to [~kevin.cyj], to further analysis this test. If the timeout value is not set properly, we can increase it. Otherwise we can give some explanations which scenarios would cause the timeout.;;;","01/Apr/20 04:25;kevin.cyj;IMO, the timeout occurs is purely because that the timeout time is too small on the test machine. I think we don't really need a timeout here and I will submit a to remove the timeout.;;;","16/Apr/20 07:36;kevin.cyj;Fix via a49f75d7053ea56991873da064b1895c5fde692a on master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reopen tests when blocking issue has been resolved,FLINK-15485,13277671,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,leonard,leonard,leonard,06/Jan/20 10:06,13/Jan/20 06:32,13/Jul/23 08:07,13/Jan/20 06:32,,,,,,1.10.0,,,,Tests,,,,,0,pull-request-available,,,,"Sometimes we  close test and left comment like 'TODO/when FLINK-xx is closed/when FLINK-xx is merged' for various reasons and ready to reopen  it after they are really fixed.

Unfortunately we missed some of them. This issue aims to reopen tests that close by

* FLINK-12088  
* FLINK-13740 
* CALCITE-1860  ",,aljoscha,jark,leonard,,,,,,,,,,,,,,,,,,,"leonardBang commented on pull request #10774: [FLINK-15485][tests] Reopen tests when blocking issue has been resolved.
URL: https://github.com/apache/flink/pull/10774
 
 
   [FLINK-15485][tests] Reopen tests when blocking issue has been resolved.
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *This pull request reopens tests that close by FLINK-12088 、FLINK-13740、CALCITE-1860 which have been resolved.*
   
   
   ## Brief change log
   
     - *update file org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala*
     - *update file org/apache/flink/table/planner/runtime/stream/table/GroupWindowTableAggregateITCase.scala*
     - *org/apache/flink/table/planner/runtime/stream/table/TableAggregateITCase.scala*
     - *update file org/apache/flink/table/plan/ExpressionReductionRulesTest.scala*
   
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/20 10:24;githubbot;600","wuchong commented on pull request #10774: [FLINK-15485][tests] Reopen tests when blocking issue has been resolved.
URL: https://github.com/apache/flink/pull/10774
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jan/20 06:31;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 13 06:32:38 UTC 2020,,,,,,,,,,"0|z0a7s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/20 13:05;ykt836;Nice cleanup!;;;","13/Jan/20 06:32;jark;1.11.0: 328a428a7e1cb684e5b0e6229f9c54687ffdec7b
1.10.0: d1a11feb687be233f88be14c20d95f57a0f5a0a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Starting jobmanager pod should respect environment config option ,FLINK-15483,13277650,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,wangyang0918,wangyang0918,06/Jan/20 08:37,08/Jan/20 13:22,13/Jul/23 08:07,08/Jan/20 13:22,1.10.0,,,,,1.10.0,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"Currently, we could use `containerized.master.env.` to set the user-defined environment variables. For Yarn, it works correctly. However, it could not take effect on Kubernetes deployment.

 

Some users have tried the flink native integration and find this problem. This is nice to have in 1.10 and not a blocker. Since we could set the environment when building the image instead.",,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,"wangyang0918 commented on pull request #10788: [FLINK-15483][kubernetes] Starting jobmanager pod should respect environment config option
URL: https://github.com/apache/flink/pull/10788
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Currently, we could use `containerized.master.env.` to set the user-defined environment variables. For Yarn, it works correctly. However, it could not take effect on Kubernetes deployment.
   
   Some users have tried the flink native integration and find this problem. This is nice to have in 1.10 and not a blocker. Since we could set the environment when building the image instead.
   
   
   ## Brief change log
   
   * Move yarn/Utils#getEnvironmentVariables to BootstrapTools
   * Set jobmanager pod env in `FlinkMasterDeploymentDecorator`
   
   
   ## Verifying this change
   * The changes could be covered the unit test `Fabric8ClientTest#testCreateFlinkMasterDeployment`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 08:54;githubbot;600","tillrohrmann commented on pull request #10788: [FLINK-15483][kubernetes] Starting jobmanager pod should respect environment config option
URL: https://github.com/apache/flink/pull/10788
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/20 13:21;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 08 13:22:48 UTC 2020,,,,,,,,,,"0|z0a7nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/20 13:22;trohrmann;Fixed via

master: 9f99d591f88a7ea85346b9b873fac331cdd12ba8
1.10.0: f1ee817b951b5b9140798d23dee9f871711ca0e3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to call Hive functions returning decimal type,FLINK-15482,13277642,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,lirui,lirui,06/Jan/20 07:43,07/Jan/20 11:34,13/Jul/23 08:07,07/Jan/20 11:34,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"Calling a Hive function with decimal return type will get:
{noformat}
java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.flink.table.dataformat.Decimal

	at ExpressionReducer$5.map(Unknown Source)
	at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:117)
{noformat}",,jark,lirui,,,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #10775: [FLINK-15482][hive] Failed to call Hive functions returning decimal type
URL: https://github.com/apache/flink/pull/10775
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix the issue of calling Hive functions that return decimals
   
   
   ## Brief change log
   
     - Call `TypeInfoDataTypeConverter` to convert data type to type info because it's the recommended way.
     - Add test case
   
   
   ## Verifying this change
   
   New test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/20 10:48;githubbot;600","wuchong commented on pull request #10775: [FLINK-15482][hive] Failed to call Hive functions returning decimal type
URL: https://github.com/apache/flink/pull/10775
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 11:33;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 11:34:48 UTC 2020,,,,,,,,,,"0|z0a7ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/20 11:34;jark;1.11.0: b195383b6b792ea1363ae340ffcfb6ef45c84677
1.10.0: 89aa00c9bbfc4bcab4d453748feccadc2182f7dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The type hint of `TableEnvironment#from_elements` in `table_environment.py` is incomplete,FLINK-15481,13277615,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,weizhong,zhongwei,zhongwei,06/Jan/20 03:02,13/Apr/21 20:39,13/Jul/23 08:07,06/Jan/20 12:37,1.10.0,,,,,1.10.0,,,,API / Python,,,,,0,pull-request-available,,,,"The type hint of `TableEnvironment#from_elements` in `table_environment.py` is incomplete:
{code:java}
        :param elements: The elements to create a table from.
        :type elements: list
        :param schema: The schema of the table.
        :type schema: pyflink.table.types.DataType
        :param verify_schema: Whether to verify the elements against the schema.
        :type verify_schema: bool
        :return: The result table.
        :rtype: pyflink.table.Table
{code}

According to current usage the `:type schema:` could also be list[str].",,hequn8128,zhongwei,,,,,,,,,,,,,,,,,,,,"WeiZhong94 commented on pull request #10770: [FLINK-15481][python] Add `list[str]` to the type hint of the parameter ""schema"" in `TableEnvironment#from_elements`.
URL: https://github.com/apache/flink/pull/10770
 
 
   ## What is the purpose of the change
   
   *This pull request adds `list[str]` to the type hint of the parameter ""schema"" in `TableEnvironment#from_elements`.*
   
   ## Brief change log
   
     - *Add `list[str]` to the type hint of the parameter ""schema"" in `TableEnvironment#from_elements`.*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/20 03:29;githubbot;600","hequn8128 commented on pull request #10770: [FLINK-15481][python] Add `list[str]` to the type hint of the parameter ""schema"" in `TableEnvironment#from_elements`.
URL: https://github.com/apache/flink/pull/10770
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/20 12:30;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 06 12:37:18 UTC 2020,,,,,,,,,,"0|z0a7fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/20 12:37;hequn8128;Fixed in
1.11.0 via 58e18c2a258cd210193d2e496066401cd4dc65b8
1.10.0 via c508e68bf210a3cbdefab83e4c8a59ca8bccb466;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FROM_BASE64 code gen type wrong,FLINK-15478,13277548,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,libenchao,libenchao,libenchao,05/Jan/20 12:28,07/Jan/20 07:16,13/Jul/23 08:07,07/Jan/20 07:16,1.10.0,1.9.1,,,,1.10.0,1.9.2,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"`FROM_BASE64` should return *String* instead of *byte[]*

If ** we use FROM_BASE64 in sql, will result in a exception:

 
{code:java}
org.apache.flink.table.planner.codegen.CodeGenException: Incompatible types of expression and result type. Expression[GeneratedExpression(result$2,isNull$1,isNull$1 = false;
result$2 = null;
if (!isNull$1) {
  
  result$2 = org.apache.flink.table.runtime.functions.SqlFunctionUtils.fromBase64(((org.apache.flink.table.dataformat.BinaryString) str$0));
  isNull$1 = (result$2 == null);
}
,BYTES,None)] type is [BYTES], result type is [STRING NOT NULL]
{code}
 

Why ScalarFunctionsTest.testFromBase64 passed?

Because we assume the result should be string in ExpressionTestBase, then add a cast operator to the result of FROM_BASE64. 

 ",,jark,libenchao,,,,,,,,,,,,,,,,,,,,"libenchao commented on pull request #10766: [FLINK-15478][table-planner-blink] Fix FROM_BASE64 code gen result type
URL: https://github.com/apache/flink/pull/10766
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   This pr fix FROM_BASE64 code gen result type.
   
   ## Brief change log
   
   This pr fix FROM_BASE64 code gen result type.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   CalcITCase.testFromBase64
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Jan/20 12:37;githubbot;600","wuchong commented on pull request #10766: [FLINK-15478][table-planner-blink] Fix FROM_BASE64 code gen result type
URL: https://github.com/apache/flink/pull/10766
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 07:13;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 07:16:57 UTC 2020,,,,,,,,,,"0|z0a70w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/20 13:16;jark;Thanks for reporting this [~libenchao].;;;","07/Jan/20 07:16;jark;1.11.0: f0fe3650791486a44762dc008da0d1acc22227cd
1.10.0: e99a929a9b9da7eb56fbf63fda7de386f27f4844
1.9.2: c016086df43069b1c1b10199a48597fc444804ab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
INSERT OVERWRITE not supported from SQL CLI,FLINK-15468,13277328,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,lirui,lirui,03/Jan/20 08:57,06/Jan/20 16:07,13/Jul/23 08:07,05/Jan/20 22:54,1.10.0,,,,,1.10.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"Running {{INSERT OVERWRITE}} from SQL CLI will get:
{noformat}
[ERROR] Unknown or invalid SQL statement.
{noformat}",,jark,lirui,lzljs3620320,phoenixjiangnan,,,,,,,,,,,,,,,,,,"lirui-apache commented on pull request #10759: [FLINK-15468][sql-client] INSERT OVERWRITE not supported from SQL CLI
URL: https://github.com/apache/flink/pull/10759
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Support INSERT OVERWRITE in SQL CLI
   
   
   ## Brief change log
   
     - Add INSERT OVERWRITE as a `SqlCommand`
     - Add test for it
   
   
   ## Verifying this change
   
   New test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 09:50;githubbot;600","bowenli86 commented on pull request #10759: [FLINK-15468][sql-client] INSERT OVERWRITE not supported from SQL CLI
URL: https://github.com/apache/flink/pull/10759
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Jan/20 22:53;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 05 22:54:46 UTC 2020,,,,,,,,,,"0|z0a5o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/20 09:03;lzljs3620320;Good catch, do you want to fix this?;;;","03/Jan/20 09:21;lirui;Yeah I'm working on it.;;;","05/Jan/20 22:54;phoenixjiangnan;master: 0a2cc58af87a0154160322d0669fcb22c736bde2
1.10: 80cfeb84dfa479fd1a41dde8863f88eca8a796ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should wait for the end of the source thread during the Task cancellation,FLINK-15467,13277326,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,Ming Li,Ming Li,03/Jan/20 08:41,22/Jun/21 13:55,13/Jul/23 08:07,16/Sep/20 19:22,1.10.1,1.9.0,1.9.1,,,1.10.3,1.11.2,1.12.0,,Runtime / Task,,,,,0,pull-request-available,,,,"     In the new mailBox model, SourceStreamTask starts a source thread to run user methods, and the current execution thread will block on mailbox.takeMail (). When a task cancels, the TaskCanceler thread will cancel the task and interrupt the execution thread. Therefore, the execution thread of SourceStreamTask will throw InterruptedException, then cancel the task again, and throw an exception.
{code:java}
//代码占位符
@Override
protected void performDefaultAction(ActionContext context) throws Exception {
   // Against the usual contract of this method, this implementation is not step-wise but blocking instead for
   // compatibility reasons with the current source interface (source functions run as a loop, not in steps).
   sourceThread.start();

   // We run an alternative mailbox loop that does not involve default actions and synchronizes around actions.
   try {
      runAlternativeMailboxLoop();
   } catch (Exception mailboxEx) {
      // We cancel the source function if some runtime exception escaped the mailbox.
      if (!isCanceled()) {
         cancelTask();
      }
      throw mailboxEx;
   }

   sourceThread.join();
   if (!isFinished) {
      sourceThread.checkThrowSourceExecutionException();
   }

   context.allActionsCompleted();
}
{code}
   When all tasks of this TaskExecutor are canceled, the blob file will be cleaned up. But the real source thread is not finished at this time, which will cause a ClassNotFoundException when loading a new class. In this case, the source thread may not be able to properly clean up and release resources (such as closing child threads, cleaning up local files, etc.). Therefore, I think we should mark this task canceled or finished after the execution of the source thread is completed.",,AHeise,kezhuw,klion26,liyu,Ming Li,Paul Lin,pnowojski,qinjunjerry,roman,shazeline,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 16 20:07:09 UTC 2020,,,,,,,,,,"0|z0a5ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/20 12:18;pnowojski;Could you [~Ming Li] elaborate what's the problem? Which method invocations? Also this code has changed quite a lot, so is this issue still valid in the current master?;;;","02/Jun/20 12:37;Ming Li;Hi，[~pnowojski].I try to use the latest release version (1.10.1), and the same problem exists: when the job is canceled, I try to load certain classes, but the jar package file has been cleared. Therefore, I think we still need to wait for the sourceThread to complete before clearing the jar file.;;;","02/Jun/20 12:41;Ming Li;Below is my test code：
{code:java}
//代码占位符

package com.flink.test;

import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.sink.DiscardingSink;
import org.apache.flink.streaming.api.functions.source.ParallelSourceFunction;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Created by liming on 2020/6/2.
 */
public class TestJobClean {
    private final Logger logger = LoggerFactory.getLogger(TestLoad.class);

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.addSource(new CustomSource())
                .addSink(new DiscardingSink<String>());
        env.execute(""testJobCancel"");
    }

    private static class CustomSource implements ParallelSourceFunction<String> {
        private final Logger logger = LoggerFactory.getLogger(CustomSource.class);
        private boolean running = true;

        public void run(SourceContext<String> sourceContext) throws Exception {
            try {
                while (running) {
                    Thread.sleep(1000);
                    sourceContext.collect(""test"");
                }
            } finally {
                try {
                    Thread.sleep(3000); //Simulate preparatory work
                } catch (InterruptedException e) {
                    Thread.sleep(3000); //Simulate preparatory work
                }
                try {
                    logger.info(""try to load clean class"");
                    Class.forName(""com.flink.test.TestLoad""); //try to load some class to do clean
                    logger.info(""load clean class success"");
                } catch (Exception e) {
                    logger.error(""load clean class failed"", e);
                }

            }
        }

        public void cancel() {
            running = false;
        }
    }
}
{code}
The logs are as follows：
2020-06-02 20:34:24,368 INFO org.apache.flink.runtime.taskmanager.Task - Attempting to cancel task Source: Custom Source -> Sink: Unnamed (1/1) (10ee411efd563d83214fd6f947dcad1e).
2020-06-02 20:34:24,369 INFO org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Sink: Unnamed (1/1) (10ee411efd563d83214fd6f947dcad1e) switched from RUNNING to CANCELING.
2020-06-02 20:34:24,369 INFO org.apache.flink.runtime.taskmanager.Task - Triggering cancellation of task code Source: Custom Source -> Sink: Unnamed (1/1) (10ee411efd563d83214fd6f947dcad1e).
2020-06-02 20:34:24,371 INFO org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Sink: Unnamed (1/1) (10ee411efd563d83214fd6f947dcad1e) switched from CANCELING to CANCELED.
2020-06-02 20:34:24,371 INFO org.apache.flink.runtime.taskmanager.Task - Freeing task resources for Source: Custom Source -> Sink: Unnamed (1/1) (10ee411efd563d83214fd6f947dcad1e).
2020-06-02 20:34:24,372 INFO org.apache.flink.runtime.taskmanager.Task - Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Unnamed (1/1) (10ee411efd563d83214fd6f947dcad1e) [CANCELED]
2020-06-02 20:34:24,373 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Un-registering task and sending final execution state CANCELED to JobManager for task Source: Custom Source -> Sink: Unnamed (1/1) 10ee411efd563d83214fd6f947dcad1e.
2020-06-02 20:34:24,383 INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile\{cpuCores=1.0000000000000000, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, allocationId: e42db2155c2aa58e98c1dce598ecb7f5, jobId: 36242552141952de9bc8f3867f9d6f6d).
2020-06-02 20:34:24,383 INFO org.apache.flink.runtime.taskexecutor.JobLeaderService - Remove job 36242552141952de9bc8f3867f9d6f6d from job leader monitoring.
2020-06-02 20:34:24,384 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Close JobManager connection for job 36242552141952de9bc8f3867f9d6f6d.
2020-06-02 20:34:24,386 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Close JobManager connection for job 36242552141952de9bc8f3867f9d6f6d.
2020-06-02 20:34:24,387 INFO org.apache.flink.runtime.taskexecutor.JobLeaderService - Cannot reconnect to job 36242552141952de9bc8f3867f9d6f6d because it is not registered.
2020-06-02 20:34:27,375 INFO com.flink.test.TestJobClean$CustomSource - try to load clean class
2020-06-02 20:34:27,376 ERROR com.flink.test.TestJobClean$CustomSource - load clean class failed
java.lang.ClassNotFoundException: com.flink.test.TestLoad
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:69)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at com.flink.test.TestJobClean$CustomSource.run(TestJobClean.java:40)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:200);;;","03/Jun/20 10:12;pnowojski;Thanks for coming back and providing a way to reproduce the problem. I haven't run your code, but it looks like you are right. I'm not entirely sure how to fix this from the top of my head, someone would have to investigate this further.

Also the problem might be a bit more profound and solution should take into an account operators spawning custom threads as well.;;;","17/Jun/20 18:44;roman;I tried to run the program above with some corrections (to stop the job after some elements and load other classes).

I  couldn't reproduce the issue: the class was loaded successfully after stopping all services.

 

[~Ming Li], are you running it on a cluster and then stopping via JM?

 

If yes, I think this is what I think is happening in your case:
 # Job is canceled in JM
 # JM sends RPC to TM to cancel the task
 # TM calls Task.cancelExecution which starts TaskCanceler and TaskInterrupter
 # Source thread ignores the interrupt
 # Main task thread proceeds to finally block in Task.doRun, where it calls cancelInvokable and notifyFinalState >>
 # >> TaskExecutor.unregisterTaskAndNotifyFinalState > taskSlotTable.removeTask >
 # >> TaskExecutor.closeJobManagerConnectionIfNoAllocatedResources, which called
closeJob > BlobLibraryCacheManager.release

If this is the case, then just joining the source thread inside cancelInvokable would solve the problem (specifically, in SourceStreamTask.cancelTask).

 

SourceStreamTask.cancelTask is called from:
 * RPC notification about checkpoint completion - fine, done asynchronously
 * TaskCanceller - can delay subsequent networkResourcesCloser.run() - which is probably also fine
 * Task.cancelInvokable - what we want

 ;;;","18/Jun/20 03:14;Ming Li;Hi,[~roman_khachatryan],In my test, I ran my test code in a standalone cluster. After running, I directly clicked cancel on the webUI to trigger a cancel process. In the test code, simulating soucethread may take a lot of time to clean up, so I try to sleep for 3 seconds and then load a class that has not been loaded by jvm. At this time, the above exception will occur.

I have tried not to sleep and then load a new class, this problem will not occur, I think this is because the cleaning of BlobLibrary has not been completed.

I also think we should join the source thread in SourceStreamTask.cancelTask to fix this problem.;;;","18/Jun/20 19:49;roman;Hi [~Ming Li], thanks for the clarification!

I was able to reproduce the issue and will submit a PR to fix it.;;;","29/Jul/20 14:24;pnowojski;Merged to master 61d06ace57, to release-1.11 1dee87ae58. Not backporting to release-1.10 due to conflicts and low impact of this bug.

Thanks [~roman_khachatryan] for fixing and [~Ming Li] for reporting!;;;","31/Jul/20 08:35;Ming Li;Thanks for this fix. I want to correct the cause of this problem: it is not that the blob file is deleted, but that the userClassloader will be closed when the task ends. Later, when trying to use this classloader to load a class (when the classloader is not specified, the classloader of the caller class will be used), a ClassNotFoundException will be thrown.;;;","31/Jul/20 08:43;pnowojski;That was also our understanding [~Ming Li]. Either way the problem should be gone as {{SourceStreamTask}} wouldn't finish until {{sourceThread}} is done and life cycle of the Task's class loader is bound to the Task's life. ;;;","31/Jul/20 08:47;roman;Thanks for the clarification [~Ming Li].

I think the fix covers this case too. In fact, I  checked it manually and it fixed the problem. But of course, a double check from your side would be highly appreciated.;;;","09/Sep/20 15:14;qinjunjerry;[~pnowojski] as discussed, could you please back port this fix to Flink 1.10 as it is impacting Flink 1.10 customers.;;;","16/Sep/20 19:21;arvid;Merged to release-1.10 a20fb4a796d8df91e4ebbcdc0b35cdd75145c574.;;;","16/Sep/20 20:07;qinjunjerry;Great! Thanks [~AHeise];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`FlinkAggregateExpandDistinctAggregatesRule` generates wrong plan for cases that have distinct aggs with filter.,FLINK-15466,13277314,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,icshuo,icshuo,icshuo,03/Jan/20 06:22,09/Jan/20 09:08,13/Jul/23 08:07,09/Jan/20 09:08,1.8.3,1.9.1,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"For the sql in batch mode,

 
{code:java}
SELECT 
   a, COUNT(a), SUM(DISTINCT b) FILTER (WHERE a > 0)
FROM MyTable 
GROUP BY a{code}
 

plan generated after logical stage is as following, which is not correct. The `Filter $4` should be `$f2 *and* $g_0`.

!image-2020-01-03-14-24-35-943.png!",,icshuo,,,,,,,,,,,,,,,,,,,,,"cshuo commented on pull request #10760: [FLINK-15466][table-planner] Fix bug of `FlinkAggregateExpandDistinct…
URL: https://github.com/apache/flink/pull/10760
 
 
   ## What is the purpose of the change
   
   Fix bug of `FlinkAggregateExpandDistinctAggregatesRule`; it'll generate wrong plan for cases where distinct aggregate calls with filter are used.
   
   ## Brief change log
     - This PR mainly fix bugs of `FlinkAggregateExpandDistinctAggregatesRule`.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Added integration tests in `DistinctAggregateITCaseBase` for end-to-end test.
     - Added tests in `DistinctAggregateTest` that validates the plan for cases with distinct aggregate calls with filter.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 10:11;githubbot;600","KurtYoung commented on pull request #10760: [FLINK-15466][table-planner] Fix bug of `FlinkAggregateExpandDistinct…
URL: https://github.com/apache/flink/pull/10760
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 09:04;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/20 06:24;icshuo;image-2020-01-03-14-24-35-943.png;https://issues.apache.org/jira/secure/attachment/12989856/image-2020-01-03-14-24-35-943.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 09 09:08:44 UTC 2020,,,,,,,,,,"0|z0a5l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/20 06:32;icshuo;@[~godfreyhe], [~jark], [~lincoln.86xy];;;","09/Jan/20 09:08;ykt836;master: 6f2e9abffb0b1ef68e4f2cf058a24524b61e88a1

1.10.0: ce7968ecafcde69596897a2243235161cd347b13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid failing when required memory calculation not accurate in BinaryHashTable,FLINK-15465,13277313,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,03/Jan/20 05:59,07/Jan/20 10:43,13/Jul/23 08:07,07/Jan/20 10:43,,,,,,1.10.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"In BinaryHashBucketArea.insertToBucket.

When BinaryHashTable.buildTableFromSpilledPartition.""Build in memory hash table"", it requires memory can put all records, if not, will fail.

Because the linked hash conflict solution, the required memory calculation are not accurate, in this case, we should apply for insufficient memory from heap.

And must be careful, the steal memory should not return to table.",,jark,lzljs3620320,,,,,,,,,,,,,,,,,,,,"JingsongLi commented on pull request #10756: [FLINK-15465][FLINK-11964][table-runtime-blink] Fix hash table bugs
URL: https://github.com/apache/flink/pull/10756
 
 
   
   ## What is the purpose of the change & Brief change log
   
   - In BinaryHashBucketArea.insertToBucket.
   When BinaryHashTable.buildTableFromSpilledPartition.""Build in memory hash table"", it requires memory can put all records, if not, will fail.
   Because the linked hash conflict solution, the required memory calculation are not accurate, in this case, we should apply for insufficient memory from heap.
   And must be careful, the steal memory should not return to table.
   
   - In HybridHashTable, first select the corresponding partition according to hashCode, and then select the bucket in the partition according to hashCode, using the same hashCode can easily cause hash collision.
   Consider doing some mix to hashCode when choosing bucket.
   Like JDK HashMap, we can just XOR some shifted bits in the cheapest possible way to reduce systematic lossage, as well as to incorporate impact of the highest bits that would otherwise never be used in index calculations because of table bounds. (bucket use power-of-two masking).  Just like:  (hash ^ (hash >>> 16))
   In some cases, if a lot of conflicts occurred, this will lead to job hang, because hash join will degenerate to nested loop join.
   
   ## Verifying this change
   
   `BinaryHashTableTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 06:04;githubbot;600","wuchong commented on pull request #10756: [FLINK-15465][FLINK-11964][table-runtime-blink] Fix required memory calculation not accurate and hash collision bugs in hash table
URL: https://github.com/apache/flink/pull/10756
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 10:41;githubbot;600",,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 10:43:14 UTC 2020,,,,,,,,,,"0|z0a5kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/20 10:43;jark;1.11.0: 69ed6feef09d36df48b2e849888f9faebdaa2981
1.10.0: 81b18957da8e35b414b6c6017d13720157340d59;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
