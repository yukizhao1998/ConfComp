Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Child-Issue),Inward issue link (Completes),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Language),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Shuffle merge finalization removes the wrong finalization state from the DB,SPARK-41792,13516054,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mridulm80,mridulm80,mridulm80,30/Dec/22 21:54,01/Jan/23 19:42,13/Jul/23 08:51,01/Jan/23 19:42,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Shuffle,,,0,,,,,,"During `finalizeShuffleMerge` in external shuffle service, if the finalization request is for a newer shuffle merge id, then we cleanup the existing (older) shuffle details and add the newer entry (for which we got no pushed blocks) to the DB.

Unfortunately, when cleaning up from the DB, we are using the incorrect AppAttemptShuffleMergeId - we remove the latest shuffle merge id instead of the existing entry.

Proposed Fix:

{code}
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RemoteBlockPushResolver.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RemoteBlockPushResolver.java
index 816d1082850..551104d0eba 100644
--- a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RemoteBlockPushResolver.java
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RemoteBlockPushResolver.java
@@ -653,9 +653,11 @@ public class RemoteBlockPushResolver implements MergedShuffleFileManager {
         } else if (msg.shuffleMergeId > mergePartitionsInfo.shuffleMergeId) {
           // If no blocks pushed for the finalizeShuffleMerge shuffleMergeId then return
           // empty MergeStatuses but cleanup the older shuffleMergeId files.
+          AppAttemptShuffleMergeId currentAppAttemptShuffleMergeId = new AppAttemptShuffleMergeId(
+                  msg.appId, msg.appAttemptId, msg.shuffleId, mergePartitionsInfo.shuffleMergeId);
           submitCleanupTask(() ->
               closeAndDeleteOutdatedPartitions(
-                  appAttemptShuffleMergeId, mergePartitionsInfo.shuffleMergePartitions));
+                  currentAppAttemptShuffleMergeId, mergePartitionsInfo.shuffleMergePartitions));
         } else {
           // This block covers:
           //  1. finalization of determinate stage
{code}",,apachespark,lyee,mridulm80,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Dec 30 22:54:34 UTC 2022,,,,,,,,,,"0|z1eggw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Dec/22 22:53;apachespark;User 'mridulm' has created a pull request for this issue:
https://github.com/apache/spark/pull/39316;;;","30/Dec/22 22:54;apachespark;User 'mridulm' has created a pull request for this issue:
https://github.com/apache/spark/pull/39316;;;",,,,,,,,,,,,,,,,
Set TRANSFORM reader and writer's format correctly,SPARK-41790,13516020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jimmyma,jimmyma,jimmyma,30/Dec/22 13:12,05/Jan/23 08:53,13/Jul/23 08:51,05/Jan/23 08:53,3.3.1,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"We'll get wrong data when transform only specify reader or writer 's row format delimited, the reason is using the wrong format to feed/fetch data to/from running script now.  In theory, writer uses inFormat to feed to input data into the running script and reader uses outFormat to read the output from the running script, but inFormat and outFormat are set wrong value currently in the following code:
{code:java}
val (inFormat, inSerdeClass, inSerdeProps, reader) =
  format(
    inRowFormat, ""hive.script.recordreader"",
    ""org.apache.hadoop.hive.ql.exec.TextRecordReader"")

val (outFormat, outSerdeClass, outSerdeProps, writer) =
  format(
    outRowFormat, ""hive.script.recordwriter"",
    ""org.apache.hadoop.hive.ql.exec.TextRecordWriter"") {code}
 

Example SQL:
{code:java}
spark-sql> CREATE TABLE t1 (a string, b string); 

spark-sql> INSERT OVERWRITE t1 VALUES(""1"", ""2""), (""3"", ""4"");

spark-sql> SELECT TRANSFORM(a, b)
         >   ROW FORMAT DELIMITED
         >   FIELDS TERMINATED BY ','
         >   USING 'cat'
         >   AS (c)
         > FROM t1;
c

spark-sql> SELECT TRANSFORM(a, b)
         >   USING 'cat'
         >   AS (c)
         >   ROW FORMAT DELIMITED
         >   FIELDS TERMINATED BY ','
         > FROM t1;
c
1    23    4{code}
 

The same sql in hive:
{code:java}
hive> SELECT TRANSFORM(a, b)
    >   ROW FORMAT DELIMITED
    >   FIELDS TERMINATED BY ','
    >   USING 'cat'
    >   AS (c)
    > FROM t1;
c
1,2
3,4

hive> SELECT TRANSFORM(a, b)
    >   USING 'cat'
    >   AS (c)
    >   ROW FORMAT DELIMITED
    >   FIELDS TERMINATED BY ','
    > FROM t1;
c
1    2
3    4 {code}
 ",,apachespark,gurwls223,jimmyma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jan 05 08:53:30 UTC 2023,,,,,,,,,,"0|z1eg9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Dec/22 13:54;apachespark;User 'mattshma' has created a pull request for this issue:
https://github.com/apache/spark/pull/39315;;;","30/Dec/22 13:54;apachespark;User 'mattshma' has created a pull request for this issue:
https://github.com/apache/spark/pull/39315;;;","05/Jan/23 08:53;gurwls223;Issue resolved by pull request 39315
[https://github.com/apache/spark/pull/39315];;;",,,,,,,,,,,,,,,
"`regexp_replace('', '[a\\\\d]{0, 2}', 'x')` causes an internal error",SPARK-41780,13515974,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,panbingkun,remziy,remziy,30/Dec/22 03:15,09/Jan/23 08:38,13/Jul/23 08:51,09/Jan/23 08:38,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"{code:scala}
scala> spark.sql(""select regexp_replace('', '[a\\\\d]{0,2}', 'x')"").show
+----------------------------------+
|regexp_replace(, [a\d]\{0,2}, x, 1)|
+----------------------------------+
|                                 x|
+----------------------------------+
 
 
scala> spark.sql(""select regexp_replace('', '[a\\\\d]{0, 2}', 'x')"").show
org.apache.spark.SparkException: The Spark SQL phase optimization failed with an internal error. Please, fill a bug report in, and provide the full stack trace.
{code}

 ",Spark3.3.0 local mode,apachespark,jira.shegalov,maxgekk,panbingkun,remziy,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/23 06:12;panbingkun;image-2023-01-04-14-12-26-126.png;https://issues.apache.org/jira/secure/attachment/13054297/image-2023-01-04-14-12-26-126.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jan 09 08:38:11 UTC 2023,,,,,,,,,,"0|z1efzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Jan/23 11:53;panbingkun;I will fix it.;;;","04/Jan/23 05:56;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39383;;;","04/Jan/23 06:13;panbingkun;This is not bug, only the error prompt is not clear.

!image-2023-01-04-14-12-26-126.png|width=547,height=312!;;;","04/Jan/23 06:42;remziy;It is a bug I guess, because an internal error is returned. Please run the code in the description.;;;","04/Jan/23 11:29;panbingkun;I have submitted the pr to show clearer error(not an internal error) prompts for this scenario.
[https://github.com/apache/spark/pull/39383];;;","09/Jan/23 08:38;maxgekk;Issue resolved by pull request 39383
[https://github.com/apache/spark/pull/39383];;;",,,,,,,,,,,,
Handle decommission request sent before executor registration,SPARK-41766,13515900,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,29/Dec/22 05:19,01/May/23 22:03,13/Jul/23 08:51,01/May/23 22:03,3.3.1,,,,,,,,,,,,,,3.5.0,,,,,Spark Core,,,0,,,,,,"When decommissioning an unregistered executor, the request will be ignored. It should send to executor after registration. ",,apachespark,dongjoon,warrenzhu25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon May 01 22:03:38 UTC 2023,,,,,,,,,,"0|z1efiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Dec/22 05:25;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39280;;;","01/May/23 22:03;dongjoon;Issue resolved by pull request 39280
[https://github.com/apache/spark/pull/39280];;;",,,,,,,,,,,,,,,,
[SQL] ParquetFilters StringStartsWith push down matching string do not use UTF-8,SPARK-41741,13515819,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,jlelehe,jlelehe,28/Dec/22 09:59,20/Feb/23 11:34,13/Jul/23 08:51,20/Feb/23 11:34,3.4.0,,,,,,,,,,,,,,3.3.3,3.4.0,,,,SQL,,,0,,,,,,"Hello ~
 
I found a problem, but there are two ways to solve it.
 
The parquet filter is pushed down. When using the like '***%' statement to query, if the system default encoding is not UTF-8, it may cause an error.
 
There are two ways to bypass this problem as far as I know
1. spark.executor.extraJavaOptions=""-Dfile.encoding=UTF-8""
2. spark.sql.parquet.filterPushdown.string.startsWith=false
 

The following is the information to reproduce this problem

The parquet sample file is in the attachment
{code:java}
spark.read.parquet(""file:///home/kylin/hjldir/part-00000-30432312-7cdb-43ef-befe-93bcfd174878-c000.snappy.parquet"").createTempView(""tmp”)
spark.sql(""select * from tmp where `1` like '啦啦乐乐%'"").show(false) {code}
!image-2022-12-28-18-00-00-861.png|width=879,height=430!
 
  !image-2022-12-28-18-00-21-586.png|width=799,height=731!
 
I think the correct code should be:
{code:java}
private val strToBinary = Binary.fromReusedByteArray(v.getBytes(StandardCharsets.UTF_8)) {code}",,apachespark,bjornjorgensen,huldar,jlelehe,ulysses,yumwang,,,,,,,,,,,,,,,,,,,,,,"28/Dec/22 10:00;jlelehe;image-2022-12-28-18-00-00-861.png;https://issues.apache.org/jira/secure/attachment/13054184/image-2022-12-28-18-00-00-861.png","28/Dec/22 10:00;jlelehe;image-2022-12-28-18-00-21-586.png;https://issues.apache.org/jira/secure/attachment/13054185/image-2022-12-28-18-00-21-586.png","09/Jan/23 03:10;jlelehe;image-2023-01-09-11-10-31-262.png;https://issues.apache.org/jira/secure/attachment/13054424/image-2023-01-09-11-10-31-262.png","09/Jan/23 10:28;jlelehe;image-2023-01-09-18-27-53-479.png;https://issues.apache.org/jira/secure/attachment/13054440/image-2023-01-09-18-27-53-479.png","28/Dec/22 09:59;jlelehe;part-00000-30432312-7cdb-43ef-befe-93bcfd174878-c000.snappy.parquet;https://issues.apache.org/jira/secure/attachment/13054182/part-00000-30432312-7cdb-43ef-befe-93bcfd174878-c000.snappy.parquet",,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Feb 20 05:27:00 UTC 2023,,,,,,,,,,"0|z1ef0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Dec/22 20:03;bjornjorgensen;[~jlelehe] can you change Affects Version/s: from 2.4.0 to 3.4.0 ? ;;;","31/Dec/22 06:48;jlelehe;[~bjornjorgensen] done;;;","09/Jan/23 02:58;yumwang;What is your {{file.encoding}}?;;;","09/Jan/23 03:13;jlelehe;[~yumwang] 

The Spark application on my side does not specify file.encoding, using the default

!image-2023-01-09-11-10-31-262.png|width=903,height=678!;;;","09/Jan/23 03:36;yumwang;What is your env? you can put the env in your terminal.;;;","09/Jan/23 10:28;jlelehe;[~yumwang] 

Do you mean this?

!image-2023-01-09-18-27-53-479.png|width=967,height=519!;;;","20/Feb/23 05:27;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/40090;;;",,,,,,,,,,,
"Any SparkThrowable (with an error class) not in error-classes.json is masked in SQLExecution.withNewExecutionId and end-user will see ""org.apache.spark.SparkException: [INTERNAL_ERROR]"" ",SPARK-41735,13515776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,allison-portis,allison-portis,28/Dec/22 01:47,30/Jan/23 08:07,13/Jul/23 08:51,30/Jan/23 08:05,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"This change [here|https://github.com/apache/spark/pull/38302/files#diff-fdd1e9e26aa1ba9d1cc923ee7c84a1935dcc285502330a471f1ade7f3ad08bf9] means that any seen error is passed to SparkThrowableHelper.getMessage(...). Any SparkThrowable with an error class (for example, if a connector uses the spark error format i.e. see ErrorClassesJsonReader) will be masked as 
{code:java}
org.apache.spark.SparkException: [INTERNAL_ERROR] Cannot find main error class 'SOME_ERROR_CLASS'{code}
in SparkThrowableHelper.getMessage since errorReader.getMessageTemplate(errorClass) will fail for the error class not defined in error-classes.json.

 ",,allison-portis,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jan 30 08:05:27 UTC 2023,,,,,,,,,,"0|z1eerc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Jan/23 03:58;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/39794;;;","30/Jan/23 03:59;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/39794;;;","30/Jan/23 08:05;cloud_fan;Issue resolved by pull request 39794
[https://github.com/apache/spark/pull/39794];;;",,,,,,,,,,,,,,,
"Session window: analysis rule ""ResolveWindowTime"" does not apply tree-pattern based pruning",SPARK-41733,13515731,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,27/Dec/22 11:26,28/Dec/22 07:01,13/Jul/23 08:51,28/Dec/22 07:01,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,Structured Streaming,,0,,,,,,"We missed to apply tree-pattern based pruning in the rule ResolveWindowTime, which leads to evaluate ResolveWindowTime unnecessarily to the other logical nodes multiple times.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Dec 28 07:01:39 UTC 2022,,,,,,,,,,"0|z1eehc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Dec/22 11:38;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/39247;;;","28/Dec/22 07:01;kabhwan;Issue resolved by pull request 39247
[https://github.com/apache/spark/pull/39247];;;",,,,,,,,,,,,,,,,
"Session window: analysis rule ""SessionWindowing"" does not apply tree-pattern based pruning",SPARK-41732,13515730,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,27/Dec/22 11:15,28/Dec/22 12:03,13/Jul/23 08:51,28/Dec/22 04:17,3.2.2,3.3.1,3.4.0,,,,,,,,,,,,3.2.4,3.3.2,3.4.0,,,SQL,Structured Streaming,,0,,,,,,"We missed to apply tree-pattern based pruning in the rule SessionWindowing, which leads to evaluate SessionWindowing unnecessarily to the other logical nodes multiple times.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Dec 28 04:17:15 UTC 2022,,,,,,,,,,"0|z1eeh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Dec/22 11:16;kabhwan;Will submit a PR sooner.;;;","27/Dec/22 11:27;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/39245;;;","27/Dec/22 11:28;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/39245;;;","28/Dec/22 04:15;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/39253;;;","28/Dec/22 04:17;kabhwan;Issue resolved via https://github.com/apache/spark/pull/39245;;;",,,,,,,,,,,,,
"Spark UI: In jobs API, numActiveStages can be negative in some cases",SPARK-41683,13515191,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,wekoms,wekoms,wekoms,22/Dec/22 08:05,21/Jan/23 15:28,13/Jul/23 08:51,21/Jan/23 15:28,3.3.1,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,"In API /api/v1/applications/.../jobs, for each job, there's one property called numActiveStages, telling how many active stages of the job.

It is calculated by the following logic:
 * When stage is submitted, numActiveStages += 1.
 * When stage is completed, numActiveStages -= 1.
 * When job is ended, for each pending stage, numActiveStages -= 1.

!image-2022-12-22-16-06-45-367.png!

!image-2022-12-23-13-24-30-412.png!

!image-2022-12-22-16-06-21-180.png!

 

However, there's possibility that there's no stage submitted event for some skipped stages. In this case, when job ends, numActiveStages may be minus. Example:
 # job 0 started with stage 0, 1, 2
 ** stage 0: pending
 ** stage 1: pending
 ** stage 2: pending
 ** numActiveStages: 0
 # stage 0 submitted
 ** stage 0: active
 ** stage 1: pending
 ** stage 2: pending
 ** numActiveStages: 1
 # stage 0 completed
 ** stage 0: completed
 ** stage 1: pending
 ** stage 2: pending
 ** numActiveStages: 0
 # job 0 ended
 ** stage 0: completed
 ** stage 1: skipped
 ** stage 2: skipped
 ** numActiveStages: -2",,apachespark,wekoms,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/22 08:06;wekoms;image-2022-12-22-16-06-21-180.png;https://issues.apache.org/jira/secure/attachment/13054060/image-2022-12-22-16-06-21-180.png","22/Dec/22 08:06;wekoms;image-2022-12-22-16-06-45-367.png;https://issues.apache.org/jira/secure/attachment/13054061/image-2022-12-22-16-06-45-367.png","23/Dec/22 05:24;wekoms;image-2022-12-23-13-24-30-412.png;https://issues.apache.org/jira/secure/attachment/13054081/image-2022-12-23-13-24-30-412.png",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jan 21 15:28:14 UTC 2023,,,,,,,,,,"0|z1eb6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Dec/22 06:02;apachespark;User 'kuwii' has created a pull request for this issue:
https://github.com/apache/spark/pull/39190;;;","23/Dec/22 06:02;apachespark;User 'kuwii' has created a pull request for this issue:
https://github.com/apache/spark/pull/39190;;;","21/Jan/23 15:28;srowen;Issue resolved by pull request 39190
[https://github.com/apache/spark/pull/39190];;;",,,,,,,,,,,,,,,
DECODE function returns wrong results when passed NULL,SPARK-41668,13515129,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,srielau,srielau,21/Dec/22 19:09,22/Dec/22 05:57,13/Jul/23 08:51,22/Dec/22 05:57,3.2.0,3.2.1,3.2.2,3.3.0,3.3.1,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,Spark Core,,,0,,,,,,"The DECODE function was implemented for Oracle compatibility. It works similar to CASE expression, but it is supposed to have one major difference: NULL == NULL
[https://docs.oracle.com/database/121/SQLRF/functions057.htm#SQLRF00631]

The Spark implementation does not observe this however:

select decode(null, 6, 'Spark', NULL, 'SQL', 4, 'rocks');

NULL

 The result is supposed to be 'SQL'",,apachespark,Gengliang.Wang,srielau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Dec 22 05:57:25 UTC 2022,,,,,,,,,,"0|z1eatc:",9223372036854775807,,,,,gengliang,,,,,,,,,,,,,,,,,"21/Dec/22 20:59;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39163;;;","22/Dec/22 02:48;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39166;;;","22/Dec/22 05:57;Gengliang.Wang;Issue resolved by pull request 39166
[https://github.com/apache/spark/pull/39166];;;",,,,,,,,,,,,,,,
Memory leak in FileSystem.CACHE when submitting apps to secure cluster using InProcessLauncher,SPARK-41599,13514855,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,risyomei,maciejsmolenski,maciejsmolenski,20/Dec/22 10:45,30/Jun/23 13:28,13/Jul/23 08:51,30/Jun/23 13:24,3.1.2,,,,,,,,,,,,,,3.5.0,,,,,Deploy,YARN,,0,,,,,,"When submitting spark application in kerberos environment the credentials of 'current user' (UserGroupInformation.getCurrentUser()) are being modified.
Filesystem.CACHE entries contain 'current user' (with user credentials) as a key.
Submitting many spark applications using InProcessLauncher cause that FileSystem.CACHE becomes bigger and bigger.
Finally process exits because of OutOfMemory error.

Code for reproduction attached.

 

Output from running 'jmap -histo' on reproduction jvm shows that the number of FileSystem$Cache$Key increases in time:

time: #instances class
1671533274: 2 org.apache.hadoop.fs.FileSystem$Cache$Key
1671533335: 11 org.apache.hadoop.fs.FileSystem$Cache$Key
1671533395: 21 org.apache.hadoop.fs.FileSystem$Cache$Key
1671533455: 30 org.apache.hadoop.fs.FileSystem$Cache$Key
1671533515: 39 org.apache.hadoop.fs.FileSystem$Cache$Key
1671533576: 48 org.apache.hadoop.fs.FileSystem$Cache$Key
1671533636: 57 org.apache.hadoop.fs.FileSystem$Cache$Key
1671533696: 66 org.apache.hadoop.fs.FileSystem$Cache$Key
1671533757: 75 org.apache.hadoop.fs.FileSystem$Cache$Key
1671533817: 84 org.apache.hadoop.fs.FileSystem$Cache$Key
1671533877: 93 org.apache.hadoop.fs.FileSystem$Cache$Key
1671533937: 102 org.apache.hadoop.fs.FileSystem$Cache$Key
1671533998: 111 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534058: 120 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534118: 135 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534178: 140 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534239: 150 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534299: 159 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534359: 168 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534419: 177 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534480: 186 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534540: 195 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534600: 204 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534661: 213 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534721: 222 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534781: 231 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534841: 240 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534902: 249 org.apache.hadoop.fs.FileSystem$Cache$Key
1671534962: 257 org.apache.hadoop.fs.FileSystem$Cache$Key
1671535022: 264 org.apache.hadoop.fs.FileSystem$Cache$Key
1671535083: 273 org.apache.hadoop.fs.FileSystem$Cache$Key
1671535143: 282 org.apache.hadoop.fs.FileSystem$Cache$Key
1671535203: 291 org.apache.hadoop.fs.FileSystem$Cache$Key",,maciejsmolenski,risyomei,stevel@apache.org,touchida,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/22 11:10;maciejsmolenski;InProcLaunchFsIssue.scala;https://issues.apache.org/jira/secure/attachment/13054011/InProcLaunchFsIssue.scala","23/Dec/22 11:44;maciejsmolenski;SPARK-41599-fixes-to-limit-FileSystem-CACHE-size-when-using-InProcessLauncher.diff;https://issues.apache.org/jira/secure/attachment/13054094/SPARK-41599-fixes-to-limit-FileSystem-CACHE-size-when-using-InProcessLauncher.diff",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 30 13:28:52 UTC 2023,,,,,,,,,,"0|z1e98o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Dec/22 09:47;stevel@apache.org;either the fs is being created by ((FileSystem.newInstance()}} and the code isn't calling close() after, or caching is disabled with ""fs.$SCHEME.impl.disable.cache"" set to true.

There's also {{HADOOP-17313. FileSystem.get to support slow-to-instantiate FS clients}} which handles many threads calling get() on slow to create clients...but that only surfaced as an issue in large worker process;;;","23/Dec/22 11:13;stevel@apache.org;1. try explicitly disabling the cache for that fs schema
2. there's a new 3.3.5 rc0 out at  https://dist.apache.org/repos/dist/dev/hadoop/hadoop-3.3.5-RC0/  ; spark picks up the jars if you build it with the right settings. eg


{code}
 build/sbt -Dhadoop.version=3.3.5 -Psnapshots-and-staging
{code}

that does fix the s3a instrumentation leakage, though if s3a fs instances are not being close()d they can still leak thread pools;;;","23/Dec/22 12:02;maciejsmolenski;[~stevel@apache.org] 

Thanks for the updates.

> either the fs is being created by ((FileSystem.newInstance()}} and the code isn't calling close() after

> try explicitly disabling the cache for that fs schema

I added missing 'close's' to places used by 'reproduction code' (SPARK-41599-fixes-to-limit-FileSystem-CACHE-size-when-using-InProcessLauncher.diff) - and I was able to avoid memory leak when running this 'reproduction code'.

However 'reproduction code' is small, this approach (ensuring that close is always called) will not work for larger applications with third party libraries (as in my case).

As I understand there is no way to avoid UGI credentials changes when submitting many apps with InProcessLauncher (I expected such a fix when I was submitting the bug).

This implies that for application (single jvm) which submits many spark apps using InProcessLauncher it is required to disable fs cache.

For my case (large application) I think I will disable fs cache.;;;","23/Dec/22 14:47;stevel@apache.org;apps can call  FileSystem.closeAllForUGI() to remove all cached entries for a given user. It's how multi-user services (hive, tez) stay in control;;;","28/Dec/22 08:39;maciejsmolenski;> apps can call  FileSystem.closeAllForUGI() to remove all cached entries for a given user. It's how multi-user services (hive, tez) stay in control

It's good to know about this method, but I think this might not apply in my case - because from my investigation it seems that ugi used in FileSystem cache changes twice during single spark submit (repro code). This makes it difficult to remove all not needed entries.;;;","18/Jan/23 10:53;stevel@apache.org;well, the challenge there becomes ""not changing that UGI"" or at least closing all fs instances for that ugi before that happens;;;","16/Jun/23 02:37;risyomei;[~stevel@apache.org] [~maciejsmolenski] 

I am having this issue as well.

I'm encountering the same problem.

Could you please guide me on how to ""explicitly disable the cache for that filesystem schema""?

I am trying to add the following configurations in my core-site.xml, but not sure if this is the right way.
{code:java}
  <property>
    <name>fs.hdfs.impl.disable.cache</name>
    <value>true</value>
  </property>
  <property>
    <name>fs.viewfs.impl.disable.cache</name>
    <value>true</value>
  </property> {code};;;","16/Jun/23 10:49;stevel@apache.org;correct. remember, all the source of hadoop is there for you to open in your IDE -it's the only way to be sure;;;","21/Jun/23 14:11;risyomei;Hi, [~stevel@apache.org], Thank you very much for your advice.

After reviewing the code, I think the following PR should be able to fix the issue.

May I ask you to take a look at it when you have time, please?

[https://github.com/apache/spark/pull/41692]

 ;;;","28/Jun/23 16:52;ci-cassandra.apache.org;User 'risyomei' has created a pull request for this issue:
https://github.com/apache/spark/pull/41692;;;","30/Jun/23 13:24;srowen;Issue resolved by pull request 41692
[https://github.com/apache/spark/pull/41692];;;","30/Jun/23 13:28;risyomei;[~srowen], [~stevel@apache.org] 

Thank you very much for your feedback and review.;;;",,,,,,
Union of tables with and without metadata column fails when used in join,SPARK-41557,13514319,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,shardulm,shardulm,17/Dec/22 07:19,20/Jan/23 06:25,13/Jul/23 08:51,20/Jan/23 06:25,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,SQL,,,0,,,,,,"Here is a test case that can be added to {{MetadataColumnSuite}} to demonstrate the issue
{code:scala}
  test(""SPARK-41557: Union of tables with and without metadata column should work"") {
    withTable(tbl) {
      sql(s""CREATE TABLE $tbl (id bigint, data string) PARTITIONED BY (id)"")
      checkAnswer(
        spark.sql(
          s""""""
            SELECT b.*
            FROM RANGE(1)
              LEFT JOIN (
                SELECT id FROM $tbl
                UNION ALL
                SELECT id FROM RANGE(10)
              ) b USING(id)
          """"""),
        Seq(Row(0))
      )
    }
  }
 {code}

Here a table with metadata columns {{$tbl}} is unioned with a table without metdata columns {{RANGE(10)}}. If this result is later used in a join, query analysis fails saying mismatch in the number of columns of the union caused by the metadata columns. However, here we can see that we explicitly project only one column during the union, so the union should be valid.

{code}
org.apache.spark.sql.AnalysisException: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 3 columns and the second input has 1 columns.; line 5 pos 16;
'Project [id#26L]
+- 'Project [id#26L, id#26L]
   +- 'Project [id#28L, id#26L]
      +- 'Join LeftOuter, (id#28L = id#26L)
         :- Range (0, 1, step=1, splits=None)
         +- 'SubqueryAlias b
            +- 'Union false, false
               :- Project [id#26L, index#30, _partition#31]
               :  +- SubqueryAlias testcat.t
               :     +- RelationV2[id#26L, data#27, index#30, _partition#31] testcat.t testcat.t
               +- Project [id#29L]
                  +- Range (0, 10, step=1, splits=None)
{code}",,gurwls223,maximethebault,shardulm,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jan 20 06:25:07 UTC 2023,,,,,,,,,,"0|z1e5y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Dec/22 07:21;shardulm;cc: [~Gengliang.Wang] [~cloud_fan]
;;;","22/Dec/22 00:29;maximethebault;Might be related to (and fixed by) SPARK-41660?

SPARK-41498 is also related to metadata columns + union;;;","20/Jan/23 06:25;shardulm;Confirmed that the test work fine in master now. Thanks!;;;",,,,,,,,,,,,,,,
Decimal.changePrecision produces ArrayIndexOutOfBoundsException,SPARK-41554,13514276,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fe2s,fe2s,fe2s,16/Dec/22 21:04,03/Feb/23 16:51,13/Jul/23 08:51,03/Feb/23 16:51,3.3.1,,,,,,,,,,,,,,3.2.4,3.3.2,3.4.0,,,SQL,,,0,,,,,,"{{Reducing Decimal scale by more than 18 produces exception.}}
{code:java}
Decimal(1, 38, 19).changePrecision(38, 0){code}
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 19
    at org.apache.spark.sql.types.Decimal.changePrecision(Decimal.scala:377)
    at org.apache.spark.sql.types.Decimal.changePrecision(Decimal.scala:328){code}
Reproducing with SQL query:
{code:java}
sql(""select cast(cast(cast(cast(id as decimal(38,15)) as decimal(38,30)) as decimal(38,37)) as decimal(38,17)) from range(3)"").show{code}
The bug exists for {{Decimal}} that is stored using compact long only, it works fine with {{Decimal}} that uses {{scala.math.BigDecimal}} internally.",,apachespark,fe2s,mehreen.ali,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Feb 03 16:51:15 UTC 2023,,,,,,,,,,"0|z1e5og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Dec/22 23:28;apachespark;User 'fe2s' has created a pull request for this issue:
https://github.com/apache/spark/pull/39099;;;","16/Dec/22 23:29;apachespark;User 'fe2s' has created a pull request for this issue:
https://github.com/apache/spark/pull/39099;;;","04/Jan/23 05:14;apachespark;User 'fe2s' has created a pull request for this issue:
https://github.com/apache/spark/pull/39381;;;","31/Jan/23 01:05;apachespark;User 'fe2s' has created a pull request for this issue:
https://github.com/apache/spark/pull/39813;;;","03/Feb/23 16:51;srowen;Resolved by https://github.com/apache/spark/pull/39099;;;",,,,,,,,,,,,,
stats and constraints in LogicalRDD may not be in sync with output attributes,SPARK-41539,13514121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,16/Dec/22 02:38,21/Dec/22 06:02,13/Jul/23 08:51,21/Dec/22 06:02,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"We encountered the case where the output of logical plan and optimized plan were different in LogicalRDD (the difference was exprId for the case), led the situation that stats and constraints are out of sync with output attributes, eventually failed the query.

We should remap stats and constraints based on the output of logical plan, assuming that the output of logical plan and optimized plan are ""slightly"" different (e.g. exprId) but ""semantically"" same.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Dec 21 06:02:37 UTC 2022,,,,,,,,,,"0|z1e4q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Dec/22 02:58;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/39082;;;","21/Dec/22 06:02;kabhwan;Issue resolved by pull request 39082
[https://github.com/apache/spark/pull/39082];;;",,,,,,,,,,,,,,,,
InterpretedUnsafeProjection and InterpretedMutableProjection can corrupt unsafe buffer when used with calendar interval data,SPARK-41535,13514100,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,15/Dec/22 21:14,20/Dec/22 00:31,13/Jul/23 08:51,20/Dec/22 00:31,3.2.3,3.3.1,3.4.0,,,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,SQL,,,0,,,,,,"This returns the wrong answer:
{noformat}
set spark.sql.codegen.wholeStage=false;
set spark.sql.codegen.factoryMode=NO_CODEGEN;

select first(col1), last(col2) from values
(make_interval(0, 0, 0, 7, 0, 0, 0), make_interval(17, 0, 0, 2, 0, 0, 0))
as data(col1, col2);

+---------------+---------------+
|first(col1)    |last(col2)     |
+---------------+---------------+
|16 years 2 days|16 years 2 days|
+---------------+---------------+
{noformat}
In the above case, {{TungstenAggregationIterator}} uses {{InterpretedUnsafeProjection}} to create the aggregation buffer and then initializes all the fields to null. {{InterpretedUnsafeProjection}} incorrectly calls {{UnsafeRowWriter#setNullAt}}, rather than {{unsafeRowWriter#write}}, for the two calendar interval fields. As a result, the writer never allocates memory from the variable length region for the two decimals, and the pointers in the fixed region get left as zero. Later, when {{InterpretedMutableProjection}} attempts to update the first field, {{UnsafeRow#setInterval}} picks up the zero pointer and stores interval data on top of the null-tracking bit set. The call to UnsafeRow#setInterval for the second field also stomps the null-tracking bit set. Later updates to the null-tracking bit set (e.g., calls to setNotNullAt) further corrupt the interval data, turning {{interval 7 years 2 days}} into {{interval 16 years 2 days}}.

Even if you fix the above bug to {{InterpretedUnsafeProjection}} so that the buffer is created correctly, {{InterpretedMutableProjection}} has a similar bug to SPARK-41395, except this time for calendar interval data:
{noformat}
set spark.sql.codegen.wholeStage=false;
set spark.sql.codegen.factoryMode=NO_CODEGEN;

select first(col1), last(col2), max(col3) from values
(null, null, 1),
(make_interval(0, 0, 0, 7, 0, 0, 0), make_interval(17, 0, 0, 2, 0, 0, 0), 3)
as data(col1, col2, col3);

+---------------+---------------+---------+
|first(col1)    |last(col2)     |max(col3)|
+---------------+---------------+---------+
|16 years 2 days|16 years 2 days|3        |
+---------------+---------------+---------+
{noformat}
These two bugs could get exercised during codegen fallback. Take for example this case where I forced codegen to fail for the Greatest expression:
{noformat}
spark-sql> select first(col1), last(col2), max(col3) from values
(null, null, 1),
(make_interval(0, 0, 0, 7, 0, 0, 0), make_interval(17, 0, 0, 2, 0, 0, 0), 3)
as data(col1, col2, col3);

22/12/15 13:06:23 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 70, Column 1: ';' expected instead of 'if'
...
22/12/15 13:06:24 WARN MutableProjection: Expr codegen error and falling back to interpreter mode
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 78, Column 1: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 78, Column 1: ';' expected instead of 'boolean'
...
16 years 2 days	16 years 2 days	3
Time taken: 5.852 seconds, Fetched 1 row(s)
spark-sql> 
{noformat}
",,apachespark,bersprockets,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Dec 20 00:31:45 UTC 2022,,,,,,,,,,"0|z1e4lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Dec/22 21:20;bersprockets;I will take a stab at fixing this shortly.;;;","18/Dec/22 23:12;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/39117;;;","18/Dec/22 23:13;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/39117;;;","20/Dec/22 00:31;gurwls223;Issue resolved by pull request 39117
[https://github.com/apache/spark/pull/39117];;;",,,,,,,,,,,,,,
GA dependencies test faild,SPARK-41522,13513969,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,15/Dec/22 03:06,15/Dec/22 08:40,13/Jul/23 08:51,15/Dec/22 08:40,3.4.0,,,,,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,Project Infra,,,0,,,,,,"{code:java}
Error: ] Some problems were encountered while processing the POMs:
58[FATAL] Non-resolvable parent POM for org.apache.spark:spark-sketch_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11
59[FATAL] Non-resolvable parent POM for org.apache.spark:spark-kvstore_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11
60[FATAL] Non-resolvable parent POM for org.apache.spark:spark-network-common_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11
61[FATAL] Non-resolvable parent POM for org.apache.spark:spark-network-shuffle_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11
62[FATAL] Non-resolvable parent POM for org.apache.spark:spark-unsafe_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11
63[FATAL] Non-resolvable parent POM for org.apache.spark:spark-tags_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11
64[FATAL] Non-resolvable parent POM for org.apache.spark:spark-catalyst_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11
65[FATAL] Non-resolvable parent POM for org.apache.spark:spark-sql_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11
66[FATAL] Non-resolvable parent POM for org.apache.spark:spark-hive_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11
67[FATAL] Non-resolvable parent POM for org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11
68[FATAL] Non-resolvable parent POM for org.apache.spark:spark-streaming-kafka-0-10_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11
69[FATAL] Non-resolvable parent POM for org.apache.spark:spark-streaming-kafka-0-10-assembly_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11
70[FATAL] Non-resolvable parent POM for org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11
71[FATAL] Non-resolvable parent POM for org.apache.spark:spark-avro_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11
72[FATAL] Non-resolvable parent POM for org.apache.spark:spark-connect_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11
73[FATAL] Non-resolvable parent POM for org.apache.spark:spark-connect-common_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 13
74[FATAL] Non-resolvable parent POM for org.apache.spark:spark-protobuf_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11
75[FATAL] Non-resolvable parent POM for org.apache.spark:spark-ganglia-lgpl_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 20, column 11
76[FATAL] Non-resolvable parent POM for org.apache.spark:spark-streaming-kinesis-asl_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 20, column 11
77[FATAL] Non-resolvable parent POM for org.apache.spark:spark-streaming-kinesis-asl-assembly_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11
78[FATAL] Non-resolvable parent POM for org.apache.spark:spark-yarn_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 20, column 11
79[FATAL] Non-resolvable parent POM for org.apache.spark:spark-network-yarn_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11
80[FATAL] Non-resolvable parent POM for org.apache.spark:spark-mesos_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 20, column 11
81[FATAL] Non-resolvable parent POM for org.apache.spark:spark-kubernetes_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 20, column 11
82[FATAL] Non-resolvable parent POM for org.apache.spark:spark-hive-thriftserver_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11
83 @ 
84Error:  The build could not read 25 projects -> [Help 1]
85Error:    
86Error:    The project org.apache.spark:spark-sketch_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/common/sketch/pom.xml) has 1 error
87Error:      Non-resolvable parent POM for org.apache.spark:spark-sketch_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2]
88Error:    
89Error:    The project org.apache.spark:spark-kvstore_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/common/kvstore/pom.xml) has 1 error
90Error:      Non-resolvable parent POM for org.apache.spark:spark-kvstore_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2]
91Error:    
92Error:    The project org.apache.spark:spark-network-common_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/common/network-common/pom.xml) has 1 error
93Error:      Non-resolvable parent POM for org.apache.spark:spark-network-common_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2]
94Error:    
95Error:    The project org.apache.spark:spark-network-shuffle_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/common/network-shuffle/pom.xml) has 1 error
96Error:      Non-resolvable parent POM for org.apache.spark:spark-network-shuffle_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2]
97Error:    
98Error:    The project org.apache.spark:spark-unsafe_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/common/unsafe/pom.xml) has 1 error
99Error:      Non-resolvable parent POM for org.apache.spark:spark-unsafe_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2]
100Error:    
101Error:    The project org.apache.spark:spark-tags_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/common/tags/pom.xml) has 1 error
102Error:      Non-resolvable parent POM for org.apache.spark:spark-tags_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2]
103Error:    
104Error:    The project org.apache.spark:spark-catalyst_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/sql/catalyst/pom.xml) has 1 error
105Error:      Non-resolvable parent POM for org.apache.spark:spark-catalyst_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2]
106Error:    
107Error:    The project org.apache.spark:spark-sql_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/sql/core/pom.xml) has 1 error
108Error:      Non-resolvable parent POM for org.apache.spark:spark-sql_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2]
109Error:    
110Error:    The project org.apache.spark:spark-hive_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/sql/hive/pom.xml) has 1 error
111Error:      Non-resolvable parent POM for org.apache.spark:spark-hive_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2]
112Error:    
113Error:    The project org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/connector/kafka-0-10-token-provider/pom.xml) has 1 error
114Error:      Non-resolvable parent POM for org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11 -> [Help 2]
115Error:    
116Error:    The project org.apache.spark:spark-streaming-kafka-0-10_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/connector/kafka-0-10/pom.xml) has 1 error
117Error:      Non-resolvable parent POM for org.apache.spark:spark-streaming-kafka-0-10_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11 -> [Help 2]
118Error:    
119Error:    The project org.apache.spark:spark-streaming-kafka-0-10-assembly_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/connector/kafka-0-10-assembly/pom.xml) has 1 error
120Error:      Non-resolvable parent POM for org.apache.spark:spark-streaming-kafka-0-10-assembly_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11 -> [Help 2]
121Error:    
122Error:    The project org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/connector/kafka-0-10-sql/pom.xml) has 1 error
123Error:      Non-resolvable parent POM for org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11 -> [Help 2]
124Error:    
125Error:    The project org.apache.spark:spark-avro_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/connector/avro/pom.xml) has 1 error
126Error:      Non-resolvable parent POM for org.apache.spark:spark-avro_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11 -> [Help 2]
127Error:    
128Error:    The project org.apache.spark:spark-connect_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/connector/connect/server/pom.xml) has 1 error
129Error:      Non-resolvable parent POM for org.apache.spark:spark-connect_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2]
130Error:    
131Error:    The project org.apache.spark:spark-connect-common_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/connector/connect/common/pom.xml) has 1 error
132Error:      Non-resolvable parent POM for org.apache.spark:spark-connect-common_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 13 -> [Help 2]
133Error:    
134Error:    The project org.apache.spark:spark-protobuf_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/connector/protobuf/pom.xml) has 1 error
135Error:      Non-resolvable parent POM for org.apache.spark:spark-protobuf_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11 -> [Help 2]
136Error:    
137Error:    The project org.apache.spark:spark-ganglia-lgpl_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/connector/spark-ganglia-lgpl/pom.xml) has 1 error
138Error:      Non-resolvable parent POM for org.apache.spark:spark-ganglia-lgpl_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 20, column 11 -> [Help 2]
139Error:    
140Error:    The project org.apache.spark:spark-streaming-kinesis-asl_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/connector/kinesis-asl/pom.xml) has 1 error
141Error:      Non-resolvable parent POM for org.apache.spark:spark-streaming-kinesis-asl_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 20, column 11 -> [Help 2]
142Error:    
143Error:    The project org.apache.spark:spark-streaming-kinesis-asl-assembly_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/connector/kinesis-asl-assembly/pom.xml) has 1 error
144Error:      Non-resolvable parent POM for org.apache.spark:spark-streaming-kinesis-asl-assembly_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 21, column 11 -> [Help 2]
145Error:    
146Error:    The project org.apache.spark:spark-yarn_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/resource-managers/yarn/pom.xml) has 1 error
147Error:      Non-resolvable parent POM for org.apache.spark:spark-yarn_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 20, column 11 -> [Help 2]
148Error:    
149Error:    The project org.apache.spark:spark-network-yarn_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/common/network-yarn/pom.xml) has 1 error
150Error:      Non-resolvable parent POM for org.apache.spark:spark-network-yarn_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2]
151Error:    
152Error:    The project org.apache.spark:spark-mesos_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/resource-managers/mesos/pom.xml) has 1 error
153Error:      Non-resolvable parent POM for org.apache.spark:spark-mesos_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 20, column 11 -> [Help 2]
154Error:    
155Error:    The project org.apache.spark:spark-kubernetes_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/resource-managers/kubernetes/core/pom.xml) has 1 error
156Error:      Non-resolvable parent POM for org.apache.spark:spark-kubernetes_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 20, column 11 -> [Help 2]
157Error:    
158Error:    The project org.apache.spark:spark-hive-thriftserver_2.12:3.4.0-SNAPSHOT (/__w/spark/spark/sql/hive-thriftserver/pom.xml) has 1 error
159Error:      Non-resolvable parent POM for org.apache.spark:spark-hive-thriftserver_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2]
160Error:  
161Error:  To see the full stack trace of the errors, re-run Maven with the -e switch.
162Error:  Re-run Maven using the -X switch to enable full debug logging.
163Error:  
164Error:  For more information about the errors and possible solutions, please read the following articles:
165Error:  [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
166Error:  [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/UnresolvableModelException
167+ reset_version
168+ find /github/home/.m2/
169+ xargs rm -rf
170+ grep spark-126747
171+ build/mvn -q versions:set -DnewVersion=3.4.0-SNAPSHOT -DgenerateBackupPoms=false
172Using `mvn` from path: /__w/spark/spark/build/apache-maven-3.8.6/bin/mvn
173Error: Process completed with exit code 1.
Run documentation build0sPost Install Java 80sPost Cache Maven local repository0sPost Cache Coursier local repository0sPost Cache Scala, SBT and Maven0sPost Checkout Spark repository1sStop containers1sComplete job0s[SPARK-41438][CONNECT][PYTHON] Implement `DataFrame.colRegex` · apache/spark@7671bc9 {code}
[https://github.com/apache/spark/actions/runs/3700382354/jobs/6268773181]

[https://github.com/apache/spark/actions/runs/3692048078/jobs/6268338213]

[https://github.com/apache/spark/actions/runs/3692967755/jobs/6266278844]

[https://github.com/apache/spark/actions/runs/3699810421/jobs/6268854902]

[https://github.com/apache/spark/actions/runs/3699832270/jobs/6268670802]

 

The first  failed for this reason was SPARK-41248，but this ticket didn't touch maven pom

 

 ",,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Dec 15 08:40:49 UTC 2022,,,,,,,,,,"0|z1e3so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Dec/22 03:10;LuciferYang;cc [~gurwls223] ;;;","15/Dec/22 05:25;LuciferYang;also cc [~dongjoon] ;;;","15/Dec/22 08:01;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39067;;;","15/Dec/22 08:40;dongjoon;Issue resolved by pull request 39067
[https://github.com/apache/spark/pull/39067];;;",,,,,,,,,,,,,,
Accumulator undercounting in the case of retry task with rdd cache,SPARK-41497,13513178,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivoson,Ngone51,Ngone51,12/Dec/22 12:22,05/Mar/23 06:11,13/Jul/23 08:51,03/Mar/23 00:11,2.4.8,3.0.3,3.1.3,3.2.2,3.3.1,,,,,,,,,,3.5.0,,,,,Spark Core,,,0,,,,,,"Accumulator could be undercounted when the retried task has rdd cache.  See the example below and you could also find the completed and reproducible example at [https://github.com/apache/spark/compare/master...Ngone51:spark:fix-acc]

  
{code:scala}
test(""SPARK-XXX"") {
  // Set up a cluster with 2 executors
  val conf = new SparkConf()
    .setMaster(""local-cluster[2, 1, 1024]"").setAppName(""TaskSchedulerImplSuite"")
  sc = new SparkContext(conf)
  // Set up a custom task scheduler. The scheduler will fail the first task attempt of the job
  // submitted below. In particular, the failed first attempt task would success on computation
  // (accumulator accounting, result caching) but only fail to report its success status due
  // to the concurrent executor lost. The second task attempt would success.
  taskScheduler = setupSchedulerWithCustomStatusUpdate(sc)
  val myAcc = sc.longAccumulator(""myAcc"")
  // Initiate a rdd with only one partition so there's only one task and specify the storage level
  // with MEMORY_ONLY_2 so that the rdd result will be cached on both two executors.
  val rdd = sc.parallelize(0 until 10, 1).mapPartitions { iter =>
    myAcc.add(100)
    iter.map(x => x + 1)
  }.persist(StorageLevel.MEMORY_ONLY_2)
  // This will pass since the second task attempt will succeed
  assert(rdd.count() === 10)
  // This will fail due to `myAcc.add(100)` won't be executed during the second task attempt's
  // execution. Because the second task attempt will load the rdd cache directly instead of
  // executing the task function so `myAcc.add(100)` is skipped.
  assert(myAcc.value === 100)
} {code}
 

We could also hit this issue with decommission even if the rdd only has one copy. For example, decommission could migrate the rdd cache block to another executor (the result is actually the same with 2 copies) and the decommissioned executor lost before the task reports its success status to the driver. 

 

And the issue is a bit more complicated than expected to fix. I have tried to give some fixes but all of them are not ideal:

Option 1: Clean up any rdd cache related to the failed task: in practice, this option can already fix the issue in most cases. However, theoretically, rdd cache could be reported to the driver right after the driver cleans up the failed task's caches due to asynchronous communication. So this option can’t resolve the issue thoroughly;

Option 2: Disallow rdd cache reuse across the task attempts for the same task: this option can 100% fix the issue. The problem is this way can also affect the case where rdd cache can be reused across the attempts (e.g., when there is no accumulator operation in the task), which can have perf regression;

Option 3: Introduce accumulator cache: first, this requires a new framework for supporting accumulator cache; second, the driver should improve its logic to distinguish whether the accumulator cache value should be reported to the user to avoid overcounting. For example, in the case of task retry, the value should be reported. However, in the case of rdd cache reuse, the value shouldn’t be reported (should it?);

Option 4: Do task success validation when a task trying to load the rdd cache: this way defines a rdd cache is only valid/accessible if the task has succeeded. This way could be either overkill or a bit complex (because currently Spark would clean up the task state once it’s finished. So we need to maintain a structure to know if task once succeeded or not. )",,apachespark,gurwls223,ivoson,juliuszsompolski,mridulm80,Ngone51,Samwel,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Mar 05 06:11:40 UTC 2023,,,,,,,,,,"0|z1dyx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Dec/22 13:24;Ngone51;[~mridulm80] [~tgraves] [~attilapiros] [~ivoson] any good ideas?;;;","13/Dec/22 07:40;mridulm80;Agree - there appears to be a bunch of scenarios where this can be triggered.
Essentially, whenever block save succeeds and task itself fails, we can end up with this scenario.
As detailed, this could be storage level with replication > 1, block decomissioning, etc - where executor or driver has replicated the data.
I think this can also happen when task itself fails - but after persisting the data (even without replication or decomm) - for example, due to some local/transient shuffle write issues (for example) or commitdenied, etc.

For the options listed:
Agree, option 1 does not solve the issue.
I am also not inclined towards option 2 due to the potential perf impact - though I would expect this to be a rare scenario.
Option 3 looks like a very involved approach, and I am not sure if we can cover all the corner cases.

I am wondering if we can modify option 4 such that it helps.
There are multiple approaches perhaps - one strawman proposal:
a) Add a bit to BlockStatus indicating whether block can be used or not. And currently this bit gets flipped when the task which computed it successfully completes.
b) Maintain a taskId -> BlockStatus* mapping - which is cleaned up whenever a task completes (if successful, then flip bit - else remove its blocks - and replicas (if any)).
c) Propagate taskId in reportBlockStatus from doPutIterator, etc - where new block is getting created in the system.

Thoughts ?;;;","13/Dec/22 13:36;Ngone51;[~mridulm80] Sounds like a better idea than option 4. But I think it still doesn't work well for the case like:

For example, a task is constructed by `rdd1.cache().rdd2`. So if the task fails due to rdd2's computation, I think rdd1's cache should still be able to reuse.

 ;;;","13/Dec/22 14:25;ivoson;I also think that option3/4(include the improved proposal [~mridulm80] suggested) would be promising, the issue could be resolved either from the accumulator side or rdd cache side.
And option4 seems more straightforward since it's a complement to existing cache mechanism. And making decision based on task status could be a feasible solution. As mentioned above, the downside is that it may be overkill. If such cases are small probability events, maybe it is also acceptable.;;;","13/Dec/22 21:19;mridulm80;> For example, a task is constructed by `rdd1.cache().rdd2`. So if the task fails due to rdd2's computation, I think rdd1's cache should still be able to reuse.

We have three cases here.
For some initial task T1:

a) Computation of rdd2 within T1 should have no issues, since initial computation would result in caching it locally - and rdd2 computation is using the result of that local read iterator [1].
b) Failure of T1 and now T2 executes for the same partition - as proposed, T2 will not use result of T1's cache, since it not marked as usable (even though the block might be still marked cached [2]).
c) Replication and/or decom and T2 runs - same case as (b).

Probably 'usable' is incorrect term - 'visible' might be better ? That is, is this block visible to others (outside of the generating task).

[1] We should special case and allow reads from the same task which cached the block - even if it has not yet been marked usable.
[2] When T1 failed, we would drop the blocks which got cached due to it. But even in the case of a race, the flag prevents the use of the cached block.;;;","14/Dec/22 05:09;Ngone51;[~mridulm80]  For b) and c), shouldn't we allow T2 to use the result of T1's cache if rdd1's computation doesn't include any accumulators?;;;","14/Dec/22 06:49;mridulm80;Agree, if we can determine that - do we have a way to do that ?;;;","14/Dec/22 07:19;Ngone51;> do we have a way to do that ?

 

[~mridulm80]  Currently, we only have the mapping between the task and accumulators. Accumulators are registered to the task via TaskContext.get() when they deserialize at the executor.

If we could have a way to know which RDD scope the accumulator within when deserializing, we could set up the mapping between the RDD and accumulators then. This probably

be the most difficult part.;;;","14/Dec/22 07:28;Ngone51;I'm thinking if we could improve the improved Option 4 by changing the rdd cache reuse condition a bit:

 

if there are no accumulators (external only probably) values changed after the rdd computation, then the rdd's cache should be marked as usable/visible no matter whether the task succeeds or fail;

 

If there are accumulators values changed after the rdd computation, then the rdd's cache should only be marked as usable/visible only when the task succeeds.

 

(let me think further and see if it's doable..);;;","14/Dec/22 07:30;mridulm80;[~Ngone51] Agree, that is what I was not sure of (whether we can detect this scenario about use of accumulators which might be updated subsequently). Note that updates to the same accumulator can happen before and after a cache in user code - so we might be able to only catch scenario when there are no accumulators.
If I am not wrong, SQL makes very heavy use of accumulators, and so most stages will end up having them anyway - right ?

I would expect this scenario (even without accumulator) to be fairly low frequency enough that the cost of extra recomputation might be fine.;;;","02/Jan/23 16:09;juliuszsompolski;Note that this issue leads to a correctness issue in Delta Merge, because it depends on a SetAccumulator as a side output channel for gathering files that need to be rewritten by the Merge: https://github.com/delta-io/delta/blob/master/core/src/main/scala/org/apache/spark/sql/delta/commands/MergeIntoCommand.scala#L445-L449
Delta assumes that Spark accumulators can overcount (in some cases where task retries update them in duplicate), but it was assumed that it should never undercount and lose output like that...

Missing some files there can result in duplicate records being inserted instead of existing records being updated.;;;","03/Jan/23 05:01;Ngone51;> If I am not wrong, SQL makes very heavy use of accumulators, and so most stages will end up having them anyway - right ?

Right.

 

> I would expect this scenario (even without accumulator) to be fairly low frequency enough that the cost of extra recomputation might be fine.
 
Agree. So shall we proceed with the improved Option 4 that was proposed by you [~mridulm80] ? [~ivoson] can help with the fix.
 ;;;","04/Jan/23 18:32;mridulm80;Sounds good [~Ngone51], thanks !;;;","08/Jan/23 16:31;apachespark;User 'ivoson' has created a pull request for this issue:
https://github.com/apache/spark/pull/39459;;;","08/Jan/23 16:31;apachespark;User 'ivoson' has created a pull request for this issue:
https://github.com/apache/spark/pull/39459;;;","03/Mar/23 00:11;mridulm80;Issue resolved by pull request 39459
[https://github.com/apache/spark/pull/39459];;;","05/Mar/23 06:11;apachespark;User 'ivoson' has created a pull request for this issue:
https://github.com/apache/spark/pull/40281;;;",
Fix lint-scala command error,SPARK-41475,13511410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dengziming,dengziming,dengziming,10/Dec/22 00:51,10/Dec/22 05:27,13/Jul/23 08:51,10/Dec/22 05:27,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Connect,,,0,,,,,,"SPARK-41369 split connect module into server and common, but didn't change related shell script.",,apachespark,dengziming,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Dec 10 05:27:01 UTC 2022,,,,,,,,,,"0|z1do08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Dec/22 00:53;apachespark;User 'dengziming' has created a pull request for this issue:
https://github.com/apache/spark/pull/39012;;;","10/Dec/22 05:27;dongjoon;Issue resolved by pull request 39012
[https://github.com/apache/spark/pull/39012];;;",,,,,,,,,,,,,,,,
Fix PlanExpression handling in EquivalentExpressions,SPARK-41468,13511062,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,09/Dec/22 12:33,13/Dec/22 08:19,13/Jul/23 08:51,12/Dec/22 10:15,3.4.0,,,,,,,,,,,,,,3.3.2,3.4.0,,,,SQL,,,0,,,,,,,,apachespark,cloud_fan,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Dec 13 08:19:01 UTC 2022,,,,,,,,,,"0|z1dluw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Dec/22 12:57;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/39010;;;","09/Dec/22 12:58;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/39010;;;","12/Dec/22 10:15;cloud_fan;Issue resolved by pull request 39010
[https://github.com/apache/spark/pull/39010];;;","13/Dec/22 08:19;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/39046;;;",,,,,,,,,,,,,,
Correctly transform the SPI services for Yarn Shuffle Service,SPARK-41458,13511009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chengpan,chengpan,chengpan,09/Dec/22 05:35,09/Dec/22 14:15,13/Jul/23 08:51,09/Dec/22 14:14,3.2.0,3.3.0,,,,,,,,,,,,,3.3.2,3.4.0,,,,Shuffle,YARN,,0,,,,,,,,apachespark,chengpan,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Dec 09 14:14:38 UTC 2022,,,,,,,,,,"0|z1dlj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Dec/22 05:43;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/38989;;;","09/Dec/22 05:44;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/38989;;;","09/Dec/22 14:14;srowen;Issue resolved by pull request 38989
[https://github.com/apache/spark/pull/38989];;;",,,,,,,,,,,,,,,
to_char throws NullPointerException when format is null,SPARK-41452,13510838,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,08/Dec/22 21:20,09/Dec/22 00:15,13/Jul/23 08:51,09/Dec/22 00:15,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"Example:
{noformat}
spark-sql> select to_char(454, null);
[INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace.
org.apache.spark.SparkException: [INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace.
...
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.ToCharacter.numberFormat$lzycompute(numberFormatExpressions.scala:227)
	at org.apache.spark.sql.catalyst.expressions.ToCharacter.numberFormat(numberFormatExpressions.scala:227)
	at org.apache.spark.sql.catalyst.expressions.ToCharacter.numberFormatter$lzycompute(numberFormatExpressions.scala:228)
	at org.apache.spark.sql.catalyst.expressions.ToCharacter.numberFormatter(numberFormatExpressions.scala:228)
	at org.apache.spark.sql.catalyst.expressions.ToCharacter.checkInputDataTypes(numberFormatExpressions.scala:236)
{noformat}
Compare to {{to_binary}}:
{noformat}
spark-sql> SELECT to_binary('abc', null);
NULL
Time taken: 3.097 seconds, Fetched 1 row(s)
spark-sql>
{noformat}
Also compare to {{to_char}} in PostgreSQL 14.6:
{noformat}
select to_char(454, null) is null as to_char_is_null;

 to_char_is_null 
-----------------
 t
(1 row)
{noformat}
",,apachespark,bersprockets,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Dec 09 00:15:10 UTC 2022,,,,,,,,,,"0|z1dkh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Dec/22 21:31;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/38986;;;","09/Dec/22 00:15;dongjoon;Issue resolved by pull request 38986
[https://github.com/apache/spark/pull/38986];;;",,,,,,,,,,,,,,,,
Make consistent MR job IDs in FileBatchWriter and FileFormatWriter,SPARK-41448,13510740,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Bone An,Bone An,Bone An,08/Dec/22 10:22,18/Feb/23 14:48,13/Jul/23 08:51,12/Dec/22 10:25,3.0.0,3.1.2,3.2.2,3.3.1,,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,Spark Core,SQL,,0,,,,,,"[SPARK-26873|https://issues.apache.org/jira/browse/SPARK-26873] fix the consistent issue for FileFormatWriter,  but [SPARK-33402|https://issues.apache.org/jira/browse/SPARK-33402] break this requirement by introducing a random long, we need to address this to expects identical task IDs across attempts for correctness.

Also FileBatchWriter doesn't follow this requirement, need to fix it as well.",,apachespark,Bone An,cloud_fan,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Dec 12 10:25:10 UTC 2022,,,,,,,,,,"0|z1djvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Dec/22 10:31;apachespark;User 'boneanxs' has created a pull request for this issue:
https://github.com/apache/spark/pull/38980;;;","12/Dec/22 10:25;cloud_fan;Issue resolved by pull request 38980
[https://github.com/apache/spark/pull/38980];;;",,,,,,,,,,,,,,,,
Do not optimize the input query twice for v1 write fallback,SPARK-41437,13510378,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,07/Dec/22 07:45,07/Dec/22 10:38,13/Jul/23 08:51,07/Dec/22 10:38,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Dec 07 10:38:27 UTC 2022,,,,,,,,,,"0|z1dhmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Dec/22 07:47;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/38942;;;","07/Dec/22 10:38;cloud_fan;Issue resolved by pull request 38942
[https://github.com/apache/spark/pull/38942];;;",,,,,,,,,,,,,,,,
Multi-Stateful Operator watermark support bug fix,SPARK-41411,13510201,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WweiL,WweiL,WweiL,06/Dec/22 18:56,06/Dec/22 23:35,13/Jul/23 08:51,06/Dec/22 23:35,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Structured Streaming,,,0,,,,,,"A typo in passing event time watermark to`StreamingSymmetricHashJoinExec` causes logic errrors. With the bug, the query would work with no error reported but producing incorrect results. ",,apachespark,kabhwan,WweiL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Dec 06 23:35:41 UTC 2022,,,,,,,,,,"0|z1dgjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Dec/22 19:03;apachespark;User 'WweiL' has created a pull request for this issue:
https://github.com/apache/spark/pull/38945;;;","06/Dec/22 19:03;apachespark;User 'WweiL' has created a pull request for this issue:
https://github.com/apache/spark/pull/38945;;;","06/Dec/22 23:35;kabhwan;Issue resolved by pull request 38945
[https://github.com/apache/spark/pull/38945];;;",,,,,,,,,,,,,,,
InterpretedMutableProjection can corrupt unsafe buffer when used with decimal data,SPARK-41395,13509957,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,05/Dec/22 19:55,09/Dec/22 12:46,13/Jul/23 08:51,09/Dec/22 12:44,3.2.3,3.3.1,3.4.0,,,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,SQL,,,0,,,,,,"The following returns the wrong answer:

{noformat}
set spark.sql.codegen.wholeStage=false;
set spark.sql.codegen.factoryMode=NO_CODEGEN;

select max(col1), max(col2) from values
(cast(null  as decimal(27,2)), cast(null   as decimal(27,2))),
(cast(77.77 as decimal(27,2)), cast(245.00 as decimal(27,2)))
as data(col1, col2);

+---------+---------+
|max(col1)|max(col2)|
+---------+---------+
|null     |239.88   |
+---------+---------+
{noformat}
This is because {{InterpretedMutableProjection}} inappropriately uses {{InternalRow#setNullAt}} to set null for decimal types with precision > {{Decimal.MAX_LONG_DIGITS}}.

The path to corruption goes like this:

Unsafe buffer at start:

{noformat}
                                          offset/len for   offset/len for
                                          1st decimal      2nd decimal

offset: 0                8                16 (0x10)        24 (0x18)        32 (0x20)
data:   0300000000000000 0000000018000000 0000000028000000 0000000000000000 0000000000000000 0000000000000000 0000000000000000
{noformat}

When processing the first incoming row ([null, null]), {{InterpretedMutableProjection}} calls {{setNullAt}} for the decimal types. As a result, the pointers to the storage areas for the two decimals in the variable length region get zeroed out.

Buffer after projecting first row (null, null):
{noformat}
                                          offset/len for   offset/len for
                                          1st decimal      2nd decimal

offset: 0                8                16 (0x10)        24 (0x18)        32 (0x20)
data:   0300000000000000 0000000000000000 0000000000000000 0000000000000000 0000000000000000 0000000000000000 0000000000000000
{noformat}

When it's time to project the second row into the buffer, UnsafeRow#setDecimal uses the zero offsets, which causes {{UnsafeRow#setDecimal}} to overwrite the null-tracking bit set with decimal data:

{noformat}
        null-tracking
        bit area
offset: 0                8                16 (0x10)        24 (0x18)        32 (0x20)
data:   5db4000000000000 0000000000000000 0200000000000000 0000000000000000 0000000000000000 0000000000000000 0000000000000000 
{noformat}
The null-tracking bit set is overwritten with 239.88 (0x5db4) rather than 245.00 (0x5fb4) because setDecimal indirectly calls setNotNullAt(1), which turns off the null-tracking bit associated with the field at index 1.


In addition, the decimal at field index 0 is now null because of the corruption of the null-tracking bit set.


When a decimal type with precision > {{Decimal.MAX_LONG_DIGITS}} is null, {{InterpretedMutableProjection}} should write a null {{Decimal}} value rather than call {{setNullAt}} (see.)

This bug could get exercised during codegen fallback. Take for example this case where I forced codegen to fail for the {{Greatest}} expression:

{noformat}
spark-sql> select max(col1), max(col2) from values
(cast(null  as decimal(27,2)), cast(null   as decimal(27,2))),
(cast(77.77 as decimal(27,2)), cast(245.00 as decimal(27,2)))
as data(col1, col2);

22/12/05 08:18:54 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 58, Column 1: ';' expected instead of 'if'
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 58, Column 1: ';' expected instead of 'if'
	at org.codehaus.janino.TokenStreamImpl.compileException(TokenStreamImpl.java:362)
	at org.codehaus.janino.TokenStreamImpl.read(TokenStreamImpl.java:149)
	at org.codehaus.janino.Parser.read(Parser.java:3787)
...
22/12/05 08:18:56 WARN MutableProjection: Expr codegen error and falling back to interpreter mode
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 43, Column 1: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 43, Column 1: ';' expected instead of 'boolean'
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1583)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1580)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 36 more
...

NULL	239.88   <== incorrect result, should be (77.77, 245.00)
Time taken: 6.132 seconds, Fetched 1 row(s)
spark-sql>
{noformat}
",,apachespark,bersprockets,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Dec 09 12:44:55 UTC 2022,,,,,,,,,,"0|z1df1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Dec/22 22:56;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/38923;;;","05/Dec/22 22:57;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/38923;;;","09/Dec/22 12:44;gurwls223;Issue resolved by pull request 38923
[https://github.com/apache/spark/pull/38923];;;",,,,,,,,,,,,,,,
The output column name of `groupBy.agg(count_distinct)` is incorrect,SPARK-41391,13509864,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ritika,podongfeng,podongfeng,05/Dec/22 11:28,31/Mar/23 04:14,13/Jul/23 08:51,31/Mar/23 04:13,3.2.0,3.3.0,3.4.0,,,,,,,,,,,,3.5.0,,,,,SQL,,,0,,,,,,"
scala> val df = spark.range(1, 10).withColumn(""value"", lit(1))
df: org.apache.spark.sql.DataFrame = [id: bigint, value: int]

scala> df.createOrReplaceTempView(""table"")

scala> df.groupBy(""id"").agg(count_distinct($""value""))
res1: org.apache.spark.sql.DataFrame = [id: bigint, count(value): bigint]

scala> spark.sql("" SELECT id, COUNT(DISTINCT value) FROM table GROUP BY id "")
res2: org.apache.spark.sql.DataFrame = [id: bigint, count(DISTINCT value): bigint]

scala> df.groupBy(""id"").agg(count_distinct($""*""))
res3: org.apache.spark.sql.DataFrame = [id: bigint, count(unresolvedstar()): bigint]

scala> spark.sql("" SELECT id, COUNT(DISTINCT *) FROM table GROUP BY id "")
res4: org.apache.spark.sql.DataFrame = [id: bigint, count(DISTINCT id, value): bigint]",,apachespark,cloud_fan,podongfeng,ritikam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Mar 31 04:13:55 UTC 2023,,,,,,,,,,"0|z1degw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Dec/22 11:51;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/38917;;;","05/Dec/22 11:51;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/38917;;;","21/Feb/23 19:25;ritikam;A better solution

 

In *RelationalGroupedDataset* changed the following method to add another case

private[this] def alias(expr: Expression): NamedExpression = expr match

{     case expr: NamedExpression => expr     case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>    

  UnresolvedAlias(a, Some(Column.generateAlias))

{color:#ff0000}   case ag: UnresolvedFunction if (containsStar(Seq(ag))) =>    UnresolvedAlias(expr, None){color}   

  case expr: Expression =>       Alias(expr, toPrettySQL(expr))()   }

In *SQLImplicit.scala* MADE CHANGE TO FOLLOWING METHOD

implicit class StringToColumn(val sc: StringContext) {
def $(args: Any*): ColumnName = {
{color:#ff0000}if (sc.parts.length == 1 && sc.parts.contains(""*"")) {{color}
{color:#ff0000}new ColumnName(name= ""*""){color}
{color:#FF0000}} else

{ new ColumnName(sc.s(args: _*)) }

This seems to work and this create a tree structure similar to what you get when you use spark sql for aggrgate queries. Major difference between scala and spark sql was that spark sql was creating an unresolvedalias for count (distinct * ) expressions where as scala was creating an alias where the alias was count(unresolvedstar()) 

scala> val df = spark.range(1, 10).withColumn(""value"", lit(1))

{*}df{*}: *org.apache.spark.sql.DataFrame* = [id: bigint, value: int]

 

scala> df.createOrReplaceTempView(""table"")

 

scala>  df.groupBy(""id"").agg(count_distinct($""*""))

{*}res1{*}: *org.apache.spark.sql.DataFrame* = [id: bigint, count(DISTINCT id, value): bigint]

 

scala> spark.sql("" SELECT id, COUNT(DISTINCT *) FROM table GROUP BY id "")

{*}res2{*}: *org.apache.spark.sql.DataFrame* = [id: bigint, count(DISTINCT id, value): bigint]

 

scala> df.groupBy(""id"").agg(count_distinct($""value""))

{*}res3{*}: *org.apache.spark.sql.DataFrame* = [id: bigint, COUNT(value): bigint]

 

scala> 

 ;;;","22/Feb/23 02:34;podongfeng;[~ritikam] thanks, would you mind sending a PR for it?;;;","22/Feb/23 05:10;ritikam;I did send a PR;;;","22/Feb/23 05:12;apachespark;User 'ritikam2' has created a pull request for this issue:
https://github.com/apache/spark/pull/40116;;;","22/Feb/23 05:12;apachespark;User 'ritikam2' has created a pull request for this issue:
https://github.com/apache/spark/pull/40116;;;","31/Mar/23 04:13;cloud_fan;Issue resolved by pull request 40116
[https://github.com/apache/spark/pull/40116];;;",,,,,,,,,,
Replace deprecated `.newInstance()` in K8s module,SPARK-41385,13509809,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,05/Dec/22 05:30,05/Dec/22 06:27,13/Jul/23 08:51,05/Dec/22 06:27,3.3.0,,,,,,,,,,,,,,3.3.2,3.4.0,,,,Kubernetes,,,0,,,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Dec 05 06:27:02 UTC 2022,,,,,,,,,,"0|z1de4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Dec/22 05:32;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38909;;;","05/Dec/22 06:27;dongjoon;Issue resolved by pull request 38909
[https://github.com/apache/spark/pull/38909];;;",,,,,,,,,,,,,,,,
Inconsistency of spark session in DataFrame in user function for foreachBatch sink in PySpark,SPARK-41379,13509784,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,04/Dec/22 23:36,05/Dec/22 06:01,13/Jul/23 08:51,05/Dec/22 06:01,3.3.2,3.4.0,,,,,,,,,,,,,3.3.2,3.4.0,,,,PySpark,Structured Streaming,,0,,,,,,"[https://docs.databricks.com/_static/notebooks/merge-in-streaming.html]

According to some manual testing against above code example in PySpark, it seems like the property of sparkSession in given DataFrame is not the same with cloned session in streaming query. In other words, {{df.sparkSession}} does not seem to be same with the cloned spark session which you can access via {{{}df._jdf.sparkSession(){}}}.

So which session to pick depends on the actual implementation of method in PySpark DataFrame, which users would never know. If it leads to pick the different session than expected, it leads to open backdoor for avoiding restrictions (e.g. AQE), unable to see session scoped resources (e.g. temp view), etc.

So it’s quite critical to sync two sessions to refer the same.",,apachespark,gurwls223,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Dec 05 06:01:10 UTC 2022,,,,,,,,,,"0|z1ddz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Dec/22 23:36;kabhwan;Going to submit a PR sooner.;;;","05/Dec/22 03:19;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/38906;;;","05/Dec/22 03:19;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/38906;;;","05/Dec/22 06:01;kabhwan;Issue resolved by pull request 38906
[https://github.com/apache/spark/pull/38906];;;",,,,,,,,,,,,,,
Fix spark-version-info.properties not found on Windows,SPARK-41377,13509765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gaurava,gaurava,gaurava,04/Dec/22 15:19,09/Dec/22 10:08,13/Jul/23 08:51,09/Dec/22 10:07,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,"h1. *What is the problem?*

The spark-version-info.properties files is read during initialization of SparkContext. The absence of the same causes the Spark AM's launch to fail with the following error -
{code}
22/12/04 17:18:24 ERROR ApplicationMaster: User class threw exception: 
java.lang.ExceptionInInitializerError
	at org.apache.spark.package$.<init>(package.scala:93)
	at org.apache.spark.package$.<clinit>(package.scala)
	at org.apache.spark.SparkContext.$anonfun$new$1(SparkContext.scala:195)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:60)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:59)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:84)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:195)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2739)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:978)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:972)
	at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30)
	at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:747)
Caused by: org.apache.spark.SparkException: Could not find spark-version-info.properties
	at org.apache.spark.package$SparkBuildInfo$.<init>(package.scala:62)
	at org.apache.spark.package$SparkBuildInfo$.<clinit>(package.scala)
	... 18 more
{code}

h1. *Why did the problem happen?*
While building Spark, the spark-version-info.properties file [is generated using bash|https://github.com/apache/spark/blob/d62c18b7497997188ec587e1eb62e75c979c1c93/core/pom.xml#L560-L564]. In Windows environment, if Windows Subsystem for Linux (WSL) is installed, it somehow overrides the other bash executables in the PATH, as noted in SPARK-40739. The bash in WSL has a different mounting configuration and thus, the target location specified for spark-version-info.properties won't be the expected location. Ultimately, this leads to spark-version-info.properties to get excluded from the spark-core jar, thus causing the SparkContext initialization to fail with the above depicted error message.

h1. *How can the problem be solved?*
I propose to fix this issue by enhancing the build system to automatically detect and switch between using Powershell for Windows and Bash for non-Windows OS. This yields a seamless build experience for all the platform users. Particularly for Windows users, to not worry about the conflicts with different bash installations.",Windows 10,apachespark,dongjoon,gaurava,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,powershell,XML,,Fri Dec 09 10:07:32 UTC 2022,,,,,,,,,,"0|z1dduw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Dec/22 15:21;apachespark;User 'GauthamBanasandra' has created a pull request for this issue:
https://github.com/apache/spark/pull/38903;;;","09/Dec/22 10:07;dongjoon;This is resolved via https://github.com/apache/spark/pull/38903;;;",,,,,,,,,,,,,,,,
Executor netty direct memory check should respect spark.shuffle.io.preferDirectBufs,SPARK-41376,13509758,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chengpan,chengpan,chengpan,04/Dec/22 13:16,09/Dec/22 18:18,13/Jul/23 08:51,08/Dec/22 00:15,3.2.0,,,,,,,,,,,,,,3.2.4,3.3.2,3.4.0,,,Spark Core,,,0,,,,,,,,apachespark,chengpan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Dec 08 11:27:14 UTC 2022,,,,,,,,,,"0|z1ddtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Dec/22 13:26;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/38901;;;","04/Dec/22 13:26;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/38901;;;","08/Dec/22 00:15;srowen;Issue resolved by pull request 38901
[https://github.com/apache/spark/pull/38901];;;","08/Dec/22 11:26;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/38981;;;","08/Dec/22 11:27;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/38982;;;",,,,,,,,,,,,,
Avoid empty latest KafkaSourceOffset,SPARK-41375,13509735,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wechar,wechar,wechar,03/Dec/22 19:09,08/Dec/22 08:13,13/Jul/23 08:51,08/Dec/22 08:13,3.3.1,,,,,,,,,,,,,,3.3.2,3.4.0,,,,Structured Streaming,,,0,,,,,,"We found the offsetLog recorded an empty offset `{}` for the KafkaSource:

!image-2022-12-04-03-11-11-428.png!
It occurs only once but this empty offset will cause the data duplication.

{*}Root Cause{*}:

The root cause is that Kafka consumer may get empty partitions in extreme cases like getting partitions while Kafka cluster is reassigning partitions.
{code:scala}
// org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer
  private def partitionsAssignedToConsumer(
      body: ju.Set[TopicPartition] => Map[TopicPartition, Long],
      fetchingEarliestOffset: Boolean = false)
    : Map[TopicPartition, Long] = uninterruptibleThreadRunner.runUninterruptibly {

    withRetriesWithoutInterrupt {
      // Poll to get the latest assigned partitions
      consumer.poll(0)
      val partitions = consumer.assignment() // partitions may be empty

      if (!fetchingEarliestOffset) {
        // Call `position` to wait until the potential offset request triggered by `poll(0)` is
        // done. This is a workaround for KAFKA-7703, which an async `seekToBeginning` triggered by
        // `poll(0)` may reset offsets that should have been set by another request.
        partitions.asScala.map(p => p -> consumer.position(p)).foreach(_ => {})
      }

      consumer.pause(partitions)
      logDebug(s""Partitions assigned to consumer: $partitions."")
      body(partitions)
    }
  }
{code}
*Solution:*
Add offset filter for latestOffset.",,apachespark,kabhwan,wechar,,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/22 19:11;wechar;image-2022-12-04-03-11-11-428.png;https://issues.apache.org/jira/secure/attachment/13053491/image-2022-12-04-03-11-11-428.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Dec 08 08:13:12 UTC 2022,,,,,,,,,,"0|z1ddo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Dec/22 19:22;apachespark;User 'wecharyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/38898;;;","03/Dec/22 19:23;apachespark;User 'wecharyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/38898;;;","08/Dec/22 08:13;kabhwan;Issue resolved by pull request 38898
[https://github.com/apache/spark/pull/38898];;;",,,,,,,,,,,,,,,
Update ORC to 1.8.1,SPARK-41374,13509447,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,02/Dec/22 23:41,03/Dec/22 03:20,13/Jul/23 08:51,03/Dec/22 03:20,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Build,,,0,,,,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Dec 03 03:20:26 UTC 2022,,,,,,,,,,"0|z1dbw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Dec/22 23:47;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38894;;;","02/Dec/22 23:47;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38894;;;","03/Dec/22 03:20;dongjoon;Issue resolved by pull request 38894
[https://github.com/apache/spark/pull/38894];;;",,,,,,,,,,,,,,,
Stages UI page fails to load for proxy in some yarn versions ,SPARK-41365,13509200,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,miracle,miracle,miracle,02/Dec/22 09:46,16/Dec/22 19:34,13/Jul/23 08:51,16/Dec/22 19:31,3.3.1,,,,,,,,,,,,,,3.3.2,3.4.0,,,,Web UI,,,0,,,,,,"My environment CDH 5.8 , click to enter the spark UI from the yarn interface
when visit the stage URI, it fails to load,  URI is
{code:java}
http://<yarn-url>:8088/proxy/application_1669877165233_0021/stages/stage/?id=0&attempt=0 {code}
!image-2022-12-02-17-53-03-003.png|width=430,height=697!

Server error stack trace:
{code:java}
Caused by: java.lang.NullPointerException
	at org.apache.spark.status.api.v1.StagesResource.$anonfun$doPagination$1(StagesResource.scala:207)
	at org.apache.spark.status.api.v1.BaseAppResource.$anonfun$withUI$1(ApiRootResource.scala:142)
	at org.apache.spark.ui.SparkUI.withSparkUI(SparkUI.scala:147)
	at org.apache.spark.status.api.v1.BaseAppResource.withUI(ApiRootResource.scala:137)
	at org.apache.spark.status.api.v1.BaseAppResource.withUI$(ApiRootResource.scala:135)
	at org.apache.spark.status.api.v1.StagesResource.withUI(StagesResource.scala:31)
	at org.apache.spark.status.api.v1.StagesResource.doPagination(StagesResource.scala:206)
	at org.apache.spark.status.api.v1.StagesResource.$anonfun$taskTable$1(StagesResource.scala:161)
	at org.apache.spark.status.api.v1.BaseAppResource.$anonfun$withUI$1(ApiRootResource.scala:142)
	at org.apache.spark.ui.SparkUI.withSparkUI(SparkUI.scala:147)
	at org.apache.spark.status.api.v1.BaseAppResource.withUI(ApiRootResource.scala:137)
	at org.apache.spark.status.api.v1.BaseAppResource.withUI$(ApiRootResource.scala:135)
	at org.apache.spark.status.api.v1.StagesResource.withUI(StagesResource.scala:31)
	at org.apache.spark.status.api.v1.StagesResource.taskTable(StagesResource.scala:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){code}
 

The issue is similar to, the final phenomenon of the issue is the same, because the parameter encode twice
https://issues.apache.org/jira/browse/SPARK-32467
https://issues.apache.org/jira/browse/SPARK-33611

The two issues solve two scenarios to avoid encode twice:
1. https redirect proxy
2. set reverse proxy enabled (spark.ui.reverseProxy)  in Nginx 

But if encode twice due to other reasons, such as this issue (yarn proxy), it will also fail",as above,apachespark,miracle,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/22 09:53;miracle;image-2022-12-02-17-53-03-003.png;https://issues.apache.org/jira/secure/attachment/13053435/image-2022-12-02-17-53-03-003.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Dec 16 04:35:18 UTC 2022,,,,,,,,,,"0|z1dadk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Dec/22 10:12;apachespark;User 'yabola' has created a pull request for this issue:
https://github.com/apache/spark/pull/38882;;;","02/Dec/22 10:12;apachespark;User 'yabola' has created a pull request for this issue:
https://github.com/apache/spark/pull/38882;;;","16/Dec/22 04:34;apachespark;User 'yabola' has created a pull request for this issue:
https://github.com/apache/spark/pull/39087;;;","16/Dec/22 04:35;apachespark;User 'yabola' has created a pull request for this issue:
https://github.com/apache/spark/pull/39087;;;",,,,,,,,,,,,,,
allow simple name access of using join hidden columns after subquery alias,SPARK-41350,13507850,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,01/Dec/22 13:37,19/Dec/22 06:07,13/Jul/23 08:51,05/Dec/22 01:24,3.3.1,,,,,,,,,,,,,,3.3.2,3.4.0,,,,SQL,,,0,,,,,,,,apachespark,cloud_fan,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Dec 19 06:07:50 UTC 2022,,,,,,,,,,"0|z1d21s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Dec/22 13:50;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/38862;;;","01/Dec/22 13:50;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/38862;;;","05/Dec/22 01:24;gurwls223;Issue resolved by pull request 38862
[https://github.com/apache/spark/pull/38862];;;","15/Dec/22 14:02;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39077;;;","15/Dec/22 14:02;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39077;;;","19/Dec/22 06:07;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39121;;;",,,,,,,,,,,,
Reading V2 datasource masks underlying error,SPARK-41344,13507725,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wforget,kecheung,kecheung,01/Dec/22 01:08,28/Jan/23 07:19,13/Jul/23 08:51,29/Dec/22 11:27,3.3.0,3.3.1,3.4.0,,,,,,,,,,,,3.4.0,,,,,SQL,,,1,,,,,,"In Spark 3.3, 
 # DataSourceV2Utils, the loadV2Source calls: {*}(CatalogV2Util.loadTable(catalog, ident, timeTravel).get{*}, Some(catalog), Some(ident)).
 # CatalogV2Util.scala, when it tries to *loadTable(x,x,x)* and it fails with any of these exceptions NoSuchTableException, NoSuchDatabaseException, NoSuchNamespaceException, it would return None
 # Coming back to DataSourceV2Utils, None was previously returned and calling None.get results in a cryptic error technically ""correct"", but the *original exceptions NoSuchTableException, NoSuchDatabaseException, NoSuchNamespaceException are thrown away.*

 

*Ask:*

Retain the original error and propagate this to the user. Prior to Spark 3.3, the *original error* was shown and this seems like a design flaw.

 

*Sample user facing error:*

None.get
java.util.NoSuchElementException: None.get
    at scala.None$.get(Option.scala:529)
    at scala.None$.get(Option.scala:527)
    at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:129)
    at org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:209)
    at scala.Option.flatMap(Option.scala:271)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:207)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)

 

*DataSourceV2Utils.scala - CatalogV2Util.loadTable(x,x,x).get*
[https://github.com/apache/spark/blob/7fd654c0142ab9e4002882da4e65d3b25bebd26c/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala#L137]

*CatalogV2Util.scala - Option(catalog.asTableCatalog.loadTable(ident))*

{*}{{*}}[https://github.com/apache/spark/blob/7fd654c0142ab9e4002882da4e65d3b25bebd26c/sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala#L341]

*CatalogV2Util.scala - catching the exceptions and return None*
[https://github.com/apache/spark/blob/7fd654c0142ab9e4002882da4e65d3b25bebd26c/sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala#L344]",,apachespark,ganaakruti,gurwls223,kecheung,planga82,wforget,,,,,,,,,,,,,SPARK-42222,,,SPARK-35803,,,,,,"03/Dec/22 17:24;ganaakruti;image-2022-12-03-09-24-43-285.png;https://issues.apache.org/jira/secure/attachment/13053490/image-2022-12-03-09-24-43-285.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Dec 29 11:27:39 UTC 2022,,,,,,,,,,"0|z1d1a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Dec/22 01:10;kecheung;Github link that is causing the issue
https://github.com/apache/spark/commit/feba05f181f594b53cf9c832cd44b00be2646938;;;","01/Dec/22 01:33;gurwls223;[~kecheung] are you interested in submitting a PR?;;;","01/Dec/22 04:56;kecheung;[~hyukjin.kwon] I currently don't have the bandwidth for this at the moment. I noticed this issue when I was upgrading a connector to spark 3.3. So I did the investigation and concluded that this looks like a design issue since users will never know what's the ""real"" error should they face NoSuchTableException, NoSuchDatabaseException, NoSuchNamespaceException.

 

Hopefully someone who's familiar or has extra time can investigate the fix further.;;;","02/Dec/22 06:01;wforget;I want to work on this and will send a PR later.;;;","02/Dec/22 06:07;apachespark;User 'wForget' has created a pull request for this issue:
https://github.com/apache/spark/pull/38871;;;","02/Dec/22 06:07;apachespark;User 'wForget' has created a pull request for this issue:
https://github.com/apache/spark/pull/38871;;;","03/Dec/22 17:09;ganaakruti;[~wforget] - wondering if we could get a return type of Try[_] instead of Option[_] -> None (which is what we get in case of error at this point). 

The callee in this case, DataFrameReader, I see is applying `.getOrElse` to try v1 load. In this scenario, when the table or entity is not present at all, would it make sense to throw the error as soon the exception is caught instead of trying alternatives? I could be wrong, but like to ask this question. See below code in DataFrameReader - 

 

!image-2022-12-03-09-24-43-285.png!  

CC - [~kecheung] ;;;","05/Dec/22 06:35;wforget;[~ganaakruti]  As far as I understand, V1 and V2 only have different implementations for data source operations, they should have the same tables view. The table that cannot be found in V2 should also not be found in V1.

 

cc [~planga82] Could you please take a look?;;;","06/Dec/22 23:16;planga82;In this case the provider has been detected as DataSourceV2 and also implements SupportsCatalogOptions, so if it fails at that point, it does not make sense to try it as DataSource V1.

The CatalogV2Util.loadTable function catches NoSuchTableException, NoSuchDatabaseException and NoSuchNamespaceException to return an option, which makes sense in other places where it is used, but not at this point. Maybe the best solution is to have another function that does not catch those exceptions to use in this case and does not return an option.;;;","07/Dec/22 01:26;wforget;[~planga82]  Thanks for your reply, I have submitted a PR [https://github.com/apache/spark/pull/38871], can you help me review it?

 

> Maybe the best solution is to have another function that does not catch those exceptions to use in this case and does not return an option.

Does this mean we need to add a new method in CatalogV2Util?;;;","07/Dec/22 19:46;kecheung;+1 [~planga82]. I like this approach of having another function so the real exception can be propagated;;;","07/Dec/22 19:51;kecheung;[~wforget] I believe he just means duplicate CatalogV2Util.loadTable as a new function with signature CatalogV2Util.loadTableThrowsException : Table. The only difference would be you just don't catch the exceptions. Then change this to your new function ({*}CatalogV2Util.loadTableThrowsException(catalog, ident, timeTravel){*}, Some(catalog), Some(ident)). This solves the problem of masking the original exception;;;","29/Dec/22 11:27;gurwls223;Issue resolved by pull request 38871
[https://github.com/apache/spark/pull/38871];;;",,,,,
RocksDB state store WriteBatch doesn't clean up native memory,SPARK-41339,13507499,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kimahriman,kimahriman,kimahriman,30/Nov/22 16:40,01/Dec/22 06:50,13/Jul/23 08:51,01/Dec/22 06:50,3.3.1,,,,,,,,,,,,,,3.3.2,3.4.0,,,,SQL,Structured Streaming,,0,,,,,,"The RocksDB state store uses a WriteBatch to hold updates that get written in a single transaction to commit. Somewhat indirectly abort is called after a successful task which calls writeBatch.clear(), but the data for a writeBatch is stored in a std::string in the native code. Not sure why it's stored as a string, but it is. [rocksdb/write_batch.h at main · facebook/rocksdb · GitHub|https://github.com/facebook/rocksdb/blob/main/include/rocksdb/write_batch.h#L491]

writeBatch.clear simply calls rep_.clear() and rep._resize() ([rocksdb/write_batch.cc at main · facebook/rocksdb · GitHub|https://github.com/facebook/rocksdb/blob/main/db/write_batch.cc#L246-L247]), neither of which actually releases the memory built up by a std::string instance. The only way to actually release this memory is to delete the WriteBatch object itself.

Currently, all memory taken by all write batches will remain until the RocksDB state store instance is closed, which never happens during the normal course of operation as all partitions remain loaded on an executor after a task completes.",,apachespark,kabhwan,kimahriman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Dec 01 06:50:53 UTC 2022,,,,,,,,,,"0|z1czvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Nov/22 23:55;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/38853;;;","01/Dec/22 06:50;kabhwan;Issue resolved by pull request 38853
[https://github.com/apache/spark/pull/38853];;;",,,,,,,,,,,,,,,,
BroadcastExchange does not support the execute() code path. when AQE enabled,SPARK-41336,13507415,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,JacobZheng,JacobZheng,30/Nov/22 09:13,02/Dec/22 02:23,13/Jul/23 08:51,02/Dec/22 02:23,3.2.0,,,,,,,,,,,,,,3.2.2,,,,,SQL,,,0,,,,,,"I am getting an error when running the following code.

{code:java}
val df1 = spark.read.format(""delta"").load(""/table/n4bee1a51083e49e6adacf2a"").selectExpr(""ID"",""TITLE"")
val df2 = spark.read.format(""delta"").load(""/table/db8e1ef7f0fdb447d8aae2e7"").selectExpr(""ID"",""STATUS"").filter(""STATUS == 3"")
val df3 = spark.read.format(""delta"").load(""/table/q56719945d2534c9c88eb669"").selectExpr(""EMPNO"",""TITLE"",""LEAVEID"",""WFINSTANCEID"",""SUBMIT1"").filter(""SUBMIT1 == 1"")
val df4 = spark.read.format(""delta"").load(""/table/pd39b547fb6c24382861af92"").selectExpr(""`年月`"")
val jr1 = df3.join(df2,df3(""WFINSTANCEID"")===df2(""ID""),""inner"").select(df3(""EMPNO"").as(""NEWEMPNO""),df3(""TITLE"").as(""NEWTITLE""),df3(""LEAVEID""))
val jr2 = jr1.join(df1,jr1(""LEAVEID"")===df1(""ID""),""LEFT_OUTER"").select(jr1(""NEWEMPNO""),jr1(""NEWTITLE""),df1(""TITLE"").as(""TYPE""))
val gr1 = jr2.groupBy(jr2(""NEWEMPNO"").as(""EMPNO__0""),jr2(""TYPE"").as(""TYPE__1"")).agg(Map.empty[String,String]).toDF(""EMPNO"",""TYPE"")
val temp1 = gr1.selectExpr(""*"",""9 as KEY"")
val temp2 = df4.selectExpr(""*"",""9 as KEY"")
val jr3 = temp1.join(temp2,temp1(""KEY"")===temp2(""KEY""),""OUTER"").select(temp1(""EMPNO""),temp1(""TYPE""),temp1(""KEY""),temp2(""`年月`""))
jr3.show(200)
{code}

The error message is as follows

{code:java}
java.lang.UnsupportedOperationException: BroadcastExchange does not support the execute() code path.
  at org.apache.spark.sql.errors.QueryExecutionErrors$.executeCodePathUnsupportedError(QueryExecutionErrors.scala:1655)
  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecute(BroadcastExchangeExec.scala:203)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)
  at org.apache.spark.sql.execution.adaptive.QueryStageExec.doExecute(QueryStageExec.scala:119)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)
  at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:526)
  at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:454)
  at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:453)
  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:497)
  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:750)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)
  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:325)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:443)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:338)
  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:366)
  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:338)
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)
  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2728)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2935)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:326)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:806)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:765)
{code}

When I switch to version 3.0.1 or set spark.sql.adaptive.enabled=false, the code runs successfully.

{code:java}
+- ShuffleQueryStage 3
   *(4) LocalLimit 200
   +- *(4) Project [cast(null as string) AS EMPNO#8304, cast(null as string) AS TYPE#8319, 年月#8338, KEY#8341]
      +- BroadcastQueryStage 2
         +- BroadcastExchange IdentityBroadcastMode, [id=#9619]
            +- *(3) Project [年月#8338, 10 AS KEY#8341]
               +- *(3) ColumnarToRow
                  +- FileScan parquet [年月#8338] Batched: true, DataFilters: [], Format: Parquet, Location: TahoeLogFileIndex(1 paths)[/table/pd39b547fb6c24382861af92], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<年月:string>
{code}


",,JacobZheng,yumwang,,,,,,,,,,,,,,,,,,SPARK-39551,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Dec 01 11:17:26 UTC 2022,,,,,,,,,,"0|z1czd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Nov/22 09:41;yumwang;Could you use Spark 3.3.1?;;;","01/Dec/22 05:51;JacobZheng;This case runs successfully on spark3.3.1. I will check the differences between the two versions. [~yumwang];;;","01/Dec/22 11:17;JacobZheng;[SPARK-39551][SQL][3.2] Add AQE invalid plan check solved this case.;;;",,,,,,,,,,,,,,,
Fix SparkStatusTracker.getExecutorInfos by switch On/OffHeapStorageMemory info,SPARK-41327,13507334,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lingyuan,lingyuan,dongjoon,29/Nov/22 22:08,30/Nov/22 19:49,13/Jul/23 08:51,30/Nov/22 19:49,2.4.8,3.0.3,3.1.3,3.2.2,3.3.1,3.4.0,,,,,,,,,3.2.3,3.3.2,3.4.0,,,Spark Core,,,0,,,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Nov 30 19:49:03 UTC 2022,,,,,,,,,,"0|z1cyv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Nov/22 22:27;apachespark;User 'ylybest' has created a pull request for this issue:
https://github.com/apache/spark/pull/38843;;;","30/Nov/22 19:49;dongjoon;Issue resolved by pull request 38843
[https://github.com/apache/spark/pull/38843];;;",,,,,,,,,,,,,,,,
AM shutdown hook fails with IllegalStateException if AM crashes on startup (recurrence of SPARK-3900),SPARK-41313,13507186,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,xinglin,xinglin,xinglin,29/Nov/22 05:04,04/Dec/22 14:25,13/Jul/23 08:51,04/Dec/22 14:25,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,YARN,,0,,,,,,"SPARK-3900 fixed the {{IllegalStateException}} in cleanupStagingDir in ApplicationMaster's shutdownhook. However, SPARK-21138 accidentally reverted/undid that change when fixing the ""Wrong FS"" bug. Now, we are seeing SPARK-3900 reported by our users at Linkedin. We need to bring back the fix for SPARK-3900.

The illegalStateException when creating a new filesystem object is due to the limitation in Hadoop that we can not register a shutdownhook during shutdown. So, when a spark job fails during pre-launch, as part of shutdown, cleanupStagingDir would be called. Then, if we attempt to create a new filesystem object for the first time, HDFS would try to register a hook to shutdown KeyProviderCache when creating a ClientContext for DFSClient. As a result, we hit the {{IllegalStateException}}. We should avoid the creation of a new filesystem object in cleanupStagingDir() when it is called in a shutdown hook. This was introduced in SPARK-3900. However, SPARK-21138 accidentally reverted/undid that change. We need to bring back that fix to Spark to avoid the {{IllegalStateException}}.

  ",,apachespark,xinglin,xkrogen,,,,,,,,,,,,,,,,,,,,,SPARK-3900,SPARK-21138,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Dec 04 14:25:08 UTC 2022,,,,,,,,,,"0|z1cxy8:",9223372036854775807,,,,,xkrogen,,,,,,,,,,,,,,,,,"29/Nov/22 05:57;apachespark;User 'xinglin' has created a pull request for this issue:
https://github.com/apache/spark/pull/38832;;;","29/Nov/22 05:57;apachespark;User 'xinglin' has created a pull request for this issue:
https://github.com/apache/spark/pull/38832;;;","04/Dec/22 14:25;srowen;Issue resolved by pull request 38832
[https://github.com/apache/spark/pull/38832];;;",,,,,,,,,,,,,,,
applyInPandasWithState can produce incorrect key value in user function for timed out state,SPARK-41261,13505868,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,25/Nov/22 07:01,12/Dec/22 18:10,13/Jul/23 08:51,27/Nov/22 02:01,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Structured Streaming,,,0,,,,,,We observed the issue that user function retrieves incorrect key in user function for timed out state. After RCA we figured out this could happen when the columns of grouping keys are not placed sequentially at earliest place.,,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Nov 27 02:01:45 UTC 2022,,,,,,,,,,"0|z1cpu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Nov/22 07:01;kabhwan;Will submit a PR shortly.;;;","25/Nov/22 07:22;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/38798;;;","27/Nov/22 02:01;gurwls223;Issue resolved by pull request 38798
[https://github.com/apache/spark/pull/38798];;;",,,,,,,,,,,,,,,
YarnAllocator.rpIdToYarnResource map is not properly updated,SPARK-41254,13505759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,John Zhang,John Zhang,John Zhang,24/Nov/22 13:11,28/Nov/22 14:26,13/Jul/23 08:51,28/Nov/22 14:26,3.1.0,3.2.0,3.3.1,,,,,,,,,,,,3.2.4,3.3.2,3.4.0,,,YARN,,,0,,,,,,"Log messages ""INFO YarnAllocator: Resource profile 0 doesn't exist, adding it"" repeats multiple times in yarn stderr.

The log should be outputted only once because it happens in _YarnAllocator.createYarnResourceForResourceProfile_ on a default ResourceProfile

After digging into the code, I found a bug caused by misleading usage of _ConcurrentHashMap.contains_",,apachespark,John Zhang,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Nov 28 14:26:28 UTC 2022,,,,,,,,,,"0|z1cp5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Nov/22 13:27;apachespark;User 'CavemanIV' has created a pull request for this issue:
https://github.com/apache/spark/pull/38790;;;","28/Nov/22 14:26;srowen;Issue resolved by pull request 38790
[https://github.com/apache/spark/pull/38790];;;",,,,,,,,,,,,,,,,
Regression in IntegralDivide returning null instead of 0,SPARK-41219,13505202,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,razajafri,razajafri,22/Nov/22 02:57,31/Jan/23 04:00,13/Jul/23 08:51,31/Jan/23 04:00,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"There seems to be a regression in Spark 3.4 Integral Divide

 
{code:java}
scala> val df = Seq(""0.5944910"",""0.3314242"").toDF(""a"")
df: org.apache.spark.sql.DataFrame = [a: string]

scala> df.selectExpr(""cast(a as decimal(7,7)) div 100"").show
+---------------------------------+
|(CAST(a AS DECIMAL(7,7)) div 100)|
+---------------------------------+
|                             null|
|                             null|
+---------------------------------+
{code}
 

While in Spark 3.3.0
{code:java}
scala> val df = Seq(""0.5944910"",""0.3314242"").toDF(""a"")
df: org.apache.spark.sql.DataFrame = [a: string]

scala> df.selectExpr(""cast(a as decimal(7,7)) div 100"").show
+---------------------------------+
|(CAST(a AS DECIMAL(7,7)) div 100)|
+---------------------------------+
|                                0|
|                                0|
+---------------------------------+
{code}
 ",,apachespark,cloud_fan,razajafri,ulysses,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jan 31 04:00:22 UTC 2023,,,,,,,,,,"0|z1clq0:",9223372036854775807,,,,,revans2,,,,,,,,,,,,,,,,,"22/Nov/22 06:56;gurwls223;cc [~yumwang]] FYI;;;","22/Nov/22 09:14;yumwang;cc [~ulysses];;;","22/Nov/22 11:29;ulysses;I'm looking at this;;;","22/Nov/22 11:58;ulysses;it seems the root reason is decimal.toPrecision will break when change to decimal(0, 0)
{code:java}
val df = Seq(0).toDF(""a"")
// return 0
df.selectExpr(""cast(0 as decimal(0,0))"").show
// reutrn 0
df.select(lit(BigDecimal(0)) as ""c"").show
// return null
df.select(lit(BigDecimal(0)) as ""c"").selectExpr(""cast(c as decimal(0,0))"").show
{code};;;","22/Nov/22 12:48;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/38760;;;","23/Nov/22 06:14;razajafri;Thank you for looking into this issue. I have also noticed that the `IntegralDivide` has the output dataType = LongType, so why is it also overriding the `resultDecimalType`?? 

It will never be called AFAIK, it's only called from `dataType` in `BinaryArithmetic`;;;","23/Nov/22 11:40;ulysses;[~razajafri] it would use `resultDecimalType`. Before going to Long, the result is Decimal. Can see the code in `DivModeLike`.;;;","01/Dec/22 15:06;srowen;Is this the same as https://issues.apache.org/jira/browse/SPARK-41207 ?;;;","31/Jan/23 04:00;cloud_fan;Issue resolved by pull request 38760
[https://github.com/apache/spark/pull/38760];;;",,,,,,,,,
Update ORC to 1.7.7,SPARK-41202,13503734,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,19/Nov/22 07:55,19/Nov/22 12:17,13/Jul/23 08:51,19/Nov/22 12:17,3.3.2,,,,,,,,,,,,,,3.3.2,,,,,Build,,,0,,,,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Nov 19 12:17:41 UTC 2022,,,,,,,,,,"0|z1cco8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Nov/22 07:59;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38724;;;","19/Nov/22 08:00;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38724;;;","19/Nov/22 12:17;dongjoon;Issue resolved by pull request 38724
[https://github.com/apache/spark/pull/38724];;;",,,,,,,,,,,,,,,
Streaming query metrics is broken with mixed-up usage of DSv1 streaming source and DSv2 streaming source,SPARK-41199,13503698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,18/Nov/22 20:46,19/Nov/22 13:42,13/Jul/23 08:51,19/Nov/22 13:42,3.2.2,3.3.1,3.4.0,,,,,,,,,,,,3.4.0,,,,,Structured Streaming,,,0,,,,,,"(It seems like a long standing issue. It probably applies to 2.x as well. I just marked the version line we did not EOL.)

If a streaming query contains both DSv1 and DSv2 streaming sources together, it only collects metrics properly for DSv1 sources.

 ",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Nov 19 13:42:56 UTC 2022,,,,,,,,,,"0|z1ccg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Nov/22 21:12;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/38719;;;","19/Nov/22 13:42;kabhwan;Issue resolved by pull request 38719
[https://github.com/apache/spark/pull/38719];;;",,,,,,,,,,,,,,,,
Streaming query metrics is broken with CTE,SPARK-41198,13503691,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,18/Nov/22 19:03,20/Nov/22 21:44,13/Jul/23 08:51,20/Nov/22 21:44,3.2.2,3.3.1,3.4.0,,,,,,,,,,,,3.4.0,,,,,Structured Streaming,,,0,,,,,,"We have observed a case the metrics are not available for the streaming query which contains CTE.

Looks like CTE was inlined in analysis phase in Spark 3.1.x and it was changed to be inlined in optimization phase in Spark 3.2.x. ProgressReporter depends on analyzed plan, hence the change made ProgressReporter to see CTE nodes, which ends up with having different number of leaf nodes between analyzed plan and executed plan.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Nov 20 21:44:32 UTC 2022,,,,,,,,,,"0|z1cceo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Nov/22 20:28;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/38717;;;","18/Nov/22 20:28;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/38717;;;","20/Nov/22 21:44;kabhwan;Issue resolved by pull request 38717
[https://github.com/apache/spark/pull/38717];;;",,,,,,,,,,,,,,,
Ignore `collect data with single partition larger than 2GB bytes array limit` in `DatasetLargeResultCollectingSuite` as default,SPARK-41193,13503352,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,18/Nov/22 04:27,12/Dec/22 18:10,13/Jul/23 08:51,22/Nov/22 06:51,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Tests,,,0,,,,,,"Test this suite with **Java 8/11/17** on Linux/MacOS On Apple Silicon with following commands:

- Maven:

```
build/mvn clean install -DskipTests -pl sql/core -am
build/mvn clean test -pl sql/core -Dtest=none -DwildcardSuites=org.apache.spark.sql.DatasetLargeResultCollectingSuite
```

and 

 

```
dev/change-scala-version.sh 2.13 
build/mvn clean install -DskipTests -pl sql/core -am -Pscala-2.13
build/mvn clean test -pl sql/core -Pscala-2.13 -Dtest=none -DwildcardSuites=org.apache.spark.sql.DatasetLargeResultCollectingSuite
```

- SBT:

```
build/sbt clean ""sql/testOnly org.apache.spark.sql.DatasetLargeResultCollectingSuite""
```


```
dev/change-scala-version.sh 2.13 
build/sbt clean ""sql/testOnly org.apache.spark.sql.DatasetLargeResultCollectingSuite"" -Pscala-2.13
```


All test failed with `java.lang.OutOfMemoryError: Java heap space` as follows:

```
10:19:56.910 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.OutOfMemoryError: Java heap space
        at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
        at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
        at org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)
        at org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)
        at org.apache.spark.serializer.SerializerHelper$$$Lambda$2321/1995130077.apply(Unknown Source)
        at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
        at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
        at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
        at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
        at org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:271)
        at org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:271)
        at org.apache.spark.util.Utils$$$Lambda$2324/69671223.apply(Unknown Source)
        at org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:249)
        at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:271)
        at org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)
        at org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)
        at org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$2323/1073743200.apply(Unknown Source)
        at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
        at org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)
        at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
        at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
        at org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:599)
```",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Nov 22 06:51:12 UTC 2022,,,,,,,,,,"0|z1cac0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Nov/22 04:44;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/38704;;;","22/Nov/22 06:51;gurwls223;Issue resolved by pull request 38704
[https://github.com/apache/spark/pull/38704];;;",,,,,,,,,,,,,,,,
Task finished before speculative task scheduled leads to holding idle executors,SPARK-41192,13503350,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,toujours33,toujours33,toujours33,18/Nov/22 04:13,21/Dec/22 03:36,13/Jul/23 08:51,21/Dec/22 03:36,3.2.2,3.3.1,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,dynamic_allocation,,,,,"When task finished before speculative task has been scheduled by DAGScheduler, then the speculative tasks will be considered as pending and count towards the calculation of number of needed executors, which will lead to request more executors than needed
h2. Background & Reproduce

In one of our production job, we found that ExecutorAllocationManager was holding more executors than needed. 

We found it's difficult to reproduce in the test environment. In order to stably reproduce and debug, we temporarily annotated the scheduling code of speculative tasks in TaskSetManager:363 to ensure that the task be completed before the speculative task being scheduled.
{code:java}
// Original code
private def dequeueTask(
    execId: String,
    host: String,
    maxLocality: TaskLocality.Value): Option[(Int, TaskLocality.Value, Boolean)] = {
  // Tries to schedule a regular task first; if it returns None, then schedules
  // a speculative task
  dequeueTaskHelper(execId, host, maxLocality, false).orElse(
    dequeueTaskHelper(execId, host, maxLocality, true))
} 
// Speculative task will never be scheduled
private def dequeueTask(
    execId: String,
    host: String,
    maxLocality: TaskLocality.Value): Option[(Int, TaskLocality.Value, Boolean)] = {
  // Tries to schedule a regular task first; if it returns None, then schedules
  // a speculative task
  dequeueTaskHelper(execId, host, maxLocality, false)
}  {code}
Referring to examples in SPARK-30511

You will see when running the last task, we would be hold 38 executors (see attachment), which is exactly (149 + 1) / 4 = 38. But actually there are only 2 tasks in running, which requires Math.min(20, 2/4) = 20 executors indeed.
{code:java}
./bin/spark-shell --master yarn --conf spark.speculation=true --conf spark.executor.cores=4 --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.minExecutors=20 --conf spark.dynamicAllocation.maxExecutors=1000 {code}
{code:java}
val n = 4000
val someRDD = sc.parallelize(1 to n, n)
someRDD.mapPartitionsWithIndex( (index: Int, it: Iterator[Int]) => {
if (index > 3998) {
    Thread.sleep(1000 * 1000)
} else if (index > 3850) {
    Thread.sleep(50 * 1000) // Fake running tasks
} else {
    Thread.sleep(100)
}
Array.fill[Int](1)(1).iterator{code}
 

I will have a PR ready to fix this issue",,apachespark,mridulm80,toujours33,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/22 05:05;toujours33;dynamic-executors;https://issues.apache.org/jira/secure/attachment/13052356/dynamic-executors","18/Nov/22 05:05;toujours33;dynamic-log;https://issues.apache.org/jira/secure/attachment/13052357/dynamic-log",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Dec 21 03:36:56 UTC 2022,,,,,,,,,,"0|z1cabk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Nov/22 07:36;apachespark;User 'toujours33' has created a pull request for this issue:
https://github.com/apache/spark/pull/38709;;;","18/Nov/22 09:43;apachespark;User 'toujours33' has created a pull request for this issue:
https://github.com/apache/spark/pull/38711;;;","18/Nov/22 09:43;apachespark;User 'toujours33' has created a pull request for this issue:
https://github.com/apache/spark/pull/38711;;;","21/Dec/22 03:36;mridulm80;Issue resolved by pull request 38711
[https://github.com/apache/spark/pull/38711];;;",,,,,,,,,,,,,,
Add an environment to switch on and off namedtuple hack ,SPARK-41189,13503342,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,18/Nov/22 03:26,12/Dec/22 18:10,13/Jul/23 08:51,18/Nov/22 08:59,3.3.1,3.4.0,,,,,,,,,,,,,3.4.0,,,,,PySpark,,,0,,,,,,SPARK-32079 removed the namedtuple hack but there are still bugs being fixed in the cloudpickle upstream. This JIRA aims to have a switch to on and off this.,,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Nov 18 08:59:52 UTC 2022,,,,,,,,,,"0|z1ca9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Nov/22 03:35;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/38700;;;","18/Nov/22 03:35;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/38700;;;","18/Nov/22 08:59;gurwls223;Issue resolved by pull request 38700
[https://github.com/apache/spark/pull/38700];;;",,,,,,,,,,,,,,,
Set executorEnv OMP_NUM_THREADS to be spark.task.cpus by default for spark executor JVM processes,SPARK-41188,13503328,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,18/Nov/22 03:16,28/Feb/23 02:05,13/Jul/23 08:51,19/Nov/22 09:27,3.3.1,,,,,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,ML,Spark Core,,0,,,,,,"Set executorEnv OMP_NUM_THREADS to be spark.task.cpus by default for spark executor JVM processes, this is for limiting the thread number for OpenBLAS routine to the number of cores assigned to this executor because some spark ML algorithms calls OpenBlAS via netlib-java",,apachespark,weichenxu123,,,,,,,,,,,,,,,,,,,,,SPARK-42596,,,SPARK-42613,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Nov 18 03:24:33 UTC 2022,,,,,,,,,,"0|z1ca6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Nov/22 03:24;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/38699;;;",,,,,,,,,,,,,,,,,
[Core] LiveExecutor MemoryLeak in AppStatusListener when ExecutorLost happen,SPARK-41187,13503315,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yimo_yym,yimo_yym,yimo_yym,18/Nov/22 02:42,12/Dec/22 04:54,13/Jul/23 08:51,12/Dec/22 04:54,3.1.2,3.1.3,3.2.0,3.3.1,,,,,,,,,,,3.3.2,3.4.0,,,,Spark Core,,,0,,,,,,"We have a long running thriftserver, which we found memory leak happened. One of the memory leak is like below.  !image-2022-11-18-10-57-49-230.png!

The event queue size in our prod env is set to very large to avoid message drop, but we still find the message drop in log. And the event processing time is very long , event is accumulated in queue.

In heap dump we found LiveExecutor  instances number is also become very huge. After check the heap dump, Finally we found the reason. 

!image-2022-11-18-11-01-57-435.png!

The reason is:

For a shuffle map stage tasks, if a executor lost happen,  the finished task will be resubmitted, and send out a taskEnd Message with reason ""Resubmitted"" in TaskSetManager.scala, this will cause the  activeTask in AppStatusListner's liveStage become negative
{code:java}
override def executorLost(execId: String, host: String, reason: ExecutorLossReason): Unit = {
  // Re-enqueue any tasks that ran on the failed executor if this is a shuffle map stage,
  // and we are not using an external shuffle server which could serve the shuffle outputs.
  // The reason is the next stage wouldn't be able to fetch the data from this dead executor
  // so we would need to rerun these tasks on other executors.
  if (isShuffleMapTasks && !env.blockManager.externalShuffleServiceEnabled && !isZombie) {
    for ((tid, info) <- taskInfos if info.executorId == execId) {
      val index = info.index
      // We may have a running task whose partition has been marked as successful,
      // this partition has another task completed in another stage attempt.
      // We treat it as a running task and will call handleFailedTask later.
      if (successful(index) && !info.running && !killedByOtherAttempt.contains(tid)) {
        successful(index) = false
        copiesRunning(index) -= 1
        tasksSuccessful -= 1
        addPendingTask(index)
        // Tell the DAGScheduler that this task was resubmitted so that it doesn't think our
        // stage finishes when a total of tasks.size tasks finish.
        sched.dagScheduler.taskEnded(
          tasks(index), Resubmitted, null, Seq.empty, Array.empty, info)
      }
    }
  }{code}
 

!image-2022-11-18-11-09-34-760.png!

if liveStage activeTask is negative, it will never be removed, thus cause the executor moved to deadExecutors will never to removed, cause it need to check there is no stage submission less than its remove time before removed. 
{code:java}
/** Was the specified executor active for any currently live stages? */
private def isExecutorActiveForLiveStages(exec: LiveExecutor): Boolean = {
  liveStages.values.asScala.exists { stage =>
    stage.info.submissionTime.getOrElse(0L) < exec.removeTime.getTime
  }
} 

override def onStageCompleted(event: SparkListenerStageCompleted): Unit = {
  .....

  // remove any dead executors that were not running for any currently active stages
  deadExecutors.retain((execId, exec) => isExecutorActiveForLiveStages(exec))
}{code}
 

Add the corresponding logs in prod env as attachment. The resubmitted task number is equals to the activeTasks in heap dump for that stage.

!image-20221113232214179.png!

!image-20221113232233952.png!

Hope I describe it clear, I will create a pull request later,  we just ignore the resubmitted message in AppStatusListener to fix it.",,apachespark,yimo_yym,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/22 02:57;yimo_yym;image-2022-11-18-10-57-49-230.png;https://issues.apache.org/jira/secure/attachment/13052352/image-2022-11-18-10-57-49-230.png","18/Nov/22 03:01;yimo_yym;image-2022-11-18-11-01-57-435.png;https://issues.apache.org/jira/secure/attachment/13052353/image-2022-11-18-11-01-57-435.png","18/Nov/22 03:09;yimo_yym;image-2022-11-18-11-09-34-760.png;https://issues.apache.org/jira/secure/attachment/13052354/image-2022-11-18-11-09-34-760.png","25/Nov/22 05:17;yimo_yym;image-20221113232214179.png;https://issues.apache.org/jira/secure/attachment/13053103/image-20221113232214179.png","25/Nov/22 05:18;yimo_yym;image-20221113232233952.png;https://issues.apache.org/jira/secure/attachment/13053102/image-20221113232233952.png",,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Nov 25 05:16:01 UTC 2022,,,,,,,,,,"0|z1ca3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Nov/22 03:38;apachespark;User 'wineternity' has created a pull request for this issue:
https://github.com/apache/spark/pull/38702;;;","25/Nov/22 05:16;yimo_yym;Add the corresponding logs in prod env as attachment. The resubmitted task number is equals to the activeTasks in heap dump for that stage.

 

 ;;;",,,,,,,,,,,,,,,,
Fix doctest for new version mlfow,SPARK-41186,13503310,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,18/Nov/22 02:20,19/Nov/22 01:51,13/Jul/23 08:51,19/Nov/22 01:51,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Tests,,,0,,,,,,"
                                                                                
**********************************************************************
File ""/__w/spark/spark/python/pyspark/pandas/mlflow.py"", line 168, in pyspark.pandas.mlflow.load_model
Failed example:
    run_info = client.list_run_infos(exp_id)[-1]
Exception raised:
    Traceback (most recent call last):
      File ""/usr/lib/python3.9/doctest.py"", line 1336, in __run
        exec(compile(example.source, filename, ""single"",
      File ""<doctest pyspark.pandas.mlflow.load_model[14]>"", line 1, in <module>
        run_info = client.list_run_infos(exp_id)[-1]
    AttributeError: 'MlflowClient' object has no attribute 'list_run_infos'
**********************************************************************
File ""/__w/spark/spark/python/pyspark/pandas/mlflow.py"", line 169, in pyspark.pandas.mlflow.load_model
Failed example:
    model = load_model(""runs:/{run_id}/model"".format(run_id=run_info.run_uuid))
Exception raised:
    Traceback (most recent call last):
      File ""/usr/lib/python3.9/doctest.py"", line 1336, in __run
        exec(compile(example.source, filename, ""single"",
      File ""<doctest pyspark.pandas.mlflow.load_model[15]>"", line 1, in <module>
        model = load_model(""runs:/{run_id}/model"".format(run_id=run_info.run_uuid))
    NameError: name 'run_info' is not defined
**********************************************************************
File ""/__w/spark/spark/python/pyspark/pandas/mlflow.py"", line 171, in pyspark.pandas.mlflow.load_model
Failed example:
    prediction_df[""prediction""] = model.predict(prediction_df)
Exception raised:
    Traceback (most recent call last):
      File ""/usr/lib/python3.9/doctest.py"", line 1336, in __run
        exec(compile(example.source, filename, ""single"",
      File ""<doctest pyspark.pandas.mlflow.load_model[17]>"", line 1, in <module>
        prediction_df[""prediction""] = model.predict(prediction_df)
    NameError: name 'model' is not defined
**********************************************************************
File ""/__w/spark/spark/python/pyspark/pandas/mlflow.py"", line 172, in pyspark.pandas.mlflow.load_model
Failed example:
    prediction_df
Expected:
        x1   x2  prediction
    0  2.0  4.0    1.355551
Got:
        x1   x2
    0  2.0  4.0
**********************************************************************
File ""/__w/spark/spark/python/pyspark/pandas/mlflow.py"", line 178, in pyspark.pandas.mlflow.load_model
Failed example:
    model.predict(prediction_df[[""x1"", ""x2""]].to_pandas())
Exception raised:
    Traceback (most recent call last):
      File ""/usr/lib/python3.9/doctest.py"", line 1336, in __run
        exec(compile(example.source, filename, ""single"",
      File ""<doctest pyspark.pandas.mlflow.load_model[19]>"", line 1, in <module>
        model.predict(prediction_df[[""x1"", ""x2""]].to_pandas())
    NameError: name 'model' is not defined
**********************************************************************
File ""/__w/spark/spark/python/pyspark/pandas/mlflow.py"", line 189, in pyspark.pandas.mlflow.load_model
Failed example:
    y = model.predict(features)
Exception raised:
    Traceback (most recent call last):
      File ""/usr/lib/python3.9/doctest.py"", line 1336, in __run
        exec(compile(example.source, filename, ""single"",
      File ""<doctest pyspark.pandas.mlflow.load_model[22]>"", line 1, in <module>
        y = model.predict(features)
    NameError: name 'model' is not defined
**********************************************************************
File ""/__w/spark/spark/python/pyspark/pandas/mlflow.py"", line 198, in pyspark.pandas.mlflow.load_model
Failed example:
    features['y'] = y
Exception raised:
    Traceback (most recent call last):
      File ""/usr/lib/python3.9/doctest.py"", line 1336, in __run
        exec(compile(example.source, filename, ""single"",
      File ""<doctest pyspark.pandas.mlflow.load_model[25]>"", line 1, in <module>
        features['y'] = y
    NameError: name 'y' is not defined
**********************************************************************
File ""/__w/spark/spark/python/pyspark/pandas/mlflow.py"", line 200, in pyspark.pandas.mlflow.load_model
Failed example:
    everything
Expected:
        x1   x2  z         y
    0  2.0  3.0 -1  1.376932
Got:
        x1   x2  z
    0  2.0  3.0 -1
**********************************************************************
   8 of  26 in pyspark.pandas.mlflow.load_model",,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Nov 19 01:51:37 UTC 2022,,,,,,,,,,"0|z1ca2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Nov/22 03:03;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38698;;;","18/Nov/22 03:04;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38698;;;","19/Nov/22 01:51;yikunkero;Issue resolved by pull request 38698
[https://github.com/apache/spark/pull/38698];;;",,,,,,,,,,,,,,,
Fill NA tests are flaky,SPARK-41184,13503250,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,17/Nov/22 19:02,12/Dec/22 18:10,13/Jul/23 08:51,18/Nov/22 00:43,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Connect,,,0,,,,,,"Connect's fill.na tests for python are flakey. We need to disable them, and investigate what is going on with the typing.",,apachespark,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Nov 22 12:05:29 UTC 2022,,,,,,,,,,"0|z1c9pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Nov/22 19:23;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/38694;;;","17/Nov/22 19:23;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/38694;;;","18/Nov/22 00:43;gurwls223;Issue resolved by pull request 38694
[https://github.com/apache/spark/pull/38694];;;","19/Nov/22 00:35;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/38720;;;","19/Nov/22 00:36;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/38720;;;","22/Nov/22 12:03;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/38759;;;","22/Nov/22 12:04;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/38759;;;","22/Nov/22 12:05;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/38759;;;",,,,,,,,,,
fix parser rule precedence between JOIN and comma,SPARK-41178,13503192,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,17/Nov/22 15:08,18/Nov/22 03:26,13/Jul/23 08:51,18/Nov/22 03:26,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Nov 18 03:26:37 UTC 2022,,,,,,,,,,"0|z1c9cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Nov/22 15:32;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/38691;;;","18/Nov/22 03:26;cloud_fan;Issue resolved by pull request 38691
[https://github.com/apache/spark/pull/38691];;;",,,,,,,,,,,,,,,,
maven test `protobuf` module failed,SPARK-41177,13503185,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,17/Nov/22 14:40,12/Dec/22 18:10,13/Jul/23 08:51,18/Nov/22 02:38,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Protobuf,,,0,,,,,,"run `mvn clean test -pl connector/protobuf`

 
{code:java}
ProtobufSerdeSuite:
org.apache.spark.sql.protobuf.ProtobufSerdeSuite *** ABORTED ***
  java.lang.RuntimeException: Unable to load a Suite class org.apache.spark.sql.protobuf.ProtobufSerdeSuite that was discovered in the runpath: null
  at org.scalatest.tools.DiscoverySuite$.getSuiteInstance(DiscoverySuite.scala:80)
  at org.scalatest.tools.DiscoverySuite.$anonfun$nestedSuites$1(DiscoverySuite.scala:38)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  at scala.collection.TraversableLike.map(TraversableLike.scala:286)
  ...
  Cause: java.lang.NullPointerException:
  at org.apache.spark.sql.test.SQLTestUtilsBase.testFile(SQLTestUtils.scala:466)
  at org.apache.spark.sql.test.SQLTestUtilsBase.testFile$(SQLTestUtils.scala:465)
  at org.apache.spark.sql.protobuf.ProtobufSerdeSuite.testFile(ProtobufSerdeSuite.scala:34)
  at org.apache.spark.sql.protobuf.ProtobufSerdeSuite.<init>(ProtobufSerdeSuite.scala:39)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
  at java.lang.Class.newInstance(Class.java:442)
  at org.scalatest.tools.DiscoverySuite$.getSuiteInstance(DiscoverySuite.scala:66)
  ...
ProtobufFunctionsSuite:
org.apache.spark.sql.protobuf.ProtobufFunctionsSuite *** ABORTED ***
  java.lang.RuntimeException: Unable to load a Suite class org.apache.spark.sql.protobuf.ProtobufFunctionsSuite that was discovered in the runpath: null
  at org.scalatest.tools.DiscoverySuite$.getSuiteInstance(DiscoverySuite.scala:80)
  at org.scalatest.tools.DiscoverySuite.$anonfun$nestedSuites$1(DiscoverySuite.scala:38)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  at scala.collection.TraversableLike.map(TraversableLike.scala:286)
  ...
  Cause: java.lang.NullPointerException:
  at org.apache.spark.sql.test.SQLTestUtilsBase.testFile(SQLTestUtils.scala:466)
  at org.apache.spark.sql.test.SQLTestUtilsBase.testFile$(SQLTestUtils.scala:465)
  at org.apache.spark.sql.protobuf.ProtobufFunctionsSuite.testFile(ProtobufFunctionsSuite.scala:35)
  at org.apache.spark.sql.protobuf.ProtobufFunctionsSuite.<init>(ProtobufFunctionsSuite.scala:39)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
  at java.lang.Class.newInstance(Class.java:442)
  at org.scalatest.tools.DiscoverySuite$.getSuiteInstance(DiscoverySuite.scala:66)
  ...
ProtobufCatalystDataConversionSuite:
org.apache.spark.sql.protobuf.ProtobufCatalystDataConversionSuite *** ABORTED ***
  java.lang.RuntimeException: Unable to load a Suite class org.apache.spark.sql.protobuf.ProtobufCatalystDataConversionSuite that was discovered in the runpath: null
  at org.scalatest.tools.DiscoverySuite$.getSuiteInstance(DiscoverySuite.scala:80)
  at org.scalatest.tools.DiscoverySuite.$anonfun$nestedSuites$1(DiscoverySuite.scala:38)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  at scala.collection.TraversableLike.map(TraversableLike.scala:286)
  ...
  Cause: java.lang.NullPointerException:
  at org.apache.spark.sql.test.SQLTestUtilsBase.testFile(SQLTestUtils.scala:466)
  at org.apache.spark.sql.test.SQLTestUtilsBase.testFile$(SQLTestUtils.scala:465)
  at org.apache.spark.sql.protobuf.ProtobufCatalystDataConversionSuite.testFile(ProtobufCatalystDataConversionSuite.scala:34)
  at org.apache.spark.sql.protobuf.ProtobufCatalystDataConversionSuite.<init>(ProtobufCatalystDataConversionSuite.scala:39)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
  at java.lang.Class.newInstance(Class.java:442)
  at org.scalatest.tools.DiscoverySuite$.getSuiteInstance(DiscoverySuite.scala:66)
  ...
Run completed in 717 milliseconds.
Total number of tests run: 0
Suites: completed 1, aborted 3
Tests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0
*** 3 SUITES ABORTED *** {code}
 ",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Nov 18 02:38:00 UTC 2022,,,,,,,,,,"0|z1c9aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Nov/22 14:44;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/38690;;;","18/Nov/22 02:38;gurwls223;Issue resolved by pull request 38690
[https://github.com/apache/spark/pull/38690];;;",,,,,,,,,,,,,,,,
Arrow collect should factor in failures,SPARK-41165,13502990,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,17/Nov/22 02:40,12/Dec/22 18:10,13/Jul/23 08:51,18/Nov/22 00:43,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Connect,,,0,,,,,,Connect's arrow collect path does not factor in failures. If a failure occurs the collect code path will hang.,,apachespark,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Nov 22 12:03:42 UTC 2022,,,,,,,,,,"0|z1c83k:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,,,,,"17/Nov/22 04:14;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/38681;;;","17/Nov/22 04:15;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/38681;;;","18/Nov/22 00:43;gurwls223;Issue resolved by pull request 38681
[https://github.com/apache/spark/pull/38681];;;","19/Nov/22 00:34;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/38720;;;","22/Nov/22 12:03;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/38759;;;","22/Nov/22 12:03;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/38759;;;",,,,,,,,,,,,
Anti-join must not be pushed below aggregation with ambiguous predicates,SPARK-41162,13502915,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,EnricoMi,EnricoMi,EnricoMi,16/Nov/22 15:20,12/Feb/23 11:31,13/Jul/23 08:51,06/Jan/23 03:33,3.0.3,3.1.3,3.2.3,3.3.1,3.4.0,,,,,,,,,,3.2.4,3.3.2,3.4.0,,,SQL,,,0,correctness,,,,,"The following query should return a single row as all values for {{id}} except for the largest will be eliminated by the anti-join:

{code}
val ids = Seq(1, 2, 3).toDF(""id"").distinct()
val result = ids.withColumn(""id"", $""id"" + 1).join(ids, Seq(""id""), ""left_anti"").collect()
assert(result.length == 1)
{code}

Without the {{distinct()}}, the assertion is true. With {{distinct()}}, the assertion should still hold but is false.

Rule {{PushDownLeftSemiAntiJoin}} pushes the {{Join}} below the left {{Aggregate}} with join condition {{(id#750 + 1) = id#750}}, which can never be true.

{code}
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushDownLeftSemiAntiJoin ===
!Join LeftAnti, (id#752 = id#750)                  'Aggregate [id#750], [(id#750 + 1) AS id#752]
!:- Aggregate [id#750], [(id#750 + 1) AS id#752]   +- 'Join LeftAnti, ((id#750 + 1) = id#750)
!:  +- LocalRelation [id#750]                         :- LocalRelation [id#750]
!+- Aggregate [id#750], [id#750]                      +- Aggregate [id#750], [id#750]
!   +- LocalRelation [id#750]                            +- LocalRelation [id#750]
{code}

The optimizer then rightly removes the left-anti join altogether, returning the left child only.

Rule {{PushDownLeftSemiAntiJoin}} should not push down predicates that reference left *and* right child.",,apachespark,cloud_fan,EnricoMi,shardulm,shuwang,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jan 06 03:33:42 UTC 2023,,,,,,,,,,"0|z1c7n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Nov/22 15:37;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38676;;;","16/Nov/22 15:38;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38676;;;","16/Dec/22 18:27;shardulm;[~cloud_fan] Can you help take a look at this? This is a correctness issue and affects not just master but also 3.1+ if I am not wrong. We hit this issue in production with one of our user jobs.;;;","17/Dec/22 00:30;shuwang;[~shardulm] Yes. Checked with Spark 3.1.2, and it's also an issue.;;;","19/Dec/22 17:34;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39131;;;","05/Jan/23 12:35;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39409;;;","05/Jan/23 12:36;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39409;;;","05/Jan/23 15:51;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39411;;;","05/Jan/23 15:52;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39411;;;","06/Jan/23 03:33;cloud_fan;Issue resolved by pull request 39409
[https://github.com/apache/spark/pull/39409];;;",,,,,,,,
Incorrect relation caching for queries with time travel spec,SPARK-41154,13502811,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,allison-portis,allison-portis,16/Nov/22 04:27,22/Nov/22 02:08,13/Jul/23 08:51,21/Nov/22 10:22,3.3.0,3.3.1,,,,,,,,,,,,,3.3.2,3.4.0,,,,SQL,,,0,,,,,,"[https://github.com/apache/spark/pull/34497] added AS OF syntax support to support time travel queries in SQL. When resolving these [we cache the resolved relation|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L1250] with only the qualified table name as the key, ignoring the time travel spec. Thus any subsequent queries on that table are resolved using the first's time travel spec.

This affects subqueries, CTEs, and temporary views (when created with SQL).

Queries like this will be incorrectly resolved:
{code:sql}
select * from table version as of 1
union all
select * from table version as of 0
{code}
--->
{code:sql}
select * from table version as of 1
union all
select * from table version as of 1
{code}

This was originally reported here https://github.com/delta-io/delta/issues/1479",,allison-portis,apachespark,cloud_fan,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Nov 21 10:44:50 UTC 2022,,,,,,,,,,"0|z1c700:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Nov/22 11:19;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/38687;;;","17/Nov/22 11:20;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/38687;;;","21/Nov/22 10:22;cloud_fan;Issue resolved by pull request 38687
[https://github.com/apache/spark/pull/38687];;;","21/Nov/22 10:44;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/38741;;;",,,,,,,,,,,,,,
Keep built-in file _metadata column nullable value consistent,SPARK-41151,13502804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yaohua,yaohua,yaohua,16/Nov/22 03:59,15/Feb/23 09:07,13/Jul/23 08:51,22/Nov/22 00:56,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,3.3.2,3.4.0,,,,SQL,,,0,,,,,,"In FileSourceStrategy, we add an Alias node to wrap the file metadata fields (e.g. file_name, file_size) in a NamedStruct ([here|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala#L279]). But `CreateNamedStruct` has an override `nullable` value `false` ([here|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeCreator.scala#L443]), which is different from the `_metadata` struct `nullable` value `true` ([here|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala#L467]). 

 

We should keep the nullable value the same, otherwise, the downstream optimization rules might use the nullability here and cause unexpected behaviors.",,apachespark,kabhwan,yaohua,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Feb 15 09:07:44 UTC 2023,,,,,,,,,,"0|z1c6yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Nov/22 06:09;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/38683;;;","22/Nov/22 00:56;kabhwan;Issue resolved via https://github.com/apache/spark/pull/38683;;;","22/Nov/22 01:29;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/38748;;;","23/Nov/22 22:59;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/38777;;;","23/Nov/22 23:00;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/38777;;;","05/Dec/22 06:36;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/38910;;;","05/Dec/22 06:36;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/38910;;;","15/Feb/23 09:07;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/40035;;;",,,,,,,,,,
Fix `SparkSession.builder.config` to support bool,SPARK-41149,13502781,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,16/Nov/22 01:30,12/Dec/22 18:10,13/Jul/23 08:51,17/Nov/22 06:01,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,PySpark,,,0,,,,,,"Currently, `SparkSession.builder.config` only support string type ""true"" and ""false"" to set the configuration, but doesn't work correctly for `True` and `False`.

See [https://github.com/mlflow/mlflow/pull/7307/files#r1019788056] detail.",,apachespark,itholic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Nov 17 06:01:46 UTC 2022,,,,,,,,,,"0|z1c6tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Nov/22 01:30;itholic;I'm working on it;;;","16/Nov/22 11:47;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/38673;;;","16/Nov/22 11:48;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/38673;;;","17/Nov/22 06:01;gurwls223;Issue resolved by pull request 38673
[https://github.com/apache/spark/pull/38673];;;",,,,,,,,,,,,,,
UnresolvedHint should not cause query failure,SPARK-41144,13502620,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,15/Nov/22 04:39,15/Nov/22 08:56,13/Jul/23 08:51,15/Nov/22 08:56,3.4.0,,,,,,,,,,,,,,3.3.2,3.4.0,,,,SQL,,,0,,,,,," 
{code:java}
CREATE TABLE t1(c1 bigint) USING PARQUET;
CREATE TABLE t2(c2 bigint) USING PARQUET;
SELECT /*+ hash(t2) */ * FROM t1 join t2 on c1 = c2;{code}
 

 

failed with msg:
{code:java}
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to exprId on unresolved object
  at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.exprId(unresolved.scala:147)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$AddMetadataColumns$.$anonfun$hasMetadataCol$4(Analyzer.scala:1005)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$AddMetadataColumns$.$anonfun$hasMetadataCol$4$adapted(Analyzer.scala:1005)
  at scala.collection.Iterator.exists(Iterator.scala:969)
  at scala.collection.Iterator.exists$(Iterator.scala:967)
  at scala.collection.AbstractIterator.exists(Iterator.scala:1431)
  at scala.collection.IterableLike.exists(IterableLike.scala:79)
  at scala.collection.IterableLike.exists$(IterableLike.scala:78)
  at scala.collection.AbstractIterable.exists(Iterable.scala:56)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$AddMetadataColumns$.$anonfun$hasMetadataCol$3(Analyzer.scala:1005)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$AddMetadataColumns$.$anonfun$hasMetadataCol$3$adapted(Analyzer.scala:1005) {code}",,apachespark,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Nov 15 08:56:10 UTC 2022,,,,,,,,,,"0|z1c5tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Nov/22 04:52;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/38662;;;","15/Nov/22 04:53;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/38662;;;","15/Nov/22 08:56;cloud_fan;Issue resolved by pull request 38662
[https://github.com/apache/spark/pull/38662];;;",,,,,,,,,,,,,,,
Shorten graceful shutdown time of ExecutorPodsSnapshotsStoreImpl to prevent blocking shutdown process,SPARK-41136,13502096,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,14/Nov/22 08:12,04/Dec/22 15:15,13/Jul/23 08:51,03/Dec/22 07:03,3.3.1,,,,,,,,,,,,,,3.4.0,,,,,Kubernetes,,,0,,,,,,,,apachespark,chengpan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Dec 04 15:15:09 UTC 2022,,,,,,,,,,"0|z1c2lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Nov/22 08:31;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/38651;;;","03/Dec/22 07:03;dongjoon;Issue resolved by pull request 38651
[https://github.com/apache/spark/pull/38651];;;","04/Dec/22 15:14;apachespark;User 'tedyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/38902;;;","04/Dec/22 15:15;apachespark;User 'tedyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/38902;;;",,,,,,,,,,,,,,
to_number/try_to_number throws NullPointerException when format is null,SPARK-41118,13501202,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,12/Nov/22 17:16,12/Dec/22 18:10,13/Jul/23 08:51,17/Nov/22 00:56,3.3.1,3.4.0,,,,,,,,,,,,,3.3.2,3.4.0,,,,SQL,,,0,,,,,,"Example:
{noformat}
spark-sql> SELECT to_number('454', null);
[INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. Please, fill a bug report in, and provide the full stack trace.
org.apache.spark.SparkException: [INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. Please, fill a bug report in, and provide the full stack trace.
	at org.apache.spark.SparkException$.internalError(SparkException.scala:88)
	at org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:498)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
...
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.ToNumber.numberFormat$lzycompute(numberFormatExpressions.scala:72)
	at org.apache.spark.sql.catalyst.expressions.ToNumber.numberFormat(numberFormatExpressions.scala:72)
	at org.apache.spark.sql.catalyst.expressions.ToNumber.numberFormatter$lzycompute(numberFormatExpressions.scala:73)
	at org.apache.spark.sql.catalyst.expressions.ToNumber.numberFormatter(numberFormatExpressions.scala:73)
	at org.apache.spark.sql.catalyst.expressions.ToNumber.checkInputDataTypes(numberFormatExpressions.scala:81)
{noformat}
Also:
{noformat}
spark-sql> SELECT try_to_number('454', null);
[INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. Please, fill a bug report in, and provide the full stack trace.
org.apache.spark.SparkException: [INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. Please, fill a bug report in, and provide the full stack trace.
	at org.apache.spark.SparkException$.internalError(SparkException.scala:88)
	at org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:498)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
...
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.ToNumber.numberFormat$lzycompute(numberFormatExpressions.scala:72)
	at org.apache.spark.sql.catalyst.expressions.ToNumber.numberFormat(numberFormatExpressions.scala:72)
	at org.apache.spark.sql.catalyst.expressions.ToNumber.numberFormatter$lzycompute(numberFormatExpressions.scala:73)
	at org.apache.spark.sql.catalyst.expressions.ToNumber.numberFormatter(numberFormatExpressions.scala:73)
	at org.apache.spark.sql.catalyst.expressions.ToNumber.checkInputDataTypes(numberFormatExpressions.scala:81)
	at org.apache.spark.sql.catalyst.expressions.TryToNumber.checkInputDataTypes(numberFormatExpressions.scala:146)
{noformat}
Compare to {{to_binary}} and {{try_to_binary}}:
{noformat}
spark-sql> SELECT to_binary('abc', null);
NULL
Time taken: 3.111 seconds, Fetched 1 row(s)
spark-sql> SELECT try_to_binary('abc', null);
NULL
Time taken: 0.06 seconds, Fetched 1 row(s)
spark-sql>
{noformat}
",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Nov 18 02:30:15 UTC 2022,,,,,,,,,,"0|z1bx2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Nov/22 19:34;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/38635;;;","17/Nov/22 00:56;gurwls223;Fixed in https://github.com/apache/spark/pull/38635;;;","18/Nov/22 02:29;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/38697;;;","18/Nov/22 02:30;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/38697;;;",,,,,,,,,,,,,,
Remove netty-tcnative-classes from Spark dependencyList,SPARK-41093,13500829,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chengpan,chengpan,chengpan,10/Nov/22 05:49,11/Nov/22 14:15,13/Jul/23 08:51,11/Nov/22 14:15,3.3.1,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,,,apachespark,chengpan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Nov 11 14:15:29 UTC 2022,,,,,,,,,,"0|z1bus8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Nov/22 05:56;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/38596;;;","11/Nov/22 14:15;srowen;Issue resolved by pull request 38596
[https://github.com/apache/spark/pull/38596];;;",,,,,,,,,,,,,,,,
Fix Docker release tool for branch-3.2,SPARK-41091,13500825,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,csun,csun,10/Nov/22 04:55,14/Nov/22 06:28,13/Jul/23 08:51,14/Nov/22 06:28,3.2.2,,,,,,,,,,,,,,3.2.3,,,,,Build,,,0,,,,,,"The {{do-release-docker.sh}} doesn't work in branch-3.2 now, and gives the following error:
{code:java}
#5 917.4 g++ -std=gnu++14 -shared -L/usr/lib/R/lib -Wl,-Bsymbolic-functions -Wl,-z,relro -o testthat.so init.o reassign.o test-catch.o test-example.o test-runner.o -L/usr/lib/R/lib -lR
#5 917.5 installing to /usr/local/lib/R/site-library/00LOCK-testthat/00new/testthat/libs
#5 917.5 ** R
#5 917.5 ** inst
#5 917.5 ** byte-compile and prepare package for lazy loading
#5 924.4 ** help
#5 924.6 *** installing help indices
#5 924.7 *** copying figures
#5 924.7 ** building package indices
#5 924.9 ** installing vignettes
#5 924.9 ** testing if installed package can be loaded from temporary location
#5 925.1 ** checking absolute paths in shared objects and dynamic libraries
#5 925.1 ** testing if installed package can be loaded from final location
#5 925.5 ** testing if installed package keeps a record of temporary installation path
#5 925.5 * DONE (testthat)
#5 925.8 ERROR: dependency 'pkgdown' is not available for package 'devtools'
#5 925.8 * removing '/usr/local/lib/R/site-library/devtools'
#5 925.8
#5 925.8 The downloaded source packages are in
#5 925.8        '/tmp/Rtmp3nJI60/downloaded_packages'
#5 925.8 Warning messages:
#5 925.8 1: In install.packages(c(""curl"", ""xml2"", ""httr"", ""devtools"", ""testthat"",  :
#5 925.8   installation of package 'textshaping' had non-zero exit status
#5 925.8 2: In install.packages(c(""curl"", ""xml2"", ""httr"", ""devtools"", ""testthat"",  :
#5 925.8   installation of package 'ragg' had non-zero exit status
#5 925.8 3: In install.packages(c(""curl"", ""xml2"", ""httr"", ""devtools"", ""testthat"",  :
#5 925.8   installation of package 'pkgdown' had non-zero exit status
#5 925.8 4: In install.packages(c(""curl"", ""xml2"", ""httr"", ""devtools"", ""testthat"",  :
#5 925.8   installation of package 'devtools' had non-zero exit status
#5 926.0 Error in loadNamespace(x) : there is no package called 'devtools'
#5 926.0 Calls: loadNamespace -> withRestarts -> withOneRestart -> doWithOneRestart
#5 926.0 Execution halted {code}",,apachespark,csun,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Nov 14 06:28:38 UTC 2022,,,,,,,,,,"0|z1burc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Nov/22 05:17;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/38643;;;","14/Nov/22 05:18;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/38643;;;","14/Nov/22 06:28;dongjoon;This is resolved via https://github.com/apache/spark/pull/38643;;;",,,,,,,,,,,,,,,
Fix new R_LIBS_SITE behavior introduced in R 4.2,SPARK-41056,13500268,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,09/Nov/22 02:38,12/Dec/22 18:10,13/Jul/23 08:51,09/Nov/22 07:41,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SparkR,,,0,,,,,,"R 4.2

{code}
# R
> Sys.getenv(""R_LIBS_SITE"")
[1] ""/usr/local/lib/R/site-library/:/usr/lib/R/site-library:/usr/lib/R/library'""
{code}

{code}
# R --vanilla
> Sys.getenv(""R_LIBS_SITE"")
[1] ""/usr/lib/R/site-library""
{code}

R 4.1

{code}
# R
> Sys.getenv(""R_LIBS_SITE"")
[1] ""/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library""
{code}

{code}
# R --vanilla
> Sys.getenv(""R_LIBS_SITE"")
[1] ""/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library""
{code}
",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Nov 09 07:41:51 UTC 2022,,,,,,,,,,"0|z1brbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Nov/22 02:50;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/38570;;;","09/Nov/22 07:41;gurwls223;Issue resolved by pull request 38570
[https://github.com/apache/spark/pull/38570];;;",,,,,,,,,,,,,,,,
Nondeterministic expressions have unstable values if they are children of CodegenFallback expressions,SPARK-41049,13499656,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,gboo,gboo,08/Nov/22 11:25,11/Jan/23 16:34,13/Jul/23 08:51,31/Dec/22 02:05,3.1.2,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,correctness,,,,,"h2. Expectation

For a given row, Nondeterministic expressions are expected to have stable values.
{code:scala}
import org.apache.spark.sql.functions._
val df = sparkContext.parallelize(1 to 5).toDF(""x"")
val v1 = rand().*(lit(10000)).cast(IntegerType)
df.select(v1, v1).collect{code}
Returns a set like this:
|8777|8777|
|1357|1357|
|3435|3435|
|9204|9204|
|3870|3870|

where both columns always have the same value, but what that value is changes from row to row. This is different from the following:
{code:scala}
df.select(rand(), rand()).collect{code}
In this case, because the rand() calls are distinct, the values in both columns should be different.
h2. Problem

This expectation does not appear to be stable in the event that any subsequent expression is a CodegenFallback. This program:
{code:scala}
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

val sparkSession = SparkSession.builder().getOrCreate()
val df = sparkSession.sparkContext.parallelize(1 to 5).toDF(""x"")
val v1 = rand().*(lit(10000)).cast(IntegerType)
val v2 = to_csv(struct(v1.as(""a""))) // to_csv is CodegenFallback
df.select(v1, v1, v2, v2).collect {code}
produces output like this:
|8159|8159|8159|{color:#ff0000}2028{color}|
|8320|8320|8320|{color:#ff0000}1640{color}|
|7937|7937|7937|{color:#ff0000}769{color}|
|436|436|436|{color:#ff0000}8924{color}|
|8924|8924|2827|{color:#ff0000}2731{color}|

Not sure why the first call via the CodegenFallback path should be correct while subsequent calls aren't.
h2. Workaround

If the Nondeterministic expression is moved to a separate, earlier select() call, so the CodegenFallback instead only refers to a column reference, then the problem seems to go away. But this workaround may not be reliable if optimization is ever able to restructure adjacent select()s.",,apachespark,cloud_fan,gboo,viirya,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jan 11 16:34:04 UTC 2023,,,,,,,,,,"0|z1bnjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Dec/22 20:51;apachespark;User 'NarekDW' has created a pull request for this issue:
https://github.com/apache/spark/pull/39097;;;","28/Dec/22 07:04;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39248;;;","31/Dec/22 02:05;cloud_fan;Issue resolved by pull request 39248
[https://github.com/apache/spark/pull/39248];;;","03/Jan/23 12:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39364;;;","03/Jan/23 18:23;xkrogen;[~cloud_fan]  [~gboo]  I think this should be tagged as a 'correctness' issue, do you agree?;;;","06/Jan/23 12:44;gboo;[~xkrogen] I've added the ""correctness"" label.;;;","06/Jan/23 16:52;xkrogen;Thanks! [~cloud_fan]  [~viirya]  shall we backport this to branch-3.3 and branch-3.2, given it is a correctness bug?;;;","06/Jan/23 18:02;viirya;For a correctness bug, I think we should backport it, though the patch is a kind of refactoring work.;;;","11/Jan/23 16:34;xkrogen;Thanks for the input [~viirya]! [~cloud_fan], will you work on putting together a PR for the other branches? I would guess that it's not a trivial cherry-pick given the size of the change.;;;",,,,,,,,,
Remove legacy example of round function with negative scale ,SPARK-41047,13499643,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,panbingkun,taiyang-li,taiyang-li,08/Nov/22 09:58,11/Jan/23 22:40,13/Jul/23 08:51,11/Jan/23 22:39,3.2.2,,,,,,,,,,,,,,3.4.0,,,,,Documentation,SQL,,0,,,,,,"Run this sql in spark-sql:    select round(1.233, -1); 
Error: org.apache.spark.sql.AnalysisException: Negative scale is not allowed: -1. You can use spark.sql.legacy.allowNegativeScaleOfDecimal=true to enable legacy mode to allow it.; 

But the document of round function implies that negative scale argument is allowed. 




",,apachespark,panbingkun,taiyang-li,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 11:51;panbingkun;image-2023-01-11-19-51-17-312.png;https://issues.apache.org/jira/secure/attachment/13054520/image-2023-01-11-19-51-17-312.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jan 11 22:39:45 UTC 2023,,,,,,,,,,"0|z1bngw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Nov/22 00:52;gurwls223;cc [~wuyi] [~cloud_fan]. This seems from https://github.com/apache/spark/commit/ff39c9271ca04951b045c5d9fca2128a82d50b46 https://issues.apache.org/jira/browse/SPARK-30252;;;","11/Jan/23 11:51;panbingkun;Maybe we should delete follow document

!image-2023-01-11-19-51-17-312.png!;;;","11/Jan/23 12:10;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39510;;;","11/Jan/23 12:15;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39511;;;","11/Jan/23 12:15;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39511;;;","11/Jan/23 22:39;srowen;Issue resolved by pull request 39511
[https://github.com/apache/spark/pull/39511];;;",,,,,,,,,,,,
Self-union streaming query may fail when using readStream.table,SPARK-41040,13499543,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,08/Nov/22 05:40,08/Nov/22 16:33,13/Jul/23 08:51,08/Nov/22 16:32,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Structured Streaming,,,0,,,,,,"In a self-union query, the batch plan created by the source will be shared by multiple nodes in the plan. When we transform the plan, the batch plan will be visited multiple times. Hence, the first visit will set the CatalogTable and the second visit will try to set it again and fail the query at [this line|https://github.com/apache/spark/blob/406d0e243cfec9b29df946e1a0e20ed5fe25e152/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala#L625].",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-39564,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Nov 08 16:32:59 UTC 2022,,,,,,,,,,"0|z1bmuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Nov/22 06:16;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/38553;;;","08/Nov/22 06:16;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/38553;;;","08/Nov/22 16:32;zsxwing;Issue resolved by pull request https://github.com/apache/spark/pull/38553;;;",,,,,,,,,,,,,,,
Incorrect results or NPE when a literal is reused across distinct aggregations,SPARK-41035,13499130,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,08/Nov/22 01:56,12/Dec/22 18:10,13/Jul/23 08:51,09/Nov/22 01:43,3.2.2,3.3.1,3.4.0,,,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,SQL,,,0,correctness,,,,,"This query produces incorrect results:
{noformat}
select a, count(distinct 100) as cnt1, count(distinct b, 100) as cnt2
from values (1, 2), (4, 5) as data(a, b)
group by a;

+---+----+----+
|a  |cnt1|cnt2|
+---+----+----+
|1  |1   |0   |
|4  |1   |0   |
+---+----+----+
{noformat}
The values for {{cnt2}} should be 1 and 1 (not 0 and 0).

If you change the literal used in the first aggregate function, the second aggregate function now works correctly:
{noformat}
select a, count(distinct 101) as cnt1, count(distinct b, 100) as cnt2
from values (1, 2), (4, 5) as data(a, b)
group by a;

+---+----+----+
|a  |cnt1|cnt2|
+---+----+----+
|1  |1   |1   |
|4  |1   |1   |
+---+----+----+
{noformat}
The same bug causes the following query to get a NullPointerException:
{noformat}
select a, count(distinct 1), count_min_sketch(distinct b, 0.5d, 0.5d, 1)
from values (1, 2), (4, 5) as data(a, b)
group by a;
{noformat}
If you change the literal used in the first aggregation, then the query succeeds:
{noformat}
select a, count(distinct 2), count_min_sketch(distinct b, 0.5d, 0.5d, 1)
from values (1, 2), (4, 5) as data(a, b)
group by a;

+---+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|a  |count(DISTINCT 2)|count_min_sketch(DISTINCT b, 0.5, 0.5, 1)                                                                                                                                            |
+---+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|1  |1                |[00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 01 00 00 00 04 00 00 00 00 5D 8D 6A B9 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 00]|
|4  |1                |[00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 01 00 00 00 04 00 00 00 00 5D 8D 6A B9 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 00]|
+---+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
{noformat}
",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Nov 09 01:43:20 UTC 2022,,,,,,,,,,"0|z1bkaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Nov/22 01:57;bersprockets;This is a bug in {{RewriteDistinctAggregates}}. I will take a stab at a fix in the coming days.;;;","08/Nov/22 20:25;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/38565;;;","09/Nov/22 01:43;gurwls223;Issue resolved by pull request 38565
[https://github.com/apache/spark/pull/38565];;;",,,,,,,,,,,,,,,
Failure of ProtobufCatalystDataConversionSuite.scala,SPARK-41015,13495534,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rangadi,maxgekk,maxgekk,04/Nov/22 12:08,08/Nov/22 17:49,13/Jul/23 08:51,08/Nov/22 10:19,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"To reproduce the issue, set the seed to 38:

{code}
diff --git a/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufCatalystDataConversionSuite.scala b/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufCatalystDataConversionSuite.scala
index 271c5b0fec..080bf1eb1f 100644
--- a/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufCatalystDataConversionSuite.scala
+++ b/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufCatalystDataConversionSuite.scala
@@ -123,7 +123,7 @@ class ProtobufCatalystDataConversionSuite
     StringType -> (""StringMsg"", """"))

   testingTypes.foreach { dt =>
-    val seed = 1 + scala.util.Random.nextInt((1024 - 1) + 1)
+    val seed = 38
     test(s""single $dt with seed $seed"") {

       val (messageName, defaultValue) = catalystTypesToProtoMessages(dt.fields(0).dataType)
{code}

and run the test:

{code}
build/sbt ""test:testOnly *ProtobufCatalystDataConversionSuite""
{code}
which fails with NPE:
{code}
[info] - single StructType(StructField(double_type,DoubleType,true)) with seed 38 *** FAILED *** (10 milliseconds)
[info]   java.lang.NullPointerException:
[info]   at org.apache.spark.sql.protobuf.ProtobufCatalystDataConversionSuite.$anonfun$new$2(ProtobufCatalystDataConversionSuite.scala:134)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
{code}

",,apachespark,maxgekk,rangadi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-40653,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Nov 08 10:19:07 UTC 2022,,,,,,,,,,"0|z1ay48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Nov/22 21:51;rangadi;Thanks for filing this [~maxgekk] .

cc: [~sanysandish@gmail.com] ;;;","05/Nov/22 00:13;apachespark;User 'SandishKumarHN' has created a pull request for this issue:
https://github.com/apache/spark/pull/38515;;;","08/Nov/22 10:19;maxgekk;Issue resolved by pull request 38515
[https://github.com/apache/spark/pull/38515];;;",,,,,,,,,,,,,,,
Isotonic regression result differs from sklearn implementation,SPARK-41008,13494712,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ahmed.mahran,arne.koopman,arne.koopman,03/Nov/22 15:51,09/Dec/22 07:57,13/Jul/23 08:51,08/Dec/22 14:29,3.3.1,,,,,,,,,,,,,,3.4.0,,,,,MLlib,,,1,,,,,," 
{code:python}
import pandas as pd
from pyspark.sql.types import DoubleType
from sklearn.isotonic import IsotonicRegression as IsotonicRegression_sklearn
from pyspark.ml.regression import IsotonicRegression as IsotonicRegression_pyspark

# The P(positives | model_score):
# 0.6 -> 0.5 (1 out of the 2 labels is positive)
# 0.333 -> 0.333 (1 out of the 3 labels is positive)
# 0.20 -> 0.25 (1 out of the 4 labels is positive)
tc_pd = pd.DataFrame({
    ""model_score"": [0.6, 0.6, 0.333, 0.333, 0.333, 0.20, 0.20, 0.20, 0.20],         
    ""label"": [1, 0, 0, 1, 0, 1, 0, 0, 0],         
    ""weight"": 1,     }
)

# The fraction of positives for each of the distinct model_scores would be the best fit.
# Resulting in the following expected calibrated model_scores:
# ""calibrated_model_score"": [0.5, 0.5, 0.333, 0.333, 0.333, 0.25, 0.25, 0.25, 0.25]

# The sklearn implementation of Isotonic Regression. 
from sklearn.isotonic import IsotonicRegression as IsotonicRegression_sklearn
tc_regressor_sklearn = IsotonicRegression_sklearn().fit(X=tc_pd['model_score'], y=tc_pd['label'], sample_weight=tc_pd['weight'])
print(""sklearn:"", tc_regressor_sklearn.predict(tc_pd['model_score']))
# >> sklearn: [0.5 0.5 0.33333333 0.33333333 0.33333333 0.25 0.25 0.25 0.25 ]

# The pyspark implementation of Isotonic Regression. 
tc_df = spark.createDataFrame(tc_pd)
tc_df = tc_df.withColumn('model_score', F.col('model_score').cast(DoubleType()))

isotonic_regressor_pyspark = IsotonicRegression_pyspark(featuresCol='model_score', labelCol='label', weightCol='weight')
tc_model = isotonic_regressor_pyspark.fit(tc_df)
tc_pd = tc_model.transform(tc_df).toPandas()
print(""pyspark:"", tc_pd['prediction'].values)
# >> pyspark: [0.5 0.5 0.33333333 0.33333333 0.33333333 0. 0. 0. 0. ]

# The result from the pyspark implementation seems unclear. Similar small toy examples lead to similar non-expected results for the pyspark implementation. 

# Strangely enough, for 'large' datasets, the difference between calibrated model_scores generated by both implementations dissapears. 
{code}
 ",,ahmed.mahran,apachespark,arne.koopman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Dec 09 07:57:31 UTC 2022,,,,,,,,,,"0|z1at1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Nov/22 02:54;srowen;Yeah that doesn't look right. I tried understanding the algorithm and code for an hour and couldn't quite figure it out. It's clearly because it wants to group the three 0 values at 0.2 together, and not the (0.2, 1) point. My guess is there needs to be some logic about forcing pooling of adjacent points with the same x values even when the y value changes, but I couldn't find a straightforward fix;;;","05/Dec/22 10:42;ahmed.mahran;[~srowen] I think you are right. Repeated feature/x values are pooled into a single point such that the label/y value is the weighted average of corresponding label/y values.

 

I've checked sklearn implementation: [https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/isotonic.py#L281] and [https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b611bf873bd5836748647221480071a87/sklearn/_isotonic.pyx#L66. |https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b611bf873bd5836748647221480071a87/sklearn/_isotonic.pyx#L66.]Then I did a draft scala version of the pooling function and it seems to give the same results for different few examples.

 

I'd like to pick this up if possible. Also, should the new pooling be applied always or should there be a new option?;;;","05/Dec/22 14:36;srowen;No need for an option, this seems like a bug fix. Yes if you can propose a pull request that fixes it, by all means.;;;","05/Dec/22 14:39;ahmed.mahran;Thanks, I'll manage to have a PR in a couple of days.;;;","07/Dec/22 12:37;apachespark;User 'ahmed-mahran' has created a pull request for this issue:
https://github.com/apache/spark/pull/38966;;;","07/Dec/22 12:38;apachespark;User 'ahmed-mahran' has created a pull request for this issue:
https://github.com/apache/spark/pull/38966;;;","08/Dec/22 14:29;srowen;Issue resolved by pull request 38966
[https://github.com/apache/spark/pull/38966];;;","09/Dec/22 07:56;apachespark;User 'ahmed-mahran' has created a pull request for this issue:
https://github.com/apache/spark/pull/38996;;;","09/Dec/22 07:57;apachespark;User 'ahmed-mahran' has created a pull request for this issue:
https://github.com/apache/spark/pull/38996;;;",,,,,,,,,
BigInteger Serialization doesn't work with JavaBean Encoder,SPARK-41007,13494625,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dfit99,dfit99,dfit99,03/Nov/22 15:07,08/Nov/22 01:34,13/Jul/23 08:51,08/Nov/22 01:34,3.3.1,,,,,,,,,,,,,,3.4.0,,,,,Java API,,,0,,,,,,"When creating a dataset using the [Java Bean Encoder|https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/sql/Encoders.html#bean-java.lang.Class-] with a bean that contains a field which is a {{java.math.BigInteger}} the dataset will fail to serialize correctly. When trying to serialize the dataset, Spark throws the following error:

 
{code:java}
Exception in thread ""main"" org.apache.spark.sql.AnalysisException: Cannot up cast `bigInteger` from struct<> to decimal(38,18).
 {code}
 

Reproduction steps:

Using the Java Dataset API:
 # Create a Bean with a  {{java.math.BigInteger}} field
 # Pass said Bean into the Java SparkSession {{createDataset}} function

 ",,apachespark,dfit99,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Nov 08 01:34:05 UTC 2022,,,,,,,,,,"0|z1asi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Nov/22 15:08;dfit99;FYI: Have a fix for this already, going to push out a merge request soon. ;;;","03/Nov/22 15:32;apachespark;User 'dfit99' has created a pull request for this issue:
https://github.com/apache/spark/pull/38500;;;","08/Nov/22 01:34;srowen;Issue resolved by pull request 38500
[https://github.com/apache/spark/pull/38500];;;",,,,,,,,,,,,,,,
BHJ LeftAnti does not update numOutputRows when codegen is disabled,SPARK-41003,13494493,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dzcxzl,dzcxzl,dzcxzl,03/Nov/22 04:49,28/Nov/22 08:45,13/Jul/23 08:51,28/Nov/22 08:45,3.1.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,,,apachespark,cloud_fan,dzcxzl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Nov 28 08:45:11 UTC 2022,,,,,,,,,,"0|z1arow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Nov/22 04:54;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/38489;;;","28/Nov/22 08:45;cloud_fan;Issue resolved by pull request 38489
[https://github.com/apache/spark/pull/38489];;;",,,,,,,,,,,,,,,,
Hints on subqueries are not properly propagated,SPARK-40999,13494356,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fred-db,fred-db,fred-db,02/Nov/22 15:52,18/Nov/22 13:51,13/Jul/23 08:51,18/Nov/22 13:49,3.0.0,3.0.1,3.0.2,3.0.3,3.1.0,3.1.1,3.1.2,3.1.3,3.2.0,3.2.1,3.2.2,3.3.0,3.3.1,3.4.0,3.4.0,,,,,Optimizer,Spark Core,,0,,,,,,"Currently, if a user tries to specify a query like the following, the hints on the subquery will be lost. 
{code:java}
SELECT * FROM target t WHERE EXISTS
(SELECT /*+ BROADCAST */ * FROM source s WHERE s.key = t.key){code}
This happens as hints are removed from the plan and pulled into joins in the beginning of the optimization stage, but subqueries are only turned into joins during optimization. As we remove any hints that are not below a join, we end up removing hints that are below a subquery. 

 

It worked prior to a refactoring that added hints as a field to joins (SPARK-26065) and can cause a regression if someone made use of hints on subqueries before.

 

To resolve this, we add a hint field to SubqueryExpression that any hints inside a subquery's plan can be pulled into during EliminateResolvedHint, and then pass this hint on when the subquery is turned into a join.",,apachespark,cloud_fan,fred-db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,English,,,Fri Nov 18 13:49:48 UTC 2022,,,,,,,,,,"0|z1aqug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Nov/22 10:35;apachespark;User 'fred-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/38497;;;","03/Nov/22 10:35;apachespark;User 'fred-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/38497;;;","18/Nov/22 13:49;cloud_fan;Issue resolved by pull request 38497
[https://github.com/apache/spark/pull/38497];;;",,,,,,,,,,,,,,,
"Avoid creating a directory when deleting a block, causing DAGScheduler to not work",SPARK-40987,13493997,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dzcxzl,dzcxzl,dzcxzl,01/Nov/22 11:16,30/Nov/22 03:55,13/Jul/23 08:51,30/Nov/22 03:55,3.2.2,3.3.1,,,,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,Spark Core,,,0,,,,,,"When the driver submits a job, DAGScheduler calls sc.broadcast(taskBinaryBytes).

TorrentBroadcast#writeBlocks may fail due to disk problems during blockManager#putBytes.

BlockManager#doPut calls BlockManager#removeBlockInternal to clean up the block.

BlockManager#removeBlockInternal calls DiskStore#remove to clean up blocks on disk.

DiskStore#remove will try to create the directory because the directory does not exist, and an exception will be thrown at this time.

BlockInfoManager#blockInfoWrappers block info and lock not removed.

The catch block in TorrentBroadcast#writeBlocks will call blockManager.removeBroadcast to clean up the broadcast.
Because the block lock in BlockInfoManager#blockInfoWrappers is not released, the dag-scheduler-event-loop thread of DAGScheduler will wait forever.

 

 
{code:java}
22/11/01 18:27:48 WARN BlockManager: Putting block broadcast_0_piece0 failed due to exception java.io.IOException: XXXXX.
22/11/01 18:27:48 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast {code}
 

 

 
{code:java}
""dag-scheduler-event-loop"" #54 daemon prio=5 os_prio=31 tid=0x00007fc98e3fa800 nid=0x7203 waiting on condition [0x0000700008c1e000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000007add3d8c8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at org.apache.spark.storage.BlockInfoManager.$anonfun$acquireLock$1(BlockInfoManager.scala:221)
    at org.apache.spark.storage.BlockInfoManager.$anonfun$acquireLock$1$adapted(BlockInfoManager.scala:214)
    at org.apache.spark.storage.BlockInfoManager$$Lambda$3038/1307533457.apply(Unknown Source)
    at org.apache.spark.storage.BlockInfoWrapper.withLock(BlockInfoManager.scala:105)
    at org.apache.spark.storage.BlockInfoManager.acquireLock(BlockInfoManager.scala:214)
    at org.apache.spark.storage.BlockInfoManager.lockForWriting(BlockInfoManager.scala:293)
    at org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1979)
    at org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3(BlockManager.scala:1970)
    at org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3$adapted(BlockManager.scala:1970)
    at org.apache.spark.storage.BlockManager$$Lambda$3092/1241801156.apply(Unknown Source)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:1970)
    at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:179)
    at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)
    at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)
    at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)
    at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1538)
    at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1520)
    at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1539)
    at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1355)
    at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1297)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2929)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2921)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2910)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) {code}
 

 ",,apachespark,dzcxzl,mridulm80,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Nov 30 03:55:36 UTC 2022,,,,,,,,,,"0|z1aon4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Nov/22 11:26;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/38467;;;","01/Nov/22 11:27;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/38467;;;","30/Nov/22 03:55;mridulm80;Issue resolved by pull request 38467
[https://github.com/apache/spark/pull/38467];;;",,,,,,,,,,,,,,,
Unable to download spark 3.3.0 tarball after 3.3.1 release in spark-docker,SPARK-40969,13493648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dcoliversun,dcoliversun,dcoliversun,31/Oct/22 03:06,01/Nov/22 03:24,13/Jul/23 08:51,01/Nov/22 03:14,3.3.1,,,,,,,,,,,,,,3.4.0,,,,,Spark Docker,,,0,,,,,,"Unable to download spark 3.3.0 tarball in spark-docker. 


{code:sh}
#7 0.229 + wget -nv -O spark.tgz https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz
#7 1.061 https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz:
#7 1.061 2022-10-31 02:59:20 ERROR 404: Not Found.
------
executor failed running [/bin/sh -c set -ex;     export SPARK_TMP=""$(mktemp -d)"";     cd $SPARK_TMP;     wget -nv -O spark.tgz ""$SPARK_TGZ_URL"";     wget -nv -O spark.tgz.asc ""$SPARK_TGZ_ASC_URL"";     export GNUPGHOME=""$(mktemp -d)"";     gpg --keyserver hkps://keys.openpgp.org --recv-key ""$GPG_KEY"" ||     gpg --keyserver hkps://keyserver.ubuntu.com --recv-keys ""$GPG_KEY"";     gpg --batch --verify spark.tgz.asc spark.tgz;     gpgconf --kill all;     rm -rf ""$GNUPGHOME"" spark.tgz.asc;         tar -xf spark.tgz --strip-components=1;     chown -R spark:spark .;     mv jars /opt/spark/;     mv bin /opt/spark/;     mv sbin /opt/spark/;     mv kubernetes/dockerfiles/spark/decom.sh /opt/;     mv examples /opt/spark/;     mv kubernetes/tests /opt/spark/;     mv data /opt/spark/;     mv python/pyspark /opt/spark/python/pyspark/;     mv python/lib /opt/spark/python/lib/;     cd ..;     rm -rf ""$SPARK_TMP"";]: exit code: 8
{code}
And spark 3.3.1 docker is ok

{code:sh}
=> [4/9] RUN set -ex;     export SPARK_TMP=""$(mktemp -d)"";     cd $SPARK_TMP;     wget -nv -O spark.tgz ""https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"";     wget -nv -O spark.tgz.asc ""https://downlo  77.8s
 => [5/9] COPY entrypoint.sh /opt/
{code}
",,dcoliversun,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Nov 01 03:14:19 UTC 2022,,,,,,,,,,"0|z1amig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Oct/22 03:08;dcoliversun;cc [~yikunkero][~hyukjin.kwon];;;","31/Oct/22 03:24;yikunkero;Let's use the https://archive.apache.org/dist/spark/ , mind to submit a PR? Thanks;;;","31/Oct/22 03:29;dcoliversun;[~yikunkero] fine with me, I'm working on this :);;;","01/Nov/22 03:14;yikunkero;Issue resolved by pull request 22
[https://github.com/apache/spark-docker/pull/22];;;",,,,,,,,,,,,,,
ExtractGenerator sets incorrect nullability in new Project,SPARK-40963,13493550,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,28/Oct/22 23:13,12/Dec/22 18:11,13/Jul/23 08:51,31/Oct/22 01:50,3.1.3,3.2.2,3.3.1,3.4.0,,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,SQL,,,0,correctness,,,,,"Example:
{noformat}
select c1, explode(c4) as c5 from (
  select c1, array(c3) as c4 from (
    select c1, explode_outer(c2) as c3
    from values
    (1, array(1, 2)),
    (2, array(2, 3)),
    (3, null)
    as data(c1, c2)
  )
);

+---+---+
|c1 |c5 |
+---+---+
|1  |1  |
|1  |2  |
|2  |2  |
|2  |3  |
|3  |0  |
+---+---+
{noformat}
In the last row, {{c5}} is 0, but should be {{NULL}}.

Another example:
{noformat}
select c1, exists(c4, x -> x is null) as c5 from (
  select c1, array(c3) as c4 from (
    select c1, explode_outer(c2) as c3
    from values
    (1, array(1, 2)),
    (2, array(2, 3)),
    (3, null)
    as data(c1, c2)
  )
);

+---+-----+
|c1 |c5   |
+---+-----+
|1  |false|
|1  |false|
|2  |false|
|2  |false|
|3  |false|
+---+-----+
{noformat}
In the last row, {{false}} should be {{true}}.

In both cases, at the time {{CreateArray(c3)}} is instantiated, {{c3}}'s nullability is incorrect because the new projection created by {{ExtractGenerator}} uses {{generatorOutput}} from {{explode_outer(c2)}} as a projection list. {{generatorOutput}} doesn't take into account that {{explode_outer(c2)}} is an _outer_ explode, so the nullability setting is lost.

{{UpdateAttributeNullability}} will eventually fix the nullable setting for attributes referring to {{c3}}, but it doesn't fix the {{containsNull}} setting for {{c4}} in {{explode(c4)}} (from the first example) or {{exists(c4, x -> x is null)}} (from the second example).

This example fails with a {{NullPointerException}}:
{noformat}
select c1, inline_outer(c4) from (
  select c1, array(c3) as c4 from (
    select c1, explode_outer(c2) as c3
    from values
    (1, array(named_struct('a', 1, 'b', 2))),
    (2, array(named_struct('a', 3, 'b', 4), named_struct('a', 5, 'b', 6))),
    (3, null)
    as data(c1, c2)
  )
);

22/10/27 11:53:20 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)
{noformat}
",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 31 01:50:00 UTC 2022,,,,,,,,,,"0|z1alww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Oct/22 23:14;bersprockets;I'll take a stab at fixing this in the next few days.;;;","31/Oct/22 00:14;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/38440;;;","31/Oct/22 00:14;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/38440;;;","31/Oct/22 01:50;gurwls223;Fixed in https://github.com/apache/spark/pull/38440;;;",,,,,,,,,,,,,,
Relax ordering constraint for CREATE TABLE column options,SPARK-40944,13492605,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dtenedor,dtenedor,dtenedor,27/Oct/22 22:32,31/Oct/22 23:57,13/Jul/23 08:51,31/Oct/22 23:57,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"Currently the grammar for each CREATE TABLE column is:

createOrReplaceTableColType
    : colName=errorCapturingIdentifier dataType (NOT NULL)? defaultExpression? commentSpec?
    ;

This enforces a constraint on the order of: (NOT NULL, DEFAULT value, COMMENT value). We can update the grammar to allow these options in any order instead, to improve usability.",,apachespark,dtenedor,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 31 23:57:42 UTC 2022,,,,,,,,,,"0|z1ag2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Oct/22 23:03;apachespark;User 'dtenedor' has created a pull request for this issue:
https://github.com/apache/spark/pull/38418;;;","31/Oct/22 23:57;Gengliang.Wang;Issue resolved by pull request 38418
[https://github.com/apache/spark/pull/38418];;;",,,,,,,,,,,,,,,,
Barrier: messages for allGather will be overridden by the following barrier APIs,SPARK-40932,13492470,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,wbo4958,wbo4958,wbo4958,27/Oct/22 09:24,28/Oct/22 13:07,13/Jul/23 08:51,28/Oct/22 13:07,3.3.0,3.3.1,,,,,,,,,,,,,3.3.2,3.4.0,,,,Spark Core,,,0,,,,,,"When I was working on an internal project which has not been opened source. I found this bug that the messages for Barrier.allGather may be overridden by the following Barrier APIs, which means the user can't get the correct allGather message.

 

This issue can easily repro by the following unit tests.

 

 
{code:java}
test(""SPARK-XXX, messages of allGather should not been overridden "" +
  ""by the following barrier APIs"") {

  sc = new SparkContext(new SparkConf().setAppName(""test"").setMaster(""local[2]""))
  sc.setLogLevel(""INFO"")
  val rdd = sc.makeRDD(1 to 10, 2)
  val rdd2 = rdd.barrier().mapPartitions { it =>
    val context = BarrierTaskContext.get()
    // Sleep for a random time before global sync.
    Thread.sleep(Random.nextInt(1000))
    // Pass partitionId message in
    val message: String = context.partitionId().toString
    val messages: Array[String] = context.allGather(message)
    context.barrier()
    Iterator.single(messages.toList)
  }
  val messages = rdd2.collect()
  // All the task partitionIds are shared across all tasks
  assert(messages.length === 2)
  messages.foreach(m => println(""------- "" + m))
  assert(messages.forall(_ == List(""0"", ""1"")))
} {code}
 

 

before throwing the exception by (assert(messages.forall(_ == List(""0"", ""1""))), the print log is 

 
{code:java}
------- List(, )
------- List(, ) {code}
 

 

You can see, the messages are empty which has been overridden by context.barrier() API.

 

Below is the spark log,

 

_22/10/27 17:03:50.236 Executor task launch worker for task 0.0 in stage 0.0 (TID 1) INFO Executor: Running task 0.0 in stage 0.0 (TID 1)_
_22/10/27 17:03:50.236 Executor task launch worker for task 1.0 in stage 0.0 (TID 0) INFO Executor: Running task 1.0 in stage 0.0 (TID 0)_
_22/10/27 17:03:50.949 Executor task launch worker for task 0.0 in stage 0.0 (TID 1) INFO BarrierTaskContext: Task 1 from Stage 0(Attempt 0) has entered the global sync, current barrier epoch is 0._
_22/10/27 17:03:50.964 dispatcher-event-loop-1 INFO BarrierCoordinator: Current barrier epoch for Stage 0 (Attempt 0) is 0._
_22/10/27 17:03:50.966 dispatcher-event-loop-1 INFO BarrierCoordinator: Barrier sync epoch 0 from Stage 0 (Attempt 0) received update from Task 1, current progress: 1/2._
_22/10/27 17:03:51.436 Executor task launch worker for task 1.0 in stage 0.0 (TID 0) INFO BarrierTaskContext: Task 0 from Stage 0(Attempt 0) has entered the global sync, current barrier epoch is 0._
_22/10/27 17:03:51.437 dispatcher-event-loop-0 INFO BarrierCoordinator: Current barrier epoch for Stage 0 (Attempt 0) is 0._
_22/10/27 17:03:51.437 dispatcher-event-loop-0 INFO BarrierCoordinator: Barrier sync epoch 0 from Stage 0 (Attempt 0) received update from Task 0, current progress: 2/2._
_22/10/27 17:03:51.440 dispatcher-event-loop-0 INFO BarrierCoordinator: Barrier sync epoch 0 from Stage 0 (Attempt 0) received all updates from tasks, finished successfully._
_22/10/27 17:03:51.958 Executor task launch worker for task 0.0 in stage 0.0 (TID 1) INFO BarrierTaskContext: Task 1 from Stage 0(Attempt 0) finished global sync successfully, waited for 1 seconds, current barrier epoch is 1._
_22/10/27 17:03:51.959 Executor task launch worker for task 0.0 in stage 0.0 (TID 1) INFO BarrierTaskContext: Task 1 from Stage 0(Attempt 0) has entered the global sync, current barrier epoch is 1._
_22/10/27 17:03:51.960 dispatcher-event-loop-1 INFO BarrierCoordinator: Current barrier epoch for Stage 0 (Attempt 0) is 1._
_22/10/27 17:03:51.960 dispatcher-event-loop-1 INFO BarrierCoordinator: Barrier sync epoch 1 from Stage 0 (Attempt 0) received update from Task 1, current progress: 1/2._
_22/10/27 17:03:52.437 Executor task launch worker for task 1.0 in stage 0.0 (TID 0) INFO BarrierTaskContext: Task 0 from Stage 0(Attempt 0) finished global sync successfully, waited for 1 seconds, current barrier epoch is 1._
_22/10/27 17:03:52.438 Executor task launch worker for task 1.0 in stage 0.0 (TID 0) INFO BarrierTaskContext: Task 0 from Stage 0(Attempt 0) has entered the global sync, current barrier epoch is 1._
_22/10/27 17:03:52.438 dispatcher-event-loop-0 INFO BarrierCoordinator: Current barrier epoch for Stage 0 (Attempt 0) is 1._
_22/10/27 17:03:52.439 dispatcher-event-loop-0 INFO BarrierCoordinator: Barrier sync epoch 1 from Stage 0 (Attempt 0) received update from Task 0, current progress: 2/2._
_22/10/27 17:03:52.439 dispatcher-event-loop-0 INFO BarrierCoordinator: Barrier sync epoch 1 from Stage 0 (Attempt 0) received all updates from tasks, finished successfully._
_22/10/27 17:03:52.960 Executor task launch worker for task 0.0 in stage 0.0 (TID 1) INFO BarrierTaskContext: Task 1 from Stage 0(Attempt 0) finished global sync successfully, waited for 1 seconds, current barrier epoch is 2._
_22/10/27 17:03:52.972 Executor task launch worker for task 0.0 in stage 0.0 (TID 1) INFO Executor: Finished task 0.0 in stage 0.0 (TID 1). 1040 bytes result sent to driver_
_22/10/27 17:03:52.974 dispatcher-event-loop-1 INFO TaskSchedulerImpl: Skip current round of resource offers for barrier stage 0 because the barrier taskSet requires 2 slots, while the total number of available slots is 1._
_22/10/27 17:03:52.976 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 1) in 2762 ms on 192.168.31.236 (executor driver) (1/2)_
_22/10/27 17:03:53.439 Executor task launch worker for task 1.0 in stage 0.0 (TID 0) INFO BarrierTaskContext: Task 0 from Stage 0(Attempt 0) finished global sync successfully, waited for 1 seconds, current barrier epoch is 2._
_22/10/27 17:03:53.445 Executor task launch worker for task 1.0 in stage 0.0 (TID 0) INFO Executor: Finished task 1.0 in stage 0.0 (TID 0). 1040 bytes result sent to driver_

 

After debugging, I found the [object messages|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala#L102] (Array[String]) returning to BarrierTaskContext are the same as the [original messages|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/BarrierCoordinator.scala#L107]

 

I will file a PR for this issue",,apachespark,cloud_fan,wbo4958,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Oct 28 13:07:31 UTC 2022,,,,,,,,,,"0|z1af8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Oct/22 09:57;apachespark;User 'wbo4958' has created a pull request for this issue:
https://github.com/apache/spark/pull/38410;;;","27/Oct/22 09:57;apachespark;User 'wbo4958' has created a pull request for this issue:
https://github.com/apache/spark/pull/38410;;;","28/Oct/22 13:07;cloud_fan;Issue resolved by pull request 38410
[https://github.com/apache/spark/pull/38410];;;",,,,,,,,,,,,,,,
Unhex function works incorrectly when input has uneven number of symbols,SPARK-40924,13492069,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vli-databricks,vli-databricks,vli-databricks,26/Oct/22 17:32,28/Oct/22 05:02,13/Jul/23 08:51,27/Oct/22 11:15,3.3.1,,,,,,,,,,,,,,3.3.2,3.4.0,,,,SQL,,,0,,,,,,"Try `select hex(unhex('123'))`.
",,apachespark,maxgekk,vli-databricks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Oct 27 18:11:25 UTC 2022,,,,,,,,,,"0|z1acs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Oct/22 17:54;apachespark;User 'vitaliili-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/38402;;;","27/Oct/22 11:15;maxgekk;Resolved by merging https://github.com/apache/spark/pull/38402 to master.;;;","27/Oct/22 18:10;apachespark;User 'vitaliili-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/38416;;;","27/Oct/22 18:11;apachespark;User 'vitaliili-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/38416;;;",,,,,,,,,,,,,,
Mismatch between ParquetFileFormat and FileSourceScanExec in # columns for WSCG.isTooManyFields when using _metadata,SPARK-40918,13491749,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,26/Oct/22 11:46,31/Oct/22 06:00,13/Jul/23 08:51,31/Oct/22 06:00,3.3.0,,,,,,,,,,,,,,3.3.2,,,,,SQL,,,0,,,,,,"_metadata.columns are taken into account in FileSourceScanExec.supportColumnar, but not when the parquet reader is created. This can result in Parquet reader outputting columnar (because it has less columns than WSCG.isTooManyFields), whereas FileSourceScanExec wants row output (because with the extra metadata columns it hits the isTooManyFields limit).

I have a fix forthcoming.",,apachespark,cloud_fan,juliuszsompolski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 31 06:00:42 UTC 2022,,,,,,,,,,"0|z1aat4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Oct/22 12:49;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/38397;;;","26/Oct/22 12:50;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/38397;;;","28/Oct/22 17:40;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/38431;;;","31/Oct/22 06:00;cloud_fan;Issue resolved by pull request 38431
[https://github.com/apache/spark/pull/38431];;;",,,,,,,,,,,,,,
udf could not filter null value cause npe,SPARK-40916,13491720,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,zhongjingxiong,zhongjingxiong,26/Oct/22 09:02,31/Oct/22 17:06,13/Jul/23 08:51,26/Oct/22 15:34,3.2.0,,,,,,,,,,,,,,,,,,,SQL,,,0,,,,,,"{code:sql}
select
    t22.uid,
from
(
    SELECT
        code,
        count(distinct uid) cnt
    FROM
    (
        SELECT
            uid,
            code,
            lng,
            lat
        FROM
        (
            select
             riskmanage_dw.GEOHASH_ENCODE(manhattan_dw.aes_decode(lng),manhattan_dw.aes_decode(lat),8) as code,
                uid,
                lng,
                lat,
                dt as event_time 
            from
            (
                select
                    param['timestamp'] as dt,
                    get_json_object(get_json_object(param['input'],'$.baseInfo'),'$.uid') uid,
                    get_json_object(get_json_object(param['input'],'$.envInfo'),'$.lng') lng,
                    get_json_object(get_json_object(param['input'],'$.envInfo'),'$.lat') lat 
                from manhattan_ods.ods_log_manhattan_fbi_workflow_result_log
                and get_json_object(get_json_object(param['input'],'$.bizExtents'),'$.productId')='2001' 
            )a
            and lng is not null
            and lat is not null
        ) t2
        group by uid,code,lng,lat
    ) t1
    GROUP BY code having count(DISTINCT uid)>=10
)t11
join
(
    SELECT
        uid,
        code,
        lng,
        lat
    FROM
    (
        select
            riskmanage_dw.GEOHASH_ENCODE(manhattan_dw.aes_decode(lng),manhattan_dw.aes_decode(lat),8) as code,
            uid,
            lng,
            lat,
            dt as event_time
        from
        (
            select
                param['timestamp'] as dt,
                get_json_object(get_json_object(param['input'],'$.baseInfo'),'$.uid') uid,
                get_json_object(get_json_object(param['input'],'$.envInfo'),'$.lng') lng, 
                get_json_object(get_json_object(param['input'],'$.envInfo'),'$.lat') lat 
            from manhattan_ods.ods_log_manhattan_fbi_workflow_result_log 
            and get_json_object(get_json_object(param['input'],'$.bizExtents'),'$.productId')='2001' 
        )a
        and lng is not null
        and lat is not null
    ) t2
    where substr(code,0,6)<>'wx4ey3'
    group by uid,code,lng,lat
) t22 on t11.code=t22.code
group by t22.uid
{code}
this sql can't run because `riskmanage_dw.GEOHASH_ENCODE(manhattan_dw.aes_decode(lng),manhattan_dw.aes_decode(lat),8)` will throw npe(`Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public java.lang.String com.xiaoju.automarket.GeohashEncode.evaluate(java.lang.Double,java.lang.Double,java.lang.Integer) with arguments {null,null,8}:null`), but I have filter null in my condition, the udf of manhattan_dw.aes_decode will return null if lng or lat is null, *but after I remove `where substr(code,0,6)<>'wx4ey3' `this condition, it can run normally.* 


complete :
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public java.lang.String com.xiaoju.automarket.GeohashEncode.evaluate(java.lang.Double,java.lang.Double,java.lang.Integer) with arguments {null,null,8}:null
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:1049)
	at org.apache.spark.sql.hive.HiveSimpleUDF.eval(hiveUDFs.scala:102)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.subExpr_3$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)
	at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3(basicPhysicalOperators.scala:275)
	at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3$adapted(basicPhysicalOperators.scala:274)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","spark3.2.0
hadoop2.7.3
hive2.3.9",xkrogen,zhongjingxiong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Oct 26 15:34:26 UTC 2022,,,,,,,,,,"0|z1aamo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Oct/22 15:34;zhongjingxiong; add --conf spark.sql.subexpressionElimination.enabled=false;;;",,,,,,,,,,,,,,,,,
`PandasMode` should copy keys before inserting into Map,SPARK-40907,13491222,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,podongfeng,podongfeng,25/Oct/22 07:11,22/Mar/23 00:31,13/Jul/23 08:51,25/Oct/22 14:56,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Pandas API on Spark,SQL,,0,,,,,,,,apachespark,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Oct 25 14:56:11 UTC 2022,,,,,,,,,,"0|z1a7k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Oct/22 07:15;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/38385;;;","25/Oct/22 07:15;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/38385;;;","25/Oct/22 14:56;gurwls223;Issue resolved by pull request 38385
[https://github.com/apache/spark/pull/38385];;;",,,,,,,,,,,,,,,
`Mode` should copy keys before inserting into Map,SPARK-40906,13491110,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,podongfeng,podongfeng,25/Oct/22 03:50,25/Oct/22 06:23,13/Jul/23 08:51,25/Oct/22 06:23,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,,,apachespark,cloud_fan,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Oct 25 06:23:25 UTC 2022,,,,,,,,,,"0|z1a6v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Oct/22 03:59;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/38383;;;","25/Oct/22 03:59;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/38383;;;","25/Oct/22 06:23;cloud_fan;Issue resolved by pull request 38383
[https://github.com/apache/spark/pull/38383];;;",,,,,,,,,,,,,,,
Quick submission of drivers in tests to mesos scheduler results in dropping drivers,SPARK-40902,13490644,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mridulm80,mridulm80,mridulm80,24/Oct/22 16:21,24/Oct/22 17:53,13/Jul/23 08:51,24/Oct/22 17:53,2.4.8,3.0.3,3.4.0,,,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,Mesos,,,0,,,,,,"Queued drivers in MesosClusterScheduler are ordered based on MesosDriverDescription - and the default ordering checks for priority, followed by submission time. For two driver submissions with same priority and if made in quick succession (such that submission time is same due to millisecond granularity of Date), this results in dropping the second MesosDriverDescription from the queuedDrivers - as driverOrdering returns 0 when comparing the descriptions. This jira fixes the more immediate issue with tests, but we do need to relook at this for mess scheduler in general later.

Currently, this affects tests - for example, in the latest VOTE for 3.3.1 [1] - and is not consistently reproducible unless on a fast machine.



[1] https://lists.apache.org/thread/jof098qxp0s6qqmt9qwv52f9665b1pjg",,apachespark,dongjoon,mridulm80,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 24 17:53:16 UTC 2022,,,,,,,,,,"0|z1a3zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Oct/22 16:22;mridulm80;+CC [~dongjoon], [~yumwang];;;","24/Oct/22 16:48;apachespark;User 'mridulm' has created a pull request for this issue:
https://github.com/apache/spark/pull/38378;;;","24/Oct/22 16:49;apachespark;User 'mridulm' has created a pull request for this issue:
https://github.com/apache/spark/pull/38378;;;","24/Oct/22 17:53;dongjoon;This is resolved via [https://github.com/apache/spark/pull/38378];;;",,,,,,,,,,,,,,
Unable to store Spark Driver logs with Absolute Hadoop based URI FS Path,SPARK-40901,13490643,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,swamirishi,swamirishi,swamirishi,24/Oct/22 16:20,11/Nov/22 00:19,13/Jul/23 08:51,11/Nov/22 00:19,3.2.2,3.3.0,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,Spark Config: spark.driver.log.dfsDir doesn't support absolute URI hadoop based path. It currently only supports URI path and writes only to fs.defaultFS and does not write logs to any other configured filesystem.,,apachespark,mridulm80,swamirishi,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Nov 11 00:19:07 UTC 2022,,,,,,,,,,"0|z1a3zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Oct/22 16:30;apachespark;User 'swamirishi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38377;;;","24/Oct/22 16:30;apachespark;User 'swamirishi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38377;;;","11/Nov/22 00:19;mridulm80;Issue resolved by pull request 38377
[https://github.com/apache/spark/pull/38377];;;",,,,,,,,,,,,,,,
Spark will filter out data field sorting when dynamic partitions and data fields are sorted at the same time,SPARK-40885,13489344,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,Zing,Zing,23/Oct/22 03:09,04/Mar/23 05:58,13/Jul/23 08:51,04/Mar/23 05:58,3.1.2,3.2.2,3.3.0,3.4.0,,,,,,,,,,,,,,,,SQL,,,0,,,,,,"When using dynamic partitions to write data and sort partitions and data fields, Spark will filter the sorting of data fields.

 

reproduce sql:
{code:java}
CREATE TABLE `sort_table`(
  `id` int,
  `name` string
  )
PARTITIONED BY (
  `dt` string)
stored as textfile
LOCATION 'sort_table';CREATE TABLE `test_table`(
  `id` int,
  `name` string)
PARTITIONED BY (
  `dt` string)
stored as textfile
LOCATION
  'test_table';//gen test data
insert into test_table partition(dt=20221011) select 10,""15"" union all select 1,""10"" union  all select 5,""50"" union  all select 20,""2"" union  all select 30,""14""  ;
set spark.hadoop.hive.exec.dynamici.partition=true;
set spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict;

// this sql sort with partition filed (`dt`) and data filed (`name`), but sort with `name` can not work
insert overwrite table sort_table partition(dt) select id,name,dt from test_table order by name,dt;
 {code}
 

The Sort operator of DAG has only one sort field, but there are actually two in SQL.(See the attached drawing)

 

It relate this issue : https://issues.apache.org/jira/browse/SPARK-40588",,apachespark,EnricoMi,xkrogen,Zing,,,,,,,,,,,,,,,,SPARK-41959,,,,,,,,"23/Oct/22 03:12;Zing;1666494504884.jpg;https://issues.apache.org/jira/secure/attachment/13051296/1666494504884.jpg",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://issues.apache.org/jira/browse/SPARK-40588,,,,,,,,,,9223372036854775807,,,,,,Wed Feb 01 06:47:37 UTC 2023,,,,,,,,,,"0|z19vyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Oct/22 12:47;apachespark;User 'zzzzming95' has created a pull request for this issue:
https://github.com/apache/spark/pull/38356;;;","10/Jan/23 06:59;EnricoMi;This should be fixed by SPARK-41959 / [#39475|http://example.com|https://github.com/apache/spark/pull/39475].;;;","01/Feb/23 06:47;EnricoMi;This has been fixed in 3.4.0 and 3.5.0: https://github.com/apache/spark/pull/37525#pullrequestreview-1275234100;;;",,,,,,,,,,,,,,,
Fix broadcasts in Python UDFs when encryption is enabled,SPARK-40874,13488659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,21/Oct/22 14:08,12/Dec/22 18:10,13/Jul/23 08:51,22/Oct/22 01:39,3.4.0,,,,,,,,,,,,,,3.1.4,3.2.3,3.3.2,3.4.0,,PySpark,,,0,,,,,,"The following Pyspark script:
{noformat}
bin/pyspark --conf spark.io.encryption.enabled=true

...

bar = {""a"": ""aa"", ""b"": ""bb""}
foo = spark.sparkContext.broadcast(bar)
spark.udf.register(""MYUDF"", lambda x: foo.value[x] if x else """")
spark.sql(""SELECT MYUDF('a') AS a, MYUDF('b') AS b"").collect()
{noformat}
fails with:
{noformat}
22/10/21 17:14:32 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)/ 1]
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/Users/petertoth/git/apache/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 811, in main
    func, profiler, deserializer, serializer = read_command(pickleSer, infile)
  File ""/Users/petertoth/git/apache/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 87, in read_command
    command = serializer._read_with_length(file)
  File ""/Users/petertoth/git/apache/spark/python/lib/pyspark.zip/pyspark/serializers.py"", line 173, in _read_with_length
    return self.loads(obj)
  File ""/Users/petertoth/git/apache/spark/python/lib/pyspark.zip/pyspark/serializers.py"", line 471, in loads
    return cloudpickle.loads(obj, encoding=encoding)
EOFError: Ran out of input
{noformat}",,apachespark,petertoth,roczei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Oct 22 01:39:41 UTC 2022,,,,,,,,,,"0|z19rs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Oct/22 15:45;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/38334;;;","22/Oct/22 01:39;gurwls223;Issue resolved by pull request 38334
[https://github.com/apache/spark/pull/38334];;;",,,,,,,,,,,,,,,,
KubernetesConf.getResourceNamePrefix creates invalid name prefixes,SPARK-40869,13488606,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tstadler,tstadler,tstadler,21/Oct/22 10:29,03/Nov/22 17:35,13/Jul/23 08:51,03/Nov/22 17:35,3.3.0,,,,,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,Kubernetes,,,0,,,,,,"If `KubernetesConf.getResourceNamePrefix` is called with e.g. `_name_`, it generates an invalid name prefix, e.g. `-name-0123456789abcdef`.",,apachespark,dongjoon,tstadler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Nov 03 17:35:27 UTC 2022,,,,,,,,,,"0|z19rg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Oct/22 10:37;apachespark;User 'tobiasstadler' has created a pull request for this issue:
https://github.com/apache/spark/pull/38331;;;","21/Oct/22 10:38;apachespark;User 'tobiasstadler' has created a pull request for this issue:
https://github.com/apache/spark/pull/38331;;;","03/Nov/22 17:35;dongjoon;Issue resolved by pull request 38331
[https://github.com/apache/spark/pull/38331];;;",,,,,,,,,,,,,,,
Flaky test ProtobufCatalystDataConversionSuite,SPARK-40867,13488518,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,LuciferYang,LuciferYang,21/Oct/22 08:57,24/Oct/22 03:18,13/Jul/23 08:51,22/Oct/22 21:22,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Tests,,,0,,,,,,"* [https://github.com/LuciferYang/spark/actions/runs/3295309311/jobs/5433733419]
 * [https://github.com/LuciferYang/spark/actions/runs/3291252601/jobs/5425183034]

{code:java}
[info] ProtobufCatalystDataConversionSuite:
[info] - single StructType(StructField(int32_type,IntegerType,true)) with seed 167 *** FAILED *** (39 milliseconds)
[info]   Incorrect evaluation (codegen off): from_protobuf(to_protobuf([0], /home/runner/work/spark/spark/connector/protobuf/target/scala-2.12/test-classes/protobuf/catalyst_types.desc, IntegerMsg), /home/runner/work/spark/spark/connector/protobuf/target/scala-2.12/test-classes/protobuf/catalyst_types.desc, IntegerMsg), actual: [null], expected: [0] (ExpressionEvalHelper.scala:209)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
[info]   at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Assertions.fail(Assertions.scala:933)
[info]   at org.scalatest.Assertions.fail$(Assertions.scala:929)
[info]   at org.scalatest.funsuite.AnyFunSuite.fail(AnyFunSuite.scala:1564)
[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkEvaluationWithoutCodegen(ExpressionEvalHelper.scala:209)
[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkEvaluationWithoutCodegen$(ExpressionEvalHelper.scala:199)
[info]   at org.apache.spark.sql.protobuf.ProtobufCatalystDataConversionSuite.checkEvaluationWithoutCodegen(ProtobufCatalystDataConversionSuite.scala:33)
[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkEvaluation(ExpressionEvalHelper.scala:87)
[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkEvaluation$(ExpressionEvalHelper.scala:82)
[info]   at org.apache.spark.sql.protobuf.ProtobufCatalystDataConversionSuite.checkEvaluation(ProtobufCatalystDataConversionSuite.scala:33)
[info]   at org.apache.spark.sql.protobuf.ProtobufCatalystDataConversionSuite.checkResult(ProtobufCatalystDataConversionSuite.scala:43)
[info]   at org.apache.spark.sql.protobuf.ProtobufCatalystDataConversionSuite.$anonfun$new$2(ProtobufCatalystDataConversionSuite.scala:122)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:750) {code}
re-run can pass",,LuciferYang,sanysandish@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 24 03:18:15 UTC 2022,,,,,,,,,,"0|z19qwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Oct/22 09:00;LuciferYang;cc [~rangadi]  and [~sanysandish@gmail.com] ;;;","22/Oct/22 21:22;sanysandish@gmail.com;[~LuciferYang] this issue got resolved through https://github.com/apache/spark/pull/38286;;;","24/Oct/22 03:18;LuciferYang;Thanks [~sanysandish@gmail.com] ;;;",,,,,,,,,,,,,,,
Cleanup github action warning,SPARK-40858,13487280,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,20/Oct/22 14:46,12/Dec/22 18:11,13/Jul/23 08:51,27/Oct/22 06:05,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Project Infra,,,1,,,,,,"* The `save-state` command is deprecated and will be disabled soon. Please upgrade to using Environment Files. For more information see: https://github.blog/changelog/2022-10-11-github-actions-deprecating-save-state-and-set-output-commands/
* The `set-output` command is deprecated and will be disabled soon. Please upgrade to using Environment Files. For more information see: https://github.blog/changelog/2022-10-11-github-actions-deprecating-save-state-and-set-output-commands/
* Node.js 12 actions are deprecated. For more information see: https://github.blog/changelog/2022-09-22-github-actions-all-actions-will-begin-running-on-node16-instead-of-node12/. Please update the following actions to use Node.js 16: actions/checkout, docker/setup-qemu-action, docker/setup-buildx-action, docker/build-push-action, docker/build-push-action, docker/setup-buildx-action, actions/checkout
* ....

like: https://github.com/apache/spark/actions/runs/3288503290

In order to reduce pontential impact on infra, we'd better address them one by one according actions type.",,apachespark,LuciferYang,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Oct 27 08:58:22 UTC 2022,,,,,,,,,,"0|z19j9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Oct/22 01:14;gurwls223;Is it all done [~yikunkero]? (just asking out of curiosity);;;","27/Oct/22 02:23;yikunkero;[~hyukjin.kwon] 

According latest https://github.com/apache/spark/actions/runs/3326934877, only actions/setup-python left,. After https://issues.apache.org/jira/browse/SPARK-40928 all will be done.;;;","27/Oct/22 08:58;yikunkero;All warning cleanup: https://github.com/apache/spark/actions/runs/3334910929

Resolved.;;;",,,,,,,,,,,,,,,
TimestampFormatter behavior changed when using the latest Java 8/11/17,SPARK-40851,13487194,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,LuciferYang,LuciferYang,LuciferYang,20/Oct/22 03:33,24/Oct/22 13:53,13/Jul/23 08:51,21/Oct/22 04:59,3.4.0,,,,,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,SQL,,,0,,,,,,"{code:java}
[info] *** 12 TESTS FAILED ***
[error] Failed: Total 6746, Failed 12, Errors 0, Passed 6734, Ignored 5
[error] Failed tests:
[error] 	org.apache.spark.sql.catalyst.expressions.CastWithAnsiOffSuite
[error] 	org.apache.spark.sql.catalyst.util.TimestampFormatterSuite
[error] 	org.apache.spark.sql.catalyst.expressions.CastWithAnsiOnSuite
[error] 	org.apache.spark.sql.catalyst.util.RebaseDateTimeSuite
[error] 	org.apache.spark.sql.catalyst.expressions.TryCastSuite {code}
We can reproduce this issue using Java 8u352/11.0.17/17.0.5,  the test errors are similar to the following:

run
{code:java}
build/sbt clean ""catalyst/testOnly *CastWithAnsiOffSuite"" {code}
with 8u352:
{code:java}
[info] - SPARK-35711: cast timestamp without time zone to timestamp with local time zone *** FAILED *** (190 milliseconds)
[info]   Incorrect evaluation (codegen off): cast(0001-01-01 00:00:00 as timestamp), actual: -62135617820000000, expected: -62135596800000000 (ExpressionEvalHelper.scala:209)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
[info]   at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Assertions.fail(Assertions.scala:933)
[info]   at org.scalatest.Assertions.fail$(Assertions.scala:929)
[info]   at org.scalatest.funsuite.AnyFunSuite.fail(AnyFunSuite.scala:1564)
[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkEvaluationWithoutCodegen(ExpressionEvalHelper.scala:209)
[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkEvaluationWithoutCodegen$(ExpressionEvalHelper.scala:199)
[info]   at org.apache.spark.sql.catalyst.expressions.CastSuiteBase.checkEvaluationWithoutCodegen(CastSuiteBase.scala:49)
[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkEvaluation(ExpressionEvalHelper.scala:87)
[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkEvaluation$(ExpressionEvalHelper.scala:82)
[info]   at org.apache.spark.sql.catalyst.expressions.CastSuiteBase.checkEvaluation(CastSuiteBase.scala:49)
[info]   at org.apache.spark.sql.catalyst.expressions.CastSuiteBase.$anonfun$new$198(CastSuiteBase.scala:893)
[info]   at org.apache.spark.sql.catalyst.expressions.CastSuiteBase.$anonfun$new$198$adapted(CastSuiteBase.scala:890)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.apache.spark.sql.catalyst.expressions.CastSuiteBase.$anonfun$new$197(CastSuiteBase.scala:890)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.apache.spark.sql.catalyst.util.DateTimeTestUtils$.withDefaultTimeZone(DateTimeTestUtils.scala:61)
[info]   at org.apache.spark.sql.catalyst.expressions.CastSuiteBase.$anonfun$new$196(CastSuiteBase.scala:890)
[info]   at org.apache.spark.sql.catalyst.expressions.CastSuiteBase.$anonfun$new$196$adapted(CastSuiteBase.scala:888)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.apache.spark.sql.catalyst.expressions.CastSuiteBase.$anonfun$new$195(CastSuiteBase.scala:888)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:750) {code}
but run with 8u345:

 
{code:java}
[info] Run completed in 49 seconds, 54 milliseconds.
[info] Total number of tests run: 80
[info] Suites: completed 1, aborted 0
[info] Tests: succeeded 80, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed. {code}",,apachespark,Jackey Lee,LuciferYang,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 24 02:52:20 UTC 2022,,,,,,,,,,"0|z19iqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Oct/22 09:01;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/38317;;;","21/Oct/22 04:59;maxgekk;Resolved by https://github.com/apache/spark/pull/38317;;;","24/Oct/22 02:45;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/38363;;;","24/Oct/22 02:52;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/38365;;;",,,,,,,,,,,,,,
Tests for Spark SQL Intrepetered Queries may execute Codegen,SPARK-40850,13487187,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,holden,holden,20/Oct/22 01:49,28/Jun/23 13:17,13/Jul/23 08:51,28/Jun/23 13:17,3.3.0,3.3.1,,,,,,,,,,,,,3.5.0,,,,,SQL,Tests,,0,,,,,,"We also need to set SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> ""false"" in PlanTest

 ",,fanjia,holden,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 28 13:17:29 UTC 2023,,,,,,,,,,"0|z19iow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Jun/23 13:17;maxgekk;Issue resolved by pull request 41467
[https://github.com/apache/spark/pull/41467];;;",,,,,,,,,,,,,,,,,
Upgrade infra base image to focal-20220922,SPARK-40838,13486957,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,19/Oct/22 02:43,20/Oct/22 07:54,13/Jul/23 08:51,20/Oct/22 07:54,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Project Infra,,,0,,,,,,,,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Oct 20 07:54:49 UTC 2022,,,,,,,,,,"0|z19ha0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Oct/22 02:58;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38304;;;","19/Oct/22 02:58;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38304;;;","20/Oct/22 07:54;yikunkero;Issue resolved by pull request 38304
[https://github.com/apache/spark/pull/38304];;;",,,,,,,,,,,,,,,
STORED AS serde in CREATE TABLE LIKE view does not work,SPARK-40829,13486772,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhangbutao,zhangbutao,zhangbutao,18/Oct/22 07:06,18/Oct/22 22:44,13/Jul/23 08:51,18/Oct/22 21:56,3.2.2,3.3.0,3.4.0,,,,,,,,,,,,3.2.3,3.3.2,3.4.0,,,SQL,,,0,,,,,,"SPARK-29839 added the feat for supporting STORED AS in CREATE TABLE LIKE, and we could use this syntax to create a table with specified serde based on a existing *table or view.*

I found that if we create a table based on a {*}view{*}, the serde of created table is always parquet.  However, if we use *USING syntax*  (SPARK-29421) to create a table with specified serde based a {*}view{*}, we can get the correct serde.

I think we shoud fix this to add specified serde for the created table when using *create table like view* *stored as* syntax.

 ",,apachespark,dongjoon,zhangbutao,,,,,,,,,,,,,,,,,,,,,,,SPARK-29839,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Oct 18 21:56:07 UTC 2022,,,,,,,,,,"0|z19g7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Oct/22 07:27;zhangbutao;PR https://github.com/apache/spark/pull/38295;;;","18/Oct/22 07:27;apachespark;User 'zhangbutao' has created a pull request for this issue:
https://github.com/apache/spark/pull/38295;;;","18/Oct/22 07:28;apachespark;User 'zhangbutao' has created a pull request for this issue:
https://github.com/apache/spark/pull/38295;;;","18/Oct/22 21:56;dongjoon;Issue resolved by pull request 38295
[https://github.com/apache/spark/pull/38295];;;",,,,,,,,,,,,,,
"Parquet INT64 (TIMESTAMP(NANOS,true)) now throwing Illegal Parquet type instead of automatically converting to LongType ",SPARK-40819,13486597,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,alfiewdavidson,alfiewdavidson,alfiewdavidson,17/Oct/22 11:21,22/Feb/23 23:21,13/Jul/23 08:51,06/Feb/23 09:36,3.2.0,3.2.1,3.2.2,3.2.3,3.3.0,3.3.1,3.3.2,3.4.0,,,,,,,3.2.4,3.3.2,3.4.0,,,SQL,,,0,regression,,,,,"Since 3.2 parquet files containing attributes with type ""INT64 (TIMESTAMP(NANOS, true))"" are no longer readable and attempting to read throws:

 
{code:java}
Caused by: org.apache.spark.sql.AnalysisException: Illegal Parquet type: INT64 (TIMESTAMP(NANOS,true))
  at org.apache.spark.sql.errors.QueryCompilationErrors$.illegalParquetTypeError(QueryCompilationErrors.scala:1284)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.illegalType$1(ParquetSchemaConverter.scala:105)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertPrimitiveField(ParquetSchemaConverter.scala:174)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertField(ParquetSchemaConverter.scala:90)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convert$1(ParquetSchemaConverter.scala:72)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.Iterator.foreach(Iterator.scala:941)
  at scala.collection.Iterator.foreach$(Iterator.scala:941)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.AbstractTraversable.map(Traversable.scala:108)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convert(ParquetSchemaConverter.scala:66)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convert(ParquetSchemaConverter.scala:63)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readSchemaFromFooter$2(ParquetFileFormat.scala:548)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readSchemaFromFooter(ParquetFileFormat.scala:548)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$2(ParquetFileFormat.scala:528)
  at scala.collection.immutable.Stream.map(Stream.scala:418)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:528)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:521)
  at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76) {code}
Prior to 3.2 successfully reads the parquet automatically converting to a LongType.

I believe work part of https://issues.apache.org/jira/browse/SPARK-34661 introduced the change in behaviour, more specifically here: [https://github.com/apache/spark/pull/31776/files#diff-3730a913c4b95edf09fb78f8739c538bae53f7269555b6226efe7ccee1901b39R154] which throws the QueryCompilationErrors.illegalParquetTypeError",,alfiewdavidson,apachespark,gurwls223,roczei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,,,Wed Feb 08 14:46:39 UTC 2023,,,,,,,,,,"0|z19f4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Oct/22 12:03;apachespark;User 'awdavidson' has created a pull request for this issue:
https://github.com/apache/spark/pull/38312;;;","06/Feb/23 09:36;gurwls223;Fixed in https://github.com/apache/spark/pull/38312;;;","06/Feb/23 10:23;apachespark;User 'awdavidson' has created a pull request for this issue:
https://github.com/apache/spark/pull/39901;;;","06/Feb/23 12:16;apachespark;User 'awdavidson' has created a pull request for this issue:
https://github.com/apache/spark/pull/39904;;;","06/Feb/23 13:20;apachespark;User 'awdavidson' has created a pull request for this issue:
https://github.com/apache/spark/pull/39905;;;","08/Feb/23 14:46;apachespark;User 'awdavidson' has created a pull request for this issue:
https://github.com/apache/spark/pull/39943;;;",,,,,,,,,,,,
Remote spark.jars URIs ignored for Spark on Kubernetes in cluster mode ,SPARK-40817,13486567,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,_anton,_anton,_anton,17/Oct/22 08:44,20/Jan/23 22:44,13/Jul/23 08:51,20/Jan/23 22:41,3.0.0,3.1.3,3.2.2,3.3.0,3.4.0,,,,,,,,,,3.2.4,3.3.2,3.4.0,,,Kubernetes,Spark Submit,,1,,,,,,"I discovered that remote URIs in {{spark.jars}} get discarded when launching Spark on Kubernetes in cluster mode via spark-submit.
h1. Reproduction

Here is an example reproduction with S3 being used for remote JAR storage: 

I first created 2 JARs:
 * {{/opt/my-local-jar.jar}} on the host where I'm running spark-submit
 * {{s3://$BUCKET_NAME/my-remote-jar.jar}} in an S3 bucket I own

I then ran the following spark-submit command with {{spark.jars}} pointing to both the local JAR and the remote JAR:
{code:java}
 spark-submit \
  --master k8s://https://$KUBERNETES_API_SERVER_URL:443 \
  --deploy-mode cluster \
  --name=spark-submit-test \
  --class org.apache.spark.examples.SparkPi \
  --conf spark.jars=/opt/my-local-jar.jar,s3a://$BUCKET_NAME/my-remote-jar.jar \
  --conf spark.kubernetes.file.upload.path=s3a://$BUCKET_NAME/my-upload-path/ \
  [...]
  /opt/spark/examples/jars/spark-examples_2.12-3.1.3.jar
{code}
Once the driver and the executors started, I confirmed that there was no trace of {{my-remote-jar.jar}} anymore. For example, looking at the Spark History Server, I could see that {{spark.jars}} got transformed into this:

!image-2022-10-17-10-44-46-862.png|width=991,height=80!

There was no mention of {{my-remote-jar.jar}} on the classpath or anywhere else.

Note that I ran all tests with Spark 3.1.3, however the code which handles those dependencies seems to be the same for more recent versions of Spark as well.
h1. Root cause description

I believe that the issue seems to be coming from [this logic|https://github.com/apache/spark/blob/d1f8a503a26bcfb4e466d9accc5fa241a7933667/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala#L163-L186] in {{{}BasicDriverFeatureStep.getAdditionalPodSystemProperties(){}}}.

Specifically, this logic takes all URIs in {{{}spark.jars{}}}, [filters only on local URIs,|https://github.com/apache/spark/blob/d1f8a503a26bcfb4e466d9accc5fa241a7933667/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala#L165] [uploads|https://github.com/apache/spark/blob/d1f8a503a26bcfb4e466d9accc5fa241a7933667/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala#L173] those local files to {{spark.kubernetes.file.upload.path }}and then [*replaces*|https://github.com/apache/spark/blob/d1f8a503a26bcfb4e466d9accc5fa241a7933667/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala#L182] the value of {{spark.jars}} with those newly uploaded JARs. By overwriting the previous value of {{{}spark.jars{}}}, we are losing all mention of remote JARs that were previously specified there. 

Consequently, when the Spark driver starts afterwards, it only downloads JARs from {{{}spark.kubernetes.file.upload.path{}}}.
h1. Possible solution

I think a possible fix would be to not fully overwrite the value of {{spark.jars}} but to make sure that we keep remote URIs there.

The new logic would look something like this:
{code:java}
Seq(JARS, FILES, ARCHIVES, SUBMIT_PYTHON_FILES).foreach { key =>
  val uris = conf.get(key).filter(uri => KubernetesUtils.isLocalAndResolvable(uri))
  // Save remote URIs
  val remoteUris = conf.get(key).filter(uri => !KubernetesUtils.isLocalAndResolvable(uri))
  val value = {
    if (key == ARCHIVES) {
      uris.map(UriBuilder.fromUri(_).fragment(null).build()).map(_.toString)
    } else {
      uris
    }
  }
  val resolved = KubernetesUtils.uploadAndTransformFileUris(value, Some(conf.sparkConf))
  if (resolved.nonEmpty) {
    val resolvedValue = if (key == ARCHIVES) {
      uris.zip(resolved).map { case (uri, r) =>
        UriBuilder.fromUri(r).fragment(new java.net.URI(uri).getFragment).build().toString
      }
    } else {
      resolved
    }
    // don't forget to add remote URIs
    additionalProps.put(key.key, (resolvedValue ++ remoteUris).mkString("",""))
  }
} {code}
I tested it out in my environment and it worked: {{s3a://$BUCKET_NAME/my-remote-jar.jar}} was kept in {{spark.jars}} and the Spark driver was able to download it.

I don't know the codebase well enough though to assess whether I am missing something else or if this is enough to fix the issue.

 

 

 ","Spark 3.1.3

Kubernetes 1.21

Ubuntu 20.04.1",_anton,apachespark,bjornjorgensen,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/22 08:44;_anton;image-2022-10-17-10-44-46-862.png;https://issues.apache.org/jira/secure/attachment/13051008/image-2022-10-17-10-44-46-862.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,,,Fri Jan 20 10:44:19 UTC 2023,,,,,,,,,,"0|z19exs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Oct/22 16:14;apachespark;User 'antonipp' has created a pull request for this issue:
https://github.com/apache/spark/pull/38376;;;","13/Jan/23 14:11;bjornjorgensen;If this affects 3.4 you must upgrade the ""Affects Version/s:""  ;;;","20/Jan/23 10:43;apachespark;User 'antonipp' has created a pull request for this issue:
https://github.com/apache/spark/pull/39670;;;","20/Jan/23 10:44;apachespark;User 'antonipp' has created a pull request for this issue:
https://github.com/apache/spark/pull/39669;;;",,,,,,,,,,,,,,
SymlinkTextInputFormat returns incorrect result due to enabled spark.hadoopRDD.ignoreEmptySplits,SPARK-40815,13486533,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,17/Oct/22 03:32,08/Nov/22 02:00,13/Jul/23 08:51,31/Oct/22 23:36,3.2.2,3.3.0,3.4.0,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,,,apachespark,dongjoon,ivan.sadikov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Nov 08 02:00:51 UTC 2022,,,,,,,,,,"0|z19eq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Oct/22 03:46;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38277;;;","17/Oct/22 03:46;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38277;;;","31/Oct/22 23:36;dongjoon;This is resolved via https://github.com/apache/spark/pull/38277;;;","03/Nov/22 22:20;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38504;;;","03/Nov/22 22:20;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38504;;;","08/Nov/22 02:00;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38544;;;",,,,,,,,,,,,
Infer schema for CSV files - wrong behavior using header + merge schema,SPARK-40808,13486475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ohadm,ohadm,16/Oct/22 08:24,12/Dec/22 18:10,13/Jul/23 08:51,29/Oct/22 07:01,3.2.2,,,,,,,,,,,,,,,,,,,SQL,,,0,csv,csvparser,CSVReader,,,"Hello. 
I am writing unit-tests to some functionality in my application that reading data from CSV files using Spark.

I am reading the data using:


{code:java}
header=True
mergeSchema=True
inferSchema=True{code}
When I am reading this single file:
{code:java}
File1:
""int_col"",""string_col"",""decimal_col"",""date_col""
1,""hello"",1.43,2022-02-23
2,""world"",5.534,2021-05-05
3,""my name"",86.455,2011-08-15
4,""is ohad"",6.234,2002-03-22{code}
I am getting this schema:
{code:java}
int_col=int
string_col=string
decimal_col=double
date_col=string{code}




When I am duplicating this file, I am getting the same schema.

The strange part is when I am adding new int column, it looks like spark is getting confused and think that the column that already identified as int are now string:


{code:java}
File1:
""int_col"",""string_col"",""decimal_col"",""date_col""
1,""hello"",1.43,2022-02-23
2,""world"",5.534,2021-05-05
3,""my name"",86.455,2011-08-15
4,""is ohad"",6.234,2002-03-22
File2:
""int_col"",""string_col"",""decimal_col"",""date_col"",""int2_col""
1,""hello"",1.43,2022-02-23,234
2,""world"",5.534,2021-05-05,5
3,""my name"",86.455,2011-08-15,32
4,""is ohad"",6.234,2002-03-22,2
{code}
result:
{code:java}
int_col=string
string_col=string
decimal_col=string
date_col=string
int2_col=int{code}




When I am reading only the second file, it looks fine:
{code:java}
File2:
""int_col"",""string_col"",""decimal_col"",""date_col"",""int2_col""
1,""hello"",1.43,2022-02-23,234
2,""world"",5.534,2021-05-05,5
3,""my name"",86.455,2011-08-15,32
4,""is ohad"",6.234,2002-03-22,2{code}
result:
{code:java}
int_col=int
string_col=string
decimal_col=double
date_col=string
int2_col=int{code}
For conclusion, it looks like there is a bug mixing the two features: header recognition and merge schema.",,ohadm,Zing,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/22 11:35;ohadm;test_csv.py;https://issues.apache.org/jira/secure/attachment/13051025/test_csv.py",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Oct 29 07:01:11 UTC 2022,,,,,,,,,,"0|z19edc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Oct/22 14:59;Zing;[~ohadm] 

Can you provide code to reproduce this issue.;;;","17/Oct/22 02:56;gurwls223;Yeah reproducer would be helpful to assess this ticket further.;;;","17/Oct/22 11:36;ohadm;[~Zing] [~hyukjin.kwon] 

File test_csv.py attached

As you can see the last test failed;;;","22/Oct/22 09:33;Zing;[~ohadm] 

In spark , infer csv schema will skip first line when set header option is true. But only the header of one file will be regarded as the real first line, which means that if there are two files with different headers, the header of one file will be used as data to infer schema.

 

In this case , You can keep all files with the same header to pass unit test4. 
{code:java}
//file2.csv
""int_col"",""string_col"",""double_col"",""int2_col""
12,""hello2"",1.432
22,""world2"",5.5342
32,""my name2"",86.4552
42,""is ohad2"",6.2342 {code}
 

Read the csv directory, it is reasonable to assume that all files in the directory have the same schema by default. If there are no other doubts, i will mark this issue as resolved.;;;","29/Oct/22 07:01;Zing;expected behavior;;;",,,,,,,,,,,,,
Typo fix: CREATE TABLE -> REPLACE TABLE,SPARK-40806,13486450,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,15/Oct/22 15:03,15/Oct/22 17:35,13/Jul/23 08:51,15/Oct/22 17:35,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,,,apachespark,maxgekk,panbingkun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Oct 15 17:35:12 UTC 2022,,,,,,,,,,"0|z19e7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Oct/22 15:10;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38269;;;","15/Oct/22 17:35;maxgekk;Issue resolved by pull request 38269
[https://github.com/apache/spark/pull/38269];;;",,,,,,,,,,,,,,,,
Alter partition should verify value,SPARK-40798,13486300,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,14/Oct/22 09:59,16/Nov/22 02:58,13/Jul/23 08:51,24/Oct/22 07:21,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,," 
{code:java}
CREATE TABLE t (c int) USING PARQUET PARTITIONED BY(p int);

-- This DDL should fail but worked:
ALTER TABLE t ADD PARTITION(p='aaa'); {code}",,apachespark,cloud_fan,rangareddy.avula@gmail.com,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Nov 16 02:58:49 UTC 2022,,,,,,,,,,"0|z19dag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Oct/22 10:09;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/38257;;;","14/Oct/22 10:09;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/38257;;;","24/Oct/22 07:21;cloud_fan;Issue resolved by pull request 38257
[https://github.com/apache/spark/pull/38257];;;","31/Oct/22 04:07;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/38443;;;","31/Oct/22 12:08;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/38449;;;","08/Nov/22 03:03;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/38547;;;","08/Nov/22 03:04;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/38547;;;","09/Nov/22 09:18;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/38580;;;","09/Nov/22 09:19;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/38580;;;","11/Nov/22 06:45;rangareddy.avula@gmail.com;The below issue is addressed in the current Jira. Can I add a test case for the below issue in InsertSuite.scala? 

https://issues.apache.org/jira/browse/SPARK-40988;;;","11/Nov/22 15:24;ulysses;[~rangareddy.avula@gmail.com] sure, just go ahead;;;","13/Nov/22 03:54;rangareddy.avula@gmail.com;Hi [~ulysses] 

The current Jira will solve both the *Insert* and *Alter* partition values. 
{code:java}
CREATE EXTERNAL TABLE test_partition_value_tbl ( id INT, name STRING ) PARTITIONED BY (age INT) LOCATION 'file:/tmp/spark-warehouse/test_partition_value_tbl';

-- This DDL should fail but worked: - SPARK-40798
ALTER TABLE test_partition_value_tbl ADD PARTITION(age='aaa'); 

-- This DML should fail but worked: - SPARK-40988
INSERT INTO test_partition_value_tbl PARTITION(age=""aaa"") VALUES (1, 'ABC'); {code}
Can we rename *SKIP_TYPE_VALIDATION_ON_ALTER_PARTITION* variable and update the description of this variable?

*Note:* We have a *spark.sql.sources.validatePartitionColumns* {color:#172b4d}parameter and used while reading the partition value.  This variable only we can reuse?{color}

After that, I will commit my test case.;;;","14/Nov/22 01:37;ulysses;I think the name should be fine without change. You are actually altering partition when do insert a partitioned table, right ?;;;","14/Nov/22 03:10;rangareddy.avula@gmail.com;Hi [~ulysses] 

*SPARK-40988* is created for validating the value type while inserting a partition and *SPARK-40798* is created for validating the value while altering a partition. *SPARK-40798* Jira will fix the *validating partition value* for both *insert* and *alter* partitions. So, we can use the *same configuration* parameter for both *Insert* and *Alter* partitions?;;;","14/Nov/22 03:55;ulysses;I think it's fine, the semantics of insert is `insert data` + `add partition`.;;;","14/Nov/22 03:59;rangareddy.avula@gmail.com;Hi [~ulysses] 

I will go ahead and add a test case for the *insert* operation and update the Jira description. ;;;","14/Nov/22 04:00;ulysses;thank you [~rangareddy.avula@gmail.com] ;;;","16/Nov/22 02:58;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/38667;;;"
V2 file scans have duplicative descriptions,SPARK-40775,13485970,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kimahriman,kimahriman,kimahriman,12/Oct/22 21:10,12/Dec/22 10:16,13/Jul/23 08:51,12/Dec/22 10:16,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"V2 file scans have duplication in the description. This is because FileScan uses the metadata to create the description, but each file type overrides metadata and the description adding the same metadata.

Example from a parquet agg pushdown explain:

{{ *+- BatchScan parquet file:/...[min(_3)#814, max(_3)#815, min(_1)#816, max(_1)#817, count(*)#818L, count(_1)#819L, count(_2)#820L, count(_3)#821L] ParquetScan DataFilters: [], Format: parquet, Location: InMemoryFileIndex(1 paths)[file:/..., PartitionFilters: [], PushedAggregation: [MIN(_3), MAX(_3), MIN(_1), MAX(_1), COUNT(*), COUNT(_1), COUNT(_2), COUNT(_3)], PushedFilters: [], PushedGroupBy: [], ReadSchema: struct<min(_3):int,max(_3):int,min(_1):int,max(_1):int,count(*):bigint,count(_1):bigint,count(_2)..., PushedFilters: [], PushedAggregation: [MIN(_3), MAX(_3), MIN(_1), MAX(_1), COUNT(*), COUNT(_1), COUNT(_2), COUNT(_3)], PushedGroupBy: [] RuntimeFilters: []*}}",,apachespark,cloud_fan,kimahriman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Dec 12 10:16:19 UTC 2022,,,,,,,,,,"0|z19b94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Oct/22 01:12;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/38229;;;","12/Dec/22 10:16;cloud_fan;Issue resolved by pull request 38229
[https://github.com/apache/spark/pull/38229];;;",,,,,,,,,,,,,,,,
Estimated size in log message can overflow Int,SPARK-40771,13485921,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,eejbyfeldt,eejbyfeldt,eejbyfeldt,12/Oct/22 14:52,15/Oct/22 13:24,13/Jul/23 08:51,15/Oct/22 13:24,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,"The log message from
```
             logInfo(s""Started reading broadcast variable $id with $numBlocks pieces "" +
               s""(estimated total size $estimatedTotalSize)"")

```
multiples two ints to calculate `estimatedTotalSize` which will overflow when the broadcast exceeds 2Gib.",,apachespark,eejbyfeldt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Oct 15 13:24:14 UTC 2022,,,,,,,,,,"0|z19ayg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Oct/22 14:59;apachespark;User 'eejbyfeldt' has created a pull request for this issue:
https://github.com/apache/spark/pull/38224;;;","12/Oct/22 15:00;apachespark;User 'eejbyfeldt' has created a pull request for this issue:
https://github.com/apache/spark/pull/38224;;;","15/Oct/22 13:24;srowen;Issue resolved by pull request 38224
[https://github.com/apache/spark/pull/38224];;;",,,,,,,,,,,,,,,
Fix bug in test case for catalog directory operation,SPARK-40753,13485691,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,xiaoping.huang,xiaoping.huang,xiaoping.huang,11/Oct/22 14:28,19/Oct/22 08:57,13/Jul/23 08:51,19/Oct/22 08:57,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"The implementation class of ExternalCatalog will perform folder operations when performing operations such as create/drop database/table/partition. The test case creates a folder in advance when obtaining the DB/Partition path URI, resulting in the result of the test case is not convincing enough force.",,apachespark,cloud_fan,xiaoping.huang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Oct 19 08:57:45 UTC 2022,,,,,,,,,,"0|z199k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Oct/22 14:40;apachespark;User 'huangxiaopingRD' has created a pull request for this issue:
https://github.com/apache/spark/pull/38206;;;","11/Oct/22 14:40;apachespark;User 'huangxiaopingRD' has created a pull request for this issue:
https://github.com/apache/spark/pull/38206;;;","19/Oct/22 08:57;cloud_fan;Issue resolved by pull request 38206
[https://github.com/apache/spark/pull/38206];;;",,,,,,,,,,,,,,,
"""sbt packageBin"" fails in cygwin or other windows bash session",SPARK-40739,13485497,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,philwalk,philwalk,philwalk,10/Oct/22 20:34,24/Oct/22 13:29,13/Jul/23 08:51,24/Oct/22 13:29,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,Build,Windows,,0,bash,cygwin,mingw,"msys2,",windows," In a Windows _*SHELL*_ environment, such as _*cygwin*_ or {_}*msys2/mingw64*{_}, etc,  _*Core.settings*_ in _*project/SparkBuild.scala*_ calls the wrong _*bash.exe*_ if WSL bash is present (typically at {_}*C:\Windows*{_}), causing a build failure.  This occurs even though the proper *bash.exe* is in the _*PATH*_ ahead of _*WSL*_ bash.exe.

This is fixed by [spark PR 38228|https://github.com/apache/spark/pull/38167]

There are 3 parts to the fix, implemented in _*project/SparkBuild.scala*_ :
 * determine the absolute path of the first bash.exe in the command line. 
 * determine the build environment (e.g., Linux, Darwin, CYGWIN, MSYS2, etc.)
 * For Windows SHELL environments, the first argument to the spawned Process is changed from ""bash"" to the absolute path.","The problem occurs in Windows if *_sbt_* is started from a (non-WSL) bash session.

See the spark PR link for detailed symptoms.",apachespark,philwalk,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,38228,https://github.com/apache/spark/pull/38228,,Important,,,,,,,,9223372036854775807,,,bash,java,,Mon Oct 24 13:29:35 UTC 2022,,,,,,,,,,"0|z198ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Oct/22 03:21;apachespark;User 'philwalk' has created a pull request for this issue:
https://github.com/apache/spark/pull/38167;;;","11/Oct/22 03:22;apachespark;User 'philwalk' has created a pull request for this issue:
https://github.com/apache/spark/pull/38167;;;","13/Oct/22 02:16;apachespark;User 'philwalk' has created a pull request for this issue:
https://github.com/apache/spark/pull/38228;;;","24/Oct/22 13:29;srowen;Issue resolved by pull request 38228
[https://github.com/apache/spark/pull/38228];;;",,,,,,,,,,,,,,
"spark-shell fails with ""bad array subscript"" in cygwin or msys bash session",SPARK-40738,13485496,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,philwalk,philwalk,philwalk,10/Oct/22 20:15,24/Oct/22 13:30,13/Jul/23 08:51,24/Oct/22 13:30,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Shell,Windows,,0,bash,cygwin,mingw,"msys2,",windows,"A spark pull request [spark PR|https://github.com/apache/spark/pull/38167] fixes this issue, and also fixes a build error that is also related to _*cygwin*_  and *msys/mingw* bash *sbt* sessions.

If a Windows user tries to start a *_spark-shell_* session by calling the bash script (rather than the *_spark-shell.cmd_* script), it fails with a confusing error message.  Script _*spark-class*_ calls _*launcher/src/main/java/org/apache/spark/launcher/Main.java* to_ generate command line arguments, but the launcher produces a format appropriate to the *_.cmd_* version of the script rather than the _*bash*_ version.

The launcher Main method, when called for environments other than Windows, interleaves NULL characters between the command line arguments.   It should also do so in Windows when called from the bash script.  It incorrectly assumes that if the OS is Windows, that it is being called by the .cmd version of the script.

The resulting error message is unhelpful:

 
{code:java}
[lots of ugly stuff omitted]
/opt/spark/bin/spark-class: line 100: CMD: bad array subscript
{code}
The key to _*launcher/Main*_ knowing that a request is from a _*bash*_ session is that the _*SHELL*_ environment variable is set.   This will normally be set in any of the various Windows shell environments ({_}*cygwin*{_}, {_}*mingw64*{_}, {_}*msys2*{_}, etc) and will not normally be set in Windows environments.   In the _*spark-class.cmd*_ script, _*SHELL*_ is intentionally unset to avoid problems, and to permit bash users to call the _*.cmd*_ scripts if they prefer (it will still work as before).

 ","The problem occurs in Windows if *_spark-shell_* is called from a bash session.

NOTE: the fix also applies to _*spark-submit*_ and and {_}*beeline*{_}, since they call spark-shell.",apachespark,philwalk,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,38228,https://github.com/apache/spark/pull/38228,,Important,,,,,,,,9223372036854775807,,,bash,java,,Mon Oct 24 13:30:26 UTC 2022,,,,,,,,,,"0|z198dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Oct/22 03:22;apachespark;User 'philwalk' has created a pull request for this issue:
https://github.com/apache/spark/pull/38167;;;","11/Oct/22 03:22;apachespark;User 'philwalk' has created a pull request for this issue:
https://github.com/apache/spark/pull/38167;;;","13/Oct/22 02:16;apachespark;User 'philwalk' has created a pull request for this issue:
https://github.com/apache/spark/pull/38228;;;","24/Oct/22 13:30;srowen;Issue resolved by pull request 38228
[https://github.com/apache/spark/pull/38228];;;",,,,,,,,,,,,,,
Issue with spark converting Row to Json using Scala 2.13,SPARK-40705,13485192,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Amraneze,Amraneze,Amraneze,07/Oct/22 17:42,10/Oct/22 15:19,13/Jul/23 08:51,10/Oct/22 15:19,3.3.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,,,,,,"h2. *Note: This issue can be reproduced only using Scala 2.13*

When I'm trying to convert the Row to a json to publish it, i'm getting this following error

!image-2022-10-07-19-43-42-232.png!

I tried to investigate and I found that the issue is in the matching.

!image-2022-10-07-19-43-51-892.png!

The type `ArraySeq` is not matched in `Row` class.

!image-2022-10-07-19-44-00-332.png!

This is the definition of my field

!image-2022-10-07-19-44-09-972.png!

And an example of it 

 
{code:json}
{
...
Codes: [""Test"", ""Spark"", ""Json""]
...
}{code}
The Scala version I'm using is `2.13.9`",,Amraneze,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/22 17:43;Amraneze;image-2022-10-07-19-43-42-232.png;https://issues.apache.org/jira/secure/attachment/13050146/image-2022-10-07-19-43-42-232.png","07/Oct/22 17:43;Amraneze;image-2022-10-07-19-43-51-892.png;https://issues.apache.org/jira/secure/attachment/13050147/image-2022-10-07-19-43-51-892.png","07/Oct/22 17:44;Amraneze;image-2022-10-07-19-44-00-332.png;https://issues.apache.org/jira/secure/attachment/13050148/image-2022-10-07-19-44-00-332.png","07/Oct/22 17:44;Amraneze;image-2022-10-07-19-44-09-972.png;https://issues.apache.org/jira/secure/attachment/13050149/image-2022-10-07-19-44-09-972.png",,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,,,Mon Oct 10 15:19:39 UTC 2022,,,,,,,,,,"0|z196ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Oct/22 22:17;apachespark;User 'Amraneze' has created a pull request for this issue:
https://github.com/apache/spark/pull/38154;;;","07/Oct/22 22:17;apachespark;User 'Amraneze' has created a pull request for this issue:
https://github.com/apache/spark/pull/38154;;;","10/Oct/22 15:19;srowen;Issue resolved by pull request 38154
[https://github.com/apache/spark/pull/38154];;;",,,,,,,,,,,,,,,
Performance regression for joins in Spark 3.3 vs Spark 3.2,SPARK-40703,13485166,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,bryanck,bryanck,07/Oct/22 14:38,15/Oct/22 02:22,13/Jul/23 08:51,15/Oct/22 02:22,3.3.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,,,,,,"When running the TPC-DS benchmarks using a DSv2 datasource in Spark 3.3, a performance regression vs Spark 3.2 was discovered. More specifically, it appears as if {_}EnsureRequirements.ensureDistributionAndOrdering{_}() no longer enforces a minimum number of partitions for a join distribution in some cases. This impacts DSv2 datasources, because if a scan has only a single read partition {_}DataSourceV2ScanExecBase.outputPartitioning{_}() returns a _SinglePartition_ instance. The _SinglePartition_ creates a {_}SinglePartitionShuffleSpec{_}, and {_}SinglePartitionShuffleSpec.canCreatePartitioning{_}() returns true.

Because {_}canCreatePartitioning{_}() returns true in this case, {_}EnsureRequirements.ensureDistributionAndOrdering{_}() won't enforce minimum parallelism and also will favor the single partition when considering the best distribution candidate. Ultimately this results in a single partition being selected for the join distribution, even if the other side of the join is a large table with many partitions. This can seriously impact performance of the join.

Spark 3.2 enforces minimum parallelism differently in {_}ensureDistributionAndOrdering{_}() and thus does not suffer from this issue. It will shuffle both sides of the join to enforce parallelism.

In the TPC-DS benchmark, some queries affected include 14a and 14b. This can also be demonstrated using a simple query, for example:

{{select ics.i_item_sk from catalog_sales cs join item ics on cs.cs_item_sk = ics.i_item_sk}}

...where _item_ is a small table that is read into one partition, and _catalog_sales_ is a large table. These tables are part of the TPC-DS but you can create your own. Also, to demonstrate the issue, you may need to turn off broadcast joins though that is not required for this issue to occur, it happens when running the TPC-DS with broadcast setting at default.

Attached is the plan for this query in Spark 3.2 and in Spark 3.3. The plan shows how in Spark 3.2, the join parallelism of 200 is reached by inserting an exchange after the item table scan. In Spark 3.3, no such exchange is inserted and the join parallelism is 1.",,apachespark,bryanck,csun,melin,rajesh.balamohan,singhpk234,sunchao,ulysses,viirya,yumwang,,,,,,,,,,,,SPARK-35703,,,,,,"07/Oct/22 14:51;bryanck;spark32-plan.txt;https://issues.apache.org/jira/secure/attachment/13050142/spark32-plan.txt","07/Oct/22 14:51;bryanck;spark33-plan.txt;https://issues.apache.org/jira/secure/attachment/13050143/spark33-plan.txt","09/Oct/22 16:16;bryanck;test.py;https://issues.apache.org/jira/secure/attachment/13050196/test.py",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Oct 15 02:22:45 UTC 2022,,,,,,,,,,"0|z196co:",9223372036854775807,,,,,,,,,,,,,3.3.1,,,,,,,,,"07/Oct/22 18:08;sunchao;cc [~yumwang];;;","07/Oct/22 18:12;sunchao;[~bryanck] In the above case, what is the partition spec for the other side of the join? Normally Spark should pick the partition spec with the most number of partitions and use that to shuffle the other side. ;;;","07/Oct/22 18:34;bryanck;[~sunchao] , the other side is HashPartitioning with num partitions of 200. There is a check shouldConsiderMinParallelism which ends up being false, because the SinglePartition returns true for canCreatePartitioing. The canCreatePartitioning also results in the SinglePartition being selected as the best candidate.;;;","07/Oct/22 18:38;bryanck;I tried a fix/hack that returns an UnknownPartitioning(0) instead of SinglePartition, and that fixes this issue, but not sure if that has adverse effects elsewhere.;;;","07/Oct/22 18:44;bryanck;DSv1 DataSourceScanExec returns UnknownPartitioning(0) for non-bucketed scans, and I believe this is why DSv1 isn't impacted;;;","07/Oct/22 22:00;csun;I see. The reason HashPartitioning is not picked as the best candidate in this case could be that {{spark.sql.requireAllClusterKeysForCoPartition}} is turned on by default, so its {{canCreatePartitioing}} returns false. One idea is to try setting {{spark.sql.requireAllClusterKeysForCoPartition}} to false.;;;","07/Oct/22 22:06;bryanck;I gave that a try, but setting {{spark.sql.requireAllClusterKeysForCoPartition=false}} doesn't seem to help, I still am seeing a single partition selected for the join;;;","07/Oct/22 22:28;csun;Hmm interesting. Let me try to come up with a unit test and check what happened underneath. ;;;","07/Oct/22 22:29;csun;(one idea is that {{SinglePartitionSpec#canCreatePartitioing}} should also consider {{spark.sql.requireAllClusterKeysForCoPartition}} instead of always returning true);;;","07/Oct/22 22:53;bryanck;Wouldn't that have the same problem if {{spark.sql.requireAllClusterKeysForCoPartition}} is set?;;;","07/Oct/22 23:08;csun;Hmm somehow in the unit test I was able to see that changing {{spark.sql.requireAllClusterKeysForCoPartition}} to false fixed the issue:
{code:java}
  test(""EnsureRequirements should respect spark.sql.shuffle.partitions with SinglePartition"") {
    withSQLConf(SQLConf.SHUFFLE_PARTITIONS.key -> 10.toString) {
      val plan1: SparkPlan = DummySparkPlan(
        outputPartitioning = HashPartitioning(exprA :: Nil, 15))
      val plan2 = DummySparkPlan(outputPartitioning = SinglePartition)
      val smjExec = SortMergeJoinExec(
        exprA :: exprB :: Nil, exprC :: exprD :: Nil, Inner, None, plan1, plan2)
      applyEnsureRequirementsWithSubsetKeys(smjExec) match {
        case SortMergeJoinExec(_, _, _, _,
        SortExec(_, _, DummySparkPlan(_, _, left: HashPartitioning, _, _), _),
        SortExec(_, _, ShuffleExchangeExec(right: HashPartitioning, _, _), _), _) =>
          assert(left.expressions === Seq(exprA))
          assert(right.expressions === Seq(exprC))
          assert(left.numPartitions == 15)
          assert(right.numPartitions == 15)
        case other => fail(other.toString)
      }
    }
  }
{code}

(This was added in {{EnsureRequirementsSuite}}.;;;","09/Oct/22 16:12;bryanck;[~csun] I added a pyspark script that shows the difference between DSv1 and DSv2, in case that helps.;;;","10/Oct/22 18:25;csun;Thanks [~bryanck] . Now I see where the issue is.

In your pyspark example, one side reports {{UnknownPartitioning}} while another side reports {{{}SinglePartition{}}}. Later on, Spark will insert shuffle for {{UnknownPartitioning}} so it becomes {{{}HashPartitioning{}}}. In this particular case, when Spark is deciding which side to insert shuffle, it'll pick the {{HashPartitioning}} again and convert it into the same {{HashPartitioning}} but with {{{}numPartitions = 1{}}}.

Before:
{code}
 ShuffleExchange(HashPartition(200))  <-->  SinglePartition
{code}
(suppose {{spark.sql.shuffle.partitions}} is 200)

After:
{code}
 ShuffleExchange(HashPartition(1))  <-->  SinglePartition
{code}
 
The reason Spark chooses to do in this way is because there is a trade-off between shuffle cost and parallelism. At the moment, when Spark sees that one side of the join has {{ShuffleExchange}} (meaning it needs to be shuffled anyways), and the other side doesn't, it'll try to avoid shuffling the other side. 

This makes more sense if we have:
{code}
ShuffleExchange(HashPartition(200)) <-> HashPartition(150)
{code}

as in this case, Spark will avoid shuffle the right hand side and instead just change the number of shuffle partitions on the left:
{code}
ShuffleExchange(HashPartition(150) <-> HashPartition(150)
{code}

I feel we can treat the {{SinglePartition}} as a special case here. Let me see if I can come up with a PR.;;;","10/Oct/22 18:32;bryanck;Sounds good, thanks.;;;","10/Oct/22 22:28;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/38196;;;","15/Oct/22 02:22;yumwang;Issue resolved by pull request 38196
[https://github.com/apache/spark/pull/38196];;;",,
Add permisson for infra image,SPARK-40696,13484912,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,07/Oct/22 02:57,12/Dec/22 18:10,13/Jul/23 08:51,07/Oct/22 04:28,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Project Infra,,,0,,,,,,,,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Oct 07 04:57:37 UTC 2022,,,,,,,,,,"0|z194sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Oct/22 03:57;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38144;;;","07/Oct/22 03:58;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38144;;;","07/Oct/22 04:28;gurwls223;Issue resolved by pull request 38144
[https://github.com/apache/spark/pull/38144];;;","07/Oct/22 04:57;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38145;;;",,,,,,,,,,,,,,
Add permisson for notify and status update job,SPARK-40695,13484908,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,07/Oct/22 02:00,12/Dec/22 18:10,13/Jul/23 08:51,07/Oct/22 02:06,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Project Infra,,,0,,,,,,,,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Oct 07 02:06:27 UTC 2022,,,,,,,,,,"0|z194rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Oct/22 02:04;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38141;;;","07/Oct/22 02:04;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38141;;;","07/Oct/22 02:06;gurwls223;Issue resolved by pull request 38141
[https://github.com/apache/spark/pull/38141];;;",,,,,,,,,,,,,,,
Add permisson for label github aciton job,SPARK-40694,13484906,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,07/Oct/22 01:07,12/Dec/22 18:10,13/Jul/23 08:51,07/Oct/22 02:05,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Project Infra,,,0,,,,,,,,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Oct 07 02:05:41 UTC 2022,,,,,,,,,,"0|z194r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Oct/22 01:10;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38138;;;","07/Oct/22 02:05;gurwls223;Issue resolved by pull request 38138
[https://github.com/apache/spark/pull/38138];;;",,,,,,,,,,,,,,,,
"NPE in applyInPandasWithState when the input schema has ""non-nullable"" column(s)",SPARK-40670,13484667,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,05/Oct/22 23:52,06/Oct/22 06:18,13/Jul/23 08:51,06/Oct/22 06:18,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Structured Streaming,,,0,,,,,,,,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Oct 06 06:18:16 UTC 2022,,,,,,,,,,"0|z193a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Oct/22 23:52;kabhwan;Will submit a PR soon.;;;","06/Oct/22 00:02;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/38115;;;","06/Oct/22 00:03;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/38115;;;","06/Oct/22 06:18;kabhwan;Issue resolved by pull request 38115
[https://github.com/apache/spark/pull/38115];;;",,,,,,,,,,,,,,
Switch to XORShiftRandom to distribute elements,SPARK-40660,13484529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,05/Oct/22 01:51,06/Oct/22 08:37,13/Jul/23 08:51,05/Oct/22 06:01,3.4.0,,,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,,,,,,"
{code:scala}
import java.util.Random
import org.apache.spark.util.random.XORShiftRandom
import scala.util.hashing

def distribution(count: Int, partition: Int) = {
  println((1 to count).map(partitionId => new Random(partitionId).nextInt(partition))
    .groupBy(f => f)
    .map(_._2.size).mkString("". ""))

  println((1 to count).map(partitionId => new Random(hashing.byteswap32(partitionId)).nextInt(partition))
    .groupBy(f => f)
    .map(_._2.size).mkString("". ""))

  println((1 to count).map(partitionId => new XORShiftRandom(partitionId).nextInt(partition))
    .groupBy(f => f)
    .map(_._2.size).mkString("". ""))
}

distribution(200, 4)
{code}


{noformat}
200
50. 60. 46. 44
55. 48. 43. 54
{noformat}
",,apachespark,cloud_fan,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Oct 05 13:47:55 UTC 2022,,,,,,,,,,"0|z192fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Oct/22 02:54;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/38106;;;","05/Oct/22 02:55;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/38106;;;","05/Oct/22 06:01;cloud_fan;Issue resolved by pull request 38106
[https://github.com/apache/spark/pull/38106];;;","05/Oct/22 13:46;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/38110;;;","05/Oct/22 13:47;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/38110;;;",,,,,,,,,,,,,
 Scala 2.12 + Hadoop 2 + JDK 8 Daily Test failed,SPARK-40635,13484178,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,02/Oct/22 14:43,12/Dec/22 18:10,13/Jul/23 08:51,05/Oct/22 08:40,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Tests,YARN,,0,,,,,,"[https://github.com/apache/spark/actions/runs/3164718086]

 
{code:java}
[error]     org.apache.spark.deploy.yarn.YarnShuffleAlternateNameConfigWithLevelDBBackendSuite
[error]     org.apache.spark.deploy.yarn.YarnShuffleIntegrationWithLevelDBBackendSuite
[error]     org.apache.spark.deploy.yarn.YarnClusterSuite
[error]     org.apache.spark.deploy.yarn.YarnShuffleAuthWithLevelDBBackendSuite
[error]     org.apache.spark.deploy.yarn.YarnShuffleAlternateNameConfigWithRocksDBBackendSuite
[error]     org.apache.spark.deploy.yarn.YarnShuffleIntegrationWithRocksDBBackendSuite
[error]     org.apache.spark.deploy.yarn.YarnShuffleAuthWithRocksDBBackendSuite {code}",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/22 15:36;LuciferYang;build-failed;https://issues.apache.org/jira/secure/attachment/13050027/build-failed",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Oct 05 08:40:55 UTC 2022,,,,,,,,,,"0|z1909s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Oct/22 15:21;LuciferYang;{code:java}
Exception message: Cannot run program ""bash"" (in directory ""/home/runner/work/spark/spark/resource-managers/yarn/target/org.apache.spark.deploy.yarn.YarnClusterSuite/org.apache.spark.deploy.yarn.YarnClusterSuite-localDir-nm-0_0/usercache/runner/appcache/application_1664721938509_0027/container_1664721938509_0027_02_000001""): error=7, Argument list too long
22096[info]   Stack trace: java.io.IOException: Cannot run program ""bash"" (in directory ""/home/runner/work/spark/spark/resource-managers/yarn/target/org.apache.spark.deploy.yarn.YarnClusterSuite/org.apache.spark.deploy.yarn.YarnClusterSuite-localDir-nm-0_0/usercache/runner/appcache/application_1664721938509_0027/container_1664721938509_0027_02_000001""): error=7, Argument list too long
22097[info]   	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
22098[info]   	at org.apache.hadoop.util.Shell.runCommand(Shell.java:526)
22099[info]   	at org.apache.hadoop.util.Shell.run(Shell.java:482)
22100[info]   	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776)
22101[info]   	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
22102[info]   	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
22103[info]   	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
22104[info]   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
22105[info]   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
22106[info]   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
22107[info]   	at java.lang.Thread.run(Thread.java:750)
22108[info]   Caused by: java.io.IOException: error=7, Argument list too long
22109[info]   	at java.lang.UNIXProcess.forkAndExec(Native Method)
22110[info]   	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
22111[info]   	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
22112[info]   	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
22113[info]   	... 10 more
22114[info]    {code};;;","02/Oct/22 15:42;LuciferYang;Manual test ： build/sbt ""yarn/test"" -Phadoop-2 -Pyarn and   mvn clean test -pl resource-managers/yarn -Phadoop-2 -Pyarn locally, but they all passed.

From the characteristics of the failed case, the am start failed because the start command line is too long, maybe due to the classpath contains too many jars(CLASSPATH and SPARK_DIST_CLASSPATH includes all jars in .cache);;;","03/Oct/22 02:26;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/38079;;;","05/Oct/22 08:40;gurwls223;Issue resolved by pull request 38079
[https://github.com/apache/spark/pull/38079];;;",,,,,,,,,,,,,,
Result of a single task in collect() must fit in 2GB,SPARK-40622,13484046,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liuzq12,liuzq12,liuzq12,30/Sep/22 17:54,16/Nov/22 02:57,13/Jul/23 08:51,16/Nov/22 02:57,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,SQL,,0,,,,,,"when collecting results, data from single partition/task is serialized through byte array or ByteBuffer(which is backed by byte array as well), therefore it's subject to java array max size limit(in terms of byte array, it's 2GB).

 

Construct a single partition larger than 2GB and collect it can easily reproduce the issue
{code:java}
// create data of size ~3GB in single partition, which exceeds the byte array limit
// random gen to make sure it's poorly compressed
val df = spark.range(0, 3000, 1, 1).selectExpr(""id"", s""genData(id, 1000000) as data"")

withSQLConf(""spark.databricks.driver.localMaxResultSize"" -> ""4g"") {
  withSQLConf(""spark.sql.useChunkedBuffer"" -> ""true"") {
    df.queryExecution.executedPlan.executeCollect()
  }
} {code}
 will get a OOM error from [https://github.com/AdoptOpenJDK/openjdk-jdk11/blob/master/src/java.base/share/classes/java/io/ByteArrayOutputStream.java#L125]

 

Consider using ChunkedByteBuffer to replace byte array in order to bypassing this limit",,apachespark,liuzq12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Nov 03 22:52:39 UTC 2022,,,,,,,,,,"0|z18zgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Oct/22 00:43;apachespark;User 'liuzqt' has created a pull request for this issue:
https://github.com/apache/spark/pull/38064;;;","03/Nov/22 22:52;apachespark;User 'liuzqt' has created a pull request for this issue:
https://github.com/apache/spark/pull/38505;;;","03/Nov/22 22:52;apachespark;User 'liuzqt' has created a pull request for this issue:
https://github.com/apache/spark/pull/38505;;;",,,,,,,,,,,,,,,
Bug in MergeScalarSubqueries rule attempting to merge nested subquery with parent,SPARK-40618,13483917,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dtenedor,dtenedor,dtenedor,29/Sep/22 23:42,04/Oct/22 08:23,13/Jul/23 08:51,03/Oct/22 18:25,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"There is a bug in the `MergeScalarSubqueries` rule for queries with subquery expressions nested inside each other, wherein the rule attempts to merge the nested subquery with its enclosing parent subquery. The result is not a valid plan and raises an exception in the optimizer. Here is a minimal reproducing case:
 
```
sql(""create table test(col int) using csv"")
checkAnswer(sql(""select(select sum((select sum(col) from test)) from test)""), Row(null))
```",,apachespark,dtenedor,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Oct 04 08:23:01 UTC 2022,,,,,,,,,,"0|z18yog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Sep/22 23:47;apachespark;User 'dtenedor' has created a pull request for this issue:
https://github.com/apache/spark/pull/38052;;;","03/Oct/22 18:25;Gengliang.Wang;Issue resolved by pull request 38052
[https://github.com/apache/spark/pull/38052];;;","04/Oct/22 08:23;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/38093;;;",,,,,,,,,,,,,,,
"Assertion failed in ExecutorMetricsPoller ""task count shouldn't below 0""",SPARK-40617,13483901,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,29/Sep/22 19:42,06/May/23 16:32,13/Jul/23 08:51,03/Oct/22 13:31,3.2.0,3.2.1,3.2.2,3.3.0,,,,,,,,,,,3.2.0,3.3.1,3.4.0,,,Spark Core,,,1,,,,,,"Spurious failures because of the assert:

{noformat}
22/09/29 09:46:24 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3063.0 in stage 1997.0 (TID 677249),5,main]
java.lang.AssertionError: assertion failed: task count shouldn't below 0
	at scala.Predef$.assert(Predef.scala:223)
	at org.apache.spark.executor.ExecutorMetricsPoller.decrementCount$1(ExecutorMetricsPoller.scala:130)
	at org.apache.spark.executor.ExecutorMetricsPoller.$anonfun$onTaskCompletion$3(ExecutorMetricsPoller.scala:135)
	at java.base/java.util.concurrent.ConcurrentHashMap.computeIfPresent(ConcurrentHashMap.java:1822)
	at org.apache.spark.executor.ExecutorMetricsPoller.onTaskCompletion(ExecutorMetricsPoller.scala:135)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:737)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
22/09/29 09:46:24 INFO MemoryStore: MemoryStore cleared
22/09/29 09:46:24 INFO BlockManager: BlockManager stopped
22/09/29 09:46:24 INFO ShutdownHookManager: Shutdown hook called
22/09/29 09:46:24 INFO ShutdownHookManager: Deleting directory /mnt/yarn/usercache/hadoop/appcache/application_1664443624160_0001/spark-93efc2d4-84de-494b-a3b7-2cb1c3a45426
{noformat}

",,apachespark,attilapiros,,,,,,,,,,,,,,,,,,,,,,SPARK-34779,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 03 13:42:56 UTC 2022,,,,,,,,,,"0|z18yl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Sep/22 03:41;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/38056;;;","30/Sep/22 03:41;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/38056;;;","03/Oct/22 13:31;attilapiros;Issue resolved by pull request 38056
https://github.com/apache/spark/pull/38056;;;","03/Oct/22 13:42;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/38083;;;","03/Oct/22 13:42;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/38083;;;",,,,,,,,,,,,,
On Kubernetes for long running app Spark using an invalid principal to renew the delegation token,SPARK-40612,13483837,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,29/Sep/22 12:44,16/Oct/22 08:49,13/Jul/23 08:51,30/Sep/22 21:53,3.0.0,3.0.1,3.0.2,3.0.3,3.1.0,3.1.1,3.1.2,3.1.3,3.2.0,3.2.1,3.2.2,3.3.0,,,3.2.3,3.3.1,3.4.0,,,Security,,,0,,,,,,When the delegation token fetched at the first time the principal is the current user but the subsequent token renewals are using a MapReduce/Yarn specific principal even on Kubernetes. ,,apachespark,attilapiros,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Sep 30 21:53:16 UTC 2022,,,,,,,,,,"0|z18y6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Sep/22 12:58;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/38048;;;","30/Sep/22 21:53;dongjoon;Issue resolved by pull request 38048
[https://github.com/apache/spark/pull/38048];;;",,,,,,,,,,,,,,,,
Sorting issue with partitioned-writing and AQE turned on,SPARK-40588,13483550,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,EnricoMi,swebask,swebask,27/Sep/22 22:19,10/Jan/23 16:46,13/Jul/23 08:51,09/Nov/22 08:02,3.1.3,,,,,,,,,,,,,,3.2.3,3.3.2,,,,Spark Core,,,0,correctness,,,,,"We are attempting to partition data by a few columns, sort by a particular _sortCol_ and write out one file per partition. 
{code:java}
df
    .repartition(col(""day""), col(""month""), col(""year""))
    .withColumn(""partitionId"",spark_partition_id)
    .withColumn(""monotonicallyIncreasingIdUnsorted"",monotonicallyIncreasingId)
    .sortWithinPartitions(""year"", ""month"", ""day"", ""sortCol"")
    .withColumn(""monotonicallyIncreasingIdSorted"",monotonicallyIncreasingId)
    .write
    .partitionBy(""year"", ""month"", ""day"")
    .parquet(path){code}
When inspecting the results, we observe one file per partition, however we see an _alternating_ pattern of unsorted rows in some files.
{code:java}
{""sortCol"":100000,""partitionId"":732,""monotonicallyIncreasingIdUnsorted"":6287832121344,""monotonicallyIncreasingIdSorted"":6287832121344}
{""sortCol"":1303413,""partitionId"":732,""monotonicallyIncreasingIdUnsorted"":6287877022389,""monotonicallyIncreasingIdSorted"":6287876860586}
{""sortCol"":100000,""partitionId"":732,""monotonicallyIncreasingIdUnsorted"":6287877567881,""monotonicallyIncreasingIdSorted"":6287832121345}
{""sortCol"":1303413,""partitionId"":732,""monotonicallyIncreasingIdUnsorted"":6287835105553,""monotonicallyIncreasingIdSorted"":6287876860587}
{""sortCol"":100000,""partitionId"":732,""monotonicallyIncreasingIdUnsorted"":6287832570127,""monotonicallyIncreasingIdSorted"":6287832121346}
{""sortCol"":1303413,""partitionId"":732,""monotonicallyIncreasingIdUnsorted"":6287879965760,""monotonicallyIncreasingIdSorted"":6287876860588}
{""sortCol"":100000,""partitionId"":732,""monotonicallyIncreasingIdUnsorted"":6287878762347,""monotonicallyIncreasingIdSorted"":6287832121347}
{""sortCol"":1303413,""partitionId"":732,""monotonicallyIncreasingIdUnsorted"":6287837165012,""monotonicallyIncreasingIdSorted"":6287876860589}
{""sortCol"":100000,""partitionId"":732,""monotonicallyIncreasingIdUnsorted"":6287832910545,""monotonicallyIncreasingIdSorted"":6287832121348}
{""sortCol"":1303413,""partitionId"":732,""monotonicallyIncreasingIdUnsorted"":6287881244758,""monotonicallyIncreasingIdSorted"":6287876860590}
{""sortCol"":100000,""partitionId"":732,""monotonicallyIncreasingIdUnsorted"":6287880041345,""monotonicallyIncreasingIdSorted"":6287832121349}{code}
Here is a [gist|https://gist.github.com/Swebask/543030748a768be92d3c0ae343d2ae89] to reproduce the issue. 

Turning off AQE with spark.conf.set(""spark.sql.adaptive.enabled"", false) fixes the issue.

I'm working on identifying why AQE affects the sort order. Any leads or thoughts would be appreciated!","Spark v3.1.3
Scala v2.12.13",apachespark,cloud_fan,EnricoMi,kristopherkane,swebask,xkrogen,Zing,,,,,,,,,,,,,,,,,,,,,"16/Oct/22 14:05;Zing;image-2022-10-16-22-05-47-159.png;https://issues.apache.org/jira/secure/attachment/13050990/image-2022-10-16-22-05-47-159.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jan 10 16:46:10 UTC 2023,,,,,,,,,,"0|z18wfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Oct/22 14:09;Zing;After my test, I think this is not a problem of AQE, because the reproduced code I used, and after setting spark.sql.adaptive.enabled to false, the sort still does not take effect.

 

!image-2022-10-16-22-05-47-159.png!

It can be reproduced by modifying a few parameters and running in spark local:

```
val partitions = 200
val minRand = 100
val maxRand = 300
```

The real problem seems to be in the sort + partitionBy operation.;;;","20/Oct/22 11:28;EnricoMi;Here is a more concise and complete example to reproduce this issue.

Run this with 512m memory and one executor, e.g.:
{code:bash}
spark-shell --driver-memory 512m --master ""local[1]""
{code}
{code:scala}
import org.apache.spark.sql.SaveMode

spark.conf.set(""spark.sql.adaptive.enabled"", true)

val ids = 2000000
val days = 2
val parts = 2

val ds = spark.range(0, days, 1, parts).withColumnRenamed(""id"", ""day"").join(spark.range(0, ids, 1, parts))

ds.repartition($""day"")
  .sortWithinPartitions($""day"", $""id"")
  .write
  .partitionBy(""day"")
  .mode(SaveMode.Overwrite)
  .csv(""interleaved.csv"")
{code}
Check the written files are sorted (states {{OK}} when file is sorted):
{code:bash}
for file in interleaved.csv/day\=*/part-*
do
  echo ""$(sort -n ""$file"" | md5sum | cut -d "" "" -f 1)  $file""
done | md5sum -c
{code}
Files are not sorted for Spark 3.0.x, 3.1.x, 3.2.x and 3.3.x. Current master (3.4.0) seems to be fixed.;;;","22/Oct/22 13:17;EnricoMi;Even with AQE enabled (pre Spark 3.4.0), the written files are sorted {*}unless spilling occurs{*}.

The reason is that {{FileFormatWriter}} defines a {{requiredOrdering}} as:

[https://github.com/apache/spark/blob/f74867bddfbcdd4d08076db36851e88b15e66556/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala#L188-L189]
{code:java}
val requiredOrdering = partitionColumns ++ writerBucketSpec.map(_.bucketIdExpression) ++ sortColumns
{code}
Where {{partitionColumns}} refers to {{.write.partitionBy}} and {{sortColumns}} refers to {{.write.sortBy}}, so {{[""year"", ""month"", ""day""]}} in your case.

It enforces that ordering if the DataFrame is not sorted accordingly. With AQE enabled (pre Spark 3.4), {{FileFormatWriter}} does not know about the existing ordering and introduces the sorting. This reads the sorted DataFrame (sorted by {{[""year"", ""month"", ""day"", ""sortCol""]}}) and sorts it by {{[""year"", ""month"", ""day""]}}. If the partition has to be spilled to RAM or disk, it round-robins over the spills, because they are all ""equal"" w.r.t. {{[""year"", ""month"", ""day""]}}, as all next data in the spill files of a partition have the same values for these columns (only {{sortCol}} differs, but that is not considered by this sort). Hence the order is broken by spilling.;;;","23/Oct/22 13:21;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38358;;;","01/Nov/22 20:37;swebask;I have also tested the changes against branch-3.3 and can confirm the sorting behaviour is now fixed. ;;;","09/Nov/22 08:02;cloud_fan;Issue resolved by pull request 38358
[https://github.com/apache/spark/pull/38358];;;","05/Jan/23 21:06;EnricoMi;Unfortunately, this issue persists with Spark 3.4.0, created issue SPARK-41914 to track that.;;;","10/Jan/23 16:46;xkrogen;Labeling with 'correctness' since this breaks correctness of output by breaking the sort ordering.;;;",,,,,,,,,,
"Documentation error in ""Integration with Cloud Infrastructures""",SPARK-40583,13483485,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,danitico,danitico,danitico,27/Sep/22 15:31,16/Oct/22 08:50,13/Jul/23 08:51,27/Sep/22 22:26,3.3.0,,,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,Documentation,,,0,,,,,,"The artifactId that implements the integration with several cloud infrastructures is wrong. Instead of ""hadoop-cloud-\{SCALA_VERSION}"", it should say ""spark-hadoop-cloud-\{SCALA_VERSION}"".",,apachespark,danitico,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Sep 28 20:17:32 UTC 2022,,,,,,,,,,"0|z18w0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Sep/22 15:37;apachespark;User 'danitico' has created a pull request for this issue:
https://github.com/apache/spark/pull/38021;;;","27/Sep/22 22:26;dongjoon;Issue resolved by pull request 38021
[https://github.com/apache/spark/pull/38021];;;","28/Sep/22 20:17;dongjoon;I updated the Fix Version from 3.3.1 to 3.3.2 because 3.3.1 RC2 vote start without this.;;;",,,,,,,,,,,,,,,
Non-deterministic filters shouldn't get pushed to V2 file sources,SPARK-40565,13483202,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kimahriman,kimahriman,kimahriman,26/Sep/22 12:56,07/Oct/22 05:57,13/Jul/23 08:51,07/Oct/22 05:57,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"Currently non-deterministic filters can be pushed down to V2 file sources, which is different from V1 which prevents out non-deterministic filters from being pushed.

Main consequences:
 * Things like doing a rand filter on a partition column will throw an exception:
 ** {{IllegalArgumentException: requirement failed: Nondeterministic expression org.apache.spark.sql.catalyst.expressions.Rand should be initialized before eval.}}
 * {{Using a non-deterministic UDF to collect metrics via accumulators gets pushed down and gives the wrong metrics}}",,apachespark,dongjoon,kimahriman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Oct 07 05:57:19 UTC 2022,,,,,,,,,,"0|z18ubc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Sep/22 13:04;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/38003;;;","07/Oct/22 05:57;dongjoon;Issue resolved by pull request 38003
[https://github.com/apache/spark/pull/38003];;;",,,,,,,,,,,,,,,,
"Error at where clause, when sql case executes by else branch",SPARK-40563,13483175,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ebious,ebious,26/Sep/22 09:55,12/Dec/22 18:11,13/Jul/23 08:51,30/Sep/22 03:53,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"Hello!

The Spark SQL phase optimization failed with an internal error. Please, fill a bug report in, and provide the full stack trace.
 - Spark verison 3.3.0
 - Scala version 2.12
 - DatasourceV2
 - Postgres
 - Postrgres JDBC Driver: 42+
 - Java8

Case:

select
    case
        when (t_name = 'foo') then 'foo'
        else 'default'
    end as case_when
from
    t
where
    case
        when (t_name = 'foo') then 'foo'
        else 'default'
    end *= 'foo';  -> works as expected*

*----------------------------------------------------------*

select
    case
        when (t_name = 'foo') then 'foo'
        else 'default'
    end as case_when
from
    t
where
    case
        when (t_name = 'foo') then 'foo'
        else 'default'
    end *= 'default'; -> query throw ex;*

In where clause when we try find rows by else branch, spark thrown exception:
The Spark SQL phase optimization failed with an internal error. Please, fill a bug report in, and provide the full stack trace.

Caused by: java.lang.AssertionError: assertion failed
    at scala.Predef$.assert(Predef.scala:208)

 org.apache.spark.sql.execution.datasources.v2.PushablePredicate.$anonfun$unapply$1(DataSourceV2Strategy.scala:589)

At debugger def unapply in PushablePredicate.class
when sql case return 'foo' -> function unapply accept: (t_name = 'foo'), as instance of Predicate
when sql case return 'default' -> function unapply accept: COALESCE(t_name = 'foo', FALSE) as instance of GeneralScalarExpression and assertation failed with error

 ",,ebious,yumwang,Zing,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/22 10:59;ebious;java-code-example.txt;https://issues.apache.org/jira/secure/attachment/13049761/java-code-example.txt","26/Sep/22 10:56;ebious;sql.txt;https://issues.apache.org/jira/secure/attachment/13049760/sql.txt","26/Sep/22 09:59;ebious;stack-trace.txt;https://issues.apache.org/jira/secure/attachment/13049749/stack-trace.txt",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Oct 18 02:00:18 UTC 2022,,,,,,,,,,"0|z18u5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Sep/22 15:09;Zing;I can reproduce this problem. I can try to fix this issue if no one else is working on it . :);;;","27/Sep/22 02:28;gurwls223;Please go ahead.;;;","30/Sep/22 03:52;Zing;In my test , the issue has been fixed in the latest master branch.;;;","30/Sep/22 07:42;ebious;[~Zing] 

Our respect, thanks for the help!;;;","16/Oct/22 08:58;yumwang;[~Zing] Does branch-3.3 also fixed this issue?;;;","17/Oct/22 15:48;Zing;[~yumwang] 

The problem was not fixed in branch-3.3.;;;","18/Oct/22 02:00;yumwang;Thank you [~Zing];;;",,,,,,,,,,,
Add spark.sql.legacy.groupingIdWithAppendedUserGroupBy,SPARK-40562,13483168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,26/Sep/22 09:26,16/Oct/22 08:50,13/Jul/23 08:51,27/Sep/22 09:17,3.2.3,3.3.1,3.4.0,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,,,,,,"{code:java}
scala> sql(""SELECT count(*), grouping__id from (VALUES (1,1,1),(2,2,2)) AS t(k1,k2,v) GROUP BY k1 GROUPING SETS (k2) "").show()
+--------+------------+
|count(1)|grouping__id|
+--------+------------+
|       1|           2|
|       1|           2|
+--------+------------+

scala> sql(""set spark.sql.legacy.groupingIdWithAppendedUserGroupBy=true"")
res1: org.apache.spark.sql.DataFrame = [key: string, value: string]scala> 

sql(""SELECT count(*), grouping__id from (VALUES (1,1,1),(2,2,2)) AS t(k1,k2,v) GROUP BY k1 GROUPING SETS (k2) "").show()
+--------+------------+
|count(1)|grouping__id|
+--------+------------+
|       1|           1|
|       1|           1|
+--------+------------+ {code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,SPARK-40218,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Sep 28 20:17:25 UTC 2022,,,,,,,,,,"0|z18u3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Sep/22 09:30;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38001;;;","26/Sep/22 09:31;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38001;;;","27/Sep/22 09:17;dongjoon;Issue resolved by pull request 38001
[https://github.com/apache/spark/pull/38001];;;","28/Sep/22 20:17;dongjoon;I updated the Fix Version from 3.3.1 to 3.3.2 because 3.3.1 RC2 vote start without this.;;;",,,,,,,,,,,,,,
NPE from observe of collect_list,SPARK-40535,13482769,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,maxgekk,maxgekk,22/Sep/22 13:03,16/Oct/22 08:52,13/Jul/23 08:51,23/Sep/22 11:24,3.4.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,,,,,,"The code below reproduces the issue:

{code:scala}
import org.apache.spark.sql.functions._
val df = spark.range(1,10,1,11)
df.observe(""collectedList"", collect_list(""id"")).collect()
{code}
instead of

{code}
Array(1, 2, 3, 4, 5, 6, 7, 8, 9)
{code}
it fails with the NPE:

{code:java}
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.getBufferObject(interfaces.scala:641)
	at org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.getBufferObject(interfaces.scala:602)
	at org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.serializeAggregateBufferInPlace(interfaces.scala:624)
	at org.apache.spark.sql.execution.AggregatingAccumulator.withBufferSerialized(AggregatingAccumulator.scala:205)
	at org.apache.spark.sql.execution.AggregatingAccumulator.withBufferSerialized(AggregatingAccumulator.scala:33)
{code}
",,apachespark,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Sep 28 20:16:31 UTC 2022,,,,,,,,,,"0|z18rpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Sep/22 05:52;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/37977;;;","23/Sep/22 05:53;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/37977;;;","23/Sep/22 11:24;maxgekk;Issue resolved by pull request 37977
[https://github.com/apache/spark/pull/37977];;;","28/Sep/22 20:16;dongjoon;I updated the Fix Version from 3.3.1 to 3.3.2 because 3.3.1 RC2 vote start without this.;;;",,,,,,,,,,,,,,
PartitionsAlreadyExistException in Hive V1 Command V1 reports all partitions instead of the conflicting partition,SPARK-40521,13482597,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maxgekk,srielau,srielau,21/Sep/22 14:27,07/Oct/22 12:40,13/Jul/23 08:51,07/Oct/22 12:40,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,"PartitionsAlreadyExistException in Hive V1 Command V1 reports all partitions instead of the conflicting partition

When I run:
AlterTableAddPartitionSuiteBase for Hive
The test: partition already exists
Fails in my my local build ONLY in that mode because it reports two partitions as conflicting where there should be only one. In all other modes the test succeeds.
The test is passing on master because the test does not check the partitions themselves.

Repro on master: Note that c1 = 1 does not already exist. It should NOT be listed 

create table t(c1 int, c2 int) partitioned by (c1);

alter table t add partition (c1 = 2);

alter table t add partition (c1 = 1) partition (c1 = 2);

22/09/21 09:30:09 ERROR Hive: AlreadyExistsException(message:Partition already exists: Partition(values:[2], dbName:default, tableName:t, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:c2, type:int, comment:null)], location:file:/Users/serge.rielau/spark/spark-warehouse/t/c1=2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:\{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null))

 at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.startAddPartition(HiveMetaStore.java:2744)

 at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.add_partitions_core(HiveMetaStore.java:2442)

 at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.add_partitions_req(HiveMetaStore.java:2560)

 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

 at java.base/java.lang.reflect.Method.invoke(Method.java:566)

 at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)

 at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)

 at com.sun.proxy.$Proxy31.add_partitions_req(Unknown Source)

 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.add_partitions(HiveMetaStoreClient.java:625)

 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

 at java.base/java.lang.reflect.Method.invoke(Method.java:566)

 at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)

 at com.sun.proxy.$Proxy32.add_partitions(Unknown Source)

 at org.apache.hadoop.hive.ql.metadata.Hive.createPartitions(Hive.java:2103)

 at org.apache.spark.sql.hive.client.Shim_v0_13.createPartitions(HiveShim.scala:763)

 at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createPartitions$1(HiveClientImpl.scala:631)

 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

 at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:296)

 at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)

 at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)

 at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:276)

 at org.apache.spark.sql.hive.client.HiveClientImpl.createPartitions(HiveClientImpl.scala:624)

 at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createPartitions$1(HiveExternalCatalog.scala:1039)

 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

 at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)

 at org.apache.spark.sql.hive.HiveExternalCatalog.createPartitions(HiveExternalCatalog.scala:1021)

 at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createPartitions(ExternalCatalogWithListener.scala:201)

 at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createPartitions(SessionCatalog.scala:1169)

 at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.$anonfun$run$17(ddl.scala:514)

 at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.$anonfun$run$17$adapted(ddl.scala:513)

 at scala.collection.Iterator.foreach(Iterator.scala:943)

 at scala.collection.Iterator.foreach$(Iterator.scala:943)

 at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)

 at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.run(ddl.scala:513)

 at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)

 at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)

 at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)

 at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)

 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:111)

 at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:171)

 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)

 at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)

 at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)

 at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)

 at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)

 at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)

 at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)

 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)

 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)

 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)

 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)

 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)

 at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)

 at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)

 at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)

 at org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)

...

 

*The following partitions already exists in table 't' database 'default':*

{color:#de350b}*Map(c1 -> 1)*{color}

{color:#de350b}*===*{color}

*Map(c1 -> 2)*

spark-sql> ",,apachespark,maxgekk,srielau,,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/22 17:09;srielau;Screen Shot 2022-09-21 at 10.08.44 AM.png;https://issues.apache.org/jira/secure/attachment/13049584/Screen+Shot+2022-09-21+at+10.08.44+AM.png","21/Sep/22 17:09;srielau;Screen Shot 2022-09-21 at 10.08.52 AM.png;https://issues.apache.org/jira/secure/attachment/13049583/Screen+Shot+2022-09-21+at+10.08.52+AM.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Oct 07 12:40:57 UTC 2022,,,,,,,,,,"0|z18qnc:",9223372036854775807,,,,,cloud_fan,,,,,,,,,,,,,,,,,"21/Sep/22 17:10;srielau;Hive does return the offending partition. We just need to dig it out  !Screen Shot 2022-09-21 at 10.08.44 AM.png!!Screen Shot 2022-09-21 at 10.08.52 AM.png!;;;","06/Oct/22 14:12;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/38134;;;","06/Oct/22 14:13;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/38134;;;","07/Oct/22 12:40;maxgekk;Issue resolved by pull request 38134
[https://github.com/apache/spark/pull/38134];;;",,,,,,,,,,,,,,
Treat unknown partitioning as UnknownPartitioning,SPARK-40508,13482455,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tedyu@apache.org,yuzhihong@gmail.com,yuzhihong@gmail.com,20/Sep/22 20:08,28/Sep/22 03:35,13/Jul/23 08:51,21/Sep/22 16:41,3.3.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,Spark Core,,,0,,,,,,"When running spark application against spark 3.3, I see the following :
{code}
java.lang.IllegalArgumentException: Unsupported data source V2 partitioning type: CustomPartitioning
    at org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioning$$anonfun$apply$1.applyOrElse(V2ScanPartitioning.scala:46)
    at org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioning$$anonfun$apply$1.applyOrElse(V2ScanPartitioning.scala:34)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
{code}
The CustomPartitioning works fine with Spark 3.2.1
This PR proposes to relax the code and treat all unknown partitioning the same way as that for UnknownPartitioning.",,apachespark,csun,dongjoon,sunchao,viirya,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,SPARK-37375,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Sep 21 17:47:45 UTC 2022,,,,,,,,,,"0|z18prs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Sep/22 20:11;apachespark;User 'tedyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37952;;;","21/Sep/22 16:41;csun;Issue resolved by pull request 37952
[https://github.com/apache/spark/pull/37952];;;","21/Sep/22 16:44;csun;[~dongjoon][~viirya] could you add [~yuzhihong@gmail.com] to the contributor list? I can't assign this to him.;;;","21/Sep/22 17:00;apachespark;User 'tedyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37957;;;","21/Sep/22 17:01;apachespark;User 'tedyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37957;;;","21/Sep/22 17:22;viirya;[~csun] Seems he is already in contributor list. I just assigned this ticket to him.;;;","21/Sep/22 17:25;sunchao;Oh, thanks [~viirya] ! For some reason the merge script was throwing error at me:
{code:java}

response text = {""errorMessages"":[],""errors"":{""assignee"":""User 'yuzhihong@gmail.com' cannot be assigned issues.""}}
Error assigning JIRA, try again (or leave blank and fix manually)
JIRA is unassigned, choose assignee
 {code};;;","21/Sep/22 17:46;dongjoon;Ya, the merge script sometimes hit the corner cases. BTW, [~sunchao] , you are already in the Apache Spark Admin group. You can add a user.

- [https://issues.apache.org/jira/plugins/servlet/project-config/SPARK/roles];;;","21/Sep/22 17:47;dongjoon;Previously, you were in `Contributor` and `Administrator`. I added `Committer` group to you additionally to make it sure.;;;","21/Sep/22 17:47;sunchao;Great to know. Thanks!;;;",,,,,,,,
"Configs to control ""enableDateTimeParsingFallback"" are incorrectly swapped",SPARK-40496,13482305,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,20/Sep/22 07:03,21/Sep/22 05:17,13/Jul/23 08:51,21/Sep/22 05:17,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,,,apachespark,cloud_fan,ivan.sadikov,,,,,,,,,,,,,,,,,,,,,,,SPARK-39731,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Sep 21 05:17:41 UTC 2022,,,,,,,,,,"0|z18ov4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Sep/22 07:31;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37942;;;","20/Sep/22 07:32;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37942;;;","21/Sep/22 05:17;cloud_fan;Issue resolved by pull request 37942
[https://github.com/apache/spark/pull/37942];;;",,,,,,,,,,,,,,,
"Revert ""[SPARK-33861][SQL] Simplify conditional in predicate""",SPARK-40493,13482286,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,20/Sep/22 05:23,20/Sep/22 05:39,13/Jul/23 08:51,20/Sep/22 05:26,3.2.0,3.2.1,3.2.2,3.3.0,,,,,,,,,,,3.2.3,3.3.2,,,,SQL,,,0,,,,,,Please see https://github.com/apache/spark/pull/30865#issuecomment-755285940 for more details.,,yumwang,,,,,,,,,,,,,,,,,,,,,SPARK-33861,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Sep 20 05:26:30 UTC 2022,,,,,,,,,,"0|z18oqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Sep/22 05:26;yumwang;Issue resolved by pull request 37729
https://github.com/apache/spark/pull/37729;;;",,,,,,,,,,,,,,,,,
Perform maintenance of StateStore instances when they become inactive,SPARK-40492,13482281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Chaoqin,Chaoqin,Chaoqin,20/Sep/22 04:39,26/Sep/22 02:45,13/Jul/23 08:51,26/Sep/22 02:45,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,Structured Streaming,,,0,,,,,,"Current the maintenance of StateStore is performed by a periodic task in the management thread. If a streaming query become inactive before the next maintenance task fire, its StateStore will be unloaded before cleanup.
There are 2 cases when a StateStore is unloaded.
 # StateStoreProvider is not longer active in the system, for example, when a query ends or the spark context terminates.
 # There is other active StateStoreProvider in the system, for example, when a partition is reassigned.

In case 1, we should do one last maintenance before unloading the instance.",,apachespark,Chaoqin,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Sep 26 02:45:05 UTC 2022,,,,,,,,,,"0|z18ops:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Sep/22 04:43;apachespark;User 'chaoqin-li1123' has created a pull request for this issue:
https://github.com/apache/spark/pull/37935;;;","20/Sep/22 04:44;apachespark;User 'chaoqin-li1123' has created a pull request for this issue:
https://github.com/apache/spark/pull/37935;;;","26/Sep/22 02:45;kabhwan;Issue resolved by pull request 37935
[https://github.com/apache/spark/pull/37935];;;",,,,,,,,,,,,,,,
Revert SPARK-24544 Print actual failure cause when look up function failed,SPARK-40482,13482083,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,cloud_fan,dongjoon,dongjoon,19/Sep/22 01:33,19/Sep/22 05:10,13/Jul/23 08:51,19/Sep/22 05:10,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,,,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Sep 19 05:10:22 UTC 2022,,,,,,,,,,"0|z18ni0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Sep/22 01:35;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/37896;;;","19/Sep/22 05:10;cloud_fan;Issue resolved by pull request 37896
[https://github.com/apache/spark/pull/37896];;;",,,,,,,,,,,,,,,,
Remove push-based shuffle data after query finished,SPARK-40480,13482060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wankun,wankun,wankun,18/Sep/22 14:20,08/Feb/23 08:58,13/Jul/23 08:51,14/Jan/23 10:24,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,Shuffle,,,0,,,,,,"Now spark will only cleanup shuffle data files except push-based shuffle files.
In our production cluster, push-based shuffle service will create too many shuffle merge data files as there are several spark thrift server.
Could we cleanup the merged data files after the query finished?",,apachespark,lyee,mridulm80,wankun,,,,,,,,,,,,,,,,,SPARK-38005,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jan 14 10:24:34 UTC 2023,,,,,,,,,,"0|z18ncw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Sep/22 14:27;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/37922;;;","18/Sep/22 14:28;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/37922;;;","14/Jan/23 10:24;mridulm80;Issue resolved by pull request 37922
[https://github.com/apache/spark/pull/37922];;;",,,,,,,,,,,,,,,
arrays_zip output unexpected alias column names when using GetMapValue and GetArrayStructFields,SPARK-40470,13481818,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,16/Sep/22 05:30,12/Dec/22 18:10,13/Jul/23 08:51,16/Sep/22 13:05,3.2.2,3.3.2,3.4.0,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,,,,,,"This is a follow-up for https://issues.apache.org/jira/browse/SPARK-40292. 

I forgot to fix the case when GetMapValue and GetArrayStructFields are used in arrays_zip function instead of structs.",,apachespark,ivan.sadikov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Sep 16 13:05:42 UTC 2022,,,,,,,,,,"0|z18lvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Sep/22 06:50;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37911;;;","16/Sep/22 13:05;gurwls223;Issue resolved by pull request 37911
[https://github.com/apache/spark/pull/37911];;;",,,,,,,,,,,,,,,,
Column pruning is not handled correctly in CSV when _corrupt_record is used,SPARK-40468,13481812,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,16/Sep/22 04:36,28/Sep/22 03:33,13/Jul/23 08:51,17/Sep/22 08:00,3.2.2,3.3.0,3.4.0,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,correctness,,,,,"I have found that depending on the name of the corrupt record in CSV, the field is populated incorrectly. Here is an example:
{code:java}
1,a > /tmp/file.csv

===

val df = spark.read
  .schema(""c1 int, c2 string, x string, _corrupt_record string"")
  .csv(""file:/tmp/file.csv"")
  .withColumn(""x"", lit(""A""))

Result:

+---+---+---+---------------+
|c1 |c2 |x  |_corrupt_record|
+---+---+---+---------------+
|1  |a  |A  |1,a            |
+---+---+---+---------------+{code}
 

However, if you rename the {{_corrupt_record}} column to something else, the result is different:
{code:java}
val df = spark.read 
  .option(""columnNameCorruptRecord"", ""corrupt_record"")
  .schema(""c1 int, c2 string, x string, corrupt_record string"") 
  .csv(""file:/tmp/file.csv"") .withColumn(""x"", lit(""A"")) 

Result:

+---+---+---+--------------+
|c1 |c2 |x  |corrupt_record|
+---+---+---+--------------+
|1  |a  |A  |null          |
+---+---+---+--------------+{code}
 

This is due to inconsistency in CSVFileFormat, when enabling columnPruning, we check SQLConf option for corrupt records but CSV reader relies on {{columnNameCorruptRecord}} option instead.

Also, this disables column pruning which used to work in Spark version prior to https://github.com/apache/spark/commit/959694271e30879c944d7fd5de2740571012460a.",,apachespark,ivan.sadikov,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Sep 17 08:00:54 UTC 2022,,,,,,,,,,"0|z18lu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Sep/22 05:21;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37909;;;","17/Sep/22 08:00;maxgekk;Issue resolved by pull request 37909
[https://github.com/apache/spark/pull/37909];;;",,,,,,,,,,,,,,,,
Streaming metrics is zero when select _metadata,SPARK-40460,13481781,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yaohua,yaohua,yaohua,16/Sep/22 00:34,28/Sep/22 03:34,13/Jul/23 08:51,19/Sep/22 05:34,3.3.0,3.3.1,3.3.2,3.4.0,,,,,,,,,,,3.3.1,3.4.0,,,,Structured Streaming,,,0,,,,,,"Streaming metrics report all 0 (`processedRowsPerSecond`, etc) when selecting `_metadata` column. Because the logical plan from the batch and the actual planned logical are mismatched: [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala#L348]",,apachespark,kabhwan,yaohua,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Sep 19 17:15:16 UTC 2022,,,,,,,,,,"0|z18lnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Sep/22 02:11;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/37905;;;","19/Sep/22 05:34;kabhwan;Issue resolved via https://github.com/apache/spark/pull/37905;;;","19/Sep/22 05:35;kabhwan;[~yaohua]
Just to clarify, streaming metadata column for DSv1 seems to be introduced in Spark 3.3. https://issues.apache.org/jira/browse/SPARK-38323 
Do I understand correctly? If then affect versions don't seem to be correct. ;;;","19/Sep/22 17:05;yaohua;[~kabhwan] You are right! Updated;;;","19/Sep/22 17:15;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/37932;;;",,,,,,,,,,,,,
Fix wrong reference and content in PS windows related doc,SPARK-40440,13481624,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,15/Sep/22 06:12,15/Sep/22 09:14,13/Jul/23 08:51,15/Sep/22 09:14,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Documentation,Pandas API on Spark,,0,,,,,,,,apachespark,podongfeng,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Sep 15 09:14:45 UTC 2022,,,,,,,,,,"0|z18kow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Sep/22 07:48;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37895;;;","15/Sep/22 09:14;podongfeng;Issue resolved by pull request 37895
[https://github.com/apache/spark/pull/37895];;;",,,,,,,,,,,,,,,,
Only set KeyGroupedPartitioning when the referenced column is in the output,SPARK-40429,13481573,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,huaxingao,huaxingao,huaxingao,14/Sep/22 20:19,28/Sep/22 03:32,13/Jul/23 08:51,15/Sep/22 06:07,3.3.0,3.4.0,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,,,,,,"
{code:java}
      sql(s""CREATE TABLE $tbl (id bigint, data string) PARTITIONED BY (id)"")
      sql(s""INSERT INTO $tbl VALUES (1, 'a'), (2, 'b'), (3, 'c')"")
      checkAnswer(
        spark.table(tbl).select(""index"", ""_partition""),
        Seq(Row(0, ""3""), Row(0, ""2""), Row(0, ""1""))
      )
{code}

failed with 
ScalaTestFailureLocation: org.apache.spark.sql.QueryTest at (QueryTest.scala:226)
org.scalatest.exceptions.TestFailedException: AttributeSet(id#994L) was not empty The optimized logical plan has missing inputs:
RelationV2[index#998, _partition#999] testcat.t
",,apachespark,dongjoon,huaxingao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Sep 15 16:09:43 UTC 2022,,,,,,,,,,"0|z18kdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Sep/22 20:26;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/37886;;;","15/Sep/22 06:07;dongjoon;This is resolved via [https://github.com/apache/spark/pull/37886];;;","15/Sep/22 16:09;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/37901;;;","15/Sep/22 16:09;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/37901;;;",,,,,,,,,,,,,,
Repartition of DataFrame can result in severe data skew in some special case,SPARK-40407,13481066,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wbo4958,wbo4958,wbo4958,12/Sep/22 08:38,06/Oct/22 16:57,13/Jul/23 08:51,22/Sep/22 13:00,3.0.1,3.1.1,3.1.2,3.1.3,3.2.1,3.2.2,3.3.0,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,,,,,,"{code:scala}
_val df = spark.range(0, 100, 1, 50).repartition(4)_
_val v = df.rdd.mapPartitions { iter => {_
        _Iterator.single(iter.length)_
{_}}{_}{_}.collect(){_}
_println(v.mkString("",""))_
{code}

The above simple code outputs `50,0,0,50`, which means there is no data in partition 1 and partition 2.

I just debugged it and found the RoundRobin seems to ensure to distribute the records evenly **in the same partition**, and not guarantee it between partitions.

Below is the code to generate the key

{code:scala}
      case RoundRobinPartitioning(numPartitions) =>
        // Distributes elements evenly across output partitions, starting from a random partition.
        var position = new Random(TaskContext.get().partitionId()).nextInt(numPartitions)  
        (row: InternalRow) => {
          // The HashPartitioner will handle the `mod` by the number of partitions
          position += 1
          position
        }
{code}

In this case, There are 50 partitions, each partition will only compute 2 elements. The issue for RoundRobin here is it always starts with *position=2* to do the Roundrobin.

See the output of Random
{code:scala}
scala> (1 to 200).foreach(partitionId => print(new Random(partitionId).nextInt(4) + "" ""))  // the position is always 2.
2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 
{code}

Similarly, the below Random code also outputs the same value, 

{code:scala}
(1 to 200).foreach(partitionId => print(new Random(partitionId).nextInt(2) + "" ""))

(1 to 200).foreach(partitionId => print(new Random(partitionId).nextInt(4) + "" ""))

(1 to 200).foreach(partitionId => print(new Random(partitionId).nextInt(8) + "" ""))

(1 to 200).foreach(partitionId => print(new Random(partitionId).nextInt(16) + "" ""))

(1 to 200).foreach(partitionId => print(new Random(partitionId).nextInt(32) + "" ""))
{code}

Let's go back to this case,

Consider partition 0, the total elements are [0, 1], so when shuffle writes, for element 0, the key will be (position + 1) = 2 + 1 = 3%4=3, the element 1, the key will be (position + 1)=(3+1)=4%4 = 0
consider partition 1, the total elements are [2, 3], so when shuffle writes, for element 2, the key will be (position + 1) = 2 + 1 = 3%4=3, the element 3, the key will be (position + 1)=(3+1)=4%4 = 0

 

The calculation is also applied for other left partitions since the starting position is always 2 for this case.

So, as you can see, each partition will write its elements to Partition [0, 3], which results in Partition [1, 2] without any data.

 

I will try to provide the patch to fix this issue.",,apachespark,cloud_fan,wbo4958,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Sep 22 13:00:34 UTC 2022,,,,,,,,,,"0|z18ha0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Sep/22 08:58;apachespark;User 'wbo4958' has created a pull request for this issue:
https://github.com/apache/spark/pull/37855;;;","22/Sep/22 13:00;cloud_fan;Issue resolved by pull request 37855
[https://github.com/apache/spark/pull/37855];;;",,,,,,,,,,,,,,,,
Negative size in error message when unsafe array is too big,SPARK-40403,13481034,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,12/Sep/22 00:30,12/Dec/22 18:10,13/Jul/23 08:51,14/Sep/22 02:46,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"When initializing an overly large unsafe array via {{UnsafeArrayWriter#initialize}}, {{BufferHolder#grow}} may report an error message with a negative size, e.g.:
{noformat}
java.lang.IllegalArgumentException: Cannot grow BufferHolder by size -2115263656 because the size is negative
{noformat}
(Note: This is not related to SPARK-39608, as far as I can tell, despite having the same symptom).

When calculating the initial size in bytes needed for the array, {{UnsafeArrayWriter#initialize}} uses an int expression, which can overflow. The initialize method then passes the negative size to {{BufferHolder#grow}}, which complains about the negative size.

Example (the following will run just fine on a 16GB laptop, despite the large driver size setting):
{noformat}
bin/spark-sql --driver-memory 22g --master ""local[1]""

create or replace temp view data1 as
select 0 as key, id as val
from range(0, 268271216);

create or replace temp view data2 as
select key as lkey, collect_list(val) as bigarray
from data1
group by key;

-- the below cache forces Spark to create unsafe rows
cache lazy table data2;

select count(*) from data2;
{noformat}
After a few minutes, {{BufferHolder#grow}} will throw the following exception:
{noformat}
java.lang.IllegalArgumentException: Cannot grow BufferHolder by size -2115263656 because the size is negative
	at org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:67)
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter.initialize(UnsafeArrayWriter.java:61)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.aggregate.Collect.serialize(collect.scala:73)
	at org.apache.spark.sql.catalyst.expressions.aggregate.Collect.serialize(collect.scala:37)
{noformat}
This query was going to fail anyway, but the message makes it looks like a bug in Spark rather than a user problem. {{UnsafeArrayWriter#initialize}} should calculate using a long expression and fail if the size exceeds {{Integer.MAX_VALUE}}, showing the actual initial size in the error message.
",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Sep 14 02:46:56 UTC 2022,,,,,,,,,,"0|z18h2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Sep/22 00:53;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/37852;;;","12/Sep/22 00:54;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/37852;;;","14/Sep/22 02:46;gurwls223;Issue resolved by pull request 37852
[https://github.com/apache/spark/pull/37852];;;",,,,,,,,,,,,,,,
Classes with companion object constructor fails interpreted path,SPARK-40385,13480578,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eejbyfeldt,eejbyfeldt,eejbyfeldt,08/Sep/22 05:38,12/Dec/22 18:10,13/Jul/23 08:51,22/Sep/22 02:46,3.1.3,3.2.2,3.3.0,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,,,,,,The Encoder implemented in SPARK-8288 for classes with only a companion object constructor fails when using the interpreted path.,,apachespark,eejbyfeldt,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Sep 22 02:46:58 UTC 2022,,,,,,,,,,"0|z18ea0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Sep/22 13:58;apachespark;User 'eejbyfeldt' has created a pull request for this issue:
https://github.com/apache/spark/pull/37837;;;","22/Sep/22 02:46;gurwls223;Issue resolved by pull request 37837
[https://github.com/apache/spark/pull/37837];;;",,,,,,,,,,,,,,,,
Constant-folding of InvokeLike should not result in non-serializable result,SPARK-40380,13480523,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,07/Sep/22 17:57,08/Sep/22 13:21,13/Jul/23 08:51,08/Sep/22 13:21,3.3.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,,,,,,"SPARK-37907 added constant-folding support to the {{InvokeLike}} family of expressions. Unfortunately it introduced a regression for cases when a constant-folded {{InvokeLike}} expression returned a non-serializable result. {{ExpressionEncoder}}s is an area where this problem may be exposed, e.g. when using sparksql-scalapb on Spark 3.3.0+.

Below is a minimal repro to demonstrate this issue:
{code:scala}
import org.apache.spark.sql.Column
import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute
import org.apache.spark.sql.catalyst.expressions.Literal
import org.apache.spark.sql.catalyst.expressions.objects.{Invoke, StaticInvoke}
import org.apache.spark.sql.types.{LongType, ObjectType}
class NotSerializableBoxedLong(longVal: Long) { def add(other: Long): Long = longVal + other }
case class SerializableBoxedLong(longVal: Long) { def toNotSerializable(): NotSerializableBoxedLong = new NotSerializableBoxedLong(longVal) }
val litExpr = Literal.fromObject(SerializableBoxedLong(42L), ObjectType(classOf[SerializableBoxedLong]))
val toNotSerializableExpr = Invoke(litExpr, ""toNotSerializable"", ObjectType(classOf[NotSerializableBoxedLong]))
val addExpr = Invoke(toNotSerializableExpr, ""add"", LongType, Seq(UnresolvedAttribute.quotedString(""id"")))
val df = spark.range(2).select(new Column(addExpr))
df.collect
{code}

Before SPARK-37907, this example would run fine and result in {{[[42], [43]]}}. But after SPARK-37907, it'd fail with:
{code:none}
...
Caused by: java.io.NotSerializableException: NotSerializableBoxedLong
Serialization stack:
	- object not serializable (class: NotSerializableBoxedLong, value: NotSerializableBoxedLong@71231636)
	- element of array (index: 1)
	- array (class [Ljava.lang.Object;, size 2)
	- element of array (index: 1)
	- array (class [Ljava.lang.Object;, size 3)
	- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)
	- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.apache.spark.sql.execution.WholeStageCodegenExec, functionalInterfaceMethod=scala/Function2.apply:(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeStatic org/apache/spark/sql/execution/WholeStageCodegenExec.$anonfun$doExecute$4$adapted:(Lorg/apache/spark/sql/catalyst/expressions/codegen/CodeAndComment;[Ljava/lang/Object;Lorg/apache/spark/sql/execution/metric/SQLMetric;Ljava/lang/Object;Lscala/collection/Iterator;)Lscala/collection/Iterator;, instantiatedMethodType=(Ljava/lang/Object;Lscala/collection/Iterator;)Lscala/collection/Iterator;, numCaptured=3])
	- writeReplace data (class: java.lang.invoke.SerializedLambda)
	- object (class org.apache.spark.sql.execution.WholeStageCodegenExec$$Lambda$3123/1641694389, org.apache.spark.sql.execution.WholeStageCodegenExec$$Lambda$3123/1641694389@185db22c)
  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:49)
  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)
  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:441)
{code}",,apachespark,cloud_fan,lingesh,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Sep 08 13:21:42 UTC 2022,,,,,,,,,,"0|z18dy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Sep/22 18:40;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/37823;;;","08/Sep/22 13:21;cloud_fan;Issue resolved by pull request 37823
[https://github.com/apache/spark/pull/37823];;;",,,,,,,,,,,,,,,,
Bug in Canonicalization of expressions like Add & Multiply i.e Commutative Operators,SPARK-40362,13480357,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,ashahid7,ashahid7,06/Sep/22 22:18,13/Sep/22 19:38,13/Jul/23 08:51,13/Sep/22 19:38,3.3.0,,,,,,,,,,,,,,3.3.1,,,,,SQL,,,0,spark-sql,,,,,"In the canonicalization code which is now in two stages, canonicalization involving Commutative operators is broken, if they are subexpressions of certain type of expressions which override precanonicalize, for example BinaryComparison 

Consider following expression:

a + b > 10

         GT

            |

a + b          10

The BinaryComparison operator in the precanonicalize, first precanonicalizes  children & then   may swap operands based on left /right hashCode inequality..

lets say  Add(a + b) .hashCode is >  10.hashCode as a result GT is converted to LT

But If the same tree is created 

           GT

            |

 b + a      10

The hashCode of Add(b, a) is not same as Add(a, b) , thus it is possible that for this tree

 Add(b + a) .hashCode is <  10.hashCode  in which case GT remains as is.

Thus to similar trees result in different canonicalization , one having GT other having LT 

 

The problem occurs because  for commutative expressions the canonicalization normalizes the expression with consistent hashCode which is not the case with precanonicalize as the hashCode of commutative expression 's precanonicalize and post canonicalize are different.

 

 

The test 
{quote}test(""bug X"")
Unknown macro: \{     val tr1 = LocalRelation('c.int, 'b.string, 'a.int)    val y = tr1.where('a.attr + 'c.attr > 10).analyze    val fullCond = y.asInstanceOf[Filter].condition.clone()   val addExpr = (fullCond match Unknown macro}
).clone().asInstanceOf[Add]
val canonicalizedFullCond = fullCond.canonicalized

// swap the operands of add
val newAddExpr = Add(addExpr.right, addExpr.left)

// build a new condition which is same as the previous one, but with operands of //Add reversed 

val builtCondnCanonicalized = GreaterThan(newAddExpr, Literal(10)).canonicalized

assertEquals(canonicalizedFullCond, builtCondnCanonicalized)
}
{quote}
This test fails.

The fix which I propose is that for the commutative expressions, the precanonicalize should be overridden and  Canonicalize.reorderCommutativeOperators be invoked on the expression instead of at place of canonicalize. effectively for commutative operands ( add, or , multiply , and etc) canonicalize and precanonicalize should be same.

PR:

[https://github.com/apache/spark/pull/37824]

 

 

I am also trying a better fix, where by the idea is that for commutative expressions the murmur hashCode are caluculated using unorderedHash so that it is order  independent ( i.e symmetric).

The above approach works fine , but in case of Least & Greatest, the Product's element is  a Seq,  and that messes with consistency of hashCode.",,apachespark,ashahid7,dongjoon,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Sep 13 19:38:27 UTC 2022,,,,,,,,,,"0|z18cy0:",9223372036854775807,,,,,cloud_fan,,,,,,,,,,,,,,,,,"08/Sep/22 17:13;apachespark;User 'ahshahid' has created a pull request for this issue:
https://github.com/apache/spark/pull/37824;;;","10/Sep/22 18:06;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37851;;;","10/Sep/22 18:07;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37851;;;","13/Sep/22 15:04;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37866;;;","13/Sep/22 19:38;dongjoon;Issue resolved by pull request 37866
[https://github.com/apache/spark/pull/37866];;;",,,,,,,,,,,,,
Update ORC to 1.8.0,SPARK-40323,13479995,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,04/Sep/22 00:17,04/Sep/22 02:26,13/Jul/23 08:51,04/Sep/22 02:26,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Build,,,0,,,,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Sep 04 02:26:41 UTC 2022,,,,,,,,,,"0|z18ar4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Sep/22 00:21;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37787;;;","04/Sep/22 02:26;dongjoon;Issue resolved by pull request 37787
[https://github.com/apache/spark/pull/37787];;;",,,,,,,,,,,,,,,,
Fix all dead links,SPARK-40322,13479982,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,03/Sep/22 14:24,12/Dec/22 18:11,13/Jul/23 08:51,24/Sep/22 08:12,3.4.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,Documentation,,,1,,,,,," 

[https://www.deadlinkchecker.com/website-dead-link-checker.asp]

 

 
||Status||URL||Source link text||
|-1 Not found: The server name or address could not be resolved|[http://engineering.ooyala.com/blog/using-parquet-and-scrooge-spark]|[Using Parquet and Scrooge with Spark|https://spark.apache.org/documentation.html]|
|-1 Not found: The server name or address could not be resolved|[http://blinkdb.org/]|[BlinkDB|https://spark.apache.org/third-party-projects.html]|
|404 Not Found|[https://github.com/AyasdiOpenSource/df]|[DF|https://spark.apache.org/third-party-projects.html]|
|-1 Timeout|[https://atp.io/]|[atp|https://spark.apache.org/powered-by.html]|
|-1 Not found: The server name or address could not be resolved|[http://www.sehir.edu.tr/en/]|[Istanbul Sehir University|https://spark.apache.org/powered-by.html]|
|404 Not Found|[http://nsn.com/]|[Nokia Solutions and Networks|https://spark.apache.org/powered-by.html]|
|-1 Not found: The server name or address could not be resolved|[http://www.nubetech.co/]|[Nube Technologies|https://spark.apache.org/powered-by.html]|
|-1 Timeout|[http://ooyala.com/]|[Ooyala, Inc.|https://spark.apache.org/powered-by.html]|
|-1 Not found: The server name or address could not be resolved|[http://engineering.ooyala.com/blog/fast-spark-queries-memory-datasets]|[Spark for Fast Queries|https://spark.apache.org/powered-by.html]|
|-1 Not found: The server name or address could not be resolved|[http://www.sisa.samsung.com/]|[Samsung Research America|https://spark.apache.org/powered-by.html]|
|-1 Timeout|[https://checker.apache.org/projs/spark.html]|[https://checker.apache.org/projs/spark.html|https://spark.apache.org/release-process.html]|
|404 Not Found|[https://ampcamp.berkeley.edu/amp-camp-two-strata-2013/]|[AMP Camp 2 [302 from http://ampcamp.berkeley.edu/amp-camp-two-strata-2013/]|https://spark.apache.org/documentation.html]|
|404 Not Found|[https://ampcamp.berkeley.edu/agenda-2012/]|[AMP Camp 1 [302 from http://ampcamp.berkeley.edu/agenda-2012/]|https://spark.apache.org/documentation.html]|
|404 Not Found|[https://ampcamp.berkeley.edu/4/]|[AMP Camp 4 [302 from http://ampcamp.berkeley.edu/4/]|https://spark.apache.org/documentation.html]|
|404 Not Found|[https://ampcamp.berkeley.edu/3/]|[AMP Camp 3 [302 from http://ampcamp.berkeley.edu/3/]|https://spark.apache.org/documentation.html]|
|-500 Internal Server Error-|-[https://www.packtpub.com/product/spark-cookbook/9781783987061]-|-[Spark Cookbook [301 from https://www.packtpub.com/big-data-and-business-intelligence/spark-cookbook]|https://spark.apache.org/documentation.html]-|
|-500 Internal Server Error-|-[https://www.packtpub.com/product/apache-spark-graph-processing/9781784391805]-|-[Apache Spark Graph Processing [301 from https://www.packtpub.com/big-data-and-business-intelligence/apache-spark-graph-processing]|https://spark.apache.org/documentation.html]-|
|500 Internal Server Error|[https://prevalentdesignevents.com/sparksummit/eu17/]|[register|https://spark.apache.org/news/]|
|500 Internal Server Error|[https://prevalentdesignevents.com/sparksummit/ss17/?_ga=1.211902866.780052874.1433437196]|[register|https://spark.apache.org/news/]|
|500 Internal Server Error|[https://www.prevalentdesignevents.com/sparksummit2015/europe/registration.aspx?source=header]|[register|https://spark.apache.org/news/]|
|500 Internal Server Error|[https://www.prevalentdesignevents.com/sparksummit2015/europe/speaker/]|[Spark Summit Europe|https://spark.apache.org/news/]|
|-1 Timeout|[http://strataconf.com/strata2013]|[Strata|https://spark.apache.org/news/]|
|-1 Not found: The server name or address could not be resolved|[http://blog.quantifind.com/posts/spark-unit-test/]|[Unit testing with Spark|https://spark.apache.org/news/]|
|-1 Not found: The server name or address could not be resolved|[http://blog.quantifind.com/posts/logging-post/]|[Configuring Spark's logs|https://spark.apache.org/news/]|
|-1 Timeout|[http://strata.oreilly.com/2012/08/seven-reasons-why-i-like-spark.html]|[Spark|https://spark.apache.org/news/]|
|-1 Timeout|[http://strata.oreilly.com/2012/11/shark-real-time-queries-and-analytics-for-big-data.html]|[Shark|https://spark.apache.org/news/]|
|-1 Timeout|[http://strata.oreilly.com/2012/10/spark-0-6-improves-performance-and-accessibility.html]|[Spark 0.6 release|https://spark.apache.org/news/]|
|404 Not Found|[http://data-informed.com/spark-an-open-source-engine-for-iterative-data-mining/]|[DataInformed|https://spark.apache.org/news/]|
|-1 Timeout|[http://strataconf.com/strata2013/public/schedule/detail/27438]|[introduction to Spark, Shark and BDAS|https://spark.apache.org/news/]|
|-1 Timeout|[http://strataconf.com/strata2013/public/schedule/detail/27440]|[hands-on exercise session|https://spark.apache.org/news/]|
|-500 Internal Server Error-|-[https://www.packtpub.com/product/big-data-analytics/9781785884696]-|-[Big Data Analytics with Spark and Hadoop [301 from https://www.packtpub.com/big-data-and-business-intelligence/big-data-analytics]|https://spark.apache.org/documentation.html]-|
|404 Not Found|[https://predictionio.apache.org/index.html/]|[Apache PredictionIO [301 from http://predictionio.apache.org/index.html/]|https://spark.apache.org/powered-by.html]|
|-1 Not found: The host name in the certificate is invalid or does not match|[https://engineering.tripadvisor.com/using-apache-spark-for-massively-parallel-nlp/]|[TripAdvisor [301 from http://engineering.tripadvisor.com/using-apache-spark-for-massively-parallel-nlp/]|https://spark.apache.org/powered-by.html]|
|404 Not Found|[https://www.zaloni.com/products/]|[Zaloni [301 from http://www.zaloni.com/products/]|https://spark.apache.org/powered-by.html]|
|404 Not Found|[https://spark.apache.org/docs/latest/graphx-guide.html]|[GraphX|https://spark.apache.org/documentation.html]|
|404 Not Found|[https://spark.apache.org/docs/latest/python-programming-guide.html]|[Python API|https://spark.apache.org/news/]|",,apachespark,dongjoon,LuciferYang,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Sep 28 20:17:07 UTC 2022,,,,,,,,,,"0|z18ao8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Sep/22 12:19;LuciferYang;The links related to `Spark Summit` have now been redirected to https://www.databricks.com/dataaisummit/. Is it better to keep the links, or to remove the links and only keep the text?;;;","05/Sep/22 12:24;LuciferYang;Many historical links on the news page are no longer accessible;;;","05/Sep/22 12:48;LuciferYang;[https://www.packtpub.com/big-data-and-business-intelligence/spark-cookbook] and 

[https://www.packtpub.com/big-data-and-business-intelligence/apache-spark-graph-processing]

[https://www.packtpub.com/big-data-and-business-intelligence/big-data-analytics]

not dead links;;;","23/Sep/22 08:14;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37980;;;","23/Sep/22 08:49;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37981;;;","24/Sep/22 06:38;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37984;;;","24/Sep/22 08:12;gurwls223;Issue resolved by pull request 37984
[https://github.com/apache/spark/pull/37984];;;","28/Sep/22 20:17;dongjoon;I updated the Fix Version from 3.3.1 to 3.3.2 because 3.3.1 RC2 vote start without this.;;;",,,,,,,,,,
"When the Executor plugin fails to initialize, the Executor shows active but does not accept tasks forever, just like being hung",SPARK-40320,13479950,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,choko111,miracle,miracle,03/Sep/22 02:26,26/Oct/22 06:50,13/Jul/23 08:51,26/Oct/22 06:48,3.0.0,,,,,,,,,,,,,,3.4.0,,,,,Scheduler,,,0,,,,,,"*Reproduce step:*
set `spark.plugins=ErrorSparkPlugin`
`ErrorSparkPlugin` && `ErrorExecutorPlugin` class as below (I abbreviate the code to make it clearer):
{code:java}
class ErrorSparkPlugin extends SparkPlugin {
  /**
   */
  override def driverPlugin(): DriverPlugin =  new ErrorDriverPlugin()

  /**
   */
  override def executorPlugin(): ExecutorPlugin = new ErrorExecutorPlugin()
}{code}
{code:java}
class ErrorExecutorPlugin extends ExecutorPlugin {
  private val checkingInterval: Long = 1

  override def init(_ctx: PluginContext, extraConf: util.Map[String, String]): Unit = {
    if (checkingInterval == 1) {
      throw new UnsatisfiedLinkError(""My Exception error"")
    }
  }
} {code}
The Executor is active when we check in spark-ui, however it was broken and doesn't receive any task.

*Root Cause:*

I check the code and I find in `org.apache.spark.rpc.netty.Inbox#safelyCall` it will throw fatal error (`UnsatisfiedLinkError` is fatal erro ) in method `dealWithFatalError` . Actually the  `CoarseGrainedExecutorBackend` JVM process  is active but the  communication thread is no longer working ( please see  `MessageLoop#receiveLoopRunnable` , `receiveLoop()` was broken, so executor doesn't receive any message)

Some ideas:
I think it is very hard to know what happened here unless we check in the code. The Executor is active but it can't do anything. We will wonder if the driver is broken or the Executor problem.  I think at least the Executor status shouldn't be active here or the Executor can exitExecutor (kill itself)

 ",,apachespark,LuciferYang,miracle,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Sep 22 12:57:18 UTC 2022,,,,,,,,,,"0|z18ah4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Sep/22 03:17;apachespark;User 'yabola' has created a pull request for this issue:
https://github.com/apache/spark/pull/37779;;;","03/Sep/22 03:18;apachespark;User 'yabola' has created a pull request for this issue:
https://github.com/apache/spark/pull/37779;;;","06/Sep/22 13:53;Ngone51;> Actually the  `CoarseGrainedExecutorBackend` JVM process  is active but the  communication thread is no longer working ( please see  `MessageLoop#receiveLoopRunnable` , `receiveLoop()` was broken, so executor doesn't receive any message)

Hmm, shouldn't it bring up a new `receiveLoop()` to serve RPC messages according to [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rpc/netty/MessageLoop.scala#L82-L89] . Why it doesn't?

 

Besides, why SparkUncaughtExceptionHandler doesn't catch the fatal error?

 

cc [~tgraves] [~mridulm80] ;;;","07/Sep/22 22:46;miracle;[~Ngone51] 
Shouldn't it bring up a new `receiveLoop()` to serve RPC messages?

Yes, my previous thinking was wrong. I remote debug on Executor and I found that it did catch the fatal error in [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rpc/netty/MessageLoop.scala#L82-L89] .
It will resubmit receiveLoop and in the second time it will block by [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rpc/netty/MessageLoop.scala#L69]
This Executor did not initialize successfully in the first time , so it didn't send LaunchedExecutor to Driver (you can see [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala#L172] )

So the Executor can't launch task, related PR [https://github.com/apache/spark/pull/25964] .

Why SparkUncaughtExceptionHandler doesn't catch the fatal error?
See [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/executor/Executor.scala#L284] plugins is private variable, so it was broken when initialize Executor at the beginning.;;;","22/Sep/22 12:57;Ngone51;I see. Thanks for the explaination. ;;;",,,,,,,,,,,,,
Non-deterministic hashCode() calculations for ArrayBasedMapData on equal objects,SPARK-40315,13479883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,c27kwan,c27kwan,c27kwan,02/Sep/22 12:16,06/Sep/22 13:20,13/Jul/23 08:51,06/Sep/22 13:15,3.2.2,,,,,,,,,,,,,,3.1.4,3.2.3,3.3.1,3.4.0,,Spark Core,,,0,,,,,,"There is no explicit `hashCode()` function override for the `ArrayBasedMapData` LogicalPlan. As a result, the `hashCode()` computed for `ArrayBasedMapData` can be different for two equal objects (objects with equal keys and values).

This error is non-deterministic and hard to reproduce, as we don't control the default `hashCode()` function.

We should override the `hashCode` function so that it works exactly as we expect. We should also have an explicit `equals()` function for consistency with how `Literals` check for equality of `ArrayBasedMapData`.",,apachespark,c27kwan,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Sep 06 13:15:57 UTC 2022,,,,,,,,,,"0|z18a28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Sep/22 12:40;apachespark;User 'c27kwan' has created a pull request for this issue:
https://github.com/apache/spark/pull/37771;;;","02/Sep/22 12:40;apachespark;User 'c27kwan' has created a pull request for this issue:
https://github.com/apache/spark/pull/37771;;;","06/Sep/22 10:10;apachespark;User 'c27kwan' has created a pull request for this issue:
https://github.com/apache/spark/pull/37807;;;","06/Sep/22 13:15;cloud_fan;Issue resolved by pull request 37807
[https://github.com/apache/spark/pull/37807];;;",,,,,,,,,,,,,,
Add inline Scala and Python bindings,SPARK-40314,13479875,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kimahriman,kimahriman,kimahriman,02/Sep/22 11:17,30/Sep/22 01:42,13/Jul/23 08:51,30/Sep/22 01:42,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,Add bindings for inline and inline_outer to Scala and Python function APIs,,apachespark,kimahriman,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Sep 30 01:42:58 UTC 2022,,,,,,,,,,"0|z18a0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Sep/22 11:24;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/37770;;;","02/Sep/22 11:25;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/37770;;;","30/Sep/22 01:42;podongfeng;Issue resolved by pull request 37770
[https://github.com/apache/spark/pull/37770];;;",,,,,,,,,,,,,,,
The performance will be worse after codegen,SPARK-40303,13479625,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,01/Sep/22 09:04,20/Jan/23 17:04,13/Jul/23 08:51,20/Jan/23 17:04,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"
{code:scala}
import org.apache.spark.benchmark.Benchmark

val dir = ""/tmp/spark/benchmark""
val N = 2000000
val columns = Range(0, 100).map(i => s""id % $i AS id$i"")

spark.range(N).selectExpr(columns: _*).write.mode(""Overwrite"").parquet(dir)

// Seq(1, 2, 5, 10, 15, 25, 40, 60, 100)
Seq(60).foreach{ cnt =>
  val selectExps = columns.take(cnt).map(_.split("" "").last).map(c => s""count(distinct $c)"")

  val benchmark = new Benchmark(""Benchmark count distinct"", N, minNumIters = 1)
  benchmark.addCase(s""$cnt count distinct with codegen"") { _ =>
    withSQLConf(
      ""spark.sql.codegen.wholeStage"" -> ""true"",
      ""spark.sql.codegen.factoryMode"" -> ""FALLBACK"") {
      spark.read.parquet(dir).selectExpr(selectExps: _*).write.format(""noop"").mode(""Overwrite"").save()
    }
  }

  benchmark.addCase(s""$cnt count distinct without codegen"") { _ =>
    withSQLConf(
      ""spark.sql.codegen.wholeStage"" -> ""false"",
      ""spark.sql.codegen.factoryMode"" -> ""NO_CODEGEN"") {
      spark.read.parquet(dir).selectExpr(selectExps: _*).write.format(""noop"").mode(""Overwrite"").save()
    }
  }
  benchmark.run()
}
{code}


{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark count distinct:                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
60 count distinct with codegen                   628146         628146           0          0.0      314072.8       1.0X
60 count distinct without codegen                147635         147635           0          0.0       73817.5       4.3X
{noformat}

",,apachespark,chenfu,dongjoon,FateRin,LuciferYang,rednaxelafx,ulysses,xkrogen,yumwang,,,,,,,,,,,,,,,,,SPARK-40331,,"14/Sep/22 05:23;LuciferYang;TestApiBenchmark.scala;https://issues.apache.org/jira/secure/attachment/13049263/TestApiBenchmark.scala","14/Sep/22 05:23;LuciferYang;TestApis.java;https://issues.apache.org/jira/secure/attachment/13049264/TestApis.java","14/Sep/22 05:23;LuciferYang;TestParameters.java;https://issues.apache.org/jira/secure/attachment/13049265/TestParameters.java",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jan 20 17:04:17 UTC 2023,,,,,,,,,,"0|z188gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Sep/22 09:10;yumwang;cc [~cloud_fan] [~joshrosen] [~rednaxelafx];;;","02/Sep/22 04:42;LuciferYang;Run use Java 8 with `-XX:+PrintCompilation`, I found the following logs:
{code:java}
64350 21862       4       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doConsume_0$ (2051 bytes)   COMPILE SKIPPED: unsupported incoming calling sequence (not retryable)
137245 22141 %     4       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doAggregateWithKeys_0$ @ 38 (2712 bytes)   COMPILE SKIPPED: unsupported calling sequence (not retryable) {code}
 

[~rednaxelafx] In this case, will compilation optimization degenerate to Level 3 or give up when use Java 8?;;;","02/Sep/22 05:03;LuciferYang;{code:java}
 64265 21820       3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doConsume_0$ (2051 bytes)
  64308 21862       4       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doConsume_0$ (2051 bytes)
  64350 21862       4       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doConsume_0$ (2051 bytes)   COMPILE SKIPPED: unsupported incoming calling sequence (not retryable) 
1735350 24101       3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2::hashAgg_doConsume_0$ (2052 bytes){code}
{code:java}
103941 22065 %     3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doAggregateWithKeys_0$ @ 38 (2712 bytes)
 104602 22067       3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doAggregateWithKeys_0$ (2712 bytes)
 135979 22141 %     4       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doAggregateWithKeys_0$ @ 38 (2712 bytes)
 137245 22141 %     4       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doAggregateWithKeys_0$ @ 38 (2712 bytes)   COMPILE SKIPPED: unsupported calling sequence (not retryable) {code}
The compilation relevant  logs are as above when Java 8 is used;;;","02/Sep/22 05:06;LuciferYang;If run with Java 17, the performance gap will be smaller. The compilation logs of `hashAgg_doConsume_0` and `hashAgg_doAggregateWithKeys_0` as follows:

 
{code:java}
 102158 22568       3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doConsume_0$ (2051 bytes)
 102180 22606       4       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doConsume_0$ (2051 bytes)
 102228 22606       4       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doConsume_0$ (2051 bytes)   COMPILE SKIPPED: unsupported incoming calling sequence (retry at different tier)
 102228 22619       1       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doConsume_0$ (2051 bytes)
 102240 22568       3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doConsume_0$ (2051 bytes)   made not entrant
 218296 24067       3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2::hashAgg_doConsume_0$ (2052 bytes)
 218463 22568       3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doConsume_0$ (2051 bytes)   made zombie {code}
 
{code:java}
105832 22708 %     3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doAggregateWithKeys_0$ @ 38 (2712 bytes)
 105955 22709       3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doAggregateWithKeys_0$ (2712 bytes)
 108247 22741 %     4       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doAggregateWithKeys_0$ @ 38 (2712 bytes)
 108484 22741 %     4       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doAggregateWithKeys_0$ @ 38 (2712 bytes)   COMPILE SKIPPED: unsupported calling sequence (retry at different tier)
 108727 22708 %     3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doAggregateWithKeys_0$ @ 38 (2712 bytes)   made not entrant
 108727 22743 %     1       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doAggregateWithKeys_0$ @ 38 (2712 bytes)
 218463 22708 %     3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1::hashAgg_doAggregateWithKeys_0$ @ 38 (2712 bytes)   made zombie {code};;;","02/Sep/22 07:57;rednaxelafx;Interesting, thanks for posting your findings, [~LuciferYang]!

In the JDK8 case, the message:
{code}
COMPILE SKIPPED: unsupported calling sequence (not retryable)
{code}
Indicates that the compiler deemed this method should not be attempted to compile again on any tier of compilation, and because this is an OSR compilation (i.e. loop compilation), this will mark the method as ""never try to perform OSR compilation again on all tiers"". On JDK17 this is relaxed a bit and it'll try tier 1.

Note that OSR compilation and STD compilation (normal compilation) are separate things. Marking one as not compilation doesn't affect the other. I haven't checked the IR yet but if I had to guess, the reason why this method is recorded as not OSR compilable is because there are too many live local variables at the OSR (loop) entry point, beyond what the HotSpot JVM could support.
So only OSR is affected, STD should still be fine.

In general, the tiered compilation system in the HotSpot JVM works as:
- tier 0: interpreter
- tier 1: C1 no profiling (best code quality for C1, same as HotSpot Client VM's C1)
- tier 2: C1 basic profiling (lower performance than tier 1, only used when the target level is tier 3 but the C1 queue is too long)
- tier 3: C1 full profiling (lower performance than tier 2, for collecting profile to perform tier 4 profile-guided optimization)
- tier 4: C2

As such, tiers 2 and 3 are only useful if tier 4 is available. So if a method is recorded as ""not compilable on tier 4"", the only realistic option left is to try tier 1.;;;","02/Sep/22 08:34;LuciferYang;do some investigate on linux with spark `local[2]` module, maybe help:
 # Upgrading to Java 17 will significantly improve performance (about 10times over Java 8: From 20 minutes to 1.9 minutes)

 # When using Java 8, adding `spark.sql.CodeGenerator.validParamLength = 48` is helpful for performance improvement (From 20 minutes to 3.1 minutes);;;","02/Sep/22 08:55;LuciferYang;Thank you for your explain, very clear [~rednaxelafx] .

I guess this may related to the number of `doConsume` method parameters. After some experiments, I found when the number of parameters exceeds 50, the performance of the case in the Jira description will significant deterioration.

If need change the code, maybe try to make the input parameters of the `doConsume` method fixed length will help, such as using a List or Array, but this will lose the parameter type, and I am not sure whether it is feasible in practice;;;","02/Sep/22 16:52;rednaxelafx;Nice findings [~LuciferYang]!

{quote}
After some experiments, I found when the number of parameters exceeds 50, the performance of the case in the Jira description will significant deterioration.
{quote}
Sounds reasonable. Note that in a STD compilation, the only things that need to be live at the method entry are the method parameters (both implicit ones like {{this}}, and explicit ones); however, for an OSR compilation, it would be all of the parameters/local variables that are live at the loop entry point, so in this case both the {{doConsume}} parameters and the local variables contribute to the problem.

Just FYI I have an old write on PrintCompilation and OSR here: https://gist.github.com/rednaxelafx/1165804#file-notes-md
(Gee, just realized that was from 11 years ago.......)

{quote}
maybe try to make the input parameters of the `doConsume` method fixed length will help, such as using a List or Array
{quote}
Welp, hoisting the parameters into an Arguments object is rather common in ""code splitting"" in code generators. Since we're already doing codegen, it's possible to generate tailor-made Arguments classes to retain the type information. Using List/Array would require extra boxing for primitive types and it's less ideal.
(An array-based box is already used in Spark SQL's codegen in the form of the {{references}} array. Indeed the type info is lost on the interface level and you'd have to do a cast when you get data out of it. It's still usable though.);;;","14/Sep/22 05:23;LuciferYang;I did a simple experiment to compare the following scenarios：
 # A method with 127 input parameters
 # Encapsulate the input parameters of the above method as a specific type, including 127 fields, and create a new parameter object before each call
 # Encapsulate the input parameters of the above method as a specific type, including 127 fields, reuse one parameter object and reset + refill parameter data before each call

 

I confirmed that the JIT compilation failure mentioned above will occur in the 1&2 test scenarios， the test result as follows:

 

Java 8

 
{code:java}
OpenJDK 64-Bit Server VM 1.8.0_345-b01 on Linux 5.15.0-1019-azure
Intel(R) Xeon(R) Platinum 8171M CPU @ 2.60GHz
Test sum:                                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Use multiple parameters method                    35772          36161         550          0.3        3577.2       1.0X
Use TestParameters create new                     38701          38783         115          0.3        3870.1       0.9X
Use TestParameters reuse                          17986          18125         196          0.6        1798.6       2.0X {code}
Java 11

 
{code:java}
OpenJDK 64-Bit Server VM 11.0.16+8-LTS on Linux 5.15.0-1019-azure
Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz
Test sum:                                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Use multiple parameters method                    12253          12286          46          0.8        1225.3       1.0X
Use TestParameters create new                     13644          13665          30          0.7        1364.4       0.9X
Use TestParameters reuse                          13188          13219          44          0.8        1318.8       0.9X {code}
Java 17

 
{code:java}
OpenJDK 64-Bit Server VM 17.0.4+8-LTS on Linux 5.15.0-1019-azure
Intel(R) Xeon(R) CPU E5-2673 v3 @ 2.40GHz
Test sum:                                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Use multiple parameters method                    14044          14128         119          0.7        1404.4       1.0X
Use TestParameters create new                     16174          16289         162          0.6        1617.4       0.9X
Use TestParameters reuse                          15633          15638           8          0.6        1563.3       0.9X {code}
From the test results, encapsulating and reusing specific parameter types will only alleviate the problem when using Java 8, but running programs using Java 11 or Java 17 seems to be a simpler and more effective way.

 

So for the current issue, I suggest upgrading the Java runtime environment to solve the problem. [~yumwang] 

 

Uploaded the test program to the attachment;;;","21/Sep/22 23:50;yumwang;Issue fixed by [JDK-8159720|https://bugs.openjdk.org/browse/JDK-8159720].;;;","19/Oct/22 04:10;yumwang;How to run benchmark code:
 
# Download latest spark: https://spark.apache.org/downloads.html
# start spark-shell
{code:sh}
tar -zxf spark-3.3.1-bin-hadoop3.tgz
cd spark-3.3.1-bin-hadoop3
bin/spark-shell --master ""local[2]""
{code}
#  Run benchmark code:
{code:scala}
val dir = ""/tmp/spark/benchmark""
val N = 2000000
val columns = Range(0, 100).map(i => s""id % $i AS id$i"")

spark.range(N).selectExpr(columns: _*).write.mode(""Overwrite"").parquet(dir)

Seq(40, 60).foreach { cnt =>
  val selectExps = columns.take(cnt).map(_.split("" "").last).map(c => s""count(distinct $c)"")
  val start = System.currentTimeMillis()
  spark.read.parquet(dir).selectExpr(selectExps: _*).collect()
  println(cnt + ""|"" + (System.currentTimeMillis() - start))
}
{code}
#  Output:
{noformat}
Before:
40|280273
60|581743
After backport JDK-8159720 to JDK 8:
40|20582
60|49688
{noformat}


;;;","20/Jan/23 10:50;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/39671;;;","20/Jan/23 10:50;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/39671;;;","20/Jan/23 17:04;dongjoon;Issue resolved by pull request 39671
[https://github.com/apache/spark/pull/39671];;;",,,,
CTE outer reference nested in CTE main body cannot be resolved,SPARK-40297,13479579,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maryannxue,maryannxue,maryannxue,01/Sep/22 02:38,06/Sep/22 04:42,13/Jul/23 08:51,06/Sep/22 04:42,3.3.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,,,,,,"AnalysisException ""Table or view not found"" is thrown when a CTE reference occurs in an inner CTE definition nested in the outer CTE's main body FROM clause. E.g.,

{code}
WITH cte_outer AS (
  SELECT 1
)
SELECT * FROM (
  WITH cte_inner AS (
    SELECT * FROM cte_outer
  )
  SELECT * FROM cte_inner
)
{code}
",,apachespark,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Sep 01 02:58:28 UTC 2022,,,,,,,,,,"0|z1887c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Sep/22 02:58;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/37751;;;","01/Sep/22 02:58;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/37751;;;",,,,,,,,,,,,,,,,
Allow v2 functions with literal args in write distribution and ordering,SPARK-40295,13479555,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,aokolnychyi,aokolnychyi,31/Aug/22 20:41,07/Sep/22 16:18,13/Jul/23 08:51,07/Sep/22 16:16,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,Spark should allow v2 function with literal args in write distribution and ordering.,,aokolnychyi,apachespark,csun,,,,,,,,,,,,,,,,,,,,,SPARK-37375,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Sep 07 16:16:18 UTC 2022,,,,,,,,,,"0|z1882g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Aug/22 21:45;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37749;;;","07/Sep/22 16:16;csun;Issue resolved by pull request 37749
[https://github.com/apache/spark/pull/37749];;;",,,,,,,,,,,,,,,,
Failure to create parquet predicate push down for ints and longs on some valid files,SPARK-40280,13479370,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,revans2,revans2,revans2,30/Aug/22 20:03,09/Sep/22 13:01,13/Jul/23 08:51,08/Sep/22 14:01,3.1.0,3.2.0,3.3.0,3.4.0,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,,,,,,"The [parquet format|https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#signed-integers] specification states that...

bq. {{{}INT(8, true){}}}, {{{}INT(16, true){}}}, and {{INT(32, true)}} must annotate an {{int32}} primitive type and {{INT(64, true)}} must annotate an {{int64}} primitive type. {{INT(32, true)}} and {{INT(64, true)}} are implied by the {{int32}} and {{int64}} primitive types if no other annotation is present and should be considered optional.

But the code inside of [ParquetFilters.scala|https://github.com/apache/spark/blob/296fe49ec855ac8c15c080e7bab6d519fe504bd3/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala#L125-L126] requires that for {{int32}} and {{int64}} that there be no annotation. If there is an annotation for those columns and they are a part of a predicate push down, the hard coded types will not match and the corresponding filter ends up being {{None}}.

This can be a huge performance penalty for a valid parquet file.

I am happy to provide files that show the issue if needed for testing.",,apachespark,rajesh.balamohan,revans2,tgraves,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Sep 09 13:01:06 UTC 2022,,,,,,,,,,"0|z186y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Aug/22 19:12;apachespark;User 'revans2' has created a pull request for this issue:
https://github.com/apache/spark/pull/37747;;;","09/Sep/22 12:45;apachespark;User 'zzcclp' has created a pull request for this issue:
https://github.com/apache/spark/pull/37846;;;","09/Sep/22 12:45;apachespark;User 'zzcclp' has created a pull request for this issue:
https://github.com/apache/spark/pull/37846;;;","09/Sep/22 13:00;apachespark;User 'zzcclp' has created a pull request for this issue:
https://github.com/apache/spark/pull/37847;;;","09/Sep/22 13:01;apachespark;User 'zzcclp' has created a pull request for this issue:
https://github.com/apache/spark/pull/37847;;;",,,,,,,,,,,,,
Make compute.max_rows as None working in DataFrame.style,SPARK-40270,13479217,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,30/Aug/22 05:34,12/Dec/22 18:10,13/Jul/23 08:51,30/Aug/22 07:29,3.4.0,,,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,Pandas API on Spark,,,0,,,,,,"{code}
import pyspark.pandas as ps
ps.set_option(""compute.max_rows"", None)
ps.get_option(""compute.max_rows"")
ps.range(1).style
{code}

fails as below:

{code}
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/pandas/frame.py"", line 3656, in style
    pdf = self.head(max_results + 1)._to_internal_pandas()
TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Dec 09 12:02:13 UTC 2022,,,,,,,,,,"0|z18608:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Aug/22 05:42;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37718;;;","30/Aug/22 05:43;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37718;;;","30/Aug/22 07:29;gurwls223;Issue resolved by pull request 37718
[https://github.com/apache/spark/pull/37718];;;","09/Dec/22 09:26;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39002;;;","09/Dec/22 11:47;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39006;;;","09/Dec/22 11:54;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39007;;;","09/Dec/22 12:02;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39008;;;",,,,,,,,,,,
DirectTaskResult meta should not be counted into result size,SPARK-40261,13479177,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liuzq12,liuzq12,liuzq12,29/Aug/22 20:41,01/Sep/22 00:39,13/Jul/23 08:51,01/Sep/22 00:39,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,"This issue exists for a long time (since [https://github.com/liuzqt/spark/commit/c33e55008239f417764d589c1366371d18331686)]

when calculating whether driver fetching result exceed `spark.driver.maxResultSize` limit, the whole serialized result task size is taken into account, including task metadata overhead size([accumUpdates|https://github.com/apache/spark/blob/c95ed826e23fdec6e1a779cfebde7b3364594fb5/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala#L41]) as well. However, the metadata should not be counted because they will be discarded by the driver immediately after being processed.

This will lead to exception when running jobs with tons of task but actually return small results.

Therefore we should only count `[valueBytes|https://github.com/apache/spark/blob/c95ed826e23fdec6e1a779cfebde7b3364594fb5/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala#L40]` when calculating result size limit.

cc [~joshrosen] 

 

 ",,apachespark,joshrosen,liuzq12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Sep 01 00:39:59 UTC 2022,,,,,,,,,,"0|z185rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Aug/22 21:13;apachespark;User 'liuzqt' has created a pull request for this issue:
https://github.com/apache/spark/pull/37713;;;","01/Sep/22 00:39;joshrosen;Fixed by https://github.com/apache/spark/pull/37713;;;",,,,,,,,,,,,,,,,
Fix BitSet equality check,SPARK-40247,13478972,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,28/Aug/22 12:47,29/Aug/22 07:26,13/Jul/23 08:51,29/Aug/22 07:26,3.4.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,,,,,,,,apachespark,cloud_fan,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 29 07:26:59 UTC 2022,,,,,,,,,,"0|z184i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Aug/22 12:55;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37696;;;","29/Aug/22 07:26;cloud_fan;Issue resolved by pull request 37696
[https://github.com/apache/spark/pull/37696];;;",,,,,,,,,,,,,,,,
Fix FileScan equality check when partition or data filter columns are not read,SPARK-40245,13478942,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,27/Aug/22 17:02,29/Aug/22 15:54,13/Jul/23 08:51,29/Aug/22 15:54,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,,,apachespark,cloud_fan,petertoth,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 29 15:54:20 UTC 2022,,,,,,,,,,"0|z184bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Aug/22 17:14;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37693;;;","29/Aug/22 15:54;cloud_fan;Issue resolved by pull request 37693
[https://github.com/apache/spark/pull/37693];;;",,,,,,,,,,,,,,,,
GROUPING SETS should preserve the grouping columns,SPARK-40218,13478624,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,25/Aug/22 10:07,26/Sep/22 09:33,13/Jul/23 08:51,26/Aug/22 07:26,3.2.0,,,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,SPARK-40562,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 26 07:26:06 UTC 2022,,,,,,,,,,"0|z182d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Aug/22 10:16;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/37655;;;","25/Aug/22 10:17;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/37655;;;","26/Aug/22 07:26;cloud_fan;Issue resolved by pull request 37655
[https://github.com/apache/spark/pull/37655];;;",,,,,,,,,,,,,,,
SparkSQL castPartValue does not properly handle byte & short,SPARK-40212,13478494,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brenn_,brenn_,brenn_,24/Aug/22 21:20,12/Dec/22 18:10,13/Jul/23 08:51,29/Aug/22 01:56,3.3.0,,,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,,,,,,"Reading in a parquet file partitioned on disk by a `Byte`-type column fails with the following exception:

 
{code:java}
[info]   Cause: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Byte
[info]   at scala.runtime.BoxesRunTime.unboxToByte(BoxesRunTime.java:95)
[info]   at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getByte(rows.scala:39)
[info]   at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getByte$(rows.scala:39)
[info]   at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getByte(rows.scala:195)
[info]   at org.apache.spark.sql.catalyst.expressions.JoinedRow.getByte(JoinedRow.scala:86)
[info]   at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_6$(Unknown Source)
[info]   at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$8(ParquetFileFormat.scala:385)
[info]   at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.next(RecordReaderIterator.scala:62)
[info]   at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:189)
[info]   at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
[info]   at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[info]   at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[info]   at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[info]   at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)
[info]   at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
[info]   at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
[info]   at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[info]   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[info]   at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[info]   at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[info]   at org.apache.spark.scheduler.Task.run(Task.scala:136)
[info]   at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[info]   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[info]   at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:748) {code}
I believe the issue to stem from [PartitioningUtils::castPartValueToDesiredType|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala#L533] returning an Integer for ByteType and ShortType (which then fails to unbox to the expected type):

 
{code:java}
case ByteType | ShortType | IntegerType => Integer.parseInt(value) {code}
 

The issue appears to have been introduced in [this commit|https://github.com/apache/spark/commit/fc29c91f27d866502f5b6cc4261d4943b5cccc7e] so likely affects Spark 3.2 as well, though I've only tested on 3.3.0.

 

 ",,apachespark,bersprockets,brenn_,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 29 01:56:22 UTC 2022,,,,,,,,,,"0|z181k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Aug/22 05:02;yumwang;How to reproduce this issue?;;;","25/Aug/22 14:25;apachespark;User 'BrennanStein' has created a pull request for this issue:
https://github.com/apache/spark/pull/37659;;;","25/Aug/22 14:29;brenn_;Is a PR with unit test adequate reproduction? :) Realized it was actually a very simple fix

[https://github.com/apache/spark/pull/37659]

 ;;;","29/Aug/22 01:56;gurwls223;Issue resolved by pull request 37659
[https://github.com/apache/spark/pull/37659];;;",,,,,,,,,,,,,,
Allow a dictionary in SparkSession.config in PySpark,SPARK-40202,13478322,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,24/Aug/22 05:08,12/Dec/22 18:10,13/Jul/23 08:51,25/Aug/22 02:57,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,PySpark,SQL,,0,,,,,,SPARK-40163 added a new signature in SparkSession.conf. We should better have the same one in PySpark too.,,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 25 02:57:26 UTC 2022,,,,,,,,,,"0|z180i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Aug/22 05:21;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37642;;;","24/Aug/22 05:21;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37642;;;","25/Aug/22 02:57;gurwls223;Issue resolved by pull request 37642
[https://github.com/apache/spark/pull/37642];;;",,,,,,,,,,,,,,,
mergedShuffleCleaner should have been shutdown before db closed,SPARK-40186,13478112,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,23/Aug/22 03:27,08/Sep/22 01:58,13/Jul/23 08:51,08/Sep/22 01:58,3.4.0,,,,,,,,,,,,,,,,,,,Spark Core,,,0,,,,,,"Should ensure `RemoteBlockPushResolver#mergedShuffleCleaner` have been shutdown before `RemoteBlockPushResolver#db` closed, otherwise, `RemoteBlockPushResolver#applicationRemoved` may perform delete operations on a closed db.

 

https://github.com/apache/spark/pull/37610#discussion_r951185256

 ",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 23 08:21:02 UTC 2022,,,,,,,,,,"0|z17z7k:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,,,,,"23/Aug/22 08:21;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/37624;;;",,,,,,,,,,,,,,,,,
Fix the issue with Parquet column index and predicate pushdown in Data source V1,SPARK-40169,13477866,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,ivan.sadikov,ivan.sadikov,21/Aug/22 22:47,16/Sep/22 17:51,13/Jul/23 08:51,16/Sep/22 17:46,3.2.3,3.3.1,3.4.0,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,,,,,,"This is a follow for SPARK-39833. In [https://github.com/apache/spark/pull/37419,] we disabled column index for Parquet due to correctness issues that we found when filtering data on the partition column overlapping with data schema.

 

This ticket is for permanent and thorough fix for the issue and re-enablement of the column index. See more details in the PR linked above.",,apachespark,csun,ivan.sadikov,,,,,,,,,,,,,,,,,,,,,,,SPARK-39833,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Sep 16 17:46:53 UTC 2022,,,,,,,,,,"0|z17xq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Aug/22 22:51;ivan.sadikov;I would like to work on it as it was my responsibility to come up with a proper fix for the original issue :). I will sync with [~chaosun] offline and we will come up with the strategy to address the problem properly.;;;","14/Sep/22 16:54;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/37881;;;","16/Sep/22 17:46;csun;Issue resolved by pull request 37881
[https://github.com/apache/spark/pull/37881];;;",,,,,,,,,,,,,,,
Handle FileNotFoundException when shuffle file deleted in decommissioner,SPARK-40168,13477862,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,21/Aug/22 20:24,09/Sep/22 21:22,13/Jul/23 08:51,09/Sep/22 21:22,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,"When shuffle files not found, decommissioner will handles IOException, but the real exception is as below:
{code:java}
22/08/10 18:05:34 ERROR BlockManagerDecommissioner: Error occurred during migrating migrate_shuffle_1_356
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
    at org.apache.spark.network.BlockTransferService.uploadBlockSync(BlockTransferService.scala:122)
    at org.apache.spark.storage.BlockManagerDecommissioner$ShuffleMigrationRunnable.$anonfun$run$4(BlockManagerDecommissioner.scala:120)
    at org.apache.spark.storage.BlockManagerDecommissioner$ShuffleMigrationRunnable.$anonfun$run$4$adapted(BlockManagerDecommissioner.scala:111)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.apache.spark.storage.BlockManagerDecommissioner$ShuffleMigrationRunnable.run(BlockManagerDecommissioner.scala:111)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: Failed to send RPC RPC 5697756267528635203 to /10.240.2.65:43481: java.io.FileNotFoundException: /tmp/blockmgr-98a2a29a-5231-4fed-a82e-6bc0531ad407/15/shuffle_1_356_0.index (No such file or directory)
    at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392)
    at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369)
    at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
    at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)
    at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)
    at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
    at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)
    at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)
    at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
    at io.netty.util.internal.PromiseNotificationUtil.tryFailure(PromiseNotificationUtil.java:64)
    at io.netty.channel.ChannelOutboundBuffer.safeFail(ChannelOutboundBuffer.java:723)
    at io.netty.channel.ChannelOutboundBuffer.remove0(ChannelOutboundBuffer.java:308)
    at io.netty.channel.ChannelOutboundBuffer.failFlushed(ChannelOutboundBuffer.java:660)
    at io.netty.channel.AbstractChannel$AbstractUnsafe.close(AbstractChannel.java:735)
    at io.netty.channel.AbstractChannel$AbstractUnsafe.handleWriteError(AbstractChannel.java:950)
    at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:933)
    at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:354)
    at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:895)
    at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1372)
    at io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:750)
    at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:742)
    at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:728)
    at io.netty.channel.ChannelDuplexHandler.flush(ChannelDuplexHandler.java:127)
    at io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:750)
    at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:765)
    at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)
    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    ... 1 more
Caused by: java.io.FileNotFoundException: /tmp/blockmgr-98a2a29a-5231-4fed-a82e-6bc0531ad407/15/shuffle_1_356_0.index (No such file or directory)
    at java.base/java.io.RandomAccessFile.open0(Native Method)
    at java.base/java.io.RandomAccessFile.open(RandomAccessFile.java:345)
    at java.base/java.io.RandomAccessFile.<init>(RandomAccessFile.java:259)
    at java.base/java.io.RandomAccessFile.<init>(RandomAccessFile.java:214)
    at io.netty.channel.DefaultFileRegion.open(DefaultFileRegion.java:88)
    at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:128)
    at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121)
    at org.apache.spark.network.crypto.TransportCipher$EncryptedMessage.encryptMore(TransportCipher.java:347)
    at org.apache.spark.network.crypto.TransportCipher$EncryptedMessage.transferTo(TransportCipher.java:310)
    at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:362)
    at io.netty.channel.nio.AbstractNioByteChannel.doWriteInternal(AbstractNioByteChannel.java:238)
    at io.netty.channel.nio.AbstractNioByteChannel.doWrite0(AbstractNioByteChannel.java:212)
    at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:400)
    at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:931)
    ... 17 more
22/08/10 18:05:34 WARN BlockManagerDecommissioner: Stop migrating shuffle blocks to BlockManagerId(0, 10.240.2.65, 43481, None)

{code}
This wrapped exception should be handled explicitly, further avoid unnecessary retry of this shuffle block and stop of current migration thread",,apachespark,dongjoon,warrenzhu25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Sep 09 21:22:25 UTC 2022,,,,,,,,,,"0|z17xp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Aug/22 20:33;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/37603;;;","09/Sep/22 21:22;dongjoon;Issue resolved by pull request 37603
[https://github.com/apache/spark/pull/37603];;;",,,,,,,,,,,,,,,,
Codegen compilation error when using split_part,SPARK-40152,13477673,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,bersprockets,bersprockets,19/Aug/22 21:29,29/Aug/22 11:54,13/Jul/23 08:51,21/Aug/22 19:30,3.3.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,,,,,,"The following query throws an error:
{noformat}
create or replace temp view v1 as
select * from values
('11.12.13', '.', 3)
as v1(col1, col2, col3);

cache table v1;

SELECT split_part(col1, col2, col3)
from v1;
{noformat}
The error is:
{noformat}
22/08/19 14:25:14 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 42, Column 1: Expression ""project_isNull_0 = false"" is not a type
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 42, Column 1: Expression ""project_isNull_0 = false"" is not a type
	at org.codehaus.janino.Java$Atom.toTypeOrCompileException(Java.java:3934)
	at org.codehaus.janino.Parser.parseBlockStatement(Parser.java:1887)
	at org.codehaus.janino.Parser.parseBlockStatements(Parser.java:1811)
	at org.codehaus.janino.Parser.parseBlock(Parser.java:1792)
	at 
{noformat}
In the end, {{split_part}} does successfully execute, although in interpreted mode.",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 29 11:54:36 UTC 2022,,,,,,,,,,"0|z17wjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Aug/22 21:31;bersprockets;Seems to be a simple case of missing semicolons. I think it's a very simple fix.;;;","20/Aug/22 11:43;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37589;;;","21/Aug/22 19:30;srowen;Resolved by https://github.com/apache/spark/pull/37589;;;","23/Aug/22 10:17;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37626;;;","23/Aug/22 10:17;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37626;;;","24/Aug/22 01:00;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37637;;;","24/Aug/22 01:01;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37637;;;","27/Aug/22 05:11;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37684;;;","27/Aug/22 05:11;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37684;;;","29/Aug/22 11:54;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37708;;;",,,,,,,,
Fix return type for new median(interval) function ,SPARK-40151,13477657,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,maxgekk,srielau,srielau,19/Aug/22 17:55,22/Aug/22 08:12,13/Jul/23 08:51,22/Aug/22 08:12,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,"median() right now returns an interval of the same type as the input.
We should instead match mean and avg():

The result type is computed as for the arguments:

- year-month interval: The result is an `INTERVAL YEAR TO MONTH`.
- day-time interval: The result is an `INTERVAL DAY TO SECOND`.
- In all other cases the result is a DOUBLE.",,apachespark,maxgekk,srielau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 22 08:12:24 UTC 2022,,,,,,,,,,"0|z17wfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Aug/22 07:03;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/37595;;;","21/Aug/22 07:04;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/37595;;;","22/Aug/22 08:12;maxgekk;Issue resolved by pull request 37595
[https://github.com/apache/spark/pull/37595];;;",,,,,,,,,,,,,,,
Star expansion after outer join asymmetrically includes joining key,SPARK-40149,13477570,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,otakar,otakar,19/Aug/22 09:32,22/Feb/23 23:12,13/Jul/23 08:51,07/Sep/22 11:04,3.2.0,3.2.1,3.2.2,3.3.0,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,,,,,,"When star expansion is used on left side of a join, the result will include joining key, while on the right side of join it doesn't. I would expect the behaviour to be symmetric (either include on both sides or on neither). 

Example:
{code:python}
from pyspark.sql import SparkSession
import pyspark.sql.functions as f

spark = SparkSession.builder.getOrCreate()

df_left = spark.range(5).withColumn('val', f.lit('left'))
df_right = spark.range(3, 7).withColumn('val', f.lit('right'))

df_merged = (
    df_left
    .alias('left')
    .join(df_right.alias('right'), on='id', how='full_outer')
    .withColumn('left_all', f.struct('left.*'))
    .withColumn('right_all', f.struct('right.*'))
)

df_merged.show()
{code}
result:
{code:java}
+---+----+-----+------------+---------+
| id| val|  val|    left_all|right_all|
+---+----+-----+------------+---------+
|  0|left| null|   {0, left}|   {null}|
|  1|left| null|   {1, left}|   {null}|
|  2|left| null|   {2, left}|   {null}|
|  3|left|right|   {3, left}|  {right}|
|  4|left|right|   {4, left}|  {right}|
|  5|null|right|{null, null}|  {right}|
|  6|null|right|{null, null}|  {right}|
+---+----+-----+------------+---------+
{code}
This behaviour started with release 3.2.0. Previously the key was not included on either side. 
Result from Spark 3.1.3
{code:java}
+---+----+-----+--------+---------+
| id| val|  val|left_all|right_all|
+---+----+-----+--------+---------+
|  0|left| null|  {left}|   {null}|
|  6|null|right|  {null}|  {right}|
|  5|null|right|  {null}|  {right}|
|  1|left| null|  {left}|   {null}|
|  3|left|right|  {left}|  {right}|
|  2|left| null|  {left}|   {null}|
|  4|left|right|  {left}|  {right}|
+---+----+-----+--------+---------+ {code}
I have a gut feeling this is related to these issues:
https://issues.apache.org/jira/browse/SPARK-39376
https://issues.apache.org/jira/browse/SPARK-34527
https://issues.apache.org/jira/browse/SPARK-38603

 ",,apachespark,cloud_fan,otakar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Feb 06 07:17:36 UTC 2023,,,,,,,,,,"0|z17vwg:",9223372036854775807,,,,,,,,,,,,,3.2.3,,,,,,,,,"22/Aug/22 03:19;gurwls223;[~karenfeng] FYI;;;","01/Sep/22 12:27;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/37758;;;","01/Sep/22 12:28;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/37758;;;","07/Sep/22 11:04;cloud_fan;Issue resolved by pull request 37758
[https://github.com/apache/spark/pull/37758];;;","07/Sep/22 11:27;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/37818;;;","07/Sep/22 11:27;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/37818;;;","06/Feb/23 07:16;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/39895;;;","06/Feb/23 07:17;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/39895;;;",,,,,,,,,,
Update ORC to 1.7.6,SPARK-40134,13477360,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,18/Aug/22 06:43,18/Aug/22 20:02,13/Jul/23 08:51,18/Aug/22 20:01,3.3.0,3.4.0,,,,,,,,,,,,,3.3.1,3.4.0,,,,Build,,,0,,,,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 18 20:01:38 UTC 2022,,,,,,,,,,"0|z17uls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Aug/22 06:45;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37563;;;","18/Aug/22 20:01;dongjoon;Issue resolved by pull request 37563
[https://github.com/apache/spark/pull/37563];;;",,,,,,,,,,,,,,,,
MultilayerPerceptronClassifier rawPredictionCol param missing from setParams,SPARK-40132,13477336,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,18/Aug/22 02:43,18/Aug/22 05:24,13/Jul/23 08:51,18/Aug/22 05:24,3.3.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,ML,,,0,,,,,,"https://issues.apache.org/jira/browse/SPARK-37398 inlined type hints in Pyspark ML's classification.py but inadvertently removed the parameter rawPredictionCol from MultilayerPerceptronClassifier's setParams. This causes its constructor to fail when this param is set in the constructor, as it isn't recognized by setParams, called by the constructor.",,apachespark,,,,,,,,,,,,,,,,,,,,,SPARK-37398,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 18 05:24:55 UTC 2022,,,,,,,,,,"0|z17ugg:",9223372036854775807,,,,,,,,,,,,,3.3.1,3.4.0,,,,,,,,"18/Aug/22 02:49;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/37561;;;","18/Aug/22 02:50;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/37561;;;","18/Aug/22 05:24;srowen;Resolved by https://github.com/apache/spark/pull/37561;;;",,,,,,,,,,,,,,,
Update TPCDS v1.4 q32 for Plan Stability tests,SPARK-40124,13477242,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kapilks_ms,kapilks_ms,kapilks_ms,17/Aug/22 13:26,27/Aug/22 20:00,13/Jul/23 08:51,18/Aug/22 16:56,3.3.0,,,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,,,,,,,,apachespark,dongjoon,kapilks_ms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 26 11:40:23 UTC 2022,,,,,,,,,,"0|z17tvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Aug/22 13:33;apachespark;User 'mskapilks' has created a pull request for this issue:
https://github.com/apache/spark/pull/37554;;;","18/Aug/22 16:56;dongjoon;Issue resolved by pull request 37554
[https://github.com/apache/spark/pull/37554];;;","22/Aug/22 11:57;apachespark;User 'mskapilks' has created a pull request for this issue:
https://github.com/apache/spark/pull/37615;;;","26/Aug/22 10:35;apachespark;User 'mskapilks' has created a pull request for this issue:
https://github.com/apache/spark/pull/37675;;;","26/Aug/22 10:35;apachespark;User 'mskapilks' has created a pull request for this issue:
https://github.com/apache/spark/pull/37675;;;","26/Aug/22 11:40;apachespark;User 'mskapilks' has created a pull request for this issue:
https://github.com/apache/spark/pull/37678;;;",,,,,,,,,,,,
Initialize projection used for Python UDF,SPARK-40121,13477216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,17/Aug/22 10:01,12/Dec/22 18:10,13/Jul/23 08:51,18/Aug/22 03:28,3.1.3,3.2.2,3.3.0,3.4.0,,,,,,,,,,,3.1.4,3.2.3,3.3.1,3.4.0,,PySpark,SQL,,0,,,,,,"{code}
>>> from pyspark.sql.functions import udf, rand
>>> spark.range(10).select(udf(lambda x: x)(rand())).show()
{code}

{code}
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$10(EvalPythonExec.scala:126)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1161)
	at scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 18 03:28:03 UTC 2022,,,,,,,,,,"0|z17tq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Aug/22 10:16;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37552;;;","18/Aug/22 03:28;gurwls223;Issue resolved by pull request 37552
[https://github.com/apache/spark/pull/37552];;;",,,,,,,,,,,,,,,,
Convert condition to java in DataFrameWriterV2.overwrite,SPARK-40117,13477168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wenlilooi,wenlilooi,wenlilooi,17/Aug/22 05:28,12/Dec/22 18:10,13/Jul/23 08:51,17/Aug/22 06:33,3.1.3,3.2.2,3.3.0,,,,,,,,,,,,3.1.4,3.2.3,3.3.1,3.4.0,,PySpark,SQL,,0,,,,,,"DataFrameWriterV2.overwrite() fails to convert the condition parameter to java. This prevents the function from being called.

It is caused by the following commit that deleted the `_to_java_column` call instead of fixing it: [https://github.com/apache/spark/commit/a1e459ed9f6777fb8d5a2d09fda666402f9230b9]",,apachespark,wenlilooi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 17 06:33:29 UTC 2022,,,,,,,,,,"0|z17tfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Aug/22 05:34;apachespark;User 'looi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37547;;;","17/Aug/22 05:34;apachespark;User 'looi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37547;;;","17/Aug/22 06:33;gurwls223;Issue resolved by pull request 37547
[https://github.com/apache/spark/pull/37547];;;",,,,,,,,,,,,,,,
Arrow 9.0.0 support with SparkR,SPARK-40114,13477165,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,17/Aug/22 05:18,12/Dec/22 17:51,13/Jul/23 08:51,17/Aug/22 14:47,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SparkR,,,0,,,,,,"{code}
== Failed ======================================================================
-- 1. Error (test_sparkSQL_arrow.R:103:3): dapply() Arrow optimization ---------
Error in `readBin(con, raw(), as.integer(dataLen), endian = ""big"")`: invalid 'n' argument
Backtrace:
 1. SparkR::collect(ret)
      at test_sparkSQL_arrow.R:103:2
 2. SparkR::collect(ret)
 3. SparkR (local) .local(x, ...)
 7. SparkR:::readRaw(conn)
 8. base::readBin(con, raw(), as.integer(dataLen), endian = ""big"")
-- 2. Error (test_sparkSQL_arrow.R:133:3): dapply() Arrow optimization - type sp
Error in `readBin(con, raw(), as.integer(dataLen), endian = ""big"")`: invalid 'n' argument
Backtrace:
 1. SparkR::collect(ret)
      at test_sparkSQL_arrow.R:133:2
 2. SparkR::collect(ret)
 3. SparkR (local) .local(x, ...)
 7. SparkR:::readRaw(conn)
 8. base::readBin(con, raw(), as.integer(dataLen), endian = ""big"")
-- 3. Error (test_sparkSQL_arrow.R:143:3): dapply() Arrow optimization - type sp
Error in `readBin(con, raw(), as.integer(dataLen), endian = ""big"")`: invalid 'n' argument
Backtrace:
  1. testthat::expect_true(all(collect(ret) == rdf))
       at test_sparkSQL_arrow.R:143:2
  5. SparkR::collect(ret)
  6. SparkR (local) .local(x, ...)
 10. SparkR:::readRaw(conn)
 11. base::readBin(con, raw(), as.integer(dataLen), endian = ""big"")
-- 4. Error (test_sparkSQL_arrow.R:184:3): gapply() Arrow optimization ---------
Error in `readBin(con, raw(), as.integer(dataLen), endian = ""big"")`: invalid 'n' argument
Backtrace:
 1. SparkR::collect(ret)
      at test_sparkSQL_arrow.R:184:2
 2. SparkR::collect(ret)
 3. SparkR (local) .local(x, ...)
 7. SparkR:::readRaw(conn)
 8. base::readBin(con, raw(), as.integer(dataLen), endian = ""big"")
-- 5. Error (test_sparkSQL_arrow.R:217:3): gapply() Arrow optimization - type sp
Error in `readBin(con, raw(), as.integer(dataLen), endian = ""big"")`: invalid 'n' argument
Backtrace:
 1. SparkR::collect(ret)
      at test_sparkSQL_arrow.R:217:2
 2. SparkR::collect(ret)
 3. SparkR (local) .local(x, ...)
 7. SparkR:::readRaw(conn)
 8. base::readBin(con, raw(), as.integer(dataLen), endian = ""big"")
-- 6. Error (test_sparkSQL_arrow.R:229:3): gapply() Arrow optimization - type sp
Error in `readBin(con, raw(), as.integer(dataLen), endian = ""big"")`: invalid 'n' argument
Backtrace:
  1. testthat::expect_true(all(collect(ret) == rdf))
       at test_sparkSQL_arrow.R:229:2
  5. SparkR::collect(ret)
  6. SparkR (local) .local(x, ...)
 10. SparkR:::readRaw(conn)
 11. base::readBin(con, raw(), as.integer(dataLen), endian = ""big"")
-- 7. Failure (test_sparkSQL_arrow.R:247:3): SPARK-32478: gapply() Arrow optimiz
`count(...)` threw an error with unexpected message.
Expected match: ""expected IntegerType, IntegerType, got IntegerType, StringType""
Actual message: ""org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 54) (APPVYR-WIN executor driver): org.apache.spark.SparkException: R unexpectedly exited.\nR worker produced errors: The tzdb package is not installed. Timezones will not be available to Arrow compute functions.\nError in arrow::write_arrow(df, raw()) : write_arrow has been removed\nCalls: <Anonymous> -> writeRaw -> writeInt -> writeBin -> <Anonymous>\nExecution halted\n\r\n\tat org.apache.spark.api.r.BaseRRunner$ReaderIterator$$anonfun$1.applyOrElse(BaseRRunner.scala:144)\r\n\tat org.apache.spark.api.r.BaseRRunner$ReaderIterator$$anonfun$1.applyOrElse(BaseRRunner.scala:137)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.r.ArrowRRunner$$anon$2.read(ArrowRRunner.scala:194)\r\n\tat org.apache.spark.sql.execution.r.ArrowRRunner$$anon$2.read(ArrowRRunner.scala:123)\r\n\tat org.apache.spark.api.r.BaseRRunner$ReaderIterator.hasNext(BaseRRunner.scala:113)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1490)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.sql.execution.r.ArrowRRunner$$anon$2.read(ArrowRRunner.scala:154)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2706)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2641)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2641)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2897)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2836)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2825)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: R unexpectedly exited.\nR worker produced errors: The tzdb package is not installed. Timezones will not be available to Arrow compute functions.\nError in arrow::write_arrow(df, raw()) : write_arrow has been removed\nCalls: <Anonymous> -> writeRaw -> writeInt -> writeBin -> <Anonymous>\nExecution halted\n\r\n\tat org.apache.spark.api.r.BaseRRunner$ReaderIterator$$anonfun$1.applyOrElse(BaseRRunner.scala:144)\r\n\tat org.apache.spark.api.r.BaseRRunner$ReaderIterator$$anonfun$1.applyOrElse(BaseRRunner.scala:137)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.r.ArrowRRunner$$anon$2.read(ArrowRRunner.scala:194)\r\n\tat org.apache.spark.sql.execution.r.ArrowRRunner$$anon$2.read(ArrowRRunner.scala:123)\r\n\tat org.apache.spark.api.r.BaseRRunner$ReaderIterator.hasNext(BaseRRunner.scala:113)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1490)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.sql.execution.r.ArrowRRunner$$anon$2.read(ArrowRRunner.scala:154)\r\n\t... 20 more\r\n""
Backtrace:
  1. testthat::expect_error(...)
       at test_sparkSQL_arrow.R:247:2
  7. SparkR::count(...)
  8. SparkR:::callJMethod(x@sdf, ""count"")
  9. SparkR:::invokeJava(isStatic = FALSE, objId$id, methodName, ...)
 10. SparkR:::handleErrors(returnStatus, conn)
== DONE ========================================================================

{code}

https://ci.appveyor.com/project/HyukjinKwon/spark/builds/44490387",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 17 14:47:34 UTC 2022,,,,,,,,,,"0|z17tew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Aug/22 11:05;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37553;;;","17/Aug/22 14:47;dongjoon;Issue resolved by pull request 37553
[https://github.com/apache/spark/pull/37553];;;",,,,,,,,,,,,,,,,
Finalize shuffle merge slow due to connection creation fails,SPARK-40096,13476951,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wankun,wankun,wankun,16/Aug/22 05:20,11/Nov/22 06:04,13/Jul/23 08:51,23/Sep/22 01:31,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,"*How to reproduce this issue*
 * Enable push based shuffle
 * Remove some merger nodes before sending finalize RPCs
 * Driver try to connect those merger shuffle services and send finalize RPC one by one, each connection creation will timeout after SPARK_NETWORK_IO_CONNECTIONCREATIONTIMEOUT_KEY (120s by default)

 
We can send these RPCs in *shuffleMergeFinalizeScheduler*  thread pool and handle the connection creation exception",,apachespark,mridulm80,wankun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Nov 11 06:04:01 UTC 2022,,,,,,,,,,"0|z17s3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Aug/22 05:53;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/37533;;;","23/Sep/22 01:31;mridulm80;Issue resolved by pull request 37533
[https://github.com/apache/spark/pull/37533];;;","25/Sep/22 11:41;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37989;;;","25/Sep/22 11:41;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37989;;;","04/Oct/22 07:21;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/38091;;;","11/Nov/22 06:03;apachespark;User 'mridulm' has created a pull request for this issue:
https://github.com/apache/spark/pull/38617;;;","11/Nov/22 06:04;apachespark;User 'mridulm' has created a pull request for this issue:
https://github.com/apache/spark/pull/38617;;;",,,,,,,,,,,
 Send TaskEnd event when task failed with NotSerializableException or TaskOutputFileAlreadyExistException to release executors for dynamic allocation ,SPARK-40094,13476924,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wangshengjie,wangshengjie,wangshengjie,16/Aug/22 02:13,24/Aug/22 17:09,13/Jul/23 08:51,24/Aug/22 17:09,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,"We found if task failed with NotSerializableException or TaskOutputFileAlreadyExistException, wont send TaskEnd event, and this will cause dynamic allocation not release executor normally.",,apachespark,mridulm80,wangshengjie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 24 17:09:19 UTC 2022,,,,,,,,,,"0|z17rxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Aug/22 02:13;wangshengjie;I'm working on this, a pr will be submitted later, thanks.
 ;;;","16/Aug/22 02:20;apachespark;User 'wangshengjie123' has created a pull request for this issue:
https://github.com/apache/spark/pull/37528;;;","16/Aug/22 02:21;apachespark;User 'wangshengjie123' has created a pull request for this issue:
https://github.com/apache/spark/pull/37528;;;","24/Aug/22 17:09;mridulm80;Issue resolved by pull request 37528
[https://github.com/apache/spark/pull/37528];;;",,,,,,,,,,,,,,
"Sorting of at least Decimal(20, 2) fails for some values near the max.",SPARK-40089,13476892,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,revans2,revans2,revans2,15/Aug/22 20:02,23/Aug/22 17:52,13/Jul/23 08:51,22/Aug/22 08:38,3.2.0,3.3.0,3.4.0,,,,,,,,,,,,3.1.4,3.2.3,3.3.1,3.4.0,,SQL,,,0,,,,,,"I have been doing some testing with Decimal values for the RAPIDS Accelerator for Apache Spark. I have been trying to add in new corner cases and when I tried to enable the maximum supported value for a sort I started to get failures.  On closer inspection it looks like the CPU is sorting things incorrectly.  Specifically anything that is ""999999999999999999.50"" or above is placed as a chunk in the wrong location in the outputs.

 In local mode with 12 tasks.
{code:java}
spark.read.parquet(""input.parquet"").orderBy(col(""a"")).collect.foreach(System.err.println) {code}
 

Here you will notice that the last entry printed is {{[999999999999999999.49]}}, and {{[999999999999999999.99]}} is near the top near {{[-999999999999999999.99]}}

",,apachespark,cloud_fan,revans2,tgraves,ulysses,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/22 20:03;revans2;input.parquet;https://issues.apache.org/jira/secure/attachment/13048141/input.parquet",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 22 08:38:53 UTC 2022,,,,,,,,,,"0|z17rq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Aug/22 20:04;revans2;I have been trying to debug this and it does not look like it is related to the partitioner. I can run with a single shuffle partition and I get the same results. Not sure if the prefix calculation is doing this or what.;;;","15/Aug/22 20:19;revans2;It sure looks like it is related to the prefix calculator. I think it is overflowing some how. I added some debugging into 3.2.0 and I got back

{code}
22/08/15 20:17:58 ERROR SortExec: PREFIX FOR 999999999999999999.99 IS false -9223372036854775808
{code}

The prefix should not be negative for non-negative values.
;;;","15/Aug/22 20:28;revans2;Looking at the code it appears that the prefix calculator has an overflow bug in it.

{code}
if (value.changePrecision(p, s)) value.toUnscaledLong else Long.MinValue
{code}

We are rounding up when changing the precision and when that happens we fall back to {{Long.MinValue}}  a.k.a -9223372036854775808, which results in the failure. ;;;","16/Aug/22 01:46;ulysses;thank you [~revans2] for reporting the issue, I can reproduce it by:
{code:java}
SELECT cast(col1 as decimal(20,2)) as c FROM VALUES (999999999999999999.50),(999999999999999999.49),(1.11) ORDER BY c;

-- output:
999999999999999999.50
1.11
999999999999999999.49{code}
 

do you want send a pr to fix it ?;;;","16/Aug/22 15:02;revans2;I have been trying to come up with a patch, but keep hitting some issues. I first tried to change 

{code}
 case dt: DecimalType if dt.precision - dt.scale <= Decimal.MAX_LONG_DIGITS =>
{code}

to

{code}
 case dt: DecimalType if dt.precision - dt.scale < Decimal.MAX_LONG_DIGITS =>
{code}

So that we would bypass the overflow case entirely and use the Double prefix logic. But when I do that the negative values all come after the positive values when sorting ascending. So now I have a lot of other tests/debugging that I need to run to understand what is happening there. Just because I think I have found another bug. 

[~ulysses] I don't have a ton of time that I can devote to this right now, I will keep working towards a patch, but if you want to put up one, then I would love to see it. ;;;","16/Aug/22 15:46;revans2;Never mind I figured out that there is a separate prefixComparator that does the same kinds of checks. But I have a fix that works, so I will put up a PR shortly.;;;","16/Aug/22 19:57;revans2;I put up a PR https://github.com/apache/spark/pull/37540;;;","16/Aug/22 19:58;apachespark;User 'revans2' has created a pull request for this issue:
https://github.com/apache/spark/pull/37540;;;","22/Aug/22 08:38;cloud_fan;Issue resolved by pull request 37540
[https://github.com/apache/spark/pull/37540];;;",,,,,,,,,
DAGScheduler may not schduler new stage in condition of push-based shuffle enabled,SPARK-40082,13476788,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,StoveM,Penglei Shi,Penglei Shi,15/Aug/22 09:24,22/Mar/23 06:11,13/Jul/23 08:51,22/Mar/23 06:11,3.1.1,,,,,,,,,,,,,,3.5.0,,,,,Scheduler,,,0,,,,,,"In condition of push-based shuffle being enabled and speculative tasks existing, a shuffleMapStage will be resubmitting once fetchFailed occurring, then its parent stages will be resubmitting firstly and it will cost some time to compute. Before the shuffleMapStage being resubmitted, its all speculative tasks success and register map output, but speculative task successful events can not trigger shuffleMergeFinalized because this stage has been removed from runningStages.

Then this stage is resubmitted, but speculative tasks have registered map output and there are no missing tasks to compute, resubmitting stages will also not trigger shuffleMergeFinalized. Eventually this stage‘s _shuffleMergedFinalized keeps false.

Then AQE will submit next stages which are dependent on  this shuffleMapStage occurring fetchFailed. And in getMissingParentStages, this stage will be marked as missing and will be resubmitted, but next stages are added to waitingStages after this stage being finished, so next stages will not be submitted even though this stage's resubmitting has been finished.

I have only met some times in my production env and it is difficult to reproduce。",,apachespark,mshen,Penglei Shi,StoveM,vs1004,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/22 09:32;Penglei Shi;missParentStages.png;https://issues.apache.org/jira/secure/attachment/13048118/missParentStages.png","15/Aug/22 09:32;Penglei Shi;shuffleMergeFinalized.png;https://issues.apache.org/jira/secure/attachment/13048116/shuffleMergeFinalized.png","15/Aug/22 09:32;Penglei Shi;submitMissingTasks.png;https://issues.apache.org/jira/secure/attachment/13048117/submitMissingTasks.png",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Mar 13 08:28:20 UTC 2023,,,,,,,,,,"0|z17r3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Aug/22 09:37;Penglei Shi;ping [~mshen] ;;;","16/Aug/22 06:19;mshen;[~csingh] [~mridul] 

Want to bring your attention to this ticket. This seems an issue that we previously saw. Does upstream already have the fix for this?;;;","10/Mar/23 07:14;StoveM;I also encountered this problem on our production environment and solved it。I will fix it;;;","13/Mar/23 08:27;apachespark;User 'Stove-hust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40393;;;","13/Mar/23 08:28;apachespark;User 'Stove-hust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40393;;;",,,,,,,,,,,,,
Add Imputer inputCols validation for empty input case,SPARK-40079,13476770,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,15/Aug/22 07:50,15/Aug/22 10:13,13/Jul/23 08:51,15/Aug/22 10:13,3.3.0,,,,,,,,,,,,,,3.1.4,3.2.3,3.3.1,3.4.0,,ML,,,0,,,,,,"If Imputer inputCols is empty, the `fit` works fine but when saving model, error will be raised:
{quote}AnalysisException: 
Datasource does not support writing empty or nested empty schemas.
Please make sure the data schema has at least one or more column(s).
{quote}",,apachespark,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 15 07:56:51 UTC 2022,,,,,,,,,,"0|z17qzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Aug/22 07:56;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/37518;;;","15/Aug/22 07:56;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/37518;;;",,,,,,,,,,,,,,,,
Executor ConfigMap is not mounted if profile is not default,SPARK-40065,13476622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nsuke,nsuke,nsuke,12/Aug/22 22:12,19/Aug/22 19:45,13/Jul/23 08:51,19/Aug/22 19:44,3.2.0,3.2.1,3.2.2,3.3.0,,,,,,,,,,,3.2.3,3.3.1,,,,Kubernetes,,,0,,,,,,"When executor config map is made optional in SPARK-34316, mount volume is unconditionally disabled erroneously when non-default profile is used.

When spark.kubernetes.executor.disableConfigMap is false, expected behavior is that the ConfigMap is mounted regardless of executor's resource profile. However, it is not mounted if the resource profile is non-default.",,apachespark,dongjoon,nsuke,,,,,,,,,,,,,,,,,,,SPARK-34316,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 19 19:44:36 UTC 2022,,,,,,,,,,"0|z17q2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Aug/22 01:52;apachespark;User 'nsuke' has created a pull request for this issue:
https://github.com/apache/spark/pull/37504;;;","13/Aug/22 01:53;apachespark;User 'nsuke' has created a pull request for this issue:
https://github.com/apache/spark/pull/37504;;;","19/Aug/22 19:44;dongjoon;This is resolved via https://github.com/apache/spark/pull/37504;;;",,,,,,,,,,,,,,,
"Cleanup ""<BLANKLINE>"" in doctest",SPARK-40057,13476534,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,12/Aug/22 09:23,12/Dec/22 18:11,13/Jul/23 08:51,12/Aug/22 14:05,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Documentation,PySpark,,0,,,,,,https://github.com/apache/spark/pull/37465#discussion_r943080421,,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 12 14:05:01 UTC 2022,,,,,,,,,,"0|z17piw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Aug/22 09:28;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37492;;;","12/Aug/22 09:29;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37492;;;","12/Aug/22 14:05;gurwls223;Issue resolved by pull request 37492
[https://github.com/apache/spark/pull/37492];;;",,,,,,,,,,,,,,,
Handle direct byte buffers in VectorizedDeltaBinaryPackedReader,SPARK-40052,13476489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,12/Aug/22 04:24,12/Aug/22 21:47,13/Jul/23 08:51,12/Aug/22 21:47,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,,,apachespark,csun,ivan.sadikov,,,,,,,,,,,,,,,,,,,,,,,SPARK-39872,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 12 21:47:17 UTC 2022,,,,,,,,,,"0|z17p8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Aug/22 04:28;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37485;;;","12/Aug/22 04:29;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37485;;;","12/Aug/22 21:47;csun;Issue resolved by pull request 37485
[https://github.com/apache/spark/pull/37485];;;",,,,,,,,,,,,,,,
The order of filtering predicates is not reasonable,SPARK-40045,13476305,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,caican,caican,caican,11/Aug/22 06:18,08/Feb/23 05:36,13/Jul/23 08:51,08/Feb/23 05:05,3.1.2,3.2.0,3.3.0,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"{code:java}
select id, data FROM testcat.ns1.ns2.table
where id =2
and md5(data) = '8cde774d6f7333752ed72cacddb05126'
and trim(data) = 'a' {code}
Based on the SQL, we currently get the filters in the following order:
{code:java}
// `(md5(cast(data#23 as binary)) = 8cde774d6f7333752ed72cacddb05126)) AND (trim(data#23, None) = a))` comes before `(id#22L = 2)`
== Physical Plan == *(1) Project [id#22L, data#23]
 +- *(1) Filter ((((isnotnull(data#23) AND isnotnull(id#22L)) AND (md5(cast(data#23 as binary)) = 8cde774d6f7333752ed72cacddb05126)) AND (trim(data#23, None) = a)) AND (id#22L = 2))
    +- BatchScan[id#22L, data#23] class org.apache.spark.sql.connector.InMemoryTable$InMemoryBatchScan{code}
In this predicate order, all data needs to participate in the evaluation, even if some data does not meet the later filtering criteria and it may causes spark tasks to execute slowly.

 

So i think that filtering predicates that need to be evaluated should automatically be placed to the far right to avoid data that does not meet the criteria being evaluated.

 

As shown below:
{noformat}
//  `(id#22L = 2)` comes before `(md5(cast(data#23 as binary)) = 8cde774d6f7333752ed72cacddb05126)) AND (trim(data#23, None) = a))`
== Physical Plan == *(1) Project [id#22L, data#23]
 +- *(1) Filter ((((isnotnull(data#23) AND isnotnull(id#22L)) AND (id#22L = 2) AND (md5(cast(data#23 as binary)) = 8cde774d6f7333752ed72cacddb05126)) AND (trim(data#23, None) = a)))
    +- BatchScan[id#22L, data#23] class org.apache.spark.sql.connector.InMemoryTable$InMemoryBatchScan{noformat}",,apachespark,caican,dongjoon,shardulm,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Feb 08 05:05:02 UTC 2023,,,,,,,,,,"0|z17o48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Aug/22 06:56;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/37479;;;","11/Aug/22 06:57;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/37479;;;","06/Feb/23 02:09;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/39892;;;","06/Feb/23 02:10;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/39892;;;","08/Feb/23 05:05;dongjoon;Issue resolved by pull request 39892
[https://github.com/apache/spark/pull/39892];;;",,,,,,,,,,,,,
LevelDB/RocksDBIterator.next should return false after iterator or db close,SPARK-40036,13476211,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,10/Aug/22 16:41,16/Aug/22 13:45,13/Jul/23 08:51,16/Aug/22 13:45,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,"{code:java}
@Test
public void testHasNextAndNextAfterIteratorClose() throws Exception {
  String prefix = ""test_db_iter_close."";
  String suffix = "".ldb"";
  File path = File.createTempFile(prefix, suffix);
  path.delete();
  LevelDB db = new LevelDB(path);
  // Write one records for test
  db.write(createCustomType1(0));

  KVStoreIterator<CustomType1> iter =
    db.view(CustomType1.class).closeableIterator();
  // iter should be true
  assertTrue(iter.hasNext());
  // close iter
  iter.close();
  // iter.hasNext should be false after iter close
  assertFalse(iter.hasNext());
  // iter.next should throw NoSuchElementException after iter close
  assertThrows(NoSuchElementException.class, iter::next);

  db.close();
  assertTrue(path.exists());
  FileUtils.deleteQuietly(path);
  assertFalse(path.exists());
}

@Test
public void testHasNextAndNextAfterDBClose() throws Exception {
  String prefix = ""test_db_db_close."";
  String suffix = "".ldb"";
  File path = File.createTempFile(prefix, suffix);
  path.delete();
  LevelDB db = new LevelDB(path);
  // Write one record for test
  db.write(createCustomType1(0));

  KVStoreIterator<CustomType1> iter =
    db.view(CustomType1.class).closeableIterator();
  // iter should be true
  assertTrue(iter.hasNext());
  // close db
  db.close();
  // iter.hasNext should be false after db close
  assertFalse(iter.hasNext());
  // iter.next should throw NoSuchElementException after db close
  assertThrows(NoSuchElementException.class, iter::next);

  assertTrue(path.exists());
  FileUtils.deleteQuietly(path);
  assertFalse(path.exists());
} {code}
 

For the above two cases, when iterator/db is closed, `hasNext` will return true, and `next` will return the value not obtained before close.",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 16 13:45:05 UTC 2022,,,,,,,,,,"0|z17njk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Aug/22 16:55;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/37471;;;","16/Aug/22 13:45;srowen;Resolved by https://github.com/apache/spark/pull/37471;;;",,,,,,,,,,,,,,,,
Limit improperly pushed down through window using ntile function,SPARK-40002,13475628,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,08/Aug/22 01:29,12/Dec/22 18:11,13/Jul/23 08:51,09/Aug/22 02:54,3.2.2,3.3.0,,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,correctness,,,,,"Limit is pushed down through a window using the ntile function, which causes results that differ from Hive 2.3.9, and Prestodb 0.268, and older versions of Spark (e.g., 3.1.3).

Assume this data:
{noformat}
create table t1 stored as parquet as
select *
from range(101);
{noformat}
Also assume this query:
{noformat}
select id, ntile(10) over (order by id) as nt
from t1
limit 10;
{noformat}
Spark 3.2.2, Spark 3.3.0, and master produce the following:
{noformat}
+---+---+
|id |nt |
+---+---+
|0  |1  |
|1  |2  |
|2  |3  |
|3  |4  |
|4  |5  |
|5  |6  |
|6  |7  |
|7  |8  |
|8  |9  |
|9  |10 |
+---+---+
{noformat}
However, Spark 3.1.3, Hive 2.3.9, and Prestodb 0.268 produce the following:
{noformat}
+---+---+
|id |nt |
+---+---+
|0  |1  |
|1  |1  |
|2  |1  |
|3  |1  |
|4  |1  |
|5  |1  |
|6  |1  |
|7  |1  |
|8  |1  |
|9  |1  |
+---+---+
{noformat}",,apachespark,bersprockets,huldar,,,,,,,,,,,,,,,,,,,,,SPARK-38614,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 09 02:54:40 UTC 2022,,,,,,,,,,"0|z17k00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Aug/22 20:08;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/37443;;;","09/Aug/22 02:54;gurwls223;Issue resolved by pull request 37443
[https://github.com/apache/spark/pull/37443];;;",,,,,,,,,,,,,,,,
"LevelDBIterator not close after used in `RemoteBlockPushResolver`, `YarnShuffleService` and `ExternalShuffleBlockResolver` ",SPARK-39988,13475279,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,05/Aug/22 05:00,08/Aug/22 02:14,13/Jul/23 08:51,08/Aug/22 02:14,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,YARN,,0,,,,,,"For example:
{code:java}
@VisibleForTesting
static ConcurrentMap<AppExecId, ExecutorShuffleInfo> reloadRegisteredExecutors(DB db)
    throws IOException {
  ConcurrentMap<AppExecId, ExecutorShuffleInfo> registeredExecutors = Maps.newConcurrentMap();
  if (db != null) {
    DBIterator itr = db.iterator();
    itr.seek(APP_KEY_PREFIX.getBytes(StandardCharsets.UTF_8));
    while (itr.hasNext()) {
      Map.Entry<byte[], byte[]> e = itr.next();
      String key = new String(e.getKey(), StandardCharsets.UTF_8);
      if (!key.startsWith(APP_KEY_PREFIX)) {
        break;
      }
      AppExecId id = parseDbAppExecKey(key);
      logger.info(""Reloading registered executors: "" +  id.toString());
      ExecutorShuffleInfo shuffleInfo = mapper.readValue(e.getValue(), ExecutorShuffleInfo.class);
      registeredExecutors.put(id, shuffleInfo);
    }
  }
  return registeredExecutors;
} {code}
DBIterator in above code should call close after used, otherwise, there is no other place to close it

 ",,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 08 02:14:45 UTC 2022,,,,,,,,,,"0|z17huo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Aug/22 05:08;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/37420;;;","05/Aug/22 05:09;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/37420;;;","08/Aug/22 02:14;dongjoon;Issue resolved by pull request 37420
[https://github.com/apache/spark/pull/37420];;;",,,,,,,,,,,,,,,
CheckOverflowInTableInsert returns exception rather than throwing it,SPARK-39981,13475195,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,jlowe,jlowe,04/Aug/22 15:40,12/Dec/22 18:11,13/Jul/23 08:51,05/Aug/22 09:33,3.3.1,3.4.0,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,,,,,,"CheckOverflowInTableInsert added by [SPARK-39865] returns the result of QueryExecutionErrors.castingCauseOverflowErrorInTableInsert rather than throwing it.  This results in a ClassCastException at runtime, since it cannot interpret the exception object as the expected value type.

For example:
{noformat}
scala> sql(""create table tiny(i tinyint)"")
22/08/04 10:27:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.
res0: org.apache.spark.sql.DataFrame = []

scala> sql(""insert into tiny values (1000);"")
java.lang.ClassCastException: org.apache.spark.SparkArithmeticException cannot be cast to java.lang.Byte
  at scala.runtime.BoxesRunTime.unboxToByte(BoxesRunTime.java:95)
  at org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getWriter$2(InternalRow.scala:171)
  at org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getWriter$2$adapted(InternalRow.scala:171)
  at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.$anonfun$fieldWriters$2(InterpretedMutableProjection.scala:76)
  at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.$anonfun$fieldWriters$2$adapted(InterpretedMutableProjection.scala:76)
  at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(InterpretedMutableProjection.scala:103)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$46.$anonfun$applyOrElse$75(Optimizer.scala:1994)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at scala.collection.TraversableLike.map(TraversableLike.scala:286)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
  at scala.collection.AbstractTraversable.map(Traversable.scala:108)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$46.applyOrElse(Optimizer.scala:1994)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$46.applyOrElse(Optimizer.scala:1989)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)
  at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.mapChildren(InsertIntoHiveTable.scala:73)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:550)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1989)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1987)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
  at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
  at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
  at scala.collection.immutable.List.foreach(List.scala:431)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:126)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:122)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:118)
  at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:136)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:154)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)
  at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204)
  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:249)
  at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:218)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  ... 47 elided
{noformat}
",,apachespark,jlowe,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 05 09:33:11 UTC 2022,,,,,,,,,,"0|z17hc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Aug/22 01:42;gurwls223;will make a quick fix soon. Thanks for reporting this.;;;","05/Aug/22 01:54;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37414;;;","05/Aug/22 09:33;gurwls223;Issue resolved by pull request 37414
[https://github.com/apache/spark/pull/37414];;;",,,,,,,,,,,,,,,
Change infra image to static tag,SPARK-39980,13475167,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,04/Aug/22 12:48,12/Dec/22 18:11,13/Jul/23 08:51,05/Aug/22 13:48,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Project Infra,,,0,,,,,,"Currently, we are using ubuntu:20.04, the 20.04 is not a static tag, we'd better to pin to static tag.",,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 05 13:48:14 UTC 2022,,,,,,,,,,"0|z17h5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Aug/22 13:04;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37403;;;","05/Aug/22 13:48;gurwls223;Issue resolved by pull request 37403
[https://github.com/apache/spark/pull/37403];;;",,,,,,,,,,,,,,,,
IndexOutOfBoundsException on groupby + apply pandas grouped map udf function,SPARK-39979,13475148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kimahriman,oren432,oren432,04/Aug/22 12:05,29/May/23 00:05,13/Jul/23 08:51,29/May/23 00:05,3.2.1,,,,,,,,,,,,,,3.5.0,,,,,PySpark,,,0,,,,,,"I'm grouping on relatively small subset of groups with big size groups.

Working with pyarrow version 2.0.0, machines memory is {color:#444444}64 GiB.{color}

I'm getting the following error:
{code:java}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 387 in stage 162.0 failed 4 times, most recent failure: Lost task 387.3 in stage 162.0 (TID 29957) (ip-172-21-129-187.eu-west-1.compute.internal executor 71): java.lang.IndexOutOfBoundsException: index: 2147483628, length: 36 (expected: range(0, 2147483648))
	at org.apache.arrow.memory.ArrowBuf.checkIndex(ArrowBuf.java:699)
	at org.apache.arrow.memory.ArrowBuf.setBytes(ArrowBuf.java:890)
	at org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1087)
	at org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:251)
	at org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:130)
	at org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)
	at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:92)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1474)
	at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:103)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2031)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270) {code}
Why do I hit this 2 GB limit? according SPARK-34588 this is supported, perhaps related to SPARK-34020.

Please assist.

Note:
Is it related to the usage of BaseVariableWidthVector and not BaseLargeVariableWidthVector?

 ",,apachespark,gurwls223,kimahriman,oren432,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon May 29 00:05:47 UTC 2023,,,,,,,,,,"0|z17h1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Aug/22 01:43;gurwls223;This is from Arrow limitation, it has to be fixed now. I actually manually tested before and check that it works.

Do you have a small reproducer for this?;;;","13/Jan/23 12:38;kimahriman;We have encountered this as well and I'm am working on a fix. This is a limitation of using the BaseVariableWidthVector based classes for string and binary and not the BaseLargeVariableWidthVector based types. A single string or binary vector cannot hold more than 2 GiB of data, because it stores offsets to each value and the offsets are stored as 4 byte integers. The large types use 8 bytes for the offset instead, thus removing the limitation.

Options are:
 * Add a config you can set to use the large types instead of the normal types
 * Just use the large types for everything, with the expense of an additional 4 bytes per record in the vector for storing offsets (not sure if there are additional overheads)

Let me know if there's any thoughts or opinions on which route to go.;;;","14/Jan/23 14:45;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/39572;;;","14/Jan/23 14:45;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/39572;;;","29/May/23 00:05;gurwls223;Issue resolved by pull request 39572
[https://github.com/apache/spark/pull/39572];;;",,,,,,,,,,,,,
NULL check in ArrayIntersect adds extraneous null from first param,SPARK-39976,13475001,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhu,navkumar,navkumar,04/Aug/22 04:57,15/Aug/22 12:55,13/Jul/23 08:51,15/Aug/22 12:55,3.3.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,correctness,,,,,"This is very likely a regression from SPARK-36829.

When using {{array_intersect(a, b)}}, if the first parameter contains a {{NULL}} value and the second one does not, an extraneous {{NULL}} is present in the output. This also leads to {{array_intersect(a, b) != array_intersect(b, a)}} which is incorrect as set intersection should be commutative.

Example using PySpark:

{code:python}
>>> a = [1, 2, 3]
>>> b = [3, None, 5]
>>> df = spark.sparkContext.parallelize(data).toDF([""a"",""b""])
>>> df.show()
+---------+------------+
|        a|           b|
+---------+------------+
|[1, 2, 3]|[3, null, 5]|
+---------+------------+

>>> df.selectExpr(""array_intersect(a,b)"").show()
+---------------------+
|array_intersect(a, b)|
+---------------------+
|                  [3]|
+---------------------+

>>> df.selectExpr(""array_intersect(b,a)"").show()
+---------------------+
|array_intersect(b, a)|
+---------------------+
|            [3, null]|
+---------------------+
{code}

Note that in the first case, {{a}} does not contain a {{NULL}}, and the final output is correct: {{[3]}}. In the second case, since {{b}} does contain {{NULL}} and is now the first parameter.

The same behavior occurs in Scala when writing to Parquet:


{code:scala}
scala> val a = Array[java.lang.Integer](1, 2, null, 4)
a: Array[Integer] = Array(1, 2, null, 4)

scala> val b = Array[java.lang.Integer](4, 5, 6, 7)
b: Array[Integer] = Array(4, 5, 6, 7)

scala> val df = Seq((a, b)).toDF(""a"",""b"")
df: org.apache.spark.sql.DataFrame = [a: array<int>, b: array<int>]

scala> df.write.parquet(""/tmp/simple.parquet"")

scala> val df = spark.read.parquet(""/tmp/simple.parquet"")
df: org.apache.spark.sql.DataFrame = [a: array<int>, b: array<int>]

scala> df.show()
+---------------+------------+
|              a|           b|
+---------------+------------+
|[1, 2, null, 4]|[4, 5, 6, 7]|
+---------------+------------+


scala> df.selectExpr(""array_intersect(a,b)"").show()
+---------------------+
|array_intersect(a, b)|
+---------------------+
|            [null, 4]|
+---------------------+


scala> df.selectExpr(""array_intersect(b,a)"").show()
+---------------------+
|array_intersect(b, a)|
+---------------------+
|                  [4]|
+---------------------+
{code}

",,apachespark,navkumar,tgraves,Zing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 08 12:00:56 UTC 2022,,,,,,,,,,"0|z17g4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Aug/22 09:53;Zing;I can reproduce this case , i will try to fix it.;;;","04/Aug/22 18:34;tgraves;[~cloud_fan]  [~angerszhuuu]  who worked on original issue. This sounds like correctness to me so we should add label if so.;;;","08/Aug/22 12:00;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37436;;;","08/Aug/22 12:00;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37436;;;",,,,,,,,,,,,,,
Create separate static image tag for infra cache,SPARK-39974,13474976,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,04/Aug/22 02:26,12/Dec/22 18:11,13/Jul/23 08:51,04/Aug/22 23:57,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Project Infra,,,0,,,,,,,,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 04 23:57:49 UTC 2022,,,,,,,,,,"0|z17fzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Aug/22 02:58;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37402;;;","04/Aug/22 02:58;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37402;;;","04/Aug/22 23:57;gurwls223;Issue resolved by pull request 37402
[https://github.com/apache/spark/pull/37402];;;",,,,,,,,,,,,,,,
Revert the test case of SPARK-39962 in branch-3.2 and branch-3.1,SPARK-39972,13474969,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,04/Aug/22 01:45,12/Dec/22 18:10,13/Jul/23 08:51,04/Aug/22 01:51,3.1.3,3.2.2,,,,,,,,,,,,,3.1.4,3.2.3,,,,Tests,,,0,,,,,,See https://github.com/apache/spark/pull/37390#issuecomment-1204658808,,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 04 01:51:49 UTC 2022,,,,,,,,,,"0|z17fxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Aug/22 01:51;gurwls223;Issue resolved by pull request 37401
[https://github.com/apache/spark/pull/37401];;;","04/Aug/22 01:51;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37401;;;",,,,,,,,,,,,,,,,
Global aggregation against pandas aggregate UDF does not take the column order into account,SPARK-39962,13474795,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,03/Aug/22 03:08,12/Dec/22 18:10,13/Jul/23 08:51,03/Aug/22 07:13,3.1.3,3.2.2,3.3.0,3.4.0,,,,,,,,,,,3.1.4,3.2.3,3.3.1,3.4.0,,PySpark,,,0,,,,,,"{code}
import pandas as pd 
from pyspark.sql import functions as f 

@f.pandas_udf(""double"") 
def AVG(x: pd.Series) -> float: 
    return x.mean() 


abc = spark.createDataFrame([(1.0, 5.0, 17.0)], schema=[""a"", ""b"", ""c""]) 
abc.agg(AVG(""a""), AVG(""c"")).show()
abc.select(""c"", ""a"").agg(AVG(""a""), AVG(""c"")).show()
{code}

{code}
+------+------+
|AVG(a)|AVG(c)|
+------+------+
|   1.0|  17.0|
+------+------+

+------+------+
|AVG(a)|AVG(c)|
+------+------+
|  17.0|   1.0|
+------+------+
{code}

Both have to be the same.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 04 01:53:20 UTC 2022,,,,,,,,,,"0|z17evk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Aug/22 04:08;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37390;;;","03/Aug/22 07:13;gurwls223;Issue resolved by pull request 37390
[https://github.com/apache/spark/pull/37390];;;","04/Aug/22 01:53;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37401;;;","04/Aug/22 01:53;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37401;;;","04/Aug/22 01:53;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37401;;;",,,,,,,,,,,,,
SaveIntoDataSourceCommand should recache result relation,SPARK-39952,13474727,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,02/Aug/22 15:31,03/Aug/22 17:06,13/Jul/23 08:51,03/Aug/22 17:06,3.4.0,,,,,,,,,,,,,,3.1.4,3.2.3,3.3.1,3.4.0,,SQL,,,0,,,,,,,,apachespark,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 03 17:06:11 UTC 2022,,,,,,,,,,"0|z17egg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Aug/22 15:40;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37380;;;","02/Aug/22 15:40;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37380;;;","03/Aug/22 17:06;cloud_fan;Issue resolved by pull request 37380
[https://github.com/apache/spark/pull/37380];;;",,,,,,,,,,,,,,,
Upgrade sbt-mima-plugin to 1.1.0,SPARK-39945,13474610,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,02/Aug/22 02:58,02/Aug/22 09:07,13/Jul/23 08:51,02/Aug/22 09:07,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Build,,,0,,,,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 02 09:07:20 UTC 2022,,,,,,,,,,"0|z17dqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Aug/22 03:02;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37371;;;","02/Aug/22 03:02;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37371;;;","02/Aug/22 09:07;dongjoon;This is resolved via https://github.com/apache/spark/pull/37371;;;",,,,,,,,,,,,,,,
Upgrade rocksdbjni to 7.4.4,SPARK-39943,13474606,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,02/Aug/22 02:51,02/Aug/22 09:02,13/Jul/23 08:51,02/Aug/22 09:02,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Build,,,0,,,,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 02 09:02:46 UTC 2022,,,,,,,,,,"0|z17dpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Aug/22 02:54;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37370;;;","02/Aug/22 02:55;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37370;;;","02/Aug/22 09:02;dongjoon;Issue resolved by pull request 37370
[https://github.com/apache/spark/pull/37370];;;",,,,,,,,,,,,,,,
Batch query cannot read the updates from streaming query if streaming query writes to the catalog table via DSv1 sink,SPARK-39940,13474602,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,02/Aug/22 02:13,02/Aug/22 08:56,13/Jul/23 08:51,02/Aug/22 08:56,3.2.3,3.3.1,3.4.0,,,,,,,,,,,,3.4.0,,,,,Structured Streaming,,,0,,,,,,"(I think this should be ancient issue but there's no good way to list up all affected versions, so I just pick up the recent version in each version line.)

When streaming query writes to catalog table via DSv1 sink, there is no refreshing/invalidation of the destination table, hence querying the destination table with batch query is not guaranteed to read the latest ""committed"" updates.
",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 02 08:56:45 UTC 2022,,,,,,,,,,"0|z17doo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Aug/22 02:14;kabhwan;Will submit a fix shortly.;;;","02/Aug/22 02:34;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/37368;;;","02/Aug/22 02:35;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/37368;;;","02/Aug/22 08:56;kabhwan;Issue resolved by pull request 37368
[https://github.com/apache/spark/pull/37368];;;",,,,,,,,,,,,,,
shift() func need support periods=0,SPARK-39939,13474598,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bzhaoop,bzhaoop,bzhaoop,02/Aug/22 01:50,03/Aug/22 10:40,13/Jul/23 08:51,03/Aug/22 10:40,3.2.2,,,,,,,,,,,,,,3.4.0,,,,,Pandas API on Spark,,,0,,,,,,"PySpark raises Error when we call shift func with periods=0.

The behavior of Pandas will return a same copy for the said obj.

 

PySpark:
{code:java}
>>> df = ps.DataFrame({'Col1': [10, 20, 15, 30, 45], 'Col2': [13, 23, 18, 33, 48],'Col3': [17, 27, 22, 37, 52]},columns=['Col1', 'Col2', 'Col3'])
>>> df.Col1.shift(periods=3)
22/08/02 09:37:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
22/08/02 09:37:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
22/08/02 09:37:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
22/08/02 09:37:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
22/08/02 09:37:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
0     NaN
1     NaN
2     NaN
3    10.0
4    20.0
Name: Col1, dtype: float64
>>> df.Col1.shift(periods=0)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/spark/spark/python/pyspark/pandas/base.py"", line 1170, in shift
    return self._shift(periods, fill_value).spark.analyzed
  File ""/home/spark/spark/python/pyspark/pandas/spark/accessors.py"", line 256, in analyzed
    return first_series(DataFrame(self._data._internal.resolved_copy))
  File ""/home/spark/spark/python/pyspark/pandas/utils.py"", line 589, in wrapped_lazy_property
    setattr(self, attr_name, fn(self))
  File ""/home/spark/spark/python/pyspark/pandas/internal.py"", line 1173, in resolved_copy
    sdf = self.spark_frame.select(self.spark_columns + list(HIDDEN_COLUMNS))
  File ""/home/spark/spark/python/pyspark/sql/dataframe.py"", line 2073, in select
    jdf = self._jdf.select(self._jcols(*cols))
  File ""/home/spark/.pyenv/versions/3.8.13/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1321, in __call__
    return_value = get_return_value(
  File ""/home/spark/spark/python/pyspark/sql/utils.py"", line 196, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: Cannot specify window frame for lag function
 {code}
Pandas:
{code:java}
>>> pdf = pd.DataFrame({'Col1': [10, 20, 15, 30, 45], 'Col2': [13, 23, 18, 33, 48],'Col3': [17, 27, 22, 37, 52]},columns=['Col1', 'Col2', 'Col3'])
>>> pdf.Col1.shift(periods=3)
0     NaN
1     NaN
2     NaN
3    10.0
4    20.0
Name: Col1, dtype: float64
>>> pdf.Col1.shift(periods=0)
0    10
1    20
2    15
3    30
4    45
Name: Col1, dtype: int64
 {code}","Pandas: 1.3.X/1.4.X

PySpark: Master",apachespark,bzhaoop,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 03 10:40:54 UTC 2022,,,,,,,,,,"0|z17dns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Aug/22 01:52;apachespark;User 'bzhaoopenstack' has created a pull request for this issue:
https://github.com/apache/spark/pull/37366;;;","03/Aug/22 10:40;podongfeng;Issue resolved by pull request 37366
[https://github.com/apache/spark/pull/37366];;;",,,,,,,,,,,,,,,,
Spark View creation with hyphens in column-type names fails,SPARK-39936,13474558,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Jeffrey_chen99,Jeffrey_chen99,Jeffrey_chen99,01/Aug/22 17:19,03/Aug/22 17:37,13/Jul/23 08:51,03/Aug/22 17:37,3.2.2,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,,,apachespark,cloud_fan,Jeffrey_chen99,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 03 17:37:16 UTC 2022,,,,,,,,,,"0|z17dew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Aug/22 23:32;apachespark;User 'Jeffreychen99' has created a pull request for this issue:
https://github.com/apache/spark/pull/37364;;;","03/Aug/22 17:37;cloud_fan;Issue resolved by pull request 37364
[https://github.com/apache/spark/pull/37364];;;",,,,,,,,,,,,,,,,
WindowExec should clear the final partition buffer,SPARK-39932,13474489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,01/Aug/22 10:31,12/Dec/22 18:10,13/Jul/23 08:51,02/Aug/22 09:08,3.4.0,,,,,,,,,,,,,,3.0.4,3.1.4,3.2.3,3.3.1,3.4.0,SQL,,,0,,,,,,,,apachespark,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 02 09:08:11 UTC 2022,,,,,,,,,,"0|z17czk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Aug/22 10:51;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37358;;;","02/Aug/22 09:08;gurwls223;Fixed in https://github.com/apache/spark/pull/37358;;;",,,,,,,,,,,,,,,,
Dataset.repartition(N) may not create N partitions,SPARK-39915,13474077,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,zsxwing,zsxwing,28/Jul/22 22:42,09/Sep/22 21:44,13/Jul/23 08:51,09/Sep/22 21:44,3.3.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,,,,,,"Looks like there is a behavior change in Dataset.repartition in 3.3.0. For example, `spark.range(10, 0).repartition(5).rdd.getNumPartitions` returns 5 in Spark 3.2.0, but 0 in Spark 3.3.0.",,apachespark,dongjoon,planga82,ulysses,xkrogen,yumwang,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Sep 09 21:44:29 UTC 2022,,,,,,,,,,"0|z17agg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Jul/22 22:43;zsxwing;cc [~cloud_fan] ;;;","06/Aug/22 21:24;planga82;Hi [~zsxwing] ,

I can't reproduce it, do you have a typo in range?
{code:java}
scala> spark.range(0, 10).repartition(5).rdd.getNumPartitions
res53: Int = 5{code};;;","07/Aug/22 04:12;zsxwing;
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/
         
Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_171)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark.range(10, 0).repartition(5).rdd.getNumPartitions
res0: Int = 0
{code}
;;;","20/Aug/22 22:40;yumwang;The reason is that it will return empty local relation since SPARK-35442:
https://github.com/apache/spark/blob/a077701d4cc36a9a6ce898ddd3b4e5fd506f6162/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala#L129-L130

cc [~ulysses];;;","21/Aug/22 03:42;zsxwing;Yeah. I would consider this is a bug since the doc of `repartition` explicitly says 

{code:java}
Returns a new Dataset that has exactly `numPartitions` partitions.
{code}
;;;","22/Aug/22 03:01;ulysses;Thank you [~yumwang] for ping me. I see this issue.

 

This is not only for empty relation optimization but also for other unary node which is at top of repartition, e.g.:
{code:java}
val df1 = spark.range(1).selectExpr(""id as c1"")
val df2 = spark.range(1).selectExpr(""id as c2"")
df1.join(df2, col(""c1"") === col(""c2"")).repartition(200, col(""c1"")).rdd.getNumPartitions 

-- output
1{code}
the `.rdd` of dataset will inject a unary node `DeserializeToObject`, so the protection of current AQE for repartition does not work. see `AQEUtils`. And the protection does not retain the  `RoundRobinPartitioning`, which makes this issue more complex.

 ;;;","22/Aug/22 03:08;ulysses;We may need a more strict machine to ensure the output partition number of repartition;;;","22/Aug/22 10:05;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37612;;;","29/Aug/22 10:36;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37706;;;","30/Aug/22 14:09;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37730;;;","30/Aug/22 14:09;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37730;;;","09/Sep/22 21:44;dongjoon;This is resolved via

- https://github.com/apache/spark/pull/37706

- https://github.com/apache/spark/pull/37730;;;",,,,,,
Issue with querying dataframe produced by 'binaryFile' format using 'not' operator,SPARK-39900,13473856,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Zing,benoit_roy,benoit_roy,27/Jul/22 20:19,12/Dec/22 18:10,13/Jul/23 08:51,03/Aug/22 12:26,3.2.1,3.3.0,,,,,,,,,,,,,3.1.4,3.2.3,3.3.1,3.4.0,,SQL,,,0,,,,,,"When creating a dataframe using the binaryFile format I am encountering weird result when filtering/query with the 'not' operator.

 

Here's a repo that will help describe and reproduce the issue.

[https://github.com/cccs-br/spark-binaryfile-issue]
{code:java}
git@github.com:cccs-br/spark-binaryfile-issue.git {code}
 

Here's a very simple test case that illustrate what's going on:

[https://github.com/cccs-br/spark-binaryfile-issue/blob/main/src/test/scala/BinaryFileSuite.scala]

TLDR;
{code:java}
   test(""binary file dataframe"") {
    // load files in directly into df using 'binaryFile' format.
    //     
    // - src/test/resources/files/
    //  - test1.csv
    //  - test2.json
    //  - test3.txt
    val df = spark
      .read
      .format(""binaryFile"")
      .load(""src/test/resources/files"")

    df.createOrReplaceTempView(""files"")

    // This works as expected.
    val like_count = spark.sql(""select * from files where path like '%.csv'"").count()
    assert(like_count === 1)

    // This does not work as expected.
    val not_like_count = spark.sql(""select * from files where path not like '%.csv'"").count()
    assert(not_like_count === 2)

    // This used to work in 3.2.1
    // df.filter(col(""path"").endsWith("".csv"") === false).show()
  }{code}",,apachespark,benoit_roy,Zing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 03 12:26:20 UTC 2022,,,,,,,,,,"0|z1793c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Jul/22 00:00;Zing;I can try to fix this issue.;;;","28/Jul/22 02:41;gurwls223;Please go ahead for a PR [~Zing];;;","31/Jul/22 16:33;apachespark;User 'zzzzming95' has created a pull request for this issue:
https://github.com/apache/spark/pull/37350;;;","02/Aug/22 15:36;benoit_roy;This is much appreciated! ;);;;","03/Aug/22 12:26;gurwls223;Issue resolved by pull request 37350
[https://github.com/apache/spark/pull/37350];;;",,,,,,,,,,,,,
The structural integrity of the plan is broken after UnwrapCastInBinaryComparison,SPARK-39896,13473801,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fchen,yumwang,yumwang,27/Jul/22 14:06,31/Aug/22 05:33,13/Jul/23 08:51,31/Aug/22 05:32,3.3.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,,,0,,,,,,"{code:scala}
sql(""create table t1(a decimal(3, 0)) using parquet"")
sql(""insert into t1 values(100), (10), (1)"")
sql(""select * from t1 where a in(100000, 10, 0, 1.00)"").show
{code}

{noformat}
After applying rule org.apache.spark.sql.catalyst.optimizer.UnwrapCastInBinaryComparison in batch Operator Optimization before Inferring Filters, the structural integrity of the plan is broken.
java.lang.RuntimeException: After applying rule org.apache.spark.sql.catalyst.optimizer.UnwrapCastInBinaryComparison in batch Operator Optimization before Inferring Filters, the structural integrity of the plan is broken.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.structuralIntegrityIsBrokenAfterApplyingRuleError(QueryExecutionErrors.scala:1325)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:229)
{noformat}

",,apachespark,chenfu,cloud_fan,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 31 05:32:51 UTC 2022,,,,,,,,,,"0|z178r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Jul/22 14:07;yumwang;cc [~fchen];;;","08/Aug/22 12:53;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/37439;;;","31/Aug/22 05:32;cloud_fan;Issue resolved by pull request 37439
[https://github.com/apache/spark/pull/37439];;;",,,,,,,,,,,,,,,
pyspark drop doesn't accept *cols ,SPARK-39895,13473747,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,santosh.pingale,santosh.pingale,santosh.pingale,27/Jul/22 09:49,12/Dec/22 18:11,13/Jul/23 08:51,11/Aug/22 03:14,3.0.3,3.2.2,3.3.0,,,,,,,,,,,,3.4.0,,,,,PySpark,,,0,,,,,,"Pyspark dataframe drop has following signature:

{color:#4c9aff}{{def drop(self, *cols: ""ColumnOrName"") -> ""DataFrame"":}}{color}

However when we try to pass multiple Column types to drop function it raises TypeError

{{each col in the param list should be a string}}

*Minimal reproducible example:*
{color:#4c9aff}values = [(""id_1"", 5, 9), (""id_2"", 5, 1), (""id_3"", 4, 3), (""id_1"", 3, 3), (""id_2"", 4, 3)]{color}
{color:#4c9aff}df = spark.createDataFrame(values, ""id string, point int, count int""){color}
|– id: string (nullable = true)|
|– point: integer (nullable = true)|
|– count: integer (nullable = true)|

{color:#4c9aff}{{df.drop(df.point, df.count)}}{color}
{quote}{color:#505f79}/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py in drop(self, *cols){color}
{color:#505f79}2537 for col in cols:{color}
{color:#505f79}2538 if not isinstance(col, str):{color}
{color:#505f79}-> 2539 raise TypeError(""each col in the param list should be a string""){color}
{color:#505f79}2540 jdf = self._jdf.drop(self._jseq(cols)){color}
{color:#505f79}2541{color}

{color:#505f79}TypeError: each col in the param list should be a string{color}
{quote}",,apachespark,santosh.pingale,,,,,,,,,,,,,,,,,,,,,,,,SPARK-40087,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"Support for PySpark to drop multiple ""Column"" ",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,,,Wed Aug 31 18:40:34 UTC 2022,,,,,,,,,,"0|z178f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Jul/22 18:35;apachespark;User 'santosh-d3vpl3x' has created a pull request for this issue:
https://github.com/apache/spark/pull/37333;;;","28/Jul/22 18:36;apachespark;User 'santosh-d3vpl3x' has created a pull request for this issue:
https://github.com/apache/spark/pull/37333;;;","28/Jul/22 19:42;apachespark;User 'santosh-d3vpl3x' has created a pull request for this issue:
https://github.com/apache/spark/pull/37335;;;","11/Aug/22 03:14;gurwls223;Issue resolved by pull request 37335
[https://github.com/apache/spark/pull/37335];;;","31/Aug/22 17:53;srowen;Not a big deal, but the example doesn't make sense to me. It's multiple cols in one string, not multiple strings or cols. Right? that doesn't seem like the right example;;;","31/Aug/22 18:40;santosh.pingale;I am not sure I understand your confusion.;;;",,,,,,,,,,,,
Expression transform error,SPARK-39887,13473659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,zhuml,zhuml,27/Jul/22 02:26,16/Aug/22 08:04,13/Jul/23 08:51,15/Aug/22 13:45,3.2.1,3.2.2,3.3.0,,,,,,,,,,,,3.1.4,3.2.3,3.3.1,3.4.0,,SQL,,,0,,,,,,"{code:java}
spark.sql(
  """"""
    |select to_date(a) a, to_date(b) b from
    |(select  a, a as b from
    |(select to_date(a) a from
    | values ('2020-02-01') as t1(a)
    | group by to_date(a)) t3
    |union all
    |select a, b from
    |(select to_date(a) a, to_date(b) b from
    |values ('2020-01-01','2020-01-02') as t1(a, b)
    | group by to_date(a), to_date(b)) t4) t5
    |group by to_date(a), to_date(b)
    |"""""".stripMargin).show(){code}
result is (2020-02-01, 2020-02-01), (2020-01-01, 2020-01-01)
expected (2020-02-01, 2020-02-01), (2020-01-01, 2020-01-02)",,apachespark,chenfu,cloud_fan,zhuml,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 16 08:04:54 UTC 2022,,,,,,,,,,"0|z177vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Jul/22 11:34;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/37319;;;","27/Jul/22 11:35;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/37319;;;","28/Jul/22 19:05;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37334;;;","28/Jul/22 19:05;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37334;;;","10/Aug/22 17:57;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37472;;;","12/Aug/22 08:07;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37491;;;","12/Aug/22 13:44;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37496;;;","15/Aug/22 13:45;cloud_fan;Issue resolved by pull request 37496
[https://github.com/apache/spark/pull/37496];;;","16/Aug/22 08:04;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37534;;;",,,,,,,,,
V2 SHOW FUNCTIONS command should print qualified function name like v1,SPARK-39880,13473591,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,26/Jul/22 15:25,27/Jul/22 14:00,13/Jul/23 08:51,27/Jul/22 14:00,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,,,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 27 14:00:37 UTC 2022,,,,,,,,,,"0|z177gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Jul/22 15:30;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/37301;;;","26/Jul/22 15:31;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/37301;;;","27/Jul/22 14:00;maxgekk;Issue resolved by pull request 37301
[https://github.com/apache/spark/pull/37301];;;",,,,,,,,,,,,,,,
Global limit should not inherit OrderPreservingUnaryNode,SPARK-39867,13473454,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,26/Jul/22 04:11,05/Aug/22 10:27,13/Jul/23 08:51,03/Aug/22 04:03,3.4.0,,,,,,,,,,,,,,3.1.4,3.2.3,3.3.1,3.4.0,,SQL,,,0,,,,,,"Global limit can not promise the output ordering is same with child, it actually depend on the certain physical plan.

 ",,apachespark,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 03 12:47:36 UTC 2022,,,,,,,,,,"0|z176m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Jul/22 04:25;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37284;;;","03/Aug/22 04:03;cloud_fan;Issue resolved by pull request 37284
[https://github.com/apache/spark/pull/37284];;;","03/Aug/22 07:34;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37394;;;","03/Aug/22 07:35;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37394;;;","03/Aug/22 12:44;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37397;;;","03/Aug/22 12:44;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37397;;;","03/Aug/22 12:47;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37398;;;","03/Aug/22 12:47;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37398;;;",,,,,,,,,,
V2ExpressionBuilder uses the wrong LiteralValue data type for In predicate,SPARK-39857,13473161,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,huaxingao,huaxingao,huaxingao,25/Jul/22 04:19,31/Jul/22 10:26,13/Jul/23 08:51,25/Jul/22 15:11,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"When building V2 In Predicate in V2ExpressionBuilder, InSet.dataType (which is BooleanType) is used to build the LiteralValue, InSet.child.dataType should be used instead.",,apachespark,dongjoon,huaxingao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Jul 31 10:26:49 UTC 2022,,,,,,,,,,"0|z174sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Jul/22 04:22;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/37271;;;","25/Jul/22 04:22;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/37271;;;","25/Jul/22 15:11;dongjoon;Issue resolved by pull request 37271
[https://github.com/apache/spark/pull/37271];;;","27/Jul/22 20:36;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/37324;;;","27/Jul/22 20:37;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/37324;;;","31/Jul/22 10:26;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37349;;;",,,,,,,,,,,,
Upgrade Kafka to 3.2.1,SPARK-39848,13473082,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,23/Jul/22 19:59,01/Aug/22 16:17,13/Jul/23 08:51,01/Aug/22 16:17,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Build,Structured Streaming,,0,,,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 01 16:17:14 UTC 2022,,,,,,,,,,"0|z174bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Jul/22 20:02;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37261;;;","23/Jul/22 20:03;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37261;;;","01/Aug/22 16:17;dongjoon;Issue resolved by pull request 37261
[https://github.com/apache/spark/pull/37261];;;",,,,,,,,,,,,,,,
Race condition related to interruption of task threads while they are in RocksDBLoader.loadLibrary(),SPARK-39847,13472996,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,23/Jul/22 01:42,23/Jul/22 22:46,13/Jul/23 08:51,23/Jul/22 22:46,3.2.0,,,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,Structured Streaming,,,0,,,,,,"One of our workloads experienced a rare failure in `RocksDBLoader`
{code:java}
Caused by: java.lang.IllegalThreadStateException
	at java.lang.Thread.start(Thread.java:708)
	at org.apache.spark.sql.execution.streaming.state.RocksDBLoader$.loadLibrary(RocksDBLoader.scala:51) {code}
After investigation, we determined that was due to task cancellation: if the task which starts the RocksDB library loading is interrupted, another thread may begin a load and crash with the thread state exception.

Skimming through the code in [RocksDBLoader|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBLoader.scala], I spot a potential race condition:
 * Although the native JNI call is uninterruptible, the thread which calls loadLibrary is still interruptible. Let’s call that thread the “task thread”.

 * Say we have two tasks, A and B, which both want to load the JNI library.

 * Say that Task A wins the race to perform the load and enters the synchronized block in loadLibrary(), spawns a child thread to perform the actual loading, then blocks in the loadLibraryThread.join() call.

 * If Task A is interrupted, an InterruptedException will be thrown and it will exit the loadLibrary synchronized block.

 * At this point, Task B enters the synchronized block and sees that exception == null because the loading thread is still running, so it calls loadLibraryThread.start() and hits the thread state error.

One way to fix this is to add
{code:java}
 if (loadLibraryThread.getState == Thread.State.NEW) {
        loadLibraryThread.start()
      }{code}
to ensure that only one thread starts the loadLibraryThread. If the original starter thread is interrupted then a new thread will encounter this block, skip the start(), proceed to the join() and block on the original load thread.

I will submit a PR with this fix.",,apachespark,dongjoon,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jul 23 22:46:49 UTC 2022,,,,,,,,,,"0|z173sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Jul/22 18:02;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/37260;;;","23/Jul/22 18:03;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/37260;;;","23/Jul/22 22:46;dongjoon;Issue resolved by pull request 37260
[https://github.com/apache/spark/pull/37260];;;",,,,,,,,,,,,,,,
Handle special case of null variable-length Decimal with non-zero offsetAndSize in UnsafeRow structural integrity check,SPARK-39839,13472840,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,22/Jul/22 07:19,28/Jul/22 00:18,13/Jul/23 08:51,28/Jul/22 00:18,3.1.0,3.2.0,3.3.0,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,,,,,,"The {{UnsafeRow}} structural integrity check in {{UnsafeRowUtils.validateStructuralIntegrity}} is added in Spark 3.1.0. It’s supposed to validate that a given {{UnsafeRow}} conforms to the format that the {{UnsafeRowWriter}} would have produced.

Currently the check expects all fields that are marked as null should also have its field (i.e. the fixed-length part) set to all zeros. It needs to be updated to handle a special case for variable-length {{{}Decimal{}}}s, where the {{UnsafeRowWriter}} may mark a field as null but also leave the fixed-length part of the field as {{OffsetAndSize(offset=current_offset, size=0)}}. This may happen when the {{Decimal}} being written is either a real {{null}} or has overflowed the specified precision.

Logic in {{UnsafeRowWriter}}:

in general:
{code:scala}
  public void setNullAt(int ordinal) {
    BitSetMethods.set(getBuffer(), startingOffset, ordinal); // set null bit
    write(ordinal, 0L);                                      // also zero out the fixed-length field
  } {code}
special case for {{DecimalType}}:
{code:scala}
      // Make sure Decimal object has the same scale as DecimalType.
      // Note that we may pass in null Decimal object to set null for it.
      if (input == null || !input.changePrecision(precision, scale)) {
        BitSetMethods.set(getBuffer(), startingOffset, ordinal); // set null bit
        // keep the offset for future update
        setOffsetAndSize(ordinal, 0);                            // doesn't zero out the fixed-length field
      } {code}
The special case is introduced to allow all {{DecimalType}}s (including both fixed-length and variable-length ones) to be mutable – thus need to leave space for the variable-length field even if it’s currently null.

Note that this special case in {{UnsafeRowWriter}} has been there since Spark 1.6.0, where as the integrity check was added in Spark 3.1.0. The check was originally added for Structured Streaming’s checkpoint evolution validation, so that a newer version of Spark can check whether or not an older checkpoint file for Structured Streaming queries can be supported, and/or if the contents of the checkpoint file is corrupted.",,apachespark,kabhwan,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 28 00:18:55 UTC 2022,,,,,,,,,,"0|z172ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Jul/22 07:26;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/37252;;;","28/Jul/22 00:18;kabhwan;Issue resolved by pull request 37252
[https://github.com/apache/spark/pull/37252];;;",,,,,,,,,,,,,,,,
Fix EliminateSorts remove global sort below the local sort,SPARK-39835,13472815,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,22/Jul/22 03:17,03/Aug/22 03:16,13/Jul/23 08:51,25/Jul/22 10:35,3.4.0,,,,,,,,,,,,,,3.1.4,3.2.3,3.3.1,3.4.0,,SQL,,,0,,,,,,"If a global sort below locol sort, we should not remove the global sort becuase the output partitioning can be affected.

This issue is going to worse since we pull out the V1 Write sort to logcial side.",,apachespark,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 25 12:45:37 UTC 2022,,,,,,,,,,"0|z172o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Jul/22 03:49;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37250;;;","25/Jul/22 10:35;cloud_fan;Issue resolved by pull request 37250
[https://github.com/apache/spark/pull/37250];;;","25/Jul/22 12:38;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37275;;;","25/Jul/22 12:45;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37276;;;",,,,,,,,,,,,,,
Filtered parquet data frame count() and show() produce inconsistent results when spark.sql.parquet.filterPushdown is true,SPARK-39833,13472785,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,msa,msa,21/Jul/22 22:05,12/Dec/22 18:10,13/Jul/23 08:51,21/Aug/22 10:07,3.2.1,3.3.0,,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,SQL,,,0,correctness,,,,,"One of our data scientists discovered a problem wherein a data frame `.show()` call printed non-empty results, but `.count()` printed 0. I've narrowed the issue to a small, reproducible test case which exhibits this aberrant behavior. In pyspark, run the following code:
{code:python}
from pyspark.sql.types import *
parquet_pushdown_bug_df = spark.createDataFrame([{""COL0"": int(0)}], schema=StructType(fields=[StructField(""COL0"",IntegerType(),True)]))
parquet_pushdown_bug_df.repartition(1).write.mode(""overwrite"").parquet(""parquet_pushdown_bug/col0=0/parquet_pushdown_bug.parquet"")
reread_parquet_pushdown_bug_df = spark.read.parquet(""parquet_pushdown_bug"")
reread_parquet_pushdown_bug_df.filter(""col0 = 0"").show()
print(reread_parquet_pushdown_bug_df.filter(""col0 = 0"").count())
{code}
In my usage, this prints a data frame with 1 row and a count of 0. However, disabling `spark.sql.parquet.filterPushdown` produces consistent results:
{code:python}
spark.conf.set(""spark.sql.parquet.filterPushdown"", False)
reread_parquet_pushdown_bug_df.filter(""col0 = 0"").show()
reread_parquet_pushdown_bug_df.filter(""col0 = 0"").count()
{code}
This will print the same data frame, however it will print a count of 1. The key to triggering this bug is not just enabling `spark.sql.parquet.filterPushdown` (which is enabled by default). The case of the column in the data frame (before writing) must differ from the case of the partition column in the file path, i.e. COL0 versus col0 or col0 versus COL0.",,apachespark,ivan.sadikov,msa,ulysses,,,,,,,,,,,,,,,,,,,,SPARK-40169,,PARQUET-2170,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Aug 21 10:07:43 UTC 2022,,,,,,,,,,"0|z172hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Jul/22 06:45;gurwls223;Seems like a bug from Parquet side in rowgroup filtering.;;;","27/Jul/22 07:12;ivan.sadikov;Interesting, I will take a look.;;;","04/Aug/22 05:47;ivan.sadikov;Your example should work with 

spark.conf.set(""spark.sql.caseSensitive"", ""true"")

Or when disabling column index.

spark.conf.set(""parquet.filter.columnindex.enabled"", ""false"")

 

In fact, you should be getting a log4j warning for duplicate columns:

05:44:55.016 WARN org.apache.spark.sql.execution.datasources.DataSource: Found duplicate column(s) in the data schema and the partition schema: `col`

 

That said, it is still a bug to return a different row count due to case-insensitive analysis in Spark and a bug in filtering in Parquet-Mr. I will open a PR to fix it.;;;","05/Aug/22 01:47;ivan.sadikov;It appears to be a bug in Parquet-Mr. 

There is a condition in ParquetFileReader that determines the total number of records that we expect when doing predicate pushdown. Depending on whether or not column index feature is enabled, we would either return all filtered rows from a row group or row ranges. The bug is checking column paths against an empty set which is created when the projection is empty. In this case, we should return all row group rows instead of failing the condition and returning 0 records.;;;","05/Aug/22 05:06;ivan.sadikov;I opened a PR to quickly fix it: https://github.com/apache/spark/pull/37419.

Here is the ticket to address the issue in Parquet-Mr: https://issues.apache.org/jira/browse/PARQUET-2170.;;;","05/Aug/22 05:07;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37419;;;","05/Aug/22 05:08;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37419;;;","21/Aug/22 10:07;gurwls223;Issue resolved by pull request 37419
[https://github.com/apache/spark/pull/37419];;;",,,,,,,,,,
Add a test case to read ORC table that requires type promotion,SPARK-39830,13472678,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dzcxzl,dzcxzl,dzcxzl,21/Jul/22 12:18,09/Sep/22 21:37,13/Jul/23 08:51,06/Sep/22 05:35,3.3.0,,,,,,,,,,,,,,3.3.1,3.4.0,,,,SQL,Tests,,0,,,,,,"We can add a UT to test the scenario after the ORC-1205 release.

 

bin/spark-shell
{code:java}
spark.sql(""set orc.stripe.size=10240"")
spark.sql(""set orc.rows.between.memory.checks=1"")
spark.sql(""set spark.sql.orc.columnarWriterBatchSize=1"")
val df = spark.range(1, 1+512, 1, 1).map { i =>
    if( i == 1 ){
        (i, Array.fill[Byte](5 * 1024 * 1024)('X'))
    } else {
        (i,Array.fill[Byte](1)('X'))
    }
    }.toDF(""c1"",""c2"")
df.write.format(""orc"").save(""file:///tmp/test_table_orc_t1"")
spark.sql(""create external table test_table_orc_t1 (c1 string ,c2 binary) location 'file:///tmp/test_table_orc_t1' stored as orc "")
spark.sql(""select * from test_table_orc_t1"").show() {code}
Querying this table will get the following exception
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.orc.impl.TreeReaderFactory$TreeReader.nextVector(TreeReaderFactory.java:387)
        at org.apache.orc.impl.TreeReaderFactory$LongTreeReader.nextVector(TreeReaderFactory.java:740)
        at org.apache.orc.impl.ConvertTreeReaderFactory$StringGroupFromAnyIntegerTreeReader.nextVector(ConvertTreeReaderFactory.java:1069)
        at org.apache.orc.impl.reader.tree.StructBatchReader.readBatchColumn(StructBatchReader.java:65)
        at org.apache.orc.impl.reader.tree.StructBatchReader.nextBatchForLevel(StructBatchReader.java:100)
        at org.apache.orc.impl.reader.tree.StructBatchReader.nextBatch(StructBatchReader.java:77)
        at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1371)
        at org.apache.orc.mapreduce.OrcMapreduceRecordReader.ensureBatch(OrcMapreduceRecordReader.java:84)
        at org.apache.orc.mapreduce.OrcMapreduceRecordReader.nextKeyValue(OrcMapreduceRecordReader.java:102)
        at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
 {code}
 ",,apachespark,dongjoon,dzcxzl,xkrogen,,,,,,,,,,,,,,,,ORC-1205,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Sep 09 21:37:32 UTC 2022,,,,,,,,,,"0|z171ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Jul/22 12:20;dzcxzl;cc @[~dongjoon];;;","21/Jul/22 17:27;dongjoon;Thank you, [~dzcxzl] .;;;","05/Sep/22 06:29;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/37800;;;","05/Sep/22 06:29;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/37800;;;","06/Sep/22 05:35;dongjoon;This is resolved via https://github.com/apache/spark/pull/37800;;;","06/Sep/22 11:25;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/37808;;;","09/Sep/22 21:37;dongjoon;This is backported to branch-3.3 via https://github.com/apache/spark/pull/37808;;;",,,,,,,,,,,
Upgrade log4j2 to 2.18.0,SPARK-39829,13472584,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,21/Jul/22 04:57,12/Dec/22 18:10,13/Jul/23 08:51,21/Jul/22 13:40,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Build,,,0,,,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 21 13:40:19 UTC 2022,,,,,,,,,,"0|z1718w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Jul/22 05:00;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37242;;;","21/Jul/22 05:01;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37242;;;","21/Jul/22 13:40;gurwls223;Issue resolved by pull request 37242
[https://github.com/apache/spark/pull/37242];;;",,,,,,,,,,,,,,,
Unable to connect to Presto in Pyspark: java.lang.ClassNotFoundException: com.facebook.presto.jdbc.PrestoDriver,SPARK-39813,13472183,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,dlaz,dlaz,19/Jul/22 04:50,19/Jul/22 10:48,13/Jul/23 08:51,19/Jul/22 10:48,3.3.0,,,,,,,,,,,,,,,,,,,PySpark,,,0,AWS,,,,,"My team has a bash script + python script that uses pyspark to extract data from a hive database. The scripts work when run on a server. However, we need to containerize this since we will not have access to that server in the future.

Thus, I am trying to get the job to work from a container. 

When trying to run the scripts locally or in a docker container, I am running into driver issues. Unfortunately, nobody on my team helped set up the environment on the server, where everything is working. Thus, we are having a hard time figuring out what is wrong with our local/containerized environments and cannot replicate a successful script run. 

 

From a container, I run a bash script that does the following:

 

```\{bash}

$SPARK_HOME/bin/spark-submit etl_job.py

```

 

The contents of `etl_job.py` are as follows:

```\{python}
print('\n\nStarting python job\n\n')
from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
from credentials import PRESTO_USER, PRESTO_PASSWORD, PRESTO_URL, PRESTO_SSL, SSL_PASSWORD, TEST_SCHEMA, TEST_TABLE
import pandas as pd

print('\n\nStarting spark session \n\n')
sc = SparkContext.getOrCreate()
spark = SparkSession(sc)

print('\n\nConnecting to Presto\n\n')
Prestoprod = (
spark.read.format(""jdbc"")
.option(""url"", PRESTO_URL)
.option(""user"", PRESTO_USER)
.option(""password"", PRESTO_PASSWORD)
.option(""driver"", ""com.facebook.presto.jdbc.PrestoDriver"")
.option(""SSL"", ""true"")
.option(""SSLKeyStorePath"", PRESTO_SSL)
.option(""SSLKeyStorePassword"", SSL_PASSWORD)
)

 
print('\n\nTrying to query Presto.\n\n')
query = ""select * from hive.{}.{}"".format(TEST_SCHEMA, TEST_TABLE)

results = (
Prestoprod.option(
""query"",
query

)
.load()
)
```

 

However, when I run the job, I get the following error:

```

py4j.protocol.Py4JJavaError: An error occurred while calling o33.load.
: java.lang.ClassNotFoundException: com.facebook.presto.jdbc.PrestoDriver

```

 

Here is the full log:

```

Starting python job

 

Starting spark session 

Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/07/19 04:45:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

Connecting to Presto

 

Trying to query Presto.

Traceback (most recent call last):
  File ""/home/jovyan/etl_job.py"", line 30, in <module>
    .load()
  File ""/usr/local/spark/python/pyspark/sql/readwriter.py"", line 184, in load
    return self._df(self._jreader.load())
  File ""/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py"", line 1321, in {_}{{_}}call{{_}}{_}
  File ""/usr/local/spark/python/pyspark/sql/utils.py"", line 190, in deco
    return f(*a, **kw)
  File ""/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py"", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o33.load.
: java.lang.ClassNotFoundException: com.facebook.presto.jdbc.PrestoDriver
        at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)
        at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)
        at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)
        at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)
        at scala.Option.foreach(Option.scala:437)
        at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)
        at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
        at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
        at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
        at scala.Option.getOrElse(Option.scala:201)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:568)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:833)

```

 

Other relevant info is that I am trying to run this on a container built from the jupyter/all-spark-notebook image fron dockerhub.",I am running this in a docker built from the jupyter/all-spark-notebook image.,Alistar,dlaz,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Java,python,shell,Tue Jul 19 10:48:16 UTC 2022,,,,,,,,,,"0|z16yrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Jul/22 06:04;Alistar;Did you add jdbc driver (com.facebook.presto.jdbc.PrestoDriver) to classpath of driver and executor ?
You can use
 * ""--jars"" in spark-submit command
 * add driver file to jars folder in SPARK_HOME
 * specify in spark-default.conf (spark.executor.extraClassPath and spark.driver.extraClassPath);;;","19/Jul/22 10:48;dlaz;Comment by than nguyen dinh helped me resolve this issue.

 

Thank you!;;;",,,,,,,,,,,,,,,,
"Column backticks are misplaced in the AnalysisException [UNRESOLVED_COLUMN] error message when using field with "".""",SPARK-39783,13471648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,EnricoMi,EnricoMi,EnricoMi,14/Jul/22 22:13,18/Oct/22 10:05,13/Jul/23 08:51,18/Oct/22 10:05,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"AnalysisException {{[UNRESOLVED_COLUMN.WITH_SUGGESTION]}} shows the wrong suggestion when a field with the ""."" separator is referenced without backticks.

The following code references a nested value {{{}`the`.`id`{}}}, that does not exist. The proposed / existing column names are not correctly wrapped in backticks:
{code:scala}
Seq((0)).toDF(""the.id"")
  .select(""the.id"").show()

org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `the`.`id` cannot be resolved. Did you mean one of the following? [`the`.`id`];
{code}
Instead of suggesting {{[`the`.`id`]}} in the *error message*, you would expect suggesting {{[`the.id`]}}.",,apachespark,EnricoMi,ivan.sadikov,maxgekk,planga82,,,,,,,,,,,,,,,,,,,,,SPARK-39492,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Oct 18 10:05:13 UTC 2022,,,,,,,,,,"0|z16vhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Jul/22 22:17;EnricoMi;[~srielau] [~cloud_fan] ;;;","14/Oct/22 04:27;ivan.sadikov;This is by design if I am not mistaken. Such columns need to be escaped, for example, this works just fine: 
{code:java}
Seq((0)).toDF(""the.id"").select(""`the.id`"").show() {code}
IMHO, it is not an issue.;;;","14/Oct/22 05:36;EnricoMi;This issue is about the error message, not the fact that I need to escape {{the.id}} in {{select}}.;;;","14/Oct/22 06:31;ivan.sadikov;It is not clear from the ticket, you should update the description to clarify that this is about the error message.

I don't see any issues with the error message, ""the.id"" is unresolved due to the fact that by default this is interpreted as struct field access, not a literal column name. Are you referring to the wrong suggestion in the error message?

 

I opened a PR to fix the suggestion list.;;;","14/Oct/22 07:10;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38254;;;","14/Oct/22 07:51;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38256;;;","14/Oct/22 07:51;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/38256;;;","18/Oct/22 10:05;maxgekk;Issue resolved by pull request 38256
[https://github.com/apache/spark/pull/38256];;;",,,,,,,,,,
Join‘ verbose string didn't contains JoinType,SPARK-39776,13471533,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,14/Jul/22 07:23,02/Aug/22 11:00,13/Jul/23 08:51,25/Jul/22 09:54,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,," Current verbose string don't have joinType


{code:java}
(5) BroadcastHashJoin [codegen id : 8]
Left keys [1]: [ss_sold_date_sk#3]
Right keys [1]: [d_date_sk#5]
Join condition: None
{code}

{code:java}
override def verboseStringWithOperatorId(): String = {
    val joinCondStr = if (condition.isDefined) {
      s""${condition.get}""
    } else ""None""
    if (leftKeys.nonEmpty || rightKeys.nonEmpty) {
      s""""""
         |$formattedNodeName
         |${ExplainUtils.generateFieldString(""Left keys"", leftKeys)}
         |${ExplainUtils.generateFieldString(""Right keys"", rightKeys)}
         |${ExplainUtils.generateFieldString(""Join condition"", joinCondStr)}
         |"""""".stripMargin
    } else {
      s""""""
         |$formattedNodeName
         |${ExplainUtils.generateFieldString(""Join condition"", joinCondStr)}
         |"""""".stripMargin
    }
  }
{code}
",,angerszhuuu,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 02 11:00:41 UTC 2022,,,,,,,,,,"0|z16us0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Jul/22 08:57;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37190;;;","02/Aug/22 11:00;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37378;;;",,,,,,,,,,,,,,,,
Regression due to AVRO-2035,SPARK-39775,13471526,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,nandininelson,nandininelson,14/Jul/22 06:59,05/Aug/22 03:27,13/Jul/23 08:51,05/Aug/22 03:27,3.2.1,,,,,,,,,,,,,,3.2.3,3.3.1,3.4.0,,,Spark Core,,,0,,,,,,"With the upgrade in Avro version to 1.9.0, for schema evolution Avro added https://issues.apache.org/jira/browse/AVRO-2035(enable validation of default values in schemas by default) which is causing regressions when user upgrades their Spark verion.

Repro code:
{code:java}
import org.apache.spark.sql.avro.functions._
val avroTypeStruct = s""""""
                        |{
                        |  ""type"": ""record"",
                        |  ""name"": ""struct"",
                        |  ""fields"": [
                        |    {""name"": ""id"", ""type"": ""long"", ""default"": null}
                        |  ]
                        |}"""""".stripMargin

val df = spark.range(10).select(struct('id).as(""struct""))
val avroStructDF = df.select(to_avro('struct, avroTypeStruct).as(""avro""))
avroStructDF.select(from_avro('avro, avroTypeStruct)).show()
{code}

Hive mitigated it by disabling this feature altogether in https://issues.apache.org/jira/browse/HIVE-24797  
Spark-Hive integration also imported the above changes in https://issues.apache.org/jira/browse/SPARK-34512

Can we have a fix for all the senarios?",,apachespark,cloud_fan,nandininelson,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 05 03:27:09 UTC 2022,,,,,,,,,,"0|z16uqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Jul/22 12:33;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37191;;;","05/Aug/22 03:27;cloud_fan;Issue resolved by pull request 37191
[https://github.com/apache/spark/pull/37191];;;",,,,,,,,,,,,,,,,
Add Apache Spark images info in running-on-kubernetes doc,SPARK-39761,13471384,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,13/Jul/22 10:55,15/Jul/22 00:37,13/Jul/23 08:51,15/Jul/22 00:37,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Documentation,Kubernetes,,0,,,,,,,,apachespark,XinrongM,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 15 00:34:23 UTC 2022,,,,,,,,,,"0|z16tuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Jul/22 11:06;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37174;;;","13/Jul/22 11:07;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37174;;;","15/Jul/22 00:34;XinrongM;Fixed in [https://github.com/apache/spark/pull/37174];;;",,,,,,,,,,,,,,,
NPE on invalid patterns from the regexp functions,SPARK-39758,13471329,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,13/Jul/22 06:30,14/Jul/22 14:50,13/Jul/23 08:51,13/Jul/22 14:10,3.3.0,,,,,,,,,,,,,,3.2.2,3.3.1,3.4.0,,,SQL,,,0,,,,,,"The example below reproduces the issue:

{code:sql}
spark-sql> SELECT regexp_extract('1a 2b 14m', '(?l)');
22/07/12 19:07:21 ERROR SparkSQLDriver: Failed in [SELECT regexp_extract('1a 2b 14m', '(?l)')]
java.lang.NullPointerException: null
	at org.apache.spark.sql.catalyst.expressions.RegExpExtractBase.getLastMatcher(regexpExpressions.scala:768) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
{code}
",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 13 21:19:25 UTC 2022,,,,,,,,,,"0|z16tio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Jul/22 07:41;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/37171;;;","13/Jul/22 14:10;maxgekk;Issue resolved by pull request 37171
[https://github.com/apache/spark/pull/37171];;;","13/Jul/22 20:13;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/37181;;;","13/Jul/22 20:13;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/37181;;;","13/Jul/22 21:18;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/37182;;;","13/Jul/22 21:19;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/37182;;;",,,,,,,,,,,,
Unable to set zstd compression level while writing parquet files,SPARK-39743,13470972,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Zing,yeachan153,yeachan153,11/Jul/22 10:06,08/Jan/23 15:23,13/Jul/23 08:51,10/Aug/22 19:11,3.2.0,,,,,,,,,,,,,,3.4.0,,,,,Spark Core,,,0,,,,,,"While writing zstd compressed parquet files, the following setting `spark.io.compression.zstd.level` does not have any affect with regards to the compression level of zstd.

All files seem to be written with the default zstd compression level, and the config option seems to be ignored.

Using the zstd cli tool, we confirmed that setting a higher compression level for the same file tested in spark resulted in a smaller file.",,apachespark,dongjoon,euigeun_chung,yeachan153,zhtttylzz,zhttylz,Zing,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Jan 08 15:23:32 UTC 2023,,,,,,,,,,"0|z16rbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Jul/22 16:27;Zing;[~yeachan153] 

I'm Interested in this, could you assign it to me？;;;","13/Jul/22 00:22;gurwls223;No need to assign somebody. you can start working on this directly;;;","24/Jul/22 08:40;apachespark;User 'zzzzming95' has created a pull request for this issue:
https://github.com/apache/spark/pull/37263;;;","24/Jul/22 08:41;apachespark;User 'zzzzming95' has created a pull request for this issue:
https://github.com/apache/spark/pull/37263;;;","27/Jul/22 15:19;Zing;[~yeachan153] 

spark.io.compression.zstd.level is adapted to {{{}spark.io.compression.codec{}}}. It only works on internal data.

 

If you want to set a different zstd level to write parquet files , you can set `parquet.compression.codec.zstd.level` in sparkConf, like :

 
{code:java}
val spark = SparkSession
      .builder()
      .master(""local"")
      .appName(""spark example"")
      .config(""spark.sql.parquet.compression.codec"", ""zstd"")
      .config(""parquet.compression.codec.zstd.level"", 10)  // here 
      .getOrCreate(); 

    val csvfile = spark.read.csv(""file:///home/test_data/Reviews.csv"")
    csvfile.coalesce(1).write.parquet(""file:///home/test_data/nn_parq_10""){code}
 

 ;;;","28/Jul/22 00:07;Zing;[~hyukjin.kwon]

 

can you mark this issue as `Resolved`.;;;","28/Jul/22 08:28;yeachan153;OK, thanks for the clarification. Should we look at improving the documentation for this?

For example, for AVRO files, we have a config option to determine the compression level in  https://spark.apache.org/docs/latest/configuration.html, is there a reason we also don't do that for parquet? I don't see it in the config docs or the parquet files docs: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html. If we don't have it documented, should we add it or at least add a link to https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/README.md?

The docs also say that `spark.io.compression.zstd.level` controls the compression level for the zstd codec. Based on this, I didn't realise that this only applied to internal data. Should we make that clearer in the config docs?



;;;","28/Jul/22 16:31;Zing;[~yeachan153] 

Only some more important parameters are supported in spark, such as `avro.mapred.deflate.level`. Other less important parameters are usually not supported in spark, such as `avro.mapred.zstd.level` and `parquet.compression.codec.zstd.level`. Unless there is a special reason.

In addition, the problem you mentioned does exist, and I will modify the corresponding parameter description in the document later.;;;","28/Jul/22 16:39;Zing;[~hyukjin.kwon] 

As [~yeachan153]  said, is it possible to add some clarification in spark docs or post external docs about some parameters that are not controlled by spark  (like parquet zstd level). Not sure if this is possible as I haven't found a link to other libraries documentation in the spark doc.;;;","05/Aug/22 03:25;apachespark;User 'zzzzming95' has created a pull request for this issue:
https://github.com/apache/spark/pull/37416;;;","10/Aug/22 19:11;dongjoon;Issue resolved by pull request 37416
[https://github.com/apache/spark/pull/37416];;;","06/Jan/23 02:31;euigeun_chung;I'm sorry to ask a question here but I couldn't find the answer on Internet.

Can I set the zstd compression level for ORC?;;;","08/Jan/23 15:23;Zing;[~euigeun_chung] 

 

Currently, Spark uses `airCompressor` lib to support ORC writes, and does not support setting ZSTD levels~

https://github.com/airlift/aircompressor/blob/79265477482a4594efcc87aaef92b589c6ec2277/src/main/java/io/airlift/compress/zstd/ZstdCompressor.java#L113;;;",,,,,
vis-timeline @ 4.2.1 vulnerable to XSS attacks,SPARK-39740,13470915,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shrprasa,ess-truveta,ess-truveta,11/Jul/22 05:59,24/Jun/23 15:05,13/Jul/23 08:51,24/Jun/23 15:05,3.2.1,3.3.0,,,,,,,,,,,,,3.5.0,,,,,Web UI,,,0,,,,,,"Spark UI includes visjs/vis-timeline package@4.2.1, which is vulnerable to XSS attacks ([Cross-site Scripting in vis-timeline · CVE-2020-28487 · GitHub Advisory Database|https://github.com/advisories/GHSA-9mrv-456v-pf22]). This version should be replaced with the next non-vulnerable issue - [Release v7.4.4 · visjs/vis-timeline (github.com)|https://github.com/visjs/vis-timeline/releases/tag/v7.4.4] or higher.",,ess-truveta,githubbot,unamesk15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jun 24 15:05:22 UTC 2023,,,,,,,,,,"0|z16qz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Nov/22 17:02;ess-truveta;Any updates here?;;;","16/Jun/23 09:03;githubbot;User 'shrprasa' has created a pull request for this issue:
https://github.com/apache/spark/pull/41613;;;","16/Jun/23 09:04;githubbot;User 'shrprasa' has created a pull request for this issue:
https://github.com/apache/spark/pull/41613;;;","24/Jun/23 15:04;srowen;Not clear that this affects Spark, but we can upgrade;;;","24/Jun/23 15:05;srowen;Issue resolved by pull request 41613
[https://github.com/apache/spark/pull/41613];;;",,,,,,,,,,,,,
Correctness issue when parsing dates with yyyyMMdd format in CSV and JSON,SPARK-39731,13470890,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,11/Jul/22 01:10,20/Sep/22 07:04,13/Jul/23 08:51,27/Jul/22 08:46,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,SQL,,,0,,,,,,"In Spark 3.x, when reading CSV data like this:
{code:java}
name,mydate
1,2020011
2,20201203{code}
and specifying date pattern as ""yyyyMMdd"", dates are not parsed correctly with CORRECTED time parser policy.

For example,
{code:java}
val df = spark.read.schema(""name string, mydate date"").option(""dateFormat"", ""yyyyMMdd"").option(""header"", ""true"").csv(""file:/tmp/test.csv"")

df.show(false){code}
Returns:
{code:java}
+----+--------------+
|name|mydate        |
+----+--------------+
|1   |+2020011-01-01|
|2   |2020-12-03    |
+----+--------------+ {code}
and it used to return null instead of the invalid date in Spark 3.2 or below.

 

The issue appears to be caused by this PR: [https://github.com/apache/spark/pull/32959].

 

A similar issue can observed in JSON data source.

test.json
{code:java}
{""date"": ""2020011""}
{""date"": ""20201203""} {code}
 

Running commands
{code:java}
val df = spark.read.schema(""date date"").option(""dateFormat"", ""yyyyMMdd"").json(""file:/tmp/test.json"")
df.show(false) {code}
returns
{code:java}
+--------------+
|date          |
+--------------+
|+2020011-01-01|
|2020-12-03    |
+--------------+{code}
but before the patch linked in the description it used to show:
{code:java}
+----------+
|date      |
+----------+
|7500-08-09|
|2020-12-03|
+----------+{code}
which is strange either way. I will try to address it in the PR.",,apachespark,cloud_fan,ivan.sadikov,,,,,,,,,,,,,,,,,,,,,SPARK-40215,SPARK-40496,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 27 08:46:45 UTC 2022,,,,,,,,,,"0|z16qtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Jul/22 01:24;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37147;;;","11/Jul/22 01:24;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37147;;;","27/Jul/22 08:46;cloud_fan;Issue resolved by pull request 37147
[https://github.com/apache/spark/pull/37147];;;",,,,,,,,,,,,,,,
Resolve pyspark mypy part tests.,SPARK-39714,13470614,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bzhaoop,bzhaoop,bzhaoop,08/Jul/22 03:41,12/Dec/22 18:10,13/Jul/23 08:51,13/Jul/22 00:46,3.3.0,,,,,,,,,,,,,,3.4.0,,,,,PySpark,,,0,,,,,,"Currently, pyspark failed for mypy_test. 

It might break the common code review and merge progress.

So we open this Jira for tracing the whole resolve progress.",,apachespark,bzhaoop,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 13 00:46:08 UTC 2022,,,,,,,,,,"0|z16p4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Jul/22 03:44;bzhaoop;[https://github.com/apache/spark/pull/37117]

I opened a PR for resolving annotation tests.;;;","08/Jul/22 03:45;apachespark;User 'bzhaoopenstack' has created a pull request for this issue:
https://github.com/apache/spark/pull/37117;;;","08/Jul/22 03:45;apachespark;User 'bzhaoopenstack' has created a pull request for this issue:
https://github.com/apache/spark/pull/37117;;;","13/Jul/22 00:46;gurwls223;Issue resolved by pull request 37117
[https://github.com/apache/spark/pull/37117];;;",,,,,,,,,,,,,,
Mima complains with Scala 2.13 in the master branch,SPARK-39703,13470434,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,07/Jul/22 01:16,12/Dec/22 18:11,13/Jul/23 08:51,07/Jul/22 06:21,3.4.0,,,,,,,,,,,,,,3.4.0,,,,,Build,,,0,,,,,,"{code}
[error] spark-core: Failed binary compatibility check against org.apache.spark:spark-core_2.13:3.2.0! Found 6 potential problems (filtered 933)
[error]  * the type hierarchy of object org.apache.spark.deploy.DeployMessages#LaunchExecutor is different in current version. Missing types {scala.runtime.AbstractFunction7}
[error]    filter with: ProblemFilters.exclude[MissingTypesProblem](""org.apache.spark.deploy.DeployMessages$LaunchExecutor$"")
[error]  * method requestedTotal()Int in class org.apache.spark.deploy.DeployMessages#RequestExecutors does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[DirectMissingMethodProblem](""org.apache.spark.deploy.DeployMessages#RequestExecutors.requestedTotal"")
[error]  * method copy(java.lang.String,Int)org.apache.spark.deploy.DeployMessages#RequestExecutors in class org.apache.spark.deploy.DeployMessages#RequestExecutors's type is different in current version, where it is (java.lang.String,scala.collection.immutable.Map)org.apache.spark.deploy.DeployMessages#RequestExecutors instead of (java.lang.String,Int)org.apache.spark.deploy.DeployMessages#RequestExecutors
[error]    filter with: ProblemFilters.exclude[IncompatibleMethTypeProblem](""org.apache.spark.deploy.DeployMessages#RequestExecutors.copy"")
[error]  * synthetic method copy$default$2()Int in class org.apache.spark.deploy.DeployMessages#RequestExecutors has a different result type in current version, where it is scala.collection.immutable.Map rather than Int
[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](""org.apache.spark.deploy.DeployMessages#RequestExecutors.copy$default$2"")
[error]  * method this(java.lang.String,Int)Unit in class org.apache.spark.deploy.DeployMessages#RequestExecutors's type is different in current version, where it is (java.lang.String,scala.collection.immutable.Map)Unit instead of (java.lang.String,Int)Unit
[error]    filter with: ProblemFilters.exclude[IncompatibleMethTypeProblem](""org.apache.spark.deploy.DeployMessages#RequestExecutors.this"")
[error]  * method apply(java.lang.String,Int)org.apache.spark.deploy.DeployMessages#RequestExecutors in object org.apache.spark.deploy.DeployMessages#RequestExecutors in current version does not have a correspondent with same parameter signature among (java.lang.String,scala.collection.immutable.Map)org.apache.spark.deploy.DeployMessages#RequestExecutors, (java.lang.Object,java.lang.Object)java.lang.Object
[error]    filter with: ProblemFilters.exclude[IncompatibleMethTypeProblem](""org.apache.spark.deploy.DeployMessages#RequestExecutors.apply"")
{code}

https://github.com/apache/spark/runs/7221231391?check_suite_focus=true",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 06:21:46 UTC 2022,,,,,,,,,,"0|z16o0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Jul/22 01:40;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37109;;;","07/Jul/22 06:21;gurwls223;Issue resolved by pull request 37109
[https://github.com/apache/spark/pull/37109];;;",,,,,,,,,,,,,,,,
Uncaught exception in thread executor-heartbeater java.util.ConcurrentModificationException: mutation occurred during iteration,SPARK-39696,13470364,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,eejbyfeldt,smcmullan,smcmullan,06/Jul/22 14:07,07/Apr/23 04:03,13/Jul/23 08:51,07/Apr/23 04:02,3.3.0,3.4.0,,,,,,,,,,,,,3.3.3,3.4.0,,,,Spark Core,,,3,,,,,,"{noformat}
2022-06-21 18:17:49.289Z ERROR [executor-heartbeater] org.apache.spark.util.Utils - Uncaught exception in thread executor-heartbeater
java.util.ConcurrentModificationException: mutation occurred during iteration
        at scala.collection.mutable.MutationTracker$.checkMutations(MutationTracker.scala:43) ~[scala-library-2.13.8.jar:?]
        at scala.collection.mutable.CheckedIndexedSeqView$CheckedIterator.hasNext(CheckedIndexedSeqView.scala:47) ~[scala-library-2.13.8.jar:?]
        at scala.collection.IterableOnceOps.copyToArray(IterableOnce.scala:873) ~[scala-library-2.13.8.jar:?]
        at scala.collection.IterableOnceOps.copyToArray$(IterableOnce.scala:869) ~[scala-library-2.13.8.jar:?]
        at scala.collection.AbstractIterator.copyToArray(Iterator.scala:1293) ~[scala-library-2.13.8.jar:?]
        at scala.collection.IterableOnceOps.copyToArray(IterableOnce.scala:852) ~[scala-library-2.13.8.jar:?]
        at scala.collection.IterableOnceOps.copyToArray$(IterableOnce.scala:852) ~[scala-library-2.13.8.jar:?]
        at scala.collection.AbstractIterator.copyToArray(Iterator.scala:1293) ~[scala-library-2.13.8.jar:?]
        at scala.collection.immutable.VectorStatics$.append1IfSpace(Vector.scala:1959) ~[scala-library-2.13.8.jar:?]
        at scala.collection.immutable.Vector1.appendedAll0(Vector.scala:425) ~[scala-library-2.13.8.jar:?]
        at scala.collection.immutable.Vector.appendedAll(Vector.scala:203) ~[scala-library-2.13.8.jar:?]
        at scala.collection.immutable.Vector.appendedAll(Vector.scala:113) ~[scala-library-2.13.8.jar:?]
        at scala.collection.SeqOps.concat(Seq.scala:187) ~[scala-library-2.13.8.jar:?]
        at scala.collection.SeqOps.concat$(Seq.scala:187) ~[scala-library-2.13.8.jar:?]
        at scala.collection.AbstractSeq.concat(Seq.scala:1161) ~[scala-library-2.13.8.jar:?]
        at scala.collection.IterableOps.$plus$plus(Iterable.scala:726) ~[scala-library-2.13.8.jar:?]
        at scala.collection.IterableOps.$plus$plus$(Iterable.scala:726) ~[scala-library-2.13.8.jar:?]
        at scala.collection.AbstractIterable.$plus$plus(Iterable.scala:926) ~[scala-library-2.13.8.jar:?]
        at org.apache.spark.executor.TaskMetrics.accumulators(TaskMetrics.scala:261) ~[spark-core_2.13-3.3.0.jar:3.3.0]
        at org.apache.spark.executor.Executor.$anonfun$reportHeartBeat$1(Executor.scala:1042) ~[spark-core_2.13-3.3.0.jar:3.3.0]
        at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563) ~[scala-library-2.13.8.jar:?]
        at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561) ~[scala-library-2.13.8.jar:?]
        at scala.collection.AbstractIterable.foreach(Iterable.scala:926) ~[scala-library-2.13.8.jar:?]
        at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1036) ~[spark-core_2.13-3.3.0.jar:3.3.0]
        at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238) ~[spark-core_2.13-3.3.0.jar:3.3.0]
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) ~[scala-library-2.13.8.jar:?]
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066) ~[spark-core_2.13-3.3.0.jar:3.3.0]
        at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46) ~[spark-core_2.13-3.3.0.jar:3.3.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) ~[?:?]
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
        at java.lang.Thread.run(Thread.java:833) ~[?:?] {noformat}","Spark 3.3.0 (spark-3.3.0-bin-hadoop3-scala2.13 distribution)

Scala 2.13.8 / OpenJDK 17.0.3 application compilation

Alpine Linux 3.14.3

JVM OpenJDK 64-Bit Server VM Temurin-17.0.1+12",apachespark,dongjoon,eejbyfeldt,LuciferYang,mridulm80,PDS2208,smcmullan,yuryn,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Apr 07 04:02:51 UTC 2023,,,,,,,,,,"0|z16nkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Jul/22 18:29;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/37206;;;","16/Jul/22 18:29;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/37206;;;","26/Jul/22 02:28;mridulm80;Can you give more details on how this can be reproduced [~smcmullan] ?
Thanks;;;","10/Sep/22 07:59;PDS2208;It happens randomly. Run a job and all good and then again and the exception may, or may not, appear.;;;","16/Sep/22 05:08;yuryn;same for me. Is it possible to extend any timeout?;;;","04/Apr/23 14:51;eejbyfeldt;Created a test case that consistently reproduces the issue for when using Scala 2.13: [https://github.com/apache/spark/pull/40663];;;","07/Apr/23 04:02;dongjoon;This is resolved via https://github.com/apache/spark/pull/40663;;;",,,,,,,,,,,
NotExists subquery failed with conflicting attributes,SPARK-39672,13469967,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mauzhang,mauzhang,mauzhang,04/Jul/22 13:28,14/Jul/22 05:26,13/Jul/23 08:51,14/Jul/22 05:26,3.1.3,,,,,,,,,,,,,,3.1.4,3.2.2,3.3.1,3.4.0,,SQL,,,0,,,,,,"{code:sql}
select * from
(
select v1.a, v1.b, v2.c
from v1
inner join v2
on v1.a=v2.a) t3
where not exists (
  select 1
  from v2
  where t3.a=v2.a and t3.b=v2.b and t3.c=v2.c
){code}
This query throws AnalysisException
{code:java}
org.apache.spark.sql.AnalysisException: Found conflicting attributes a#266 in the condition joining outer plan:
  Join Inner, (a#250 = a#266)
:- Project [_1#243 AS a#250, _2#244 AS b#251]
:  +- LocalRelation [_1#243, _2#244, _3#245]
+- Project [_1#259 AS a#266, _3#261 AS c#268]
   +- LocalRelation [_1#259, _2#260, _3#261]and subplan:
  Project [1 AS 1#273, _1#259 AS a#266, _2#260 AS b#267, _3#261 AS c#268#277]
+- LocalRelation [_1#259, _2#260, _3#261] {code}",,apachespark,cloud_fan,mauzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 14 05:26:46 UTC 2022,,,,,,,,,,"0|z16l80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Jul/22 14:02;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/37074;;;","04/Jul/22 14:02;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/37074;;;","14/Jul/22 05:26;cloud_fan;Issue resolved by pull request 37074
[https://github.com/apache/spark/pull/37074];;;",,,,,,,,,,,,,,,
"Streaming Deduplication should not check the schema of ""value""",SPARK-39650,13469556,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,01/Jul/22 04:17,02/Jul/22 14:23,13/Jul/23 08:51,02/Jul/22 14:23,3.1.2,3.2.1,3.3.0,3.4.0,,,,,,,,,,,3.2.2,3.3.1,3.4.0,,,Structured Streaming,,,0,,,,,,"When we use dropDuplicate() in the streaming query, specifying the columns explicitly would perform deduplication against the columns rather than all columns. 

For the structure of state in streaming deduplication, we construct the key from ""specified"" columns and value as empty row (since it's not used at all). That said, once the query specifies the columns in dropDuplicate(), all other columns should not affect the operation.

Unfortunately, even we use the empty row as value of the state store, we register the ""all columns"" as the schema for the value on state store, which leads incorrect behavior from checking schema for state store. (This is figured out as a long-standing issue, it's from the initial implementation of StreamingDeduplicateExec.)

Specifically, columns for DataFrame which is applied to streaming deduplicate should be same across the lifetime of the query, whereas the only specified columns should be same actually.

It would be ideal to change the value schema to be empty, but the change itself may not be sufficient since schema file has been already written for older streaming queries. We may need to allow state schema compatibility checker to ignore value schema if required (either config or parameter of method if feasible).",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jul 02 14:23:16 UTC 2022,,,,,,,,,,"0|z16ioo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Jul/22 04:18;kabhwan;Will post a PR sooner.;;;","01/Jul/22 04:42;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/37041;;;","02/Jul/22 14:23;kabhwan;Issue resolved by pull request 37041
[https://github.com/apache/spark/pull/37041];;;",,,,,,,,,,,,,,,
